idx,url,identifier,abstract,article_title,publisher,date,keywords,journal_title,volume,issue,ISSN
0,http://dx.doi.org/10.1007/s42803-019-00002-y,"{'string_id': '10.1007/s42803-019-00002-y', 'id_scheme': 'DOI'}","The field of digital forensics seems at first glance quite separate from archival work and digital preservation. However, professionals in both fields are trusted to attest to the identity and integrity of digital documents and traces – they are regarded as experts in the acquisition, interpretation, description and presentation of that material. Archival science and digital forensics evolved out of practice and grew into established professional disciplines by developing theoretical foundations, which then returned to inform and standardize that practice. They have their roots in legal requirements and law enforcement. A significant challenge to both fields, therefore, is the identification of records (archival focus) and evidence (digital forensics focus) in digital systems, establishing their contexts, provenance, relationships, and meaning. This paper traces the development of digital forensics from practice to theory and presents the parallels with archival science.",From time theft to time stamps: mapping the development of digital forensics from law enforcement to archival authority,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,1,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
1,http://dx.doi.org/10.1007/s42803-019-00003-x,"{'string_id': '10.1007/s42803-019-00003-x', 'id_scheme': 'DOI'}","The impact of New Technologies on writing proccess is not new at all. This digital revolution first resulted in the appearance of new text formats and the development of an ad hoc literary theory. In Angloamerican area, this revolution made philologists and patrimonial institutions reflect on the necessity of developing formats of study, edition and perennial conservation of these new formats of digital texts. What is the reason for such a delay in these disciplines that can be observed in Europe? Why can we say that digital forensics and media archaeology (Kirschenbaum) are not trasnational disciplines? In this paper, I assess the impact in Europe and in Angloamerican area of .Txtual condition. Moreover, I make a contrast between these conclusions and the answers given by three emblematic writers of the ‘new Spanish narrative’ to a survey about ways of managing and preserving digital files.","The .txtual condition, .txtual criticism and .txtual scholarly editing in Spanish philology",Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,1,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
2,http://dx.doi.org/10.1007/s42803-019-00005-9,"{'string_id': '10.1007/s42803-019-00005-9', 'id_scheme': 'DOI'}","The Media Archaeology Lab (MAL) at the University of Colorado at Boulder (U.S.A.) acts as both an archive and a site for what the authors describe as ‘anarchival’ practice-based research and research creation. ‘Anarchival’ indicates research and creative activity enacted as a complement to an existing, stable archive. In researching the One Laptop Per Child Initiative, by way of a donation of XO laptops, the MAL has devised a modular process which could be used by other research groups to investigate the gap between the intended use and the affordances of any given piece of technology.",Anarchive as technique in the Media Archaeology Lab | building a one Laptop Per Child mesh network,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,1,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
3,http://dx.doi.org/10.1007/s42803-019-00006-8,"{'string_id': '10.1007/s42803-019-00006-8', 'id_scheme': 'DOI'}","Over the last two decades publishing and distributing content on the Web has become a core part of society. This ephemeral content has rapidly become an essential component of the human record. Writing histories of the late 20th and early 21st century will require engaging with web archives. The scale of web content and of web archives presents significant challenges for how research can access and engage with this material. Digital humanities scholars are advancing computational methods to work with corpora of millions of digitized resources, but to fully engage with the growing content of two decades of web archives, we now require methods to approach and examine billions, ultimately trillions, of incongruous resources. This article approaches one seemingly insignificant, but fundamental, aspect in web design history: the use of tiny transparent images as a tool for layout design, and surfaces how traces of these files can illustrate future paths for engaging with web archives. This case study offers implications for future methods allowing scholars to engage with web archives. It also prompts considerations for librarians and archivists in thinking about web archives as data and the development of systems, qualitative and quantitative, through which to make this material available.",The invention and dissemination of the spacer gif: implications for the future of access and use of web archives,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,1,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
4,http://dx.doi.org/10.1007/s42803-019-00007-7,"{'string_id': '10.1007/s42803-019-00007-7', 'id_scheme': 'DOI'}","The aim of this article is to provide an exploratory analysis of the landscape of web archiving activities in Europe. Our contribution, based on desk research, and complemented with data from interviews with representatives of European heritage institutions, provides a descriptive overview of the state-of-the-art of national web archiving in Europe. It is written for a broad interdisciplinary audience, including cultural heritage professionals, IT specialists and managers, and humanities and social science researchers. The legal, technical and operational aspects of web archiving and the value of web archives as born-digital primary research resources are both explored. In addition to investigating the organisations involved and the scope of their web archiving programmes, the curatorial aspects of the web archiving process, such as selection of web content, the tools used and the provision of access and discovery services are also considered. Furthermore, general policies related to web archiving programmes are analysed. The article concludes by offering four important issues that digital scholars should consider when using web archives as a historical data source. Whilst recognising that this study was limited to a sample of only nine web archives, this article can nevertheless offer some useful insights into the technical, legal, curatorial and policy-related aspects of web archiving. Finally, this paper could function as a stepping stone for more extensive and qualitative research.",Web archives as a data resource for digital scholars,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,1,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
5,http://dx.doi.org/10.1007/s42803-019-00008-6,"{'string_id': '10.1007/s42803-019-00008-6', 'id_scheme': 'DOI'}","The way researchers in the arts and humanities disciplines work has changed significantly. Research can no longer be done in isolation as an increasing number of digital tools and certain types of knowledge are required to deal with research material. Research questions are scaled up and we see the emergence of new infrastructures to address this change. The DigitAl Research Infrastructure for the Arts and Humanities (DARIAH) is an open international network of researchers within the arts and humanities community, which revolves around the exchange of experiences and the sharing of expertise and resources. These resources comprise not only of digitised material, but also a wide variety of born-digital data, services and software, tools, learning and teaching materials. The sustaining, sharing and reuse of resources involves many different parties and stakeholders and is influenced by a multitude of factors in which research infrastructures play a pivotal role. This article describes how DARIAH tries to meet the requirements of researchers from a broad range of disciplines within the arts and humanities that work with (born-)digital research data. It details approaches situated in specific national contexts in an otherwise large heterogeneous international scenario and gives an overview of ongoing efforts towards a convergence of social and technical aspects.",A landscape of data – working with digital resources within and beyond DARIAH,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,1,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
6,http://dx.doi.org/10.1007/s42803-019-00010-y,"{'string_id': '10.1007/s42803-019-00010-y', 'id_scheme': 'DOI'}","Web 25. Histories from the first 25 years of the world wide web, edited by Niels Brügger, is part of the Digital Formations series. Brügger’s collection takes the challenge of placing the (hi)story of the World Wide Web in a critical perspective seriously. The volume seeks to foster those working in web archiving, internet studies or web historiography to undertake innovative, cross-disciplinary research. The editor has brought together authors who collectively have contributed to a book that is a valuable addition to the emerging scholarship surrounding the study of the web and the web’s history. The book, divided in four sections, comprises ‘a number of probes into the vast and multifaceted past of the web’ (xi). However, the volume is neither designed to be exhaustive, nor comprehensive. It is broad in scope in relation to a number of aspects: (i) its variety of topics, (ii) its combination of case studies and methodological reflections and (iii) the compilation of chapters focusing on national as well as international WWW phenomena.",Web 25. Histories from the first 25 years of the world wide web,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,1,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
7,http://dx.doi.org/10.1007/s42803-019-00011-x,"{'string_id': '10.1007/s42803-019-00011-x', 'id_scheme': 'DOI'}","The first special issue of International Journal of Digital Humanities (IJDH) is about born-digital archives, their preservation and research perspectives involving born-digital primary records in the humanities. This is not only a result of the collaboration between the journal’s editor-in-chief, Gábor Palkó, Co-Director of the Centre for Digital Humanities at the Eötvös University, who is interested in the practice and theory of digital archives, and the editor of this volume, Thorsten Ries, who conducts research on born-digital dossiers génétiques with digital forensic methods at Ghent University. It is also meant to be a programmatic call to intensify cross-sectoral collaboration between galleries, libraries, archives, and museums (GLAM institutions), digital preservation projects, and humanities research working with digital primary sources.",Born-digital archives,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,1,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
8,http://dx.doi.org/10.1007/s42803-019-00012-w,"{'string_id': '10.1007/s42803-019-00012-w', 'id_scheme': 'DOI'}","The technological developments in the field of textual scholarship lead to a renewed focus on textual variation. Variants are liberated from their peripheral place in appendices or footnotes and are given a more prominent position in the (digital) edition of a work. But what constitutes an informative and meaningful visualisation of textual variation? The present article takes visualisation of the result of collation software as point of departure, examining several visualisations of collation output that contains a wealth of information about textual variance. The newly developed collation software HyperCollate is used as a touchstone to study the issue of representing textual information to advance literary research. The article concludes with a set of recommendations in order to evaluate different visualisations of collation output.",From graveyard to graph,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
9,http://dx.doi.org/10.1007/s42803-019-00013-9,"{'string_id': '10.1007/s42803-019-00013-9', 'id_scheme': 'DOI'}","Dealing with documents that have changed through time requires keeping track of additional metadata, for example the order of the revisions. This small issue explodes in complexity when these documents are translated. Even more complicate is keeping track of the parallel evolution of a document and its translations. The fact that this extra metadata has to be encoded in formal terms in order to be processed by computers has forced us to reflect on issues that are usually overlooked or, at least, not actively discussed and documented: How do I record which document is a translation of which? How do I record that this document is a translation of that specific revision of another document? And what if a certain translation has been created using one or more intermediate translations with no access to the original document? In this paper we addresses all these issues, starting from first principles and incrementally building towards a comprehensive solution. This solution is then distilled in terms of formal concepts (e.g., translation, abstraction levels, comparability, division in parts, addressability) and abstract data structures (e.g., derivation graphs, revisions-alignment tables, source-document tables, source-part tables). The proposed data structures can be seen as a generalization of the classical evolutionary trees (e.g., stemma codicum), extended to take into account the concepts of translation and contamination (i.e., multiple sources). The presented abstract data structures can easily be implemented in any programming language and customized to fit the specific needs of a research project.","Tracking the evolution of translated documents: revisions, languages and contaminations",Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
10,http://dx.doi.org/10.1007/s42803-019-00014-8,"{'string_id': '10.1007/s42803-019-00014-8', 'id_scheme': 'DOI'}","Online book discussion is a popular activity on weblogs, specialized book discussion sites, booksellers’ sites and elsewhere. These discussions are important for research into literary reception and should be made and kept accessible for researchers. This article asks what an archive of online book discussion should and could look like, and how we could describe such an archive in terms of some of the central concepts of textual scholarship: work, document, text, transcription and variant. What could an approach along the lines of textual scholarship mean for such a collection? If such a collection holds many pieces of information that would not usually be considered text (such as demographic information about contributors), could we still call such a collection an edition, and could we call editing the activity of preparing such a collection?The article introduces some of the relevant (Dutch-language) sites, and summarizes their properties (among others: they are dynamic and vulnerable, they contain structured data and are very large) from the perspective of creating a research collection. It discusses the interpretation of some essential terms of textual studies in this context, and briefly lists a number of components that a digital edition of these sites might or should contain. It argues that such a collection is the result of scholarly work and should not be considered as 'just' a web archive.",Editing social media: the case of online book discussion,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
11,http://dx.doi.org/10.1007/s42803-019-00015-7,"{'string_id': '10.1007/s42803-019-00015-7', 'id_scheme': 'DOI'}","In the context of a specific hybrid project—a digital archive and a print edition of the complete works of American writer Charles W. Chesnutt (1858–1932)--we consider three issues: (1) the value of print editions, notwithstanding the flexibility, capaciousness, and accessibility of digital editions; (2) the distinct affordances of digital editing in general and in this case; and (3) the challenges of a hybrid approach, and the possiblility of supplementing the now standard digital approach to rendering paper manuscripts (high quality scans and TEI-compliant transcriptions) with approaches borrowed from print and print aesthetics.",Charles Chesnutt and the case for hybrid editing,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
12,http://dx.doi.org/10.1007/s42803-019-00016-6,"{'string_id': '10.1007/s42803-019-00016-6', 'id_scheme': 'DOI'}","This article argues that editors of scholarly digital editions should not be distracted by underlying technological concerns except when these concerns affect the editorial tasks at hand. It surveys issues in the creation of scholarly digital editions and the open licensing of resources and addresses concerns about underlying data models and vocabularies, such as the Guidelines of the Text Encoding Initiative. It calls for solutions which promote the collaborative creation, annotation, and publication of scholarly digital editions. The article draws a line between issues with which editors of scholarly digital editions should concern themselves and issues which may only prove to be distractions.",Opening the book: data models and distractions in digital scholarly editing,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
13,http://dx.doi.org/10.1007/s42803-019-00017-5,"{'string_id': '10.1007/s42803-019-00017-5', 'id_scheme': 'DOI'}","The concepts of original and copy, of source and facsimile, always convey particular understandings of the process of reproducing documents. This essay is an analysis of these concepts, in particular copies and facsimiles, framed within the context of digital reproduction. The activities and cases discussed are picked from two areas: digital scholarly editing and cultural heritage digitization performed by research libraries. The conceptual analysis draws on three fields of scholarly inquiry: scholarly editing, library and information science, and philosophical aesthetics.",Copies and facsimiles,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
14,http://dx.doi.org/10.1007/s42803-019-00018-4,"{'string_id': '10.1007/s42803-019-00018-4', 'id_scheme': 'DOI'}","Building on a longstanding terminological discussion in the field of textual scholarship, this essay explores the archival and editorial potential of the digital scholarly edition. Following Van Hulle and Eggert, the author argues that in the digital medium these traditionally distinct activities now find the space they need to complement and reinforce one another. By critically examining some of the early and more recent theorists and adaptors of this relatively new medium, the essay aims to shed a clearer light on some of its strengths and pitfalls. To conclude, the essay takes the discussion further by offering a broader reflection on the difficulties of providing a ‘definitive’ archival base transcription of especially handwritten materials, questioning if this should be something to aspire to for the edition in the first place.",On edited archives and archived editions,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
15,http://dx.doi.org/10.1007/s42803-019-00019-3,"{'string_id': '10.1007/s42803-019-00019-3', 'id_scheme': 'DOI'}","Digital scholarly editions are expensive to make and to maintain. As such, they prove unattainable for less established scholars like early careers and PhD students, or indeed anyone without access to significant funding. One solution could be to create tools and platforms able to provide a publishing framework for digital scholarly editions that requires neither a high-tech skillset nor big investment. I call this type of edition “Prêt-à-Porter”, to be distinguished from “haute couture” editions which are tailored to the specific needs of specific scholars. I argued that both types of editions are necessary for a healthy scholarly environment.",What future for digital scholarly editions? From Haute Couture to Prêt-à-Porter,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
16,http://dx.doi.org/10.1007/s42803-019-00020-w,"{'string_id': '10.1007/s42803-019-00020-w', 'id_scheme': 'DOI'}","This special issue on Digital Scholarly Editing introduces several new developments, both in terms of the theory and practice of textual scholarship that are taking shape in the digital medium. But instead of enumerating or summarizing them, this introduction is meant as a reflection on some of the possible ways in which our work in digital scholarly editing could be useful to the broader field of digital humanities. After all, we tend to work on a microscale compared to the forms of macroanalysis and ‘distant’ reading that are currently dominant in digital literary studies. This raises the question: how can our research be relevant to these other sub-disciplines in digital humanities?","Artificial imagination, imagine: new developments in digital scholarly editing",Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
17,http://dx.doi.org/10.1007/s42803-019-00021-9,"{'string_id': '10.1007/s42803-019-00021-9', 'id_scheme': 'DOI'}","This is a history of and a technical report on the Charles Harpur Critical Archive (CHCA), which has been in preparation since 2009. Harpur was a predominantly newspaper poet in colonial New South Wales from the 1830s to the 1860s. Approximately 2700 versions of his 700 poems in newspaper and manuscript form have been recovered. In order to manage the complexity of his often heavily revised manuscripts, traditional encoding in XML-TEI, with its known difficulties in handling overlapping structures and complex revisions, was rejected. Instead, the transcriptions were split into simplified versions and layers of revision. Markup describing textual formats was stored externally using properties that may freely overlap. Both markup and the versions and layers were merged into multi-version documents (MVDs) to facilitate later comparison, editing and searching. This reorganisation is generic in design and should be reusable in other editorial projects.",The Charles Harpur Critical Archive,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
18,http://dx.doi.org/10.1007/s42803-019-00023-7,"{'string_id': '10.1007/s42803-019-00023-7', 'id_scheme': 'DOI'}","The article presents a model for annotating textual variants. The annotations made can be queried in order to analyse and find patterns in textual variation. The model is flexible, allowing scholars to set the boundaries of the readings, to nest or concatenate variation sites, and to annotate each pair of readings; furthermore, it organizes the characteristics of the variants in features of the readings and features of the variation. After presenting the conceptual model and its applications in a number of case studies, this article introduces two implementations in logical models: namely, a relational database schema and an OWL 2 ontology. While the scope of this article is a specific issue in textual criticism, its broader focus is on how data is structured and visualized in digital scholarly editing.",Exercises in modelling: textual variants,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
19,http://dx.doi.org/10.1007/s42803-019-00024-6,"{'string_id': '10.1007/s42803-019-00024-6', 'id_scheme': 'DOI'}","3D (re)constructions of heritage sites and Digital Scholarly Editions face similar needs and challenges and have many concepts in common, although they are expressed differently. 3D (re)constructions, however, lack a framework for addressing them. The goal of this article is not to create a single or the lowest common denominator to which both DSEs and 3D models subscribe, nor is it to reduce 3D to one scholarly editing tradition. It is rather to problematise the development of a model by borrowing concepts and values from editorial scholarship in order to enable public-facing 3D scholarship to be read in the same way that scholarly editions are by providing context, transmission history, and transparency of the editorial method/decision-making process.",Textuality in 3D: three-dimensional (re)constructions as digital scholarly editions,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
20,http://dx.doi.org/10.1007/s42803-019-00025-5,"{'string_id': '10.1007/s42803-019-00025-5', 'id_scheme': 'DOI'}","The paper describes the special interest among historians in scholarly editing and the resulting editorial practice in contrast to the methods applied by pure philological textual criticism. The interest in historical ‘facts’ suggests methods the goal of which is to create formal representations of the information conveyed by the text in structured databases. This can be achieved with RDF representations of statements extracted from the text, by automatic information extraction methods, or by hand. The paper suggests the use of embedded RDF representations in TEI markup, following the practice in several recent projects, and it concludes with a proposal for a definition of the ‘assertive edition’.",The ‘assertive edition’,Springer Science and Business Media LLC,2019,,International Journal of Digital Humanities,1,2,"[{'value': '2524-7832', 'type': 'print'}, {'value': '2524-7840', 'type': 'electronic'}]"
21,http://dx.doi.org/10.1093/digitalsh/fqz013,"{'string_id': '10.1093/digitalsh/fqz013', 'id_scheme': 'DOI'}","In this article we establish a methodological and theoretical framework for the study of large collections of visual materials. Our framework, distant viewing, is distinguished from other approaches by making explicit the interpretive nature of extracting semantic metadata from images. In other words, one must ‘view’ visual materials before studying them. We illustrate the need for the interpretive process of viewing by simultaneously drawing on theories of visual semiotics, photography, and computer vision. Two illustrative applications of the distant viewing framework to our own research are draw upon to explicate the potential and breadth of the approach. A study of television series shows how facial detection is used to compare the role of actors within the narrative arcs across two competing series. An analysis of the Farm Security Administration–Office of War Information corpus of documentary photography is used to establish how photographic style compared and differed amongst those photographers involved with the collection. We then aim to show how our framework engages with current methodological and theoretical conversations occurring within the digital humanities.",Distant viewing: analyzing large visual corpora,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
22,http://dx.doi.org/10.1093/llc/15.1.109,"{'string_id': '10.1093/llc/15.1.109', 'id_scheme': 'DOI'}","It is apparent that the presentation of a text in the electronic medium as a string of electronic characters may have advantages for scholarly reading. Because it is dynamic, the text on the screen offers the reader many possible kinds of fragmentation. Study of scholarly uses of such texts shows that the segmentations in the text depend on the computer treatments available, such as indexing or linking commentaries. The hypertextual paths induced in such a way by the constraints of indexing systems or on the adjacent commentaries are different from a so-called 'open' reading (in the sense of a reader's engagement), which aims to permit the reader to define paths according to previously determined units. This statement allows us to reopen the debate on the relationship between digitization and hypertextualization. Moreover, it raises questions about the type of reading that emerges from codification and structuring models. The argument of this paper is that the status as object of electronic forms of canonical texts must be reconsidered according to the specific features of the medium.",Pour une lecture ouverte du texte electronique,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
23,http://dx.doi.org/10.1093/llc/15.1.115,"{'string_id': '10.1093/llc/15.1.115', 'id_scheme': 'DOI'}","The evolution of data processing, in terms of hardware, software, and communication structures, renders it possible today to conceive of a scholarly text edition as an electronic edition. The essential prerequisite for an electronic edition is the preparation of its object as a computer-assisted edition. The critical and synoptic edition of James Joyce's Ulysses, published in book form in 1984/86, was computer-assisted through the entire range of textual, critical, and presentational preparation. An electronic edition can be designed and realised from that book edition's database. The present contribution surveys the demands to be made of such an electronic edition and indicates what conceptual, editorial and technical operations and operationalizations are required to achieve it.",Towards an electronic edition of James Joyce's Ulysses,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
24,http://dx.doi.org/10.1093/llc/15.1.15,"{'string_id': '10.1093/llc/15.1.15', 'id_scheme': 'DOI'}","The Perseus digital library is a substantial test bed of materials on archaic and classical Greece, the early Roman empire, and early modern Europe. The Perseus architecture includes tools that fit the needs of humanists: linguistic analysis for heavily inflected languages, linking and alignment with canonical citation schemes, and terminological, spatial, and visual databases for document contextualization. These tools provide both the scalability to connect disparate entities in the digital library and a groundwork for performance of the synthetic scholarship of the humanities.",The Perseus Project: a digital library for the humanities,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
25,http://dx.doi.org/10.1093/llc/15.1.27,"{'string_id': '10.1093/llc/15.1.27', 'id_scheme': 'DOI'}","The materials for editing the New Testament consist of over 5,600 Greek manuscripts, ancient translations and citations by church fathers. Computers are of use in the study and editing of them in eight ways (i) in collating witnesses, especially to improve accuracy; (ii) in being able to alter a base text without having to revise a complicated apparatus criticus; (iii) in analysis of manuscript relationships; (iv) in the selection of the most significant witnesses; (v) in producing an edition; (vi) in the area of collaboration; (vii) they do away with the need to redo good work; (viii) they make possible a wide range of presentations, including change of presentation at will, use of multiple typefaces cheaply, presenting multiple states of the text, and placing information in levels, connected to one another by hyperlinks. Even though the goal of any major critical edition should continue to be a paper edition, it will be worth scholars' while to develop computer-based resources.",The text of the New Testament and computers: The International Greek New Testament Project,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
26,http://dx.doi.org/10.1093/llc/15.1.43,"{'string_id': '10.1093/llc/15.1.43', 'id_scheme': 'DOI'}","The Editio Critica Maior of the New Testament aims to exhibit the history of the Greek text through its first millennium as documented in more than 5,000 manuscripts from the second century onward. It will provide scholars engaged in the tasks of exegesis and textual criticism with all the relevant materials found in Greek manuscripts, patristic citations, and the early translations. The first instalment of the paper edition containing the Letter of James appeared in 1997. All the material published so far and being prepared for the next instalment is stored in databases. Thus a fundamental requirement of a future electronic edition is met. Further steps in the same direction are being currently undertaken. Transcriptions are meanwhile entered directly using the Collate program. This will help to provide the facilities for external co-editors and coworkers to participate in the Münster edition project. With respect to all the promising prospects of the 'digital age', it is most important to concentrate on what is feasible now and to prepare for the transition to electronic editing at the same time.",Editing the Greek New Testament on the threshold of the twenty-first century,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
27,http://dx.doi.org/10.1093/llc/15.1.5,"{'string_id': '10.1093/llc/15.1.5', 'id_scheme': 'DOI'}","The capacity of electronic editions to present all versions of a text, and the movement in post-modern thinking against all forms of authority, suggest that electronic editions should not present any reconstructed or eclectic text, or privilege any one text over all other texts. However, examination of three actual editions dealing with very large numbers of variant texts (Chaucer's Canterbury Tales, Dante's Commedia, and the Greek New Testament) argues the reverse. In these cases, editors might choose to present a single reconstructed text, and to privilege this text about all other texts. It is not necessary to see this reconstructed text as a precise representation of a lost original: rather, it should be seen as the text that best explains all the extant documents. Its value then lies in its efficiency as a route by which the reader may find his or her own way into the variants themselves. Such editions may then help their users to become better readers, and editions should be measured by their ability to do this rather than by their conformity to any theoretical model.",The one text and the many texts,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
28,http://dx.doi.org/10.1093/llc/15.1.51,"{'string_id': '10.1093/llc/15.1.51', 'id_scheme': 'DOI'}","The same data as used for creating the new printed Editio Critica Maior of the New Testament, commencing with the Catholic Letters, allows a genealogical analysis of the witness. The objective is to establish a comprehensive theory of the structure of the tradition. Because the tradition of the New Testament is highly contaminated this theory has to handle the problem of contamination, and also the problem of accidental rise of variants, and must be able to be verified at any passage of text. Where there are variants, the witnesses have a relation that can be described by a local stemma of the different readings. These local stemmata allow or restrict relations among witnesses in a global stemma, which must be in harmony with the total of the local stemmata. In the first phase, local stemmata were established only at places where the development of the variants is very clear. The coherencies within each attestation were analysed. This analysis rested upon the agreements of witnesses, for their genealogical relations were unknown. Nevertheless, readings could be excluded from being primary by checking the coherencies of the witnesses. Then the local stemmata must be revised in the light of the total of the genealogical data included in them. Now an analysis of genealogical coherence is possible and may help to find local stemmata for passages unsolved so far. Finally, the global stemma (or stemmata) mirroring all the relations of the local stemmata will be established by combining optimal substemmata, each containing a witness and its immediate ancestors, to produce the simplest possible tree.",Editing and genealogical studies: the New Testament,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
29,http://dx.doi.org/10.1093/llc/15.1.57,"{'string_id': '10.1093/llc/15.1.57', 'id_scheme': 'DOI'}","This paper presents two projects involving Biblical texts. The first one aims to produce a critical edition of the Old Slavic New Testament, the second tries to reconstruct the Diatessaron, a harmony of the Gospels, composed in Rome in the second century and transmitted in a great number of languages. The paper briefly describes how computer collation of Slavic manuscripts can form the basis for a critical edition and how Diatessaronic evidence in different languages can be compiled electronically. Common problems of employing modern technologies in these traditional areas of philological research are discussed and, finally, some advice is offered.",Some bits of good news: computer collation of the old Slavic New Testament and Tatian's gospel harmony,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
30,http://dx.doi.org/10.1093/llc/15.1.65,"{'string_id': '10.1093/llc/15.1.65', 'id_scheme': 'DOI'}","Scholarly editing in the Netherlands is concentrated in the Constantijn Huygens Institut (CHI) in The Hague, which is a research institute of the Royal Netherlands Academy of Arts and Sciences. The CHI produces text editions of works from every period of Dutch literary history. All genres and types of editions are represented. Editorial methods and techniques developed in other countries are adopted according to the needs of the Institute. In the past, most projects at the CHI resulted in printed publications and little consideration was given to production or presentation using electronic media. In the recent planning and development of new projects, however, an important shift can be seen in favour of these media. For two of the projects outlined in this paper this includes or has included the digitization of all relevant versions of a text through scanning and OCR. Computer programs such as TUSTEP and COLLATE will be used to compare all of those versions. The copy of the editions will be stored in SGML to make future reprints possible. Lists of authorial variants and perhaps other research material will be made available on the Internet. The third project discussed here involves the publication in its entirety of a sizeable body of nineteenth-century poetry by means of a CD-ROM/DVD or the Internet. The full-text electronic edition will also contain digital images of the original documents as well as hypertext links to the author's annotations.",Scholarly editing in The Netherlands,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
31,http://dx.doi.org/10.1093/llc/15.1.73,"{'string_id': '10.1093/llc/15.1.73', 'id_scheme': 'DOI'}","In the 1980s a group of scholars in Rome studied the methods of humanities computing, and elaborated a theory of the correct use of the computer in the different disciplines. Concerning electronic editions of texts, it is essential that they preserve all the information present in the representation on paper, which goes beyond the sequence of letters. According to this principle, two enterprises have been initiated, which are described here.","Progetti relativi a testi elettronici a Roma, accademia dei lincei e Universita La Sapienza",Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
32,http://dx.doi.org/10.1093/llc/15.1.81,"{'string_id': '10.1093/llc/15.1.81', 'id_scheme': 'DOI'}","The Text Encoding Initiative's (TEI's) Guidelines for Electronic Text Encoding and Interchange (Chicago, Oxford TEI P3) provide an extremely rich set of mechanisms for the encoding of medieval manuscripts and other primary textual sources. For a number of problems regularly encountered in the transcription of Old Norse/Icelandic materials, however, the solutions offered by the TEI have not proved entirely satisfactory. A group of scholars from several Scandinavian universities has been working for the last 4 years on a set of recommendations for modifications and extensions to the TEI DTD to be used in the encoding of such material. The present paper discusses some of these recommendations.",Encoding old Norse/Icelandic primary sources using TEI-conformant SGML,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
33,http://dx.doi.org/10.1093/llc/15.1.93,"{'string_id': '10.1093/llc/15.1.93', 'id_scheme': 'DOI'}","Modularity, professionality, portability, and integration are the key features of the TUebingen System for TExt Processing programs (TUSTEP), a professional toolbox for those academic fields where texts are the object of research. Its potential is illustrated by two examples: (i) typesetting a TEI-lite encoded text, using the TEI tags as formatting instructions; (ii) preparing a critical edition, starting from automatic collation, then semi-automatically selecting the 'substantial' variants from the collation results, transforming them into a critical apparatus, and publishing the edition both in print and electronically.",Strategies and tools for textual scholarship: the Tubingen system of text processing programs (TUSTEP),Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
34,http://dx.doi.org/10.1093/llc/15.2.121,"{'string_id': '10.1093/llc/15.2.121', 'id_scheme': 'DOI'}","Many recent studies have demonstrated that the analysis of world co-occurrence patterns can be a valuable tool for the acquisition of lexical knowledge from unstructured texts. The success of these studies raises the question of whether these results can be replicated in other languages and, in particular, in ancient Greek texts. The adaptation of these techniques for the acquisition of lexical knowledge could provide the foundation for useful philological and lexicographical research tools and multi-lingual information retrieval applications. One method that has proven useful in the study of Greek texts is the use of co-occurrence data to calculate mutual information scores. The approaches used for English language texts, however, require some modification for use with ancient Greek. This paper will describe the methods and modifications required for the analysis of collocational data from unstructured ancient Greek texts so that they produce useful results.",Co-occurrence patterns and lexical acquisition in ancient Greek texts,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
35,http://dx.doi.org/10.1093/llc/15.2.131,"{'string_id': '10.1093/llc/15.2.131', 'id_scheme': 'DOI'}","This paper summarizes our experience in optimizing representational models of inflexional (accidence) morphological analysis for different languages. Some of the linguistic engineering questions addressed in the paper include the following: How far can linguistic adequacy be maintained in morphological components of marketable machine translation software on the IBM PC platform? What generalizations can be made contributing to the more uniform approach to morphology for languages of various structures? What are the parameters that can be adjusted quantitatively to allow for general model application to different languages? A generalized approach of treating postfixal languages is proposed, which is supplemented by suggestions on its modification for languages that are clearly not postfixal (e.g. Arabic).",Morphological representation in PC-based text processing systems,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
36,http://dx.doi.org/10.1093/llc/15.2.157,"{'string_id': '10.1093/llc/15.2.157', 'id_scheme': 'DOI'}","Two recent editions of the play Edward III have regarded it as belonging to the Shakespeare canon. This paper explores the use of committees of cumulative sum charts based on line-by-line tallies of prosodic features and relative frequencies of both rare-words and common function words, assembled together and focused, as it were, by means of principal component analysis to provide a single best-fit profile of the play. Such a profile casts light, with some amendments, on previously remarked textual features of the play - notably thematic links between Acts II and IV, which contrast with the overall theme of the play. The divergence of Acts II and IV, on the one hand, and Acts I, III, and V on the other, a feature of several other plays with divided authorship, is suggestive of a Marlovian framework, reworked and added to by Shakespeare, possibly after Marlowe's death in 1593. The results of the analysis are compared with recent logometric examinations of the play's authorship. Agreement is greater than disagreement.",Edward III,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
37,http://dx.doi.org/10.1093/llc/15.2.187,"{'string_id': '10.1093/llc/15.2.187', 'id_scheme': 'DOI'}","Previous work in using artificial neural networks for computational stylistics has concentrated on using large, arbitrary network structures. This paper examines the use of the Cascade-Correlation algorithm for the construction of minimal networks. We find that a number of problems in computational stylistics with a large number of variables but a limited number of training examples may be solved successfully without resorting to large networks. The issue of redundancy in the data is also considered.",Computational stylistics using artificial neural networks,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
38,http://dx.doi.org/10.1093/llc/15.2.199,"{'string_id': '10.1093/llc/15.2.199', 'id_scheme': 'DOI'}","Kathleen Garay teaches in the Department of History and Women's Studies at McMaster University, where she is also an archivist. David Walker is the Project Manager of multimedia instructional computing at McMaster University. With the assistance of a small group of medievalists and a programmer-graphic designer they created a multimedia CD-ROM-based program on Women in the Middle Ages and Renaissance. The CD-ROM was used in class for the first time in the 1996-7 academic year, and a revised version is currently being distributed by an academic multimedia publisher. This paper presents a synthesis of two perspectives on the development of this teaching resource: teacher and computing professional.","Bringing computing into the Middle Ages: the making of sybils!, a multimedia CD-ROM",Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
39,http://dx.doi.org/10.1093/llc/15.3.251,"{'string_id': '10.1093/llc/15.3.251', 'id_scheme': 'DOI'}","In this paper, we address the issue of the effective reduction of out-of-vocabulary (OOV) words for automatic speech recognition (ASR) systems. We first of all evaluate the OOV rates produced by different vocabulary sets selected from a corpus of British English according to the raw frequency of occurrence. We demonstrate that OOV rates in realistic input from unlimited domains are much higher than has been reported in the literature for ASR systems that typically deal with only a subset of the English language. To reduce OOV rates, we then propose that the textual dispersion of word types is a more effective selection criterion for the acquisition of lexicons than the conventional method of lexical selection according to raw frequencies of occurrence. We evaluate the performance of the adjusted frequency according to the index of dispersion, and the dispersion of word types among component text categories of the training corpus. With an 80,000-word vocabulary, the estimated frequency per million words adjusted according the index of dispersion achieves and improvement of 7.3 per cent over the frequency-based approach for a large set of testing material from a variety of sources. Vocabulary sets selected according to textual dispersion alone achieve a slightly better overall OOV reduction rate of 7.5 per cent.",Out-of-vocabulary rate reduction through dispersion-based lexicon acquisition,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
40,http://dx.doi.org/10.1093/llc/15.3.291,"{'string_id': '10.1093/llc/15.3.291', 'id_scheme': 'DOI'}","This paper presents an implementation of a syntactic parser for the Modern Greek language based on the PC-PATR formalism. The Modern Greek language presents a high flexibility in its syntactic structure. The various phrasal categories (noun phrases, verb phrases, prepositional, and adverbial phrases) can be ordered in almost any permutation so as to build valid sentences. The allowable combinations of lexical categories (nouns, verbs, adjectives, etc.) forming the above phrases, as well as the combinations of these phrases, forming clauses, are encoded according to the PC-PATR formalism and are presented in this paper. The presented rules cover the majority of the syntactic phenomena of the Modern Greek language, adequate for speech and natural language applications.",A PC-PATR-Based syntactic description of modern Greek,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
41,http://dx.doi.org/10.1093/llc/15.3.313,"{'string_id': '10.1093/llc/15.3.313', 'id_scheme': 'DOI'}","A natural language generation system used for computer-aided language learning generates stimuli, or 'questions', along with one or more anticipated learner responses. To provide the learner with intelligent feedback, the system initially compares the actual responses entered by the learner with those produced by the system. The differences discovered in this way may be though of as symptoms; the eventual aim is to diagnose the deeper problems, misunderstandings of the language that underlie the symptoms, to offer corrective advice. This paper considers alternative approaches to the comparison process, and describes a two-level algorithm, based on approximate string matching, which determines and categorizes the differences between learner and system responses. The process detects variations in word order, insertions and deletions. It also reports on the use of synonyms or malwords, errors in morphology, phonological errors and typing slips. In subsequent work, these will form the basis for the adaptive generation of new stimuli, as well as for the deeper diagnosis of learner difficulties. Some typical results are shown.",A multi-level approach to the detection of second language learners errors,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
42,http://dx.doi.org/10.1093/llc/15.3.323,"{'string_id': '10.1093/llc/15.3.323', 'id_scheme': 'DOI'}","This paper describes software (vocd) that implements a solution to problems encountered in quantifying vocabulary diversity. Researchers in various fields of linguistic enquiry have calculated vocabulary diversity using the ratio of different words (Types) to total words (Tokens) - the Type-Token Ratio (TTR) - or measures derived from it. Such measures are flawed, however, because the values obtained are related to the number of words in the sample. The paper shows how the relationship between TTR and sample size can be described by a new mathematical model, which in turn leads to an innovative method of measuring vocabulary diversity. The software automates measurement from transcripts prepared in a widely used computer-readable set of conventions: the CHAT format of the CHILDES project. Options in vocd are described to show how the user can determine which linguistic items will count as valid types and tokens in the analysis. The new measure is calculated by, first, randomly sampling words from the transcript to produce a curve of the TTR against Tokens for the empirical data. Then the software finds the best fit between this empirical curve and theoretical curves calculated from the model by adjusting the value of a parameter. The parameter, D, is shown to be a valid and reliable measure of vocabulary diversity without the problems of sample size found with previous methods.",Measuring vocabulary diversity using dedicated software,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
43,http://dx.doi.org/10.1093/llc/15.3.339,"{'string_id': '10.1093/llc/15.3.339', 'id_scheme': 'DOI'}","The production of collections of grammatically analysed linguistic samples, or corpora, entails a parallel research effort in tools for exploiting such collections. This paper summarizes the new one million-word parsed British Component of the International Corpus of English (ICE-GB). We then introduce a tool designed annotating, managing, and investigating the parsed corpus, called ICECUP III. Effective grammatical exploration is driven by a query. We therefore pose the question, what is the optimum representation for expressing queries on a parsed corpus? We review existing grammatical query systems and a feasible representation in logic. We describe ICECUP's query representation, the Fuzzy Tree Fragment (FTF), which has proven extremely effective in supporting the manual correction of automatically annotated material in ICE-GB and is currently used for linguistic research. Central to the idea of FTFs is that they should be as easy to understand as possible and support researchers as they engage with the complexity of the parsed analysis. An FTF, therefore, is a kind of 'abstracted model' of a tree. We argue that FTFs are highly applicable to general linguistic research and teaching with grammatically analysed corpora. They support an exploratory approach that does not presume a detailed knowledge of the grammar in advance of search. Exploration is supported by a number of facilities, including a tool that abstracts a query from part of a corpus tree. Finally, we discuss how FTFs may be used to perform 'key construction in line' concordancing, and the use of logic to combine queries.",Exploiting fuzzy tree fragment queries in the investigation of parsed corpora,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
44,http://dx.doi.org/10.1093/llc/15.4.403,"{'string_id': '10.1093/llc/15.4.403', 'id_scheme': 'DOI'}","In this paper we argue that corpus linguistics needs to expand to cover a wider set of languages. While the reasons that some languages have not been provided with corpus data to the date are clear, the intellectual and moral imperative to extend the range of corpus linguistics is strong. However, there are technical problems to be faced in such an extension of corpus linguistics. These problems are reviewed here and possible solutions to them explored. Following on from this, we consider what possible benefits the provision of appropriate corpus data may bring to languages currently untouched by the development of corpus linguistics.",A new agenda for corpus linguistics - working with all of the world's languages,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
45,http://dx.doi.org/10.1093/llc/15.4.421,"{'string_id': '10.1093/llc/15.4.421', 'id_scheme': 'DOI'}","This paper describes the construction of a corpus of spoken Sylheti. The corpus was created to examine difficulties in the creation of spoken language corpora in which features such as code switching (simply described here as the process of switching from one language to another during the course of an interaction; however, this description disguises a host of situations, which will be examined in the paper) are common. The paper also presents a transliteration scheme for Sylheti based around the Roman alphabet.",The construction of a corpus of spoken Sylheti,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
46,http://dx.doi.org/10.1093/llc/15.4.433,"{'string_id': '10.1093/llc/15.4.433', 'id_scheme': 'DOI'}","Bangla is the second most widely spoken language in the Indian subcontinent, yet has not been the focus of much research activity in either corpus linguistics or language engineering to date. This paper describes the automatic processing of pronouns in three and a half million words of Bangla corpus data. A corpus-based analysis of Bangla pronouns is developed, and a new approach to the analysis of Bangla pronouns is taken as a consequence. On the basis of this analysis a system is then developed to identify and analyse Bangla pronouns in corpus data.",Bangla pronouns - a corpus based study,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
47,http://dx.doi.org/10.1093/llc/15.4.445,"{'string_id': '10.1093/llc/15.4.445', 'id_scheme': 'DOI'}","The paper describes a method of collecting phonetic and linguistic data while maximizing the efficient use of resources. A speech database of Welsh was recorded and hand-annotated at the level of acoustic phonetic units and intonational units. This annotation file formed the input to a semi-automatic hierarchical annotation at higher linguistic levels. The method used rules to build the structure, followed if necessary by manual editing. The annotated database was then queried for linguistically significant sets of units. The query output was analysed statistically to derive basic descriptive data and answers to phonological questions. A small amount of data yielded answers to a long-standing stress-related question: stress in polysyllabic words does not affect the duration of (non-schwa) monophthongs, which are affected far more by word-final position. In addition, it was found that an intervocalic consonant after a stressed penult vowel was longer than a consonant in other positions. This confirmed earlier measurements and established three durational categories for non-cluster consonants. Finally, it was found that place of articulation and voicing factor both have an effect on voice onset time. It is hoped that this method will be of use to minority language researchers, in facilitating the swift gathering of basic phonetic and linguistic data, and speech technology resources.",A practical method for deriving phonetic and linguistic data for minority languages,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
48,http://dx.doi.org/10.1093/llc/15.4.465,"{'string_id': '10.1093/llc/15.4.465', 'id_scheme': 'DOI'}","ORCHID (Open Linguistic Resources Chanelled toward InterDisciplinary research) is a project aimed at building linguistic resources to support research in, but not limited to, natural language processing. Based on the concept of an open architecture design, the resources must be fully compatible with those which already exist, and software tools must also be made available. This paper describes the construction of a Thai part-of-speech (POS) tagged corpus, a preliminary stage in the construction of a Thai speech corpus. The paper also details the development of a POS tagger to be used in the construction of the POS-tagged corpus. Additionally, we describe a proposal for a new tagset, based on the results of a prior multilingual machine translation project. The corpus is annotated on three levels: paragraph, sentence, and word. Text information is maintained in the form of the text information lines and the number lines, which are both utilized in data retrieval. Finally, we describe a POS neuro tagger, which consists of a three-layer perceptron with elastic input. Computer experiments show that the neuro tagger has an accuracy of 94.4 per cent for tagging ambiguous words when tested on a small Thai training corpus containing 22,311 ambiguous words. A series of comparative experiments further show that the neuro tagger is superior to the statistical models including the frequency model (a baseline model), a local n-gram model, and HMM (Hidden Markov Model).",ORCHID: building linguistic resources in Thai,Oxford University Press (OUP),2000,,Literary and Linguistic Computing,15,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
49,http://dx.doi.org/10.1093/llc/16.1.29,"{'string_id': '10.1093/llc/16.1.29', 'id_scheme': 'DOI'}","In this paper we present a specialized word generator, which has been designed as an assistant tool for Basque troubadours. Such a tool allows verse-writers to generate all the words that match with a given word termination. We deal with some interesting aspects, i.e. the dimension of the generated list and the need to establish an order of relevance among the listed items. This work can be seen as a way of reusing computational hguistic tools in the context of the Basque cultural means of expression. The technical foundations of this tool lie in a two-level morphological processor. The way in which words must be generated (starting from the end of the word) leads us to invert the generation process.",An assistant tool for verse-making in Basque based on two-level morphology,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
50,http://dx.doi.org/10.1093/llc/16.1.45,"{'string_id': '10.1093/llc/16.1.45', 'id_scheme': 'DOI'}","In the majority of cases, the pronoun it illustrates nominal anaphora, tending to refer back to another noun phrase in the text. However, in a significant minority of cases, the pronoun is used in exceptional ways that fail to demonstrate strict nominal anaphora. The identification of these uses of it is important in all fields where pronoun resolution has an impact After a survey of previous treatments of the pronoun it in the literature, some features of instances of it are proposed that can be used in a novel memory-based learning method to automatically classify those instances. On evaluating the method, it is found that the implemented system performs comparably well with respect to a rule-based system, and with an extended training set it is expected that the accuracy of the system will improve, offering greater coverage than rule-based methods.",Applying machine learning toward an automatic classification of It,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
51,http://dx.doi.org/10.1093/llc/16.1.5,"{'string_id': '10.1093/llc/16.1.5', 'id_scheme': 'DOI'}","From the perspective of a compiler of electronic corpora, one of the major challenges in the attempt to improve their quality is the need to carefully reconsider how language-external variables used to structure them could be defined and conceptualized more precisely to justify references to them as factors conditioning language variation and change. How these variables relate to one another should also be specified. In examining criteria for assessing representativeness of corpora, the concept of range is discussed to stress the evident differences between texts categorized as representatives of a specific genre. Good practices of philological computing are highlighted by illustrating what kind of information can be lost if scholarly rigour is not applied in the process of editing and/or digitizing texts.",Structured text corpora in the study of language variation and change,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
52,http://dx.doi.org/10.1093/llc/16.1.59,"{'string_id': '10.1093/llc/16.1.59', 'id_scheme': 'DOI'}","One of the aims of linguistics is to infer from the ever-growing mass of actualdata available the implicit, virtual organization underlying the apparent disorder and diversity of surface phenomena. The procedure invariably consists in establishing and revealing the latent links between the fundamental entities justifying the dichotomy between language and speech, namely langue and parole (Saussure), competence and performance (Chomsky), system and process, substance and form (Hjelmslev), or sense and signification (GuiJlaume). This ever-present crucial duality is also at work in computational linguistics, where the chief question is how to reach, beyond the surface of observed facts, for the latent abstract organization, thus enabling the observer (i.e. the linguist) to gain access to knowledge that can be generalized. Tree-representation is a powerful means of evincing the inherent structure of mutually dependent data to account for the respective dependence or independence of the represented objects by means of a hierarchic tree where clearly outlined categories are paired and embedded. Frequently, modern linguists tend to be interested more in the relative closeness of objects than in their belonging to this or that closed class. Additive, as opposed to hierarchic, trees do away with watertight partitions between objects and lay the stress on notions such as proximity and opposition. The question of course arises of the possibility of representing any two distinct sets of original data in one tree-figure. The only procedure available is to attempt to achieve a consensus by fuson of the original two trees by means of a new algorithm. The algorithm is explained in detail and applied to various subsets of the tagged version of the LOB Corpus.",On consensus between tree-representations of linguistic data,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
53,http://dx.doi.org/10.1093/llc/16.1.77,"{'string_id': '10.1093/llc/16.1.77', 'id_scheme': 'DOI'}","When a noise verb is used to indicate verbal communication, factors from both the source domain of the verb (perception) and the target domain (communication) play a role in determining the argument structure of the sentence. While the target domain supplies a syntactic structure, the source domain's semantics constrain the degree to which that syntactic structure can be exploited. This can be determined by comparing noise verbs in this use with manner-of-communication verbs, which are superficially similar, but native to communication. Data for these two classes of verbs were drawn from the British National Corpus. The data were annotated with frame-semantic markup, as described in the Berkeley FrameNet Project. We compared the presence, type of syntactic realization, and position of the semantically annotated arguments for both classes of verbs. We found that noise and manner verbs show statistically significant differences in these three areas. For instance, noise verbs are more focused on the form of the message than manner verbs: noise verbs appear more frequently with a quoted message. In addition, there are differences other than the complementation patterns: certain noise verbs are biased with respect to speakers' genders, message types, and even orthography in quoted messages.",Shouting and screaming: manner and noise verbs in communication,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
54,http://dx.doi.org/10.1093/llc/16.1.99,"{'string_id': '10.1093/llc/16.1.99', 'id_scheme': 'DOI'}","The paper describes the creation of a digital network of the extant Middle High German dictionaries. As these works of reference, i.e. the Mittelhochdeutsches Worterbuch by Benecke-Muller-Zarncke, the Mittelhochdeutsches Handwdrterbuch and its supplement by Lexer, and the Findebuch, are very closely interconnected and can only be used simultaneously, they were ideal candidates for the composition of an electronic dictionary compound. In creating that network, the data have been encoded according to the Guidelines of the Text Encoding Initiative. Such markup when applied to historical dictionaries raised some problems, which are discussed in the second part of this paper. The main section deals with the graphical user interface of the digital dictionary and shows various searches in the dictionary database. Examples of philologjc relevance are not only given as far as full text retrieval is concerned but also for queries with the aid of preclassified criteria such as, for example, parts of speech, specific text types, or word forms in foreign languages. The institutional and organizational preconditions of the digitization are finally outiined. The simultaneous use of the dictionaries closely tied together already in their printed form will be made much easier by the digital network. Moreover, this network might well be a starting point for a web that interlinks all major dictionaries of the historical periods and of the dialects of the German language..",New directions in middle high German lexicography: dictionaries interlinked electronically,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
55,http://dx.doi.org/10.1093/llc/16.2.121,"{'string_id': '10.1093/llc/16.2.121', 'id_scheme': 'DOI'}","Jacob and Wilhelm Grimm's Deutsches Wörterbuch (DWB) comprises the most extensive documentation of the German language: for more than one hundred years—the longest period of publication for a German dictionary—generations of lexicographers have contributed about 350,000 entries to the DWB, which is divided into sixteen large volumes (thirty-two sections), containing a total of 67,744 columns. The DWB also reflects more than one hundred years of institutional, lexicographical, historical, and political history. This paper describes the project ‘Deutsches Wörterbuch on CD-ROM and on the Internet’, established at the University of Trier in 1998, stressing in particular the significance of the Standard Generalized Markup Language (SGML) and the guidelines developed by the Text Encoding Initiative (TEI) for the creation of a digitized version of the DWB. In addition, a prototype for a CD-ROM version will be introduced, emphasizing the importance of the electronic DWB, via its transformation into the electronic medium, for opening up new possibilities in using the rich dictionary material.",Books into bytes: Jacob and Wilhelm Grimm's Deutsches Worterbuch on CD-ROM and on the Internet,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
56,http://dx.doi.org/10.1093/llc/16.2.135,"{'string_id': '10.1093/llc/16.2.135', 'id_scheme': 'DOI'}","This paper presents an overview of the goals, architecture, and usability of Kirrkirr, a Java-based visualization tool for XML dictionaries, currently being used with a dictionary for Warlpiri, an Australian Aboriginal language. It discusses the underlying lexicon structure, and shows how a computer interface can effectively select from and display that content in various ways. The views of the dictionary include a graph view, which shows a network of semantically related words, and a formatted text view that can be customized via XSLT stylesheets. The paper argues that indigenous language dictionaries have normally been written for linguists, whereas the educational needs of other users have not been adequately met. It discusses the strengths of a computer dictionary interface in providing more help to native speaker users than a conventional dictionary, but argues that the possibilities for the visualization of dictionary information on computers have so far been insufficiently exploited. The paper concludes by briefly discussing observational and task-based testing of the dictionary with native speakers and learners.",Kirrkirr: software for browsing and visual exploration of a structured Warlpiri,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
57,http://dx.doi.org/10.1093/llc/16.2.153,"{'string_id': '10.1093/llc/16.2.153', 'id_scheme': 'DOI'}","The Scottish National Dictionary (SND) is the standard historical dictionary of modern Scots, covering the period from 1700 to the present. This paper describes the current project to digitize the SND to produce the eSND, which will eventually be output on the Internet. It includes a brief description of the SND itself, outlining its history, content, and structure, and describes how the eSND will differ from the printed text. The various stages of the eSND project are discussed, using examples from the work in progress: (1) the data capture, which is being achieved through scanning and optical character recognition (OCR) of the printed text; (2) the conversion of the OCR data to full Extensible Markup Language mark-up, including details of the actual mark-up scheme (which is based on the Text Encoding Initiative guidelines), and how this has been adapted to suit the SND text; (3) the integration of the original Supplement and new material; (4) the development of search tools and a Web interface. Details are also given of the new proposal to combine the eSND with an electronic version of the Dictionary of the Older Scottish Tongue (eDOST), sharing the same mark-up scheme, search software, and interface, to produce a comprehensive electronic resource covering Scots from the early medieval period to the present day.",The Electronic Scottish National Dictionary (eSND): work in progress,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
58,http://dx.doi.org/10.1093/llc/16.2.161,"{'string_id': '10.1093/llc/16.2.161', 'id_scheme': 'DOI'}","This paper describes the philosophy behind what represents one of the most ambitious projects of its kind ever to have been undertaken in the Spanish-speaking world: the Miguel de Cervantes Digital Library (http://cervantesvirtual.com/). It explains the reasons behind its creation, the private–public sector alliance that has made it possible, and the new ground being explored by its creators in terms of the new services it offers to its audience worldwide and of innovative application of digital methods. The final section of the paper deals with the technical underpinnings of this project at present and in the future, reporting continuing research and development activities being carried out at the Miguel de Cervantes Digital Library in the field of text markup and derived applications, such as automatic transformation of documents to different formats and complex searches performed upon the small textual objects defined by the markup scheme. A brief survey of works done on Named Entity Recognition that can be applied to automatic markup is also included. Finally, there are some comments on the research lines we intend to follow concerning information retrieval and filtering from structurally marked-up texts.This is a fascinating period in the history of libraries and publishing. For the first time, it is possible to build large-scale services where collections of information are stored in digital formats and retrieved over the networks (Arms, 2000).",The Miguel de Cervantes Digital Library: the Hispanic voice on the web,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
59,http://dx.doi.org/10.1093/llc/16.2.179,"{'string_id': '10.1093/llc/16.2.179', 'id_scheme': 'DOI'}","What will be the critical editions in the electronic era? Hubert de Phalèse, a research centre in La Sorbonne-Nouvelle University (Paris III), in accordance with its pragmatic approach to literary computing problems, decided to launch this debate by putting on line a critical edition of the complete works of Lautréamont/Isidore Ducasse (http://www.cavi.univ-paris3.fr/phalese). This edition is an integral hypertext (in which nearly every word of the text is linked with a comment), which gathers all that one usually finds in critical editions, but on a scale that does not have equivalents on paper: variants, philological, literary and encyclopedic comments, biography, bibliography, iconography, index, etc. This prototype poses, concretely, a certain number of problems, on several levels:Technical: Which interface is to be used? The purely automatic search engines (including the uses of Java and other script languages) appeared unsuited and a new device of computer-assisted indexing was developed. It makes it possible to provide for the user a lemmatized index and, especially, lexical cards, which can be enriched at will. The current solution of setting on-line presents some inconveniences but it has the advantage of proposing to the greatest number of users the consultation of the edition and of inviting them to take part in it.Contents: The new support is, virtually, infinite. What is a critical edition to contain now that we are no longer concerned with its volume? All the versions of the text, for example, can now be proposed with the reading. But are we to publish the intertexts, contemporary works, criticism, etc. ? How can the interconnection, in a network, of several resources enrich a critical edition? Under which scientific and legal conditions?Validation: Can this type of edition be regarded as more reliable than the paper editions? According to which protocols will such editions be judged? One of the risks is the appearance of a great quantity of work without scientific guarantee. How will the possibilities of collective work and permanent updating modify our design of what a philological work should be?Publication: Who will deal with the building and the diffusion expenses of such electronic products? Will the redistribution of the budget headings in this type of edition lead academics to transform themselves into diffusers, or will traditional editors change their practice? In addition, new prospects open with the critical edition, which it will be necessary to evaluate and explore to determine their real potentialities.Work in group: data processing and. the Internet support the participation of a growing number of speakers around an intellectual work. What will be the roles of each of these (project director, data-processing specialists, humanists, students, active readers, etc.)? The concepts even of authors and readers will no longer have the same direction.Real time: the possibility of permanent update offered by an Internet site makes it possible to revalue the traditional concepts. Indeed, it is no longer essential to put on-line a totally completed work, and the noted errors can be immediately corrected. In addition, this type of edition makes it possible to account for the topicality of research in the field, which connects it with a review (of which the periodicity is much higher than that of any scientific review).Interactivity: the possibility of putting creators and users of electronic publishing in contact by means of electronic mail also connects this type of edition to a permanent conference. It is possible, in the long, term, that scientific communities (specialists in an author, for example) will gather around great electronic projects, which they would make live by publishing the results of their work there.Cost: the very low cost of setting on-line such an edition (I except the working time of researchers) makes it possible at the same time to consider some undertakings that traditional editors would not consider (very large corpus, work of interest only for few specialists) and to allow researchers to publish in good working conditions works of small size or that do not fit the framework of current university editions.Multimedia: what can be the contribution of multimedia to a scientific work like a critical edition? Everything in this field remains to be invented, because the traditional edition accustomed us to purely textual tools, primarily for reasons of cost. The sound and visual illustrations will bring to the literary text a very interesting dimension (publication of manuscripts, interpretations, inconographics documents, contemporary pieces of music, etc.), from the teaching point of view as well as in the research field, but it is necessary to be wary of the easy effects that have accustomed us, the general public, to electronic publishing. It is very urgent to answer these questions because the share of electronic documentation in literary studies would have, as in the other documentary fields, to increase until gradually replacing the traditional supports. Consequently, the survival of the texts and their formal characteristics will be closely related to the devices that will ensure their transmission, their conservation, and their reading.",The FALMER Project: an electronic critical edition,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
60,http://dx.doi.org/10.1093/llc/16.2.189,"{'string_id': '10.1093/llc/16.2.189', 'id_scheme': 'DOI'}","Any project in the electronic medium that lays claim to scholarly authority will require adaptations in both the presentation and the dissemination of its materials. Work in progress on an electronic edition of Lyrical Ballads illustrates how these changes may be realized, and brings home the lesson that the evolution of electronic publishing will necessitate innovations that are not just technological but also institutional. A new method of marshalling scholarly apparatus called ‘dynamic collation’ has been employed in this project, which reconceptualizes how variant readings can be presented in the digital medium. The real challenge for online publication, however, is what might be called the lack of mature institutional structures on the Internet. So long as anyone can publish anything on WWW, the quality of its materials remains questionable; furthermore, in the absence of independent web-based publishing houses, personal postings are all too often ephemeral. The imprimatur of an established publisher (in this case, the Cambridge University Press) would be a guarantee of the reliability and durability of these immaterial texts, but entails the development of new publication partnerships. It seems the future of electronic publishing depends upon asking questions about more than technical standards. Online scholarly archives must also meet standards of peer evaluation, editorial practice, and institutional backing that have traditionally ensured the quality of print publications.",New models for electronic publishing,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
61,http://dx.doi.org/10.1093/llc/16.2.199,"{'string_id': '10.1093/llc/16.2.199', 'id_scheme': 'DOI'}","In a range of courses taught in the English department at The College of Wooster, student use technology to interrogate ideas in ways novel in humanistic study. In an otherwise conventional seminar room, networked computers (to the Internet as well as locally) along the walls and both commercial and locally developed software tools represent the use of technology as ‘equipment for language’. As our description of how student use the tools (from page-layout programs to a new counting program) show, technology in this setting adds power to student' ability to question and therefore to understand—in the context of a kind of discussion as old as learning.",A toolbox for the electronic classroom,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
62,http://dx.doi.org/10.1093/llc/16.3.205,"{'string_id': '10.1093/llc/16.3.205', 'id_scheme': 'DOI'}","American authorities, persuaded by Donald Foster's stylometric evidence, believe that Funeral Elegy by W. S. (FE) is at least possibly, and perhaps indisputably by Shakespeare. British authorities disagree sharply. Brian Vickers, Richard Kennedy, and Gilles Monsarrat argue that John Ford wrote the Elegy. We examine both ascriptions by applying to Ford's poems the same kind of common-authorship, exclusionary-evidence tests that we previously applied to Shakespeare's poems and play verse. We conclude that the odds are strongly against the Americans. If W. S. is either Ford or Shakespeare, Ford seems by far the more likely candidate. Counting firm rejections only, the Elegy fails sixteen of thirty-three Shakespeare tests and only one of twenty-nine Ford tests. If the distinguishing traits of both authors are Poisson-distributed—as some seem to be—the odds that the Elegy's scores could have arisen by chance from one corpus or the other are about 3,000 times better for Ford than they are for Shakespeare.",Smoking guns and silver bullets: could John Ford have written the Funeral Elegy?,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
63,http://dx.doi.org/10.1093/llc/16.3.233,"{'string_id': '10.1093/llc/16.3.233', 'id_scheme': 'DOI'}","The opening portions of the Pauline epistles display characteristic patterns that are scale related. They are the result of interaction between cycles comprising groups of longer and shorter sentences and the contrast between an opening high-scale section and a following low-scale section of the text. This interaction produces patterns in the several epistles which are not identical but whose features are scale related. Similar patterns may be produced mathematically, and the correspondence of the sentence patterns found in the texts to those of the mathematical model indicates that there are links between epistles that have not hitherto been identified. Studies using traditional or statistical methods have not yet taken into account the scale variable, which is ubiquitous in literature. No longer can texts be assumed to be homogeneous in this respect. The identification of these scale-related patterns is of importance in matters of author determination.",A computer model for the Pauline epistles,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
64,http://dx.doi.org/10.1093/llc/16.3.251,"{'string_id': '10.1093/llc/16.3.251', 'id_scheme': 'DOI'}","Recent research on gender differences in language has mostly addressed cognitive differences. These differences have been observed on different cognitive verbal and non–verbal tasks and conclusions on the variability in language production and comprehension have been drawn from their results. In this paper, a different approach is presented. This pilot study examines lexical richness measures in conversational speech across a total of thirty subjects. All subjects were recorded and transcribed in a conversational setting. Their transcribed speech was analysed using a set of lexical richness measures based on word–frequencies. On the basis of these measurements, statistical discriminant analysis is able to classify the two groups with 90 per cent (74 per cent with leave–one–out cross–validation) correct prediction rate at a statistically significant level (P = 0. 03). The results are discussed in detail, including correlation and principal components analysis. The paper concludes that there are interesting differences across the two groups on the measures studied and further research in this area is needed.",A pilot study on gender differences in conversational speech on lexical in richness measures,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
65,http://dx.doi.org/10.1093/llc/16.3.265,"{'string_id': '10.1093/llc/16.3.265', 'id_scheme': 'DOI'}","Syntactic analysis often plays an important role in natural language processing application systems such as document processing and question–answering. To use parsed results in the application systems, the parser should be efficient without losing accuracy. In this paper, we present a method for Korean dependency analysis using three types of chunking and lexical co–occurrences extracted from a large corpus. The chunking, which is crucial for reducing disambiguation decisions in the parsing process, is conducted using a finite state transducer and lexical collocation. In addition, lexical information has a great impact on parsing for a free-ordered language such as Korean, as the lexical association is more important than the word order in analysing such languages. The parser that we propose is a hybrid system directed by statistical data and syntactic rules, and based on right–to–left analysis to effectively treat sentences in Korean, which is a head final language. Experiments show that the method is very effective in that it gives accuracy as well as efficiency by reducing irrelevant parsing decisions.",Efficient dependency analysis for Korean sentences based on lexical association and multi-layered chunking,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
66,http://dx.doi.org/10.1093/llc/16.3.287,"{'string_id': '10.1093/llc/16.3.287', 'id_scheme': 'DOI'}","The World of Dante [http://www.iath.virginia.edu/dante/] offers a hypermedia environment for the study of the Inferno. Developed under the auspices of the Institute for Advanced Technologies in the Humanities (IATH), the major sections of the project include a visual archive of approximately three hundred images; the TEI-tagged Italian text of the Inferno, presented through Dynaweb; and a text visualization, a VRML model of the Inferno. This paper describes the visual material found on the project, explains the tagging criteria employed in the markup of the poem, and clarifies the type of research that the project makes possible. The project went online in December 1997. Readers have often been struck by the clarity, variety, and complexity of Dante's comparisons. T. S. Eliot considered Dante unequalled in his ability to form ‘clear visual images’ (Eliot, Dante, Faber & Faber, London, 1929, p. 22). In his essay, ‘A Reading of the Inferno’, Eliot singles out for praise the remarkable quality of Dante's ‘visual imagination’, citing as an example the comparison of a group of men who gaze at Dante and Virgil in the dark to an old tailor peering at the eye of his needle (Inf. 15. 21). Eliot's remarks concerning the remarkable nature of Dante's similes arise in the midst of a discussion of the poetics of the Inferno. Hell is the most material of the three realms of the Afterlife. Dante mentions about one hundred places in the Inferno, sixty in the Purgatorio, and twenty in the Paradiso. Dante's Hell is made up of precipitous slopes, barren forests, thunderous waterfalls, bloody rivers, and burning sands. To convey the savage nature of these places, Dante often evokes the landscape of his own Italy. Yet the multitude of natural, architectural, and geographical allusions that constitute Dante's material world are unfamiliar to most readers. One of the principal objectives of The World of Dante is to make Dante's universe more accessible to students of the poem.",The World of Dante: a hypermedia archive for the study of the Inferno,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
67,http://dx.doi.org/10.1093/llc/16.4.353,"{'string_id': '10.1093/llc/16.4.353', 'id_scheme': 'DOI'}","The study first establishes a set of formal properties of Ulysses through a computational approach based on frequency counts of the ninety-nine most common words of the text. The common words are first used to discriminate interior monologue, dialogue, and narrative, and then to discriminate between the different narrative styles of the text. The discriminations are achieved by means of multivariate statistics, such as principal component analysis, and by distribution tests (Student's t-test and Mann-Whitney test). Using the linguistic premise that all matter is meaning, as well as Bakhtin's argument that all language is ideologically saturated, the study then explores the relationship between common words, meaning, and ideology. It concentrates on the Gerty MacDowell section of episode 13 of Ulysses in order to show how common words that appear more frequently in that episode than in others—such as two modals, two causal conjunctions, and one preposition—are integral to the various syntactic structures that differentiate styles and contribute to the meaning and ideology of the text. The article links these discriminations to Bakhtin's concept of polyphony and to his discussion of the ‘creation of specific novelistic images of languages’. Its conclusion, therefore, is that computational analysis of style can open interpretation to details of form, meaning, and ideology that enable humanities computing to make a distinctive contribution to literary criticism.","The Statistical Analysis of Style: Reflections on Form, Meaning, and Ideology in the 'Nausicaa' Episode of Ulysses",Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
68,http://dx.doi.org/10.1093/llc/16.4.375,"{'string_id': '10.1093/llc/16.4.375', 'id_scheme': 'DOI'}","The sentence length distribution curve is modified by three non-rational components: scale, contrast, and monumentality. In this paper these components are isolated using graphical techniques. Combining batches of different types of sentence that are conceived at different scale levels also modifies the distribution. The effect of these variables is to give the sentence length distribution curve of each work of literature a unique profile. In longer composite texts, these variables average out and the curves converge on an ultimate distribution curve.",Graphical Analysis of the Sentence Length Distribution Curve and Non-rational Components,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
69,http://dx.doi.org/10.1093/llc/16.4.389,"{'string_id': '10.1093/llc/16.4.389', 'id_scheme': 'DOI'}","English has two main sources for words: German and Latin. Distinct from each other, they have polarized our language into high diction and low (‘diglossia’). Latinate words denote the intellectual world; Germanic words, the physical. Latinate words are indicators of status and education. Austen painted and delineated her characters by giving their speeches different densities of Latinate words. Higher densities of Latinate words sometimes indicate intelligence and moral seriousness, at other times, they expose a character's formality or hypocrisy. Lower densities indicate lesser intelligence or, in the case of sailors, humble birth. The characters whose densities are very close to the narrator are Austen's four great heroines, Elinor Dashwood, Elizabeth Bennet, Emma Woodhouse, and Anne Elliot.",The Density of Latinate Words in the Speeches of Jane Austen's Characters,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
70,http://dx.doi.org/10.1093/llc/16.4.403,"{'string_id': '10.1093/llc/16.4.403', 'id_scheme': 'DOI'}","Fifty years after the Confederate assault on the third day of the battle of Gettysburg, the widow of the General after whom the assault is named, George Pickett, published letters purportedly written to her by her husband, many of them from the field of battle itself, during the four-year-long American Civil War. These letters fit into the ‘Lost Cause’ literature of the late nineteenth and early twentieth centuries. There are, however, anachronisms and other factual problems in the published letters, and they have been questioned, at least in part, by writers and historians of the Civil War. Other historians believe them to be essentially genuine. This paper conducts a stylometric investigation of the Pickett Letters as a complement to traditional historical research. Our investigation strongly suggests that Pickett's widow, LaSalle Corbell Pickett, did compose the published letters. Eleven handwritten letters are, however, thought to be from George's hand.",A Widow and her Soldier: Stylometry and the American Civil War,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
71,http://dx.doi.org/10.1093/llc/16.4.421,"{'string_id': '10.1093/llc/16.4.421', 'id_scheme': 'DOI'}","This paper investigates the effectiveness and accuracy of multivariate analysis, specifically cluster analysis, of the frequencies of very frequent words in distinguishing texts by different authors and grouping texts by a single author. An examination of groups of texts by known authors shows that cluster analyses typically achieve an accuracy rate of less than 90 per cent for contemporary novels, modern British and American novels, and contemporary literary critical texts, both on relatively large groups of texts and on smaller subsets of those texts. Although limiting the analysis to third-person narration, and disambiguating homographic function words improves the results, inaccuracies remain. Furthermore, small groups of problematic texts extracted from the larger groups in simulated authorship studies also fail to cluster correctly. These failures suggest general rather than local problems with the technique, and cast doubt on the effectiveness of cluster analysis for authorship attribution and stylistic study.",Statistical Stylistics and Authorship Attribution: an Empirical Investigation,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
72,http://dx.doi.org/10.1093/llc/16.4.445,"{'string_id': '10.1093/llc/16.4.445', 'id_scheme': 'DOI'}","In this paper, a system is presented that performs an automated morphological categorization of Greek words extracted from a corpus. This system processes morphologically the words via the repetitive application of a masking-and-matching technique. It is found that the introduction of a priori information regarding the grammar of the Greek language considerably improves the word segmentation accuracy. The system accuracy is evaluated by comparing the word segmentation with the entries of a morphological lexicon of the Greek language. The experimental results indicate that the output of the automated system is—for the majority of words—in agreement with the entries of the morphological lexicon. The proposed system is successfully applied to the generation of specialized morphological lexica on the basis of corpora consisting of term-intensive documents. Finally, possible extensions of the proposed system to other languages as well as to cover the derivation phenomenon for the Greek language are briefly reviewed.",Automatic Corpora-based Stemming in Greek,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
73,http://dx.doi.org/10.1093/llc/16.4.467,"{'string_id': '10.1093/llc/16.4.467', 'id_scheme': 'DOI'}","Many stemmatological methods require estimates of pairwise distances between manuscripts, where distance is some measure of the number of changes that have occurred during copying along the path linking the two manuscripts. If a pair of manuscripts are separated from their common ancestor by more than one copy, more than one change may have occurred at some locations in the text, and the observed distance between two manuscripts may underestimate the actual number of changes. We derive a simple estimate of the actual number of changes given the observed number of changes, using a mathematical model for copying errors. This estimate is little affected by the size of the lexicon, the average rate at which copying errors are made, and the number of words for which a given word might be mistaken. Variation in error rates among scribes has no effect, and variation in error rates among words is probably unimportant. We recommend the routine use of this formula. However, variation in error rates among locations in the text can strongly affect the relationships between observed and actual distances. Such variation might easily arise in poetry because of the constraints of rhyme. Two priorities for future work are testing the underlying model for copying errors, and determining patterns of variation in error rates among locations.",Estimating Distances between Manuscripts Based on Copying Errors,Oxford University Press (OUP),2001,,Literary and Linguistic Computing,16,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
74,http://dx.doi.org/10.1093/llc/17.1.1,"{'string_id': '10.1093/llc/17.1.1', 'id_scheme': 'DOI'}","This paper presents a stemmer for processing document and query words to facilitate searching databases of Amharic text. An iterative stemmer has been developed that involves the removal of both prefixes and suffixes and that also takes account of letter inconsistency and reiterative verb forms. Application of the stemmer to a test file of 1221 words suggested that appropriate stems were generated for ca. 95 per cent of them, with only limited overstemming and understemming.",Stemming of Amharic Words for Information Retrieval,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
75,http://dx.doi.org/10.1093/llc/17.1.103,"{'string_id': '10.1093/llc/17.1.103', 'id_scheme': 'DOI'}","The application of computing to the disciplines of the humanities has two principal outcomes: useful results for the field of application and failures completely to demonstrate what is known. These failures, an inevitable feature of modelling, point to the key question for humanities computing, how we know what we know, and so to the beginning of its own scholarly enquiry. This, I argue, proceeds along three branches, the algorithmic, the metatextual, and the representational. Examining the first of these here I argue for research toward an open‐ended, interoperable set of primitives based on previous work in the field and designed for the emerging digital library environment. To set the stage for their further development I argue that the field as a whole does not wait on a theoretical formulation of what humanists do, rather should look to the tradition of experimental knowledge‐making as this has been illuminated in recent years by historians, philosophers, and sociologists of science.","Humanities Computing: Essential Problems, Experimental Practice",Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
76,http://dx.doi.org/10.1093/llc/17.1.19,"{'string_id': '10.1093/llc/17.1.19', 'id_scheme': 'DOI'}","This paper presents a semi‐automatic ontology construction method using various resources, and an ontology‐based word sense disambiguation method in machine translation. To acquire a reasonably practical ontology in limited time and with less manpower, we extend the Kadokawa thesaurus by inserting additional semantic relations into its hierarchy, which are classified as case relations and other semantic relations. The former can be obtained by converting valency information and case frames from previously built computational dictionaries used in machine translation. The latter can be acquired from concept co‐occurrence information, which is extracted automatically from large corpora. The ontology stores rich semantic constraints among 1110 concepts, and enables a natural language processing system to resolve semantic ambiguities by making inferences with the concept network of the ontology. In practical machine translation systems, our word sense disambiguation method achieved a 6.0 per cent and 7.9 per cent improvement over methods that do not use an ontology for each Japanese and Korean translation.",Language Independent and Practical Ontology in Korean-Japanese Machine Translation Systems,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
77,http://dx.doi.org/10.1093/llc/17.1.49,"{'string_id': '10.1093/llc/17.1.49', 'id_scheme': 'DOI'}","Over the last decade, HTML has moved from being the new file format of a new network service—the Web—to a well‐known but often abused barrier to communication. The advantages of changing to more formalized systems in XML are clear, but deployment has been hindered by the slow development of browsers, editors, and other software suitable for academic work. XML has greater potential than HTML for use off the Web, especially in the humanities, so the concept of ‘life beyond the Web’ for textual scholarship can be expressed in terms of opportunities for researchers. This paper is an expansion of a talk given to a Symposium of the Computer Science and English Initiative (COSEI) held in February 2000 at University College Dublin.",Is There Life Beyond the Web?,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
78,http://dx.doi.org/10.1093/llc/17.1.89,"{'string_id': '10.1093/llc/17.1.89', 'id_scheme': 'DOI'}","In any academic field, research advances tend to percolate naturally to higher education in that field. In recent years, there has been a slow but steady increase in the number of courses and degree programmes in humanities computing. This paper presents some reflections on the status of humanities computing in higher education, in terms of curricula, degrees, and international student and staff mobility. The most important issue is the question of what a humanities computing degree should offer, in view of the wide interdisciplinarity of the field. Different institutions have coped with this question in very different ways. With potentially far‐reaching consequences on methodology in the various relevant disciplines, humanities computing is bound to change both what and how humanities students learn. Curriculum innovation that aims to integrate computing in the humanities is a difficult process that requires reflection, co‐operation, teacher training, and other supporting actions.",Some Reflections on Studies in Humanities Computing,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
79,http://dx.doi.org/10.1093/llc/17.2.133,"{'string_id': '10.1093/llc/17.2.133', 'id_scheme': 'DOI'}","In this essay I explore a variety of experimental poetic tendencies produced in Spain during the twentienth century. Some of the most popular experimental poetic practices are visual poetry, sound poetry, phonetic poetry, computer poetry, video poetry, and mail art. Because these tendencies were not born in Spain we have to take into consideration international and historical avantgarde practices such as concretism, phonetic poetry, and others. I also study the different interests between multiple artistic languages employed in historical avant‐garde poetry and in more recent experimental poetry. Another aspect studied is the political background especially in the sixties and seventies in Spain and how it conditioned the development and durability of different groups and artistic expressions. Another important factor taken into consideration is the use and development of the different technologies available. Technology is not a medium in this kind of poetry but part of the message and for that purpose it can be used in different ways in the construction of a poem. Because of the complexity of the subject, and the difficulty of getting access to rare books and materials I give a panoramic view – with special attention to selected poems and authors – of the most visible experimental poetic manifestations produced in Spain. But there is a need to do more specialized studies in order to gain a better comprehension of these alternative poetic practices that are becoming more and more popular today.",Experimental Poetic Practices in Spain,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
80,http://dx.doi.org/10.1093/llc/17.2.157,"{'string_id': '10.1093/llc/17.2.157', 'id_scheme': 'DOI'}","This paper investigates the relative effectiveness and accuracy of multivariate analysis, specifically cluster analysis, of the frequencies of very frequent words and the frequencies of very frequent word sequences in distinguishing texts by different authors and grouping texts by a single author. Cluster analyses based on frequent words are fairly accurate for groups of texts by known authors, whether the texts are long sections of modern British and US novels or shorter sections of contemporary literary critical texts, but they are only rarely completely accurate. When frequent word sequences are used instead of frequent words or in addition to them, however, the accuracy of the analyses often improves, sometimes dramatically, especially when personal pronouns are eliminated. Analyses based on frequent sequences even provide completely correct results in some cases where analyses based on frequent words fail. They also produce superior results for small groups of problematic novels and critical texts extracted from the larger corpora. Such successes suggest that analyses based on frequent word sequences constitute improved tools for authorship and stylistic studies.",Frequent Word Sequences and Statistical Stylistics,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
81,http://dx.doi.org/10.1093/llc/17.2.181,"{'string_id': '10.1093/llc/17.2.181', 'id_scheme': 'DOI'}","Determination of authorship, using methods such as those of Mosteller and Wallace, has been obliquely criticized by literary scholars for years. However, the most radical critique of these methods has, under the umbrella term ‘theory’, emerged since the 1960s in the writings of Barthes, Foucault, and Derrida. Thanks to their influence, authorship attribution is now distinguished from authorship ascription with only the latter applying to literary and linguistic computing. Various criticisms are examined in detail. Useful as these criticisms are, the philosophical roots of the postmodernist critiques are destructive of any attempt to discover who wrote what.",Linguistic Computing in the Shadow of Postmodernism,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
82,http://dx.doi.org/10.1093/llc/17.2.193,"{'string_id': '10.1093/llc/17.2.193', 'id_scheme': 'DOI'}","Many manuscripts of the Saddharmapundarika, which are among the most important manuscripts for the study of Buddhism, have been discovered in very different localities and are classified according to their place of discovery into the following three groups: Nepalese, Kashmirian, and Central Asian manuscripts. For the genealogical classification of these manuscripts, principal component analysis and cluster analysis, which describe the similarities between the verses of the different manuscripts, were applied to the data. As a result, we could successfully classify these manuscripts into two large groups and several smaller groups: one large group consists of ten paper manuscripts from Nepal and the other comprises nine palm‐leaf plus two paper manuscripts. The Kashmir and Central Asian manuscripts and a few of the Nepal manuscripts belong to the small groups.",Genealogical Classification of Saddharmapundarika Manuscripts Based on Many-Variable Analysis,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
83,http://dx.doi.org/10.1093/llc/17.2.207,"{'string_id': '10.1093/llc/17.2.207', 'id_scheme': 'DOI'}","Word alignment in bilingual or multilingual parallel corpora has been a challenging issue for natural language engineering. An efficient algorithm for automatically aligning word translation equivalents across different languages will be of use for a number of practical applications such as multilingual lexical construction, machine translation, etc. This paper presents a hybrid algorithm for English–Chinese word alignment, which incorporates co‐occurrence association measures, word distribution distances, English word lemmatization, and part‐of‐speech information. Eleven co‐occurrence association coefficients and eight distance measures of word distribution are explored to compare their efficiency for word alignment. The paper also describes an experiment in which the algorithm is evaluated on sentence‐aligned English–Chinese parallel corpora. In the experiment, the algorithm produced encouraging success rates on two test corpora, with the highest success rate of 89.37 per cent. It provides a practical tool for extracting word translation equivalents from English–Chinese parallel corpora.",Word Alignment in English-Chinese Parallel Corpora,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
84,http://dx.doi.org/10.1093/llc/17.2.231,"{'string_id': '10.1093/llc/17.2.231', 'id_scheme': 'DOI'}","Automatic keyword extraction is an extremely interesting prospect for computational humanists because of its potential as a tool to aid scholarship in the humanities. Keyword discovery routines can help organize large collections of texts and perhaps even guide scholars to the discovery of important elements in their source materials. It is not clear, however, that the methods designed to extract keywords from paper abstracts or newswire texts will be effective for literary texts that are not written in English. This paper describes the modifications required to the traditional tf★idf keyword discovery algorithm so that it will extract valid keywords from literary texts written in Ancient Greek.",Keyword Extraction from Ancient Greek Literary Texts,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
85,http://dx.doi.org/10.1093/llc/17.2.245,"{'string_id': '10.1093/llc/17.2.245', 'id_scheme': 'DOI'}","This paper describes the general principles, design, and present state of the Czech National Corpus (CNC) project. The corpus has been designed to provide a firm basis for the study of both the contemporary written Czech (a goal well attainable with the present resources) and the Czech language beyond the limits of contemporary written texts (a long‐term commitment including the building of a corpus of spoken Czech and diachronic and dialectal corpora). The work on the CNC project, now in the eighth year of its official existence, has resulted in the completion of SYN2000, a 100‐million‐word corpus of contemporary written Czech, the organization of the cores of spoken, diachronic, and dialectal corpora, and the finding of workable solutions to some general theoretical problems involved in the building of these corpora.","The Czech National Corpus: Principles, Design, and Results",Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
86,http://dx.doi.org/10.1093/llc/17.3.267,"{'string_id': '10.1093/llc/17.3.267', 'id_scheme': 'DOI'}","This paper is a companion to my ‘Questions of authorship: attribution and beyond’, in which I sketched a new way of using the relative frequencies of the very common words for comparing written texts and testing their likely authorship. The main emphasis of that paper was not on the new procedure but on the broader consequences of our increasing sophistication in making such comparisons and the increasing (although never absolute) reliability of our inferences about authorship. My present objects, accordingly, are to give a more complete account of the procedure itself; to report the outcome of an extensive set of trials; and to consider the strengths and limitations of the new procedure. The procedure offers a simple but comparatively accurate addition to our current methods of distinguishing the most likely author of texts exceeding about 1,500 words in length. It is of even greater value as a method of reducing the field of likely candidates for texts of as little as 100 words in length. Not unexpectedly, it works least well with texts of a genre uncharacteristic of their author and, in one case, with texts far separated in time across a long literary career. Its possible use for other classificatory tasks has not yet been investigated.",'Delta': a Measure of Stylistic Difference and a Guide to Likely Authorship,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
87,http://dx.doi.org/10.1093/llc/17.3.289,"{'string_id': '10.1093/llc/17.3.289', 'id_scheme': 'DOI'}","This paper presents an overview of the Pascal Database System. The Pascal Database includes all the text from the Œuvres complèetes de Blaise Pascal in four volumes. The online database was released experimentally in October 2000. It is possible to display material, perform a vocabulary search, and make frequency lists of material in the database via the Internet. The content display can access each volume, plus manuscript data, edition, references, annotations of J. Mesnard, and other documents, which is a great advantage when studying the material. The vocabulary search can perform Boolean searches with ‘And’, ‘Or’, and ‘Not’, and can also use the wild card ‘★’. Frequency lists can be made using alphabetical or frequency order, and it is even possible to create a list based on the alphabetical order of the reversed words. Finally, we comment on the personal pronouns in Pascal's letters and discuss the uses of the word figure in the second volume of Pascal's work.",The Pascal Digital Archive,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
88,http://dx.doi.org/10.1093/llc/17.3.311,"{'string_id': '10.1093/llc/17.3.311', 'id_scheme': 'DOI'}","Until printing was invented, texts were copied by hand. The probability with which changes were introduced during copying was affected by the kind of text and society. We cannot usually estimate the probability of change directly. Instead, we develop an indirect method. We derive a relationship between the number of manuscripts in the tradition and the mean number of copies separating a randomly chosen pair of manuscripts. Given the rate at which the proportion of words that are different increases with the mean number of copies separating two manuscripts, we can then estimate the probability of change. We illustrate our method with an analysis of Lydgate's medieval poem The Kings of England.",How Accurate Were Scribes? A Mathematical Model,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
89,http://dx.doi.org/10.1093/llc/17.3.323,"{'string_id': '10.1093/llc/17.3.323', 'id_scheme': 'DOI'}","This paper introduces an intelligent tutoring system designed to help student translators learn to appreciate the distinction between literal translation and liberal translation, an important and forever debated point in the literature of translation, and some other methods of translation lying between these two extremes. We identify four prominent kinds of translation methods commonly discussed in the translation literature—word‐for‐word translation, literal translation, semantic translation, and communicative translation—and attempt to extract computationally expedient definitions for them from two researchers' discussions on them. We then apply these computational definitions to the preparation of our translation corpus to be used in the intelligent tutoring system. In the basic working mode the system offers a source sentence for the student to translate, compares it with the inbuilt versions, and decides on the most likely method of translation used through a translation unit matching algorithm. The student can guess where on the literal and liberal continuum their translation stands by viewing this verdict and by comparing their translation with other versions for the same sentence. In the advanced working mode, the student learns some translation techniques such as the contrastive analysis approach to teaching translation, while appreciating the working of translation methods in relation to these techniques.",Computer-Assisted Teaching of Translation Methods,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
90,http://dx.doi.org/10.1093/llc/17.3.345,"{'string_id': '10.1093/llc/17.3.345', 'id_scheme': 'DOI'}","This paper proposes a solution to the problem of handling scribal abbreviations in TEI‐conformant transcriptions of medieval texts, following a conservative editorial strategy. A key distinction is drawn between alphabetic abbreviations, which represent sequences of letters, and logographic abbreviations which represent whole words. The TEI elements 〈expan〉 and 〈abbrev〉 can be used systematically to separate these two types: alphabetic abbreviations will be expanded in the main text, recording the abbreviated form (including TEI entities representing the main abbreviation marks) as an attribute of 〈expan〉, while logographic abbreviations will be represented in their abbreviated form, with the expanded form recorded as an attribute of 〈abbrev〉. The proposals are illustrated from common abbreviations and short text samples from tenth‐century Latin–Portuguese and thirteenth‐century Old Portuguese.",Encoding Medieval Abbreviations for Computer Analysis (from Latin-Portuguese and Portuguese Non-literary Sources),Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
91,http://dx.doi.org/10.1093/llc/17.4.373,"{'string_id': '10.1093/llc/17.4.373', 'id_scheme': 'DOI'}","In this paper, we put forward a fully developed system for the teaching of Modern Greek Language (MGL). The system comprises a parser and generator for Modern Greek sentences as well as a computational lexicon, encoding morphological, syntactic, and semantic information for words. In this paper, we present the major components of the system, highlighting their suitability for the teaching of MGL in an experimental, open, and cooperative educational environment. The proposed system can be used either in a classroom environment or by Internet correspondence for the teaching of MGL as a native or foreign language.",A Complete and Comprehensive System for Modern Greek Language Processing Proposed as a Modern Greek Language Call Method Developer,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
92,http://dx.doi.org/10.1093/llc/17.4.401,"{'string_id': '10.1093/llc/17.4.401', 'id_scheme': 'DOI'}","The problem of automatically determining the gender of a document's author would appear to be a more subtle problem than those of categorization by topic or authorship attribution. Nevertheless, it is shown that automated text categorization techniques can exploit combinations of simple lexical and syntactic features to infer the gender of the author of an unseen formal written document with approximately 80 per cent accuracy. The same techniques can be used to determine if a document is fiction or non‐fiction with approximately 98 per cent accuracy.",Automatically Categorizing Written Texts by Author Gender,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
93,http://dx.doi.org/10.1093/llc/17.4.413,"{'string_id': '10.1093/llc/17.4.413', 'id_scheme': 'DOI'}","A conceptual glossary is a textual reference work that combines the features of a thesaurus and an index verborum. In it, the word occurrences within a given text are classified, disambiguated, and indexed according to their membership of a set of conceptual (i.e. semantic) fields. Since 1994, we have been working towards building a set of conceptual glossaries for the Latin Vulgate Bible. So far, we have published a conceptual glossary to the Gospel according to John and are at present completing the analysis of the Gospel according to Mark and the minor epistles. This paper describes the background to our project and outlines the steps by which the glossaries are developed within a relational database framework.",Developing Conceptual Glossaries for the Latin Vulgate Bible,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
94,http://dx.doi.org/10.1093/llc/17.4.427,"{'string_id': '10.1093/llc/17.4.427', 'id_scheme': 'DOI'}","This paper outlines a project currently under way in the Linguistics Department at the University of Arizona to create electronic dictionaries of indigenous languages of the south‐west USA and make them available over the Web for language instruction as well as for linguistic, psycholinguistic, and anthropological research. Working with three languages—Tohono O'odham, Navajo, and Hiaki—we have created an XML scheme that serves as a general template for structuring and archiving language databases. We describe the process of compiling databases for different languages and converting these databases to XML, which contains all the relevant information in a manner that is easily accessible. We discuss the general programming scheme used for searching, and the interfaces used for presenting the dictionary on the Web, which include several front ends for different user groups. We end with a discussion of how to ensure that special characters are displayed properly on the Web.",Web-based Dictionaries for Languages of the South-west USA,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
95,http://dx.doi.org/10.1093/llc/17.4.439,"{'string_id': '10.1093/llc/17.4.439', 'id_scheme': 'DOI'}","Scale‐related patterns are found in all thirteen Pauline epistles. To test their distinctiveness, graphs of other texts, ancient and modern, comprising more than a million words, have been scrutinized; this survey has failed to detect any similar patterns. They may therefore be related to Pauline authorship. The longer passages claimed to be interpolations are tested against these scale‐related patterns and are found to be essential parts of the original texts. Further scale‐related patterns are found in 1 and 2 Peter (which received wisdom claims are pseudonymous writings) and in Hebrews. Consideration of these patterns and of the partnership of Paul and Silvanus in mission, leads to a possible solution to the problem of the hapaxes and throws light on the points of contact between the Paulines (including the Pastorals), 1 and 2 Peter, and Hebrews.","Interpolations, Pseudographs, and the New Testament Epistles",Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
96,http://dx.doi.org/10.1093/llc/17.4.457,"{'string_id': '10.1093/llc/17.4.457', 'id_scheme': 'DOI'}",We describe an attempt to analyse the temporal structure of discourse in Modern Greek following the principles of Asher's Segmented Discourse Representation Theory. We focus on discourse relations of a temporal and causal interest and the use of linguistic knowledge for the determination of these relations. This analysis is applied to a corpus of short newspaper articles reporting car accidents in Modern Greek and the discourse grammar is implemented using the Attribute Logic Engine.,An SDRT Approach to the Temporal Structure of Modern Greek Narrative Texts,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
97,http://dx.doi.org/10.1093/llc/17.4.475,"{'string_id': '10.1093/llc/17.4.475', 'id_scheme': 'DOI'}","This paper presents a method for designing and organizing a multi‐purpose morpheme‐based lexical database for Modern Greek. The authors are in favour of multi‐purpose lexical databases, to avoid a repetition of effort from one application to another, and of morpheme‐based lexica, to achieve flexibility, reusability, expandability, and compact representation of data for future developments. The suggested method for modelling the lexical database in the word‐processing function is the Entity/Relationship model, according to the linguistic theory of Generative Lexical Morphology. In the framework of this model, which depicts rich linguistic information, we can introduce new data structures for storing the morphemes. These new data structures are matrix encoding schemes; one type, called the Cartesian Lexicon, has been designed as a part of our research. The matrix data structures combine the advantages of hash‐tables and tries, which are very popular data structures in supporting machine readable dictionaries. Our system was tested on the Modern Greek language, and demonstrated a satisfactory overall performance in word‐processing. These methods could also be applicable to other languages having morphological systems similar to Modern Greek.",Modelling a Morpheme-based Lexicon for Modern Greek,Oxford University Press (OUP),2002,,Literary and Linguistic Computing,17,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
98,http://dx.doi.org/10.1093/llc/18.1.101,"{'string_id': '10.1093/llc/18.1.101', 'id_scheme': 'DOI'}","This article describes the background and architecture of The Versioning Machine, a software tool designed to display and compare multiple versions of texts. The display environment provides for features traditionally found in codex‐based critical editions, such as annotation and introductory material. It also takes advantage of opportunities afforded by electronic publishing, such as providing a frame to compare diplomatic versions of witnesses side by side, allowing for manipulatable images of the witness to be viewed alongside the diplomatic edition, and providing users with an enhanced typology of notes.",The Versioning Machine,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
99,http://dx.doi.org/10.1093/llc/18.1.11,"{'string_id': '10.1093/llc/18.1.11', 'id_scheme': 'DOI'}","Since February 2002, a first version of the Deutsche Wörterbuch (DWB) by Jacob and Wilhelm Grimm has been available on the web. A CD‐ROM beta version has been available since December 2002. This paper will focus on the steps involved in drawing up an electronic version of the DWB and, by demonstrating the design of the Graphical User Interface (GUI), will show how common standards of digitization were taken into account and user needs were anticipated during the production process. The history and structure of the DWB will be outlined first to point out some characteristics of the dictionary. The process of retrodigitization from printed page to electronic dictionary will be briefly described and, while giving an overview of the DWB GUI, the importance of content‐based markup and a user‐friendly but powerful GUI as a necessary precondition for sensible and effective access to the dictionary contents will be stressed. The title of this paper, Towards the User, can thus be interpreted in two ways: during the digitization of the DWB, we consider the needs of the users, and by digitization, we hope to open up this huge amount of data and lexicological information for researchers.",Towards the User: The Digital Edition of the Deutsche Worterbuch by Jacob and Wilhelm Grimm,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
100,http://dx.doi.org/10.1093/llc/18.1.23,"{'string_id': '10.1093/llc/18.1.23', 'id_scheme': 'DOI'}","In recent years, the use of large corpora has revolutionized the way we study language. There are now numerous well‐established corpus projects, which have set the standard for future corpus‐based research. As more and more corpora are developed and technology continues to offer greater and greater scope, the emphasis has shifted from corpus size to establishing norms of good practice. There is also an increasingly critical appreciation of the crucial role played by corpus design. Corpus design can, however, present peculiar problems for particular types of source material. The Scottish Corpus of Texts and Speech (SCOTS) is the first large‐scale corpus project specifically dedicated to the languages of Scotland, and therefore it faces many unanswered questions, which will have a direct impact on the corpus design. The first phase of the project will focus on the language varieties Scots and Scottish English, varieties that are themselves notoriously difficult to define. This paper outlines the complexities of the Scottish linguistic situation, before going on to examine the problematic issue of how to construct a well‐balanced and representative corpus in what is largely uncharted territory. It argues that a well‐formed corpus cannot be constructed in a linguistic vacuum, and that familiarity with the overall language population is essential before effective corpus sampling techniques, methodologies, and categorization schema can be devised. It also offers some preliminary methodologies that will be adopted by SCOTS.",The Scottish Corpus of Texts and Speech: Problems of Corpus Design,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
101,http://dx.doi.org/10.1093/llc/18.1.39,"{'string_id': '10.1093/llc/18.1.39', 'id_scheme': 'DOI'}","Markup licenses inferences about a text. But the information warranting such inferences may not be entirely explicit in the syntax of the markup language used to encode the text. This paper describes a Prolog environment for exploring alternative approaches to representing facts and rules of inference about structured documents. It builds on earlier work proposing an account of how markup licenses inferences, and of what is needed in a specification of the meaning of a markup language. Our system permits an analyst to specify facts and rules of inference about domain entities and properties as well as facts about the markup syntax, and to construct and test alternative approaches to translation between representation layers. The system provides a level of abstraction at which the performative or interpretive meaning of the markup can be explicitly represented in machine‐readable and executable form.",A Logic Programming Environment for Document Semantics and Inference,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
102,http://dx.doi.org/10.1093/llc/18.1.49,"{'string_id': '10.1093/llc/18.1.49', 'id_scheme': 'DOI'}","The paper is a report on a case in forensic linguistics in which linguistic and computational approaches are combined to answer the question whether it can be proved if a digital recording has been tampered with. With the growing use of digital applications, the chances of digital forgery are increasing significantly. Accordingly, the detection of tampering with audio recordings is also becoming an important task for forensic linguists. In the given case, we assumed that the most straightforward way of tampering with the given digital audio recording might have been the removal of some material and so our aim was to identify the location of this kind of tampering in the file. Due to the complexity of the given task the approach presented is interdisciplinary: first, it uses a traditional semantic analysis to identify possible discontinuous segments of the recorded text; secondly, it introduces an experimental phonetic approach to identify cues of the digital cutting of the audio signal; thirdly, it applies statistical calculations to specify the bit‐level characteristics of audio recordings. The combination of these measurements proved to be quite helpful in answering the initial question, and the proposed new methodologies can be used in further areas of linguistics and computation.",Forensic Linguistics: its Contribution to Humanities Computing,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
103,http://dx.doi.org/10.1093/llc/18.1.63,"{'string_id': '10.1093/llc/18.1.63', 'id_scheme': 'DOI'}",This paper looks at the usability of XML for the electronic publication of field reports by commercial archaeological units. The field reports fall into the field of grey literature as they are produced as client reports by commercial units as part of the planning process and do not receive official publication or widespread dissemination. The paper uses a small commercial unit called ARCUS at the University of Sheffield as a case study and to mark up a sample of excavation report using XML and the TEI Lite DTD. It also looks at the possibility of incorporating controlled archaeological vocabulary into the DTD. The paper comes to the conclusion that the electronic publication of grey reports would be very useful as it would allow a quicker response time and a rapid dissemination of information within the fast‐moving and changing environment of commercial archaeology. XML would be a useful tool for the publication of field reports as it would allow practitioners to selectively download separate sections of field reports that are of particular importance to them and to improve the searchability of reports on the web. It is recognized that national archaeological institutions will also have to accept electronic versions of field reports in order for them to be able to be built into the financial framework of a commercial project design.,The Publication of Archaeological Excavation Reports Using XML,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
104,http://dx.doi.org/10.1093/llc/18.1.77,"{'string_id': '10.1093/llc/18.1.77', 'id_scheme': 'DOI'}","This paper explains why and how the digitization project METAe applies METS (Metadata Encoding and Transmission Standard) as encoding scheme for automatically extracted metadata. In contrast to TEI (Text Encoding Initiative) and other markup languages, METS allows encoding of the whole range of structural, descriptive, and administrative metadata in a systematic way. As the METS schema permits the integration of other existing standards, it provides a highly flexible output that can be converted easily to the individual needs of digital libraries. An innovative aspect of the METAe data structure is the ALTO file (‘Analysed layout and text object’), which contains the layout structures as well as the text passages of book pages. Structural maps of the METS schema are used to compose the logical and the physical structures out of ALTO and image files.",METAe--Automated Encoding of Digitized Texts,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
105,http://dx.doi.org/10.1093/llc/18.1.89,"{'string_id': '10.1093/llc/18.1.89', 'id_scheme': 'DOI'}","Schema languages concentrate on grammatical constraints on document structures, i.e. hierarchical relations between elements in a tree‐like structure. In this paper, we complement this concept with a methodology for defining and applying structural constraints from the perspective of a single element. These constraints can be used in addition to the existing constraints of a document grammar. There is no need to change the document grammar. Using a hierarchy of descriptions of such constraints allows for a classification of elements. These are important features for tasks such as visualizing, modelling, querying, and checking consistency in textual data. A document containing descriptions of such constraints we call a ‘context specification document’ (CSD). We describe the basic ideas of a CSD, its formal properties, the path language we are currently using, and related approaches. Then we show how to create and use a CSD. We give two example applications for a CSD. Modelling co‐referential relations between textual units with a CSD can help to maintain consistency in textual data and to explore the linguistic properties of co‐reference. In the area of textual, non‐hierarchical annotation, several annotations can be held in one document and interrelated by the CSD. In the future we want to explore the relation and interaction between the underlying path language of the CSD and document grammars.",Testing Structural Properties in Textual Data: Beyond Document Grammars,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
106,http://dx.doi.org/10.1093/llc/18.2.117,"{'string_id': '10.1093/llc/18.2.117', 'id_scheme': 'DOI'}","New ways of documenting and describing language via electronic media coupled with new ways of distributing the results via the World‐Wide Web offer a degree of access to language resources that is unparalleled in history. At the same time, the proliferation of approaches to using these new technologies is causing serious problems relating to resource discovery and resource creation. This paper describes the infrastructure that the Open Language Archives Community (OLAC) has built to address these problems. Its technical and usage infrastructures address problems of resource discovery by constructing a single virtual library of distributed resources. Its governance infrastructure addresses problems of resource creation by providing a mechanism through which the language‐resource community can express its consensus on recommended best practices.",The Open Language Archives Community: An Infrastructure for Distributed Archiving of Language Resources,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
107,http://dx.doi.org/10.1093/llc/18.2.129,"{'string_id': '10.1093/llc/18.2.129', 'id_scheme': 'DOI'}","This study is a test case in the use of stylometric techniques to provide an entrance into questions of literary criticism and interpretation. The study applies multivariate analysis to two texts of Charles Brockden Brown, sometimes considered the first professional writer in the United States. Both a scatter graph of a principal components analysis and a cluster analysis show that individual chapters from each of two novels (Wieland and Carwin) group together, except for three chapters of Wieland that cluster with the Carwin chapters. One chapter of Wieland that clusters with the Carwin chapters is narrated by the same character who narrates all of Carwin, thus providing statistical evidence that Brown has created a narrator with a distinctive voice. Accounting for the clustering of the other two chapters calls for a consideration of several of the more crucial and problematic interpretative issues in the novel, and suggests that quantitative analysis can indeed provide background and evidence for literary critical discussion and understanding.",Charles Brockden Brown: Quantitative Analysis and Literary Interpretation,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
108,http://dx.doi.org/10.1093/llc/18.2.139,"{'string_id': '10.1093/llc/18.2.139', 'id_scheme': 'DOI'}","Karl Lachmann's edition from 1833 still provides the basis for Parzival scholarship. Although the text has subsequently been revised in parts, a fundamentally new edition considering all extant manuscripts is required. Computer technology offers means for tackling this task in an effective and reliable manner. A critical electronic edition will give access to the manuscript material, which may be published stage by stage, corresponding to different sections of the text. Such an edition will allow users to consult a base text, electronically linked to an apparatus of variants, to manuscript transcriptions, and to facsimiles. Browsing among these components, readers will experience the extent to which the Parzival romance was open to textual variance in the course of its transmission (an aspect stressed by theories of the so‐called ‘New Philology’). Furthermore, new stemmatological methods borrowed from evolutionary biology (phylogeny) will provide insight into manuscript groupings that may reflect early textual versions that relate to the semi‐oral status of vernacular literary culture. Thus, an electronic edition will be the essential prerequisite of any new Parzival book edition. But it also constitutes an edition in its own right, revealing the discursive and visual richness of medieval text traditions and involving the readers in the editorial process.",New Philology and New Phylogeny: Aspects of a Critical Electronic Edition of Wolfram's Parzival,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
109,http://dx.doi.org/10.1093/llc/18.2.151,"{'string_id': '10.1093/llc/18.2.151', 'id_scheme': 'DOI'}","Emily Dickinson's experimental poetic compositions present exceptional challenges to the textual editor using TEI to mark up Dickinson's manuscript writings, particularly in the realm of tagging variants and versions. Focusing in particular on her use of intratextual and subtextual variant words, phrases, lines, and line groups, this paper examines several different scenarios for tagging variants and discusses each strategy's strengths and weaknesses. Bearing in mind textual theories that stress the autonomous nature of textual versions produced by variation, this essay also imagines future computing tools that rely upon TEI‐conformant tagging to automate visual representations of variant versions. Ultimately, no entirely satisfactory method of encoding Dickinson's variants emerges, as more simple encoding strategies fail to capture Dickinson's complexities in a way that can generate automated display, and more complicated strategies produce awkward, cumbersome code and retain TEI's known difficulties with tagging multiple and overlapping hierarchies.",Witnessing Dickinson's Witnesses,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
110,http://dx.doi.org/10.1093/llc/18.2.167,"{'string_id': '10.1093/llc/18.2.167', 'id_scheme': 'DOI'}","The inability of computing humanists to break into the mainstream of literary critical scholarship may be attributed to the prevalence of scientific methodologies and metaphors in humanities computing research—methodologies and metaphors that are wholly foreign not only the language of literary criticism, but to its entire purpose. Breaking out of this unfortunate misalignment entails reaching for more appropriate paradigms. The ‘algorithmic criticism’ here proposed rejects the empiricist vision of the computer as a means by which critical interpretations may be verified, and instead seeks to locate computational processes within the rich tradition of interpretive endeavours (usually aligned more with art than criticism), which seek not to constrain meaning, but to guarantee its multiplicity. Computational processes, which are perhaps more conformable to this latter purpose, may be usefully viewed as ways of providing the necessary conditions for interpretive insight. Algorithmic criticism seeks, therefore, in the narrowing forces of constraint embodied and instantiated in the strictures of algorithmic processing, an analogue to the liberating potentialities of art and the ludic values of humanistic inquiry. It proposes that we reconceive computer‐assisted text analysis as an activity best employed not in the service of a heightened critical objectivity, but as one that embraces the possibilities of that deepened subjectivity upon which critical insight depends.",Special Section: Reconceiving Text Analysis: Toward an Algorithmic Criticism,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
111,http://dx.doi.org/10.1093/llc/18.2.175,"{'string_id': '10.1093/llc/18.2.175', 'id_scheme': 'DOI'}","The strengths of current text‐analysis tools lie in their ability to perform a variety of formal, enumerative, or statistical functions. These functions concord well with scientific perspectives of textual criticism. Much less evident is how current text‐analysis tools help read and experience literature. Design of new tools, it is argued, should give full space to how literary critics interact with texts, rather than simply focus on what computers can do well. Principles of reading, synthesis, and play are explored in relation to a prototype version of HyperPo: Text Analysis and Exploration Tools.",Computer-Assisted Reading: Reconceiving Text Analysis,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
112,http://dx.doi.org/10.1093/llc/18.2.185,"{'string_id': '10.1093/llc/18.2.185', 'id_scheme': 'DOI'}","This paper is intended to begin a discussion about the need for a future text analysis software tool to model a number of aspects of text analysis as it is practised by critics who are not computing humanists, and that have been barely considered by most developers. It begins by putting this analysis in the context of Douglas Englebart's famous model of computer–human interaction expressed originally by his Augment system. It continues by examining a number of examples of how technology (ranging from 3 × 5 cards to topic maps) is currently used to support critical analyses, and points out that an important aspect of the critical process, which is recognized at least in part by the 3 × 5 card model but not by mainstream humanities computing methods, is the sense of critical analysis as a process rather than merely the presentation of a finished product. The paper finishes by examining some of the underlying concepts present in text‐analysis tools used within the social sciences that attempt to deal with this temporal aspect of analysis, and proposes a need to examine the real practices of critics in the light of these issues.",Finding a Middle Ground between 'Determinism' and 'Aesthetic Indeterminacy': a Model for Text Analysis Tools,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
113,http://dx.doi.org/10.1093/llc/18.2.209,"{'string_id': '10.1093/llc/18.2.209', 'id_scheme': 'DOI'}",The author revisits the question of what text analysis could be. He traces the tools from their origin in the concordance. He argues that text‐analysis tools produce new texts generated from queries through processes implemented on the computer. These new texts come from the decomposition of original texts and recomposition into hybrid new works for interpretation. The author ends by presenting a portal model for how text‐analysis tools can be made available to the community.,"What is Text Analysis, Really?",Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
114,http://dx.doi.org/10.1093/llc/18.3.235,"{'string_id': '10.1093/llc/18.3.235', 'id_scheme': 'DOI'}",Stylometric studies have revealed differences between the first four Pauline epistles and the Pastoral Epistles. They also show that there is some kind of affinity between the Pastorals and the Petrine Epistles. Scalometric analysis suggests that this affinity is due to stylistic features. The identification of the linguistic features that separate the first four epistles from the Pastorals and the Petrines provides a method of distinguishing between two styles in the epistles. Differences that were previously thought to point to differences in authorship are now seen to refer to differences in style that may be found within the works of one author.,Two Styles in the New Testament Epistles,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
115,http://dx.doi.org/10.1093/llc/18.3.249,"{'string_id': '10.1093/llc/18.3.249', 'id_scheme': 'DOI'}","The goal of this paper is to present a procedure for the automatic retrieval of idiomatic expressions from large text corpora. The procedure combines text segmentation techniques and Latent Semantic Analysis. Three indices were computed on the basis of the three‐fold hypothesis that: (1) idiomatic expressions should have few neighbours; (2) idiomatic expressions should demonstrate low semantic proximity between the words composing them; (3) idiomatic expressions should demonstrate low semantic proximity between the expression and the preceding and subsequent segments. The result of this procedure shows that we have not yet reached a fully automatic retrieval of idioms from large corpora, but this first trial has shown that we are on the way. The procedure reduces the amount of data to consider to less than a quarter (23.8 per cent) of the original data, of which one‐fifth (20.9 per cent) is idiomatic, and nearly 60 per cent (58.8 per cent) is phraseological in nature. In other words, this procedure drastically improves and facilitates hand‐based retrieval. In addition, these first results already permit some linguistic exploitation of the retrieved idioms.",Towards Automatic Retrieval of Idioms in French Newspaper Corpora,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
116,http://dx.doi.org/10.1093/llc/18.3.261,"{'string_id': '10.1093/llc/18.3.261', 'id_scheme': 'DOI'}","This paper examines the effectiveness of multivariate analysis of the frequencies of frequent collocations in characterizing authorial style. Cluster analyses of collocations over various spans, types, and linkages are performed on groups of texts by known authors to determine how well the frequencies of those collocations correctly attribute the texts to their authors and distinguish them from texts by other authors. In each case the results are compared with those based on the frequencies of frequent words and the frequencies of frequent sequences of words. Cluster analyses based on frequent words and sequences ascribe many of the texts to their correct authors. However, analyses based on frequent collocations are more accurate for several groups of texts, sometimes producing more completely correct attributions than analyses based on either words or sequences and sometimes producing the only completely correct attributions. They also produce results for small groups of problematic novels and critical texts extracted from the larger corpora that are often superior to those based on frequent words or frequent sequences. Finally, they perform better than analyses based on frequent words or sequences in simulated authorship attribution scenarios. Cluster analysis based on frequent collocations provides a robust and effective method of authorship attribution.",Frequent Collocations and Authorial Style,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
117,http://dx.doi.org/10.1093/llc/18.3.287,"{'string_id': '10.1093/llc/18.3.287', 'id_scheme': 'DOI'}","This paper aims to show that model validation is of great importance to ensure the predictive accuracy of a statistical model. By extending the use of logistic regression analysis, it further demonstrates the value of logistic modelling of non‐discrete linguistic categories in language performance. This statistical technique is illustrated on a corpus‐based study of the theory on the grammatical factors for the oscillation between the use and omission of the definite article preceding multi‐word organization names (e.g. the Foreign Office, Mansfield College) in the English language. By validating the preliminary model on fresh corpora, the final logistic model can capture more precisely the gradience in the grammatical factors that affect article usage preceding multi‐word organization names. As the logistic model is a model of language in use rather than a purely statistical model, this paper further translates the regression coefficients into the probability statements that a name is favouring the use of the definite article.",Validating the Logistic Model of Article Usage Preceding Multi-word Organization Names with the Aid of Computer Corpora,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
118,http://dx.doi.org/10.1093/llc/18.3.315,"{'string_id': '10.1093/llc/18.3.315', 'id_scheme': 'DOI'}","The usefulness of corpora in the study of syntagmatic relations has often been advocated. Without underestimating its importance, this paper aims at showing another advantage of monolingual corpora: the comparison of the behaviour of different word‐forms derived from the same lexeme. Two corpora have been chosen to illustrate this point, COBUILD's Bank of English and CREA (Reference Corpus of Contemporary Spanish), and two different but related case studies are thus presented by way of examples. Apparently, the differences between the variants selected in each language concern only gender and number, as is the case with the Spanish samples, or they are compounds with synonymous meanings, as happens with the English examples. But the observation of the data from the corpora has revealed some relevant characteristics concerning frequency rates and functions that contradict, in a considerable number of cases, our initial expectations—the assumption that all the forms of a lexeme differ from one another only formally (in the case of Spanish because of grammatical agreement) but not in their uses and meanings.",The Role of Corpora in the Study of Paradigmatic Relations; the Cases of COBUILD's Bank of English and CREA (Reference Corpus of Contemporary Spanish),Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
119,http://dx.doi.org/10.1093/llc/18.4.341,"{'string_id': '10.1093/llc/18.4.341', 'id_scheme': 'DOI'}","This paper investigates style variation in George Orwell's Nineteen Eighty-Four and William Golding's The Inheritors using multivariate analysis, specifically, cluster analysis of the frequencies of frequent words. Baseline tests on a corpus including these and four other novels show that traditional authorship attribution techniques correctly distinguish all sections of each novel from all sections of the other five and correctly cluster all sections of each novel. They are also very successful in distinguishing the section of Nineteen Eighty-Four that purports to be a political tract by Emmanuel Goldstein from the rest of the novel. They are less successful in distinguishing the style of the final chapter of The Inheritors, where critics have argued that a sudden shift of point of view leads to a radical variation in style. The nature of this stylistic variation suggests a modification in the way that frequent words are selected for analysis—a modification that gives improved results for both novels and sharply distinguishes the final chapter from the rest of The Inheritors. A further test of the modified technique on an unusual section of The Picture of Dorian Gray suggests that it may be more widely useful in studies of style variation.",Multivariate Analysis and the Study of Style Variation,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
120,http://dx.doi.org/10.1093/llc/18.4.361,"{'string_id': '10.1093/llc/18.4.361', 'id_scheme': 'DOI'}","This paper presents the newly released Lancaster Corpus of Mandarin Chinese (LCMC), a Chinese match for the FLOB and Frown corpora of British and American English. We first discuss the major decisions we took when building the corpus. These relate to sampling, text collection, mark-up, and annotation. Following from this we use the corpus to study aspect marking in Chinese and British/American English. The study shows that although Chinese and English are typologically different, aspect markers in the two languages show a strikingly similar distribution pattern, especially across the two broad categories of narrative and expository texts. The study also reveals some important differences in the distribution of aspect markers in Chinese versus English and British versus American English across fifteen text categories, and provides an account of these differences.",Aspect Marking in English and Chinese: Using the Lancaster Corpus of Mandarin Chinese for Contrastive Language Study,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
121,http://dx.doi.org/10.1093/llc/18.4.379,"{'string_id': '10.1093/llc/18.4.379', 'id_scheme': 'DOI'}","Using an adaptation of C. and D. Labbé's method of intertextual distances, fourteen short texts by three literary scholars are shown to be distinguishable by author. Of equal interest is the proximity of texts which treat similar themes, or employ similar sub-genres. The interaction of the factors which contribute to intertextual distances enriches the approach and goes some way towards bridging the gap between computational stylistics and traditional literary criticism.","Intertextual Distances, Three Authors",Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
122,http://dx.doi.org/10.1093/llc/18.4.389,"{'string_id': '10.1093/llc/18.4.389', 'id_scheme': 'DOI'}","Large-scale corpus-based research within translation studies is just taking its first steps. The paper begins by discussing some fundamental ideas and concepts underlying descriptive, corpus-based translation studies. These include translationese, translation-specific language, which refers to linguistic features that are either specific to translations or occur with a significantly higher or lower frequency in translations than in target-language originals, and universals of translation, i.e. features which are hypothesized to be common to all translated texts regardless of text type and language pair. The paper reports some research findings based on the children's literature subcorpus of the larger Corpus of Translated Finnish compiled at the Savonlinna School of Translation Studies, University of Joensuu. The aim of the research was to find potential features of translationese in Finnish translations of children's books. The features discussed include complex nonfinite constructions, clause connectives, and keywords. It was discovered that high frequencies of nonfinite constructions, lack of colloquial words, and specific uses of certain conjunctions are qualities which distinguish Finnish translations from non-translations and can hence be considered features of translationese in Finnish children's literature. The findings are also looked at from the perspective of translation universals, which are both supported and contradicted. Finally, the paper touches upon the difficulties caused to computer analysis by Finnish, a synthetic language, and the problems of creating a balanced corpus.",Genre-specific Features of Translationese? Linguistic Differences between Translated and Non-translated Finnish Children's Literature,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
123,http://dx.doi.org/10.1093/llc/18.4.407,"{'string_id': '10.1093/llc/18.4.407', 'id_scheme': 'DOI'}","Biological techniques can be used to reconstruct the stemmata of text traditions. Here, we describe methods for assessing the reliability of the results. We use compatibility matrices to detect sections of the text with different patterns of transmission. By constructing stemmata from subsets of increasing size, we estimate the minimum amount of data needed to produce a reliable stemma. We use consistency indices to assess the overall reliability of the stemma and the level of support that individual variants give to the stemma. Bootstrap analyses allow us to reject features of the stemma that result from only a few variants. We apply these techniques to the stemma for the Miller's Tale in Chaucer's Canterbury Tales.",How Reliable is a Stemma? An Analysis of Chaucer's Miller's Tale,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
124,http://dx.doi.org/10.1093/llc/18.4.423,"{'string_id': '10.1093/llc/18.4.423', 'id_scheme': 'DOI'}","Large, real world, data sets have been investigated in the context of Authorship Attribution of real world documents. Ngram measures can be used to accurately assign authorship for long documents such as novels. A number of 5 (authors) × 5 (movies) arrays of movie reviews were acquired from the Internet Movie Database. Both ngram and naive Bayes classifiers were used to classify along both the authorship and topic (movie) axes. Both approaches yielded similar results, and authorship was as accurately detected, or more accurately detected, than topic. Part of speech tagging and function-word lists were used to investigate the influence of structure on classification tasks on documents with meaning removed but grammatical structure intact.",Ngram and Bayesian Classification of Documents for Topic and Authorship,Oxford University Press (OUP),2003,,Literary and Linguistic Computing,18,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
125,http://dx.doi.org/10.1093/llc/19.1.105,"{'string_id': '10.1093/llc/19.1.105', 'id_scheme': 'DOI'}","How was a medieval manuscript meant to be read? This is a question that has concerned me for a long time in my work with Old Swedish manuscripts from Vadstena Abbey. In many manuscripts we can find traces of the historical reading situation; for example, pointing hands, marginal notes, etc. Such signals had an important function for the medieval reader, but they are rarely put forward in modern printed editions. I maintain that many of these paratextual notes can be explained with the help of hypertext theory, and be emphasized in a digital edition. I discuss this possibility by giving some examples from Scandinavian composite manuscripts. I show how digital technology together with new philological theory can give new life to medieval manuscripts, as digital editions together with the use of linking give the modern reader a deeper understanding of manuscript culture. This is possible because new philology revalues the concrete textual witnesses of a manuscript and takes each single version of a text into discussion. A printed edition is a much too clumsy tool if the aim is to give the modern reader a clear view of the uses of manuscript during the Middle Ages, but with digital technology an edition can be more complete by applying different layers of information.","Medieval Manuscripts, Hypertext and Reading. Visions of Digital Editions",Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
126,http://dx.doi.org/10.1093/llc/19.1.119,"{'string_id': '10.1093/llc/19.1.119', 'id_scheme': 'DOI'}","After Professor Marcel De Smedt of the University of Leuven introduced scholarly editing of modern texts as a discipline in Flanders in the 1980s, the worrying fact emerged during the last decade of the twentieth century that Flemish universities and scholarly research groups were falling well behind in the field of scholarly editing. As a reaction, the inter-university task force Genese was founded in 1993 with its main goal to promote and coordinate the theories and practice of scholarly editing in Flanders. The next decisive step was taken by the Royal Academy of Dutch Language and Literature (Koninklijke Academie voor Nederlandse Taal‐ en Letterkunde—KANTL) when they decided to make scholarly editing their primary objective as of January 1998. That decision paved the way for the founding of the Centre for Scholarly Editing and Document Studies (Centrum voor Teksteditie en Bronnenstudie—CTB), which started on 1 August 2000 as a research institute of the Academy, and which has become the centre of expertise in the field of (electronic) scholarly editing in the Low Countries.",Editorial Theory and Practice in Flanders and the Centre for Scholarly Editing and Document Studies,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
127,http://dx.doi.org/10.1093/llc/19.1.17,"{'string_id': '10.1093/llc/19.1.17', 'id_scheme': 'DOI'}","The nature of a scholarly edition, as of any bibliographical tool, is determined by the historical, medial, social, and rhetorical dimensions of the genre. This ‘situatedness’ puts constraints on the force of scholarly editions: what they can and what they cannot do. Claims have been made for the potent reproductive force of scholarly editions, as well as for the making of massive digital facsimile and transcription archives that can be used as platforms for producing new critical editions. This article questions the legitimacy of such assumptions when combined with idealist notions of documents, texts, and editions. That the nature of editions is rhetorical rather than neutral, social rather than individualistic, and one of complex translation rather than simple transmission, for instance, suggests that the versatility and reproductivity of the edited material itself will be limited by significant factors. Recognizing this makes us better equipped at subjecting digital archives and editions, and the claims some of their surrounding discourses make, to critical inquiry.",How Reproductive is a Scholarly Edition?,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
128,http://dx.doi.org/10.1093/llc/19.1.35,"{'string_id': '10.1093/llc/19.1.35', 'id_scheme': 'DOI'}",Digital editions make it possible to create a collection of all existing copies of a text including digital facsimiles. Is this a problem if it means that there will be editions that are in fact collections of full variant texts with no selected or edited reading text? This paper argues that both archival editions with digital facsimiles and encoded source texts (digital diplomatic editions) and digital critical texts can and must exist side by side. It is also suggested that from high quality diplomatically encoded source texts it is possible to automatically extract texts that either directly or with some further encoding/editing can function as a base text for editions of different types and which build on different editorial philosophies. The editions produced at the Wittgenstein Archives in Bergen and in the project Henrik Ibsen’s Writings in Oslo are used as examples of projects supporting the author’s arguments.,Monkey Business--or What is an Edition?,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
129,http://dx.doi.org/10.1093/llc/19.1.45,"{'string_id': '10.1093/llc/19.1.45', 'id_scheme': 'DOI'}","When theorizing about correspondence reconstruction and sorting, we first need both a definition of ‘a letter’ and of ‘correspondence’. In this article, we propose such definitions and investigate the impact of our understanding of what correspondence reconstruction is on the production of the DALF formal framework for the transcription of epistolary material. The focus of the paper is on the instruments built into DALF for allowing all types of sorting and classification, hence correspondence reconstruction.",Presentational and Representational Issues in Correspondence Reconstruction and Sorting,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
130,http://dx.doi.org/10.1093/llc/19.1.55,"{'string_id': '10.1093/llc/19.1.55', 'id_scheme': 'DOI'}","In Norway, the project Henrik Ibsen’s Writings is currently establishing a new historical–critical edition (both electronically and in print) of the complete writings of playwright Henrik Ibsen. In the years the project has existed, there has been a continuing internal discussion on the relationship between philology and text encoding. This paper outlines the philological principles of the project and describes its methods of establishing texts and ensuring quality. It also looks at and describes, in detail, the consequences of combining philology and text encoding through examples of problems solved in the encoding of complex changes in manuscripts as well as parallel structures in verse dramas. The paper concludes that it is very important, in a project such as Henrik Ibsen’s Writings, to focus on the relationship between philology and text encoding because of the influence, even in the smallest details, of these on each other.",Philology Meets Text Encoding in the New Scholarly Edition of Henrik Ibsen's Writings,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
131,http://dx.doi.org/10.1093/llc/19.1.73,"{'string_id': '10.1093/llc/19.1.73', 'id_scheme': 'DOI'}","This article discusses three principal types of transcription with reference to vernacular Medieval Nordic sources: a facsimile transcription with graphic details, a diplomatic transcription with few graphic details, but with all phonemic distinctions upheld, and a normalized transcription with regularized orthography. The article argues that all three levels of transcription can be accommodated in a single multi-level encoding, using the flexibility of Extensible Markup Language (XML) and the Text Encoding Initiative (TEI) guidelines. Special characters and abbreviation marks are encoded using entities and linking these to Unicode characters, partly in the Private Use Area. Words are encoded on one or more levels using specified elements and supplied with lexicographical and grammatical information by way of attributes. Display in various formats such as HTML and PDF is managed by stylesheets. Thus, a single XML file allows for several views of the text, suitable for different user groups, such as linguists, historians, and literary critics.",Parallel Views: Multi-level Encoding of Medieval Nordic Primary Sources,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
132,http://dx.doi.org/10.1093/llc/19.1.9,"{'string_id': '10.1093/llc/19.1.9', 'id_scheme': 'DOI'}","For the benefit of the readers and the authors of this special issue on electronic scholarly editing, this short article introduces the Text Encoding Initiative (TEI) and the TEI Consortium (TEI‐C), which are mentioned and referred to throughout this issue and it provides many suggestions for further reading.",An Introduction to the TEI and the TEI Consortium,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
133,http://dx.doi.org/10.1093/llc/19.1.93,"{'string_id': '10.1093/llc/19.1.93', 'id_scheme': 'DOI'}",This article has two aims. First I argue that the old habits of the printed book often have influence on our way of thinking in our work with electronic texts. I stress here the importance of evaluating the new possibilities that the new technological tools provide for philological work. In the second part I present a project that is currently working with the Old Swedish material from the Vadstena monastery in Sweden. Central to this project is to provide electronically encoded transcriptions of the primary sources from Vadstena.,Computing Medieval Primary Sources from the Vadstena Monastery: Arguments for the Primary Source Text,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
134,http://dx.doi.org/10.1093/llc/19.2.145,"{'string_id': '10.1093/llc/19.2.145', 'id_scheme': 'DOI'}","Empirical analysis of any natural language needs to be substantiated with the statistical findings because without adequate knowledge from statistics any linguistic study can fall into the quicksand of mistaken data handling and false observation. Recent introduction of various sub‐disciplines (computational linguistics, corpus linguistics, forensic linguistics, applied linguistics, lexicology, stylometrics, lexicography, and language teaching, etc.) requires various statistical results of language properties to understand the language as well as to design sophisticated tools and software for language technology. Keeping this in mind, we present here some simple frequency counts of characters found in the Bangla text corpus. Also, we empirically evaluate their functional behaviours in the language with close reference to the corpus. Here we verify previously made observations, as well as make some new observations required for various works of language technology in Bangla.",Frequency and Function of Characters Used in the Bangla Text Corpus,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
135,http://dx.doi.org/10.1093/llc/19.2.161,"{'string_id': '10.1093/llc/19.2.161', 'id_scheme': 'DOI'}","Within the last twenty years historians of science and technology have asked how a recent history might be written, and within the last ten interest has significantly increased, culminating in an online project at MIT. Since humanities computing owes its existence to developments in recent technology, and needs to become historically self‐aware to be fully of the humanities, work toward an historiography of recent things is deeply relevant. In this essay I draw on this work to highlight the difficulties and opportunities of such an historiography, in particular its ethnographic character and the tempting lure of prediction. I focus on the crucial question of tacit object‐knowledge, concluding that it is gained by concernful action. I recommend that we awaken from a progress‐and‐democratization chronicle to a genuine history of scholarly technology.",As It Almost Was: Historiography of Recent Things,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
136,http://dx.doi.org/10.1093/llc/19.2.181,"{'string_id': '10.1093/llc/19.2.181', 'id_scheme': 'DOI'}","Among the history plays, King John is historically prior to the two Shakespeare tetralogies. Partly for this reason it is the least well‐known of the early history plays. Despite an increasing recognition by scholars that collaboration was the norm before the mid‐1590s, the Shakespearean integrity of King John has been hitherto unquestioned. Stylistic markers, previously recognized for their ability to discriminate authors, however, consistently differentiate two bodies of text within the play, one of which is unmistakably Shakespeare’s. Marlowe’s authorship of the non‐Shakespeare text, though improvable, is a plausible inference.",King John Divided,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
137,http://dx.doi.org/10.1093/llc/19.2.197,"{'string_id': '10.1093/llc/19.2.197', 'id_scheme': 'DOI'}","This article describes a method for discriminating among registers of Modern Greek and among authors within a given register. Two issues have been investigated: (a) whether register discrimination can successfully exploit linguistic information reflecting the evolution of a language (such as diglossia features of the Modern Greek language) and (b) what kind of linguistic information and which statistical techniques may be applied to author discrimination within one register. Using clustering techniques and variables reflecting the diglossia situation, we have successfully discriminated registers in Modern Greek. However, diglossia information on its own has not been shown sufficient for author discrimination within one register. Instead, other linguistic features, including PoS distribution and discourse tendencies, have been combined with methods such as discriminant analysis in order to obtain a high degree of accuracy.",Discriminating the Registers and Styles in the Modern Greek Language-Part 1: Diglossia in Stylistic Analysis,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
138,http://dx.doi.org/10.1093/llc/19.2.221,"{'string_id': '10.1093/llc/19.2.221', 'id_scheme': 'DOI'}","This article describes a method for discriminating among authors within a given register of Modern Greek. The focus here is to determine to what extent the stylistic differences among authors can be detected with a high degree of accuracy for a set of texts belonging to a well‐defined register. To that end, the chosen register is characterized by a well‐defined sub‐language, from which a corpus of more than 1,000 documents has been created. To discriminate the texts according to author style, a series of experiments have been performed using statistical techniques. Each text has been represented by a vector covering several linguistic aspects, in an effort to determine the most effective style markers. The experimental results indicate that the proposed approach can successfully separate the author styles for a given register. An extensive study of the effectiveness of the different variable categories has been performed. For instance, diglossia information on its own is not sufficient for author discrimination. Instead, a systematic evaluation process indicates that part‐of‐speech, structural and algorithmically derived lemma‐frequency variables are the most important style markers, their use leading to an author discrimination accuracy exceeding 90%.",Discriminating the Registers and Styles in the Modern Greek Language-Part 2: Extending the Feature Vector to Optimize Author Discrimination,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
139,http://dx.doi.org/10.1093/llc/19.3.253,"{'string_id': '10.1093/llc/19.3.253', 'id_scheme': 'DOI'}","What could and should be the relationship between research archives of endangered cultural heritage materials and the originating community? This paper argues that recent developments in distributed computing in a networked environment have allowed us to re‐imagine this relationship in a way that profoundly changes the role of the archive and reinforces the desirability of establishing ongoing reciprocal relationships with cultural heritage communities. Some possibilities are suggested drawing from experience with PARADISEC (the Pacific and Regional Archive for Digital Sources in Endangered Cultures, established in 2003 as a collaborative venture between the University of Sydney, the University of Melbourne, and the Australian National University) and with local community‐based digital archives in the remote Australian communities of Belyuen and Wadeye. Repatriation and rights, planning principles for establishment and sustainability of local digital archives in community cultural centres, and models for a staged approach in setting up ongoing relationships with rights holders are discussed. The paper argues that digital archives, as distributed virtual institutions, need to engage with a number of different communities of interest: not only the individuals, communities, and institutions that own the cultural heritage objects we preserve, but also the wider academic community and international standards‐setting bodies. Planning for our archives’ digital future means imagining ourselves as actors and creators within that virtual society.",Turning It All Upside Down . . . Imagining a distributed digital audiovisual archive,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
140,http://dx.doi.org/10.1093/llc/19.3.289,"{'string_id': '10.1093/llc/19.3.289', 'id_scheme': 'DOI'}","In the context of ongoing research into new methods and techniques for literary research we describe a primary implementation of a web application called Autonom, intended to be developed into a framework for textual parsing algorithms that may be used by literary researchers to trace literary phenomena in texts. We describe the technical parsing fundamentals and good practices the development of the framework is based upon, we clarify different design considerations and choices and we present an overview of the current state of implementation and functionality. We also demonstrate the application of a proper name parsing algorithm implemented within the framework, meant to be the first step in a new method for the research of names in literary texts. The algorithm is tested on Karel Glastra van Loon’s novel Lisa’s adem (Lisa’s Breath, 2001). We go into the results that this test has yielded so far and summarily describe some of the consequences for the analysis of the names in the novel. We conclude with a short description of the directions new developments could take.",Modelling Features of Characters: Some Digital Ways to Look at Names in Literary Texts,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
141,http://dx.doi.org/10.1093/llc/19.3.303,"{'string_id': '10.1093/llc/19.3.303', 'id_scheme': 'DOI'}","The aim of this paper is to show that the appropriation of ICTs is determined by a field’s specific cultural identity. Knowledge is not a homogeneous whole, but a patchwork of heterogeneous fields. These fields are most visible as embodied in academic disciplines, which have distinct cultural identities shaped by intellectual and social considerations. Scholarly communication systems evolve over time within the context of these cultural identities. The paper discusses the cultural shaping of ICTs by drawing on an ongoing ethnographic study within corpus‐based linguistics. The findings suggest that cultural elements such as ‘task‐uncertainty’, ‘mutual‐dependency’, heterogeneity, and institutional configurations will influence the appropriateness of a specific ICT infrastructure for a particular intellectual community. For example, fields that have a highly politicized and tightly controlled research culture will develop a coherent field‐based strategy for the uptake and use of ICTs, whereas domains that are pluralistic and have a loosely organized research culture will appropriate ICTs in an ad‐hoc localized manner. These findings demonstrate that overlooking cultural diversity in the development and implementation of ICT infrastructures and policies could prove detrimental for fields that do not map onto ‘big science’ conceptualizations of knowledge production. Furthermore, the paper demonstrates that effective understanding similarity and difference in patterns of scholarly communication needs to take the fine‐grain of specialist fields as the unit of analysis, rather than the course‐grain of the discipline.",The Cultural Shaping of ICTs within Academic Fields: Corpus-based Linguistics as a Case Study,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
142,http://dx.doi.org/10.1093/llc/19.3.321,"{'string_id': '10.1093/llc/19.3.321', 'id_scheme': 'DOI'}","This paper discusses the utilization of content‐based image retrieval (CBIR) for searching and retrieving images, to enhance access to digital image collections. Organizations have taken advantage of new technologies and funding opportunities to digitize their image collections, resulting in a greater need for efficient storage and retrieval systems. Images are used by a wide and diverse group of people, including picture researchers, historians and design professionals. Research into image use indicates that some image users have very specific needs, others are more interested in material conveying abstract concepts, and some do not want specific images but want to browse for inspiration. Cataloguing and indexing practices vary considerably, despite the existence of several tools to aid the process. Many organizations use in‐house schemes or no formal methods at all. Manual indexing effectiveness is a problem area that affects both practitioners and users. Image seeking behaviour is a complex interaction between contextual factors that ultimately affect how a user searches, selects, and uses images. CBIR is a technique in which images are selected via features automatically extracted from the images. Research into CBIR in practice found that, whilst user views were mixed, there was sufficient evidence that visual searching for images could be a useful feature of a digital image library, particularly if used in combination with text‐based descriptors.",Enhancing Visual Resources for Searching and Retrieval--Is Content-based Image Retrieval a Solution?,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
143,http://dx.doi.org/10.1093/llc/19.3.335,"{'string_id': '10.1093/llc/19.3.335', 'id_scheme': 'DOI'}","The visualization and analysis of spatial data can shed new light on the nature and meaning of data throughout the Humanities. However such work is often avoided because it is seen as requiring expensive new hardware and software resources or involving a substantial expenditure of time and effort overcoming a steep learning curve before worthwhile results can be obtained. The Centre for Computing in the Humanities at King’s College London collaborates with researchers in a variety of humanities disciplines on a range of projects that often involve a component of spatial data. This has given the Centre an opportunity to explore methods of visualizing, analysing and displaying spatial data in a range of humanities disciplines. This paper discusses some of the intellectual, research and practical issues affecting the use of spatial data in Humanities Computing projects. It is illustrated by examples from a number of projects at King’s College. Although it is grounded on specific examples to support the points being made the main aim is to draw out a series of general themes which affect the use of spatial data by researchers throughout the Humanities and increase awareness of what can be achieved with minimal resources and a little creative thought.",The Visualization of Spatial Data in the Humanities,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
144,http://dx.doi.org/10.1093/llc/19.3.351,"{'string_id': '10.1093/llc/19.3.351', 'id_scheme': 'DOI'}","E‐CITIES is a concept for a system that would bring research knowledge of the cultural heritage, in particular the history of cities, into popular use. The idea was developed as a model of multilingual cultural content management and dissemination. The system could be implemented as a fully functional and reproducible system applicable to any city, anywhere. Much of the technology is being or has been developed for use in other applications. E‐CITIES would offer important political, economic, and cultural benefits. This paper explores the technologies that would be required and identifies those that are available and developing. It puts the case that an E‐CITIES system is not just desirable but, increasingly, necessary.",City Histories Revealed,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
145,http://dx.doi.org/10.1093/llc/19.3.373,"{'string_id': '10.1093/llc/19.3.373', 'id_scheme': 'DOI'}","As the physical building of a national library can serve as a tangible expression of political and cultural philosophy, a given digital archive manifests ideological features of the national legacy it preserves and disseminates electronically. Millennial discourses have influenced national library building projects in both physical and digital archives. However, a simple analogy between conventional and electronic spaces is inadequate, because national policies on digitizing documents and regulating access engender contradictory impulses in archivists and policy makers. Although considerable attention has recently focused on the ‘right to read’, the physical space of a document archive is constituted by prohibitions on reading. In a 2002 survey, the degree of regulation varied greatly depending on national context. The Bibliothèque Nationale de France made its digital collection widely and anonymously available, but closely surveiled readers in its physical space. The Library of Congress offered democratic access and embraced an ‘open source’ approach to cataloguing, but corporate and public interests were in conflict, and ‘born digital’ documents created a policy crisis. The British Library offered an interface that emulated turning pages of rare tomes but came late to prioritize searchable text encoding. In contrast, the Danvers Archival Center offered a model of a ‘local’ archive that asserted its social function in a particular community but also claimed a role in shaping digital resources.",Reading Room(s): Building a National Archive in Digital Spaces and Physical Places,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
146,http://dx.doi.org/10.1093/llc/19.3.385,"{'string_id': '10.1093/llc/19.3.385', 'id_scheme': 'DOI'}","This paper describes the work of The Elements of Drawing, a project to digitize the teaching collection assembled by John Ruskin at the University of Oxford. It outlines John Ruskin’s links with Oxford, his reasons for creating the collection as an aid to his teaching of drawing at the University, and the ways in which he organized and catalogued the collection. It then discusses the particular benefits which digitization brings to the collection, and outlines the methods being used to digitize the collection as a series of images, texts, catalogue data, and associated metadata, and how it is being made available over the world wide web.",The Elements of Drawing,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
147,http://dx.doi.org/10.1093/llc/19.3.397,"{'string_id': '10.1093/llc/19.3.397', 'id_scheme': 'DOI'}","Textual encoding is one of the main focuses of Humanities Computing. However, existing encoding schemes and initiatives focus on ‘text’ from the character level upwards, and are of little use to scholars, such as papyrologists and palaeographers, who study the constituent strokes of individual characters. This paper discusses the development of a markup system used to annotate a corpus of images of Roman texts, resulting in an XML representation of each character on a stroke by stroke basis. The XML data generated allows further interrogation of the palaeographic data, increasing the knowledge available regarding the palaeography of the documentation produced by the Roman Army. Additionally, the corpus was used to train an Artificial Intelligence system to effectively ‘read’ in stroke data of unknown text and output possible, reliable, interpretations of that text: the next step in aiding historians in the reading of ancient texts. The development and implementation of the markup scheme is introduced, the results of our initial encoding effort are presented, and it is demonstrated that textual markup on a stroke level can extend the remit of marked‐up digital texts in the humanities.",Downs and Acrosses: Textual Markup on a Stroke Level,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
148,http://dx.doi.org/10.1093/llc/19.3.415,"{'string_id': '10.1093/llc/19.3.415', 'id_scheme': 'DOI'}","The New Comedy Masks project deals with objects—miniature ancient theatre masks—of immense cultural significance, and with texts—the comedies of Menander—that are at the origins of the European comic tradition. By 3D scanning the miniatures and enlarging them to life‐size, the project has initiated a programme of practice‐based research in the studio, leading to live performances that are the first to use objectively reconstructed ancient masks. The paper describes the considerations that apply at different stages of the process, from the digitization of the artifact to mask‐construction and performance experimentation.",Digital Resources for Practice-based Research: The New Comedy Masks Project,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
149,http://dx.doi.org/10.1093/llc/19.4.431,"{'string_id': '10.1093/llc/19.4.431', 'id_scheme': 'DOI'}","We present a computational system for morphological analysis and annotation of the Qur'an, for research and teaching purposes. The system facilitates a variety of queries on the Qur'anic text that make reference not only to the words, but also to their linguistic attributes. The core of the system is a set of finite-state based rules which describe the morpho-phonological and morpho-syntactic phenomena of the Qur'anic language. Using a finite-state toolbox we apply the rules to the Qur'anic text and obtain full morphological analysis of its words. The results of the analysis are stored in an efficient database and are accessed through a graphical user interface which facilitates the presentation of complex queries. The system is currently being used for teaching and research purposes; we exemplify its usefulness for investigating several morphological, syntactic, semantic, and stylistic aspects of the Qur'anic text.",Morphological Analysis of the Qur'an,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
150,http://dx.doi.org/10.1093/llc/19.4.453,"{'string_id': '10.1093/llc/19.4.453', 'id_scheme': 'DOI'}","Delta, a simple measure of the difference between two texts, has been proposed by John F. Burrows as a tool in authorship attribution problems, particularly in large ‘open’ problems in which conventional methods of attribution are not able to limit the claimants effectively. This paper tests Delta's effectiveness and accuracy, and shows that it works nearly as well on prose as it does on poetry. It also shows that much larger numbers of frequent words are even more accurate than the 150 that Burrows tested. Automated methods that allow for tests on large numbers of differently selected words show that removing personal pronouns and words for which a single text supplies most of the occurrences greatly increases the accuracy of Delta tests. Further tests suggest that large changes in Delta and Delta z-scores from the likeliest to the second likeliest author typically characterize correct attributions, that differences in point of view among the texts are more significant than differences in nationality, and that combining several texts for each author in the primary set reduces the effect of intra-author variability. Although Delta occasionally produces errors in attribution with characteristics that would normally lead to a great deal of confidence, the results presented here confirm its usefulness in the preliminary stages of authorship attribution problems.",Testing Burrows's Delta,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
151,http://dx.doi.org/10.1093/llc/19.4.477,"{'string_id': '10.1093/llc/19.4.477', 'id_scheme': 'DOI'}","John F. Burrows has proposed Delta, a simple new measure of textual difference, as a tool for authorship attribution, and has shown that it has great potential, especially in attribution problems where the possible authors are numerous and difficult to limit by traditional methods. In tests on prose, Delta has performed nearly as well as for Burrows's verse texts. A series of further tests using automated methods, however, shows that two modified methods of calculating Delta and three alternatives to or transformations of Delta produce results that are even more accurate. Four of these five new measures produce much better results than Delta both on a very diverse group of 104 novels and on a group of forty-four smaller contemporary literary critical texts. Although further testing is needed, Delta and its modifications should prove valuable and effective tools for authorship attribution.",Delta Prime?,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
152,http://dx.doi.org/10.1093/llc/19.4.497,"{'string_id': '10.1093/llc/19.4.497', 'id_scheme': 'DOI'}","The sentence-lengths of sixteen essays by Goldsmith are examined in relation to data from ten essays (we call these ‘doubtfuls’) which have been attributed to him. Comparisons between the ‘doubtfuls’ and the known Goldsmiths are made with reference to the χ2 goodness-of-fit test, and the method of reciprocal averaging. The Goldsmith essays form a close group, with four of the ‘doubtful’ essays well outside, two less remote and four within the Goldsmith cluster. Comparison with fifty essays by nine of Goldsmith's contemporaries reveals the distinctiveness of his sentence-length patterns, and strengthens the probability that the four least doubtful essays are his. In the case of Goldsmith, then, sentence-length may be considered a reliable stylistic marker.",Sentence-length and Authorship Attribution: the Case of Oliver Goldsmith,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
153,http://dx.doi.org/10.1093/llc/19.4.509,"{'string_id': '10.1093/llc/19.4.509', 'id_scheme': 'DOI'}","This paper describes the work carried out on the EMILLE Project (Enabling Minority Language Engineering), which was undertaken by the Universities of Lancaster and Sheffield. The primary resource developed by the project is the EMILLE Corpus, which consists of a series of monolingual corpora for fourteen South Asian languages, totalling more than 96 million words, and a parallel corpus of English and five of these languages. The EMILLE Corpus also includes an annotated component, namely, part-of-speech tagged Urdu data, together with twenty written Hindi corpus files annotated to show the nature of demonstrative use in Hindi. In addition, the project has had to address a number of issues related to establishing a language engineering (LE) environment for South Asian language processing, such as translating 8-bit language data into Unicode and producing a number of basic LE tools. The development of tools for EMILLE has contributed to the ongoing development of the LE architecture GATE, which has been extended to make use of Unicode. GATE thus plugs some of the gaps for language processing R&D necessary for the exploitation of the EMILLE corpora.",Corpus Linguistics and South Asian Languages: Corpus Creation and Tool Development,Oxford University Press (OUP),2004,,Literary and Linguistic Computing,19,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
154,http://dx.doi.org/10.1093/llc/fqaa002,"{'string_id': '10.1093/llc/fqaa002', 'id_scheme': 'DOI'}","The extraction of large amounts of multilingual parallel text from web resources is a widely used technique in natural language processing. However, automatically collected parallel corpora usually lack precise metadata, which are crucial to accurate data analysis and interpretation. The combination of automated extraction procedures and manual metadata enrichment may help address this issue. Wikipedia is a promising candidate for the exploration of the potential of said combination of methods because it is a rich source of translations in a large number of language pairs and because its open and collaborative nature makes it possible to identify and contact the users who produce translations. This article tests to what extent translated texts automatically extracted from Wikipedia by means of neural networks can be enriched with pertinent metadata through a self-submission-based user survey. Special emphasis is placed on data usefulness, defined in terms of a catalogue of previously established assessment criteria, most prominently metadata quality. The results suggest that from a quantitative perspective, the proposed methodology is capable of capturing metadata otherwise not available. At the same time, the crowd-based collection of data and metadata may face important technical and social limitations.",Automatically extracted parallel corpora enriched with highly useful metadata? A Wikipedia case study combining machine learning and social technology,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
155,http://dx.doi.org/10.1093/llc/fqaa005,"{'string_id': '10.1093/llc/fqaa005', 'id_scheme': 'DOI'}","The aim of this article was to investigate the influence of medical discourse of the turn of the nineteenth and the twentieth centuries through a case study of dramatic works of the Polish modernism. It was based on a detailed analysis of ninety plays randomly selected out of all performances staged between 1890 and 1913 at two major theatres from Krakow and Warsaw. The author analysed the ways of dramatizing symptoms of hysteria and compared the language used in describing disorders in female and male characters, combining quantitative and qualitative methods of text analysis. Almost all the plays included the symptoms of hysteria; most of them presented characters prone to hysteria. The number of symptoms and hysterical characters increased in periods when new medical theories had gained popularity. The image of hysteria that emerges from the surveyed works both defied the stereotype of hysteria as a specifically feminine illness and confirmed these stereotypes. Dramatic works could have played an important role in the process of popularizing hysteria.",The art of nerves: A quantitative and qualitative analysis of drama at the turn of nineteenth and twentieth century,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
156,http://dx.doi.org/10.1093/llc/fqaa006,"{'string_id': '10.1093/llc/fqaa006', 'id_scheme': 'DOI'}","Data mining, statistics, and data analysis are popular techniques to study datasets and extract knowledge from them. In this article, principal component analysis and factor analysis were applied to cluster thirteen different given arrangements about the Suras of the Holy Quran. The results showed that these thirteen arrangements can be categorized in two parts such that the first part includes Blachère, Davood, Grimm, Nöldeke, Bazargan, E’temad-al-Saltane and Muir, and the second part includes Ebn Nadim, Jaber, Ebn Abbas, Hazrat Ali, Khazan, and Al-Azhar.",Statistical approaches in literature: An application of principal component analysis and factor analysis to analyze the different arrangements about the Quran’s Suras,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
157,http://dx.doi.org/10.1093/llc/fqaa007,"{'string_id': '10.1093/llc/fqaa007', 'id_scheme': 'DOI'}","Computing machines allow quantitative analysis of large databases of text, providing knowledge that is difficult to obtain without using automation. This article describes Universal Data Analysis of Text (UDAT) —a text analysis method that extracts a large set of numerical text content descriptors from text files and performs various pattern recognition tasks such as classification, similarity between classes, correlation between text and numerical values, and query by example. Unlike several previously proposed methods, UDAT is not based on frequency of words and links between certain key words and topics. The method is implemented as an open-source software tool that can provide detailed reports about the quantitative analysis of sets of text files, as well as exporting the numerical text content descriptors in the form of comma-separated values files to allow statistical or pattern recognition analysis with external tools. It also allows the identification of specific text descriptors that differentiate between classes or correlate with numerical values and can be applied to problems related to knowledge discovery in domains such as literature and social media. UDAT is implemented as a command-line tool that runs in Windows, and the open source is available and can be compiled in Linux systems. UDAT can be downloaded from http://people.cs.ksu.edu/∼lshamir/downloads/udat.",UDAT: Compound quantitative analysis of text using machine learning,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
158,http://dx.doi.org/10.1093/llc/fqaa008,"{'string_id': '10.1093/llc/fqaa008', 'id_scheme': 'DOI'}","This article examines the impact of the digital humanities on information science and information scientists and how information scientists can contribute to digital collaborations in the digital humanities. This article uses three basic concepts—user-researcher, digital information system, and digital research object—as the framework for a description of the digital humanities shift and discusses the consequences of this shift for information science with reference to these concepts. The second part of the article investigates the two pillars of the digital humanities shift, acknowledgement of digital structure and the exploring mind, in more detail. In the case of correspondences, acknowledgement of digital structure involves respecting the ‘thing-like’ properties of letters that cannot be handled (searched, classified, etc.) unless they are translated into digitized metadata; this elevates metadata to genuine digital research objects. These theoretical issues are illustrated by the Prior archive, a collection of digitized letters, manuscript drafts, and other ‘grey’ material bequeathed from the New Zealand logician-philosopher Arthur Norman Prior. The concept of the ‘exploring mind’ is connected to a movement in information science, away from the needful user and information gap paradigm towards more open and exploratory information behaviour in the digital humanities. A concluding literature review examines selected works of information science that deals with non-standard, serendipitous information behaviour and identifies exploratory and serendipitous design features of information systems for the digital humanities. These features are represented in a taxonomy consisting of six design categories.","‘Attention, attention, exploring minds acknowledge digital structure!’ The shift to digital humanities has happened, so what should information scientists do in response?",Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
159,http://dx.doi.org/10.1093/llc/fqaa009,"{'string_id': '10.1093/llc/fqaa009', 'id_scheme': 'DOI'}","By incorporating computational methods into reading literary texts, this study examines the literary implications of the ‘vocabulary density’ and frequency of nouns and adjectives in T. S. Eliot’s poetry. This study analyzes 4,689,655 words from forty-seven poets available on Project Gutenberg, a catalog spanning from the eighteenth century to the early twentieth century. The data illustrate both the continuity and discontinuity found in English and American poetry dependent on conventional divisions between literary movements: eighteenth century, Romanticism, Imagism, and Modernism. The findings shed light on the similarities and differences between Eliot’s poetry and others’, particularly in terms of Franco Moretti’s concept of ‘modern epic’ and his methodology of ‘distant reading’. Through this combined quantitative and qualitative research, this article ultimately upholds the notion that the linguistic distinction of Eliot’s high modernist poetry lies, by and large, in his use of invented and equivocal words that reflects and represents an artistic response to modern human, cultural, social conditions, and experiment with poetic diction and polyphonic voice in the early twentieth century.",Implications of vocabulary density for poetry: Reading T. S. Eliot’s poetry through computational methods,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
160,http://dx.doi.org/10.1093/llc/fqaa010,"{'string_id': '10.1093/llc/fqaa010', 'id_scheme': 'DOI'}","Following Dr Barber’s unfortunate criticism (Barber, 2018, Marlowe and overreaching: a misuse of stylometry. Digital Scholarship in the Humanities, 34:1–12), in which she, with an obvious lack of familiarity with them, subjected the Rolling Delta procedures used, to the caveats of Delta and traditional stylometry, this article makes use of an extended methodological framework and applies Rolling Delta to the target texts with a totality of reference texts. The outcome is different from the expected, since the author of Tamburlaine 1 and 2 emerges as stylistically also dominant in the anonymous play The Tragedy of Locrine, in Kyd’s closet play Cornelia, in Peele’s The Battle of Alcazar and David and Bethsabe. In contrast, the official Marlowe corpus relates stylistically to contemporary authors, but not to the two Tamburlaines. Traditional scholarship and learning do not refute conjectures of misattributed Peele plays and there are also strong indications that plays associated with Lord Strange’s Men nominally became Marlowe plays when Henslowe acquired them in 1594 for his Admiral’s Men and printers made use of the cult of personality, in which the author’s death became an important factor in the marketing of printed playbooks. Otherwise, there is no documentary and empirical evidence that Marlowe wrote the plays in question. The canonization of the plays occurred only in the nineteenth century, and the Marlowe we have inherited—the poet, spy, atheist, homosexual, and so on—is almost entirely an invention of the twentieth century (Hooks, 2018, Making Marlowe. In Melnikoff, K. and Knutson, R. (eds), Christopher Marlowe, Theatrical Commerce, and the Book Trade. Cambridge: University Press, p. 98).",The Marlowe corpus revisited,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
161,http://dx.doi.org/10.1093/llc/fqaa011,"{'string_id': '10.1093/llc/fqaa011', 'id_scheme': 'DOI'}","Using citation analysis, we consider the role of gender in citation practices in conference special issues of Digital Scholarship in the Humanities. Our examination of citations in Digital Humanities conference special issues from 2006 to 2015 demonstrates gender bias in citational practices. This bias is consistent with broader trends in citational politics across the academy more broadly but is a threat to equity and justice within the scholarly community. We further offer proposals for improving citational practices to resist gender bias. Quantifying the impact of gender on citations, we argue, is one approach to understanding gender inequalities within digital humanities communities and to generating solutions to promote the broadest representation of digital humanities scholarship in scholarly communications.",Citational politics: Quantifying the influence of gender on citation in Digital Scholarship in the Humanities,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
162,http://dx.doi.org/10.1093/llc/fqaa014,"{'string_id': '10.1093/llc/fqaa014', 'id_scheme': 'DOI'}","This article investigates the collocational behavior of English modal auxiliaries such as may and might with the aim of finding corpus-based measures that distinguish between different modal expressions and that allow insights into why speakers may choose one over another in a given context. The analysis uses token-based semantic vector space modeling (Heylen et al., 2015, Monitoring polysemy. Word space models as a tool for large-scale lexical semantic analysis. Lingua, 157: 153–72; Hilpert and Correia Saavedra, 2017, Using token-based semantic vector spaces for corpus-linguistic analyses: From practical applications to tests of theoretical claims. Corpus Linguistics and Linguistic Theory) in order to determine whether different modal auxiliaries can be distinguished in terms of their collocational profiles. The analysis further examines whether different senses of the same auxiliary exhibit divergent collocational preferences. The results indicate that near-synonymous pairs of modal expressions, such as may and might or must and have to, differ in their distributional characteristics. Also, different senses of the same modal expression, such as deontic and epistemic uses of may, can be distinguished on the basis of distributional information. We discuss these results against the background of previous empirical findings (Hilpert, 2016, Construction Grammar and its Application to English, 2nd edn. Edinburgh: Edinburgh University Press, Flach, in press, Beyond modal idioms and modal harmony: a corpus-based analysis of gradient idiomaticity in modal-adverb collocations. English Language and Linguistics) and theoretical issues such as degrees of grammaticalization (Correia Saavedra, 2019, Measurements of Grammaticalization: Developing a Quantitative Index for the Study of Grammatical Change. PhD Dissertation, Université de Neuchâtel) and the avoidance of synonymy (Bolinger, 1968, Entailment and the meaning of structures. Glossa, 2(2): 119–27).",Disentangling modal meanings with distributional semantics,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
163,http://dx.doi.org/10.1093/llc/fqaa015,"{'string_id': '10.1093/llc/fqaa015', 'id_scheme': 'DOI'}","The objective of this study is to answer the question: Do the central nodes of a social network of characters correspond with the protagonists of a theater play? To answer this question we evaluate different measures of centrality along with other textual quantitative values in relation to other manually annotated metadata on a corpus of twenty five dramatic plays of the Spanish theatre of the Silver Age (1868-1936). The found results show that centrality correlates moderately with importance, but the correlation with the textual quantitative values is stronger.",¿Existe correlación entre importancia y centralidad? Evaluación de personajes con redes sociales en obras teatrales de la Edad de Plata,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
164,http://dx.doi.org/10.1093/llc/fqaa016,"{'string_id': '10.1093/llc/fqaa016', 'id_scheme': 'DOI'}","Recently, with an increasing number of metaphor studies being conducted, research on metaphor interpretation has set off an upsurge. Although a multitude of studies on the interpretation of metaphors exists, many are limited to the understanding of literal meanings without attempting an interpretation of hidden emotions in metaphorical expressions. There are particularly few studies on metaphorical emotions interpretation in literary studies with rich and implicit emotions, such as classical Chinese poetry. This study proposes the interpretation of the metaphorical emotions of special objects in Chinese poetry based on emotion distribution. We present a statistical approach to calculate the emotion distribution of our target objects by exploiting contextual emotion mining. According to the emotion distribution, the emotion with the highest probability is considered the metaphorical emotion of the target object. Subsequently, the metaphorical emotion can be determined as a positive or negative sentiment based on expert annotations. Using the proposed method, we have tested two representative objects, ‘月’ (moon) and ‘风’ (wind), and the accuracy performances were 84% and 83.33%, respectively, for sentiment detection and 66% and 70% for emotion-specific metaphorical interpretation. The results demonstrate that our approach can be used to assist readers with metaphorical emotional understanding in Chinese poetry.",Interpretation of metaphors in Chinese poetry: Where did Li Bai place his emotions?,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
165,http://dx.doi.org/10.1093/llc/fqaa017,"{'string_id': '10.1093/llc/fqaa017', 'id_scheme': 'DOI'}","Correctness of the designed system is one of the most important issues in the software development process. Therefore, various tests have been defined and designed to help software teams develop software with little or no problem. Finding a proper link between test class and the class under the test is an important but difficult task. Finding this relation helps the developers to conduct regression tests more efficiently. In this paper, we seek to propose a model for recovering traceable links between test classes and the classes under the test. The proposed method encompasses three parts: (1) method for extracting keywords and the measure of similarity of a specific part of code, (2) backward chain method based on a rule-based system, (3) using hybrid model to find traceable links between test classes and the code under test. This study uses three open-source and one industrial source projects to conduct experiments. The results are satisfactory compared to previous studies.",Traceability mining between unit test and source code based on textual analysis applied to software systems,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
166,http://dx.doi.org/10.1093/llc/fqaa018,"{'string_id': '10.1093/llc/fqaa018', 'id_scheme': 'DOI'}","This article revisits the study of the poetry communities, sisa, in the eighteenth-century Seoul, in terms of their membership, structural patterns, and boundaries of interaction. By applying methods in the digital humanities such as social network analysis and geographical information systems, I argue that literary activities in the sisa were performed beyond the factional demarcations, and cultural leverage was determined by their positional advantage in the community structure, not imposed by the political dominance. The factional affiliations of the participants were not directly correlated with the formation of cultural identities. The poetry communities functioned as major loci of cultural textual networks in the late Chosŏn Seoul. Paying no heed to hierarchical distinction, the participants enjoyed the convivial spontaneous environments and shared their literary works with one another. Focusing on the collaborative nature of the era, many modern scholars have examined the kinship, social classes, and political orientations of the community members; furthermore, some argue that those who belonged to influential political factions also exercised their cultural influence and set mainstream literary trends. My research differs from previous contributions in its emphasis on the relational community structure and the information flow. Measuring the homogeneity, centralities, and membership, I prove that the cultural practices were much more complicated beyond the social–political system, and explain how politically marginalized people were able to gain leverage in the cultural world.",Poetry in action: Networks of literary communication and the cultural leverage in the eighteenth-century Seoul,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
167,http://dx.doi.org/10.1093/llc/fqaa019,"{'string_id': '10.1093/llc/fqaa019', 'id_scheme': 'DOI'}","The growing amount of openly available research data enables various possibilities of reuse. Data can be analyzed, visualized, or even further processed, enriched, and combined with other sources to enable new research questions and a different view on the material. Several catalogues and research tools aggregate collections on specific topics to make them searchable and reusable. To be able to bring together different collections, a common data standard is necessary. This article discusses how semantic web technologies can be used to connect digital as well as analogue music catalogues and music editions. The article first discusses how music can be searched using its characteristic melody and presents the approach of the open-source search engine for music incipits, IncipitSearch. Subsequently, the advantages of semantic web standards for musicology are highlighted. Then, the underlying RDF- and schema.org-based metadata standard, which is used to aggregate and distribute the data, is discussed. The article concludes with an outlook on research perspectives for digital musicology and musicology in general.",Interconnecting music repositories with semantic web technologies—an RDF- and schema.org-based approach,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
168,http://dx.doi.org/10.1093/llc/fqaa021,"{'string_id': '10.1093/llc/fqaa021', 'id_scheme': 'DOI'}","This article presents a relational database capable of integrating data from a variety of types of written sources as well as material remains. In response to historical research questions, information from such diverse sources as documentary, bioanthropological, isotopic, and DNA analyses has been assessed, homogenized, and situated in time and space. Multidisciplinary ontologies offer complementary and integrated perspectives regarding persons and goods. While responding to specific research questions about the impact of globalization on the isthmus of Panama during the sixteenth and seventeenth centuries, the data model and user interface promote the ongoing interrogation of diverse information about complex, changing societies. To this end, the application designed makes it possible to search, consult, and download data that researchers have contributed from anywhere in the world.",The integration of heterogeneous information from diverse disciplines regarding persons and goods,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
169,http://dx.doi.org/10.1093/llc/fqaa022,"{'string_id': '10.1093/llc/fqaa022', 'id_scheme': 'DOI'}","Although most would agree that the future of the scholarly edition lies in the digital medium, it is the print scholarly edition that is still more often cited and read. The production of digital scholarly editions (DSEs) is still seen as an experimental field whose methodology has not yet settled to the extent that a digital editing project can be approached with the same confidence as the making of a print edition. This article describes an experimental conversion of a print scholarly edition—Giacomo Leopardi’s Idilli by Paola Italia (2008)—into a DSE. This posed a challenge due to the complexity of its internal evidence, but was also relatively short and suitable for an experimental edition. Our objective was to assimilate into a web-based DSE all the information contained in the text and apparatus of the print edition. We also sought to discover whether the making of a DSE today that could fully utilize the affordances of the web, would necessarily place a significant technical load on editors who are more accustomed to solving textual problems. We review briefly a number of generic tools for making DSEs and describe two attempts at making our own DSE of Leopardi’s Idilli: a wiki edition whose primary purpose was pedagogical and a DSE based on the software used to make the Charles Harpur Critical Archive (Eggert, 2019, Charles Harpur Critical Archive. http://charles-harpur.org). We compare these experiences and draw conclusions about the prospects of making DSEs today.",From print to digital: A web edition of Giacomo Leopardi’s Idilli,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
170,http://dx.doi.org/10.1093/llc/fqaa023,"{'string_id': '10.1093/llc/fqaa023', 'id_scheme': 'DOI'}","Higher education operates in a quickly changing, progressively more globalized, cosmopolitan, and interconnected world (Bauman, 2000, Globalization: The Human Consequences. New York/Chichester: Columbia University Press; Appiah, 2006, Cosmopolitanism: Ethics in a World of Strangers. New York: W.W. Norton & Co; Zuckerman, 2013, Rewire: Digital Cosmopolitans in the Age of Connection. New York: W. W. Norton & Company). At the same time, substantive inequalities between people and places mean that this connectivity and knowledge is unevenly spread (Hallberg Adu, 2014, What is the opposite of a knowledge society? A critical reflection from Ghana. In Amoah, L. (ed.), Impacts of the Knowledge Society on Economic and Social Growth in Africa. IGI Global). For our students, the future leaders of this unequal world, critical reasoning becomes a key skill, and perhaps especially so for students in the Global South. This paper argues that digital humanities (DH) can provide both a theoretical framework for decolonizing the academy and technological solutions to hurdles in this process. The paper argues that assignments, their theoretical underpinnings, and implementation are key to decolonizing higher education. It describes three accessible technology-driven assignments with DH pedagogy created for diverse classrooms at Ashesi University in Ghana and discusses their outcomes.",The promise of digital humanities pedagogy: Decolonizing a diverse classroom in Ghana,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
171,http://dx.doi.org/10.1093/llc/fqaa024,"{'string_id': '10.1093/llc/fqaa024', 'id_scheme': 'DOI'}","This article brings current core concerns of multimodality into dialogue with approaches identified with media archaeology. I begin by considering the development of multimodality in relation to the emphasis that media archaeologists place on non-linear and parallel histories, the relativity of ‘newness’, and cyclical thinking. I then move on to consider some of the respective conceptual undergirding of multimodality and media archaeology, focussing on key issues of materiality and media specificity, signs and signals, media convergence and commensurability. I argue that this juxtaposition brings fresh perspectives to the question of ‘mode’. Significantly, these are attuned both to social and formal considerations, but in ways that differ from both social semiotic orientations and other approaches to multimodality. Having considered these fundamentals, I turn to questions of interactivity, product, and process, and the blurring of boundaries between categories such as reading and writing. As a final intersection, I bring the growing interest in integrating quantitative, corpus-based methods in multimodal analysis into dialogue with the prioritization of the digital archive as a site of specific media archaeological interest with inherent potential for algorithmic manipulation. I conclude with some observations about the status of multimodality and media archaeology as communities and, more specifically, the potential for complementarity between them.",Multimodality and media archaeology: Complementary optics for looking at digital stuff?,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
172,http://dx.doi.org/10.1093/llc/fqaa025,"{'string_id': '10.1093/llc/fqaa025', 'id_scheme': 'DOI'}","The experiences of murdered victims of Nazi persecutions perished with them. This article discusses how text and data mining technology has helped to recover fragments of these lost experiences out of 2,500 oral history interviews with survivors. This gave rise to Let them Speak, a data edition of Holocaust testimonies. The first part situates the challenge of revealing lost experiences in historiography, and argues that the experience of murdered victims can be reconstructed through the collective experience. The second part shows how text and data mining techniques assisted the author to identify some pieces of the collective experience. The third part presents how web technology and visualization are used to render pieces of the collective experience as testimonial fragments of the Holocaust. ",Recovering and rendering silenced experiences of genocides: testimonial fragments of the Holocaust,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
173,http://dx.doi.org/10.1093/llc/fqaa027,"{'string_id': '10.1093/llc/fqaa027', 'id_scheme': 'DOI'}","Local historical documents originated from daily life of people belong to special collection resources that were not published publicly. They are valuable assets of universities and libraries. At present, most documents had only finished digitalization or partial datalization work. However, the requirements of deep knowledge mining in documents data, providing visual analysis, and effectively supporting the research of historic humanities scholars had not been fully met. Taking the local historical documents project of Shanghai Jiao Tong University as an example, using relevant techniques of digital humanities (DH), the in-depth analysis and utilization research of documents data were carried out. On the one hand, the core database of the documents was established based on standardizing metadata cataloguing and establishing metadata association. On the other hand, based on the core database, an intelligent DH system platform was constructed. The platform is to realize full-field retrieval and display of the documents, text analysis, association analysis, statistics, and visual presentation of knowledge. In addition, in the process of using the platform for research, humanities scholars can continuously expand the data dimensions and the relationships between data, achieve intelligent supplementation of documents data and platform self-learning. The concept of DH has led to a new direction of database construction and platform development. In the exploration and practice of DH, libraries should continue to widen thinking, improve service and innovation capabilities, and provide better research perspectives, research environments, research support, and research experience for humanities scholars.",From collection resources to intelligent data: Construction of intelligent digital humanities platform for local historical documents of Shanghai Jiao Tong University,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
174,http://dx.doi.org/10.1093/llc/fqaa028,"{'string_id': '10.1093/llc/fqaa028', 'id_scheme': 'DOI'}","Building upon Walsh’s Comic Book Markup Language (CMBL) used for encoding text features of comics documents, this essay explores how CBML can be modified and expanded using additional Text Encoding Initiative (TEI) features to reflect alternative theoretical and critical approaches to comics. In doing so, this essay argues that markup languages offer not only a means for analyzing encoded documents but also a means for analyzing critical approaches to documents. Because markup language reflects the critical stance of whoever produces the encoding, any revision to the markup potentially reflects a revision to the critical theoretical framework from which the encoder operates. As such, implementation of markup language in comics studies can function not only as a metalanguage for describing comics but also as a form of meta-criticism. To this end, this essay explores methods for incorporating CBML and TEI to reflect commonly opposed approaches to analyzing comics documents.",On the use of XML markup language in comics criticism,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
175,http://dx.doi.org/10.1093/llc/fqaa030,"{'string_id': '10.1093/llc/fqaa030', 'id_scheme': 'DOI'}","Complex network approach provides language research with quantitative measures that can capture global features of language. Although translational language has been recognized as a ‘third code’ by some researchers, its independence still calls for further and quantitative validation in an overall manner. In this study, we intend to examine this independence and explore comprehensively its features. We investigated macroscopically translational language from English into Chinese and from Chinese into English by comparing with its source language and native language through syntactic dependency networks. The results show that: (1) translational language presents small-world and scale-free properties like most languages do; (2) however, it is independent of and different from both source language and native language in terms of its network parameters; (3) its network parameters show values eclectic between source language and native language, and this eclectic tendency may be regarded as a new candidate for universal features of translational language, which certainly needs further validation in other genres and language pairs. This study also corroborates that quantitative linguistic method of complex network approach can be well utilized in the study of translational language.",A syntactic dependency network approach to the study of translational language,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
176,http://dx.doi.org/10.1093/llc/fqaa033,"{'string_id': '10.1093/llc/fqaa033', 'id_scheme': 'DOI'}","Open social scholarship highlights outreach and partnerships by emphasizing community-driven initiatives in an attempt to bridge the gap between the practices of the university and the goals of the community. Over the last few years, the Electronic Textual Cultures Lab at the University of Victoria has introduced a number of initiatives to this end, including the Open Knowledge Program and Open Scholarship Awards. In describing these initiatives, the article engages the larger framework of community engagement and public-facing scholarship. The guiding questions for this article and our work more broadly are: how can we productively put open social scholarship into practice? What type of scholarship is considered public facing? What is the best practice around co-creating knowledge in the humanities with communities that are academic-aligned or non-academic?",Open social scholarship in action,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
177,http://dx.doi.org/10.1093/llc/fqh032,"{'string_id': '10.1093/llc/fqh032', 'id_scheme': 'DOI'}","In this paper, we present a system performing morphological analysis and synthesis for the Greek language. Its main features are the use of a single framework to describe and classify morphological data into well-defined classes characterized by their own set of properties and mechanisms and the structuring of the database into distinct levels corresponding to the different levels of morphological information present in the above framework. The resulting system is characterized by simple and flexible algorithms with 100% success in the recognition and generation of morphological forms of the language independently of the complexity of the data they are handling.",The Computational Modern Greek Morphological Lexicon—An Efficient and Comprehensive System for Morphological Analysis and Synthesis,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
178,http://dx.doi.org/10.1093/llc/fqh041,"{'string_id': '10.1093/llc/fqh041', 'id_scheme': 'DOI'}","At the University of Groningen we have emphasized a simple view of humanities computing as computing in service of the humanities. This means that we seek to answer scholarly questions in linguistics, history, and art history by using the computer, exploiting especially its ability to process large amounts of data and the transparency of its processing. We have shied away from questions of digital culture, avoided overemphasis on pedagogical applications of computers, and eschewed visions of scientific revolution—including, in particular, the revolutionary idea that humanities computing is a discipline, preferring to think of it instead as a federation of disciplines, whose practitioners find it opportune to collaborate for reasons of some common problems. We have discovered that our ability to deal with large amounts of data marks the distinctive contributions we can make to humanities scholarship.",Computational Contributions to the Humanities,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
179,http://dx.doi.org/10.1093/llc/fqh042,"{'string_id': '10.1093/llc/fqh042', 'id_scheme': 'DOI'}","The ink and stylus tablets discovered at the Roman Fort of Vindolanda are a unique resource for scholars of ancient history. However, the stylus tablets have proved particularly difficult to read. This paper describes the initial stages in the development of a computer system designed to aid historians in the reading of the stylus tablets. A detailed investigation was undertaken, using Knowledge Elicitation techniques borrowed from Artificial Intelligence, Cognitive Psychology, and Computational Linguistics, to elicit the processes experts use whilst reading an ancient text. The resulting model was used as the basis of a computer architecture to construct a system which takes in images of the tablets and outputs plausible interpretations of the documents. It is demonstrated that using Knowledge Elicitation techniques can further the understanding of complex processes in the humanities, and that these techniques can provide an underlying structure for the basis of a computer system that replicates that process. As such it provides significant insight into how experts work in the humanities, whilst providing the means to develop tools to assist them in their complex task.",Reading the Readers: Modelling Complex Humanities Processes to Build Cognitive Systems,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
180,http://dx.doi.org/10.1093/llc/fqh043,"{'string_id': '10.1093/llc/fqh043', 'id_scheme': 'DOI'}","The term document is used in various contexts, often referring to very different things. This article argues that we need to avoid a restrictive, essentialist definition of the concept and instead study the cognitive models that guide our way of viewing documents in different situations. Examples are drawn from the Library and Information field to show how the view of documents is influenced by different cognitive models and how more complex understandings may be described in terms of clusters of models. Such a set of tools for discussing the concept will be particularly useful as we are facing a whole range of new types of ‘documents’ made possible by digital media.",What's in a Name? Contextualizing the Document Concept1,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
181,http://dx.doi.org/10.1093/llc/fqh044,"{'string_id': '10.1093/llc/fqh044', 'id_scheme': 'DOI'}","This paper describes a pilot project to evaluate the use of Geographical Information System (GIS) and website technology to explore, integrate, and display quantative and qualitative information about forced migration in a region of Macedonia from 1880 to the present day. Approaches, techniques, and technologies that would be required for a potential large-scale project were explored and challenges that would be encountered and possible novel solutions that could be applied were identified. The project demonstrated that a great deal could be achieved in the visualization of spatial data at a relatively low cost in terms of finance, time and expertise. It concluded that current GIS technology has many weaknesses when applied to humanities data. However, there is considerable potential for applying existing methods in new and imaginative ways. The project gave an indication of some of this potential and the possible future development of Geographical Information Science approaches specifically for the humanities. The project is one of a new style of digital projects in the humanities that make use of image, spatial database, and text-based technologies. By taking a range of unpublished material and exploring ways of producing a digital museum of cultural heritage aimed primarily at the communities of its subject matter it exhibits the broader social role of humanities computing and the resources it develops.",The Application of a Geographical Information System to the Creation of a Cultural Heritage Digital Resource,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
182,http://dx.doi.org/10.1093/llc/fqh045,"{'string_id': '10.1093/llc/fqh045', 'id_scheme': 'DOI'}","In this paper we discuss the results of the Nomen Nescio Named Entity Recognition project, a joint effort for the mainland Scandinavian languages—Norwegian, Swedish, and Danish. Five research groups have been involved, and developed NE recognizers using rule-based as well as statistical methods. We focus particularly on the choice of semantic categories and the problems regarding metonymy and semantic polysemy. Furthermore, we discuss the extent to which different approaches to these problems have different effects on the different types of systems, and look at two strategies, which we call Function over Form, and Form over Function.",Named Entity Recognition for the Mainland Scandinavian Languages,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
183,http://dx.doi.org/10.1093/llc/fqh046,"{'string_id': '10.1093/llc/fqh046', 'id_scheme': 'DOI'}","An approach to the unification of XML (Extensible Markup Language) documents with identical textual content and concurrent markup in the framework of XML-based multi-layer annotation is introduced. A Prolog program allows the possible relationships between element instances on two annotation layers that share PCDATA to be explored and also the computing of a target node hierarchy for a well-formed, merged XML document. Special attention is paid to identity conflicts between element instances, for which a default solution that takes into account metarelations that hold between element types on the different annotation layers is provided. In addition, rules can be specified by a user to prescribe how identity conflicts should be solved for certain element types.",Unification of XML Documents with Concurrent Markup,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
184,http://dx.doi.org/10.1093/llc/fqh047,"{'string_id': '10.1093/llc/fqh047', 'id_scheme': 'DOI'}","Digital Humanities (DH) and Digital Library (DL) projects are complex systems that require specialized programming skills. Many encoders cannot take their work to the next level by transforming their collections of structured XML texts into a web searchable and browsable database. Often teams of text encoders are able to encode their texts with a high degree of sophistication, but unless they have funds to hire a programmer, their collections far too often remain on local disk storage away from public access. <teiPublisher> aims to relieve some of this burden by providing the tools to manage an extensible, modular and configurable XML-based repository which will house, search, browse, and display documents encoded in TEI-Lite on the world wide web. <teiPublisher> provides an administrative interface that allows DL and DH administrators to upload and delete documents from a web accessible repository; analyze XML documents to determine elements for searching and browsing; refine ontology development; select inter and intra document links; partition the repository into collections; create backups; generate search, browse, and display pages; customize the interface; and associate XSL transformation scripts and CSS stylesheets to obtain different target outputs (HTML, PDF, etc.).",<teiPublisher>: A Repository Management System for TEI Documents,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
185,http://dx.doi.org/10.1093/llc/fqh048,"{'string_id': '10.1093/llc/fqh048', 'id_scheme': 'DOI'}","In this paper we describe the mix of text-oriented and data-oriented materials that have arisen during the process of conceptualising the Durham Liber Vitae (DLV) project. We have found a mixing of text- and data-oriented materials common in our projects, and that some aspects of SGML and XML markup's conceptual orientation—particularly the strong preference for asserting associations between elements by hierarchy and containment (the OHCO model)—have often obscured the presence of data-oriented (non-hierarchical) elements in the materials, and or encouraged inadequate ways to represent them. Although discussion of XML and its modelling abilities within the Computing Humanities community have tended to focus on issues arising in the OHCO model, the OHCO model itself is not the only modelling approach that XML markup provides. This paper demonstrates a way of taking conventional data modelling diagrams (inherently not OHCO in orientation) and modelling them for XML markup in a way that uses XML's preferred OCHO/containment approach where-ever possible, and XML's link-oriented association (e.g. ID/IDREF) approach between different hierarchies when essential. It then touches on aspects of ownership and reference that seem to lie behind XML's containment and linking association strategies. Finally, it describes some of the difficulties that standard XML tools such as XSLT and XPath (obviously primarily designed with the OHCO model in mind) have when dealing with links in XML, and shows an example of where XQuery's syntax—born out of work with relational databases—better handles queries based around linking.",Documents and Data: Modelling Materials for Humanities Research in XML and Relational Databases,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
186,http://dx.doi.org/10.1093/llc/fqh049,"{'string_id': '10.1093/llc/fqh049', 'id_scheme': 'DOI'}","An increasing number of scholars think a new instrument to publish academic work is needed. In fact, the scientific journals are in a monopoly market that makes access to scientific information very expensive. The richest university libraries use 80–90% of their budgets for the purchase of scientific journals and nevertheless are able to afford only a small part of academic literature. For scholarly publications in the Humanities there is not a monopoly market—there is no market at all. Public libraries have less and less money for monographs. The Humanities are in constant crisis as far as the publication of scholarly editions is concerned. This is especially true of genetic and facsimile editions, but it is also the case for all projects where the requirements of scholarly work are in conflict with the realities of the book market. Furthermore, the access to libraries and archives holding the primary sources for scholarly work is often difficult, expensive and unsatisfactory. The HyperLearning project is an extension of the HyperNietzsche project, which tries to solve the difficulties outlined above. This short research report describes the HyperLearning project, focusing on its technological activities. In the first part we will delineate the path from HyperNietzsche to HyperLearning. The following parts are an overview of three major technical research areas of HyperLearning.",The HyperLearning Project: Towards a Distributed and Semantically Structured e-research and e-learning Platform,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
187,http://dx.doi.org/10.1093/llc/fqh050,"{'string_id': '10.1093/llc/fqh050', 'id_scheme': 'DOI'}","The Centre for Irish and Celtic Studies at the University of Ulster is currently producing a digital dictionary of medieval Irish (eDIL) based on the standard Dictionary of the Irish Language published by the Royal Irish Academy, Dublin. This paper addresses some of the problems encountered in the digitization process, including data capture, processing non-standard characters, modifications to the TEI guidelines, automatic generation of tags, and the establishment of a lexical view while preserving the original format of the paper dictionary.",Digitizing a Dictionary of Medieval Irish: the eDIL Project,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
188,http://dx.doi.org/10.1093/llc/fqh051,"{'string_id': '10.1093/llc/fqh051', 'id_scheme': 'DOI'}","Character idiolects in Henryk Sienkiewicz's trilogy were studied in the original and in two English translations by Jeremiah Curtin and W. S. Kuniczak. The method used was Burrows's technique of multivariate analysis of correlation matrices of relative frequencies of the most frequent words in the dialogue. The aim of the study was to verify the intuitions of traditional interpretations, to acquire a more comprehensive view of the phenomenon, and to obtain new insights into the nature of idiolect differentiation in Sienkiewicz. Multidimensional scaling plots for the original yielded patterns of idiolect differentiation by nationality, social status, gender, and age. Corresponding plots for the two translations preserved many of these patterns and exhibited strong similarities to each other. More studies including modified methods (including Burrows's Delta) are needed to observe further and explain why exactly patterns of similarity/difference between character idiolects are so strongly preserved in translation.",Burrowing into Translation: Character Idiolects in Henryk Sienkiewicz's Trilogy and its Two English Translations,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
189,http://dx.doi.org/10.1093/llc/fqi001,"{'string_id': '10.1093/llc/fqi001', 'id_scheme': 'DOI'}","Chaucer's Wife of Bath's Prologue survives in both hand-written and early print witnesses dating from the 15th century. The introduction of material from more than one exemplar into a new copy results in contamination of a textual tradition. This contamination causes problems in standard phylogenetic analysis. We use an application of the maximum χ2 method (developed for the detection of recombination in DNA sequences) to identify locations where scribes may have changed their exemplar whilst copying the tale. Our results are largely in agreement with other published sources, indicating that this method may prove useful in the analysis of a contaminated tradition.",The Identification of Exemplar Change in the Wife of Bath's Prologue Using the Maximum Chi-Squared Method,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
190,http://dx.doi.org/10.1093/llc/fqi002,"{'string_id': '10.1093/llc/fqi002', 'id_scheme': 'DOI'}","In order to organize the widely dispersed manuscripts of Walt Whitman, The Walt Whitman Archive, in partnership with the University of Nebraska-Lincoln Libraries, has utilized the power of Encoded Archival Description (EAD) to create a single, scholarly enhanced guide to Whitman's poetry manuscripts. This integrated finding guide to Whitman's poetry manuscripts includes item-level description, links to repository guides that provide both location information and collection context, links to digital images of the manuscripts, and links to Text Encoding Initiative (TEI) transcriptions. In creating such a guide, we had to work cooperatively across disciplines and institutions, expand the use of EAD, and address how best to integrate description and transcription (EAD and TEI files). This essay describes our procedure as we created the integrated guide. From collecting finding aids and creating partnerships with other institutions, to developing a proper encoding standard and establishing good cross-department working relations, our project has embodied many of the benefits and challenges of digital work in the humanities. By identifying our procedures, and by laying out our future hurdles, we hope we can advance knowledge about Whitman and about how scholars and archivists can collaborate effectively to advance research, improve access, and realize the potential of EAD.",Ordering Chaos: An Integrated Guide and Online Archive of Walt Whitman's Poetry Manuscripts,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
191,http://dx.doi.org/10.1093/llc/fqi014,"{'string_id': '10.1093/llc/fqi014', 'id_scheme': 'DOI'}","Body image—especially self-perceptions of body boundaries—can have a significant impact on emotional well-being, personality, and behaviour. Fisher and Cleveland developed a scoring system for identifying two categories of body boundary imagery (Barrier and Penetration) in Rorschach test protocols, which Newbold has since extended to the analysis of narrative text. This paper describes the initial development of a content analysis dictionary (the Body Type Dictionary) for automating Barrier and Penetration scoring on English-language texts. To demonstrate its use and to provide a preliminary measure of validation, the dictionary is applied to a set of fictional fetish narratives and to samples from mainstream romantic fiction. The results demonstrate that the fetish narratives contain a significantly greater amount of Barrier imagery than the mainstream writing samples, which tallies with previous observations about body boundaries and appears to support the claim that writers with uncertain self-perceived boundaries will use more body boundary imagery in their writing. Suggestions for further validation studies and applications are given.",Development and Application of a Content Analysis Dictionary for Body Boundary Research,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
192,http://dx.doi.org/10.1093/llc/fqi021,"{'string_id': '10.1093/llc/fqi021', 'id_scheme': 'DOI'}","This paper explores the Greek participle and its use in the works of Lysias. I will argue that in Lysias' works, narrative descriptions of violence are characterized by the unusually frequent use of the participle. I will further show that the association of high participle density and narratives about violence are a subset of a larger pattern relating to use of the participle in Lysias' works. In this pattern, Lysias uses unusually large numbers of participles: (1) only within the narrative and argumentative sections of the speeches; (2) to structure the work and mark the conclusion of narrative arcs and lines of argument; (3) in their role as a structuring device, these passages also provide immediacy and momentum to the argument or narrative descriptions of events; and (4) to mark a return in subject matter to the case at hand and to focus the attention of the jury on the question that is before them.",Talking About Violence: Clustered Participles in the Speeches of Lysias,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
193,http://dx.doi.org/10.1093/llc/fqi028,"{'string_id': '10.1093/llc/fqi028', 'id_scheme': 'DOI'}","This article is concerned with the study of register variation, the process of focusing on the similarities and dissimilarities between register categories in terms of various linguistic phenomena. The British National Corpus World Edition, which is a 100 million word collection of British English, will be used to study the characterization of register variation by identifying their linguistic characteristics. By means of multivariate analysis, the variation of the occurrence of selected linguistic features among registers will be classified. A multivariate analysis holds out the promise of being able to systematize the register categories in the corpus while also revealing the characteristic linguistic features of the groups classified.In this article, by focusing on a sociolinguistic variable which is fairly systematically associated with ‘social class’ in the British National Corpus, the dimensions revealed by the multivariate analysis were interpreted linguistically. That is, the linguistic dimension concerned with ‘formal style’ versus ‘casual style’ proved the validity of the social variable in the British National Corpus and enabled its characterization in the light of linguistic features. Furthermore, several words which pertain to interjection, filler, modal auxiliary verb, and negation, i.e. hmm, ay, may, ’d, not, nae, and so on turned out to be crucial markers to characterize the register in which texts are used.",A Study of Register Variation in the British National Corpus,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
194,http://dx.doi.org/10.1093/llc/fqi029,"{'string_id': '10.1093/llc/fqi029', 'id_scheme': 'DOI'}","This article studies different aspects of a new approach to word sense disambiguation using statistical information gained from a monolingual corpus of the target language. Here, the source language is English and the target is Persian, and the disambiguation method can be directly applied in the system of English-to-Persian machine translation for solving lexical ambiguity problems in this system. Unlike other disambiguation approaches, using corpora for handling the problem, which use the Most Likelihood Model in their statistical works, this article proposes the Random Numbers Model. We believe that this model is more reasonable from the scientific point of view and find that it offers the most precise and accurate results. This method has been tested for a selected set of English texts containing multiple-meaning words with respect to Persian language and the results are encouraging.",Word Sense Disambiguation Using Target Language Corpus in a Machine Translation System,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
195,http://dx.doi.org/10.1093/llc/fqi030,"{'string_id': '10.1093/llc/fqi030', 'id_scheme': 'DOI'}","Accurate transcription is difficult and time-consuming. It is therefore worth choosing the transcription strategy that will yield the smallest number of errors for a given total effort. We use a mathematical model of the transcription process to compare two basic strategies: a single transcription with repeated checking, and a pair of transcriptions with a smaller amount of checking. Our model for checking is an adequate description of the rate of error detection in a real transcription. We show how to optimize the proportion of effort allocated to checking locations where the two transcriptions disagree, and discuss the factors that favour either of the strategies. We suggest how one might design an optimal transcription strategy in practice.",Optimal Strategies for Accurate Transcription,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
196,http://dx.doi.org/10.1093/llc/fqi031,"{'string_id': '10.1093/llc/fqi031', 'id_scheme': 'DOI'}","Charters are of crucial importance as a source when studying the history of the Middle Ages and the early modern era. There are a number of projects aiming at making charters available in digital format. All of these projects, as far as they use XML as the technological basis for their work, were represented at a conference in April 2004. It was found that, despite some structural differences in the individual approaches, there is considerable potential for integration as regards the individual charter as a legally relevant text. This potential can be tapped by using common standards for the tagging/encoding of charters. This article presents a proposal for such a standard. The proposed standard follows the existing TEI standard, but also contains a number of significant enhancements. The article also looks at the problems caused by overlapping structures and presents an architecture for a search engine that could help to bring together charters from all over Europe.",Towards a Standard of Encoding Medieval Charters with XML,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
197,http://dx.doi.org/10.1093/llc/fqi032,"{'string_id': '10.1093/llc/fqi032', 'id_scheme': 'DOI'}","The article studies the interaction between technical choices in virtual reality (VR) models and approaches to cultural history, for teaching and research. It is based on an ongoing project undertaken by ‘Cultures Anglophones et Technologies de l'Information’ at Paris-Sorbonne: ‘Montmartre in the jazz age’, a model of Montmartre in the late 1920s, when African-American musicians brought jazz to cabarets. Integrating it into the curriculum modifies the research/teaching interaction: it involves students in projects in progress; it encourages them to undertake interdisciplinary studies, involving computing skills and research in topography or music history. The issues that arise in the authoring of the project are a contribution to the problematics of cultural history; modelling the Montmartre environment from early documents (maps, photographs, which have to be edited with specialized software) shows how relative and incomplete they are—and thus how relative and composite our own modelling is. This composite character is in keeping with the ‘spirit of place’ of a multicultural area; VR is an elaborate structure, starting from a 3D model, with interior and exterior views, wireframes and textures; the medium is suited to the message: the recreation of a complex environment of spaces and subspaces throwing into focus contact points between French traditional songs and American jazz. The VR projects underline issues in the humanities; involving students increases their awareness of new problematics. The present article shows how new methods in humanities computing, in the field of visualization, alter and enrich humanities disciplines. It documents the authoring of a VR model, and the introduction of an Information Technology research project into the university teaching process: ‘Montmartre in the jazz age’, a multidisciplinary humanities adventure involving a digital reconstruction of part of this famous Paris area, the ‘Lapin Agile’ cabaret in the first instance. It argues that the involvement of research students in such projects not only gives them IT skills, but also adds to their critical understanding of humanities issues.",‘VR “Montmartre in the Jazz Age” ’: The Problematics of Virtual Reality in Researching and Teaching Multicultural History,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
198,http://dx.doi.org/10.1093/llc/fqi033,"{'string_id': '10.1093/llc/fqi033', 'id_scheme': 'DOI'}","In recent years, Latin America has been one of the world's fastest growing areas for Internet connectivity. While numerous studies have examined the factors contributing to this communications explosion, this article concentrates upon one of its effects—the proliferation of freely available, scholarly, peer-reviewed electronic journals in the fields of literary, cultural and area studies. This article argues that in the field of Latin American studies, the majority of e-journals are being produced in Latin American countries, rather than in the US or the UK for example. It is Latin American academics, rather than their US and UK counterparts, who are embracing new technologies and the opportunities facilitated for effective dissemination of research. In order to understand this marked move towards electronic scholarly journals, this article outlines the state of Internet connectivity in the region, the financial and material constraints and other restrictions placed upon academic publication, and the lack of international visibility of Latin American scholarly print journals. While questions need to be addressed as to the future sustainability and preservation of these free journals, many of them managed by individual academics and funded by their universities, this article argues that electronic publishing offers Latin American academics an unprecedented opportunity to disseminate their research. Furthermore, this model gives international academics immediate, free access to important research that is emerging from the continent, which is the subject of study. Such access has the potential to revolutionize the way that international academics approach Latin American studies and to encourage a greater degree of international academic debate.",E-Journal Proliferation in Emerging Economies: The Case of Latin America,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
199,http://dx.doi.org/10.1093/llc/fqi034,"{'string_id': '10.1093/llc/fqi034', 'id_scheme': 'DOI'}","The paper presents the outcome of the pilot phase of a major project which aims to build a digital resource for the study of historical Chinese texts with a view to facilitating linguistic analysis of the language, particularly from a diachronic point of view. The approach to general problems for a diachronic corpus is discussed. Details of the tag set and the tagging system devised are given. The development of a sophisticated automatic mark-up scheme for Chinese texts from widely different time periods and genres is indicated.",Sheffield Corpus of Chinese for Diachronic Linguistic Study1,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
200,http://dx.doi.org/10.1093/llc/fqi035,"{'string_id': '10.1093/llc/fqi035', 'id_scheme': 'DOI'}","This article discusses the teaching and learning that takes place during the project work performed by final year students on a B.A. Humanities with Applied Computing degree course. This is perhaps the first time that the student has had to tackle a large piece of independent research work. It marks a period of transition between the much more constrained and often unavoidably artificial exercises of earlier undergraduate years and the ill-defined and open-ended problems of the real world. As such it acts as a valuable preparation for the independent analytical thinking required in every profession. The pedagogy of the project work, methods of assessment and details of the support offered to students are explored. The article also discusses the students' experiences of research processes in the humanities and humanities computing. It is illustrated with two case study projects. The experiences of students and staff during the projects raise many issues concerning humanities computing as a field of study and are used as a starting point for exploring the nature of the discipline.","Teaching, Learning and Research in Final Year Humanities Computing Student Projects",Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
201,http://dx.doi.org/10.1093/llc/fqi036,"{'string_id': '10.1093/llc/fqi036', 'id_scheme': 'DOI'}","Although there has been an increased uptake of technology-based outputs and methods in humanities research, substantial barriers remain to the full realization of potential benefits in this area. Some of these barriers resonate with the most basic challenges defined by the field of Human–Computer Interaction, including the move from computational to faciliatory computing, non-user-centred industrial design of research tools, the application of inappropriate experience models by users, and a cultural mismatch between researchers and technicians. This article develops a model of digital development in the humanities as a ‘value chain’, along which critical gaps appear when the only actors in the chain are the researcher and the technical support staff. Having identified these gaps, the article then goes on to suggest ways in which a dedicated Digital Humanities Intermediary (DHI) is able to provide much needed skills and support of a non-specifically technical nature at critical moments in the projects' development. This kind of intermediary can be instrumental in increasing technology use among humanities research staff, providing a wide range of technical knowledge and other support functions to the relevant faculties in a form that fits well with their own language and preferred ways of working. The success of the position is dependant on solid networks and the enlightened support of the University, however, as it is still very much at the stage of reacting and capitalizing on opportunities as they are encountered, rather than simply slotting in to a stable, static, proven structure.",The Role of the Professional Intermediary in Expanding the Humanities Computing Base,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
202,http://dx.doi.org/10.1093/llc/fqi037,"{'string_id': '10.1093/llc/fqi037', 'id_scheme': 'DOI'}","Semantic Web applications in the humanities that visualize knowledge are still few and far between. The Visual Contextualization of Digital Content (VICODI) project brought together Semantic Web technologies with the concepts of contextualization and visualization of knowledge, an approach which we term visual contextualization. The goal was to enhance users' understanding of digital content in the domain of history. It succeeded in doing this by creating an ontology-based web portal of European history where extra historical knowledge or ‘context’ is added to resources and visualized through textual hyperlinks and interactive Scalable Vector Graphics historical maps. VICODI also created a history-specific ontology. In this article the novel approach of visual contextualization is introduced in conjunction with a detailed explanation of the core elements of the VICODI portal. The article also addresses several of the problems encountered in developing a Semantic Web application for a humanities domain.",Applying the Semantic Web: The VICODI Experience in Creating Visual Contextualization for History,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
203,http://dx.doi.org/10.1093/llc/fqi039,"{'string_id': '10.1093/llc/fqi039', 'id_scheme': 'DOI'}","In this article we describe an approach to the identification of ‘translationese’ based on monolingual comparable corpora and machine learning techniques for text categorization. The article reports on experiments in which support vector machines (SVMs) are employed to recognize translated text in a corpus of Italian articles from the geopolitical domain. An ensemble of SVMs reaches 86.7% accuracy with 89.3% precision and 83.3% recall on this task. A preliminary analysis of the features used by the SVMs suggests that the distribution of function words and morphosyntactic categories in general, and personal pronouns and adverbs in particular, are among the cues used by the SVMs to perform the discrimination task. A follow-up experiment shows that the performance attained by SVMs is well above the average performance of ten human subjects, including five professional translators, on the same task. Our results offer solid evidence supporting the translationese hypothesis, and our method seems to have promising applications in translation studies and in quantitative style analysis in general. Implications for the machine learning/text categorization community are equally important, both because this is a novel application and especially because we provide explicit evidence that a relatively knowledge-poor machine learning algorithm can outperform human beings in a text classification task.",A New Approach to the Study of Translationese: Machine-learning the Difference between Original and Translated Text,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
204,http://dx.doi.org/10.1093/llc/fqi040,"{'string_id': '10.1093/llc/fqi040', 'id_scheme': 'DOI'}","Existing software for handling textual variants suffers from a number of faults, and is generally designed for a narrow range of text types. This paper develops a new data structure for variants, suitable for a wider range of texts, which also solves most of the problems associated with the representation of variant data. A prototype applet, which can graphically display the new data structure is described, as also the current state of the editor being developed from it.",Graphical Editor for Manuscripts,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
205,http://dx.doi.org/10.1093/llc/fqi041,"{'string_id': '10.1093/llc/fqi041', 'id_scheme': 'DOI'}","This paper presents the results of a statistical analysis performed on the forty-seven novels of Anthony Trollope. It is shown that there is a trend in the distribution of chapter lengths for each novel towards a state of increased evenness. Some possible causes of the trend are considered, although the most probable explanation is conjectural.",The Evolution of Order in the Chapter Lengths of Trollope's Novels,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
206,http://dx.doi.org/10.1093/llc/fqi042,"{'string_id': '10.1093/llc/fqi042', 'id_scheme': 'DOI'}","This paper is a case study for examining how a small-corpus-based approach can contribute to research in stylistics. Specifically, we have built small corpora of the two Alice books and retrieved, using WordSmith Tools suite, first, verbs of saying and their adverbials to elucidate how Alice speaks to others in the stories, and secondly, modifiers of ‘Alice’ to get the images of the main character. An analysis of these data reveals that Alice's role in each book is quite distinct: an unexpected visitor thrown into the passive state in Wonderland and an active explorer in Looking-Glass. These findings objectively serve to reinforce our argument over what Alice is called through the perusal of the texts. Alice's roles in the two books are thus interactively supported by the small-corpus-based approach and the non-corpus-based approach, which may explore the validity of the interfaced approach, the collaborative work of quantitative processing and qualitative speculation.",A Small-Corpus-Based Approach to Alice's Roles,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
207,http://dx.doi.org/10.1093/llc/fqi044,"{'string_id': '10.1093/llc/fqi044', 'id_scheme': 'DOI'}","Recently prominent readers in Shakespeare have embraced Warren B. Austin's 1969 computer-based study which concludes that Henry Chettle wrote Greene's Groatsworth of Wit. However, Austin's study is flawed primarily because Austin excludes hosts of data related to his conclusion, while also misreading the data that continuously point to Robert Greene as Groatsworth's author. Austin studies just five of Greene's thirty-two known prose works and rules out studying many of Greene's words on the sole basis of their subject matter, words like repent that connect Greene to the writing of Groatsworth. Austin is also silent about Chettle's stated role as copyist and overseer in preparing and printing Groatsworth. Prominent in this discussion are the six ‘Greene plus’ words Austin identifies, but does not analyse, that appear often in Groatsworth and Greene's other prose writings, but never in Chettle. Especially important are the forty-one rare and unique words presented here that Austin excludes from his study and which constitute direct evidence of Greene's hand in writing the complete text of Groatsworth. Nor does Austin study the orthography of Groatsworth, which differs significantly from Chettle's Kind Harts Dreame and suggests different authors for each work. Austin's findings should, therefore, be set aside, while renewed consideration is given to the lexical and orthographical evidence presented in this article that continues to identify Greene as Groatsworth's author, that is, as someone familiar enough with Shakespeare's early theatre practices to criticize them.",Computing Error: Reassessing Austin's Study of Groatsworth of Wit,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
208,http://dx.doi.org/10.1093/llc/fqi045,"{'string_id': '10.1093/llc/fqi045', 'id_scheme': 'DOI'}","Digital-only subscription is increasingly popular as a means of journal and book delivery among our major libraries. The advantages of digital delivery are apparent, but unlike traditional publications, digital subscriptions are commonly not housed within national boundaries. With an increasingly large proportion of book and journal subscriptions being digital only, this presents an as yet unquantified risk to the collections of the major research and state libraries. At present very little attention is directed to the continuity of access to increasingly important research resources through periods of economic, social or military instability. This is typical of long-term resource management on the Internet. A model for managing the risks associated with these new directions must address both business risks of digital collection continuity and systems issues of content discovery, sharing and reuse. Escrow contracts are an established method to guarantee continuity of business when licensing business-critical software applications. The article examines low cost community driven resource sharing networks (the GratisNe case study) and new approaches to content syndication. A case for the establishment of a digital escrow database at the community level is presented with an architecture that embraces both the business and systems issues of long-term management of the digital resource supply.",Systematic Approaches to Long Term Digital Collection Management,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
209,http://dx.doi.org/10.1093/llc/fqi046,"{'string_id': '10.1093/llc/fqi046', 'id_scheme': 'DOI'}","This article focuses on the conceptual issues faced by scholarly editors and textual studies specialists. Theoretical debate in this general field is still active as digital texts present special problems and magnify others. Older theory and methodology are hampered by unacknowledged, sometimes inappropriate cultural values and other limitations, and are not always useful in connection with digital texts. Nevertheless, the distinction between the abstract work and its concrete expression is influential both within and outside the field. In this approach, the concept of authenticity relates to the degree of change a work undergoes or the accuracy of the ‘instructions’ for its reconstitution. Whether the digital text is best thought of as immaterial or material is not as crucial as might first appear. The way a digital text is made visible is important, though potentially paradoxical. In order to be workable, the concept of authentication by instructions needs further technical assistance, like that provided by the Just-in-Time Markup System. But, despite its limitations, traditional textual scholarship still has much to offer textual studies in digital environments.","Original, Authentic, Copy: Conceptual Issues in Digital Texts",Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
210,http://dx.doi.org/10.1093/llc/fqi047,"{'string_id': '10.1093/llc/fqi047', 'id_scheme': 'DOI'}","Accuracy of transcription is vital when preparing a scholarly version of an existing document. This process has not changed with the advent of electronic editions. In fact, ensuring the continued accuracy of a transcription in the digital realm is more difficult because a file, unlike a piece of paper, does not retain information about its previous states and it is therefore possible that accidental changes can go undetected unless the content is continually checked against the original.This article presents a new, character-set-independent, programming algorithm that allows for the ongoing authentication of the textual content of files being marked up with SGML-like languages. The study also describes an implementation of this algorithm and how it can be used with existing software tools to provide a more efficient and trusted editing environment for creating and editing marked-up files.The Just In Time Authentication Mechanism (JITAM) algorithm was developed in response to the need for some form of automated authentication mechanism for projects already employing embedded markup and is seen as a preparatory step that editors can take with their projects before making the leap to the more versatile Just In Time Markup (JITM) system.",A New Technique for Authenticating Content in Evolving Marked-up Documents,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,22,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
211,http://dx.doi.org/10.1093/llc/fqi048,"{'string_id': '10.1093/llc/fqi048', 'id_scheme': 'DOI'}","Linguistics and musicology, along with other fieldwork-based disciplines, have obligations to facilitate access to research results by the communities whose cultural heritage is recorded and analysed, especially when the languages and musics in question are otherwise little documented, have few speakers or performers, and are threatened by the global dominance of English. This article presents the early results of our planning for establishment of a digital resource to preserve and make accessible recordings and other documentation of Murrinh-patha public dance-songs at Wadeye, a remote Indigenous community in Australia's Northern Territory. With the recent establishment of the Wadeye Knowledge Centre, copies of recordings previously left in the community by researchers have been digitized and made available through computer workstations. Many of these digitized recordings, however, have poor or no documentation and thus are difficult to locate and access. The most urgent task is to work with elderly performers and composers to assemble metadata about the oldest recordings of songs and who composed and performed them. In order to maximise local accessibility and use, both elders and young people will be involved in planning and creation of a bilingual search interface to the collection. Planning must also consider sustainability issues through integration with other local initiatives, appropriate use of open standards and formats, locally sustainable technical platforms, and regular backup and maintenance.","Communities of Interest: Issues in Establishing a Digital Resource on Murrinh-patha song at Wadeye (Port Keats), NT",Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
212,http://dx.doi.org/10.1093/llc/fqi049,"{'string_id': '10.1093/llc/fqi049', 'id_scheme': 'DOI'}","Imitative texts of high quality are of some importance to students of attribution, especially those who use computational methods. The authorship of such texts is always likely to be difficult to demonstrate. In some cases, the identity of the author is a question of interest to literary scholars. Even when that is not so, students of attribution face a challenge. If we cannot distinguish between original and imitation in such cases, we must always concede that an imitator may have been at work. Shamela (1741) has always been regarded as a brilliant parody. When it is subjected to our standard common-words tests of authorship, it yields mixed results. A new procedure, in which special word-lists are established according to a predetermined set of rules, proves more effective. It needs, however, to be tried in other cases.",Who wrote Shamela? Verifying the Authorship of a Parodic Text,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
213,http://dx.doi.org/10.1093/llc/fqi050,"{'string_id': '10.1093/llc/fqi050', 'id_scheme': 'DOI'}","This essay emerges from the recent debates in editorial theory and, on the practical level, from a project for producing electronic scholarly editions. It reflects on the nature of text, explores the implications for text encoding in relation to recent debate, and outlines a methodology using stand-off markup within which text encoding can respond to the theoretically enunciated problems.","Text-encoding, Theories of the Text, and the ‘Work-Site’",Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
214,http://dx.doi.org/10.1093/llc/fqi051,"{'string_id': '10.1093/llc/fqi051', 'id_scheme': 'DOI'}","Te Ara, the Encyclopedia of New Zealand was launched in 2005, funded by the New Zealand government. It was designed from the beginning to be published online, perhaps the first national encyclopedia to be born digital. It complements the works of M. McKinnon (1997, The New Zealand Historical Atlas. Auckland: David Bateman Ltd.) and W. H. Oliver and C. Orange (1990–2000, The Dictionary of New Zealand Biography, five volumes. Wellington: Allen & Unwin and Department of Internal Affairs). The encyclopedia is designed to be the first port of call for reliable information about New Zealand and a gateway to the country's natural and cultural treasures. Its digital form allows searchability, linking to other sites, cumulative publication (one ‘theme’ a year), regular updating, and a parallel publication of a digitized 1966 encyclopedia, as well as multimedia content and interactive aspects such as user-defined graphs. In addition, it allows material to be layered for different audiences, from younger school children to scholars and researchers, and with sections in Maori as well as English. Eventually, as attitudes change and new discoveries are made, some of the earliest entries will need rewriting. The encyclopedia will mutate and be in a constant state of revision and self-criticism. The online encyclopedia of a nation is a project full of promise, which it is hoped others will soon emulate.","Planning an Online Encyclopedia of a Nation: The Example of Te Ara, the Encyclopedia of New Zealand",Oxford University Press (OUP),2005,,Literary and Linguistic Computing,20,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
215,http://dx.doi.org/10.1093/llc/fqi062,"{'string_id': '10.1093/llc/fqi062', 'id_scheme': 'DOI'}","Stemming from the precomputer era, respectable historical dictionaries like those originating from the 19th century, often need considerable adaptation to make their wealth of linguistic material as fully exploitable as digital dictionaries of modern design do. With a view to increasing retrieval possibilities, two combined improvements will be given particular attention here, viz. uniformization of both source references and dates, mainly accompanying the numerous quotations which serve as evidence in lexicographical articles, and the insertion of new dates, wherever still missing. In this article, we shall study the case of the Dutch Woordenboek der Nederlandsche Taal, where strategies to cope with the problem may be illustrated well. The focus of the article is on how regular expressions, in the form of prefabricated templates and scripts, allow to bring about a gradual but effective transformation.",Computerized Restoration of Historical Dictionaries: Uniformization and Date-assigning in Dictionary Quotations of the Woordenboek der Nederlandsche Taal,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
216,http://dx.doi.org/10.1093/llc/fqi063,"{'string_id': '10.1093/llc/fqi063', 'id_scheme': 'DOI'}","How to measure proximities and oppositions in large text corpora? Intertextual distance provides a simple and interesting solution. Its properties make it a good tool for text classification, and especially for tree-analysis which is fully presented and discussed here. In order to measure the quality of this classification, two indices are proposed. The method presented provides an accurate tool for literary studies—as is demonstrated by applying it to two areas of French literature, Racine's tragedies and an authorship attribution experiment.",A Tool for Literary Studies: Intertextual Distance and Tree Classification,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
217,http://dx.doi.org/10.1093/llc/fqi064,"{'string_id': '10.1093/llc/fqi064', 'id_scheme': 'DOI'}","The emergence of new media technologies and their integration into the creative process has led to an explosion of hybrid works whose complexities (both conceptual and material) challenge received ideas about the nature of art and its relationship with the future. ‘Variable media’ has been coined as a descriptor for creative projects incorporating elements whose viability within future incarnations of the same works may be compromised. The flexible and imaginative approach taken by the innovative new media arts community has much to offer information professionals facing the conservation challenges presented by digital materials. By the same token, the potential application of principles emerging in the field of digital preservation extends well beyond digital resources to encompass works of art and creative enterprises whose material constitution paradoxically threatens them with extinction. The paramount role played by ‘process’ (as opposed to ‘object’) has led us to explore various metadata development initiatives, including an extensive online questionnaire developed by the Variable Media Initiative at the Guggenheim Museum in New York, and the IFLA Requirements of Bibliographic Standards, with a view to combining elements of each and developing a new model to accommodate the complex documentation of the life cycles of new media artworks and of digital objects.",Cryogenics and Creativity: The Frankenstein Factor in Cultural Preservation,Oxford University Press (OUP),2005,,Literary and Linguistic Computing,21,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
218,http://dx.doi.org/10.1093/llc/fqi067,"{'string_id': '10.1093/llc/fqi067', 'id_scheme': 'DOI'}","This article describes the operation of two new tests of authorship and offers some results. Both tests rely on controlled contrasts of word-frequency and both exclude the very common words, which have been put to such good use in recent years. One test treats of words used with some consistency by a target-author but more sporadically by others. The second treats of words used sporadically by the target-author but not by most others. (The inclusion of words that some other authors use avoids the strict constraint that has impoverished this form of evidence.) In suitable cases, both tests prove very accurate. The fact that evidence of authorship can be detected in these three distinct frequency-strata helps to explain why such tests should work at all and so encourages the development of even better ones.",All the Way Through: Testing for Authorship in Different Frequency Strata,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,22,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
219,http://dx.doi.org/10.1093/llc/fql014,"{'string_id': '10.1093/llc/fql014', 'id_scheme': 'DOI'}","This study investigated the automatic modelling of space and time in narratives involving dining in a restaurant. We built a program that (1) uses information extraction techniques to convert narrative texts into templates containing key information about the dining episodes discussed in the narratives, (2) constructs commonsense reasoning problems from the templates, (3) uses commonsense reasoning and a commonsense knowledge base to build models of the dining episodes, and (4) generates and answers questions by consulting the models. We describe the program and present the results of running it on a corpus of web texts and American literature.",Modelling Space and Time in Narratives about Restaurants,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,22,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
220,http://dx.doi.org/10.1093/llc/fql015,"{'string_id': '10.1093/llc/fql015', 'id_scheme': 'DOI'}","This article is based on a session given by the authors at the ACH/ALLC conference at the University of Victoria in June 2005. It discusses the prospects for partnership between the humanities and computing from the alternative perspective afforded by Empirical Modelling (EM). Perceived dualities that separate the two cultures of science and art are identified as the primary impediment to this partnership. A vision for ‘human computing’ that promises to dissolve these dualities is outlined. The key characteristics and potential for EM for the humanities are illustrated with reference to a modelling exercise on the theme of Schubert's Erlkönig. This highlights how each of the six varieties of modelling identified by McCarty can be represented within an EM model. The implications of EM are discussed with reference to McCarty's account of the key role for modelling in the humanities, in relation to James's ‘philosophic attitude’ of Radical Empiricism and to ideas from phenomenological sources.",Human Computing--Modelling with Meaning,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
221,http://dx.doi.org/10.1093/llc/fql016,"{'string_id': '10.1093/llc/fql016', 'id_scheme': 'DOI'}",This article presents a disambiguation method which diminishes the functional combinations of the words of a sentence taking into account the context in which they appear. This process is built in two phases: the first phase is based on the local syntactic structures of the Spanish language and reaches an average yield of 87%. The second one is supported by syntactic tree representation and pushes the results up to an approximate high end of 96%. This process constitutes the starting point towards an automated syntactic analysis.,Functional Disambiguation Based on Syntactic Structures,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
222,http://dx.doi.org/10.1093/llc/fql017,"{'string_id': '10.1093/llc/fql017', 'id_scheme': 'DOI'}","The text ontology debates inspired by the descriptive encoding practices of the Text Encoding Initiative (TEI) community have been conducted between literary theorists concerned with the adequacy of such encoding to capture the interpretative and playful aspects of literary appreciation on the one hand, and those who regard encoding as one of the formal sciences and seek greater disambiguation in the interests of more efficient machine processing. We argue for a practice-oriented view that has not been represented adequately by either of these poles. Our position has received unexpected support from the systematic realist philosophy of John Anderson which we encountered in digitizing his lecture notes held by the University of Sydney Archives. The process of encoding the lecture notes informed our understanding of the problems of encoding primary source materials, but Anderson's realism also located the space we sought to occupy in the TEI debates between the technical, formal model of encoding and the anti-realist preferences of many literary scholars. In this article, we argue on the basis of our reflections the need for further empirical studies of real world encoding practices as these new documentary forms are integrated into existing institutional and informational processes.",In the Philosophy Room: Australian Realism and Digital Content Development,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
223,http://dx.doi.org/10.1093/llc/fql018,"{'string_id': '10.1093/llc/fql018', 'id_scheme': 'DOI'}","PériCulture is the name of a research project at the Université de Montréal which is part of a larger project based at the Université de Sherbrooke. The parent project aimed to form a research network for managing Canadian digital cultural content. The general research objective of PériCulture was to study indexing methods for web-based non-textual cultural content, specifically still images. The research results reported here build on work in image indexing and automatic (text) indexing by studying properties of text associated with images in a networked environment to try to gain some understanding of how the ancillary text associated with images on web pages can be exploited to index the corresponding images. We studied this question in the context of selected web sites, i.e. that contained multimedia objects, that had text associated with these objects (broader than file names and captions), that were bilingual (English and French), and that housed Canadian digital cultural content. We identified keywords that were useful in indexing and studied their proximity to the object described. Potential indexing terms were identified in various HTML tags and full text (each considered a different source of ancillary text). Our study found that a large number of useful indexing terms are available in the ancillary text of many web sites with cultural content, and that ancillary text of different sources have variable usefulness in retrieval. Our results suggest that these terms can be manipulated in a number of ways in automated retrieval systems to improve search results.",Using Ancillary Text to Index Web-based Multimedia Objects,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
224,http://dx.doi.org/10.1093/llc/fql019,"{'string_id': '10.1093/llc/fql019', 'id_scheme': 'DOI'}","Despite a century of research, statistical and computational methods for authorship attribution are neither reliable, well-regarded, widely used, or well-understood. This article presents a survey of the current state of the art as well as a framework for uniform and unified development of a tool to apply the state of the art, despite the wide variety of methods and techniques used. The usefulness of the framework is confirmed by the development of a tool using that framework that can be applied to authorship analysis by researchers without a computing specialization. Using this tool, it may be possible both to expand the pool of available researchers as well as to enhance the quality of the overall solutions [for example, by incorporating improved algorithms as discovered through empirical analysis (Juola, P. (2004a). Ad-hoc Authorship Attribution Competition. In Proceedings 2004 Joint International Conference of the Association for Literary and Linguistic Computing and the Association for Computers and the Humanities (ALLC/ACH 2004), Göteborg, Sweden)].",A Prototype for Authorship Attribution Studies,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
225,http://dx.doi.org/10.1093/llc/fql020,"{'string_id': '10.1093/llc/fql020', 'id_scheme': 'DOI'}","In this article, we describe our interdisciplinary project ‘Rule-based search in text databases with nonstandard orthography (RSNSR)’ in support of the conservation of cultural heritage, especially for the German reception of the philosopher Nietzsche. We present a rule-based fuzzy search engine that allows users to retrieve text data independently of its orthographical realization. The rules used are derived from statistical analyses, historical publications, linguistic principles, and expert knowledge. Our Web-based tool is intended for experts as well as interested amateurs. Along with its present features, further functions are currently worked out. Among them are automatic rule derivation and finer result classification through a generalized Levenshtein similarity measure. Our work is associated with the recently launched project Deutsch Diachron Digital (DDD) to build a complete diachronic corpus of German for the first time with texts from the ninth century (Old High German) to the present (Modern German).",Rule-based Search in Text Databases with Nonstandard Orthography,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
226,http://dx.doi.org/10.1093/llc/fql021,"{'string_id': '10.1093/llc/fql021', 'id_scheme': 'DOI'}","We present our experience in developing an on-line infrastructure to support collaborative analysis of text, which we distinguish from existing, well-explored efforts to create annotative electronic editions. Using Faulkner's The Sound and the Fury as our primary text case, we outline the features and rationale of our collaborative framework, called Callimachus. We present our findings concerning that text and explore how these findings only became possible after breaking with the received wisdom concerning the application of XML and the Text Encoding Initiative to such analytical projects.",Callimachus—Avoiding the Pitfalls of XML for Collaborative Text Analysis,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
227,http://dx.doi.org/10.1093/llc/fql022,"{'string_id': '10.1093/llc/fql022', 'id_scheme': 'DOI'}","Humanities Computing is an emergent field. The activities described as ‘Humanities Computing’ continue to expand in number and sophistication, yet no concrete definition of the field exists, and there are few academic departments that specialize in this area. Most introspection regarding the role, meaning, and focus of “Humanities Computing” has come from a practical and pragmatic perspective from scholars and educators within the field itself. This article provides an alternative, externalized, viewpoint of the focus of Humanities Computing, by analysing the discipline through its community, research, curriculum, teaching programmes, and the message they deliver, either consciously or unconsciously, about the scope of the discipline. It engages with Educational Theory to provide a means to analyse, measure, and define the field, and focuses specifically on the ACH/ALLC 2005 Conference to identify and analyse those who are involved with the humanities computing community.",Disciplined: Using Educational Studies to Analyse ‘Humanities Computing’,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
228,http://dx.doi.org/10.1093/llc/fql023,"{'string_id': '10.1093/llc/fql023', 'id_scheme': 'DOI'}","Ever since its initial publication four hundred years ago, thousands of editions, most often illustrated, have been published of Cervantes' masterpiece, Don Quixote. Imagery has become an integral part of the reception and interpretation of the text. To date, a comprehensive collection of these images, the textual iconography of the Quixote, has not been published. We report in this paper on overcoming two key obstacles: limitations on the availability of materials and limitations due to the technical and financial characteristics of print-based dissemination. Our digital iconography makes a rich artistic tradition accessible to readers for the first time, and reveals a wealth of information about the historical, cultural, and literary contexts into which the Quixote has been placed.","Visual Knowledge: Textual Iconography of the Quixote, a Hypertextual Archive",Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
229,http://dx.doi.org/10.1093/llc/fql031,"{'string_id': '10.1093/llc/fql031', 'id_scheme': 'DOI'}","This special edition of Literary and Linguistic Computing comprises a selection of the papers presented at the 17th Joint International Conference of the Association for Computers and the Humanities (ACH) and the Association for Literary and Linguistic Computing (ALLC), which took place at the University of Victoria, British Columbia (Canada), from 15–18 June 2005. The conference was a great success thanks to the efforts of the local organizing team headed by Peter Liddell and including Scott Gerrity, Stewart Arneil, Martin Holmes, Greg Newton, Judy Nazar and Ray Siemens. The academic programme was compiled by an international programme committee chaired by Alejandro Bia and comprising Julia Flanders, Neil Fraistat, Simon Horobin, Joseph Jones, Lisa Lena Opas-Hänninen, Concha Sanz-Miguel, Susan Schreibman and Michael Sperberg-McQueen.",Introduction,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
230,http://dx.doi.org/10.1093/llc/fql032,"{'string_id': '10.1093/llc/fql032', 'id_scheme': 'DOI'}","This article presents elements of a Correspondence Analysis (CA) approach to the measurement of linguistic distances in dialectology. It argues that both linguistic and spatial factors are part of an explanation of geolinguistic variation, and it shows how the exploratory and graphical properties of CA can contribute to such an explanation. The application is a study of the different realizations of the phoneme /r/ in Acadian French, a dialect spoken in Canada. Data are from the Atlas linguistique du vocabulaire maritime acadien and include over 5,000 tokens from eighteen localities. Using chi-square distances, the analysis results in a two-dimensional space that arranges the localities along continua. Linguistic interpretation of this space, based on those features of /r/ that are identified as accounting for the structuring of these continua, suggests a hierarchy of phonological processes—including alternation between apical and dorsal articulations (in French words), and replacement of the retroflex rhotic found in English-origin words by apical and dorsal variants. Two external spatial factors, local concentration of francophone speakers and spheres of activity, are shown to correlate with the linguistic distances among localities.",Geographic Variation in Acadian French /r /: What Can Correspondence Analysis Contribute Toward Explanation?,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
231,http://dx.doi.org/10.1093/llc/fql033,"{'string_id': '10.1093/llc/fql033', 'id_scheme': 'DOI'}","Aristotle long ago divided kinds of study into technē and epistēmē, which we can roughly translate into the modern terms ‘art’ and ‘science’. It is certainly the case that computational dialectologists do well with the Art (technē), in our technical construction and execution of statistical experiments, and we have two different prominent models to choose from, each one corresponding to a mode of scientific discovery, either to deductive or to inductive scientific procedure. But that in itself should not be the whole story. The Science (epistēmē) of computational dialectology lies in the creation of arguments from our statistical results that are appropriate to the scientific procedure that motivates us. It is not so clear that computational dialectologists have done so well with their Science. What do the results of the technical work really mean? In what way are they associated with particular choices of linguistic theory? Is it the case that, after all of our technical hard work, we find only what we are looking for? In this paper, I will suggest that an appropriate use of the technical results of computational dialectology requires that practitioners take a more subtle approach to the theory that motivates the study in the first place, especially to the relationship between perception and production of language.",Art and Science in Computational Dialectology,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
232,http://dx.doi.org/10.1093/llc/fql034,"{'string_id': '10.1093/llc/fql034', 'id_scheme': 'DOI'}","Dialectometric techniques analyze linguistic variation quantitatively, allowing one to aggregate over what are frequently rebarbative geographic patterns of individual linguistic variants, such as which word is used for a particular concept in a language area, or which sounds are used in particular words. This leads to general formulations of the relation between linguistic variation and explanatory factors. Dialectometric techniques are maturing continuously, paving the way to genuinely new opportunities for the explanation of linguistic variation. These include, most prominently, techniques for analyzing syntactic variation, techniques for comparing the relative importance of different individual linguistic variables, techniques for comparing the relative importance of linguistic levels such as pronunciation, vocabulary, and/or prosody, and many more. This article serves as an introduction to a special issue of Literary and Linguistic Computing devoted to presenting a new work constituting Progress in Dialectometry: Toward Explanation.",Progress in Dialectometry: Toward Explanation,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
233,http://dx.doi.org/10.1093/llc/fql036,"{'string_id': '10.1093/llc/fql036', 'id_scheme': 'DOI'}","The two West-Germanic languages Dutch and Afrikaans are so closely related that they can be expected to be mutually intelligible to a large extent. The present investigation focuses on written language. Comprehension was established by means of cloze tests on the basis of two newspaper articles. Results suggest that it is easier for Dutch subjects to understand written Afrikaans than it is for South African subjects to understand written Dutch. In order to explain the results, attitudes as well as several types of linguistic distances were assessed. The relations between attitude scales and intelligibility scores were few and weak. Asymmetries in the linguistic relationships between the two languages are probably more important, especially the asymmetries in the number of noncognates and the opacity of the relatedness of cognates. These asymmetries are caused by historical developments in Dutch and Afrikaans, with respect to the lexicon, grammar, and spelling.",Mutual Comprehensibility of Written Afrikaans and Dutch: Symmetrical or Asymmetrical?,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
234,http://dx.doi.org/10.1093/llc/fql037,"{'string_id': '10.1093/llc/fql037', 'id_scheme': 'DOI'}","Within the scope of the dialectometry project at Salzburg University, a windows application was developed that implements algorithms to support the dialectometric evaluation of altas data. Visual DialectoMetry (VDM) covers all the steps of the dialectometric evaluation procedure: management of preclassified atlas data, calculation of matrices, visualizations using different types of maps, and cluster analysis. This article assumes basic knowledge of VDMs features, but differs from the linguistic interpretations given in Goebl's article by concentrating on the application technology. VDM is written in C++ and uses the Microsoft Foundation Classes (MFC), a library that wraps the Windows application programming interface (API) and the JET library in C++ classes. It holds the data for every project in one MS ACCESS database file which is accessible via MFC. Users can choose from different projects—different linguistic atlases—or they may evaluate their own data with VDM after having converted and imported their data into the VDM database format. This article outlines the design of the VDM database as far as the storage of the data matrix is concerned. It discusses possibilities of performance optimization by distinguishing calculations that can be done in real time from calculations that have to be performed off-line (ahead of time). Despite the large number of geographical information systems (GIS) on the market the generation of thematic maps in real time is not a trivial task. The technical solution implemented in VDM generates maps from two layers: a Windows Meta File (WMF) and a view of polygon objects whose color is recalculated as response to user actions.","Database Design and Technical Solutions for the Management, Calculation, and Visualization of Dialect Mass Data",Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
235,http://dx.doi.org/10.1093/llc/fql038,"{'string_id': '10.1093/llc/fql038', 'id_scheme': 'DOI'}","In the period between 1999 and 2002, Jørn Almberg and Kristian Skarbø compiled a database which consists of recordings and phonetic transcriptions of translations of the fable ‘The North Wind and the Sun’ in about fifty Norwegian dialects. On the basis of fifteen of these recordings, Charlotte Gooskens carried out a perception experiment (Gooskens and Heeringa, 2004). In this experiment she investigated the distances between the fifteen dialects as perceived by the speakers themselves.On the basis of the phonetic transcriptions, Wilbert Heeringa (2004) measured computational linguistic distances between the fifteen Norwegian varieties (Gooskens and Heeringa, 2004). Distances were calculated by means of Levenshtein distance, which finds the minimum cost of changing one pronunciation into another by inserting, substituting or deleting phonetic segments. Gooskens and Heeringa (2004) correlated the perceptual distances with these computational distances and found a significant correlation of r = 0.67. In the computational distances, pronunciational, lexical, and morphological variation is processed, but these levels are not studied separately.The contribution of this article is that we measure pronunciational, lexical, and prosodic distances separately. Within pronunciational distances we distinguish between consonants and vowels on the one hand, and between substitutions and insertions/deletions on the other hand. When correlating the separate levels with perception and using multiple linear regression analyses we found that pronunciation is most important in perception and especially vowel substitutions play a major role.","The Relative Contribution of Pronunciational, Lexical, and Prosodic Differences to the Perceived Distances between Norwegian Dialects",Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
236,http://dx.doi.org/10.1093/llc/fql039,"{'string_id': '10.1093/llc/fql039', 'id_scheme': 'DOI'}","Previous studies of American English have identified a number of robust patterns involving the vowel system, such as the Northern Cities Chain Shift and the Southern Vowel Shift. These studies primarily employ methods which treat separately the phonetic properties of specific vowels as produced by individual speakers which are later assembled into complete vowel systems. While this provides a useful picture of production, it is not adequate for comparison with dialect perception studies, where interpretation of the results often requires some understanding of the correlations among linguistic features and between those features and individual talkers. We conducted a factor analysis of the duration and first and second formant frequencies of each of the fourteen vowels produced by forty-eight speakers representing six regional varieties of American English and both genders. The data were submitted to factor analysis using maximum likelihood estimation and Varimax rotation. Results confirmed significant correlations between regional dialect and acoustic–phonetic properties of the vowel systems, although these patterns are complicated by interactions with gender. These results illustrate the utility of factor analytic methods in examining systematic variation across an entire linguistic system such as the vowels.",North American English Vowels: A Factor-analytic Perspective,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
237,http://dx.doi.org/10.1093/llc/fql040,"{'string_id': '10.1093/llc/fql040', 'id_scheme': 'DOI'}","Since the early studies by Sokal (1988) and Cavalli-Sforza et al. (1989), there has been an increasing interest in depicting the history of human migrations by comparing genetic and linguistic differences that mirror different aspects of human history. Most of the literature concerns continental or macroregional patterns of variation, while regional and microregional scales were investigated less successfully. In this article we concentrate on the Netherlands, an area of only 40,000 km2.The focus of the article is on the analysis of surnames, which have been proven to be reliable genetic markers since in patrilineal systems they are transmitted—virtually unchanged—along generations, similar to a genetic locus on the Y-chromosome. We shall compare their distribution to that of dialect pronunciations, which are clearly culturally transmitted (children learn one of the linguistic varieties they are exposed to, normally that of their peers in the same area or that of their families). Since surnames, at the time of their introduction, were words subject to the same linguistic processes that otherwise result in dialect differences, one might expect the distribution of surnames to be correlated with dialect pronunciation differences. But we shall argue that once the collinear effects of geography on both genetics and cultural transmission are taken into account, there is in fact no statistically significant association between the two. We show that surnames cannot be taken as a proxy for dialect variation, even though they can be safely used as a proxy to Y-chromosome genetic variation.We work primarily with regression analyses, which show that both surname and dialect variation are strongly correlated with geographic distance. In view of this strong correlation, we focus on the residuals of the regression, which seeks to explain genetic and linguistic variation on the basis of geography (where geographic distance is the independent variable, and surname diversity or linguistic diversity is the dependent variable). We then seek a more detailed portrait of the geographic patterns of variation by identifying the ‘barriers’ (namely the areas where the residuals are greatest) by applying the Monmonier algorithm.We find the results historically and geographically insightful, hopefully leading to a deeper understanding of the role of the local migrations and cultural diffusion that are responsible for surname and dialect diversity.",To What Extent are Surnames Words? Comparing Geographic Patterns of Surname and Dialect Variation in the Netherlands,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
238,http://dx.doi.org/10.1093/llc/fql041,"{'string_id': '10.1093/llc/fql041', 'id_scheme': 'DOI'}","Dialectometric techniques for analyzing variation in the aggregate are maturing rapidly, but there is still little agreement on how to extract linguistic structure from aggregate comparison. The present study explores one means of comparing aggregate analyses in order to determine linguistically concise characterizations of restrictions, essentially the use of factor analysis (FA). Using the Southern states data which Guy Lowman collected as part of LAMSAS, we apply FA to the vowels involved in aggregate analyses in order to determine which alternations in pronunciation tend most to co-occur.",Identifying Linguistic Structure in Aggregate Comparison,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
239,http://dx.doi.org/10.1093/llc/fql042,"{'string_id': '10.1093/llc/fql042', 'id_scheme': 'DOI'}","This paper documents the many taxometric and cartographic achievements of the Salzburg school of dialectometry. The paper discusses the following topics: (1) problems of measurement of linguistic atlas data (with particular consideration of Romance linguistic atlases), (2) establishment of the data matrix, (3) choice of the similarity index (Relative and Weighted Identity Value), (4) generation of the respective similarity and distance matrices, (5) their subsequent cartographic exploitation, which encompasses the following cartographic tools: similarity maps, parameter maps, dendrograms (and their spatial projection), and correlation maps. The ultimate purpose of these highly sophisticated cartographic techniques (choropleth and isopleth maps) is to increase our knowledge of the complex mechanisms of the dialectal management of space by man. From a methodological point of view our paper deals with problems related to (Romance) dialectology and linguistic geography, historical linguistics, numerical classification, statistics and statistical cartography. The examples are drawn from the French linguistic atlas ALF (Atlas linguistique de la France) published by Jules Gilliéron and Edmond Edmont (Paris: Champion, 1902–1910, 10 volumes) more than one hundred years ago. The taxometric calculations and their respective visualizations are realized by a powerful computer program called ‘Visual DialectoMetry’ (VDM), created by Edgar Haimerl (Blaustein, Germany) between 1997 and 2000 in Salzburg, which is freely available for research purposes.",Recent Advances in Salzburg Dialectometry,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
240,http://dx.doi.org/10.1093/llc/fql043,"{'string_id': '10.1093/llc/fql043', 'id_scheme': 'DOI'}",This research applies dialectometric methods to purely syntactic dialect data. It will be shown that there is geographic cohesion in syntactic variation when viewed in the aggregate. The amount of syntactic variation which can be accounted for by geography will be determined. Dialectometric techniques will be used to develop an additive measure of syntactic differences. Multidimensional scaling will be applied to visualise the geographic distribution of the Dutch dialects with respect to syntactic variation in the aggregate. The Dutch dialect map based on a syntactic measure will be compared with a dialect map based on subjective judgements and a dialect map based on pronunciation differences to put the syntactic measurement results into perspective. An alternative way to measure syntactic distance will be presented and will provide indications for future research to more accurately quantify syntactic variation.,Measuring Syntactic Variation in Dutch Dialects,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,21,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
241,http://dx.doi.org/10.1093/llc/fql044,"{'string_id': '10.1093/llc/fql044', 'id_scheme': 'DOI'}","The chi-squared test is used to find the vocabulary most typical of seven different ICAME corpora, each representing the English used in a particular country. In a closely related study, Leech and Fallon (1992, Computer corpora – what do they tell us about culture? ICAME Journal, 16: 29–50) found differences in the vocabulary used in the Brown Corpus of American English and that the Lancaster–Oslo–Bergen Corpus of British English. They were mainly interested in those vocabulary differences which they assumed to be due to cultural differences between the United States and Britain, but we are equally interested in vocabulary differences which reveal linguistic preferences in the various countries in which English is spoken. Whether vocabulary differences are cultural or linguistic in nature, they can be used for the automatic classification according to variety of English of texts of unknown provenance. The extent to which the vocabulary differences between the corpora represent vocabulary differences between the varieties of English as a whole depends on the extent to which the corpora represent the full range of topics typical of their associated cultures, and thus there is a need for corpora designed to represent the topics and vocabulary of cultures or dialects, rather than stratified across a set range of topics and genres. This will require methods to determine the range of topics addressed in each culture, then methods to sample adequately from each topical domain.",Use of the Chi-Squared Test to Examine Vocabulary Differences in English Language Corpora Representing Seven Different Countries,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,22,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
242,http://dx.doi.org/10.1093/llc/fql048,"{'string_id': '10.1093/llc/fql048', 'id_scheme': 'DOI'}","The search for a reliable expression to measure an author's lexical richness has constituted many statisticians' holy grail over the last decades in their attempt to solve some controversial authorship attributions. The greatest effort has been devoted to find a formula grounded on the computation of tokens, word-types, most-frequent-word(s), hapax legomena, hapax dislegomena, etc., such that it would characterize a text successfully, independent of its length. In this line, Yule's K and Zipf 's Z seem to be generally accepted by scholars as reliable measures of lexical repetition and lexical richness by computing content and function words altogether.1 Given the latter's higher frequency, they prove to be more reliable identifiers when isolatedly computed in p.c.a. and Delta-based attribution studies, and their rate to the former also measures the functional density of a text. In this paper, we aim to show that each constant serves to measure a specific feature and, as such, they are thought to complement one another since a supposedly rich text (in terms of its lemmas) does necessarily have to characterize by its low functional density, and vice versa. For this purpose, an annotated corpus of the West Saxon Gospels (WSG) and Apollonius of Tyre (AoT) has been used along with a huge raw corpus.",Function Words in Authorship Attribution Studies,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,22,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
243,http://dx.doi.org/10.1093/llc/fql049,"{'string_id': '10.1093/llc/fql049', 'id_scheme': 'DOI'}","This article discusses the development of integrated multilingual Web databases to help the preservation of the Native American language East Cree. The creation of digital online resources for threatened aboriginal languages presents many technical, educational and ethical challenges. We focus here on the technical challenges in order to discuss both the problems encountered in this particular context, and the solutions we have considered and explored. We illustrate our discussion with examples from an Oral Stories Database we developed in collaboration with Cree education consultants and speakers in 2002–04. We advocate an approach that includes fast-prototyping, open-source development, and design for the database engine that balances speed, availability, features, and resources. We discuss the impact the combination of this technical approach and the participatory action research method is having on language maintenance.",Developing Web Databases for Aboriginal Language Preservation,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,22,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
244,http://dx.doi.org/10.1093/llc/fqm001,"{'string_id': '10.1093/llc/fqm001', 'id_scheme': 'DOI'}","In a meeting at King's College London in May 2000, John Unsworth proposed a list of seven ‘scholarly primitives’ which he claimed were ‘self-understood’ functions forming the basis for ‘higher-level scholarly projects, arguments, statements [and] interpretations’ (Unsworth, 2000). He claimed that his list summarized activities that were ‘basic to scholarship across eras and across media’, and went on to say that an analysis of these scholarly primitives might result in a clearer sense of how computing tools could support the scholarly endeavour. Here we focus on the primitive that was second on Unsworth's list, after ‘Discovering’: ‘Annotation’.Our work on annotation arises out of a developing awareness that established Humanities Computing (HC) areas of interest, do not seem always to connect with the actual process of the research work being carried out by most humanists. We claimed in Bradley (2005) that a fundamentally different usage paradigm than those in operation in established HC was necessary to even notice, and then follow-up on, the potential of scholarly annotation as a computer-supported activity.This article presents our experiences, and the eventual outcomes, of the process of developing annotations tools for the Online Chopin Variorum Edition project (OCVE).1 Beginning with a brief overview of activities related to annotation in Humanities Computing and Computing Science, we introduce the visible parts of the OCVE project, and address some discussion to the structures behind the scenes that support what it does, reporting what worked and what did not. We conclude by analysing the significance of our findings and describing the direction we think our annotation tool will take.",Supporting Annotation as a Scholarly Tool--Experiences From the Online Chopin Variorum Edition,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,22,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
245,http://dx.doi.org/10.1093/llc/fqm002,"{'string_id': '10.1093/llc/fqm002', 'id_scheme': 'DOI'}","In recent years, linguists have become increasingly interested in the language of the Internet—both as an object of investigation as well as a source of authentic data to complement traditional electronic corpora. However, Internet-derived data is typically very messy data and a conversion process is often required in order to enable researchers to carry out a reliable quantitative investigation of the patterns observed with the help of standard corpus tools. In this article, I discuss the technical and methodological aspects involved in creating a large corpus of asynchronous computer-mediated communication by downloading and post-processing hundreds of thousands messages posted in twelve Usenet newsgroups. After describing how messages can be arranged into hierarchically structured discussion threads, I focus at some length on the strategies that are required to correctly assign authorship to the different textual elements in individual messages. My algorithms have a success rate of well over 90% for most newsgroups and the resulting corpus can thus serve as a suitable basis for an investigation into the interactive strategies employed in this particular type of written communication.",Processing Internet-derived Text--Creating a Corpus of Usenet Messages,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,22,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
246,http://dx.doi.org/10.1093/llc/fqm003,"{'string_id': '10.1093/llc/fqm003', 'id_scheme': 'DOI'}","This article reports on experiments performed with a large corpus, aiming at separating texts according to the author style. The study initially focusses on whether the classification accuracy regarding the author identity may be improved, if the text topic is known in advance. The experimental results indicate that this kind of information contributes to more accurate author recognition. Furthermore, as the diversity of a topic set increases, the classification accuracy is reduced. In general, the experimental results indicate that taking into account knowledge regarding the text topic can lead to the construction of specialized models for each author with higher classification accuracy. For example, by focussing on a specific topic, the accuracy with which the author identity is determined increases, the exact amount depending on the specific topic. This also applies when the topic of the text is more broadly determined, as a set of topic categories.In an associated task, the most salient parameters within an 85-parameter vector are studied, for a number of subsets of the corpus, where each subset contains speeches from a single topic. These studies indicate that the salient parameters are the same for the different subsets. Two fixed data vectors have been defined, using 16 and 25 parameters, respectively. The classification accuracy obtained, even with the smallest data vector, is only 5% less than with the complete vector. This indicates that the parameters retained in the reduced vectors bear a large amount of discriminatory information and suffice for an accurate classification of the corpus.",Employing Thematic Variables for Enhancing Classification Accuracy Within Author Discrimination Experiments,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,22,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
247,http://dx.doi.org/10.1093/llc/fqm004,"{'string_id': '10.1093/llc/fqm004', 'id_scheme': 'DOI'}","We apply default inheritance hierarchies to generating the morphology of Hebrew verbs. This approach represents inflectional exponents as markings associated with the application of rules by which complex word forms are deduced from simpler roots or stems. The high degree of similarity among verbs of different conjugation classes allows us to formulate general rules; these general rules are, however, sometimes overridden by conjugation-specific rules. Similarly, a verb's form within a particular conjugation is determined both by default rules and by overriding rules specific to lexical stem peculiarities. Our result is a concise set of rules defining the morphology of Hebrew verbs in all conjugations. We express these rules in KATR, both a formalism for default inheritance hierarchies and associated software for generating the forms specified by those rules. As we describe the rules, we point out general strategies for expressing morphology in KATR. We conclude by discussing KATR's advantages over ordinary DATR for the representation of morphological systems and our plans for KATR's successor, LATR.",A Default Inheritance Hierarchy for Computing Hebrew Verb Morphology,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,22,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
248,http://dx.doi.org/10.1093/llc/fqm006,"{'string_id': '10.1093/llc/fqm006', 'id_scheme': 'DOI'}","Estimating the relative frequencies of linguistic features is a fundamental task in linguistic computation. As the amount of text or speech that is available from a given user of the language typically varies greatly, and the sample sizes tend to be small, the most straightforward methods do not always give the most informative answers. Bootstrap and Bayesian methods provide techniques for handling the uncertainty in small samples. We describe these techniques for estimating frequencies from small samples, and show how they can be applied to the study of linguistic change. As a test case, we use the introduction of the pronoun you as subject in the data provided by the Corpus of Early English Correspondence (c. 1410–1681).",How to Handle Small Samples: Bootstrap and Bayesian Methods in the Analysis of Linguistic Change,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,22,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
249,http://dx.doi.org/10.1093/llc/fqm008,"{'string_id': '10.1093/llc/fqm008', 'id_scheme': 'DOI'}","The model described here relies on the key concepts of topology, i.e. neighbourhood and equivalence of shape. A linguistic object L is studied in text T by means of one or several local questions Q. The set of successive local answers is processed so as to provide a global function characterizing the textual space under scrutiny. We begin with short sequences of tenses to illustrate the way in which to explore originally Emile Benveniste's concepts of history and discourse.1 We then supply life-size examples of other objects selected for their heuristic value. We go on to demonstrate the model at work on the distribution of strings of finite (F) and non-finite (n) verbal forms in the LOB Corpus of English. A topological chart is produced as the synthetic image mirroring the locations of the relevant linguistic entities throughout the text. All the individual strings concatenating any number of F and n are classified in a table. Alternatively, individual full-text strings can be extracted. We then proceed to refine the notion of lexical distribution in ‘rafales’ in a lemmatized corpus of Latin texts, the purpose being to test the stability of the distributions in individual texts of selected verbs and assess whether a verb's behaviour is related to its semantic status. The final section is devoted to other Latin texts. The use of segments of equal length makes it possible to draw up the narrative profile of each author as revealed by his handling of tenses in main clauses.",Trees and After: The Concept of Text Topology. Some Applications to Verb-Form Distributions in Language Corpora,Oxford University Press (OUP),2006,,Literary and Linguistic Computing,22,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
250,http://dx.doi.org/10.1093/llc/fqm009,"{'string_id': '10.1093/llc/fqm009', 'id_scheme': 'DOI'}","During the process of writing a comprehensive dictionary of Finnish dialects, a large set of maps describing the regional distribution of the dialect words have been compiled in electronic form. In this article, we set out to analyse this corpus of data in order to gain new insight on the variation of Finnish dialects. We use a wide range of multivariate data analysis methods, including principal components analysis, independent components analysis, clustering, and multidimensional scaling. We explain how to preprocess the data to overcome the problem of uneven sampling caused by the way the data has been collected. We discuss the results obtained by these methods and compare them to the traditional view of Finnish dialect groups.",Multivariate Analysis of Finnish Dialect Data An Overview of Lexical Variation,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,22,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
251,http://dx.doi.org/10.1093/llc/fqm010,"{'string_id': '10.1093/llc/fqm010', 'id_scheme': 'DOI'}","The present article is concerned with the problem of automatic database population via information extraction (IE) from web pages obtained from heterogeneous sources, such as those retrieved by a domain crawler. Specifically, we address the task of filling single multi-field templates from individual documents, a common scenario that involves free-format documents with the same communicative goal such as job adverts, CVs, or meeting/seminar announcements. We discuss challenges that arise in this scenario and propose solutions to them at different levels of the processing of web page content. Our main focus is on the issue of information extraction, which we address with a two-step machine learning approach that first aims to determine segments of a page that are likely to contain relevant facts and then delimits specific natural language expressions with which to fill template fields. We also present a range of techniques for the enrichment of web pages with semantic annotations, such as recognition of named entities, domain terminology and coreference resolution, and examine their effect on the information extraction method. We evaluate the developed IE system on the task of automatically populating a database with information on language resources available on the web.",Discovery of Language Resources on the Web: Information Extraction from Heterogeneous Documents,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,22,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
252,http://dx.doi.org/10.1093/llc/fqm011,"{'string_id': '10.1093/llc/fqm011', 'id_scheme': 'DOI'}","We present a quantitative analysis of 442 pieces of fiction published between 5 October 1992 and 17 September 2001 in the New Yorker magazine. We address two independent questions using the same data set. First, we examine whether changes in the Executive Editor or Fiction Editor are associated with significant changes in the type of fiction published at the New Yorker. Second, we examine whether New Yorker authors write fiction more often than not about characters with whom they share demographic traits. We find that changes in Fiction Editor at the New Yorker are associated with numerous significant, quantifiable changes in the magazine's fiction and that these effects are greater than those associated with a change in the New Yorker's Executive Editor. We also find that authors of New Yorker fiction write significantly more often than not about protagonists who share their race, gender, and country of origin and who are within or below their age range. The same is true of secondary characters except in the case of gender.",A Statistical Analysis of Editorial Influence and Author Character Similarities in 1990s New Yorker Fiction,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,22,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
253,http://dx.doi.org/10.1093/llc/fqm012,"{'string_id': '10.1093/llc/fqm012', 'id_scheme': 'DOI'}","The Middle Dutch Arthurian romance Roman van Walewein (‘Romance of Gawain’) is attributed in the text itself to two authors, Penninc and Vostaert. Very little quantitative research into this dual authorship has been done. This article describes our progress in applying different non-traditional authorship attribution methods to the text of Walewein. After providing an introduction to the romance and an overview of earlier research, we evaluate previous statements on authorship and stylistics by applying both Yule's measure of lexical richness and Burrows's Delta. To find out whether these new methods would confirm or even enhance our present knowledge about the differences between the two authors, we applied an adapted version of John Burrows's Delta procedure. The adapted version seems to be able to distinguish the double authorship of the romance. It also helps us to confirm some and to reject other earlier statements about the position in the text where the second author started his work.",Delta for Middle Dutch Author and Copyist Distinction in Walewein,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,22,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
254,http://dx.doi.org/10.1093/llc/fqm017,"{'string_id': '10.1093/llc/fqm017', 'id_scheme': 'DOI'}","This paper reports the completion of the first expansion phase of the Sheffield Corpus of Chinese (SCC). We describe the major improvements we made in expanding the corpus. They involve the coverage of time periods, choice of text types and categories, and selection of individual texts; the mark up scheme and the integral search and analysis tool. We use the developing SCC to examine Li and Thompson's; (1974, 1975, 1976) controversial postverbal predominance hypothesis for prepositional phrases (PPs) in Archaic Chinese and their word order change hypothesis for PPs in general in the history of the Chinese language. Our study provides no evidence for the postverbal predominance hypothesis for PPs in Archaic Chinese and the word order change hypothesis for PPs in general from postverbal in Archaic Chinese to preverbal in Modern Chinese. Our findings show that postverbal and preverbal PPs have been in coexistence and there have always been more occurrences of preverbal PPs than postverbal PPs in all the time periods covered in the current SCC. Although use of some PPs declined in some time periods and use of others emerged in other time periods, there was never a predominant position for PPs in any time period in the history of Chinese. We show differences in the distribution of PPs in different time periods and provide an account of the syntactic positions of PPs in those time periods.",Syntactic Positions of Prepositional Phrases in the History of Chinese: Using the Developing Sheffield Corpus of Chinese for Diachronic Linguistic Studies,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,22,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
255,http://dx.doi.org/10.1093/llc/fqm018,"{'string_id': '10.1093/llc/fqm018', 'id_scheme': 'DOI'}","Remediation refers to the re-presentation of old media in new media. This article studies remediation in electronic products in library collections, especially the digital facsimile. Early English Books Online (EEBO) is a particularly interesting example, not only because of its scholarly importance, but also because of its multi-layered genesis from printed work to microfilm (Early English Books (EEB)) to digital (EEBO) facsimile, and to the text encoding initiative EEBO-TCP, a joint ProQuest and Text Creation Partnership (TCP) project. The article analyses the impact of filters and limits of remediation in relation to EEBO and its predecessor EEB, such as the choice to duplicate a single copy of a work as bi-tonal black and white images, and to scholarly work.",Metamorphosis: Remediation in Early English Books Online (EEBO),Oxford University Press (OUP),2007,,Literary and Linguistic Computing,22,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
256,http://dx.doi.org/10.1093/llc/fqm019,"{'string_id': '10.1093/llc/fqm019', 'id_scheme': 'DOI'}","Fifteen items in the Weekly Magazine have been attributed to Goldsmith. Our study uses traditional kinds of internal evidence (mainly verbal parallels) together with evidence from selected linguistic features. A preliminary analysis identifies features which best distinguish Goldsmith samples from those of a number of contemporary authors. Using this selection of features, we calculate the distances of the fifteen Weekly items from the cluster of Goldsmith samples; an item at too large a distance is unlikely to be his. A parallel investigation is based on sentence-length statistics. We conclude that seven essays may plausibly be assigned to Goldsmith, that he probably co-authored two pieces, and that in three cases he merely made minor additions to material from other sources.",Goldsmith's Contributions to the Weekly Magazine,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,22,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
257,http://dx.doi.org/10.1093/llc/fqm020,"{'string_id': '10.1093/llc/fqm020', 'id_scheme': 'DOI'}","The basic assumption of quantitative authorship attribution is that the author of a text can be selected from a set of possible authors by comparing the values of textual measurements in that text to their corresponding values in each possible author's writing sample. Over the past three centuries, many types of textual measurements have been proposed, but never before have the majority of these measurements been tested on the same dataset. A large-scale comparison of textual measurements is crucial if current techniques are to be used effectively and if new and more powerful techniques are to be developed. This article presents the results of a comparison of thirty-nine different types of textual measurements commonly used in attribution studies, in order to determine which are the best indicators of authorship. Based on the results of these tests, a more accurate approach to quantitative authorship attribution is proposed, which involves the analysis of many different textual measurements.",Quantitative Authorship Attribution: An Evaluation of Techniques,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,22,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
258,http://dx.doi.org/10.1093/llc/fqm021,"{'string_id': '10.1093/llc/fqm021', 'id_scheme': 'DOI'}","This article examines the actual and potential use of software tools in research in the arts and humanities focusing on audiovisual (AV) materials such as recorded speech, music, video and film. The quantity of such materials available to researchers is massive and rapidly expanding. Researchers need to locate the material of interest in the vast quantity available, and to organize and process the material once collected. Locating and organizing often depend on metadata and tags to describe the actual content, but standards for metadata for AV materials are not widely adopted. Content-based search is becoming possible for speech, but is still beyond the horizon for music, and even more distant for video. Copyright protection hampers research with AV materials, and Digital Rights Management (DRM) systems threaten to prevent research altogether. Once material has been located and accessed, much research proceeds by annotation, for which many tools exist. Many researchers make some kind of transcription of materials, and would value tools to automate this process. Such tools exist for speech, though with important limits to their accuracy and applicability. For music and video, researchers can make use of visualizations. A better understanding (in general terms) by researchers of the processes carried out by computer software and of the limitations of its results would lead to more effective use of Information and Communications Technology (ICT).","Tools for Searching, Annotation and Analysis of Speech, Music, Film and Video A Survey",Oxford University Press (OUP),2007,,Literary and Linguistic Computing,22,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
259,http://dx.doi.org/10.1093/llc/fqm026,"{'string_id': '10.1093/llc/fqm026', 'id_scheme': 'DOI'}","We introduce a systematic approach to language change quantification by studying unconsciously used language features in time-separated parallel translations. For this purpose, we use objective style markers such as vocabulary richness and lengths of words, word stems and suffixes, and employ statistical methods to measure their changes over time. In this study, we focus on the change in Turkish in the second half of the twentieth century. To obtain word stems, we first introduce various stemming techniques and show that they are highly effective. Our statistical analyses show that over time, for both text and lexicon, the length of Turkish words has become significantly longer, and word stems have become significantly shorter. We also show that suffix lengths have become significantly longer for types and the vocabulary richness based on word stems has shrunk significantly. These observations indicate that in contemporary Turkish one would use more suffixes to compensate for the fewer stems to preserve the expressive power of the language at the same level. Our approach can be adapted for quantifying the change in other languages.",Language Change Quantification Using Time-separated Parallel Translations,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,22,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
260,http://dx.doi.org/10.1093/llc/fqm028,"{'string_id': '10.1093/llc/fqm028', 'id_scheme': 'DOI'}","Assessments of Goldsmith's contribution to the Busy Body have fluctuated widely. We examine the ten possible attributions, gathering evidence from verbal parallels, selected linguistic features, and measures of sentence-length, together with idiosyncrasies of vocabulary and syntax in the ‘doubtful’ essays themselves. We conclude that apart from the essay on London clubs, which he later acknowledged, only one piece can be attributed to Goldsmith with any confidence.",Goldsmith and the Busy Body,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,22,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
261,http://dx.doi.org/10.1093/llc/fqm037,"{'string_id': '10.1093/llc/fqm037', 'id_scheme': 'DOI'}","The First International Conference of the Alliance of Digital Humanities Organizations (ADHO) was special in many ways, not least because it was the first time that the Association for Computers and the Humanities (ACH) and the Association for Literary and Linguistic Computing (ALLC) convened their joint conference under this new name. Held at the Université Paris-Sorbonne, Digital Humanities 2006 (to give it its shorter title) was organized by Liliane Gallet-Blanchard, Marie-Madeleine Martinet and other members of Cultures Anglophones et Technologies de l’Information (CATI). There were 214 participants from twenty-four countries. One hundred and twenty papers and sessions and thirty-one posters were presented.",Digital Humanities 2006: When Two Became Many,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
262,http://dx.doi.org/10.1093/llc/fqm038,"{'string_id': '10.1093/llc/fqm038', 'id_scheme': 'DOI'}","The Corpus of Electronic Texts (CELT) project at University College Cork is an on-line corpus of multilingual texts that are encoded in TEI conformant SGML/XML. As of September 2006, the corpus has 9.3 million words online. Over the last five years, doctoral work carried out at the project has focused on the development of lexicographical resources spanning the years c. AD 700–1700, and on the development of tools to integrate the corpus with these resources. This research has been further complimented by the Linking Dictionaries and Text project, a North–South Ireland collaboration between the University of Ulster, Coleraine, and University College Cork. The Linking Dictionaries and Text project will reach completion in October 2006. This article focuses on CELT's latest research project, the Digital Dinneen project, that aims to create an integrated edition of Patrick S. Dinneen's Foclóir Gaedhilge agus Béarla (Irish-English Dictionary). In this article, the newly developed research infrastructure—that is the culmination of the doctoral research carried out at CELT and the Linking Dictionaries and Text collaboration—will be described, and ways that the Digital Dinneen will be integrated into this infrastructure established. Finally, avenues of future research will be pointed to.",Developing Integrated Editions of Minority Language Dictionaries: The Irish Example,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
263,http://dx.doi.org/10.1093/llc/fqm039,"{'string_id': '10.1093/llc/fqm039', 'id_scheme': 'DOI'}","As digital libraries have expanded to absorb existing collections as well as to create new ones, it has become clear that cross collection discovery is not simply desirable, but is increasingly a necessity demanded by users. Similarly, in the digital humanities community, thematic research collections once distinct from one another now would seem to benefit from interoperability. However, efforts to aggregate disparate resources are often stymied by differing metadata schema and controlled vocabulary. Using the lessons learned from the Thomas MacGreevy Archive, The University of Maryland Libraries designed its digital repository to provide for discovery across object types and collections using Fedora as the underlying architecture. To facilitate access to multiple collections within one repository, University of Maryland developed a flexible metadata standard. This metadata schema is used to describe varying types of materials at varying levels of granularity, while allowing for controlled vocabularies appropriate to specific collections.",Cross-collection Searching: A Pandora's Box or the Holy Grail?,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
264,http://dx.doi.org/10.1093/llc/fqm040,"{'string_id': '10.1093/llc/fqm040', 'id_scheme': 'DOI'}","As the number of scholarly encoded digital texts is increasing, creating models of these kinds of texts with the help of digital tools is becoming more and more interesting. In connection with this type of work, it is important to have a clear understanding of what these particular models are based on. They will clearly be based on certain readings of the source texts, but we need to keep track of the relationships between the texts, readings of the texts and the models based on such readings.In this article, a problem of potentially great significance for this kind of modelling is discussed. The problem is called the exhibition problem and is based on the difference in ordinary linguistic communication between asserting a fact, e.g. that a certain person has a certain name, and exhibiting the same fact. In many cases, the latter is modelled as if it was the former. As a solution to this problem, an event-oriented modelling method is proposed.",The Exhibition Problem. A Real-life Example with a Suggested Solution,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
265,http://dx.doi.org/10.1093/llc/fqm041,"{'string_id': '10.1093/llc/fqm041', 'id_scheme': 'DOI'}","Information about place and location is an essential part of research in the humanities. There are many ways that methods and tools for structuring, visualizing and analysing space, spatial behaviour, and spatial relationships can benefit humanities research but the use of spatial information in digital scholarship by humanists remains very limited. The developing role of the study of place and location through geographical information systems (GIS) and other digital tools is discussed briefly before examining the factors that are inhibiting the use of spatial data in our research. The influences of current research practice and the attitudes of scholarly institutions in the humanities are examined. This article will explore some of the potential research applications but, possibly more importantly; it will also examine why that potential is being developed so slowly and discuss a possible way forward for the community.",The Inhibition of Geographical Information in Digital Humanities Scholarship,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
266,http://dx.doi.org/10.1093/llc/fqm042,"{'string_id': '10.1093/llc/fqm042', 'id_scheme': 'DOI'}","The emerging discipline of ‘digital humanities’ has been plagued by a perceived neglect on the part of the broader humanities community. The community as a whole tends not to be aware of the tools developed by DH practitioners (as documented by the recent surveys by Siemens et al.), and tends not to take seriously many of the results of scholarship obtained by DH methods and tools. This article argues for a focus on deliverable results in the form of useful solutions to common problems that humanities scholars share, instead of simply new representations. The question to address is what needs the humanities community has that can be dealt with using DH tools and techniques, or equivalently what incentive humanists have to take up and to use new methods. This can be treated in some respects like the computational quest for the ‘killer application’—a need of the user group that can be filled, and by filling it, create an acceptance of that tool and the supporting methods/results. Some definitions and examples are provided both to illustrate the idea and to support why this is necessary. The apparent alternative is the status quo, where digital research tools are brilliantly developed, only to languish in neglect and disuse.",Killer Applications in Digital Humanities,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
267,http://dx.doi.org/10.1093/llc/fqm043,"{'string_id': '10.1093/llc/fqm043', 'id_scheme': 'DOI'}","This article examines the hypothesis that a speaker uses the HAVE + noun construction when (s)he wants to express somebody's positive qualities. This hypothesis was tested with two different corpora using two different retrieval programs. The numerous steps that are necessary show that it can take a considerable amount of time to reach sound conclusions and suggests the need to improve the building of those corpora or their retrieval software programs. χ2 and z-score tests were used to analyse the results. The above hypothesis proves to be too strong: although the verb HAVE is never significantly associated more frequently with shortcomings, the verb BE is sometimes associated more frequently with qualities. The lists of qualities and shortcomings expressed with HAVE or with BE show that BE collocates with a wider range of qualities than shortcomings. These lists also indicate that the qualities often quoted by the theoretical linguist are not those most frequently found in corpora. They also point to differences between the qualities that collocate with HAVE and those that collocate with BE. The use of corpora enabled me to widen the range of qualities or shortcomings that are said to collocate with HAVE or BE, and revealed that, contrary to the hypothesis, the HAVE + noun construction can apply to inanimate subject referents.",BE and HAVE: Qualities and Shortcomings,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
268,http://dx.doi.org/10.1093/llc/fqm044,"{'string_id': '10.1093/llc/fqm044', 'id_scheme': 'DOI'}","In this article, we describe the respective approaches we have taken when addressing issues of spelling variation in German and English historical texts. More specifically, we describe an experiment to evaluate automatic techniques for the development of letter replacement heuristics against manually created gold standards of known letter replacements rules. As will become clear, the motivation for the research differs according to the team of researchers: the German researchers are seeking to develop a search engine for historical texts; the English researchers want to improve the results obtained when applying corpus linguistic techniques (developed for modern language) to historical data. However, the respective teams do share a longer term goal of assessing whether it is possible to develop a generic spelling detection tool for Indo-European languages.",The Identification of Spelling Variants in English and German Historical Texts: Manual or Automatic?,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
269,http://dx.doi.org/10.1093/llc/fqm045,"{'string_id': '10.1093/llc/fqm045', 'id_scheme': 'DOI'}","There are now many online, digital resources in the humanities, and their creation is funded by various governmental, academic, and philanthropic sources. What happens to these resources after completion is very poorly understood. No systematic survey of digital resource usage in the humanities has ever been undertaken—and the factors for use and non-use of digital resources are unknown. The LAIRAH (Log Analysis of Internet Resources in the Arts and Humanities) Project is a 15-month long study into the factors which determine long-term use and neglect of digital resources in the Arts and Humanities. Using quantitative Deep Log Analysis techniques to understand real-time user behaviour and qualitative user workshops to gain an understanding of user approaches to digital resources in the arts and humanities, the study identifies factors that may predispose a digital resource to become used or neglected in the long-term. This article provides an overview of the techniques used in the LAIRAH project, and presents some preliminary results that may be of use to both the creators of digital resources in the humanities, and the funders of these projects, to ensure that significant intellectual effort and time, and financial resources, are not wasted in the creation of projects that are then neglected by the user community.",If You Build It Will They Come? The LAIRAH Study: Quantifying the Use of Online Resources in the Arts and Humanities through Statistical Analysis of User Log Data,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
270,http://dx.doi.org/10.1093/llc/fqm048,"{'string_id': '10.1093/llc/fqm048', 'id_scheme': 'DOI'}","The DATABASE OF ELECTRONIC TEXTS of the LEXICON OF BAVARIAN DIALECTS IN AUSTRIA (WBÖ) (TEXTKORPUS zum WBÖ)—The Lexicon of Bavarian Dialects in Austria (Wörterbuch der bairischen Mundarten in Österrreich [WBÖ]) is based on a collection of about 4 million single records. They represent the variety of the regional, social and historical Bavarian dialects. About 10% of all entries are excerpts from texts of various types. The lexicographer has to draw the illustrative quotations from the original texts. The digitized full-texts have been dated and localized, thus each quotation can be placed in time and area enabling the lexicogapher to choose a representative number of examples. For the definitions, the most appropriate and illustrative quotations have to be found, in order to actively support the lexicographer's work, the Institute of Lexicography of Austrian Dialects and Names / Institut für Österreichische Dialekt- und Namenlexika (http://www.oeaw.ac.at/dinamlex) started a new project: the so-called DBÖ (Database of the Bavarian Dialects in Austria / Datenbank der bairischen Mundarten in Österreich), financed by the Österreichische Akademie der Wissenschaften / Austrian Academy of Sciences. One main database includes the so-called Hauptkatalog, the main archive; there are additional databases which are necessary to retrieve correct information about the questionnaire, the localization of the word and the date of its recording. One component of the DBÖ is the database of electronic texts of the WBÖ, including some 90 Austrian texts spanning several centuries, representing the Austrian dialects. The original texts are scanned using the Austrian OCR-programme proLector V1.20 (A.1) which allows the training of various fonts. The machine-readable texts are converted in TUSTEP (Tübinger System von Textverarbeitungs-Programmen / Tübingen System of Text Processing Programmes). The TUSTEP-files get an alphanumeric key, which allows one to retrieve each quotation from the database in a chronological order and to sort it according to its localization. Finally the texts are broken down into the requisite sized pieces for quoting in the WBÖ-entries. Using a special programme the quotations can easily be reconnected and replaced in the proper context from which they were drawn. The digital texts are a valuable source for the lexicographer freeing him from the monotony of checking over and over again, thus leaving him more time for his proper work, namely the writing of entries for the WBÖ. Furthermore, the digital texts are important for speeding up the dictionary's publication in accordance with the guidelines of the Straffungskonzept 1993 and 1998.",Zitate per Mausklick? Das Textkorpus zum WORTERBUCH DER BAIRISCHEN MUNDARTEN IN OSTERREICH (WBO) als leistungsstarkes Werkzeug fur die lexikographische Praxis,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
271,http://dx.doi.org/10.1093/llc/fqn003,"{'string_id': '10.1093/llc/fqn003', 'id_scheme': 'DOI'}","While Burrows's intuitive and elegant ‘Delta’ measure for authorship attribution has proven to be extremely useful for authorship attribution, a theoretical understanding of its operation has remained somewhat obscure. In this article, I address this issue by introducing a geometric interpretation of Delta, which further allows us to interpret Delta as a probabilistic ranking principle. This interpretation gives us a better understanding of the method's fundamental assumptions and potential limitations, as well as leading to several well-founded variations and extensions.",Interpreting Burrows's Delta: Geometric and Probabilistic Foundations,Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
272,http://dx.doi.org/10.1093/llc/fqn004,"{'string_id': '10.1093/llc/fqn004', 'id_scheme': 'DOI'}","Today's corpus tools offer the user a wide range of features that greatly facilitate the linguistic analysis of large amounts of authentic language data (e.g. frequency distributions, collocations, keywords, etc.). However, these tools typically fail to address the fundamental need of the linguist to add interpretive information to a concordance or query result, by coding individual concordance lines for structural, functional, discoursal, and other features in a flexible way. The ability to add such qualitative data is indispensable to a fuller understanding of the phenomenon under investigation as it allows the linguist to produce more rigorous descriptions—and theories—about language in use. Our article has two aims: first, to assess the merits and drawbacks of existing solutions, by surveying what can be achieved using state-of-the-art corpus tools and generic database software; second, we draw up a set of desiderata and recommendations for the incorporation of flexible encoding features into future corpus tools. We describe an initial step in this direction, with a recent enhancement to the BNCweb corpus analysis software. More generally, we hope our suggestions will lead to linguists and software developers working together more closely to ensure that the needs of the former are provided for by the available technology.","Corpus Tools and Methods, Today and Tomorrow: Incorporating Linguists' Manual Annotations",Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
273,http://dx.doi.org/10.1093/llc/fqn005,"{'string_id': '10.1093/llc/fqn005', 'id_scheme': 'DOI'}","Princeton—Stanford Working Papers in Classics (PSWPC) is a web-based series of work-in-progress scripts by members of two leading departments of classics. It introduces the humanities to a new form of scholarly communication and represents a major advance in the free availability of classical-studies scholarship in cyberspace. This article both reviews the initial performance of this open-access experiment and the benefits and challenges of working papers more generally for classical studies. After 2 years of operation PSWPC has proven to be a clear success. This series has built up a large international readership and a sizeable body of pre-prints and performs important scholarly and community-outreach functions. As this performance is largely due to its congruency with the working arrangements of ancient historians and classicists and the global demand for open-access scholarship, the series confirms the viability of this means of scholarly communication, and the likelihood of its expansion in our discipline. But modifications are required to increase the benefits this series brings and the amount of scholarship it makes freely available online. Finally, departments wishing to replicate its success will have to consider other important developments, such as the increasing availability of post-prints, the linking of research funding to open access, and the emergence of new cyber-infrastructure.","Working Papers, Open Access, and Cyber-infrastructure in Classical Studies",Oxford University Press (OUP),2007,,Literary and Linguistic Computing,23,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
274,http://dx.doi.org/10.1093/llc/fqn010,"{'string_id': '10.1093/llc/fqn010', 'id_scheme': 'DOI'}","One of the first decisions made in any research concerns the selection of an appropriate scale of analysis—are we looking out into the heavens, or down into atoms? To conceive a digital library as a collection of a million books may restrict analysis to only one level of granularity. In this article, we examine the consequences and opportunities resulting from a shift in scale, where the desired unit of interpretation is something smaller than a text: it is a keyword, a motif, or a metaphor. A million books distilled into a billion meaningful components become raw material for a history of language, literature, and thought that has never before been possible. While books herded into genres and organized by period remain irregular, idiosyncratic, and meaningful in only the most shifting and context-dependent ways, keywords or metaphors are lowest common denominators. At the semantic level—the level of words, images, and metaphors—long-term regularity and patterns emerge in collection, analysis, and taxonomy. This article follows the foregoing course of thought through three stages: first, the manual curation of a high quality database of metaphors; second, the expansion of this database through automated and human-assisted techniques; finally, the description of future experiments and opportunities for the application of machine learning, data mining, and natural language processing techniques to help find patterns and meaning concealed at this important level of granularity.",Mining millions of metaphors,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
275,http://dx.doi.org/10.1093/llc/fqn011,"{'string_id': '10.1093/llc/fqn011', 'id_scheme': 'DOI'}","The new drama analysis program IDAP provided charts containing frequency distributions of speech lengths in Shakespeare's plays. Following previous investigations that showed maximum values at the length of four words for plays produced after the opening of the Globe in 1599, and nine words for plays produced before 1599, the present analysis turned to the four plays The Merry Wives of Windsor, King Henry IV, 2, Much Ado About Nothing and King Henry V which already indicate changes in style. Composite curves not only characterize the four texts as transitional plays that embody the old style expressed by a maximum of nine words, but also impending changes expressed by a maximum of four words while the transition was indicated by the maximum of six words. Statistical results thus confirm the theories of stylistic and biographical changes before 1599 that James Shapiro had put down in his work 1599: A Year in the Life Of William Shakespeare. London: Faber & Faber, 2005.",More statistical observations on speech lengths in Shakespeare's plays,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
276,http://dx.doi.org/10.1093/llc/fqn012,"{'string_id': '10.1093/llc/fqn012', 'id_scheme': 'DOI'}","The use of corpora that are divided into temporally ordered stages is becoming increasingly wide-spread in historical corpus linguistics. This development is partly due to the fact that more and more resources of this kind are being developed. Since the assessment of frequency changes over multiple periods of time is a relatively recent practice, there are few agreed-upon standards of how such trends should be statistically interpreted. This article addresses the need for a basic analytical toolbox that is specifically tailored to the interpretation of frequency changes in multistage diachronic corpora. We present a number of suggestions for the analysis of data that analysts commonly face in historical studies, but also in the study of language acquisition.",Assessing frequency changes in multistage diachronic corpora: Applications for historical corpus linguistics and the study of language acquisition,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,24,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
277,http://dx.doi.org/10.1093/llc/fqn013,"{'string_id': '10.1093/llc/fqn013', 'id_scheme': 'DOI'}","This paper describes preliminary sketches for a formal account of transcription as it is performed in scholarly editing and in the creation of digital resources. After a general outline of our approach, we present two formal models of transcription. The first addresses only the very simplest cases, the second addresses some but not all of the gaps in the first. Finally, we mention some less simple cases and discuss some elaborations of the model which we hope to develop in future work.",What is transcription?,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
278,http://dx.doi.org/10.1093/llc/fqn014,"{'string_id': '10.1093/llc/fqn014', 'id_scheme': 'DOI'}","Here I survey activities in the digital humanities as a primary source for our conceptualization of the field. I argue for the fundamental nature of modelling to these humanities and describe three varieties: analytical, synthetic and improvisational. I argue that these three kinds are distributed unevenly over the affected fields according to the degree to which each primarily reports on its objects of study, interprets them or invents new genres of expression. The changes in the disciplines are of course incremental—old things done better, more thoroughly and so forth. But what requires our attention and effort is the refiguration of them, of disciplinarity itself and of the conflicted economies in which academic work is increasingly taking place. I conclude by recommending that the institutional structures we build for the digital humanities should reflect the nature of the practice as it has emerged in the last few decades.",What's going on?,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
279,http://dx.doi.org/10.1093/llc/fqn015,"{'string_id': '10.1093/llc/fqn015', 'id_scheme': 'DOI'}","This article presents an empirical evaluation of text classification methods in literary domain. This study compared the performance of two popular algorithms, naïve Bayes and support vector machines (SVMs) in two literary text classification tasks: the eroticism classification of Dickinson's poems and the sentimentalism classification of chapters in early American novels. The algorithms were also combined with three text pre-processing tools, namely stemming, stopword removal, and statistical feature selection, to study the impact of these tools on the classifiers’ performance in the literary setting. Existing studies outside the literary domain indicated that SVMs are generally better than naïve Bayes classifiers. However, in this study SVMs were not all winners. Both algorithms achieved high accuracy in sentimental chapter classification, but the naïve Bayes classifier outperformed the SVM classifier in erotic poem classification. Self-feature selection helped both algorithms improve their performance in both tasks. However, the two algorithms selected relevant features in different frequency ranges, and therefore captured different characteristics of the target classes. The evaluation results in this study also suggest that arbitrary feature-reduction steps such as stemming and stopword removal should be taken very carefully. Some stopwords were highly discriminative features for Dickinson's erotic poem classification. In sentimental chapter classification, stemming undermined subsequent feature selection by aggressively conflating and neutralizing discriminative features.",An evaluation of text classification methods for literary study,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
280,http://dx.doi.org/10.1093/llc/fqn016,"{'string_id': '10.1093/llc/fqn016', 'id_scheme': 'DOI'}","Thought processes are enhanced when ways are found to link external perception with internal mental processes by the use of graphic aids. Such aids range from scribbled diagrams to sophisticated linkages between thought, images, and text such as those employed by Leonardo da Vinci. These tools allow visual perception to be harnessed in the dynamic processes associated with the creation or discovery of new knowledge. Digital humanists are applying digital versions of these age-old tools in many areas of research, from the graphs generated by text analysis applications to virtual reality models of ancient buildings, methods known collectively as ‘digital visualization’. This article begins with a brief review of the current application of visualization in the digital humanities before moving on to establish a context for digital visualization within ‘traditional’ humanities scholarship. This provides a context for an examination of what is required in order to ensure that digital visualization work is performed with identifiable intellectual rigour. The London Charter is used as a case study for a possible framework for the development of appropriate methods and standards. Digital visualization as a scholarly methodology is discussed and demonstrated as being part of a continuum of established academic practice rather than something that is in some way new, ‘revolutionary’, or lacking in rigorous scholarly value.",Digital visualization as a scholarly activity,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
281,http://dx.doi.org/10.1093/llc/fqn017,"{'string_id': '10.1093/llc/fqn017', 'id_scheme': 'DOI'}","Although many digital humanities resources are being developed for online use, there is little understanding of why some become popular, whilst others are neglected. Through log analysis techniques, the LAIRAH project identified twenty-one popular and well-used digital humanities projects, and in order to ascertain the factors they had in common, which predisposed them to be well used, conducted in-depth interviews with the creators of these resources. This article presents the findings of the study, highlighting areas that developers should be aware of, and providing a set of recommendations for both funders and creators, which should ensure that a digital humanities resource will have the best possible chance of being used in the long term.",The master builders: LAIRAH research on good practice in the construction of digital humanities projects,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
282,http://dx.doi.org/10.1093/llc/fqn018,"{'string_id': '10.1093/llc/fqn018', 'id_scheme': 'DOI'}","This article focuses on the use of technologies traditionally associated with knowledge representation to express complex associations between entities in historical texts that have been marked up in XML, according to the Text Encoding Initiative guidelines. In particular, we describe our exploration of the potential role of an ontology in facilitating the interpretation of implicit and hidden associations in the sources of interest, examining its use, and limits in a digital humanities project in connection with editing tools and delivery issues. We demonstrate our findings based on the Henry III Fine Rolls project, where an ontology—built using the RDF (Resource Description Framework)/OWL (Web Ontology Language) technologies—is being developed to make explicit information about person, place, and subject entities marked up as instances in the core texts themselves. For any historian, there is a natural tension between primary sources (as documentary records) and the analysis that produces a context for interpretation. We will argue that the combination of core mark-up (encoded in TEI) and an ontology (in RDF/OWL) provides a powerful model for representing the complexity of this tension and facilitates the necessarily dynamic process of scholarly interpretation.",Expressing complex associations in medieval historical documents: the Henry III Fine Rolls Project,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
283,http://dx.doi.org/10.1093/llc/fqn019,"{'string_id': '10.1093/llc/fqn019', 'id_scheme': 'DOI'}","As the use of data mining and machine learning methods in the humanities becomes more common, it will be increasingly important to examine implicit biases, assumptions, and limitations these methods bring with them. This article makes explicit some of the foundational assumptions of machine learning methods, and presents a series of experiments as a case study and object lesson in the potential pitfalls in the use of data mining methods for hypothesis testing in literary scholarship. The worst dangers may lie in the humanist's; ability to interpret nearly any result, projecting his or her own biases into the outcome of an experiment—perhaps all the more unwittingly due to the superficial objectivity of computational methods. We argue that in the digital humanities, the standards for the initial production of evidence should be even more rigorous than in the empirical sciences because of the subjective nature of the work that follows. Thus, we conclude with a discussion of recommended best practices for making results from data mining in the humanities domain as meaningful as possible. These include methods for keeping the the boundary between computational results and subsequent interpretation as clearly delineated as possible.",Meaning and mining: the impact of implicit assumptions in data mining for the humanities,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
284,http://dx.doi.org/10.1093/llc/fqn020,"{'string_id': '10.1093/llc/fqn020', 'id_scheme': 'DOI'}","The particular reading difficulties engendered by the complicated patterns of repetition in The Making of Americans by Gertrude Stein make it almost impossible to read this text in a traditional, linear manner. However, by visualizing certain patterns and looking at the text ‘from a distance’ through textual analytics and visualizations, we are enabled to make readings that were formerly inhibited. Initial analysis on Making within the MONK (metadata offer new knowledge) project (http://www.monkproject.org/) has yielded evidence which suggests that the text is intricately and purposefully structured. Using text mining to retrieve repetitive patterns and treating each as a single object makes it possible to visualize and compare the three dimensions upon which these repetitions co-occur—by length, frequency, and location—in a single view. Certainly, reading The Making of Americans in a traditional way appears to have yielded limited material for scholarly work, but reading the text differently, as an object of pairings or as parts of combinations, ultimately works in contrast to the supposition that the text is only meaningful to the extent that it defeats making meaning. A distant view of the text's structure allows us to read the text as an object that becomes, as it continues to turn in on itself with a centrifugal force, a whole history without beginning or ending.",'A thing not beginning and not ending': using digital tools to distant-read Gertrude Stein's The Making of Americans,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
285,http://dx.doi.org/10.1093/llc/fqn021,"{'string_id': '10.1093/llc/fqn021', 'id_scheme': 'DOI'}","Pliny is a piece of software that is meant to stimulate discussion within the Digital Humanities (DH) about how tools might be built that could find greater acceptance within the wider humanities community; something that has eluded the DH to date. Unlike many other tool projects within the DH, which are meant to show new and novel ways to apply technology to transform scholarly practice, Pliny is designed to support the act of conventional scholarly interpretation. It is meant to be a tool that blends so well into the task of the development of an interpretation, as scholars actually conventionally practice it, as to be almost invisible. In this, it follows some of the H-LAM/T design principles of Douglas Englebart, some of whose principles can be seen in software such as the word processor. In this article, several of the principle elements of conventional scholarly practice are described—centred on the act of annotation, notetaking, and the using of these notes as the basis for exploring ideas that emerge from working with the objects of study. Pliny's design is then discussed in the context of how aspects of its design—its affordances—support the scholar who is working with these elements. In particular, it illustrates an approach to the modelling of notes and associated ideas at the time when they are still largely un- or only partially structured.",Thinking about interpretation: Pliny and scholarship in the humanities,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
286,http://dx.doi.org/10.1093/llc/fqn022,"{'string_id': '10.1093/llc/fqn022', 'id_scheme': 'DOI'}","We present an algorithm as evidence of the possibility of a truly automated stylometric authorship attribution tool, based on committees of artificial neural networks. Neural networks have an advantage over traditional statistical stylometry in that they are inherently nonlinear, and therefore can consider nonlinear interactions between stylometric variables. The algorithm presented (1) is intended to demonstrate the feasibility of an automated approach using neural networks and (2) highlights important areas for further research. We present results of two separate test experiments—Shakespeare and Marlowe, and the Federalist Papers—as a demonstration of the method's; generality. In both cases, our algorithm produces committees that correctly predict the test works, without requiring the usual precursory statistical study to determine efficacious stylometric measures.",An algorithm for automated authorship attribution using neural networks,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
287,http://dx.doi.org/10.1093/llc/fqn023,"{'string_id': '10.1093/llc/fqn023', 'id_scheme': 'DOI'}","Dante's Monarchia, a fourteenth century treatise on political theory which survives in 20 manuscripts and the editio princeps, has been studied extensively by scholars using traditional analytical methods to establish textual transmission. It was selected as a suitable tradition for a blind study to test the application of computer-based phylogenetic methods to the stemmatic analysis of manuscript relationships. Our results show that these methods—maximum parsimony, NeighborNet and the Supernetwork algorithm—are capable of producing stemmata in very close agreement with those produced by traditional stemmatic analysis, including the identification of texts that change exemplar in the course of copying. The phylogenetic methods can correctly indicate the affiliations both before and after the point of exemplar change. The maximum chi-squared method (developed to detect recombination in DNA sequences) is able to indicate the region of exemplar change, allowing the precise location to be ascertained by textual analysis.",Dante's Monarchia as a test case for the use of phylogenetic methods in stemmatic analysis,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
288,http://dx.doi.org/10.1093/llc/fqn030,"{'string_id': '10.1093/llc/fqn030', 'id_scheme': 'DOI'}","This article attempts to ask some fundamental questions about editing in the digital age, and give some answers to these questions. It is argued that a concentration on digital methods, for themselves, may neglect the base questions facing any editor: why is the editor making this edition; from whom is the editor making this edition? Indeed, in some respects thinking about text encoding for digital purposes has been built on assumptions which are, for editors, simply wrong. In particular, the concept of what ‘text’ is, upon which (for instance) the Text Encoding Initiative principles are based (what Renear calls ‘realist’), is positivist, overconfident, simplistic and neglects the materiality of actual text instances. This view is opposed by what Renear calls ‘anti-realism’: texts do not have an independent existence, but are constructed by individual and collective acts of perception. In concrete terms, ‘anti-realism’ sees editions as made to serve the needs of the reader, as acts of interpretation and not as representations of some concrete reality: this is Pichler's view of the Wittgenstein transcripts, and the author's views of the Canterbury Tales project transcripts. However, it is argued that both realist and anti-realist extremes are dangerous: ‘realism’ can lead to editions which are arrogant and out-of-touch; anti-realism to editions which are reductionist and etiolated. In place of either extreme, we should substitute a different aim: to challenge readers to make new texts for themselves as they read, by finding new ways of presenting material so that both we editors and those who use our editions become better readers.","What text really is not, and why editors have to learn to swim",Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
289,http://dx.doi.org/10.1093/llc/fqn031,"{'string_id': '10.1093/llc/fqn031', 'id_scheme': 'DOI'}","The dank cellar surveys rather critically the litter of casualty electronic editions and the false bases and limited goals that informed so many early—that is, current—efforts; and it points hopefully to the best early, though still inadequate, efforts to provide electronic texts responsibly and with added scholarly value. It looks at some problems of representing Victorian fiction.",The dank cellar of electronic texts,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
290,http://dx.doi.org/10.1093/llc/fqn032,"{'string_id': '10.1093/llc/fqn032', 'id_scheme': 'DOI'}","Immersive multimedia performances, especially in the theater, installation art, and computer games, suggest to us interesting models for reconceiving the possibilities of textual editing in digital media. Traditionally, textual editions have taken different forms for different audiences of readers. Editing protocols, including the critical apparatus, are determined in part by those forms. Mostly this has meant conceiving of a given text as produced for a scholarly, classroom, or popular audience. However different these types of editions, they share familiar textual ontologies, developed primarily over the past 200 years and based on print technology. We suggest instead that editors begin thinking of digital editions primarily as ‘editorial environments’, with spatial, temporal, procedural, performative, and participatory properties. An electronic edition is always already a virtual world. A digital edition is an electronic environment. Citing as an example our experiment in the MOO with Shelley's sonnet ‘Ozymandias’, we imagine the role of the editor as textual ecologist/dramaturge/gamemaster, maximizing the resources of digital environments.",Editing Environments: The Architecture of Electronic Texts,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
291,http://dx.doi.org/10.1093/llc/fqn033,"{'string_id': '10.1093/llc/fqn033', 'id_scheme': 'DOI'}","Digital modes of editing ask us to re-examine the past century of editorial theory and to situate emerging editorial approaches within this history. Using the computer as a new textual medium has brought about a renewed interest in the conditions for representation. This article concerns itself with how books and computers, respectively, represent texts, and how critical editing mediates or organizes those representations. It was written in 1997 as a critical response to J.J. McGann's essay ‘The Rationale of Hypertext’.","Material text, immaterial text, and the electronic environment",Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
292,http://dx.doi.org/10.1093/llc/fqn034,"{'string_id': '10.1093/llc/fqn034', 'id_scheme': 'DOI'}","The mutability of electronic editions confronts editors with a new world, in which large parts of current editorial theory must be re-thought, based as it often is on assumptions based on the properties of paper editions. Software can adapt more easily than paper to the needs and interests of the reader, which means many choices about the selection of information in an edition and its presentation to the reader no longer need to be fixed for all time, but can be left open for the reader. Software also tends to have a very short lifetime compared to paper; in order to remain usable for more than a few years, electronic editions must find ways of representing the essential information of the edition in software-independent, non-proprietary ways.",How to teach your edition how to swim,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
293,http://dx.doi.org/10.1093/llc/fqn035,"{'string_id': '10.1093/llc/fqn035', 'id_scheme': 'DOI'}","While letters and correspondence materials serve as (in)valuable sources of information for historians, philologists, (socio-)linguists, biographers, and textual critics, modern editorial theory merely assigns them a secondary role. Contrary to this traditional documentary view, the authors of this article argue for a treatment of epistolary materials as primary sources in their own right. They propose a generalized text-base approach of encoded and annotated correspondence materials that can accomodate the generation of versatile user-driven electronic editions. This approach needs to address current lacunae in markup theory and practice, resulting in a lack for either provisions for the encoding of letter-specific phenomena in texts, or encoding features for such generative editions. A closer look at broader editorial theories reveals a deeper lack of understanding of the nature and hence definition of correspondence materials. The authors propose a Jakobsonian communicative definition of letters that to a great deal can be mapped onto the textual model of the Text Encoding Initiative (TEI). The second part of this article discusses the motivation for and practical realization of Digital Archive of Letters in Flanders (DALF), a formal framework for encoding correspondence materials which is defined as a TEI customization. Its most important features for capturing detailed metadata as well as letter-specific source phenomena are analysed and discussed against the text-ontological background sketched out before.","Describing, transcribing, encoding, and editing modern correspondence material: a textbase approach",Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
294,http://dx.doi.org/10.1093/llc/fqn036,"{'string_id': '10.1093/llc/fqn036', 'id_scheme': 'DOI'}","The concept of data in the humanistic academy carries a heavy cultural freight: as a reductionist yet efficient representation of complex textual significance. Far from being an invention of the digital age, this conception of the role of quantification has a prehistory whose terms continue to resonate in modern debates about digital editing and digitally mediated scholarship. This essay explores these terms and the anxieties they reflect, concluding that digital representation is no less textually and methodologically rich, and no less a production of knowledge, than its print counterpart.",Data and Wisdom: Electronic Editing and the Quantification of Knowledge,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
295,http://dx.doi.org/10.1093/llc/fqn038,"{'string_id': '10.1093/llc/fqn038', 'id_scheme': 'DOI'}","Digital editions have some distinct features that are not present in digital libraries. Therefore it is somewhat worrisome that there are far more digital libraries than digital editions. This essay argues that the reason for this is not only a pressure towards all-inclusiveness but also the fact that scholarly editions are addressing both scholars and common readers, each of them having their own expectations of what a digital edition should actually offer. The essay suggests that we should get away from the idea of access to data as the principal merit of the edition and suggests a model of criticism instead, meaning that editors should represent their work as providing critical points of view on the texts they are offering, with their actual contents thrown in.",Access,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
296,http://dx.doi.org/10.1093/llc/fqn039,"{'string_id': '10.1093/llc/fqn039', 'id_scheme': 'DOI'}","This article revisits the question of the intellectual adequacy of the print critical edition. Contemporary theory and current digital practice have encouraged editors and users of editions to dismiss various aspects of the print critical edition–particularly the reading text and the critical apparatus–as artifacts of an obsolete technology. Using database theory, the author shows how many of these basic elements in fact represent the most intellectually efficient possible way of organizing information about texts and the readings of their underlying witnesses. By recognizing the inherent sophistication of the classical model, digital editors can improve of print practice by exploiting features of the new medium that make it easier to present such data in interactive ways.",Back to the future: what digital editors can learn from print editorial practice,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
297,http://dx.doi.org/10.1093/llc/fqn040,"{'string_id': '10.1093/llc/fqn040', 'id_scheme': 'DOI'}","Mormon prophet Joseph Smith (1805–44) claimed that more than two-dozen ancient individuals (Nephi, Mormon, Alma, etc.) living from around 2200 BC to 421 AD authored the Book of Mormon (1830), and that he translated their inscriptions into English. Later researchers who analyzed selections from the Book of Mormon concluded that differences between selections supported Smith's claim of multiple authorship and ancient origins. We offer a new approach that employs two classification techniques: ‘delta’ commonly used to determine probable authorship and ‘nearest shrunken centroid’ (NSC), a more generally applicable classifier. We use both methods to determine, on a chapter-by-chapter basis, the probability that each of seven potential authors wrote or contributed to the Book of Mormon. Five of the seven have known or alleged connections to the Book of Mormon, two do not, and were added as controls based on their thematic, linguistic, and historical similarity to the Book of Mormon. Our results indicate that likely nineteenth century contributors were Solomon Spalding, a writer of historical fantasies; Sidney Rigdon, an eloquent but perhaps unstable preacher; and Oliver Cowdery, a schoolteacher with editing experience. Our findings support the hypothesis that Rigdon was the main architect of the Book of Mormon and are consistent with historical evidence suggesting that he fabricated the book by adding theology to the unpublished writings of Spalding (then deceased).",Reassessing authorship of the Book of Mormon using delta and nearest shrunken centroid classification,Oxford University Press (OUP),2008,,Literary and Linguistic Computing,23,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
298,http://dx.doi.org/10.1093/llc/fqn043,"{'string_id': '10.1093/llc/fqn043', 'id_scheme': 'DOI'}","Indian alchemy, a branch of traditional Indian medicine (Āyurveda), has produced a corpus of texts that are difficult to date using regular philological techniques. This article describes a contents-based computational method that is capable of calculating the relative chronology of these texts. Central parts of alchemical literature are encoded in a language model that can be understood by a computer and then compared with an alignment algorithm. Phylogenetic trees derived from these alignments show regularities in the ordering of alchemical texts, and these may be interpreted as temporal patterns. Processing these patterns with a minimization algorithm, we are able to compute a relative chronology of the corpus, which is largely consistent with results obtained using traditional philological techniques.",A chronometric approach to Indian alchemical literature,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
299,http://dx.doi.org/10.1093/llc/fqn044,"{'string_id': '10.1093/llc/fqn044', 'id_scheme': 'DOI'}","This research1 evaluates the extent to which lexical diversity, measured by frequent content words, hapax legomena, and type-token ratios (TTRs), is dependent on three features of the genre of the oral Indo-Aryan cultic poetry represented by the literary corpus of the Ṛgveda (ca. 165,000 tokens): characteristic choice of subject matter, usage of refrains, and the attribution of hymns to distinct poetic collectives. Analysis of 255 texts of 200 tokens showed that hymns on popular topics and where refrains were attested have a significantly higher rate of high-frequency content words and a lower ratio of once-occurring types. A higher TTR is observed in the hymns of specific family origin. Complexity of genre can be interpreted as a result of different discourse strategies of the poets. Overall, conservative mythological texts are characterized by regularity in word usage. Occurrence of content words, in the entire corpus, with lexemes denoting ‘deities’ on the one side and ‘nature’ on the other is accounted for by the factor of semantics, which deals with the structure of narrative.",Lexical Diversity in a Literary Genre: A Corpus Study of the Rgveda,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
300,http://dx.doi.org/10.1093/llc/fqp001,"{'string_id': '10.1093/llc/fqp001', 'id_scheme': 'DOI'}","The purpose of this article is to introduce and explore forensic philology in the context of electronic text editing. Drawing primarily on the example provided by the development of a TEI P5 conformant edition of Hafgeirs saga Flateyings, an alleged Icelandic saga forgery attested in a single, unsigned eighteenth century paper manuscript, this discussion explains how literary, linguistic, and transmission-level interpretations can be employed to describe the saga text and to bear witness to its origin and transmission process. It further explains how encoding the metadata described in these interpretations beside the data described in (near)zero-level text can be accomplished without sacrificing the role of the manuscript as artefact and without sacrificing the appearance of the text as it occurs on the page.",The TEI as luminol: Forensic philology in a digital age,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
301,http://dx.doi.org/10.1093/llc/fqp002,"{'string_id': '10.1093/llc/fqp002', 'id_scheme': 'DOI'}","Given a collection of imperfect copies of a textual document, the aim of stemmatology is to reconstruct the history of the text, indicating for each variant the source text from it was copied. We describe an experiment involving three artificial benchmark data sets to which a number of computer-assisted stemmatology methods were applied. Contrary to earlier similar experiments, we propose and use a numerical criterion to evaluate all the solutions. Moreover, our primary data set is significantly larger than used before. The results suggest the superiority of two computer-assisted methods amongst those tested: the maximum parsimony method implemented in the PAUP* software package and a related compression-based method we have proposed in earlier work.",Evaluating methods for computer-assisted stemmatology using artificial benchmark data sets,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
302,http://dx.doi.org/10.1093/llc/fqp003,"{'string_id': '10.1093/llc/fqp003', 'id_scheme': 'DOI'}","We report on finished work in a project that is concerned with providing methods, tools, best practice guidelines, and solutions for sustainable linguistic resources. The article discusses several general aspects of sustainability and introduces an approach to normalizing corpus data and metadata records. Moreover, the architecture of the sustainability platform implemented by the authors is described.","Sustainability of annotated resources in linguistics: A web-platform for exploring, querying, and distributing linguistic corpora and other resources",Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
303,http://dx.doi.org/10.1093/llc/fqp005,"{'string_id': '10.1093/llc/fqp005', 'id_scheme': 'DOI'}","For the purposes of large-scale analysis of XML/SGML files, converting humanities texts into a common form of markup represents a technical challenge. The MONK (Metadata Offer New Knowledge) Project has developed both a common format, TEI Analytics (a TEI subset designed to facilitate interoperability of text archives) and a command-line tool, Abbot, that performs the conversion. Abbot relies upon a new technique, schema harvesting, developed by the author to convert text documents into TEI-A. This article has two aims: first, to describe the TEI-A format itself and, second, to outline the methods used to convert files. More generally, it is hoped that the techniques described will lead to greater interoperability of text documents for text analysis in a wider context.",TEI Analytics: converting documents into a TEI format for cross-collection text analysis,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
304,http://dx.doi.org/10.1093/llc/fqp006,"{'string_id': '10.1093/llc/fqp006', 'id_scheme': 'DOI'}","This article describes work undertaken by the VERA project to investigate how archaeologists work with information technology (IT) on excavation sites. We used a diary study to research the usual patterns of behaviour of archaeologists digging the Silchester Roman town site during the summer of 2007. Although recording had previously been undertaken using pen and paper, during the 2007 season a part of the dig was dedicated to trials of IT and archaeologists used digital pens and paper and Nokia N800 handheld PDAs to record their work. The goal of the trial was to see whether it was possible to record data from the dig whilst still on site, rather than waiting until after the excavation to enter it into the Integrated Archaeological Database (IADB) and to determine whether the archaeologists found the new technology helpful. The digital pens were a success, however, the N800s were not successful given the extreme conditions on site. Our findings confirmed that it was important that technology should fit in well with the work being undertaken rather than being used for its own sake, and should respect established work flows. We also found that the quality of data being entered was a recurrent concern as was the reliability of the infrastructure and equipment.",iTrench: A study of user reactions to the use of information technology in field archaeology,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
305,http://dx.doi.org/10.1093/llc/fqp007,"{'string_id': '10.1093/llc/fqp007', 'id_scheme': 'DOI'}","The Henry III Fine Rolls project is a collaborative project between the National Archives in the UK, the departments of History and the Centre for Computing in the Humanities at King's College London, and the department of History and American Studies at Canterbury Christ Church University. Its aim is to produce a digital and print edition of the Fine Rolls from the reign of the 13th-century English King Henry III (1216–72). At the core of the resource are the translated summaries of the fine rolls which have been encoded in TEI XML, complemented by an overarching RDF/OWL conceptual model and digital facsimiles. In this article, we reflect on the ontological complexities of a dual publication, by bringing together various theoretical frameworks. Our aim is to take inspiration from these theories and connect them to the experience of producing two objects of different materiality but of very close scope. Ultimately, we will also explain how some of these reflections have been used to design a study for evaluating the utility of this edition.",Reflecting on a dual publication: Henry III Fine Rolls print and web,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
306,http://dx.doi.org/10.1093/llc/fqp008,"{'string_id': '10.1093/llc/fqp008', 'id_scheme': 'DOI'}","Annotated Facsimile Edition (AFED) is a high-level model for representing macro-level structure in digital facsimiles. AFED models a facsimile as a set of images with multiple orderings or collations. The structure of these collations are encoded by ‘annotations’ that define a range of images in the collation and describe the properties of the content object identified by the annotations (for example, chapter, paragraph, page, poem). Separate annotation streams encode multiple analytical perspectives, for example, the physical structure of the edition (volumes, pages, and lines) and the poetic structure (poems, titles, epigraphs, and stanzas). Annotations within a single analytical perspective—but not those from different perspectives—follow a hierarchical structure. We discuss our initial results in implementing AFED and using it to deploy a reading interface for AJAX enabled rich-client Web applications. The primary contribution of our work is a general-purpose model for representing digital facsimiles that focuses on the major conceptual structures present among the contents of documents drawn from a wide range of sources. AFED provides a highly flexible model that can serve as a substrate for developing tools designed to support visual document editing during the exploratory stages of scholarly research.",Annotated Facsimile Editions: Defining macro-level structure for image-based electronic editions,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
307,http://dx.doi.org/10.1093/llc/fqp009,"{'string_id': '10.1093/llc/fqp009', 'id_scheme': 'DOI'}","Given that the nature of research work involves computers and a variety of skills and expertise, Digital Humanities researchers are working collaboratively within their institutions and with others nationally and internationallly to undertake the research. This work typically involves the need to coordinate efforts between academics, undergraduate and graduate students, research assistants, computer programmers, librarians, and other individuals as well as the need to manage financial and other resources. Despite this use of collaboration, there has been little formal research on team development within this community. This article reports on a research project exploring the nature of Digital Humanities research teams. Drawing upon interviews with members of the community, a series of exemplary patterns and models of research collaboration are identified and outlined. Important themes include a definition of team which focuses on common tasks and outcomes as well as a need for responsibility and accountability to the team as a whole; elements of a successful team which include clear task definition and productive working relationships over the life of the project and beyond, a need for balance between digital and face-to-face communication and collaboration tools, and potential for more deliberate training in collaboration and team work. The article concludes with recommendations for the individual team members, project leaders, and teams.","'It's a team if you use ""reply all"" ': An exploration of research teams in digital humanities environments",Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
308,http://dx.doi.org/10.1093/llc/fqp010,"{'string_id': '10.1093/llc/fqp010', 'id_scheme': 'DOI'}","The content in information systems and virtual reconstructions in the cultural heritage sector is to a large degree directly based on information deduced from the study of texts. In many cases, even if the texts are available electronically, the links from the deduced facts to the original texts are not available and in many cases very costly to re-establish. Reproducibility of results is a core concept in text-based research as in all research. Thus, such links should be expressed explicitly in the systems and in accordance with the data standards developed in the fields of text encoding and conceptual modelling. To do this it is necessary to create a combined understanding of text encoding represented by the TEI guidelines and the understanding of conceptual models represented by initiatives like the CIDOC CRM and FRBRoo. In this article, we study a part of this complex by comparing the expressive power of the real world descriptions TEI P5 by mapping central parts of the CIDOC CRM onto TEI P5. It is clear that the TEI P5 has moved a great step in the direction towards an event-oriented model compared with TEI P4. Our use of CIDOC CRM as a yardstick shows that the expressiveness of TEI P5 can be greatly improved by extending the scope of very restricted elements like the relation element and adding a few new elements to the TEI.",TEI and cultural heritage ontologies: Exchange of information?,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
309,http://dx.doi.org/10.1093/llc/fqp012,"{'string_id': '10.1093/llc/fqp012', 'id_scheme': 'DOI'}","As libraries increasingly undertake digitization projects, it behooves us to consider the collection/capture, organization, preservation, and dissemination of all forms of documentation, including and beyond written text. While several libraries have funded projects which acknowledge the need to digitize other forms of text, few have extended the digital projects to include film, much less performed texts. Further, as more performing arts incorporate born-digital elements, use digital tools to create media-rich performance experiences, and look to the possibility for digital preservation of the performance text, the capture of the performance event and its born-digital artefacts must be considered. This article, then, presents a first look at the ARTeFACT project, undertaken at the University of Virginia Library in collaboration with an introductory course in Engineering and a student choreographer at Brenau University Women's College. Historical intersections of technology and dance are introduced, theoretical concerns of using technology in dance are considered, the processes involved in the creation, capture, and preservation of dance data are discussed along with the technologies used to produce an interactive dance performance.",Performance as digital text: Capturing signals and secret messages in a media-rich experience,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
310,http://dx.doi.org/10.1093/llc/fqp015,"{'string_id': '10.1093/llc/fqp015', 'id_scheme': 'DOI'}","Text Encoding Initiative (TEI) is an organization, a research community, and a markup language. Looking back into the history of these three TEIs, this article tries to describe what has been achieved and what its future challenges will be. The historical analysis is based on a closer look at the development of the TEI-L and topics covered by the Guidelines. A final section outlines possible roles of the TEI as an infrastructure for digital libraries and disciplinary virtual environments.",TEI in a crystal ball,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
311,http://dx.doi.org/10.1093/llc/fqp016,"{'string_id': '10.1093/llc/fqp016', 'id_scheme': 'DOI'}","This article describes the life cycle of a TEI Document within TextGrid, an eHumanities platform for scholarly text processing, in which structured search is based on the TEI framework and metadata with restricted values. A workbench is provided that offers tools for handling TEI documents, TextGridLab, making it easier to annotate, process, search, and persistently store new digitized texts. The digitization and annotation of the Campe dictionary1 serves as a first test bed. The overall framework of TextGrid is very generic and can handle different types of text (literary editions, linguistic corpora, lexica) as well as heterogeneous data formats (plain text, XML/TEI, images). In fact, the TextGrid repository, TextGridRep, is designed as a digital virtual library over federated archives, where humanities projects are invited to participate. Sharing of data is enabled by means of a grid-based architecture. Specifically the middleware includes most of the treatment of authorization, search, and file management. TextGrid is entirely based on open source software including Eclipse2 and Globus Toolkit.",TEI documents in the grid,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
312,http://dx.doi.org/10.1093/llc/fqp017,"{'string_id': '10.1093/llc/fqp017', 'id_scheme': 'DOI'}","The TEI Consortium has taken on the task of maintaining the Guidelines for Electronic Text Encoding and Interchange. This article describes how the latest major revision to these Guidelines was developed over the course of >6 years by the members of the TEI Technical Council and workgroups charged and overseen by the Council and gives background information and reasoning for the decisions taken. Among the new additions for P5, two of the most outstanding, the chapters on Names, Dates, People, and Places and on digital facsimiles are treated in some more detail. The article concludes with a brief account of the decisions made with respect to customization and conformance.",The making of TEI P5,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
313,http://dx.doi.org/10.1093/llc/fqp018,"{'string_id': '10.1093/llc/fqp018', 'id_scheme': 'DOI'}","The Text Encoding Initiative (TEI)1 has provided a complex and comprehensive system of provisions for scholarly text encoding. Although a major focus of the ‘digital humanities’ domain, and despite much teaching effort by the TEI community, there is a lack of teaching materials available, which would encourage the adoption of the TEI's recommendations and the widespread use of its text encoding guidelines in the wider academic community. This article describes the background, plans, and aims of the TEI by Example project, and why we believe it is a necessary addition to the materials currently provided by the TEI itself. The teaching materials currently available are not suited to the needs of self directed learners, and the development of stand alone, online tutorials in the TEI are an essential addition to the extant resources, in order to encourage and facilitate the uptake of TEI by both individuals and institutions.",Teaching TEI: The Need for TEI by Example,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
314,http://dx.doi.org/10.1093/llc/fqp019,"{'string_id': '10.1093/llc/fqp019', 'id_scheme': 'DOI'}","This article reports on the details behind a poster presented a the Text Encoding Initiative (TEI) Members' Meeting at the University of Maryland, College Park, in November 2007. It looks at the creation of of af scholarly electronic edition of a late-medieval play, The Conversion of Saint Paul from Bodleian MS Digby 133 using TEI P5 XML. In addition to exploring various new features available in the TEI P5 Guidelines, it also examines the methodology used to create the text, up-scaling from purely presentation markup to descriptive markup, and how this might simplify the creation of such editions. In an attempt to create an interoperable, flexible, and agile edition, it stores anything not directly related to the transcription of the text in separate files in a stand-off manner. In an attempt to experiment with creating a resource which leverages the advantages of networked editions, it documents the attempt to interoperate with the Middle English Dictionary. Although this first appears to be a failure, it highlights some of the inherent problems in attempting to build editions that are dependent on the resources of others. The article concludes with an urge to text encoders to make more of an effort to share examples of, both good and bad, community practice.",Converting Saint Paul: A new TEI P5 edition of The Conversion of Saint Paul using stand-off methodology,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
315,http://dx.doi.org/10.1093/llc/fqp020,"{'string_id': '10.1093/llc/fqp020', 'id_scheme': 'DOI'}","This article presents the results of the work on kundige bok, one of Göttingen's town records, containing late medieval town law. Due to the fact that this law was frequently subject to change, the text itself was revised over and over again, giving evidence for its frequent use and its dynamic nature. What has come to us, is, thus, a multi-layered text in which all layers represent a different (e.g. chronological) stage of the town law. Consequently they have to be regarded, processed and represented equally. A dynamic text like this requires a dynamic representation. The article shows how an electronic scholarly edition of a multi-layered text can be created and used, first, to reconstruct the genesis of the text; second, to make this evolution understandable, processable and visible; and third, with the text as a witness to display the development of urban law and urban life in the Late Middle Ages.This article: outlines the challenge of editing a multi-layered medieval manuscript;discusses why this leads to a new understanding of a critical edition of such a text; andintroduces the techniques used to create the electronic edition of kundige bok, in particular highlighting the linkage between the two dimensions of ‘text’and ‘time’ based on the TEI P5 scheme.",Reconstructing the textual evolution of a medieval manuscript,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
316,http://dx.doi.org/10.1093/llc/fqp021,"{'string_id': '10.1093/llc/fqp021', 'id_scheme': 'DOI'}","This article investigates the issues to be faced while producing a digital edition of a representative medieval text: Statuta Comunis Vicentie (1264). Statuta comunis are collections of civic rules very common in Northern Italy since the twelfth century. We are dealing specifically with the ones of Vicenza, a town near Venice. Statuta were usually organized in a single or multiple codices, including the collection of the civic rules, matched and allotted in big chapters (libri) according to the subject. Another fundamental characteristic is the constant review of the original text in different periods, generally due to changes in the government or in the organization of the city. Therefore, the most relevant matters to deal with are: first, description of metadata; second, structural analysis of the text; third, the markup of the additions and amendments; and fourth; identification of specific semantic values, in particular personal names, organizational names and names of places. This article outlines the reasons for choosing XML/TEI for the project, how to address the four matters listed above and how the chosen standard can be customized to treat the peculiar aspects of this text according to the traditional editing practice.",The digital edition of the Statuta comunis Vicentie of 1264,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
317,http://dx.doi.org/10.1093/llc/fqp022,"{'string_id': '10.1093/llc/fqp022', 'id_scheme': 'DOI'}","This article addresses the need for TEI display tools. In order to illustrate the need for display tools, we begin with a brief review of the tools that are currently available, summarizing in particular those listed on the TEI Wiki Tools page. We then turn to a discussion of our work on the development of the TEIViewer (http://teiviewer.org), a simple, JavaScript-driven, portable display tool designed to facilitate the online representation of and interaction with elements and attributes described within select modules of the TEI P5 Guidelines and encoded as layers of data and metadata in TEI-XML documents. We explain how the TEIViewer works by describing the interactions between the XML source layer, the display layer generated via XSL, and the interactive layer powered by jQuery and CSS; and we explain why we chose the jQuery JavaScript library to manage the Viewer's functionality as well as the advantages of this decision. Finally we describe current implementations and plans for release.",The TEIViewer: Facilitating the transition from XML to web display,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
318,http://dx.doi.org/10.1093/llc/fqp023,"{'string_id': '10.1093/llc/fqp023', 'id_scheme': 'DOI'}","Translation, adaptation, and other forms of appropriation of literary works can result in bodies of parallel texts. For the purpose of studying appropriation strategies, it is important to be able to annotate digital representations of these parallel text structures. This article uses early modern emblem culture (books of engravings or woodcuts, accompanied by mottos and explanatory texts) to investigate the forms this text parallelism may take. It defines requirements for annotation definition and proposes a TEI (Text Encoding Initiative) extension to implement these requirements. In the proposed encoding scheme, TEI feature structures will be used for storing annotation information. This scheme should be useful for annotating parallel text structures as well as for other annotation tasks. The annotation scheme assumes the annotated texts are available in XML. If this is not the case (there is no electronic version of the text at all or perhaps only a facsimile) the article suggests the definition of a TEI proxy document. A TEI proxy document contains enough of the structural aspects of the texts to serve as a basis for attaching annotations to the text. Outside of the annotation context, proxy documents may serve as a basis for adding functionality to image-based editions.",Towards a TEI-based encoding scheme for the annotation of parallel texts,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
319,http://dx.doi.org/10.1093/llc/fqp024,"{'string_id': '10.1093/llc/fqp024', 'id_scheme': 'DOI'}","This article shows that the TEI tag set for feature structures can be adopted to represent a heterogeneous set of linguistic corpora. The majority of corpora is annotated using markup languages that are based on the Annotation Graph framework, the upcoming Linguistic Annotation Format ISO standard, or according to tag sets defined by or based upon the TEI guidelines. A unified representation comprises the separation of conceptually different annotation layers contained in the original corpus data (e.g. syntax, phonology, and semantics) into multiple XML files. These annotation layers are linked to each other implicitly by the identical textual content of all files. A suitable data structure for the representation of these annotations is a multi-rooted tree that again can be represented by the TEI and ISO tag set for feature structures. The mapping process and representational issues are discussed as well as the advantages and drawbacks associated with the use of the TEI tag set for feature structures as a storage and exchange format for linguistically annotated data.",SusTEInability of linguistic resources through feature structures,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
320,http://dx.doi.org/10.1093/llc/fqp025,"{'string_id': '10.1093/llc/fqp025', 'id_scheme': 'DOI'}","Bilingual dictionaries are vital resources in many areas of natural language processing. Numerous methods of machine translation require bilingual dictionaries of large coverage, but less-frequent language pairs rarely have any digitalized resources of such kind. Since the need for these resources is increasing, but the human resources are scarce for less represented languages, efficient automatized methods are imperative. This article presents a fully automated, robust intermediate language-based bilingual dictionary generation method that uses the WordNet of the intermediate language to build a new bilingual dictionary. We propose the usage of WordNet in order to increase accuracy; we also introduce a bidirectional selection method with a flexible threshold to maximize recall. The evaluations showed 79% accuracy and 51% weighted recall, outperforming representative pivot language-based methods. A dictionary generated with this method will still need manual post-editing, but the improved recall and precision decrease the work of human correctors.",Dictionary generation for less-frequent language pairs using WordNet,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
321,http://dx.doi.org/10.1093/llc/fqp026,"{'string_id': '10.1093/llc/fqp026', 'id_scheme': 'DOI'}","The work of the Shakespeare Clinic of Claremont McKenna College, led by Ward E.Y. Elliott and Robert J. Valenza, is recognized for its pioneering computer analysis of many early modern texts to determine whether William Shakespeare (1564–1616) wrote the works traditionally ascribed to him. The Clinic achieved its primary objective of eliminating all other known candidates and thus confirming that Shakespeare wrote them. Two general methods of analysis were applied to whole plays and variable-sized large texts: Discrete Composite Analysis and Continuous Composite Analysis.. The first uses univariate analysis to determine acceptance or rejection of forty-eight stylometric tests for each text. The second uses a multi-dimensional composite mean for Shakespeare derived from all forty-eight in order to determine acceptance or rejection for each text. This article notes the omission of Discrete Analysis to take into consideration statistical dependencies between the forty-eight tests, the partly arbitrary ‘handfitting’ of acceptance–rejection boundaries for each of the forty-eight tests, the failure to take into full account the factor of chronology, and the absence of discussion of the part played by prior probabilities as to existing beliefs concerning attribution. By this last point, I mean the role played by the existing traditional consensus as to Shakespeare attribution, prior to linguistic analysis. For Continuous Analysis, it is noted that the stated probabilities are not true probabilities as acknowledged, and that the resulting acceptance–rejection levels for them are calibrated in line with prior beliefs. Principal component analysis is shown to give improved results in dealing with co-authored Shakespeare plays, Henry VIII, Timon of Athens, and Pericles. This does not invalidate the overall aim of the Shakespeare Clinic.",Untangling the derivatives: points for clarification in the findings of the Shakespeare Clinic,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
322,http://dx.doi.org/10.1093/llc/fqp027,"{'string_id': '10.1093/llc/fqp027', 'id_scheme': 'DOI'}","The dangers of computational approaches to authorship attribution in the absence of an adequate set of training texts for the claimant authors are well known. This study aims to show, however, that significant progress can be made even where conditions are quite problematic. We investigate a difficult authorship question involving three texts, ostensibly by three authors, each of whom wrote nothing else. Only one of the texts can be unquestionably ascribed to a known author, and this author has been suggested as the true author of one of the two remaining texts. We investigate these three texts, along with similar texts by other authors, using cluster analysis, Delta analysis, t-testing, and PCA. We also create simulations of our authorship problem using sets of three texts of known authorship by one, two, and three authors. We test these sets using correct and incorrect assumptions of authorial difference, and then compare the results with analyses of our three texts based on the same range of assumptions. By combining information from all of these tests, we achieve what we believe is a persuasive, if not conclusive, solution to a significant and long-standing question concerning the authorship of Maria Warda's violently anti-Mormon Female Life Among the Mormons. At the same time, we demonstrate methods for making progress in cases where conditions are less than ideal.",An exercise in non-ideal authorship attribution: the mysterious Maria Ward,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,24,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
323,http://dx.doi.org/10.1093/llc/fqp030,"{'string_id': '10.1093/llc/fqp030', 'id_scheme': 'DOI'}","Part I of this series, [doi:10.1093/llc/fqp029], applied our ‘new-optics’ methodology to the ‘Shakespeare’ scenes in STMO and concluded that it had too much Shakespeare discrepancy to fit comfortably into the Canon. We considered it an improbable, but not impossible Shakespeare ascription for the 1600s and placed it for now in the High Apocrypha. We thought it extremely improbable that the whole of STMO could be by Shakespeare, or that the ‘Shakespeare’ parts could have been written in the 1590s. Part II, published here, addresses the ‘Shakespeare’ scenes of Edward III. Taken separately, four of the five ‘Shakespeare’ blocks of Edw3 fall inside our Shakespeare ballpark. So does a sixth block, scenes 4.05–4.09. If we followed the consensus strictly, all five Shakespeare blocks, taken as a group, would not make a probable solo Shakespeare ascription. However, if we switched 4.04 to ‘non-Shakespeare,’ and 4.05–4.09 to ‘Shakespeare,’ the revised Shakespeare blocks would be a plausible Shakespeare ascription even as a group, justifying the inclusion of Edw3 in the Canon as partly Shakespeare's: 1.02; 2.01–2.02; and 4.05–4.09. The odds that the ‘non-Shakespeare’ scenes, collectively, or individually (except for 4.05–4.09) could be his are vanishingly low. The full article may be found online at http://www.claremontmckenna.edu/facultysites/govt/ FacMember/welliott/UTConference/2ToughNuts.pdf",Two tough nuts to crack: did Shakespeare write the 'Shakespeare' portions of Sir Thomas More and Edward III? Part II: Conclusion,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,25,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
324,http://dx.doi.org/10.1093/llc/fqp031,"{'string_id': '10.1093/llc/fqp031', 'id_scheme': 'DOI'}","Linguists studying grammar often describe their models using a syntax tree. Drawing a syntax tree involves the depiction of a rooted tree with additional syntactic features using specific domain conventions. TreeForm assists users in developing syntax trees, complete with movement lines, coreference, and feature association, in order to explore their syntactic theories and explain them to their colleagues. It is a drag-and-drop alternative to LaTeX and labelled bracket notation tools already available, which many linguists find difficult to use. We compare the output of TreeForm to those existing tools and show that it is able to better respect the conventions of the domain. We assess how easily linguists learn to use TreeForm through a series of cognitive walkthroughs. Our reviews find that TreeForm is a viable alternative to existing tools.",TreeForm: Explaining and exploring grammar through syntax trees,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,25,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
325,http://dx.doi.org/10.1093/llc/fqp032,"{'string_id': '10.1093/llc/fqp032', 'id_scheme': 'DOI'}","There has been dramatic growth in information communication technologies (ICT) infrastructure for the arts and humanities research community in recent years in the UK and elsewhere. No domain-wide survey of how researchers are using ICT and what they perceive their future needs to be has been undertaken previously and consequently what is needed in terms of a generic ICT infrastructure to support arts and humanities research is not well understood. The RePAH (Research Portals for the Arts and Humanities) Project is an AHRC funded study into the user needs for information portals to support research in the Arts and Humanities. It uses a combination of questionnaires, focus groups and Delphi opinion gathering, combined with server log-analysis data, to identify users’ information discovery strategies, Internet usage patterns, awareness and attitudes towards current services and technologies and responses to what future portal developments can deliver. Responsibility for funding this kind of infrastructure is split between a number of different agencies. This makes sector-wide information gathering for strategic planning and development difficult. The results of this study may help providers to understand where the priorities lie for the arts and humanities research community and help users to appreciate some of the possibilities within their grasp.",Research portals in the arts and humanities,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,25,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
326,http://dx.doi.org/10.1093/llc/fqp033,"{'string_id': '10.1093/llc/fqp033', 'id_scheme': 'DOI'}","The authors have worked over several years on a software tool to make word counts from an archive of old-spelling early modern English plays and poems. In this article we present the outcome, a computational model for dealing automatically with variant spelling, implemented in an application which we call an ‘Intelligent Archive’. We also reflect on the perspective on Early Modern English, and on the probabilistic aspect of language in general, gained from working through the practical problems which arose in establishing the model.","Old spellings, new methods: automated procedures for indeterminate linguistic data",Oxford University Press (OUP),2009,,Literary and Linguistic Computing,25,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
327,http://dx.doi.org/10.1093/llc/fqp034,"{'string_id': '10.1093/llc/fqp034', 'id_scheme': 'DOI'}","The article examines how the etymological composition of the Sanskrit lexicon is influenced by time and whether this composition can be used to date Sanskrit texts automatically. For this purpose, statistical tests are applied to a corpus of lexically analyzed texts. Results reported in the article may contribute to the diachronic lexicography of Sanskrit and help to develop computational methods for analyzing anonymous and undated Sanskrit texts.",Etymological trends in the Sanskrit vocabulary,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,25,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
328,http://dx.doi.org/10.1093/llc/fqp035,"{'string_id': '10.1093/llc/fqp035', 'id_scheme': 'DOI'}","Using statistically derived keywords to characterize texts has become an important research method for digital humanists and corpus linguists in areas such as literary analysis and the exploration of genre difference. Keywords—and the associated concepts of ‘keyness’ and ‘key-keyness’—have inspired conferences and workshops, many and varied research papers, and are central to several modern corpus processing tools. In this article, we present evidence that (at least for the task of biographical sentence classification) frequent words characterize texts better than keywords or key-keywords. Using the naïve Bayes learning algorithm in conjunction with frequency-, keyword-, and key-keyword-based text representation to classify a corpus of biographical sentences, we discovered that the use of frequent words alone provided a classification accuracy better than either the keyword or key-keyword representations at a statistically significant level. This result suggests that (for the biographical sentence classification task at least) frequent words characterize texts better than keywords derived using more computationally intensive methods.",Mining a corpus of biographical texts using keywords,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,25,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
329,http://dx.doi.org/10.1093/llc/fqp036,"{'string_id': '10.1093/llc/fqp036', 'id_scheme': 'DOI'}","This article provides quantitative evidence for a hypothesis concerning fourth-century translations of Indian Buddhist texts from Prakrit and Sanskrit into Chinese. Using a Variable Length n-Gram Feature Extraction Algorithm, principal component analysis and average linkage clustering we are able to show that 24 sutras, attributed by the tradition to different translators, were in fact translated by the same translator or group of translators. Since part of our method is based on assigning weight to n-grams, the analysis is capable of yielding distinctive features, i.e. strings of Chinese characters, that are characteristic of the translator(s). This is the first time that these techniques have successfully been applied to medieval Chinese texts. The results of this study open up a number of new directions for the lexicographic and syntactic study of early Chinese translations of Buddhist texts.",Quantitative evidence for a hypothesis regarding the attribution of early Buddhist translations,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,25,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
330,http://dx.doi.org/10.1093/llc/fqp037,"{'string_id': '10.1093/llc/fqp037', 'id_scheme': 'DOI'}","New technologies have always influenced communication, by adding new ways of communication to the existing ones and/or changing the ways in which existing forms of communication are utilized. This is particularly obvious in the way in which computer-mediated communication (CMC) has had an impact on communication. In this exploratory article, we are concerned with some characteristics of a newly evolving form of Spanish Internet orthography that differ from standard Spanish spelling. Three types of deviations from ‘the norm’ are considered: a reduction (post-vocalic d/[ð] deletion in -ado), a transformation (namely the spelling change from ch to x), and reduplication (of characters). Based on a corpus of approximately 2.7 million words of regionally balanced informal internet Spanish compiled in 2008, we describe the spelling changes and discuss a variety of sometimes interacting factors governing the rates of spelling variants such as overall frequency effects, functional (pragmatic, sociolinguistic, and iconicity-related) characteristics, and phonological constraints. We also compare our findings to data from Mark Davies's (2002) Corpus del Español (100 million words, 1200s–1900s, http://www.corpusdelespanol.org) as well as other sources and relate them to the discussion of the register/genre of Internet language.",k dixez? A corpus study of Spanish Internet orthography,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
331,http://dx.doi.org/10.1093/llc/fqp038,"{'string_id': '10.1093/llc/fqp038', 'id_scheme': 'DOI'}","The Coruña Corpus (CC): a Collection of Samples for the Historical Study of English Scientific Writing is a project on which the MUSTE group has been working since 2003 in the University of A Coruña (Spain). It has been designed as a tool for the study of language change in English scientific writing in general as well as within the different scientific disciplines (excluding medicine) between 1650 and 1900. Its purpose is to facilitate investigation at all linguistic levels, although, in principle, phonology is not included among our intended research topics. At the same time, we believe that the CC is an excellent tool for the study of scientific register/style at particular moments in history: it also offers the researcher the chance to analyse how this ‘specific English’ behaves from a synchronic point of view. To allow for socio-linguistic research using these scientific texts, we have included, when possible, some personal details about the author of each sample and, even, about the work from which the sample has been extracted in a separate file. From a technical point of view, all the texts have been keyed in following the Text Encoding Initiative conventions and saved in the XML format. The use of an extended mark-up language will make wide distribution and exploitation possible. Moreover, in order to retrieve information from the compiled data, we have decided to create a corpus management tool. Loosely speaking, the Coruña Corpus Tool is an Information Retrieval system, where the indexed textual repository is a set of compiled documents that constitutes the CC.",CETA in the Context of the Coruna Corpus,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,25,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
332,http://dx.doi.org/10.1093/llc/fqp039,"{'string_id': '10.1093/llc/fqp039', 'id_scheme': 'DOI'}","Although vast amounts of authentic materials are now available online to help language learners build up vocabulary and language skills in many languages, the consumer is almost ‘spoiled for choice’, and often at a loss where to begin. This article aims to provide an overall menu of language- learning and teaching websites, to help both students and teachers select more useful Computer-Assisted Language Learning (CALL) sites and programs, showing how to combine them into an effective online reading and vocabulary learning program for either classroom- or self-access. It will help both kinds of users to be able to better filter through the rivers of online data, to find and focus on what we will refer to as the ‘CALL or ESL/EFL gold’, meaning websites most content- and media-rich for English as a Second/Foreign Language teaching or learning. Though our discussion will be in English mostly about using these sites to improve English learning, many of the sites included in this framework can be used for learning as many as 140 major languages of the world. While most language learners are still using more traditional classrooms and textbooks, CALL-based instruction is growing rapidly. The practical problem for both teachers and students who have such resources, however, is to find some practical tool to plow through the plethora of online data, useful in helping schools to make a sensible CALL system to help students learn language and vocabulary most enjoyably and effectively. This article begins to suggest how to construct such an integrated CALL program, including many well-designed sites that combine the advantages of using authentic materials with online tools to help simplify them and provide various kinds of language learning support that can aid both students and teachers. Ten established research-based goals for L2 reading are integrated into a proposed framework for using online reading programs in a way that follows a clear ‘Taxonomy of Vocabulary Development (Deeper Lexical Processing)’.",Constructing a roadmap to more systematic and successful online reading and vocabulary acquisition,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,25,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
333,http://dx.doi.org/10.1093/llc/fqp040,"{'string_id': '10.1093/llc/fqp040', 'id_scheme': 'DOI'}","Twenty-eight items in the British Magazine have been attributed to Goldsmith, the majority in Essays and Criticisms by Dr. Goldsmith (1798). The strength of the external evidence depends on the authority of that collection; it is shakier than its editor claimed. Internal evidence comes from a Goldsmith ‘profile’ of stylistic features, established by reference to contemporary essayists, from measures of sentence-length, distinctive features of the attributed essays, and parallels of word and thought. We conclude that most of the attributed essays, including those proposed since 1798, are unlikely to be Goldsmith's.",Goldsmith and the 'British Magazine': A reconsideration,Oxford University Press (OUP),2009,,Literary and Linguistic Computing,25,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
334,http://dx.doi.org/10.1093/llc/fqq001,"{'string_id': '10.1093/llc/fqq001', 'id_scheme': 'DOI'}","We compare and benchmark the performance of five classification methods, four of which are taken from the machine learning literature, in a classic authorship attribution problem involving the Federalist Papers. Cross-validation results are reported for each method, and each method is further employed in classifying the disputed papers and the few papers that are generally understood to be coauthored. These tests are performed using two separate feature sets: a “raw” feature set containing all words and word bigrams that are common to all of the authors, and a second “pre-processed” feature set derived by reducing the raw feature set to include only words meeting a minimum relative frequency threshold. Each of the methods tested performed well, but nearest shrunken centroids and regularized discriminant analysis had the best overall performances with 0/70 cross-validation errors.",A comparative study of machine learning methods for authorship attribution,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
335,http://dx.doi.org/10.1093/llc/fqq002,"{'string_id': '10.1093/llc/fqq002', 'id_scheme': 'DOI'}","The use of corpus material and methods represents a major methodological innovation in Chinese historical linguistics. The very exciting findings uncovered in this article may be seen as the first systematic large-scale investigation of the various morpho-syntactic patterns underpinning the evolution of Chinese lexis. In this article, we have made a ground-breaking investigation into the diverse lexical modes and patterns which have emerged and developed in each major period in Chinese history, in which the generation of corpus linguistic data and the subsequent computational statistical modelling have been essential.",A corpus-based study of lexical periodization in historical Chinese,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
336,http://dx.doi.org/10.1093/llc/fqq003,"{'string_id': '10.1093/llc/fqq003', 'id_scheme': 'DOI'}","Categorization and taxonomy are topical issues in intertextuality studies. Instead of increasing the number of overlapping or contradictory definitions (often established with reference to limited databases) which exist even for key concepts such as “allusion ” or “quotation”, we propose an electronically implemented data-driven approach based on the isolation, analysis and description of a number of relevant parameters such as general text relation, marking for quotation, modification etc. If a systematic parameter analysis precedes discussions of possible correlations and the naming of features bundles as composite categories, a dynamic approach to categorization emerges which does justice to the varied and complex phenomena in this field. The database is the HyperHamlet corpus, a chronologically and generically wide-ranging collection of Hamlet references that confront linguistic and literary researchers with a comprehensive range of formal and stylistic issues. Its multi-dimensional encodings and search facilities provide the indispensable ‘freedom from the analytic limits of hardcopy', as Jerome McGann put it. The methodological and heuristic gains include a more complete description of possible parameter settings, a clearer recognition of multiple parameter settings (as implicit in existing genre definitions), a better understanding of how parameters interact, descriptions of disregarded literary phenomena that feature unusual parameter combinations and, finally, descriptive labels for the most polysemous areas that may clarify matters without increasing taxonomical excess.",A 'key to all quotations'? A corpus-based parameter model of intertextuality,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
337,http://dx.doi.org/10.1093/llc/fqq004,"{'string_id': '10.1093/llc/fqq004', 'id_scheme': 'DOI'}","We report on a significant correlation between lexical items containing complex vowels in their present day canonical forms, and prosodic-syntactic boundaries in Milton’s Paradise Lost, where all line terminals, whether end-stopped or run-on, plus line-medials with associated punctuation, constitute boundary tokens and equate to gold-standard phrase break annotations. Real-world knowledge of present day English pronunciation is projected onto each word token in two different versions, constituting two phrasing variants, of Book 1 of the poem via ProPOSEL, a prosody and part-of-speech English lexicon developed by the authors; and pertinent differences in place of articulation of English vowels in Milton’s day and ours are also discussed. The chi-squared test for independence returns a two-tailed P-value of less than 0.0001 for the association of this vowel subset and phrase breaks in both samples. This leads us to speculate that the poet’s unpremeditated use of complex vowels—which slow down verse movement in Paradise Lost and thus generate rhythmic junctures—may represent a phrasing device habitual not just to poets but to native English speakers in general. Concurrent work on a corpus of present-day British English speech corroborates our findings. Hence, complex vowels may constitute new predictive features in phrasing models for English.",Holy smoke: vocalic precursors of phrase breaks in Milton's Paradise Lost,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
338,http://dx.doi.org/10.1093/llc/fqq005,"{'string_id': '10.1093/llc/fqq005', 'id_scheme': 'DOI'}","Of all American literary mysteries The Diary of a Public Man has been perhaps the most perplexing. Set principally in Washington DC, it covers a short but critical period in the nation’s history, the secession winter of 1860–61. The Diary entries are dated during the last months of James Buchanan’s ill-fated administration and the first 2 weeks after Abraham Lincoln’s inauguration. The publisher refused to name the author yet, despite the Diary’s anonymity, it has been used and quoted by historians for more than a century. It is clearly the work of an exceptionally gifted writer. The Diarist pictures himself as a strong Union man, much worried whether the crisis can be resolved without resort to war. Naturally there has been much speculation as to the Diary’s authorship. This article describes how traditional and non-traditional methods of authorship attribution may be employed on the Diary, which we believe to have been written by William Henry Hurlbert. We argue that the joint interdisciplinary approach employed in this article should be the way in which attributional research is conducted. Information on the traditional attribution section of this article is adapted from the forthcoming book by Daniel W. Crofts, A Secession Crisis Enigma: William Henry Hurlbert and ‘The Diary of a Public Man’ (Louisiana University Press, 2010)",The diary of a public man: a case study in traditional and non-traditional authorship attribution,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
339,http://dx.doi.org/10.1093/llc/fqq006,"{'string_id': '10.1093/llc/fqq006', 'id_scheme': 'DOI'}","In this article, we review the process of building ODIN, the Online Database of Interlinear Text (http://odin.linguistlist.org) a multilingual repository of linguistically analyzed language data. ODIN is built from interlinear text that has been harvested from scholarly linguistic documents posted on the web. At the time of this writing, ODIN holds nearly 190,000 instances of interlinear text representing annotated language data for more than 1,000 languages (representing data from >10% of the world's languages). ODIN's charter has been to make these data available to linguists and other language researchers via search, providing the facility to find instances of language data and related resources (i.e. the documents from which data were extracted) by language name, language family, and even annotations used to markup the data (e.g. NOM, ACC, ERG, PST, 3SG). Further, we have sought to enrich the data we have collected and extract ‘knowledge’ from the enriched content. To enrich the data, we use a variety of statistical tagging and parsing methods applied in the English translations. An enhanced search facility allows users to find data across languages for a variety of syntactic constructions and constituent orders, facilitating unprecedented automated and online discovery of language data.",Developing ODIN: A Multilingual Repository of Annotated Language Data for Hundreds of the World's Languages,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
340,http://dx.doi.org/10.1093/llc/fqq007,"{'string_id': '10.1093/llc/fqq007', 'id_scheme': 'DOI'}","Embedded generalized markup, as applied by digital humanists to the recording and studying of our textual cultural heritage, suffers from a number of serious technical drawbacks. As a result of its evolution from early printer control languages, generalized markup can only express a document’s ‘logical’ structure via a repertoire of permissible printed format structures. In addition to the well-researched overlap problem, the embedding of markup codes into texts that never had them when written leads to a number of further difficulties: the inclusion of potentially obsolescent technical and subjective information into texts that are supposed to be archivable for the long term, the manual encoding of information that could be better computed automatically, and the obscuring of the text by highly complex technical data. Many of these problems can be alleviated by asserting a separation between the versions of which many cultural heritage texts are composed, and their content. In this way the complex interconnections between versions can be handled automatically, leaving only simple markup for individual versions to be handled by the user.",The inadequacy of embedded markup for cultural heritage texts,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
341,http://dx.doi.org/10.1093/llc/fqq008,"{'string_id': '10.1093/llc/fqq008', 'id_scheme': 'DOI'}","This article presents the design of an interoperable database for digital documentation of oral discourse genres in multiple languages. Focussing on stylistic form and cultural specificity of artistic expression, the categories of study that serve as data fields build on contextual and functional approaches to verbal art performance. The database is part of a larger project known as VOVA (VOcal and Verbal Arts archives) that seeks to create digital tools for editing and annotating stylized oral discourse for purposes of comparative study of oral traditions and the preservation of endangered languages. Detailed descriptions of fields and numerous examples of the type of data solicited by VOVA, taken from leading scholarship in the field, help to clarify the scientific aims of the project. Search modes for consulting the database are also provided. Relations between the symbol-making and symbol-using activities of language use, text editing, and the digital humanities are discussed in light of the anthropological and linguistic research that will serve as a basis for a systematic study of stylistics in speech.",Digital documentation of oral discourse genres,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
342,http://dx.doi.org/10.1093/llc/fqq011,"{'string_id': '10.1093/llc/fqq011', 'id_scheme': 'DOI'}","This article deals with the lemmatization of Middle Dutch literature. This text collection—like any other medieval corpus—is characterized by an enormous spelling variation, which makes it difficult to perform a computational analysis of this kind of data. Lemmatization is therefore an essential preprocessing step in many applications, since it allows the abstraction from superficial textual variation, for instance in spelling. The data we will work with is the Corpus-Gysseling, containing all surviving Middle Dutch literary manuscripts dated before 1300 AD. In this article we shall present a language-independent system that can ‘learn’ intra-lemma spelling variation. We describe a series of experiments with this system, using Memory-Based Machine Learning and propose two solutions for the lemmatization of our data: the first procedure attempts to generate new spelling variants, the second one seeks to implement a novel string distance metric to better detect spelling variants. The latter system attempts to rerank candidates suggested by a classic Levenshtein distance, leading to a substantial gain in lemmatization accuracy. This research result is encouraging and means a substantial step forward in the computational study of Middle Dutch literature. Our techniques might be of interest to other research domains as well because of their language-independent nature.",Weigh your words--memory-based lemmatization for Middle Dutch,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
343,http://dx.doi.org/10.1093/llc/fqq013,"{'string_id': '10.1093/llc/fqq013', 'id_scheme': 'DOI'}","Applications of authorship attribution `in the wild’ [Koppel, M., Schler, J., and Argamon, S. (2010). Authorship attribution in the wild. Language Resources and Evaluation. Advanced Access published January 12, 2010:10.1007/s10579-009-9111-2], for instance in social networks, will likely involve large sets of candidate authors and only limited data per author. In this article, we present the results of a systematic study of two important parameters in supervised machine learning that significantly affect performance in computational authorship attribution: (1) the number of candidate authors (i.e. the number of classes to be learned), and (2) the amount of training data available per candidate author (i.e. the size of the training data). We also investigate the robustness of different types of lexical and linguistic features to the effects of author set size and data size. The approach we take is an operationalization of the standard text categorization model, using memory-based learning for discriminating between the candidate authors. We performed authorship attribution experiments on a set of three benchmark corpora in which the influence of topic could be controlled. The short text fragments of e-mail length present the approach with a true challenge. Results show that, as expected, authorship attribution accuracy deteriorates as the number of candidate authors increases and size of training data decreases, although the machine learning approach continues performing significantly above chance. Some feature types (most notably character n-grams) are robust to changes in author set size and data size, but no robust individual features emerge.",The effect of author set size and data size in authorship attribution,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,26,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
344,http://dx.doi.org/10.1093/llc/fqq014,"{'string_id': '10.1093/llc/fqq014', 'id_scheme': 'DOI'}","This article examines the use of lexical bundles—repeated word groups of differing lengths—in two German Bible translations and analyzes their use in relation to the comparative readability of the two texts. The texts included in this study are the four Gospels and the book of Acts in Martin Luther’s classic translation of the Bible and the modern translation Hoffnung für alle (Hfa). The study is both quantitative and qualitative in nature, looking at lexical bundle statistics in aggregate and at specific uses of lexical bundles in context. The results indicate that the older translation used many more lexical bundles, but types that were used by the newer version were used more effectively. Overall, however, the older version has the advantage in readability due to its greater use of lexical bundles.",Lexical bundles and German bibles,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,26,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
345,http://dx.doi.org/10.1093/llc/fqq015,"{'string_id': '10.1093/llc/fqq015', 'id_scheme': 'DOI'}","A metrical system is the particular rhythm upon which a verse is structured. Classical Ancient Greek poetry demonstrates a wide variety of metrical systems, the most ancient one being the hexameter. Two of the longest and most famous poems of Ancient Greek poetry, namely Iliad and Odyssey, were composed in the hexameter by Homer. This system is named ‘hexameter’, because the verse is divided into six sections, and so is the rhythm of reciting it. Each section has a fixed scheme, thus it can have only two or three syllables in a predefined combination. The aim of this project was the development of a program that will automatically scan such a verse, by using the least possible computing and linguistic resources. The term ‘scansion’ denotes the discovery of the particular pattern of the metrical system of the verse. That is in which positions of the verse long syllables are located, in which positions are short, and how these syllables form the fixed scheme of every section of the verse. Words inside verses were carefully selected to conform to the above standards.",Computerized Scansion of Ancient Greek Hexameter,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,26,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
346,http://dx.doi.org/10.1093/llc/fqq016,"{'string_id': '10.1093/llc/fqq016', 'id_scheme': 'DOI'}","Vladimir Levenshtein’s edit distance algorithm is used to reveal disparities between delimiter stripped texts of the Senate amended Treaty of Fort Laramie with Sioux, etc., 1851 as corrected in a previous study, and of other federal copies of this transaction. All of the latter deviated markedly from that newly created version, reflecting errors of exclusion, of the absence in some transcripts of the Senate modification, of editorial decisions made by Charles J. Kappler during the preparation of his treaty compilations at the beginning of the twentieth century, and of spelling. These results confirmed that the instrument was until now never published in its complete formal state. This study may serve as a model for future text analyses that might benefit from the employment of Levenshtein’s metric.","Comparing nearly identical treaty texts: a note on the Treaty of Fort Laramie with Sioux, etc., 1851 and Levenshtein's edit distance metric",Oxford University Press (OUP),2010,,Literary and Linguistic Computing,26,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
347,http://dx.doi.org/10.1093/llc/fqq017,"{'string_id': '10.1093/llc/fqq017', 'id_scheme': 'DOI'}","We develop an aggregate measure of syntactic difference for automatically finding common syntactic differences between collections of text. With the use of this measure, it is possible to mine for differences between, for example, the English of learners and natives, or between related dialects. If formulated in advance, hypotheses can also be tested for statistical significance. It enables us to find not only absence or presence, but also under- and overuse of specific constructs. We have applied our measure to the English of Finnish immigrants in Australia to look for traces of Finnish grammar in their English. The outcomes of this detection process were analysed and found to be insightful. A report is included in this article. Besides explaining our method, we also go into the theory behind it, including permutation statistics, and the custom normalizations required for applying these tests to syntactical data. We also explain how to use the software we developed to apply this method to new corpora, and give some suggestions for further research.",Automatically Extracting Typical Syntactic Differences from Corpora,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,26,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
348,http://dx.doi.org/10.1093/llc/fqq018,"{'string_id': '10.1093/llc/fqq018', 'id_scheme': 'DOI'}","The Corpus of Contemporary American English is the first large, genre-balanced corpus of any language, which has been designed and constructed from the ground up as a ‘monitor corpus’, and which can be used to accurately track and study recent changes in the language. The 400 million words corpus is evenly divided between spoken, fiction, popular magazines, newspapers, and academic journals. Most importantly, the genre balance stays almost exactly the same from year to year, which allows it to accurately model changes in the ‘real world’. After discussing the corpus design, we provide a number of concrete examples of how the corpus can be used to look at recent changes in English, including morphology (new suffixes –friendly and –gate), syntax (including prescriptive rules, quotative like, so not ADJ, the get passive, resultatives, and verb complementation), semantics (such as changes in meaning with web, green, or gay), and lexis––including word and phrase frequency by year, and using the corpus architecture to produce lists of all words that have had large shifts in frequency between specific historical periods.",The Corpus of Contemporary American English as the first reliable monitor corpus of English,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
349,http://dx.doi.org/10.1093/llc/fqq019,"{'string_id': '10.1093/llc/fqq019', 'id_scheme': 'DOI'}","Most memory institutions are now engaging with digitizing holdings to provide online access. Although, recent developments in technology have allowed users to create high quality digital resources out with institutional boundaries, little consideration has been given to the potential contribution that the general public can make to digitizing our cultural heritage. This article seeks to scope the growing trend of the creation of amateur online museums, archives, and collections, and demonstrates that the best examples of this endeavor can teach best practice to traditional memory institutions in how to make their collections useful, interesting, and used by online communities.",Digital curiosities: resource creation via amateur digitization,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
350,http://dx.doi.org/10.1093/llc/fqq020,"{'string_id': '10.1093/llc/fqq020', 'id_scheme': 'DOI'}","Constructing readings of damaged and abraded ancient documents is a difficult, complex, and a time-consuming task. It frequently involves reference to a variety of linguistic and archaeological datasets and the integration of previous knowledge of similar documentary material. Due to the involved and lengthy reading process, it is often difficult to record and recall how the final interpretation of the document was reached and which competing hypotheses were presented, adopted, or discarded in the process of reading. This article discusses the development of the application called DUGA, which uses Decision Support System (DSS) technology to aid the day-to-day reading of damaged documents. Such an application will facilitate the process of transcribing texts by providing a framework in which scholars can record, track, and trace their progress. DUGA will include a word search facility of external resources such as the Vindolanda ink tablets through the knowledge base Web Service called APPELLO. This functionality will support the scholars through their reading process by suggesting words, which may confirm current interpretations or inspire new ones. Furthermore, DUGA will allow continuity between working sessions, and the complete documentation of the reading process, that has hitherto been implicit in published editions.",Towards a decision support system for reading ancient documents,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
351,http://dx.doi.org/10.1093/llc/fqq021,"{'string_id': '10.1093/llc/fqq021', 'id_scheme': 'DOI'}","In this article, we argue that, just as an edition of a book can be a means of reifying a theory about how books should be edited, so can the creation of an experimental digital prototype be understood as conveying an argument about designing interfaces. Building on this premise, we explore theoretical affinities shared by recent design and book history scholarship, and connect those theories to the emerging practice of peer-reviewing digital objects in scholarly contexts. We suggest a checklist for subjecting prototypes directly to peer review: Is the argument reified by the prototype contestable, defensible, and substantive?Does the prototype have a recognizable position in the context of similar work, either in terms of concept or affordances?Is the prototype part of a series of prototypes with an identifiable trajectory?Does the prototype address possible objections?Is the prototype itself an original contribution to knowledge?We also outline some implications for funding agencies interested in supporting researchers who are designing experimental computer prototypes. For instance, if a series of prototypes functions as a set of smaller arguments within a larger debate, it might be more appropriate to fund the sequence rather than treating each project as an individual proposal.",How a prototype argues,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
352,http://dx.doi.org/10.1093/llc/fqq022,"{'string_id': '10.1093/llc/fqq022', 'id_scheme': 'DOI'}","The sustainability of digital humanities research projects is a pressing issue for humanities computing. Currently, even well-established large digital projects like the Linguistic Atlas Project (LAP) are at future risk because funding and other resources are contingent on grant funding or faculty status of the director, neither of which will necessarily be available to maintain the project over time. The mission of the university library, however, includes archiving and dissemination, now increasingly of digital materials as well as traditional paper. Collaboration with the university library is the only realistic option for long-term sustainability of digital humanities projects in the current environment. Unlike paper collections, which only require secure storage, digital projects also require the means of adaptation to new electronic media and operating environments. Even data storage requires that materials from digital projects be included in library media refresh cycles, which will include transfer of old data to new media as technology develops. Projects like LAP should provide resources to assist the library in starting the project archive, including staff time, and funding for equipment. Project metadata must be provided and, to the extent possible, integrated with library systems and finding aids. Project staff will also need to maintain a Web presence and tools developed for the project. Such cooperation leads toward the development of a digital institutional repository, in which research results and tools may be maintained in the library, not just for the humanities but across many disciplines.",Library collaboration with large digital humanities projects,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
353,http://dx.doi.org/10.1093/llc/fqq024,"{'string_id': '10.1093/llc/fqq024', 'id_scheme': 'DOI'}","As scholarly publishing transitions from a static medium (paper) to a digital one, a necessary tension has emerged between a static, or archival, approach to content, and the dynamic, ever-evolving requirements of online resources. Two years after the release of the NINES website (the Networked Infrastructure for nineteenth-century Electronic Scholarship), the first online peer-reviewing organization and hub for the aggregation of scholarly resources in nineteenth-century studies, the development team decided to evaluate the site’s efficacy and update it accordingly. But because of NINES’s unique position as a federation of scholar-driven (mostly primary-source oriented) sites as well as a forward-thinking software developer, it was essential that any changes to the site remain true to the institution’s dual roles. This article explores the NINES redesign as an experiment in implementing usability studies and user-centered design to enhance its appeal within our community. This narrative is meant as a case study, one to be considered within the larger context of the questions and challenges faced by those creating, managing, and using digital projects.",Testing NINES,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
354,http://dx.doi.org/10.1093/llc/fqq026,"{'string_id': '10.1093/llc/fqq026', 'id_scheme': 'DOI'}","This article details a modeling methodology that is appropriate for historical, functional documents that are to be digitally represented and hosted within a software environment for humanities research. The functionality is derived from Use Case modeling that can be undertaken in consultation with the User Group. The Use Cases are an expression of the whole-system model as they embody the interaction of User, with the document, in the software environment. The encoding mechanism largely practiced within the humanities computing community is represented by the TEI, which seeks to provide a set of guidelines for encoding humanities documents. However, TEI offers no guidance in relation to creating an encoding of a document that is supportive of the software environment that will host it, the interaction mechanisms required, or the User. We argue that modeling with recourse to the Logical, the Physical and the Interaction classes enables not just the generation of an appropriate encoding scheme, but also the software to manipulate it. We situate Use Case methodology within Activity Theory and relate this to the humanities computing community. The argument is framed in relation to the creation of a digital edition of an 18th century Spanish Account Book manuscript.",Appropriate Use Case modeling for humanities documents,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,25,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
355,http://dx.doi.org/10.1093/llc/fqq027,"{'string_id': '10.1093/llc/fqq027', 'id_scheme': 'DOI'}","Moving from a traditional dialect geography research methodology to one in which data are processed electronically, and where visualization is used as a research tool, can be of great benefit to dialect geography. A working environment offering full support for using visualization as a research tool could take dialect geography into the era of e-Science. Despite the advent of electronic data processing, electronic publishing and Geographic Information Systems (GIS), an analysis of the most important computerized tools for dialect geography research suggests that there is little support for the use of modern data mining and analysis techniques connected to visualization for the analysis and interpretation of dialect data. In this article, we use the electronic publication of two major dialect dictionaries to illustrate the value of visualization as a research tool by showing how visual data mining and combining dialect data with independent data sets applies to dialect geography research. We argue that there is no need for large-scale software development because visualization, as a research tool, is supported to a large extent by geo-browsers such as ‘Google Earth’, which make it possible to flexibly combine and visualize different types of geo-referenced data.",Visualization as a research tool for dialect geography using a geo-browser,Oxford University Press (OUP),2010,,Literary and Linguistic Computing,26,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
356,http://dx.doi.org/10.1093/llc/fqq028,"{'string_id': '10.1093/llc/fqq028', 'id_scheme': 'DOI'}","Since the 1970s, the Regressive Imagery Dictionary (RID) has been widely used as a content analysis tool for both psychological and literary research on texts. Today, besides the original English version, it exists in translations for seven other languages. However, the wide-ranging validation studies conducted on the English version have mostly not been replicated for the various translations, hence the validity of these translations must rest for the time being on their concurrent validity with the English original. This article examines the concurrent validity of the German, Latin, and Portuguese translations of the RID. Taking the English RID as a de facto standard, it uses translations of the psalms (N = 150) to check how far the three translations of the RID correspond to the English original in identifying whether there is a significant dominance of primary or secondary process lexis in a text. Overall, compared against the English version, the Latin translation has 77.33% accuracy, the German translation 68%, and the Portuguese translation 56.67%. In terms of the sensitivity and specificity of classification, the Latin translation performs quite well on both measures; in contrast, the German translation is conservative, whilst the Portuguese translation is liberal.","The Regressive Imagery Dictionary: A test of its concurrent validity in English, German, Latin, and Portuguese",Oxford University Press (OUP),2010,,Literary and Linguistic Computing,26,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
357,http://dx.doi.org/10.1093/llc/fqq029,"{'string_id': '10.1093/llc/fqq029', 'id_scheme': 'DOI'}","The nearest shrunken centroid (NSC) methodology, originally developed for high-dimensional genomics problems, was recently applied in a stylometric study. Although NSC has many advantages, stylometric problems usually differ from genomics problems in several important ways: texts are of a wide range of sizes, a large series of texts are often the subjects for classification, and most importantly the set of candidate authors cannot usually be assumed to be closed. Consequently, naïve application of NSC methodology can produce misleading results. We extend the NSC methodology for more general application to stylometry. Reanalysis of the Book of Mormon using the open-set NSC method produced dramatically different results from a closed-set NSC analysis.",Extended nearest shrunken centroid classification: A new method for open-set authorship attribution of texts of varying sizes,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
358,http://dx.doi.org/10.1093/llc/fqr001,"{'string_id': '10.1093/llc/fqr001', 'id_scheme': 'DOI'}","Hongloumeng by Xueqin Cao (Hsueh-ch‘in Ts'ao) is generally considered one of the greatest classical Chinese novel. Of all nine published English translations known today, the one translated by Hawkes and Minford (the Story of the Stone, Penguin, 1973–86) and the other by Yang and Yang (A Dream of Red Mansions1, Foreign Languages Press in Beijing, 1978–80) are the best known among translators and literary scholars. Over the years, both have been carefully scrutinized and much critiqued. Translators and translation scholars have been engaged in heated debates over salient features of the translations, strategies employed by the translators, the possible effects of the two translations and so on [cf. Liu and Gu (1997) On translation of cultural contents in Hong Lou Meng [in Chinese]. Chinese Translators Journal, 1: 16–19; Wang (2001) A Comparative Study of the English Translations of Poetry in Hong Lou Meng. Xi’an: Shanxi Normal University Press; Feng (2006) On the Translation of Hong Lou Meng [in Chinese]. Shanghai: Shanghai Foreign Language Education Press; Liu (2008), Translating tenor: With reference to the English versions of Hong Lou Meng. Meta, 53(3): 528–48], with the eventual aim to determine which translation better captures the style of the original text or author. Like many debates of similar nature, no definitive conclusions have been reached despite such an intense interest. We believe a corpus-assisted examination [Baker, M. (2000). Towards a methodology for investigating the style of a literary translator. Target, 12(2): 241–66; Baker, M. (1993). Corpus linguistics and translation studies: Implications and applications. In Gill, F., Baker, M., and Tognini-Bonelli, E. (eds), Text and Technology: In Honour of John Sinclair. Amsterdam: Benjamins, pp. 233–50] of the two translations will provide more convincing analysis and can better describe the differences in the translation style of the two famous translations. A particular effort is further made to interpret the reasons for the different strategies adopted by the two different pairs of translators in the social, political, and ideological context of the translations.",Translation Style and Ideology: a Corpus-assisted Analysis of two English Translations of Hongloumeng,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
359,http://dx.doi.org/10.1093/llc/fqr004,"{'string_id': '10.1093/llc/fqr004', 'id_scheme': 'DOI'}","Many corpus linguists make the tacit assumption that part-of-speech frequencies remain constant during the period of observation. In this article, we will consider two related issues: (1) the reliability of part-of-speech tagging in a diachronic corpus and (2) shifts in tag ratios over time. The purpose is both to serve the users of the corpus by making them aware of potential problems, and to obtain linguistically interesting results. We use noun and pronoun ratios as diagnostics indicative of opposing stylistic tendencies, but we are also interested in testing whether any observed variation in the ratios could be accounted for in sociolinguistic terms. The material for our study is provided by the Parsed Corpus of Early English Correspondence (PCEEC), which consists of 2.2 million running words covering the period 1415–1681. The part-of-speech tagging of the PCEEC has its problems, which we test by reannotating the corpus according to our own principles and comparing the two annotations. While there are quite a few changes, the mean percentage of change is very small for both nouns and pronouns. As for variation over time, the mean frequency of nouns declines somewhat, while the mean frequency of pronouns fluctuates with no clear diachronic trend. However, women consistently use more pronouns than men, while men use more nouns than women. More fine-grained distinctions are needed to uncover further regularities and possible reasons for this variation.",Variation in noun and pronoun frequencies in a sociohistorical corpus of English,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
360,http://dx.doi.org/10.1093/llc/fqr013,"{'string_id': '10.1093/llc/fqr013', 'id_scheme': 'DOI'}","We present a large-scale longitudinal study of lexical and syntactic changes in language in Alzheimer's disease using complete, fully parsed texts and a large number of measures, using as our subjects the British novelists Iris Murdoch (who died with Alzheimer's), Agatha Christie (who was suspected of it), and P.D. James (who has aged healthily). We avoid the limitations and deficiencies of Garrard et al.'s [(2005), The effects of very early Alzheimer's disease on the characteristics of writing by a renowned author. Brain, 128 (2): 250–60] earlier study of Iris Murdoch. Our results support the hypothesis that signs of dementia can be found in diachronic analyses of patients’ writings, and in addition lead to new understanding of the work of the individual authors whom we studied. In particular, we show that it is probable that Agatha Christie indeed suffered from the onset of Alzheimer's while writing her last novels, and that Iris Murdoch exhibited a ‘trough’ of relatively impoverished vocabulary and syntax in her writing in her late 40s and 50s that presaged her later dementia.",Longitudinal detection of dementia through lexical and syntactic changes in writing: a case study of three British novelists,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
361,http://dx.doi.org/10.1093/llc/fqr015,"{'string_id': '10.1093/llc/fqr015', 'id_scheme': 'DOI'}","Digital papyrology encompasses artefact digitization and digital support for its interpretation. Digitization is never neutral, and this article presents how, within the e-Science and Ancient Documents project (eSAD), we are developing a software tool that strives to support the act of interpretation while both avoiding spurious exactitude and allowing genuine uncertainty. We first assert that digitization is both sampling and interpreting. Our model of papyrological interpretation thus takes on board the types of expertise that papyrologists draw onto while interpreting ancient and scarcely legible documents. Mimesis serving as a guiding principle, we present how we digitize our text-bearing artefacts (in particular incised documents), taking into account the real-world strategies of the experts. We then argue that, throughout the interpretation process, uncertainty plays a key role, which we illustrate with the example of a Roman stylus tablet that was interpreted twice 92 years apart. To allow the expression of uncertainty, we show how mimesis is again our design strategy: our tool aims to enable the experts to trace the text—a strategy we observed them deploying; further it will support reasoning about hypotheses of interpretation by setting an epistemological framework in which pieces of evidence towards hypotheses of interpretation can be evaluated as in crossword puzzle solving—another expert strategy.",Digitizing the act of papyrological interpretation: negotiating spurious exactitude and genuine uncertainty,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
362,http://dx.doi.org/10.1093/llc/fqr016,"{'string_id': '10.1093/llc/fqr016', 'id_scheme': 'DOI'}","Digital Humanities faces many issues in the current financial and educational climate. In this closing plenary from the Digital Humanities conference 2010 at King’s College London, major concerns about the current role and function of Digital Humanities are raised, demonstrating the practical and theoretical aspects of Digital Humanities research in regard to an individual project at University College London: Transcribe Bentham. It is suggested that those in the Digital Humanities have to be more aware of our history, impact, and identity, if the discipline is to continue to flourish in tighter economic climes, and that unless we maintain and establish a more professional attitude towards our scholarly outputs, we will remain ‘present, not voting’ within the academy. The plenary ends with suggestions as to how the individual, institution, and funding body can foster and aid the Digital Humanities, ensuring the field’s relevance and impact in today’s academic culture. This article is a transcript of what was planned to be said at DH2010, although the spoken plenary digresses from the following in places. The video of the speech can be viewed at http://www.arts-humanities.net/video/dh2010_keynote_melissa_terras_present_not_voting_digital_humanities_panopticon.","Present, not voting: Digital Humanities in the Panopticon: closing plenary speech, Digital Humanities 2010",Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
363,http://dx.doi.org/10.1093/llc/fqr017,"{'string_id': '10.1093/llc/fqr017', 'id_scheme': 'DOI'}","Referring to the concept of archival bond, we define stories as formed by documents that relate to a target activity and developed a method called paragraph alignment to find these documents. The method computes archival bond by measuring the cosine similarity between document paragraphs. We tested the method in a chaotic case study collection created in a shared server by different authors. Results demonstrate that this method is more efficient to find stories than calculating the cosine similarity between entire documents. This research helps archivists make sense of collections that are considered inaccessible and whose stories may otherwise be lost.",Finding stories in the archive through paragraph alignment,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
364,http://dx.doi.org/10.1093/llc/fqr018,"{'string_id': '10.1093/llc/fqr018', 'id_scheme': 'DOI'}","Spoken discourse is a uniquely valuable source of data in cognitive research. A natural way of representing spoken discourse is in the form of a transcript in standard orthography. However, since transcribing is, for neuroscientists at any rate, no more than a means to an end, many researchers give only cursory descriptions of the transcription process, including the assessment of agreement between transcribers. This article introduces a novel approach to the systematic assessment of agreement between transcripts. The method first involves the automated alignment of two texts, followed by the automatic identification and quantification of discrepancies. A similarity score is then computed, providing researchers with a tool to evaluate the accuracy of the pair of transcripts in question. Most importantly, the automated production of a set of comparison tables reveals and summarizes the actual mismatches found, making it possible to identify common causes of discrepancy. Through applying this approach to medical data collected for an investigation of dementia, the present study demonstrates its value in the amendment of transcripts and the improvement of transcription practices, which pave the way towards more reliable transcriptions for research purposes.",Techniques for transcribers: assessing and improving consistency in transcripts of spoken language,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
365,http://dx.doi.org/10.1093/llc/fqr019,"{'string_id': '10.1093/llc/fqr019', 'id_scheme': 'DOI'}","At the time Darwin first published the Origin of Species, the word ‘evolution’ was used by most biologists of the time to refer not only to specific development, as is the case today, but also to embryological development. Darwin's own stance in that matter is however open to debate, his rare use of the word making it hard to determine whether it is strictly specific or dual, and thus whether the author's conception of evolution is representative or ahead of its time. While this situation certainly stimulates philological, historical, and philosophical debates, it however complicates any attempt to settle the matter on a strict lexical basis, thus making standard text-mining techniques ineffective. To address this specific issue, a computer-assisted method for ‘reading Darwin between the lines’ is here attempted and described: by using an iterative concordance clustering algorithm, this approach aims at ‘digging’ into Darwin's concept of evolution as found in the sixth edition of the Origin of Species, regardless of any proper designation. In light of the results thus obtained, the concept of evolution in the sixth edition of the Origin of Species appears closer to its modern and strictly specific interpretation, inferences made to words related to embryological development being rather rare.",The concept of evolution in the Origin of Species: a computer-assisted analysis,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
366,http://dx.doi.org/10.1093/llc/fqr020,"{'string_id': '10.1093/llc/fqr020', 'id_scheme': 'DOI'}","The focus of this article is a system for visualizing social network data derived from a TEI-encoded corpus of texts. It describes the collection of biographies of historical Chinese Buddhist monks, which constitutes this corpus and the TEI markup, in particular the innovative concept of a ‘nexus-point’ that was originally applied to them with the goal of producing GIS-like visualizations [see Bingenheimer, M., Hung, J.-J., and Wiles, S. (2009). Markup meets GIS - Visualizing the ‘Biographies of Eminent Buddhist Monks’. In Banissi, E. et al. (eds), Proceedings of Information Visualization IV 2009. IEEE Computer Society: 550–4.]. Over the course of this work, it became clear that a data set of nexus-points could be derived from this markup which would support a representation of the social network which can be inferred from the corpus. The nature of this social network is explored and some interesting preliminary applications are suggested. The software architecture which supports the visualization, based on the Prefuse toolkit, is introduced. Finally, the scope for the future development of the corpus and the system are discussed, and some avenues for potentially fruitful analysis are suggested. Throughout the article, it is argued that the methods and techniques employed here are applicable well beyond the present context. In describing this project of social network visualization, it is demonstrated that a well-marked-up TEI corpus can, with very little additional technical overhead and using the same markup, serve as the basis for multiple representations of the same data.",Social network visualization from TEI data,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
367,http://dx.doi.org/10.1093/llc/fqr021,"{'string_id': '10.1093/llc/fqr021', 'id_scheme': 'DOI'}","The challenge in literary computing is (1) to model texts, to produce digital editions and (2) to model the meaning of literary phenomena which readers have in their mind when reading a text. Recently, an approach was proposed to describe and present structure and attributes of literary characters (i.e. the mental representation in a reader’s mind), to explore, and to compare different representations using an ontology. In order to expand the ontology for literary characters, users must manually extract information about characters from literary texts and, again manually, add them to the ontology. In this contribution, I present an application that supports users when working with ontologies in literary studies. Therefore, semi-automatic suggestions for including information in an ontology are generated. The challenge of my approach is to encode aspects of literary characters in a text and to fit it automatically to the ontology of literary characters. The application has been tested by using an extract of the novel ‘Melmoth the Wanderer’ (1820), written by Charles Robert Maturin. For the main character, Melmoth, 72 instances were generated and assigned successfully to the ontology. In conclusion, I think that this approach is not limited to the theme of character descriptions; it can also be adapted to other topics in literary computing and Digital Humanities.",Text encoding and ontology--enlarging an ontology by semi-automatic generated instances,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
368,http://dx.doi.org/10.1093/llc/fqr022,"{'string_id': '10.1093/llc/fqr022', 'id_scheme': 'DOI'}","Corpus linguistics and Geographical Information Systems (GIS) are approaches exploiting computer-based methodologies in the study of, respectively, language and language usage, and spatial patterns in geographical databases. We present an approach that uses corpus methods to bridge the gap between the textual content of a corpus (and, thus, the typically textual concerns of many branches of the humanities) and the geo-referenced database at the heart of a GIS. Using part-of-speech tagging to extract instances of proper nouns from a corpus, and a gazetteer to limit these instances to those representing place–names, a database of the places mentioned in a corpus can be created, visualized, and analysed using GIS technology. It is also possible to visualize the meanings associated with particular place–names, by building GIS databases on the collocation of place–names with particular semantic categories in their immediate context. In this way, we can create maps that visualize the geographical distribution of mentions of concepts such as war, government, or money in a particular data set. The approach cannot be entirely automated and some manual intervention is required. Nevertheless, the method is clearly valuable for the interpretation of spatial phenomena in text corpora.",Visual GISting: bringing together corpus linguistics and Geographical Information Systems,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
369,http://dx.doi.org/10.1093/llc/fqr023,"{'string_id': '10.1093/llc/fqr023', 'id_scheme': 'DOI'}","It is universally recognized that humans process speech and language in chunks, each meaningful in itself. Any two renditions or assimilations of a given sentence will exhibit similarities and discrepancies in the distribution of phrase breaks. Automated phrase break prediction assigns pauses to plain text as input, evaluated against human performance encapsulated in ‘gold standard’ boundary annotations in a speech corpus. This article advocates an enhanced feature set for phrase break prediction, incorporating non-traditional prosodic features. The authors have developed ProPOSEL, a prosody and part-of-speech English lexicon, as text annotation and text analytics tool. Application of ProPOSEL has so far uncovered a statistically significant correlation in English between certain sound patterns (i.e. the diphthongs and triphthongs of Received Pronunciation) and phrase breaks in very different genres. Thus, presence or absence of a complex vowel could easily be incorporated as an extra non-traditional classificatory feature in phrase break models. Our approach also suggests new possibilities for statistical analysis of texts, particularly authorship and genre, via favoured sound and rhythmic patterns in addition to lexis. Moreover, we suggest that our approach of text data-mining descriptive annotations of projected prosody for Text-to-Speech Synthesis and stylistic analysis is applicable to other languages.",Non-traditional prosodic features for automated phrase break prediction,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
370,http://dx.doi.org/10.1093/llc/fqr024,"{'string_id': '10.1093/llc/fqr024', 'id_scheme': 'DOI'}","Computing is not the only way to model and simulate humanities problems. In the specific field of conflict simulation, there is a long and continuing tradition of using manual modelling techniques such as maps and counters to create playable games which mirror some of the dynamics of real armed conflicts. Computer games are not automatically superior to such manual models, since mass market commercial software focuses far more on entertainment than on realistic simulation, and since the enormous capabilities of computers tend to encourage detailed incorporation of quantifiable technicalities at the expense of the vital but much less tractable human element. The biggest limitation of computer models is their limited transparency and design accessibility for non-programmers such as humanities students and scholars. Manual modelling offers a valuable ‘bridge’ between computing and traditional humanities scholarship, allowing easier generation and use of specifically tailored models, and building synergistic relationships which foster more widespread and effective adoption of digital techniques.",The benefits and limits of computerization in conflict simulation,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
371,http://dx.doi.org/10.1093/llc/fqr026,"{'string_id': '10.1093/llc/fqr026', 'id_scheme': 'DOI'}","Paraphrasing is very useful for many applications that normally involve deep linguistic alterations of a sentence, like summarization, textual entailment and question answering, and usually require sophisticated external resources, pre-processing, and semantic thesauri. This article presents a methodology for generating shallow linguistic alterations of Modern Greek sentences making use of only a low-resource chunker and taking advantage of the freedom in phrase ordering of the language. A statistical significance testing process is applied for extracting ‘swappable’ phrase bigrams. A supervised filtering phase follows, which helps to remove erroneous paraphrasing schemata, taking into account the context in which the alteration is to take place. Unlike most previous approaches to paraphrasing, the proposed process is knowledge-poor (and thus quite easily portable to other languages with a syntactic structure similar to Modern Greek), robust (applicable to any type of text), domain independent, and leads to the generation of a significant number of paraphrases, by allowing the application of more than one syntactic alterations per sentence. The significance of the automatically generated paraphrases is shown by their application in hiding secret information underneath a cover text in a steganographic communication channel. For this purpose, they need not be sophisticated linguistic alterations, but grammatically correct and significant in number, to ensure security. A steganographic security and capacity analysis of the presented implementation, as well as an explanatory description of the trade-off between them, is included to show the usefulness and the practical value of the methodology.",Linguistic steganography with knowledge-poor paraphrase generation,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
372,http://dx.doi.org/10.1093/llc/fqr028,"{'string_id': '10.1093/llc/fqr028', 'id_scheme': 'DOI'}","In addition to drawing upon content experts, librarians, archivists, developers, programmers, managers, and others, many emerging digital projects also pull in disciplinary expertise from areas that do not typically work in team environments. To be effective, these teams must find processes—some of which are counter to natural individually oriented work habits—which support the larger goals and group-oriented work of these digital projects. This article will explore the similarities and differences in approaches within and between members of the Digital Libraries (DL) and Digital Humanities (DH) communities by formally documenting the nature of collaboration in these teams. While there are many similarities in approaches between DL and DH project teams, some interesting differences exist and may influence the effectiveness of a digital project team with membership that draws from these two communities. Conclusions are focused on supporting strong team processes with recommendations for documentation, communication, training, and the development of team skills and perspectives.",A tale of two cities: implications of the similarities and differences in collaborative approaches within the digital libraries and digital humanities communities,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
373,http://dx.doi.org/10.1093/llc/fqr031,"{'string_id': '10.1093/llc/fqr031', 'id_scheme': 'DOI'}","This article examines the success of authorship attribution of Burrows’s Delta in several corpora representing a variety of languages and genres. Contrary to the approaches of our predecessors, who only investigated the attributive effectiveness of the very top of the list of the most frequent words, hundreds of possible combinations of word vectors were tested in this study, not solely starting with the most frequent word in each corpus. The results show that Delta works best for prose in English and German and less well for agglutinative languages such as Polish or Latin.",Deeper Delta across genres and languages: do we really need the most frequent words?,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
374,http://dx.doi.org/10.1093/llc/fqr034,"{'string_id': '10.1093/llc/fqr034', 'id_scheme': 'DOI'}","This article describes research aimed at improving the accuracy of an information extraction (IE) system by treating coordinate structures systematically. Commas, coordinating conjunctions, and adjacent comma–conjunction pairs are considered to be potential indicators of coordination in natural language. A recursive algorithm is implemented which converts sentences containing classified potential coordinators into sequences of simple sentences. Several approaches to the classification of potential coordinators are presented, one exploiting memory-based learning, another exploiting the publicly available Stanford parser, and a hybrid approach that classifies commas and conjunctions using the former system and comma–conjunction pairs using the latter. The article describes the initial set of features developed for exploitation by the memory-based classifier and presents optimization of that classifier. A baseline system is also described. The sentence simplification module was exploited by an IE system. With regard to the automatic classifiers that form the basis for simplification, comparative evaluation demonstrated that IE can be performed with greatest accuracy when exploiting the hybrid classifier. It also demonstrated that a simple baseline classifier induces improved accuracy when compared to systems that ignore the presence of coordinate structures in input sentences. The article presents an analysis of the errors made by the different sentence simplification modules and the IE system that exploits them. Directions for future research are suggested.",Comparing methods for the syntactic simplification of sentences in information extraction,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,26,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
375,http://dx.doi.org/10.1093/llc/fqr038,"{'string_id': '10.1093/llc/fqr038', 'id_scheme': 'DOI'}","The Chymistry of Isaac Newton project, an online scholarly edition of Newton's alchemical manuscripts, has engaged in a process to include a number of core alchemical symbols into the Unicode standard, a standard for digital representation of characters and symbols from the world's languages, scripts, and writing systems. Our article explores the relationship between information technology standardization and humanities research. We discuss Newton's engagement with alchemy and explore the graphic dimensions of alchemical discourse. We illustrate this discussion with examples of Newton's use of alchemical symbols. We examine Unicode itself, particularly a core Unicode principle distinguishing between the abstract character and the image or glyph of the character, and we discuss the tensions between this core principle and the representation of graphic, symbolic, and pictorial discourse. We describe our experience with the Unicode proposal process and illustrate again—this time with an organizational scheme for the symbols—how the technical standardization process forced a reexamination of our historical materials. Our conclusions reemphasize the potential for mutually beneficial relationships between certain types of information technology standardization and humanities research and suggest that study of the graphic qualities of alchemical discourse, especially in light of competing theories of text represented by standards like Unicode, may contribute to our understanding of the increasingly graphic, iconic, and pictorial nature of information and communication.",The liberty of invention: alchemical discourse and information technology standardization,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,27,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
376,http://dx.doi.org/10.1093/llc/fqr039,"{'string_id': '10.1093/llc/fqr039', 'id_scheme': 'DOI'}","This study is to investigate the translator's fingerprints as manifested in his/her style in translation. It reports a case study of two Chinese translations of Ulysses, adopting a corpus-based approach. The parallel subcorpora of the self-built Bilingual Corpus of Ulysses (BCU) consist of Joyce's Ulysses and its two Chinese versions produced by Xiao (1994 Tran. Ulysses, Nanjing: Yilin Press) and Jin (1997 Tran. Ulysses, Beijing: People's Literature Publishing House), respectively, and the comparable subcorpora include Xiao's original writings in Chinese. The comparison of the keyword lists shows that Xiao, the literary writer and translator, leaves some traces of lexical idiosyncrasy in his composition and translation. On the syntactic level the comparison reveals that due to the interference of the English language Xiao post-positions more adverbial clauses in translation than in composition, a feature that distinguishes the translated text from non-translated original writing. This indicates that the fingerprints of the translator are left on the translated text both as a result of his/her linguistic idiosyncrasy and of the interference and constraints of the languages s/he is dealing with in translation.",Looking for translator's fingerprints: a corpus-based study on Chinese translations of Ulysses,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,27,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
377,http://dx.doi.org/10.1093/llc/fqr043,"{'string_id': '10.1093/llc/fqr043', 'id_scheme': 'DOI'}","We show how in colonial Potosí (present-day Bolivia) social and political stability was achieved through the self-organization of society through the repetition of religious rituals. Our analysis shows that the population of Potosí develops over the time a series of cycles of rituals and miracles as a response to social upheaval and natural disasters and that these cycles of religious performance become crucial mechanisms of cooperation among different ethnic and religious groups. Our methodology starts with a close reading and annotation of the Historia de Potosí by Bartolomé Arzans. Then, we model the religious cycles of miracles and rituals and store all social and cultural information about the cycles in a multirelational graph database. Finally, we perform graph analysis through traversals queries in order to establish facts concerning social networks, historical evolution of behaviors, types of participation of miraculous characters according to dates, parts of the city, ethnic groups, etc. It is also important to note that the religious activity at the group level gave native communities a way to participate in the social life. It also guaranteed that the city performed its role as producer of silver in the global economic structure of the Spanish empire. This case proves the importance of religion as a mechanism of stability and self-organization in periods of social or political turbulence. The multidisciplinary methodology combining traditional humanistic techniques with graph analysis shows a great potential for other sociological, historical, and literary problems.",The Potosi principle: religious prosociality fosters self-organization of larger communities under extreme natural and economic conditions,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,27,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
378,http://dx.doi.org/10.1093/llc/fqr045,"{'string_id': '10.1093/llc/fqr045', 'id_scheme': 'DOI'}","In order to investigate in explicit detail the way that y- and th- pronouns alternate in the Shakespearean corpus, I have undertaken a collocational analysis of the full corpus of Shakespeare's 37 plays and found that (1) second-person pronouns can be disambiguated based on context alone, (2) y- pronouns seem to be used in more formal situations or when an inferior is addressing a social better, and (3) the th- pronoun is reserved for addressing peers, servants, or other familiar personages. Through the Python Natural Language Toolkit (Bird et al., 2009, Natural Language Processing with Python. Sebastopol, CA: O'Reilly Media), I implemented a Naïve Bayes classifier that in effect treats each occurrence of a second-person pronoun as a black box that must be resolved into either a y- pronoun or a th- pronoun based only on the surrounding words. Using tenfold cross-validation, the classifier achieves an accuracy of 78.3% when fellow th- and y- pronouns are excluded from the context and 88.0% when we allow fellow th- and y- pronouns to assist in classification. Most interesting, however, are the context words that prove most informative in categorizing the pronouns. Significantly, the words most useful in classifying a pronoun as a y- pronoun include high-register words such as lordship, madam, lords, and sir. After a group of conjugated second-person verbs like art and wert, the words most associated with th- pronouns are words such as torment, nuncle, lesser, and villain. The ability to discriminate between forms based only on context confirms the hypothesis that the two classes of second-person pronoun are indeed used distinctly in the Shakespearean corpus. The list of words most helpful in making that distinction strongly suggests a difference in formality. We can also gain additional insight into the plays by examining some of the unexpected words that collocate with either one form or the other.",A Naive Bayes classifier for Shakespeare's second-person pronoun,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,27,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
379,http://dx.doi.org/10.1093/llc/fqr049,"{'string_id': '10.1093/llc/fqr049', 'id_scheme': 'DOI'}","Variation and change in relativization strategies has been well documented (e.g. Ball 1996: 46, Biber and Clark 2002, Biber, Johansson, Leech, Conrad and Finegan 1999, Johansson 2006, Lehmann 2002). Certain types of relative clause, namely that-relatives and zero relatives, were difficult to retrieve from plain-text corpora. Studies therefore either relied on manual extraction of data or a subset of possible relativization strategies. In some text types, however, the zero relative is an important member of the class of possible relativizers. Recent advances in syntactic annotation should have made that-relatives and zero relatives more accessible to automatic retrieval. In this article, we test precision and recall of searches on a modest-sized corpus, i.e. scientific texts from ARCHER (A Representative Corpus of Historical English Registers), as a preliminary to future work on the large corpora which are increasingly becoming available. The parser retrieved some false positives and at the same time missed some relevant data. We discuss structural reasons for both kinds of shortcoming as well as the possibilities and limitations of parser adaptation.",Retrieving relatives from historical data,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,27,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
380,http://dx.doi.org/10.1093/llc/fqr050,"{'string_id': '10.1093/llc/fqr050', 'id_scheme': 'DOI'}","This article provides an account of the steps involved in adapting IBM's Languageware natural language processing software to a large corpus of highly non-standard 17th century documents. It examines the challenges encountered as part of this process, and outlines the approach adopted to provide a robust and reusable tool for the linguistic analysis of early modern source texts.",Natural language processing and early-modern dirty data: applying IBM Languageware to the 1641 depositions,Oxford University Press (OUP),2011,,Literary and Linguistic Computing,27,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
381,http://dx.doi.org/10.1093/llc/fqs001,"{'string_id': '10.1093/llc/fqs001', 'id_scheme': 'DOI'}","Old Chinese phonology is considered one of the most difficult branches of learning in Chinese philology. The traditional methodology has been empirical and its conclusions are very hard to understand and remember for modern people who do not use Old Chinese any more. This article, based on the theories and findings in Old Chinese phonology studies, uses the graph model in mathematics as the method to describe the initial-rhyme relationships of Old Chinese, and the Euclidean distance on two-dimensional plane to represent the similarity of the sounds in Chinese words (or characters) in the phonological harmony. It provides a more intuitive method of quantitative analysis, which makes the qualitative phonological relationships of Old Chinese easier to be measured and compared. Although the model is still in its infancy and there is room for refinement, it may bring insights to Chinese study, such as the research of Xiesheng series in Chinese characters, or Chinese etymology.",Graph model of Old Chinese phonological system and computing,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
382,http://dx.doi.org/10.1093/llc/fqs002,"{'string_id': '10.1093/llc/fqs002', 'id_scheme': 'DOI'}","Natural language text analysis presupposes the encoding of morphological phenomena. In this article, we present some particularities of Modern Greek and the way these are encoded in the presented electronic lexicon. The project plan of its development combined both simple planning algorithms and more elaborate ones for the generation and recognition processes. The resulted lexicon exhibits fast access to its contents and easy content management. It is re-usable and modular enough to support existing NLP applications.",Design and implementation of an electronic lexicon for Modern Greek,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
383,http://dx.doi.org/10.1093/llc/fqs003,"{'string_id': '10.1093/llc/fqs003', 'id_scheme': 'DOI'}","We describe a new supervised machine learning approach for detecting authorship deception, a specific type of authorship attribution task particularly relevant for cybercrime forensic investigations, and demonstrate its validity on two case studies drawn from realistic online data sets. The core of our approach involves identifying uncharacteristic behavior for an author, based on a writeprint extracted from unstructured text samples of the author's writing. The writeprints used here involve stylometric features and content features derived from topic models, an unsupervised approach for identifying relevant keywords that relate to the content areas of a document. One innovation of our approach is to transform the writeprint feature values into a representation that individually balances characteristic and uncharacteristic traits of an author, and we subsequently apply a Sparse Multinomial Logistic Regression classifier to this novel representation. Our method yields high accuracy for authorship deception detection on the two case studies, confirming its utility.",Detecting authorship deception: a supervised machine learning approach using author writeprints,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
384,http://dx.doi.org/10.1093/llc/fqs004,"{'string_id': '10.1093/llc/fqs004', 'id_scheme': 'DOI'}","This article discusses the crowdsourced manuscript transcription project Transcribe Bentham, and how it will impact upon long-established editorial practices at the Bentham Project, University College London, which is producing the new and authoritative edition of The Collected Works of Jeremy Bentham. We site Transcribe Bentham in the burgeoning field of scholarly crowdsourcing projects, and, by detailing our experiences of running and administering the project, attempt to assess the potential benefits of engaging the public in humanities research. The article examines the conceptualization and development of Transcribe Bentham, and how editorial practices at the Bentham Project may change as a result. We account for the design of the bespoke transcription tool which is at the project's heart, and which allows volunteers to transcribe the material and encode it in TEI-compliant XML. We attempt to answer five key questions: is crowdsourcing the transcription of complex manuscripts cost-effective? Is crowdsourcing exploitative? Are volunteer-produced transcripts of sufficient quality for editorial use and uploading to a digital repository, and what quality controls are required? Does crowdsourcing ensure sustainability and widen access to this priceless material? And finally, should the success of a project like Transcribe Bentham be measured solely according to cost-effectiveness or the volume of work produced, or do considerations of public engagement and access outweigh such concerns?",Transcription maximized; expense minimized? Crowdsourcing and editing The Collected Works of Jeremy Bentham*,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
385,http://dx.doi.org/10.1093/llc/fqs007,"{'string_id': '10.1093/llc/fqs007', 'id_scheme': 'DOI'}","Proper names in literary texts have different functions. The most important one in real life, identification, is only one of these. Some others are to make the fiction more ‘real’ or to present ideas about a character by using a name with certain meanings or associations to manipulate the reader’s expectations. A description of the functions of a certain name in a certain text becomes relevant when the researcher can point out how it compares to the functions of other names and names in other texts. The article describes how research into names in literary texts needs a quantitative approach to reach a higher level of relevancy. To get a first impression of what may be normal in literary texts, a corpus of twenty-two Dutch and twenty-two English novels and ten translations into the other language in both sets were gathered. The occurrences of all names in these novels have been tagged for those data categories that seemed useful for the literary stylistic research planned. Some first results of the statistics are presented and the use of the approach is illustrated by means of an analysis of the use of geographical names in the Dutch novel Boven is het stil by Gerbrand Bakker and its English translation by David Colmer, The Twin. In the evaluation of the results, special attention is paid to the status of currently available digital tools for named entity recognition and classification, followed by a wish-list for the tools that this kind of research really needs.",Names in novels: An experiment in computational stylistics,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
386,http://dx.doi.org/10.1093/llc/fqs009,"{'string_id': '10.1093/llc/fqs009', 'id_scheme': 'DOI'}","The remarkable advances in the field of ICT have led to the appearance of interesting innovations in literature classrooms, one of which is multimedia. Multimedia has been proven to be a powerful learning tool as it is able to provide extensive learning opportunities, thus breaking away from the traditional and restrictive ‘chalk and talk’ type of teaching. This study examined the incorporation of an after-reading assignment called ‘The Multimedia Project’ in a literature classroom. It involved ninety-six students taking English literature courses at the Faculty of Modern Languages and Communication, Universiti Putra Malaysia. Multimedia can be defined in a variety of ways, but for this project multimedia refers to a literary text presentation, primarily made using sound and images. Through this project, the students had opportunities to explore and develop their knowledge and critically analyze the literary texts covered in class. This study relied on two types of analysis: an evaluation of the students’ multimedia presentations and a survey of the students’ opinions regarding the project. The findings indicate that the multimedia project proved to be effective in advancing students’ literary experience and critical appreciation. The students’ opinions also confirmed the viability of multimedia as a practical application tool in teaching literature as well as in promoting visual literacy.",Advancing aesthetic literary experience through a multimedia project,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
387,http://dx.doi.org/10.1093/llc/fqs010,"{'string_id': '10.1093/llc/fqs010', 'id_scheme': 'DOI'}","The present study explores automatic classification of Swedish politicians and their speeches into classes based on personal traits—gender, age, and political affiliation—as a means for measuring and analyzing how these traits influence language use. Support Vector Machines classified 200-word passages, represented by binary bag-of-word-forms vectors. Different feature selections were tried. The performance of the classifiers was assessed using test data from authors unseen in the training data. Author-level predictions derived from twenty-one text-level predictions reached an accuracy rate of 81.2% for gender, 89.4% for political affiliation, and 78.9% for age. Classification concerning each basic distinction was applied to general populations of politicians and to cohorts defined by the other classes. The outcomes suggest that the extent to which these personal traits are expressed in language use varies considerably among the different cohorts and that different traits affect different layers of the vocabulary. The accuracy rates for gender classification were higher for the right wing and older cohorts than for the opposite ones. Age prediction gave higher accuracy for the right wing cohort. Political classification gave the highest accuracy rates when all forms were included in the feature sets, whereas feature sets restricted to verbs or function words gave the highest scores for gender prediction, and the lowest ones for political classification.","Automatic prediction of gender, political affiliation, and age in Swedish politicians from the wording of their speeches--A comparative study of classifiability",Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
388,http://dx.doi.org/10.1093/llc/fqs013,"{'string_id': '10.1093/llc/fqs013', 'id_scheme': 'DOI'}","This article explores building blocks in extant and emerging social media toward the possibilities they offer to the scholarly edition in electronic form, positing that we are witnessing the nascent stages of a new ‘social’ edition existing at the intersection of social media and digital editing. Beginning with a typological formulation of electronic scholarly editions, activities common to humanities scholars who engage with texts as expert readers are considered, noting that many methods of engagement both reflect the interrelated nature of long-standing professional reading strategies and are social in nature; extending this frame work, the next steps in the scholarly edition’s development in its incorporation of social media functionality reflect the importance of traditional humanistic activities and workflows, and include collaboration, incorporating contributions by its readers and re-visioning the role of the editor away from that of ultimate authority and more toward that of facilitator of reader involvement. Intended to provide a ‘toolkit’ for academic consideration, this discussion of the emerging social edition points to new methods of textual engagement in digital literary studies and is accompanied by two integral, detailed appendices, published in Digital Humanities Quarterly under the title ‘Pertinent discussions toward modeling the social edition: Annotated bibliographies’ (http://www.digitalhumanities.org/dhq/vol/6/1/000111/000111.html): one addressing issues pertinent to online reading and interaction, and another on social networking tools.",Toward modeling the social edition: An approach to understanding the electronic scholarly edition in the context of new and emerging social media,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
389,http://dx.doi.org/10.1093/llc/fqs014,"{'string_id': '10.1093/llc/fqs014', 'id_scheme': 'DOI'}","Author discrimination consists of checking whether two texts are written by the same author or not. In this investigation, we try to make an author discrimination between the Quran (The holy words and statements of God in the Islamic religion) and the Hadith (statements said by the prophet Muhammad). The Quran is taken in its entirety, whereas for the Prophet’s statements, we chose only the certified texts of the Bukhari book. Thus, three series of experiments are done and commented on. The first series of experiments analyses the two books in a global form (the text of every book is analyzed as a unique big text). It concerns nine different experiments. The second series of experiments analyses the two books in a segmental form (four different segments of text are extracted from every book). It concerns five different experiments. The third series of experiments makes an automatic authorship attribution of the two books in a segmental form by employing several classifiers and several types of features. The sizes of the segments are more or less in the same range (four different text segments, with approximately the same size, are extracted from every book). It concerns two different experiments. This investigation sheds light on an old enigma, which has not been solved for 14 centuries: in fact, all the results of this investigation have shown that the two books should have two different authors.",Author discrimination between the Holy Quran and Prophet's statements,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
390,http://dx.doi.org/10.1093/llc/fqs017,"{'string_id': '10.1093/llc/fqs017', 'id_scheme': 'DOI'}","Previous research in conceptual history, the study of change over time of key terms and value systems, has been carried out manually using a restricted number of pre-identified texts. We propose that a method combining techniques from corpus and computational linguistics can be exploited to support conceptual history with semantic searches on a vast sample of texts. To exemplify this method, we focus on a fundamental concept in modern science, the experimental method, in order to trace when the pre-existing and primarily religious concept of experiment (or experience) took on its modern, scientific meaning. We contrast a manual approach using the existing Early English Books Online search interface with an automatic method using corpus linguistics software and methods to turn the transcribed portion of the same dataset into a corpus. Both approaches allow us to separate the religious and scientific senses and plot their change over time. We observe a rapid change in the meaning of experimental from overwhelmingly religious to largely scientific within the 1660s. However, the automatic corpus method is much more efficient and will support future scholars in carrying out iterative studies in a matter of minutes rather than through weeks of painstaking work. Such methodological innovation has the potential to support the formation of new research questions, which could not have been considered previously.",Experiments in 17th century English: manual versus automatic conceptual history,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
391,http://dx.doi.org/10.1093/llc/fqs018,"{'string_id': '10.1093/llc/fqs018', 'id_scheme': 'DOI'}","Those working within the academy are drawing collaborations, both within and across their home institutions, to undertake increasingly complex and sophisticated research questions, a trend supported by the granting agencies. Advances in computers and telecommunications have facilitated these collaborations by allowing teams to recruit the right person and expertise, regardless of location. To ensure that they are able to achieve their research objectives, these teams must find methods and means to maximize the benefits associated generally with collaboration while minimizing challenges and to also address those challenges more specifically associated with the geographical, linguistic and cultural diversity. This article contributes to this understanding by reporting on interviews conducted with individuals with experience in teams that cross geographical, linguistic and cultural boundaries and focus on understanding the benefits and challenges associated with collaboration with membership and the strategies used to strengthen them. Whereas these teams benefit from these collaborations, they experience many challenges, including language and cultural differences, differing access to and comfort levels with technology, and often conflicting requirements from funding agencies and stakeholders. These teams have devised strategies at project and individual levels, with clearly articulated project plans, project liaisons, flexibility and other soft and language skills, multiple communication channels, and others, to mitigate these challenges. The article concludes with recommendations for funders, projects and individuals.","A trip around the world: Accommodating geographical, linguistic and cultural diversity in academic research teams",Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
392,http://dx.doi.org/10.1093/llc/fqs020,"{'string_id': '10.1093/llc/fqs020', 'id_scheme': 'DOI'}","In an earlier study that identified previously unrecognized writings of the young Thomas Hobbes, questions were raised about the authorship of some of Francis Bacon’s published works. This article reports a follow-up study in which two independent statistical analyses of Bacon’s English works both conclude that, whereas Bacon’s autographic writings show clearly that they are authored by the same person; almost none of his published works can be matched statistically with the autographs. The most likely explanation for this dramatic finding is that Bacon’s well-known reliance on secretaries may have been sufficiently extensive that his writing patterns are obscured or replaced by theirs. This finding suggests a far simpler explanation for a wide array of anomalies in Bacon’s works than others have offered. The study further identifies some of Bacon’s works written during a period when Thomas Hobbes was his secretary, which match Hobbes’s writing pattern.",Who wrote Bacon? Assessing the respective roles of Francis Bacon and his secretaries in the production of his English works,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
393,http://dx.doi.org/10.1093/llc/fqs021,"{'string_id': '10.1093/llc/fqs021', 'id_scheme': 'DOI'}","Test scores for linguistic features, word-patterns and word-lengths, in conjunction with parallels of word and thought, can provide corroborative internal evidence both for and against the attribution to Goldsmith of items in the Critical Review. While this evidence strengthens the case for most of the attributions in Friedman's edition of the Collected Works, and endorses his decision to reject some claimants, it also suggests a few modifications to the canon.",Goldsmith's contributions to the 'Critical Review': a supplement,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
394,http://dx.doi.org/10.1093/llc/fqs026,"{'string_id': '10.1093/llc/fqs026', 'id_scheme': 'DOI'}","This essay proposes a methodology for the analysis of poetic digital works, based on the procedural model. This methodology is here applied to Jean-Marie Dutey’s le mange-texte. The article will provide a reminder of the definition of some concepts and highlight the various effects on the work of the changes in technical contexts of execution. The program is also being studied in a rhetorical dimension. This paper demonstrates that a reading of the screen is not enough to reach all the aesthetic representations of the work and aims to complete it with an operation of meta-reading. We will then analyze the rhetorical level induced by the execution and show why the obsolescence of the work does not mean the death of the work.",Signs and apparatus in digital poetry: The example of Jean-Marie Dutey's le mange-texte,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
395,http://dx.doi.org/10.1093/llc/fqs027,"{'string_id': '10.1093/llc/fqs027', 'id_scheme': 'DOI'}","Like most authors of digital works of the narrative genre, Gregory Chatonsky is opposed to the idea that plots should be written according to the novelistic traditions. His hyperfiction entitled The Subnetwork is no exception. For the clash of heterogeneous media in this work to produce a ‘community of metaphors’, as opposed to a dialectical reasoning or a conventional narrative, every single media must be indifferently compatible with each other. Occasional relationships are thus established between different worlds, different parts of individual and collective history, which highlights a more fundamental relation of co-membership, where heterogeneous elements are always likely to assemble according to the ‘brotherhood of a new metaphor’ (Rancière, 2003, Le destin des images. Paris: La Fabrique, p. 67). The range of metaphorical brotherhood yet widens in The Subnetwork, through the introduction of animated texts and the possibility that readers are given to ‘manipulate’ (interact with) the work. Using a semio-pragmatic methodology developed at University Paris 8, I will first examine in detail the construction of meaning in these combinations between texts and movement or manipulation and their relationship with the contexts in a reading process. Digital literature often experiments with unexpected combinations based on a (de-)coherence between text, movement, and manipulation gestures, called animation figures and manipulation figures: I would situate a part of the poetic potential of digital literature in these ‘spaces of indeterminacy’.",Animation and manipulation figures in digital literature and the poetics of (de-)coherence: As exemplified by Gregory Chatonsky's The Subnetwork,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
396,http://dx.doi.org/10.1093/llc/fqs028,"{'string_id': '10.1093/llc/fqs028', 'id_scheme': 'DOI'}","Museum collections represent a highly challenging search space. This article proposes a novel approach for co-referent record identification which is suitable for use across multiple separate collections. The proposed approach is intended to be suitable for use despite highly imprecise/uncertain attribute values in the records. It is hoped that this can be achieved through a combination of aspects from the fields of probabillistic record linkage, document classification, and fuzzy clustering.",Improving record matching in imprecise and uncertain datasets,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
397,http://dx.doi.org/10.1093/llc/fqs030,"{'string_id': '10.1093/llc/fqs030', 'id_scheme': 'DOI'}","This contribution comments on certain works of digital literature in order to demonstrate their use of formal and thematic literary techniques and show that ‘wreaders’ have their own creative pattern. Playing with hyperlinks involves disorganized thinking, associative laxity and conceptual, and linguistic alterations that infringe the linear construction of the literary paradigm. The mechanism that appears during this creative ‘wreadering–wandering’ process operates in such a way that time and space converge to produce what we will call ‘digital entropy’, a creative force in which it is nowadays possible to identify cultural keywords. The expansive nature of this ‘entropy’ means that digital work arising from the local becomes universal. A discussion is included on the new ‘complex digital ego’, which operates under multiple cultural and transliterary parameters to denounce the crisis in the analogue aesthetic. Some of the characteristics attributable to digital literature—collective experimentation and the rupture of narrative linearity—are exemplified in literary stories from collective and experimental Hispanic digital literature. We also avail ourselves of some examples of pioneer digital literary works written in Spanish, in which a number of specific qualities can be found in the very virtual nature of the text. This study presents an original approach to these works in relation to three pathways to reading: Hypertextual Reading, Ekphrastic Reading, and Serendipity Reading.",Poetics of crisis or crisis of poetics in digital reading/writing?: The case of Spanish digital literature*,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
398,http://dx.doi.org/10.1093/llc/fqs031,"{'string_id': '10.1093/llc/fqs031', 'id_scheme': 'DOI'}","It has been an accepted notion that ‘inter-textuality’ is the basis of all communication. Games, inasmuch as they use semiotic systems, are no exception. At the same time the special rhetorical use of specific references and allusions to other texts, particularly in literature, is also an established fact. It is however, quite surprising to find rhetorical inter-textuality in computer games, even in single player action of the shooting type, because of the demands such games make on the player’s time and attention. Following a theoretical grounding in theories of allusion (Ben-Porat, 1976, 1978) and game theory (Aarseth, 1997, 1999, 2005a,b, 2006, 2012), the article presents six distinct types of rhetorical allusions, discussing them in relation to the narrativity level of the game to which they belong, to the source text position vis-à-vis the game, and to published evidence concerning the identification of such literary canonic traces and reflections on their contribution to the game. The results are hardly surprising, but still interesting. Game-produced narratives are not literary, certainly not literary inter-textual texts. Consequently, although there are numerous inter-textual relations between computer games and canonic, as well as popular literary texts, very few of them can be actualized as rhetorical allusions, and even less will. They are not treated as such by those who use them, though their authors may think and act otherwise. Neither triggers nor a high concentration of narrative elements effects actualizations. Significantly, gamers’ behavior is not different from that of average readers and the nature of the triggers is typical of contemporary culture: they consist of minimal cultural units, former attributes of canonic works that have become detached from and independent of their sources.",Allusive inter-textuality in computer games,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
399,http://dx.doi.org/10.1093/llc/fqs032,"{'string_id': '10.1093/llc/fqs032', 'id_scheme': 'DOI'}","This esssay intends to understand the relationships between arts (especially literature) and technologies, in our current environment characterized by intense technological saturation. Anyway, so far, there is nothing substantially new in this contrast we want to establish between arts and technology: the first ones are always based on some techniques (which are mostly internal to arts) and have constantly tensioned relationships with technologies (which are mostly external to arts). And when I say tensioned relationships, it means that I am trying to understand the distances and rapprochements between arts and technology, without having to choose definitely a possibility or another, as it has often been done by artists, aesthetes, critics, and theorists.",Retard of progress,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
400,http://dx.doi.org/10.1093/llc/fqs033,"{'string_id': '10.1093/llc/fqs033', 'id_scheme': 'DOI'}","Tesserae is a web-based tool for automatically detecting allusions in Latin poetry. Although still in the start-up phase, it already is capable of identifying significant numbers of known allusions, as well as similar numbers of allusions previously unnoticed by scholars. In this article, we use the tool to examine allusions to Vergil’s Aeneid in the first book of Lucan’s Civil War. Approximately 3,000 linguistic parallels returned by the program were compared with a list of known allusions drawn from commentaries. Each was examined individually and graded for its literary significance, in order to benchmark the program’s performance. All allusions from the program and commentaries were then pooled in order to examine broad patterns in Lucan’s allusive techniques which were largely unapproachable without digital methods. Although Lucan draws relatively constantly from Vergil’s generic language in order to maintain the epic idiom, this baseline is punctuated by clusters of pointed allusions, in which Lucan frequently subverts Vergil’s original meaning. These clusters not only attend the most significant characters and events but also play a role in structuring scene transitions. Work is under way to incorporate the ability to match on word meaning, phrase context, as well as metrical and phonological features into future versions of the program.",The Tesserae Project: intertextual analysis of Latin poetry,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
401,http://dx.doi.org/10.1093/llc/fqs034,"{'string_id': '10.1093/llc/fqs034', 'id_scheme': 'DOI'}","The article explores whether stylometric methods used in non-traditional authorship attribution can help to gain more insight in how medieval scribes dealt with the text they were copying. The research corpus consists of transcriptions from all more or less complete manuscripts of Jacob van Maerlant’s Scolastica, a Middle Dutch translation/adaptation of Peter Comestor’s Medieval Latin Historia scholastica. Five episodes were selected totalling seventy samples of around 1,200 tokens each. Cluster analysis and principal component analysis are used to compare the copies per episode. The results based on the Middle Dutch texts were highly influenced by irrelevant spelling variation. Since the aim was to explore differences on the content level of the copies, the analyses were extended to lemmatised versions of the texts. The results based on the lemmatised texts are a good starting point for the exploration of the ways in which Scolastica scribes dealt with the work they were copying. The main results of the exploration are that most copies of an episode cluster closely together, with only occasional outliers. The outliers are not the same for each episode, which suggests that a separate analysis and explanation per episode is needed—scribes may have had reasons to elaborate on one topic and to leave another untouched, for instance under the instruction of a patron. The article closes with some pointers for the next steps in the research, which are expected to need other kinds of methods for analysis.",The secret life of scribes. Exploring fifteen manuscripts of Jacob van Maerlant's Scolastica (1271),Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
402,http://dx.doi.org/10.1093/llc/fqs035,"{'string_id': '10.1093/llc/fqs035', 'id_scheme': 'DOI'}","Ever since scholars in the humanities have studied computer games, the relationship between play and narrative has been a much contested issue. Much dissent stems from incompatible basic assumptions about play and narrative, which, this article argues, can be reconciled by a formalist approach to games and narrative on a structural level. First, event structures and story structures are shown to be central to various theories of narrative. Correlating these findings with Espen Aarseth’s reflections upon nonlinearity, an understanding of narrative revolving around event logic is developed. Building on the theory of games developed by Roger Caillois, the article then develops a model of games in which three layers of structures are governed by three types of rules. The most abstract of these layers arranges game elements in a meta-structure which is based on both ludic and narrative logic. In a final step, nonlinear game structures are explained within this model and categorized in a typology that orders them by the type of agency players can execute.",Narrative rules? Story logic and the structures of games,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
403,http://dx.doi.org/10.1093/llc/fqs036,"{'string_id': '10.1093/llc/fqs036', 'id_scheme': 'DOI'}","This study investigates the problem of appropriate choice of texts for the training set in machine-learning classification techniques. Although intuition suggests picking the most typical texts (whatever ‘typical’ means) by the authors studied, any arbitrary choice might substantially affect the final results. Thus, to eschew cherry picking, we introduce a method of verification of the choice of ‘typical’ samples, inspired by k-fold cross-validation procedures. Namely, we use a bootstrap-like approach to choose randomly, in 500 iterations, the samples for the training and the test sets. Next, we examine the obtained 500 attribution accuracy scores: if the density function shows widespread results, the corpus is assumed to be very sensitive to the permutations of the training set. To test this methodology empirically, we have selected roughly similar corpora in five languages: English, French, German, Italian, and Polish. The results show considerable resistance of the English corpus to permutations, while the other corpora turned out to be more dependent on the choice of the samples; the Polish corpus produces both accuracy and consistency below any acceptable standards.","Do birds of a feather really flock together, or how to choose training samples for authorship attribution",Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
404,http://dx.doi.org/10.1093/llc/fqs037,"{'string_id': '10.1093/llc/fqs037', 'id_scheme': 'DOI'}","This article discusses the reception of electronic literature, focusing on the development of new literary reading rituals in electronic environments in the Hispanic world. It consists of three parts: a revision of the situation of Hispanic reader communities in the electronic environment; the description of the results of a series of literary on-screen reading experiences with university students, and a final exposition of some reading strategies required to bridge the gap between print and online literature.",Literary reading rituals and practices on new interfaces,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,27,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
405,http://dx.doi.org/10.1093/llc/fqs040,"{'string_id': '10.1093/llc/fqs040', 'id_scheme': 'DOI'}","Estrategias Contemporáneas de Lectura is a project focused on research and teaching that emphasizes the importance of interpretation in the study of Classical Antiquity. The aim of the project is to support research with digital tools that document the process by which a perspective point or horizon of interpretation is constructed, by following the development of individual and group work processes. The progress of the project has posed theoretical and technical challenges related to a diversity of methodologies and expectations of the project. However, the development of these discussions has showed the relevance of interpretation and its complexity, making it a fruitful experience.",Documenting horizons of interpretation in philosophy,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
406,http://dx.doi.org/10.1093/llc/fqs044,"{'string_id': '10.1093/llc/fqs044', 'id_scheme': 'DOI'}","We present WordSeer, an exploratory analysis environment for literary text. Literature study is a cycle of reading, interpretation, exploration, and understanding. While there is now abundant technological support for reading and interpreting literary text in new ways through text-processing algorithms, the other parts of the cycle—exploration and understanding—have been relatively neglected. We are motivated by the literature on sensemaking, an area of computer science devoted to supporting open-ended analysis on large collections of data. Our software system integrates tools for algorithmic processing of text with interaction techniques that support the interpretive, exploratory, and note-taking aspects of scholarship. At present, the system supports grammatical search and contextual similarity determination, visualization of patterns of word context, and examination and organization of the source material for comparison and hypothesis building. This article illustrates its capabilities by analyzing language-use differences between male and female characters in Shakespeare’s plays. We find that when love is a major plot point, the language Shakespeare uses to refer to women becomes more physical, and the language referring to men becomes more sentimental. Future work will incorporate additional sensemaking tools to aid comparison, exploration, grouping, and pattern recognition.",Supporting exploratory text analysis in literature study,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
407,http://dx.doi.org/10.1093/llc/fqs045,"{'string_id': '10.1093/llc/fqs045', 'id_scheme': 'DOI'}","Digitization of Japanese historical documents has gained much attraction in the field of humanities in Japan recently, and numbers of documents are already available in digitized text format. However, text analysis of these documents has rarely been done mainly due to the lack of natural language processing tools that can handle pre-modern Japanese. In this article, we propose a method to extract and visualize the relationships among persons from Japanese historical documents with an aid of supplementary information such as personal name and place name indices. The goal of the method is to extract dynamics of relationships among historical persons. The method utilizes locational information to obtain latent relationships among persons based on their spatial activities. The proposed method is applied to a Japanese historical chronicle written in the 12th century. Experimental results showed a strong correspondence to the known historical facts, and the results of a user survey completed by researchers of Japanese history demonstrated some potential for the method to serve as a new approach in the fields of humanities.",Visualization of relationships among historical persons from Japanese historical documents,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
408,http://dx.doi.org/10.1093/llc/fqs046,"{'string_id': '10.1093/llc/fqs046', 'id_scheme': 'DOI'}","The aim of this article was to distinguish different authorial layers within a 15th-century chronicle, a rare medieval autograph or author’s copy (Croniken van der Duytscher Oirden or Jüngere Hochmeisterchronik), and to test specifically the validity of claims concerning the original composition of the text. A better apprehension of the creative process involved in composing the Croniken is essential for the interpretation and understanding of the purpose and intended audience of the text. Furthermore, it gives an insight into the historiographical activities in the ‘peripheral’ bailiwicks of the Teutonic Order. Computational techniques, in this case John Burrows’ tried-and-tested Delta method, play an invaluable role in both the solution of these issues as well as in pointing in the direction of new enquiries. Here, the Delta method was used to create a walking window, only 2,000 words in length, across the entire chronicle. Despite the small sample size, chosen because in the present case, a finer granularity and precision in detecting the shifts in authorial styles was as important as reliability, Delta was able to pick out distinct parts of the chronicle, some as short as just >500 words.",Layer on layer. 'Computational archaeology' in 15th-century Middle Dutch historiography,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
409,http://dx.doi.org/10.1093/llc/fqs047,"{'string_id': '10.1093/llc/fqs047', 'id_scheme': 'DOI'}","This study explores the linguistic application of bipartite spectral graph partitioning, a graph-theoretic technique that simultaneously identifies clusters of similar localities as well as clusters of features characteristic of those localities. We compare the results using this approach with previously published results on the same dataset using cluster and principal component analysis (Shackleton 2007). Although the results of the spectral partitioning method and Shackleton’s approach overlap to a broad extent, the analyses offer complementary insights into the data. The traditional cluster analysis detects some clusters that are not identified by the spectral partitioning analysis, whereas the reverse also occurs. Similarly, the principal component analysis and the spectral partitioning analysis detect many overlapping but also some different linguistic variants. The main benefit of the bipartite spectral graph partitioning method over the alternative approaches remains its ability to simultaneously identify sensible geographical clusters of localities with their corresponding linguistic features.",Analyzing phonetic variation in the traditional English dialects: Simultaneously clustering dialects and phonetic features,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
410,http://dx.doi.org/10.1093/llc/fqs048,"{'string_id': '10.1093/llc/fqs048', 'id_scheme': 'DOI'}","In our earlier work, an approach to defining dialect areas using multidimensional scaling (MDS) of the total collection of available raw data (from a region of Romania) has produced results that showed ‘some’ but ‘not all’ of the dialect distinctions that were anticipated. To investigate this situation, we have extended our approach in two ways, one methodological and one technical. Methodologically, we have switched from looking at raw data to examining interpretive maps based on recognized dialect distinctions. Further, we have categorized these interpretations as phonetic (regular and irregular), morphophonemic, morphological, and lexical, examining each category separately. The result is a much clearer set of dialect distinctions, as seen in the MDS pictures. However, the dialect distinctions vary by category, leading us to make suggestions about the role of each category in defining the notion of dialect. Our technical extension is the creation and use of a 3D viewer for looking at the MDS pictures. We project the linguistic-distance space into three, instead of two, dimensions, and manipulate the resulting structure interactively, thus uncovering and eliminating any accidental ‘closeness’, as sometimes happens in the 2D case. Strikingly, the resulting 3D objects seem to be very flat, which strongly suggests that there are only two relevant dimensions for distinguishing these dialects, although the two dimensions do not correspond exclusively to geographic dimensions. The result of these extensions is that the multidimensional approach becomes even more viable as a way of selecting dialect and dialect-transition areas, and perhaps more accessible for use with languages and dialects beyond our own study area.",Defining dialect regions with interpretations: Advancing the multidimensional scaling approach,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
411,http://dx.doi.org/10.1093/llc/fqs049,"{'string_id': '10.1093/llc/fqs049', 'id_scheme': 'DOI'}","The aim of this article was to present a new tool for doing dialectometry. The program, called ‘DiaTech’, incorporates features of previous programs, especially features of the VDM program created under the direction of H. Goebl. Its main goal is to motivate dialectology studies and dialectologists by providing a new comfortable and efficient tool. With regard to the linguistic features of the program, the most novel feature of ‘DiaTech’ is its ability to manage ‘multiple responses’, that is, for each question, the dialectologist has the option to record more than one item. As far as its basic technological features are concerned, the ‘DiaTech’ program can be used with different platforms; it has an interface in different languages. Regarding statistical procedures, the DiaTech program gives access to a wide variety of distances, including those that are most typically used in dialectometry for bidimensional and multivariate analyses (cluster, correlation, etc.). In addition, the program makes use of different algorithms for visualization and classification. Finally, it provides the necessary tools to create different types of maps with a polygonized background: question by question maps (as in a linguistic atlas), identity maps, honeycomb maps, synopsis of skewness, standard deviation, dendrogramic maps, correlation maps, etc.",'DiaTech': A new tool for dialectology,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
412,http://dx.doi.org/10.1093/llc/fqs050,"{'string_id': '10.1093/llc/fqs050', 'id_scheme': 'DOI'}","The relation between the semantic meaning of a lexical variable and the spatial distribution of its variants has been a common field of interest in traditional German dialectology throughout the past century. In the literature in question, several claims regarding the nature of this relation can be found that identify certain semantic properties with spatial distribution patterns. These assumptions are the result of observations the authors made through the qualitative study of their data. Whether these findings can be substantiated in a quantitative analysis, however, has not yet been tested. Neither has a systematic explanatory framework that accounts for these postulated relations been devised. This article investigates these issues by using methods from geostatistical dialectometry that allow certain distributional characteristics of linguistic variables to be quantified. The indices obtained in this way are then tested against the relevance of subject areas. Building on the results, an explanatory framework for their interpretation with respect to basic mechanisms of language variation and change in space is introduced.",Lexical meaning and spatial distribution. Evidence from geostatistical dialectometry,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
413,http://dx.doi.org/10.1093/llc/fqs051,"{'string_id': '10.1093/llc/fqs051', 'id_scheme': 'DOI'}","This article presents a statistical comparison of common patterns of regional phonetic and lexical variation in American English based on the results of two previous dialect studies. Because these two studies are based on datasets that represent different cities, this article also introduces a general method for comparing dialect maps that are based on different sets of locations. This method of comparison consists of two steps. First, the dialects maps are defined across a shared set of reference locations through ordinary kriging. Second, these normalized maps are correlated to each other in order to estimate the similarity between the original dialect maps. The results of this comparison show that regional phonetic and lexical variation follow similar patterns in Modern American English.",A statistical comparison of regional phonetic and lexical variation in American English,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
414,http://dx.doi.org/10.1093/llc/fqs052,"{'string_id': '10.1093/llc/fqs052', 'id_scheme': 'DOI'}","This article investigates several linguistic changes which are ongoing in north-western Catalan using a contemporary corpus. We take advantage of a range of dialectometric methods that allow us to calculate and analyse the linguistic distance between varieties in apparent time from an aggregate perspective. Specifically, we pay attention to the process of structural dialect loss due to linguistic advergence to standard and eastern Catalan in many north-western Catalan dialects located in Catalonia (Spain) and Andorra. We also provide evidence that the dialect leveling taking place in these two areas strongly contrasts with the relative stability of the Catalan dialects on the other side of the Catalan–Aragonese border in Spain, where Catalan is not an official language. These opposite sociolinguistic situations (Catalonia and Andorra have strong language policies to support Catalan, whereas Aragon does not) have triggered a twofold process of vertical advergence between the Catalan spoken in Catalonia and Andorra towards the prestigious varieties, on the one hand; and of horizontal divergence between these dialects and those located in Aragon, on the other hand. This situation has notably strengthened the border differences between Aragon and Catalonia during the last 80 years. This article is one of the first attempts to study the border effects not only between regions belonging to different countries but also between different administrative regions ‘within’ the same country. In addition, we investigate the different roles of urban versus rural areas, providing support for the view that the spatial and hierarchical diffusion patterns are complementary.",Linguistic advergence and divergence in north-western Catalan: A dialectometric investigation of dialect leveling and border effects,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
415,http://dx.doi.org/10.1093/llc/fqs053,"{'string_id': '10.1093/llc/fqs053', 'id_scheme': 'DOI'}","Traditional Estonian dialect classifications are based on the phonology, morphology, and lexis, and there are very few studies about syntax available. The present article is the first quantitative syntactic study of Estonian dialects. We concentrate on constructions consisting of finite and non-finite verbs, and we apply contemporary statistical methods to explore the syntactic variation. Our results show that even bare token frequencies can identify syntactic patterns quite well, and that analyses exploiting collostructional methods makes the variational patterns even clearer. We use correspondence analysis and clustering to detect geographic influence on variation. The results suggest a syntax-based classification of dialects differs from the traditional classifications based mainly on phonology and lexis. Our data reveal systematic differences between eastern and western dialects at the syntactic level, whereas analyses based on phonology and lexis distinguish mainly between northern and southern dialects. The western dialects make more use of analytic constructions consisting of a finite and a non-finite verb form.",Variation of verbal constructions in Estonian dialects,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
416,http://dx.doi.org/10.1093/llc/fqs054,"{'string_id': '10.1093/llc/fqs054', 'id_scheme': 'DOI'}","There has been widespread excitement in recent years about the emergence of large-scale digital initiatives (LSDIs) such as Google Book Search. Although many have become excited at the prospect of a digital recreation of the Library of Alexandria, there has also been great controversy surrounding these projects. This article looks at one of these controversies: the suggestion that mass digitization is creating a virtual rubbish dump of our cultural heritage. It discusses some of the quantitative methods being used to analyse the big data that have been created, and two major concerns that have arisen as a result. First, there is the concern that quantitative analysis has inadvertently fed a culture that favours information ahead of traditional research methods. Second, little information exists about how LSDIs are used for any research other than quantitative methods. These problems have helped to fuel the idea that digitization is destroying the print medium, when in many respects it still closely remediates the bibliographic codes of the Gutenberg era. The article concludes that more work must be done to understand what impact mass digitization has had on all researchers in the humanities, rather than just the early adopters, and briefly mentions the work that the author is undertaking in this area.",Mass digitization and the garbage dump: The conflicting needs of quantitative and qualitative methods,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
417,http://dx.doi.org/10.1093/llc/fqs055,"{'string_id': '10.1093/llc/fqs055', 'id_scheme': 'DOI'}","In 1457, the Florentine merchant, Giovanni Rucellai, started to compose a family records and commonplace book, the Zibaldone Quaresimale. His goal was to transmit knowledge to his sons. This article is a report on the computer-assisted analysis of the Zibaldone and the knowledge Giovanni intended to transmit. The 2nd and 3rd sections treat the theoretical framework applied for knowledge structure analysis and the technical requirements this imposed. In the 4th section, I explain how I transformed the manuscript into a semantically and linguistically annotated corpus with the help of semi-automatic procedures. I also discuss the integrated research environment that I developed to store and link information from secondary and primary sources. In the 6th section, through a concrete research problem, the analysis of social perception, I demonstrate the practical application and the power of the digital environment. This section also presents how the knowledge system described in the codex is reconstructed with the help of a Web Ontology Language ontology. Finally, I argue that a project in Digital Humanities can bring new insights only if a comprehensive theoretical model directs the design and the use of the digital environment; one that functions as a framework to interpret the data that the digital environment produces.",The computer-assisted analysis of a medieval commonplace book and diary (MS Zibaldone Quaresimale by Giovanni Rucellai),Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
418,http://dx.doi.org/10.1093/llc/fqs056,"{'string_id': '10.1093/llc/fqs056', 'id_scheme': 'DOI'}","The importance of abductive reasoning is increasingly emphasized within diverse research domains in which interpretation plays a central role. This reasoning type appears to provide an answer to various significant issues in diverse domains, especially in combination with deductive and inductive reasoning, as C. S. Peirce eventually presented it in his process of (scientific) inquiry. Central to our interpretation of Peirce’s process of inquiry, thus stands a cycle of abductive, deductive, and inductive reasoning, which is iterated continuously with the surrounding world as its subject. This world here is understood as the environment that surrounds us and that we experience through our senses. The continuous iteration of Peirce’s process of inquiry allows constructing experience-based knowledge dynamically. This kind of knowledge comes closer to the dynamic memory and reasoning that is required for active interpretation. Issues that might be better understood and addressed accordingly involve not only interpretation but also idea generation, creativity, surprise, and so forth. This article documents a part of our efforts in simulating such a reasoning cycle using currently available technologies. Diverse hypotheses and conclusions have been made in these simulation efforts. For instance, we hypothesize that Peirce’s reasoning cycle might be configured in reasoning levels, with each level handling information or patterns in different levels of invariance and meaning. These efforts have resulted in an environment in which a reasoning agent processes basic colour information and gradually builds up interpretations of such colours based on incoming information.",Including the power of interpretation through a simulation of Peirce's process of inquiry,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
419,http://dx.doi.org/10.1093/llc/fqs057,"{'string_id': '10.1093/llc/fqs057', 'id_scheme': 'DOI'}","A careful investigation of synchronic patterns of linguistic variation with underlying linguistic features can lead to important insights into the comprehension of diachronic phonetic processes. In this article, we showed that the method of spectral partitioning of bipartite graphs applied to synchronic dialectal data can effectively and reliably be used to investigate diachronic processes, thus contributing to a deeper understanding of the relationship between synchronic variation and diachronic change. This was illustrated through a case study carried out on Tuscan dialects, focusing on so-called Tuscan ‘gorgia’, a lenition process consisting of the spirantization of stop consonants. In particular, from a quantitative analysis of the sound correspondences involving voiceless and voiced stops, we tracked the evolution of the spirantization phenomenon in several respects. First, we tracked spirantization geographically, across Tuscany from the influential center of Florence to the peripheral areas. Second, we tracked it phonologically, from voiceless to voiced stops, and within each voicing class from velars to dentals and then to bilabials. Finally, we tracked it demographically, with young speakers using the most innovative sound correspondences more than old speakers. The fact that these results are in line with the literature on the topic of Tuscan ‘gorgia’ demonstrates the potential of the method of spectral partitioning of bipartite graphs with respect to the reconstruction of diachronic processes starting from diatopically distributed synchronic dialectal data.",Synchronic patterns of Tuscan phonetic variation and diachronic change: Evidence from a dialectometric study,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
420,http://dx.doi.org/10.1093/llc/fqs058,"{'string_id': '10.1093/llc/fqs058', 'id_scheme': 'DOI'}","One of the principle signs that speech is a complex system is the nonlinear arrangement of frequencies of variants in linguistic survey data. When the counts are charted by frequency, they form an asymptotic hyperbolic curve (A-curve) at every scale of analysis. The shape of the curve is sensitive to sample size: a small sample is unlikely to show an A-curve. So, too, categorization: too large a number of categories makes the data appear linear because of the small number of tokens in each category, while allowing too few categories, such as the two data points from binary categories, also gives us a line, not a curve. The A-curve can only be observed when the number of categories into which the data are sorted lies between these two extremes. Common practice in dialectology and sociolinguistics has been to establish a small number of possible categories such as phonemes for pronunciation, or to notice only the few most frequently occurring variants and to ignore the rest. Such methods cannot address the underlying complexity of the data. In this essay, we discuss the Gini coefficient, used in economics, as a means to measure optimal nonlinearity. In an experiment where pronunciation data from survey research on the American English vowel system are analyzed in various subsamples, we demonstrate that A-curves do exist in the data in all cases, and we establish parameters for the interaction of sample size and number of categories in the design of valid and reliable experiments.",Scaled measurement of geographic and social speech data,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
421,http://dx.doi.org/10.1093/llc/fqs060,"{'string_id': '10.1093/llc/fqs060', 'id_scheme': 'DOI'}","‘Dialectal stratigraphy’ aims to focus on the cartography of ancient lexicon. It seeks to represent cartographically how the diatopic variation of a sample of sixty words has evolved. The project begins with the earliest period of written Catalan and indicates the words’ geographical location and their graphic form and semantic progression over time. Unlike conventional atlases which use only a single axis, two axes of linguistic representation are used in dialectal stratigraphy, those of space and time. The distribution of words in the map is represented in layers depicting linguistic change over time. From a methodological point of view, a number of dynamic maps have been created. The maps have a chronological axis that: (1) allows the words to be traced to their initial geographical place; (2) shows their evolution over the passage of the centuries; and (3) makes it possible to observe its geographical movement. In addition, this comprehensive overview allows us to track the course of Eastern and Western Catalan dialects which have been subject of so much controversy of late and also provides information about transitional areas. The aim of this article is to show the potential of this technique for exploring the reasons for changes in lexicon.",Dynamic cartography with diachronic data: Dialectal stratigraphy,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
422,http://dx.doi.org/10.1093/llc/fqs063,"{'string_id': '10.1093/llc/fqs063', 'id_scheme': 'DOI'}","Land deeds were the only proof of ownership in pre-1900 Taiwan. They are indispensable for the studies of Taiwan’s social, anthropological, and economic evolution. We have built a full-text digital library that contains almost 40,000 land deeds. The deeds in our collection range over 250 years and are collected from over 100 sources. The unprecedented volume and diversity of the sources provide an exciting source of primary documents for historians. But they also pose an interesting challenge: how to tell if two land deeds are related. In this article, we describe an approach to discover two important relations: successive transactions and allotment agreements involving the same property. Our method enabled us to construct 6,035 such transaction pairs. We also introduce a notion of ‘land transitivity graph’ to capture the transitivity embedded in these transactions. We discovered 2,436 such graphs, the largest of which includes 104 deeds. Some of these graphs involve land behavior that had never been studied before.",Discovering land transaction relations from land deeds of Taiwan,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
423,http://dx.doi.org/10.1093/llc/fqs064,"{'string_id': '10.1093/llc/fqs064', 'id_scheme': 'DOI'}","This research-in-progress report describes ongoing work on a doctoral dissertation, which attempts to model the prototypical structure of the tourist brochure as a multimodal artefact. By using a multimodal corpus based on the Genre and Multimodality model, the dissertation investigates how the brochures use both language and image to fulfil their communicative function. This article focuses on a specific aspect of the prototypical structure, that is, how the brochures organize the content in the layout and signal its interrelations. The article suggests that the layout and the rhetorical relations between the content are intertwined in what may be called the ‘rhetoric-layout interface’. By drawing on a sample of three brochures published in 1984, 1988, and 2006, the article explores how this interface shapes the multimodal structure of the tourist brochures. The relationship between the rhetorical and the layout structure is suggested to be reciprocal: adopting a particular configuration in the rhetorical structure also affects the range of choices available in the layout structure and vice versa.The article concludes that the interface between the rhetorical and the layout structure warrants attention in modeling the prototypical structure, and suggests that certain structural signals related to the use of two-dimensional space require specific attention in corpus analysis. Moreover, extensive cross-layer analyses with larger corpora will be required to tease out the range of different configurations in the tourist brochures, and to create a prototypical model of the multimodal artefact.",The interface between rhetoric and layout in multimodal artefacts,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
424,http://dx.doi.org/10.1093/llc/fqs065,"{'string_id': '10.1093/llc/fqs065', 'id_scheme': 'DOI'}","In this study, we propose an integrated method to automatically evaluate very brief summaries (around 50 words) using the computational tool latent semantic analysis (LSA). The method proposed is based on a regression equation calculated with a corpus of a 100 summaries (the training sample) and is validated on a different sample of summaries (validation sample). The equation incorporates two parameters extracted from LSA: (1) the semantic similarity of the summary, measured using the summary–expert summaries method and (2) the vector length. The study is based on a sample of 786 summaries by students at four academic levels. All of these students summarized either an expository or a narrative text; their summaries were then evaluated by four graders on a scale of 0–10. The results support three ideas. First, that incorporating both parameters into the method is more successful than the traditional cosine measure. The reliability of LSA for evaluating summaries rises >0.80 level for the expository text. Second, that LSA shows practically the same level of sensitivity as the human graders to the quality of the summaries at different academic levels. Third, that the method overcomes a serious limitation of LSA: its difficulties evaluating very brief texts.",Using latent semantic analysis to grade brief summaries: A study exploring texts at different academic levels,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,28,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
425,http://dx.doi.org/10.1093/llc/fqs066,"{'string_id': '10.1093/llc/fqs066', 'id_scheme': 'DOI'}","We examine the assertion that the two-parameter log-normal distribution is an appropriate parametric model for the shot length distributions of Hollywood films. A review of the claims made in favour of assuming log-normality for shot length distributions finds them to be lacking in methodological detail and statistical rigour. We find there is no supporting evidence to justify the assumption of log-normality in general for shot length distributions. In order to test this assumption, we examined a total of 134 Hollywood films from 1935 to 2005, inclusive, to determine goodness-of-fit of a normal distribution to log-transformed shot lengths of these films using four separate measures: the ratio of the geometric mean to the median; the ratio of the shape factor σ to the estimator σ* = √(2 × ln (⁠/M)); the Shapiro–Francia test; and the Jarque–Bera test. Normal probability plots were also used for visual inspection of the data. The results show that, while a small number of films are well modelled by a log-normal distribution, this is not the case for the overwhelming majority of films tested (125 out of 134). Therefore, we conclude there is no justification for claiming the log-normal distribution is an adequate parametric model of shot length data for Hollywood films and recommend the use of robust statistics that do not require underlying parametric models for the analysis of film style.",The log-normal distribution is not an appropriate parametric model for shot length distributions of Hollywood films,Oxford University Press (OUP),2012,,Literary and Linguistic Computing,30,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
426,http://dx.doi.org/10.1093/llc/fqs067,"{'string_id': '10.1093/llc/fqs067', 'id_scheme': 'DOI'}","TextGrid’s Text-Image-Link-Editor is used to link segments of text with sections on the corresponding image. A typical application is the linking of scans of facsimiles with their transcriptions, though these texts can also be created during the linking process, which allows also for image annotations. The information on the linking between manuscript fragments and the corresponding transcription is itself stored in TEI P5. TextGrid is a virtual research environment for the humanities disciplines dealing with texts in a wide sense (philologies, epigraphy, linguistics, musicology, art history, etc.). The joint research project TextGrid is part of the D-Grid initiative and is funded by the German Federal Ministry of Education and Research (BMBF) for the period starting from 1 June 2009 to 31 May 2012 (reference number: 01UG0901A). TextGrid consists of two principal building blocks, the grid-based back-end TextGridRep that hosts both infrastructure services and the repository layer for access to research data and long-term archiving, and the user-facing TextGrid Laboratory (TextGridLab). The TextGridLab, a single point of entry to the virtual research environment, provides integrated access to both new and existing tools and services via user-friendly software based on the eclipse framework [TG]. TBLE is a key component of the TextGridLab that has been under continuous development since 2008 and is by now in practical use.","The text-image-link-editor: A tool for linking facsimiles and transcriptions, and image annotations",Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
427,http://dx.doi.org/10.1093/llc/fqs068,"{'string_id': '10.1093/llc/fqs068', 'id_scheme': 'DOI'}","An often-overlooked challenge to the field of digital humanities is the lack of interest in the discipline by young scholars. Despite being the so-called digital natives, many of my museum studies students have no desire to engage with new technology. This short paper is a snapshot of my attempt to teach digital curation and online exhibit development within the framework of my material culture seminar. It analyses 3 years of student exhibits developed using Omeka and points to new directions the project will go over the next several years. I began the research project by asking the following questions of my students: What does material culture look like on the web? How do you curate it? How does the public interact with virtual objects? What is the relationship between virtual and physical museum artifacts? However, after seeing the students struggle with basic web development, I expanded my own research questions to include: What skills do emerging professionals need and how can we integrate technical training into an academic program?",Omeka in the classroom: The challenges of teaching material culture in a digital world,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
428,http://dx.doi.org/10.1093/llc/fqs069,"{'string_id': '10.1093/llc/fqs069', 'id_scheme': 'DOI'}","This article reports on a study of XML-markup experiences as reading practices of secondary students studying English literature in a public high school in Vancouver, British Columbia, Canada. Since training in the digital humanities (DH) has historically been restricted to those in undergraduate and graduate programs, an important consideration in DH education is how we might implement DH methods in secondary school curricula with a view to introducing prospective scholars to the field prior to their admission to post-secondary education. A concomitant goal would be to investigate a new locus for DH education, at a different level of education and in a different institutional environment, to observe how or if DH might migrate from the locale to which it has acclimatized. Our work maps a method for developing this pedagogy and presents findings on students’ semantic tagging of two short stories: Ernest Hemingway’s (1927) ‘Hills Like White Elephants’ and Sean O’Faolain’s (1948) ‘The Trout’. Analysis of this tagging reveals markup as reading practice and describes how students negotiate between the experiences of reading, how these experiences may be realized by text, and the ways in which XML markup—as process—mediates between.",Reading practices and digital experiences: An investigation into secondary students' reading practices and XML-markup experiences of fiction,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
429,http://dx.doi.org/10.1093/llc/fqs070,"{'string_id': '10.1093/llc/fqs070', 'id_scheme': 'DOI'}","In this article, we examine the application of computational stylometry to psychological profiling. We adapt several techniques, which have proven useful for author identification to the problem of identifying an individual author’s Myers-Briggs personality type indicator from the statistical features of the text. The Myers-Briggs type indicator assigns four binary classifications to define personality type: Extrovert–Introvert, Intuitive–Sensing, Thinking–Feeling, and Judging–Perceiving. For this study, we use the Personae corpus, which consists of 145 Dutch-language texts all pertaining to a specific topic, each labeled with the Myers-Briggs personality profile of the author (Luyckx K and Daelemans W, Personae: A Corpus for Author and Personality Prediction from Text, In Proceedings of the 6th Language Resources and Evaluation Conference. Marrakech, Morocco: International Conference on Language Resources and Evaluation. 2008). Our system builds upon earlier work by Luyckx and Daelemans (Using Syntactic Features to Predict Author Personality from Text. In Proceedings of Digital Humanities 2008, Oulu, Finland: Digital Humanities, pp. 146–9.) to provide a set of best practices for personality profiling. We propose a more sophisticated modeling technique, combined with more advanced feature selection and state-of-the-art analysis methods from author identification to achieve a significant improvement over previous systems.",Psychological profiling through textual analysis,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
430,http://dx.doi.org/10.1093/llc/fqs073,"{'string_id': '10.1093/llc/fqs073', 'id_scheme': 'DOI'}","This study draws from a large corpus of Congressional speeches from the 101st to the 110th Congress (1989–2008), to examine gender differences in language use in a setting of political debates. Female legislators’ speeches demonstrated characteristics of both a feminine language style (e.g. more use of emotion words, fewer articles) and a masculine one (e.g. more nouns and long words, fewer personal pronouns). A trend analysis found that these gender differences have consistently existed in the Congressional speeches over the past 20 years, regardless of the topic of debate. The findings lend support to the argument that gender differences in language use persist in professional settings like the floor of Congress.",Language and gender in Congressional speech,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
431,http://dx.doi.org/10.1093/llc/fqs074,"{'string_id': '10.1093/llc/fqs074', 'id_scheme': 'DOI'}","One of the major challenges in the process of machine translation is word sense disambiguation (WSD), which is defined as choosing the correct meaning of a multi-meaning word in a text. Supervised learning methods are usually used to solve this problem. The disambiguation task is performed using the statistics of the translated documents (as training data) or dual corpora of source and target languages. In this article, we present a supervised learning method for WSD, which is based on K-nearest neighbor algorithm. As the first step, we extract two sets of features: the set of words that have occurred frequently in the text and the set of words surrounding the ambiguous word. In order to improve the classification accuracy, we perform a feature selection process and then propose a feature weighting strategy to tune the classifier. In order to show that the proposed schemes are not language dependent, we apply the suggested schemes to two sets of data, i.e. English and Persian corpora. The evaluation results show that the feature selection and feature weighting strategies have a significant effect on the accuracy of the classification system. The results are also encouraging compared with the state of the art.",An accurate word sense disambiguation system based on weighted lexical features,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
432,http://dx.doi.org/10.1093/llc/fqs076,"{'string_id': '10.1093/llc/fqs076', 'id_scheme': 'DOI'}","This article will explain the structure and function of a digital research environment for Buddhist studies entitled Research Base for Indian and Buddhist Studies (RBIB). In the field of Buddhist studies (and especially in the area of Indian Buddhism), scholars face various obstacles in their efforts to share materials—in both paper and digital media. In establishing the RBIB, we have considered it to be of paramount importance to preserve the continuity of the prior tradition of studies within the paper medium, while at the same time developing a methodology that takes best advantage of the digital medium. After identifying six basic conditions that will satisfy the demands of both approaches, we have implemented a Web collaboration system that fulfills these conditions. Our system also makes functional parts of the RBIB that enable the presentation of the relationships between arbitrary fragments of related texts in various canonical languages through a user-friendly interface, which in turn allows users to learn how to edit and browse relationship data. Our system has thus far been highly evaluated by testers, and it has been further facilitated by recent developments in Information Communications Technology. Humanities digitization has a deep mutual relationship with this new technology.",Towards a digital research environment for Buddhist studies,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
433,http://dx.doi.org/10.1093/llc/fqs077,"{'string_id': '10.1093/llc/fqs077', 'id_scheme': 'DOI'}","This article describes the work performed in the Pattern Redundancy Analysis for Document Image Indexing and Transcription research project. The project focused on layout analysis, text/graphics separation, optical character recognition (OCR), and text transcription processes dedicated to old and precious books. The originality of this work relies on the analysis and exploitation of pattern redundancy in documents to enable the efficient indexing and quick transcription of books and the identification of typographic materials. For these purposes, we have developed two software packages. The first, AGORA, performs page layout analysis, text/graphics separation, and pattern (letterform) extraction simultaneously. These patterns are then processed to group similar patterns together in single clusters so that different letterforms of a book can be extracted and analysed to compute redundancy rates. This process allows a significant reduction of the number of letterforms to be recognized. Once the clustering of letterforms is done, a user may assign a label to each cluster using the second software, RETRO. Labels are then automatically assigned to each corresponding character to perform the text transcription of the whole book. Thus, if 90% of the letterforms are detected as redundant, only one character out of ten must be labelled by the user to transcribe the book. Moreover, this transcription method allows us to deal easily with the special characters that appear frequently in old books. It is also possible to use our clustering approach to extract and create new font packages from specific printing material (e.g. from rare books printed with particular types or woodblocks). These new font packages could be incorporated into the training step of optical fonts recognition methods to improve the recognition results of OCRs on rare or specific books. The identification of typographic materials could also be useful for the study of both the aesthetic (such as how the thickness and shape of printing types evolved from the 15th to the mid-16th century) and economic aspects of printing historically. Until the second half of the 16th century, for instance, printing types circulated among workshops, and printers frequently sold or lent types to their fellows.","Interactive layout analysis, content extraction, and transcription of historical printed books using Pattern Redundancy Analysis",Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
434,http://dx.doi.org/10.1093/llc/fqt001,"{'string_id': '10.1093/llc/fqt001', 'id_scheme': 'DOI'}","In digital humanities, within a core semantic scope, the term ‘text’ occurs ubiquitously, with both mass and count noun senses. This article sets out to define the relationship between the two senses—between some text and a text—and in particular to say what makes a text discrete. Three characteristics of a scholarly edition (considered the normative instance of a countable text) are isolated and discussed in relation to several marginal cases. I conclude that two of them—the representation of language and intent to communicate—give us text in the mass sense. Examining the third characteristic—that the communication be complete within its bounds—it becomes clear that it is impossible to say that an entity is intrinsically a text because the count noun sense of text is—as Renear and Dubin assert about three of the four Functional Requirements for Bibliographic Records Group 1 entity types—a role, not a type.",On the term 'text' in digital humanities,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
435,http://dx.doi.org/10.1093/llc/fqt002,"{'string_id': '10.1093/llc/fqt002', 'id_scheme': 'DOI'}","Quantifying the similarity or dissimilarity between documents is an important task in authorship attribution, information retrieval, plagiarism detection, text mining, and many other areas of linguistic computing. Numerous similarity indices have been devised and used, but relatively little attention has been paid to calibrating such indices against externally imposed standards, mainly because of the difficulty of establishing agreed reference levels of inter-text similarity. The present article introduces a multi-register corpus gathered for this purpose, in which each text has been located in a similarity space based on ratings by human readers. This provides a resource for testing similarity measures derived from computational text-processing against reference levels derived from human judgement, i.e. external to the texts themselves. We describe the results of a benchmarking study in five different languages in which some widely used measures perform comparatively poorly. In particular, several alternative correlational measures (Pearson r, Spearman rho, tetrachoric correlation) consistently outperform cosine similarity on our data. A method of using what we call ‘anchor texts’ to extend this method from monolingual inter-text similarity-scoring to inter-text similarity-scoring across languages is also proposed and tested.",Document dissimilarity within and across languages: A benchmarking study,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
436,http://dx.doi.org/10.1093/llc/fqt008,"{'string_id': '10.1093/llc/fqt008', 'id_scheme': 'DOI'}","In recent years, great availability of various language resources in different forms as well as rapid development of computer technology and programming skills have made researchers in the fields of linguistics and computer science cooperate in solving different problems of computational linguistics and natural language processing. Building large monolingual as well as bilingual corpora in digital forms and storing them in computer memories has enabled linguists and language engineers to automatically explore techniques for processing information with the help of various computer programs without any need to manually collect and analyze data.One of the main applications of monolingual corpora can be seen in developing automatic spell-checking systems. In such systems, a large monolingual corpus can function as a database instead of a monolingual dictionary. In the present study, it has been tried to demonstrate the effectiveness of a large monolingual corpus of Persian in improving the output quality of a spell-checker developed for this language.In the present spelling correction system, the three phases of error detection, making suggestions, and ranking suggestions are performed in three separate stages. An experiment was carried out to evaluate the performance of the spell-checking system.",FarsiSpell: A spell-checking system for Persian using a large monolingual corpus,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
437,http://dx.doi.org/10.1093/llc/fqt009,"{'string_id': '10.1093/llc/fqt009', 'id_scheme': 'DOI'}","This study investigates data from the BBC Voices project, which contains a large amount of vernacular data collected by the BBC between 2004 and 2005. The project was designed primarily to collect information on vernacular speech around the UK for broadcasting purposes. As part of the project, a web-based questionnaire was created, to which tens of thousands of people supplied their way of denoting thirty-eight variables that were known to exhibit marked lexical variation. Along with their variants, those responding to the online prompts provided information on their age, gender, and—significantly for this study—their location, this being recorded by means of their postcode. In this study, we focus on the relative frequency of the top ten variants for all variables in every postcode area. By using hierarchical spectral partitioning of bipartite graphs, we are able to identify four contemporary geographical dialect areas together with their characteristic lexical variants. Even though these variants can be said to characterize their respective geographical area, they also occur in other areas, and not all people in a certain region use the characteristic variant. This supports the view that dialect regions are not clearly defined by strict borders, but are fuzzy at best.",Analyzing the BBC Voices data: Contemporary English dialect areas and their characteristic lexical variants,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
438,http://dx.doi.org/10.1093/llc/fqt011,"{'string_id': '10.1093/llc/fqt011', 'id_scheme': 'DOI'}","Although the aggregation of many linguistic variables has provided new insights into the structure of language varieties, aggregation studies have been criticized for obscuring the behavior of individual input variables. Previous solutions to this criticism consisted of extensive post-hoc calculations, simple correlation measures, or highly complex algorithms. We think that these solutions can be improved. Therefore, the current article proposes a creative use of Individual Differences Scaling (INDSCAL) as an alternative, more straightforward solution. INDSCAL is a branch of Multidimensional Scaling, which is currently the preferred dimension reduction technique for most aggregation studies. The link to the existing methodology and the simplicity of its rationale are the main advantages of INDSCAL. The article introduces INDSCAL by means of a non-linguistic example, a discussion of the mathematical properties, and a case study on the lexical convergence between Belgian and Netherlandic Dutch in a corpus of language from 1950 and 1990. The case study shows how INDSCAL reproduces the results of a typical aggregation study, but elegantly keeps open the possibility of investigating the behavior of individual variables.",Transparent aggregation of variables with Individual Differences Scaling,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
439,http://dx.doi.org/10.1093/llc/fqt014,"{'string_id': '10.1093/llc/fqt014', 'id_scheme': 'DOI'}","MONK is a web-based text mining software application hosted by the University of Illinois Library that enables researchers to analyze encoded digital texts from select databases and digital archives. This study examines sets of quantitative and qualitative data to explore the usage of MONK as a research tool: the author analyzes eighteen months of web analytics data from the MONK website and responses from five interviews with MONK users to examine the ways in which MONK has been most commonly used by researchers. In the paper's analysis, the author considers the implications of MONK's use in digital humanities research and teaching, and how a digital humanities tool such as MONK can be maintained for public use. This study ultimately explores how user studies of digital humanities tools can reveal insights into humanities scholars' needs for using digital tools to pursue new research methodologies, and argues that studying the usability and preservation of digital humanities tools will enable information professionals to address humanities scholars' needs for their digital scholarship.",Under the Workbench: An analysis of the use and preservation of MONK text mining research software,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
440,http://dx.doi.org/10.1093/llc/fqt015,"{'string_id': '10.1093/llc/fqt015', 'id_scheme': 'DOI'}","We compute the rate of textual signals of risk of war recognizable in series of consecutive political speeches about a disputed issue serious enough to entail an international conflict. The speeches concern Iran’s nuclear program. We trace textual signals forewarning of risks of war that reactions to this affair lead to. The thrust of the textual analysis rests on the interplay of affiliation and power words in continuous texts, following D. C. McClelland’s model for anticipating wars. The speeches are those of Iranian President Mahmoud Ahmadinejad, US Secretary of State Hillary R. Clinton, Iranian Grand Ayatollah Ali Khamenei, and Israeli Prime Minister Benjamin Netanyahu. Prefiguring a military confrontation before it occurs involves structuring information from unstructured data. Despite such imperfect knowledge, by the end of January 2012, our results show a receding risk of war on the Iranian side, but an increasing risk on the American one, while remaining ambiguous on the Israeli one.",Textual fingerprints of risk of war,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,1,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
441,http://dx.doi.org/10.1093/llc/fqt016,"{'string_id': '10.1093/llc/fqt016', 'id_scheme': 'DOI'}","This article describes PlotVisML, a simple, flexible XML schema for encoding literary narratives that was developed by an interdisciplinary team of researchers in literary studies, interface design, computing studies, and education as part of a research project on reading, writing, and teaching complex literary narrative. PlotVisML is a simple, adaptable schema consisting of five key elements: <action>, <dialogue>, and <narration> (tags for marking up narrative events), and <character> and <object> (tags for encoding narrative objects). Fictional narratives that have been marked up using PlotVisML can be visualized in PlotVis, a digital scholarly tool that allows users to model and interact with literary narratives in three dimensions. Both PlotVis, an interactive visualization tool, and PlotVisML, our custom XML schema for encoding literary narratives, were designed to permit challenging new views on familiar plotlines and, more importantly, to depart from conventional ways of modeling narrative in literary instruction. In discussing the process of developing PlotVisML, we contribute to the ongoing discussion of text encoding as a form of close reading (e.g., Liepert, 2009).",Challenging new views on familiar plotlines: A discussion of the use of XML in the development of a scholarly tool for literary pedagogy,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
442,http://dx.doi.org/10.1093/llc/fqt018,"{'string_id': '10.1093/llc/fqt018', 'id_scheme': 'DOI'}","Most authorship attribution studies have focused on works that are available in the language used by the original author (Holmes, 1994; Juola, 2006) because this provides a direct way of examining an author's linguistic habits. Sometimes, however, questions of authorship arise regarding a work only surviving in translation. One example is ‘Constance’, the putative ‘last play’ of Oscar Wilde, only existing in a supposed French translation of a lost English original.The present study aims to take a step towards dealing with cases of this kind by addressing two related questions: (1) to what extent are authorial differences preserved in translation; (2) to what extent does this carry-over depend on the particular translator?With these aims, we analysed 262 letters written by Vincent van Gogh and by his brother Theo, dated between 1888 and 1890, each available in the original French and in an English translation. We also performed a more intensive investigation of a subset of this corpus, comprising forty-eight letters, for which two different English translations were obtainable. Using three different indices of discriminability (classification accuracy, Hedge's g, and area under the receiver operating characteristic curve), we found that much of the stylistic discriminability between the two brothers was preserved in the English translations. Subsidiary analyses were used to identify which lexical features were contributing most to inter-author discriminability.Discrimination between translation sources was possible, although less effective than between authors. We conclude that ‘handprints’ of both author and translator can be found in translated texts, using appropriate techniques.",Found in translation: To what extent is authorial discriminability preserved by translators?,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
443,http://dx.doi.org/10.1093/llc/fqt019,"{'string_id': '10.1093/llc/fqt019', 'id_scheme': 'DOI'}","In this article I develop a set of simple algorithms for deriving syllable count information for words from fixed-meter poetry. The focus is on the determination of what features of language or meter might be most useful. I therefore first review what factors might be useful for this, selecting those that require as little information as possible about the language in question and making as few computational demands as possible. We end up with algorithms based on: (i) the number of syllables in each line, (ii) the number of words in each line, (iii) the number of letters in those words, and (iv) the frequency of those words.I test these algorithms on corpora from English and Welsh, getting parallel results in both cases. The results establish that the variables I identify do have significant success in deriving syllable count, but that work remains to be done.",Calculating syllable count automatically from fixed-meter poetry in English and Welsh,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
444,http://dx.doi.org/10.1093/llc/fqt020,"{'string_id': '10.1093/llc/fqt020', 'id_scheme': 'DOI'}","Pearson’s chi-squared test is probably the most popular statistical test used in corpus linguistics, particularly for studying linguistic variations between corpora. Oakes and Farrow (2007) proposed various adaptations of this test to allow for the simultaneous comparison of more than two corpora while also yielding an almost correct Type I error rate (i.e. claiming that a word is most frequently found in a variety of English, when in actuality this is not the case). By means of resampling procedures, the present study shows that when used in this context, the chi-squared test produces far too many significant results, even in its modified version. Several potential approaches to circumventing this problem are discussed in the conclusion.",Inadequacy of the chi-squared test to examine vocabulary differences between corpora,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
445,http://dx.doi.org/10.1093/llc/fqt021,"{'string_id': '10.1093/llc/fqt021', 'id_scheme': 'DOI'}","We define a model of discourse coherence based on Barzilay and Lapata’s entity grids as a stylometric feature for authorship attribution. Unlike standard lexical and character-level features, it operates at a discourse (cross-sentence) level. We test it against and in combination with standard features on nineteen book-length texts by nine nineteenth-century authors. We find that coherence alone performs often as well as and sometimes better than standard features, though a combination of the two has the highest performance overall. We observe that despite the difference in levels, there is a correlation in performance of the two kinds of features.",Patterns of local discourse coherence as a feature for authorship attribution,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
446,http://dx.doi.org/10.1093/llc/fqt023,"{'string_id': '10.1093/llc/fqt023', 'id_scheme': 'DOI'}","This study is designed to analyze the use of nominalization in English translations of Chinese literary prose based on eight English translations of Chinese novels. It follows ‘Lees, R. (1963). The Grammar of English Nominalizations. The Hague: Mouton’ in defining English nominalization as a nominalized transform of a finite verbal form and ‘Mathesius, V. (1975). Selected Writings in English and General Linguistics. The Hague: Mouton’ theory of ‘complex condensation of the sentence’. It describes English nominalization from the formal-syntactic level as adverbial, in the positions of subject and object, condensing a finite clausal structure. In the qualitative analysis, various effects of the use of nominalization are described based on three English versions of the Chinese classic novel Hong Lou Meng. In the quantitative analysis, three general patterns of the use of nominalization are found in the eight English translations of Chinese novels: it is predominantly used as adverbial (as opposed to in the positions of object and subject), in the form of gerundive nominalization (as opposed to derived and zero-derived nominalizations), and in the narrative (as opposed to dialogues). In comparison with nominalization used in some English novels, it is found that nominalization is significantly more used in the English translations of Chinese novels at large.",A corpus-based study of nominalization in English translations of Chinese literary prose,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
447,http://dx.doi.org/10.1093/llc/fqt025,"{'string_id': '10.1093/llc/fqt025', 'id_scheme': 'DOI'}","The political negotiation, erection, and fall of national and cultural borders represent an issue that frequently occupies the media. Given the historical importance of boundaries as a marker of cultural identity, as well as their function to separate and unite people, the Body Type Dictionary (BTD; Wilson, 2006) represents a suitable computerized content analysis measure to analyse vocabulary qualified to measure body boundaries and their penetrability. Out of this context, this study aimed to assess the inter-method reliability of the BTD (Wilson, 2006) in relation to Fisher and Cleveland’s (1956, 1958) manual scoring system for high and low barrier personalities. The results indicated that Fisher and Cleveland’s manually coded barrier and penetration imagery scores showed an acceptable positive correlation with the computerized frequency counts of the BTD’s coded barrier and penetration imagery scores, thereby indicating an inter-method reliability. In addition, barrier and penetration imagery correlated positively with primordial thought language in the picture response test, and narratives of everyday and dream memories, thereby indicating correlational validity.",Assessing the inter-method reliability and correlational validity of the Body Type Dictionary,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
448,http://dx.doi.org/10.1093/llc/fqt026,"{'string_id': '10.1093/llc/fqt026', 'id_scheme': 'DOI'}","In this article, the application of Ant-Colony Optimization (ACO) to a morphological segmentation task is described, where the aim is to analyse a set of words into their constituent stem and ending. A number of criteria for determining the optimal segmentation are evaluated comparatively while at the same time investigating more comprehensively the effectiveness of the ACO system in defining appropriate values for system parameters. Owing to the characteristics of the task at hand, particular emphasis is placed on studying the ACO process for learning sessions of a limited duration. Morphological segmentation becomes hardest in highly inflectional languages, where each stem is associated with a large number of distinct endings. Consequently, the present article investigates morphological segmentation of words from a highly inflectional language, specifically Ancient Greek, by combining pattern-recognition principles with limited linguistic knowledge. To weigh these sources of knowledge, a set of weights is used as a set of system parameters, to be optimized via ACO. ACO-based experimental results are shown to be of a higher quality than those achieved by manual optimisation or ‘randomised generate and test’ methods. This illustrates the applicability of the ACO-based approach to the morphological segmentation task.",Optimizing word segmentation tasks using ant colony metaheuristics,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
449,http://dx.doi.org/10.1093/llc/fqt027,"{'string_id': '10.1093/llc/fqt027', 'id_scheme': 'DOI'}","The study investigates to what extent traditional stylistics and non-traditional stylometry can co-operate in the study of translations in terms of translatorial style. Stylistic authorship attribution methods based on a multivariate analysis of most-frequent-word frequencies are used in attempts at identifying translators. While these methods usually identify the author of the original rather than the translator, a case study is presented of the Polish translation of a single novel by Virginia Woolf, Night and Day, in which one translator took over from the other; the point of this takeover has been successfully identified with the above-mentioned methods.",The stylistics and stylometry of collaborative translation: Woolf's Night and Day in Polish,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
450,http://dx.doi.org/10.1093/llc/fqt028,"{'string_id': '10.1093/llc/fqt028', 'id_scheme': 'DOI'}","The frequencies of individual words have been the mainstay of computer-assisted authorial attribution over the past three decades. The usefulness of this sort of data is attested in many benchmark trials and in numerous studies of particular authorship problems. It is sometimes argued, however, that since language as spoken or written falls into word sequences, on the ‘idiom principle’, and since language is characteristically produced in the brain in chunks, not in individual words, n-grams with n higher than 1 are superior to individual words as a source of authorship markers. In this article, we test the usefulness of word n-grams for authorship attribution by asking how many good-quality authorship markers are yielded by n-grams of various types, namely 1-grams, 2-grams, 3-grams, 4-grams, and 5-grams. We use two ways of formulating the n-grams, two corpora of texts, and two methods for finding and assessing markers. We find that when using methods based on regularly occurring markers, and drawing on all the available vocabulary, 1-grams perform best. With methods based on rare markers, and all the available vocabulary, strict 3-gram sequences perform best. If we restrict ourselves to a defined word-list of function-words to form n-grams, 2-grams offer a striking improvement on 1-grams.","Language chunking, data sparseness, and the value of a long marker list: explorations with word n-grams and authorial attribution",Oxford University Press (OUP),2013,,Literary and Linguistic Computing,29,2,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
451,http://dx.doi.org/10.1093/llc/fqt030,"{'string_id': '10.1093/llc/fqt030', 'id_scheme': 'DOI'}","In an article published in Literary and Linguistic Computing, Redfern argues against the use of a lognormal distribution and summarizes previous work as ‘lacking in methodological detail and statistical rigour’. This response will summarize the article’s methodology and conclusion, arguing that while Redfern finds that films are not ‘perfectly’ lognormal, this is hardly evidence worthy of the ultimate conclusion that a lognormal fit is ‘inappropriate’. Perfection is fleeting, and cannot be expected when modeling real data. Reanalysis of Redfern’s methodology and findings shows that the lognormal distribution offers a pretty good fit.","Horseshoes, handgrenades, and model fitting: the lognormal distribution is a pretty good model for shot-length distribution of Hollywood films",Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
452,http://dx.doi.org/10.1093/llc/fqt031,"{'string_id': '10.1093/llc/fqt031', 'id_scheme': 'DOI'}","Much research in translation studies indicates that translated texts are ontologically different from original non-translated ones. Translated texts, in any language, can be considered a dialect of that language, known as ‘translationese’. Several characteristics of translationese have been proposed as universal in a series of hypotheses. In this work, we test these hypotheses using a computational methodology that is based on supervised machine learning. We define several classifiers that implement various linguistically informed features, and assess the degree to which different sets of features can distinguish between translated and original texts. We demonstrate that some feature sets are indeed good indicators of translationese, thereby corroborating some hypotheses, whereas others perform much worse (sometimes at chance level), indicating that some ‘universal’ assumptions have to be reconsidered.In memoriam: Miriam Shlesinger, 1947–2012",On the features of translationese,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
453,http://dx.doi.org/10.1093/llc/fqt032,"{'string_id': '10.1093/llc/fqt032', 'id_scheme': 'DOI'}","Stemmatology, or the reconstruction of the transmission history of texts, is a field that stands particularly to gain from digital methods. Many scholars already take stemmatic approaches that rely heavily on computational analysis of the collated text (e.g. Robinson and O’Hara 1996; Salemans 2000; Heikkilä 2005; Windram et al. 2008 among many others). Although there is great value in computationally assisted stemmatology, providing as it does a reproducible result and allowing access to the relevant methodological process in related fields such as evolutionary biology, computational stemmatics is not without its critics. The current state-of-the-art effectively forces scholars to choose between a preconceived judgment of the significance of textual differences (the Lachmannian or neo-Lachmannian approach, and the weighted phylogenetic approach) or to make no judgment at all (the unweighted phylogenetic approach). Some basis for judgment of the significance of variation is sorely needed for medieval text criticism in particular. By this, we mean that there is a need for a statistical empirical profile of the text-genealogical significance of the different sorts of variation in different sorts of medieval texts. The rules that apply to copies of Greek and Latin classics may not apply to copies of medieval Dutch story collections; the practices of copying authoritative texts such as the Bible will most likely have been different from the practices of copying the Lives of local saints and other commonly adapted texts. It is nevertheless imperative that we have a consistent, flexible, and analytically tractable model for capturing these phenomena of transmission. In this article, we present a computational model that captures most of the phenomena of text variation, and a method for analysis of one or more stemma hypotheses against the variation model. We apply this method to three ‘artificial traditions’ (i.e. texts copied under laboratory conditions by scholars to study the properties of text variation) and four genuine medieval traditions whose transmission history is known or deduced in varying degrees. Although our findings are necessarily limited by the small number of texts at our disposal, we demonstrate here some of the wide variety of calculations that can be made using our model. Certain of our results call sharply into question the utility of excluding ‘trivial’ variation such as orthographic and spelling changes from stemmatic analysis.",Beyond the tree of texts: Building an empirical model of scribal variation through graph analysis of texts and stemmata,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
454,http://dx.doi.org/10.1093/llc/fqt035,"{'string_id': '10.1093/llc/fqt035', 'id_scheme': 'DOI'}","Ambiguity is one of the most significant problems in Natural Language Processing. This difficulty may not be apparent to native speakers because of their natural ability at resolving it using contextual information and common sense knowledge. In contrast, current computer applications are still lacking the ability to disambiguate complex texts efficiently. The most common type of ambiguity is lexical ambiguity, and this is noticed even in highly inflectional languages such as Greek. In the present article, all the patterns of predictable lexical ambiguity in Modern Greek Language are registered, verified and quantified as occurred in the Neurolingo computational lexicon, after a study of morpho-syntactic characteristics that differentiate the ambiguous words.",Analysis of lexical ambiguity in Modern Greek using a computational lexicon,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
455,http://dx.doi.org/10.1093/llc/fqt036,"{'string_id': '10.1093/llc/fqt036', 'id_scheme': 'DOI'}","The purpose of this article is to examine the linguistic differences between poems written by ‘amateurs’ and those written by ‘professionals’ and then to use these characteristics to rank a number of contemporary American poems. The corpus of poems used consist of 100 poems randomly selected from a recent anthology of professional poets and a control group of 100 poems written by amateurs. The poems were reduced to ninety-eight linguistic and psycholinguistic variables, and these were used in a machine learning algorithm to build an ensemble classifier. The accuracy of the classifier was 84.5%. The probability scores of the individual poems was then used to rank the professional poems on a continuum representing amateur at one extreme and professional at the other, thereby providing an objective means of ranking contemporary poems.",Ranking contemporary American poems,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
456,http://dx.doi.org/10.1093/llc/fqt037,"{'string_id': '10.1093/llc/fqt037', 'id_scheme': 'DOI'}","Structured Prosopography provides a formal model for representing prosopography: a branch of historical research that traditionally has focused on the identification of people that appear in historical sources. Since the 1990s, KCL’s Department of Digital Humanities has been involved in the development of structured prosopographical databases using a general ‘factoid-oriented’ model of structure that links people to the information about them via spots in primary sources that assert that information. Recent developments, particularly the World Wide Web, and its related technologies around the Semantic Web, have promoted the possibility to both interconnecting dispersed data, and allowing it to be queried semantically. To the purpose of making available our prosopographical databases on the Semantic Web, in this article we review the principles behind our established factoid-based model and reformulate it using a more interoperable approach, based on knowledge representation principles and formal ontologies. In particular, we are going to focus primarily on a high-level semantic analysis of the factoid notion, on its relation to other cultural heritage standards such as CIDOC-CRM, and on the modularity and extensibility of the proposed solutions.",Factoid-based prosopography and computer ontologies: towards an integrated approach,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
457,http://dx.doi.org/10.1093/llc/fqt038,"{'string_id': '10.1093/llc/fqt038', 'id_scheme': 'DOI'}","The article describes an ongoing project that aims at building a reference corpus of German computer-mediated communication (CMC) as a new component of an already existing reference corpus of written contemporary German. The ‘Deutsches Referenzkorpus zur internetbasierten Kommunikation’ (DeRiK) shall include data from the most prominent CMC genres amongst German Internet users and, thus, close a gap in the coverage of the corpus resources in the project ‘Digitales Wörterbuch der deutschen Sprache’ (DWDS), which are maintained and provided by the Berlin-Brandenburg Academy of Sciences and Humanities. The focus of the article is on the role of the DeRiK component within the DWDS framework, on sampling issues, and on CMC-specific issues of corpus annotation.",DeRiK: A German reference corpus of computer-mediated communication,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
458,http://dx.doi.org/10.1093/llc/fqt039,"{'string_id': '10.1093/llc/fqt039', 'id_scheme': 'DOI'}","In computational stylistics, any influence of unwanted noise—e.g. caused by an untidily prepared corpus—might lead to biased or false results. Relying on contaminated data is similar to using dirty test tubes in a laboratory: it inescapably means falling into systematic error. An important question is what degree of nonchalance is acceptable to obtain sufficiently reliable results. The present study attempts to verify the impact of unwanted noise in a series of experiments conducted on several corpora of English, German, Polish, Ancient Greek, and Latin prose texts. In 100 iterations, a given corpus was gradually damaged, and controlled tests for authorship were applied. The first experiment was designed to show the correlation between a dirty corpus and attribution accuracy. The second was aimed to test how disorder in word frequencies—produced by scribal and/or editorial modifications—affects the attribution abilities of particular corpora. The goal of the third experiment was to test how much ‘authorial’ data a given text needs to have to trace authorial fingerprint through a mass of external quotations.",Mind your corpus: systematic errors in authorship attribution,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
459,http://dx.doi.org/10.1093/llc/fqt040,"{'string_id': '10.1093/llc/fqt040', 'id_scheme': 'DOI'}","Used here to describe the investigation of significant sound or prosodic patterns within the context of a system that can translate these patterns into comparative visualizations across texts, the term ‘distant listening’ is used provocatively to suggest that readers might interpret prosodic patterns as ‘noise’ (or seemingly unintelligible information) with close reading practices. In this study, we show that these same patterns appear coherent and discoverable within ProseVis, a visualization tool that supports these hermeneutics within a discovery-based paradigm that allows for new ways of making meaning. Charles Bernstein discusses ‘close listening’ as possibly contradictory to ‘ “readings” of poems that are based exclusively on the printed text and that ignore the poet’s own performances, the “total” sound of the work, and the relation of sound to semantics’ (Bernstein, 1998, p. 4). Likewise, this study considers the efficacy of using prosodic textual elements as features for similarity metrics instead of or alongside words and n-gram frequencies. In particular, this discussion describes the continued development of this work as a contribution to and within the context of authorship attribution and stylometric studies that consider the interpretability of prosodic features. To that end, in the first part of this discussion, we place the study within the theoretical and practical context of author attribution studies. In the second part of this discussion, we consider how changing similarity metric calculations through the inclusion and exclusion of certain prosodic features (such as tone and stress) and algorithmic parameters (such as the window size of sounds and weighting power) can facilitate the discovery of previously unidentifiable author-similarity patterns. Finally, in the third part of this study, we explore questions of identity construction within this framework of author attribution analysis by comparing ‘Melanctha’, the longest story in Gertrude Stein’s Three Lives (1909), with 150 different narrative voices from the First Person Narratives of the Documenting the American South collection.",Distant Listening to Gertrude Stein's 'Melanctha': Using Similarity Analysis in a Discovery Paradigm to Analyze Prosody and Author Influence,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
460,http://dx.doi.org/10.1093/llc/fqt041,"{'string_id': '10.1093/llc/fqt041', 'id_scheme': 'DOI'}","Redfern [(2012), The lognormal distribution is not an appropriate parametric model for shot length distributions of Hollywood films. Literary and Linguistic Computing, doi:10.1093/llc/fqs066] disputes the idea that the lognormal distribution is a suitable parametric model for the shot length distribution of films. The force of his arguments against the idea is diminished by problems in his exposition, the most serious is the manner in which hypothesis testing is deployed. The application fails to deal adequately with the effect sample size has on P-values, and this compromises much of the analysis. There is, nevertheless, strong evidence of a fairly systematic departure from lognormality, manifest in the fact that distributions mostly remain positively skewed after log-transformation. This is not recognized in the article and thus not exploited. The present article shows that after a second transformation, normality can be achieved for well over half the films, which are thus distributionally regular in this sense. Some suggestions as to why lognormality, or any other form of distributional regularity, might be of interest are offered at the start of the article, which concludes with an illustration of how the establishment of a distributional ‘norm’ might then be exploited.",On the distributional regularity of shot lengths in film,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
461,http://dx.doi.org/10.1093/llc/fqt044,"{'string_id': '10.1093/llc/fqt044', 'id_scheme': 'DOI'}","This article demonstrates that the history of computing in the humanities is an almost uncharted research topic. It argues that this oversight must be remedied as a matter of urgency so that the evolutionary model of progress that currently dominates the field can be countered. We describe the ‘Hidden Histories’ pilot project and explore the origins and practice of oral history; in the corresponding issue of Digital Humanities Quarterly, five oral history interviews that we carried out during the project are presented. We conclude that the selection of interviews presented here demonstrate that oral history is an important and productive methodology in such research. The five oral history interviews form primary sources, which can be used in the writing of a history of computing in the humanities; furthermore, they contain new information and interpretations, which cannot be gleaned from published scholarly articles, for example, information about the varied entry routes into the field that have existed and the interrelationship between myth and history in the narratives we create about the emergence of digital humanities.",Oral History and the Hidden Histories project: towards histories of computing in the humanities,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
462,http://dx.doi.org/10.1093/llc/fqt045,"{'string_id': '10.1093/llc/fqt045', 'id_scheme': 'DOI'}","Skaldic poetry is a highly complex textual phenomenon both in terms of the intricacy of the poetry and its contextual environment. Extensible Markup Language (XML) applications such as that of the Text Encoding Initiative provide a means of semantic representation of some of these complexities. XML, however, has limitations in representing semantic relationships that do not conform to the tree model. This article presents the relational data model as a way of representing the structure of skaldic texts and their contextual environment. The relational data model raises both problems and possibilities for this type of project. The main problem addressed here is the representation of the syntagmatic structures of texts in a data model that is not intrinsically ordered. The advantages are also explored, including networked data editing and management, quantitative linguistic analysis, dynamic representation of the data, and the ability to extend the structure and reuse data for related projects without creating redundancy.",Relational data modelling of textual corpora: The Skaldic Project and its extensions,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
463,http://dx.doi.org/10.1093/llc/fqt046,"{'string_id': '10.1093/llc/fqt046', 'id_scheme': 'DOI'}","The Department of Classics at Tufts University and the Perseus Project have jointly designed and tested an integrated platform (the Perseids Platform) on which students and scholars can collaboratively transcribe, edit, and translate Latin and Greek texts, creating vetted open source digital editions. This project, while giving students the opportunity to work with original untranslated documents, also contributes to the efforts of the scholarly community worldwide to meet the challenge of publishing large numbers of primary source documents online while preserving high editorial standards. The platform integrates the Son of SUDA Online software, originally developed to edit papyrological texts, and the Collections, Indexes, and Texts, with Extensions architecture, originally developed by the Center for Hellenic Studies of Harvard University to support the Homer Multitext Project. The present article discusses our scholarly and pedagogical objectives in developing the platform, the technical challenges we faced in the course of our work, and the results we obtained.",Developing a New Integrated Editing Platform for Source Documents in Classics,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
464,http://dx.doi.org/10.1093/llc/fqt047,"{'string_id': '10.1093/llc/fqt047', 'id_scheme': 'DOI'}","Different computational models have been proposed to automatically determine the most probable author of a disputed text (authorship attribution). These models can be viewed as special approaches in the text categorization domain. In this perspective, in a first step we need to determine the most effective features (words, punctuation symbols, part-of-speech, bigram of words, etc.) to discriminate between different authors. To achieve this, we can consider different independent feature-scoring selection functions (information gain, gain ratio, pointwise mutual information, odds ratio, chi-square, bi-normal separation, GSS, Darmstadt Indexing Approach (DIA), and correlation coefficient). Other term selection strategies have also been suggested in specific authorship attribution studies. To compare these two families of selection procedures, we have extracted articles from two newspapers and belonging to two categories (sports and politics). To enlarge the basis of our evaluations, we have chosen one newspaper written in the English language (‘Glasgow Herald’) and a second one in Italian (‘La Stampa’). The resulting collections contain from 987 to 2,036 articles written by four to ten columnists. Using the Kullback–Leibler divergence, the chi-square measure and the Delta rule as attribution schemes, this study found that some simple selection strategies (based on occurrence frequency or document frequency) may produce similar, and sometimes better, results compared with more complex ones.",Comparative evaluation of term selection functions for authorship attribution,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
465,http://dx.doi.org/10.1093/llc/fqt049,"{'string_id': '10.1093/llc/fqt049', 'id_scheme': 'DOI'}","This contribution discusses some of the challenges involved in building an ontology for research about the philosopher Ludwig Wittgenstein. It pays special attention to different ontological conceptions (event based versus object based). It also discusses how best to model, within the ontology, conflicting views emerging in both Wittgenstein’s work and Wittgenstein scholarship. The contribution presents relevant work in progress at the Wittgenstein Archives at the University of Bergen, which has a special focus on Wittgenstein’s Nachlass, his philosophical estate.",Sharing and debating Wittgenstein by using an ontology,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
466,http://dx.doi.org/10.1093/llc/fqt050,"{'string_id': '10.1093/llc/fqt050', 'id_scheme': 'DOI'}","In this article we propose an approach to the study of art history based on geography of Hispanic Baroque art by digital means that showcase the multiplicity of possible places of art. Our study advances four elements of a digital geography of art (communities, semantic maps, areas, and flows)—a methodology that can be expanded in future Digital Humanities research.",Towards a digital geography of Hispanic Baroque art,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
467,http://dx.doi.org/10.1093/llc/fqt053,"{'string_id': '10.1093/llc/fqt053', 'id_scheme': 'DOI'}","In this article, we provide a discussion of the concept of visual interactive workflows, how they relate to our previous work on structured surfaces, and how they have been adapted to experiments in managing articles for journal publication and managing biographical histories being written and tagged in XML. We conclude with a user experience study of the prototypes, which suggests that they are relatively acceptable at the level of reflective response, but might benefit from more iteration in their use of process and element metaphors.",Visual workflow interfaces for editorial processes,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
468,http://dx.doi.org/10.1093/llc/fqt054,"{'string_id': '10.1093/llc/fqt054', 'id_scheme': 'DOI'}","Although a substantial corpus of digital materials is now available to scholarship across the disciplines, objective evidence of their use, impact, and value, based on a robust assessment, is sparse. Traditional methods of assessment of impact in the humanities, notably citation in scholarly publications, are not an effective way of assessing impact of digital content. These issues are problematic in the field of Digital Humanities where there is a need to effectively assess impact to justify its continued funding and existence. A number of qualitative and quantitative methods exist that can be used to monitor the use of digital resources in various contexts although they have yet to be applied widely. These have been made available to the creators, managers, and funders of digital content in an accessible form through the TIDSR (Toolkit for the Impact of Digital Scholarly Resources) developed by the Oxford Internet Institute. In 2011, the authors of this article developed the SPHERE project (Stormont Parliamentary Hansards: Embedded in Research and Education) specifically to use TIDSR to evaluate the use and impact of The Stormont Papers, a digital collection of the Hansards of the Stormont Northern Irish Parliament from 1921 to 1972. This article presents the methodology, findings, and analysis of the project. The authors argue that TIDSR is a useful and, critically, transferrable method to understand and increase the impact of digital resources. The findings of the project are modified into a series of wider recommendations on protecting the investment in digital resources by increasing their use, value, and impact. It is reasonable to suggest that effectively showing the impact of Digital Humanities is critical to its survival.",Assessing and measuring impact of a digital collection in the humanities: An analysis of the SPHERE (Stormont Parliamentary Hansards: Embedded in Research and Education) Project,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
469,http://dx.doi.org/10.1093/llc/fqt057,"{'string_id': '10.1093/llc/fqt057', 'id_scheme': 'DOI'}","According to Saussure (Saussure and Harris, 2001), there are two systems of writing. Phonetic systems try to respond to the succession of sounds that make up a word, based on the irreducible elements used in speaking. In an ideographic system, each word is represented by a single sign that is unrelated to the sounds of the word itself, but expresses the idea which the word represents. The classic example of an ideographic writing system is Chinese. There is a common misunderstanding that treats each Chinese character as a separate sign. Hanzi (the system of Chinese Characters) serves as a kind of common information carrier created and shared by the whole Chinese society, and as such it is obviously neither disorderly nor unsystematic. On the contrary, there must be some intrinsic laws that make Hanzi the most mature ideographic system in the world. As Complex Network analysis has become an effective tool to analyze complex systems, this article aims to use some of these methods to identify such intrinsic laws of the Hanzi system.",Complex network perspective on graphic form system of Hanzi,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
470,http://dx.doi.org/10.1093/llc/fqt058,"{'string_id': '10.1093/llc/fqt058', 'id_scheme': 'DOI'}","This research article provides a summary of our approach in providing integrated multilingual access to diverse digital archives of Japanese Ukiyo-e prints. We propose a novel method, which generates links across databases using Linked Data. A retrieval system has been developed for realizing the proposed method, which dynamically creates links to miscellaneous Linked Data resources from authority data. By using the proposed method, users would allowed to access to additional data about a certain record in multiple databases not depending on languages and formats of each database. Experimental evaluations have conducted to measure reliability of the generated links. The experimental results show high accuracy rate and prove the effectiveness of the proposed method. Links to major Linked Data resources were created successfully.",Linked data driven multilingual access to diverse Japanese Ukiyo-e databases by generating links dynamically,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
471,http://dx.doi.org/10.1093/llc/fqt059,"{'string_id': '10.1093/llc/fqt059', 'id_scheme': 'DOI'}","There is a need in the humanities for a 3D WebGIS with analytical tools that allow researchers to analyze 3D models linked to spatially referenced data. Geographic Information Systems (GIS) allow for complex spatial analysis of 2.5D data. For example, they offer bird’s eye views of landscapes with extruded building footprints, but one cannot ‘get on the ground’ and interact with true 3D models from a pedestrian perspective. Meanwhile, 3D models and virtual environments visualize data in 3D space, but analytical tools are simple rotation or lighting effects. The MayaArch3D Project is developing a 3D WebGIS—called QueryArch3D—to allow these two distinct approaches to ‘talk to each other’ for studies of architecture and landscapes—in this case, the eighth-century Maya kingdom of Copan, Honduras. With this tool, researchers can search and query, in real time via a virtual reality (VR) environment, segmented 3D models of multiple resolutions (as well as computer-assisted design and reality-based) that are linked to attribute data stored in a spatial database. Beta tests indicate that this tool can assist researchers in expanding questions and developing new analytical methods in humanities research. This article summarizes the results of a pilot project that started in 2009, with an art historian and an archaeologist’s collaborative research on the ancient Maya kingdom and UNESCO World Heritage site of Copan in Honduras—called MayaArch3D. The project researches innovative approaches to integrate GIS, 3D digital models, and VR environments online for teaching and research on ancient architecture and landscapes. It has grown into an international, interdisciplinary project that brings together art historians, archaeologists, and cultural resource managers with experts in remote sensing, photogrammetry, 3D modeling, and VR. The Start Up Phase was funded by two National Endowment for the Humanities, Digital Humanities Start-Up grants to the University of New Mexico (PI: Jennifer von Schwerin) and developed and beta tested a pipeline and prototype 3D WebGIS—called QueryArch3D. The prototype version is available at http://mayaarch3d.org/project-history/). Project results indicate that it is possible to bridge the gap between 3D and GIS to create a resource for researchers of Maya architecture to compare and analyze 3D models and archaeological data in the context of a geographically referenced, VR landscape.",The MayaArch3D project: A 3D WebGIS for analyzing ancient architecture and landscapes,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
472,http://dx.doi.org/10.1093/llc/fqt060,"{'string_id': '10.1093/llc/fqt060', 'id_scheme': 'DOI'}","Scholars are using the Web every day to search, read, collaborate, and ultimately do their research. While some of the basic activities that the scholars do, such as reading and writing papers, are already well supported in the digital world, some essential scholarly primitives, such as annotation, augmentation, contextualization, and externalization, do not yet have clear support in terms of software tools. What scholars ultimately do during their research activity is to iteratively and collaboratively create new knowledge. With the advent of the Digital Humanities, we now have the opportunity—and technology—to capture at least a part of this knowledge and make it available as machine-processable data so to be better explorable and discoverable. In this paper, we present and discuss Pundit: a novel semantic annotation tool that enables scholars to collect, annotate, and contextualize Web resources. Deep-linking is used in conjunction with an RDF-based data model to allow granular selection of content (e.g. text excerpts, image fragments). Pundit aims at enabling scholars to produce meaningful machine-readable data that captures the semantics of their annotations. By providing a customizable annotation environment, where domain specific vocabularies can be loaded, and easy ways of integrating with existing Web archives or libraries, Pundit enables users to publish their annotations and collaboratively build a semantic graph. Such a graph can be consumed via HTTP APIs and standard SPARQL, thus allowing existing Linked Data applications to easily work with the data and Web clients in general to build specific visualizations.",Pundit: augmenting web contents with semantics,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
473,http://dx.doi.org/10.1093/llc/fqt061,"{'string_id': '10.1093/llc/fqt061', 'id_scheme': 'DOI'}","In this article we apply up-to-date methods of quantitative language comparison, inspired by algorithms successfully applied in bioinformatics to decode DNA and determine the genetic relatedness of humans, to language data in an attempt to shed light on the current situation of a family of languages called Dogon, which are spoken in Mali, West Africa. Our aim is to determine the linguistic subgroupings of these languages, which we believe will shed light on their prehistory, highlight the linguistic diversity of these groups and which may ultimately inform studies on the cultural boundaries of these languages.",Investigating the relatedness of the endangered Dogon languages,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,28,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
474,http://dx.doi.org/10.1093/llc/fqt063,"{'string_id': '10.1093/llc/fqt063', 'id_scheme': 'DOI'}","Hildegard of Bingen (1098–1179) is one of the most influential female authors of the Middle Ages. From the point of view of computational stylistics, the oeuvre attributed to Hildegard is fascinating. Hildegard dictated her texts to secretaries in Latin, a language of which she did not master all grammatical subtleties. She therefore allowed her scribes to correct her spelling and grammar. Especially Hildegard’s last collaborator, Guibert of Gembloux, seems to have considerably reworked her works during his secretaryship. Whereas her other scribes were only allowed to make superficial linguistic changes, Hildegard would have permitted Guibert to render her language stylistically more elegant. In this article, we focus on two shorter texts: the Visio ad Guibertum missa and Visio de Sancto Martino, both of which Hildegard allegedly authored during Guibert’s secretaryship. We analyze a corpus containing the letter collections of Hildegard, Guibert, and Bernard of Clairvaux using a number of common stylometric techniques. We discuss our results in the light of the Synergy Hypothesis, suggesting that texts resulting from collaboration can display a style markedly different from that of the collaborating authors. Finally, we demonstrate that Guibert must have reworked the disputed visionary texts allegedly authored by Hildegard to such an extent that style-oriented computational procedures attribute the texts to Guibert.",Collaborative authorship in the twelfth century: A stylometric study of Hildegard of Bingen and Guibert of Gembloux,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
475,http://dx.doi.org/10.1093/llc/fqt065,"{'string_id': '10.1093/llc/fqt065', 'id_scheme': 'DOI'}","When evaluating machine translation outputs, linguistics is usually taken into account implicitly. Annotators have to decide whether a sentence is better than another or not, using, for example, adequacy and fluency criteria or, as recently proposed, editing the translation output so that it has the same meaning as a reference translation, and it is understandable. Therefore, the important fields of linguistics of meaning (semantics) and grammar (syntax) are indirectly considered. In this study, we propose to go one step further towards a linguistic human evaluation. The idea is to introduce linguistics implicitly by formulating precise guidelines. These guidelines strictly mark the difference between the sub-fields of linguistics such as: morphology, syntax, semantics, and orthography. We show our guidelines have a high inter-annotation agreement and wide-error coverage. Additionally, we examine how the linguistic human evaluation data correlate with: among different types of machine translation systems (rule and statistical-based); and with adequacy and fluency.",Towards human linguistic machine translation evaluation,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
476,http://dx.doi.org/10.1093/llc/fqt066,"{'string_id': '10.1093/llc/fqt066', 'id_scheme': 'DOI'}","The aim of this study is to find such a minimal size of text samples for authorship attribution that would provide stable results independent of random noise. A few controlled tests for different sample lengths, languages, and genres are discussed and compared. Depending on the corpus used, the minimal sample length varied from 2,500 words (Latin prose) to 5,000 or so words (in most cases, including English, German, Polish, and Hungarian novels). Another observation is connected with the method of sampling: contrary to common sense, randomly excerpted ‘bags of words’ turned out to be much more effective than the classical solution, i.e. using original sequences of words (‘passages’) of desired size. Although the tests have been performed using the Delta method (Burrows, J.F. (2002). ‘Delta’: a measure of stylistic difference and a guide to likely authorship. Literary and Linguistic Computing, 17(3): 267–87) applied to the most frequent words, some additional experiments have been conducted for support vector machines and k-NN applied to most frequent words, character 3-grams, character 4-grams, and parts-of-speech-tag 3-grams. Despite significant differences in overall attributive success rate between particular methods and/or style markers, the minimal amount of textual data needed for reliable authorship attribution turned out to be method-independent.","Does size matter? Authorship attribution, small samples, big problem",Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
477,http://dx.doi.org/10.1093/llc/fqt068,"{'string_id': '10.1093/llc/fqt068', 'id_scheme': 'DOI'}","This article explores a concept of method in computer-assisted literary criticism, using a current digital humanities project as a case study. The project is investigating aspects of intertextuality between English poetry and the Oxford English Dictionary, second edition (OED2). In the course of adopting, applying, and adapting methods to guide computer-assisted comparisons between OED2 and poetry corpora, questions have arisen about the desired relations among research question, method, result, and outcome. Arguing that additional deliberation on what method means to us is now both appropriate and essential to the maturing discipline of digital humanities, in this article I discuss what digital methods have shown us about OED2 and Geoffrey Hill’s notoriously intertextual long poem The Triumph of Love (1998), both as an example and an illustration of one way of reflecting on these questions: a concept of digital method as tautology.",Method as tautology in the digital humanities,Oxford University Press (OUP),2013,,Literary and Linguistic Computing,30,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
478,http://dx.doi.org/10.1093/llc/fqu001,"{'string_id': '10.1093/llc/fqu001', 'id_scheme': 'DOI'}","We describe, evaluate, and improve the automatic annotation of diachronic corpora at the levels of word-class, lemma, chunks, and dependency syntax. As corpora we use the ARCHER corpus (texts from 1600 to 2000) and the ZEN corpus (texts from 1660 to 1800). Performance on Modern English is considerably lower than on Present Day English (PDE). We present several methods that improve performance. First we use the spelling normalization tool VARD to map spelling variants to their PDE equivalent, which improves tagging. We investigate the tagging changes that are due to the normalization and observe improvements, deterioration, and missing mappings. We then implement an optimized version, using VARD rules and preprocessing steps to improve normalization. We evaluate the improvement on parsing performance, comparing original text, standard VARD, and our optimized version. Over 90% of the normalization changes lead to improved parsing, and 17.3% of all 422 manually annotated sentences get a net improved parse. As a next step, we adapt the parser’s grammar, add a semantic expectation model and a model for prepositional phrases (PP)-attachment interaction to the parser. These extensions improve parser performance, marginally on PDE, more considerably on earlier texts—2—5% on PP-attachment relations (e.g. from 63.6 to 68.4% and from 70 to 72.9% on 17th century texts). Finally, we briefly outline linguistic applications and give two examples: gerundials and auxiliary verbs in the ZEN corpus, showing that despite high noise levels linguistic signals clearly emerge, opening new possibilities for large-scale research of gradient phenomena in language change.",Parsing early and late modern English corpora,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
479,http://dx.doi.org/10.1093/llc/fqu002,"{'string_id': '10.1093/llc/fqu002', 'id_scheme': 'DOI'}","The most common method of publishing new discoveries about art conservation techniques and research has been through traditional full-text publications. Such corpora typically only support searching via metadata (e.g. title, authors, or keywords) and full-text. In particular, it is difficult to discover valuable information about the chemical processes, experimental results, or preservation treatments associated with the conservation of paintings from a specific genre. This article addresses this problem by focusing on the extraction of structured data (that complies with a pre-defined ontology) from a distributed corpus of publications about painting conservation. Our specific extraction method involves a unique combination of named entity recognition (using gazetteer-based and machine learning-based methods) followed by relationship extraction (using rule-based and machine learning-based methods). The resulting structured data are stored in a resource description framework triple store, and a Web-based graphical user interface enables the SPARQL querying, retrieval, and display of the search results. The results from applying our techniques to a corpus of publications on art conservation indicate that our approach achieves higher quality precision and recall in extracting named entities and relations from publications, relative to alternative existing approaches.",Extracting structured data from publications in the Art Conservation Domain,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
480,http://dx.doi.org/10.1093/llc/fqu004,"{'string_id': '10.1093/llc/fqu004', 'id_scheme': 'DOI'}","The three main objectives of the research project ‘No Problem Has Solution: A Digital Archive of the Book of Disquiet’1 are: (1) to represent the dynamics of the acts of writing and editing in the production of the LdoD2; (2) to explore the potential of the digital medium to simulate the history of this dynamics; and (3) to create a space for virtualizing the LdoD capable of fostering new dynamics of reading, editing, and researching in the encounter between readers and its material corpus of written fragments. This article describes the project’s rationale and proposes a conceptual model of the functions that the software application to be developed should be able to perform. In this model, the electronic encoding of the fragments should support radial configurations that respond to multiple interactions. This model of the dynamic and reconfigurative iterability of the archive will enable users to virtualize the book according to four functions: reader-function, editor-function, book-function, and author-function.",A model for a virtualLdoD,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
481,http://dx.doi.org/10.1093/llc/fqu005,"{'string_id': '10.1093/llc/fqu005', 'id_scheme': 'DOI'}","Digital Humanities (DH) has come a long way towards establishing itself as a dynamic and innovative field of study. However, it has been pointed out that the DH community predominantly comprises scholars from a handful of mainly English-speaking countries, and a current challenge is achieving a broader internationalization of the DH community. This article provides an overview of the landscape in terms of geo-linguistic diversity, as well as reviewing current DH initiatives to broaden regional and linguistic diversity and identifies some of the main challenges ahead. The aim of this article is to serve as a benchmark of the current situation and suggest areas where further research is required.",Geographical and linguistic diversity in the Digital Humanities,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
482,http://dx.doi.org/10.1093/llc/fqu007,"{'string_id': '10.1093/llc/fqu007', 'id_scheme': 'DOI'}","Interoperability is the key term within the framework of the European-funded research project Interedition,1 whose aim is ‘to encourage the creators of tools for textual scholarship to make their functionality available to others, and to promote communication between scholars so that we can raise awareness of innovative working methods’. The tools developed by Interedition’s ‘Prototyping’ working group were tested by other research teams, which formulate strategic recommendations. To this purpose, the Centre for Manuscript Genetics (University of Antwerp), the Huygens Institute for the History of the Netherlands (The Hague), and the University of Würzburg have been working together within the framework of Interedition. One of the concrete results of collaboration is the development and fine-tuning of the text collation tool CollateX.2 In this article, we would like to investigate how the architecture of a digital archive containing modern manuscripts can be designed in such a way that users can autonomously collate textual units of their choice with the help of the collation tool CollateX and thus decide for themselves how efficiently this digital architecture functions—as an archive, as a genetic dossier, or as an edition. The first part introduces CollateX and its internal concepts and heuristics as a tool for digitally supported collation. How this tool can be integrated in the infrastructure of an electronic edition is discussed in part two. The third and final part examines the possibility of deploying CollateX for the collation of modern manuscripts by means of a test case: the Beckett Digital Manuscript Project (www.beckettarchive.org).",Computer-supported collation of modern manuscripts: CollateX and the Beckett Digital Manuscript Project,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
483,http://dx.doi.org/10.1093/llc/fqu008,"{'string_id': '10.1093/llc/fqu008', 'id_scheme': 'DOI'}","Quantitative stylometry of ten translations of the same Bible passage into English, followed by Ward clustering, produces a dendrogram that reflects the well-known history and intent of the translations. We conclude that quantitative stylometry combined with clustering is a useful tool for reconstructing literary history.",Stylometric classification of different translations of the same text into the same language,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
484,http://dx.doi.org/10.1093/llc/fqu011,"{'string_id': '10.1093/llc/fqu011', 'id_scheme': 'DOI'}","In this article, I present two case studies that show how biographical and intellectual history can benefit from corpus-based linguistics and how databases from different disciplines can cross-pollinate. (i) Combining information from the PASE.ac.uk prosopography and syntactically annotated corpora, I show that the choice of auxiliary with ofslægen ‘killed’ in the Anglo-Saxon Chronicle is related to how the killing occurred, with wearð ofslægen consistently signalling death in battle. This finding sheds new light on the deaths of people like king Osred (716) and the earls Burghelm and Muca (822). (ii) The syntactic choice between it happened that X Y-ed or X happened to Y in late Middle English texts appears to determine whether the scribe/author believes X to be in control of what happens, providing novel evidence on medieval views of accountability levels with regard to adultery, sinning, and casualties. Particular attention is paid to the language use of the scribe of the late medieval Alphabet of Tales and how it reveals his pragmatic attitude towards sex outside marriage.",What grammar reveals about sex and death: interdisciplinary applications of corpus-based linguistics,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
485,http://dx.doi.org/10.1093/llc/fqu012,"{'string_id': '10.1093/llc/fqu012', 'id_scheme': 'DOI'}","This study examines the relationship between the frequencies of clause combination and the distribution of discourse-pragmatic markers of cohesion in a subsample of the Susanne corpus. It addresses the theory that clause grammar constitutes a form of grammar-cued discourse coherence which functions as an integrated system with other methods of managing coherence in language. Evidence is sought for whether increased clause density in a corpus correlates with a reduction in explicit cohesive devices. To address this, a computational approach is outlined for the coding of cohesion in a corpus, using a semi-automated data mining procedure. To validate this approach, it is compared with cohesion measures on the same data using the NLP tool Coh-Metrix 3.0. The two approaches are shown to positively correlate on a series of measures, suggesting they significantly overlap in quantifying the cohesion construct. The final analysis of the tagged corpus indicates that as frequencies of clause combination increase in a text, the use of explicit lexical cohesive devices decrease. Also, higher frequencies of clause combination positively correlate with an increased use of grammatical cohesive devices. Findings are interpreted as generally aligning with the expectations of the theoretical framework known as the Adaptive Approach to Grammar.",An analysis of the relationship between cohesion and clause combination in English discourse employing NLP and data mining approaches,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
486,http://dx.doi.org/10.1093/llc/fqu014,"{'string_id': '10.1093/llc/fqu014', 'id_scheme': 'DOI'}","The study of intertextuality, or how authors make artistic use of other texts in their works, has a long tradition, and has in recent years benefited from a variety of applications of digital methods. This article describes an approach for detecting the sorts of intertexts that literary scholars have found most meaningful, as embodied in the free Tesserae website http://tesserae.caset.buffalo.edu/. Tests of Tesserae Versions 1 and 2 showed that word-level n-gram matching could recall a majority of parallels identified by scholarly commentators in a benchmark set. But these versions lacked precision, so that the meaningful parallels could be found only among long lists of those that were not meaningful. The Version 3 search described here adds a second stage scoring system that sorts the found parallels by a formula accounting for word frequency and phrase density. Testing against a benchmark set of intertexts in Latin epic poetry shows that the scoring system overall succeeds in ranking parallels of greater significance more highly, allowing site users to find meaningful parallels more quickly. Users can also choose to adjust both recall and precision by focusing only on results above given score levels. As a theoretical matter, these tests establish that lemma identity, word frequency, and phrase density are important constituents of what make a phrase parallel a meaningful intertext.",Modeling the scholars: Detecting intertextuality through enhanced word-level n-gram matching,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
487,http://dx.doi.org/10.1093/llc/fqu015,"{'string_id': '10.1093/llc/fqu015', 'id_scheme': 'DOI'}","Computer simulation is the only practical way to model diffusion of cultural features, including speech. We describe the use of a cellular automaton to model feature diffusion as the adaptive aspect of the complex system of speech. Throughout hundreds of iterations that correspond to the daily interaction of speakers across time, we can watch regional distributional patterns emerge as a consequence of simple update rules. A key feature of our simulations is validation with respect to distributions known to occur in survey data. We focus on the importance of appropriate visualizations to observe what is happening during the process of diffusion, with comparison between visualizations of actual survey data and visualizations applied to our simulation. In this way, we believe that we are breaking new ground in simulation of cultural interactions as complex systems. The study of speech as a complex system addresses language as an aspect of culture that emerges from human interaction. We believe that successful simulation of speech in cultural interaction as a complex system can suggest how other aspects of humanities, such as sites, artifacts, or styles in archaeology, can diffuse and change across space and time. Our successful simulation confirms our complex systems approach, and indicates how appropriate use of visualizations makes this possible.",Simulation of the Complex System of Speech Interaction: Digital Visualizations,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
488,http://dx.doi.org/10.1093/llc/fqu016,"{'string_id': '10.1093/llc/fqu016', 'id_scheme': 'DOI'}","Based on Burrows's measure of stylometric difference that uses frequencies of most frequent words, Rolling Delta is a method for revealing stylometric signals of two (or more) authors in a collaborative text. It is applied here to study the texts written jointly by Joseph Conrad and Ford Madox Ford, producing results that generally confirm the usual critical consensus on the visibility of the two author's hand. It also confirms that Ford's claims to a sizeable fragment in Nostromo are unfounded.","Collaborative authorship: Conrad, Ford and Rolling Delta",Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
489,http://dx.doi.org/10.1093/llc/fqu017,"{'string_id': '10.1093/llc/fqu017', 'id_scheme': 'DOI'}","This article addresses the ‘meaning problem’ of unsupervised topic modeling algorithms using a tool called the Networked Corpus, which offers a way to visualize topic models alongside the texts themselves. We argue that the relationship between quantitative methods and qualitative interpretation can be reframed by investigating the long history of machine learning procedures and their historical antecedents. The new method of visualization presented by the Networked Corpus enables users to compare the results of topic models with earlier methods of topical representation such as the 18th-century subject index. Although the article provides a brief description of the tool, the primary focus is to describe an argument for this kind of comparative analysis between topic models and older genres that perform similar tasks. Such comparative analysis provides a new method for developing conceptual histories of the categories of meaning on which the topic model and the index depend. These devices are linked by a shared attempt to represent what a text is ‘about’, but the concept of ‘aboutness’ has evolved over time. The Networked Corpus enables researchers to discover congruities and contradictions in how topic models and indexes represent texts in order to examine what kinds of information each historically situated device prioritizes.",Visibility and meaning in topic models and 18th-century subject indexes,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
490,http://dx.doi.org/10.1093/llc/fqu019,"{'string_id': '10.1093/llc/fqu019', 'id_scheme': 'DOI'}","This paper examines prospects and limitations of citation studies in the humanities. We begin by presenting an overview of bibliometric analysis, noting several barriers to applying this method in the humanities. Following that, we present an experimental tool for extracting and classifying citation contexts in humanities journal articles. This tool reports the bibliographic information about each reference, as well as three features about its context(s): frequency, location-in-document, and polarity. We found that extraction was highly successful (above 85%) for three of the four journals, and statistics for the three citation figures were broadly consistent with previous research. We conclude by noting several limitations of the sentiment classifier and suggesting future areas for refinement.","Citations, contexts, and humanistic discourse: Toward automatic extraction and classification",Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
491,http://dx.doi.org/10.1093/llc/fqu020,"{'string_id': '10.1093/llc/fqu020', 'id_scheme': 'DOI'}","For more than 40 years now, modern theories of literature insist on the role of paraphrases, rewritings, citations, reciprocal borrowings, and mutual contributions of many kinds. The notions of ‘intertextuality’, ‘transtextuality’, and ‘hypertextuality/hypotextuality’ were introduced in the seventies and eighties to approach these phenomena. Through the Phœbus project, computer scientists from the computer science laboratory of the University Pierre and Marie Curie collaborate with the literary teams of Paris-Sorbonne University to develop efficient tools for literary studies that take advantage of modern computer science techniques to detect borrowings of huge masses of texts and to help put them in context. In this context, we have developed a piece of software that automatically detects and explores networks of textual reuses in classical literature. This article describes the principles on which our program is based, the significant results that have already been obtained and the prospective for the near future. It is divided into four parts. The first part recalls the distinction between various types of borrowings like plagiarism, pastiches, citations, etc. The second enumerates the criteria that are retained to characterize reuses and citations on which we are focusing here. The third part describes the implementation and shows its efficiency by comparison with manual detection. Finally, we show some of the results that have already been obtained with the Phœbus program.",Automatic detection of reuses and citations in literary texts,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
492,http://dx.doi.org/10.1093/llc/fqu022,"{'string_id': '10.1093/llc/fqu022', 'id_scheme': 'DOI'}","In this slightly modified version of my 2013 Roberto Busa Prize lecture, I look from the first four decades of digital humanities through its present toward a possible future. I find a means to construct this future by paying close attention to the enemy we need in order to grow: the fear that closed down the horizons of imaginative exploration during the years of the Cold War and that re-presents itself now clothed in numerous techno-scientific challenges to the human.",Getting there from here. Remembering the future of digital humanities: Roberto Busa Award lecture 2013,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
493,http://dx.doi.org/10.1093/llc/fqu023,"{'string_id': '10.1093/llc/fqu023', 'id_scheme': 'DOI'}","The article focuses on two related issues: authorship distinction and the analysis of characters’ voices in fiction. It deals with the case of Elisabeth Wolff and Agatha Deken, two women writers from the Netherlands who collaboratively published several epistolary novels at the end of the 18th century. First, the task division between the two authors will be analysed based on their usage of words and their frequencies. Next, any stylistic differences between the characters (letter writers) will be dealt with. The focus lies on Wolff’s and Deken’s first joint novel, Sara Burgerhart (1782). As to the authorship, nothing clearly showed a clear task division, which implies that Deken’s and Wolff’s writing styles are very much alike. This confirms findings of other scholars, who found that collaborating authors jointly produce a style that is distinguishable from both authors’ personal styles. As to stylistic differences in the voices of the characters in Sara Burgerhart, it was found that only a couple of the letter writers are clearly distinguishable compared with the main characters in the novel. I experimented with two possible tools to zoom in on the exact differences between those characters, but the methods are still too subjective to my taste. In the follow-up research, I will look further than words and their frequencies as building stones of literary style.",Epistolary voices. The case of Elisabeth Wolff and Agatha Deken,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
494,http://dx.doi.org/10.1093/llc/fqu026,"{'string_id': '10.1093/llc/fqu026', 'id_scheme': 'DOI'}","This paper charts the origins, trajectory, development, challenges, and conclusion of Project Bamboo, a humanities cyberinfrastructure initiative funded by the Andrew W. Mellon Foundation between 2008 and 2012. Bamboo aimed to enhance arts and humanities research through the development of infrastructure and support for shared technology services. Its planning phase brought together scholars, librarians, and IT staff from a wide range of institutions, in order to gain insight into the scholarly practices Bamboo would support, and to build a community of future developers and users for Bamboo’s technical deliverables. From its inception, Bamboo struggled to define itself clearly and in a way that resonated with scholars, librarians, and IT staff alike. The early emphasis on a service-oriented architecture approach to supporting humanities research failed to connect with scholars, and the scope of Bamboo’s ambitions expanded to include scholarly networking, sharing ideas and solutions, and demonstrating how digital tools and methodologies can be applied to research questions. Funding constraints for Bamboo’s implementation phase led to the near-elimination of these community-oriented aspects of the project, but the lack of a shared vision that could supersede the individual interests of partner institutions resulted in a scope around which it was difficult to articulate a clear narrative. When Project Bamboo ended in 2012, it had failed to realize its most ambitious goals; this article explores the reasons for this, including technical approaches, communication difficulties, and challenges common to projects that bring together teams from different professional communities.",What Ever Happened to Project Bamboo?,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
495,http://dx.doi.org/10.1093/llc/fqu027,"{'string_id': '10.1093/llc/fqu027', 'id_scheme': 'DOI'}","This article examines the impacts of capturing correspondence metadata through an exhaustive discussion of how details such as the sender and recipient of a letter, their respective addresses, and the date of writing can be entered in an intuitive and accurate fashion. The focus is on the development of fundamental input mechanisms, which can be reused for specific metadata items. Our discussion results in the implementation of a proof-of-concept application, which shows that only a single text box is needed for each metadata item, without violating any of the self-imposed goals and requirements.",An optimized platform for capturing metadata of historical correspondence,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
496,http://dx.doi.org/10.1093/llc/fqu031,"{'string_id': '10.1093/llc/fqu031', 'id_scheme': 'DOI'}","This article provides a brief description of Mapping the Catalogue of Ships, which maps the towns and contingents of Homer’s Catalogue of Ships, analyzing the poet’s knowledge and use of ancient Greek geography. We offer a brief account of the questions that drive our research, detail our novel method to analyze Homer’s poetry in terms of geospatial organization, and summarize the geospatial organizational principles that we have discovered. We discuss the necessity of a digital format to our research and the presentation of our argument, which requires simultaneous attention to literary, geographical, archival, and bibliographical material. The article also details the Neatline (neatline.org) platform that allows us to achieve these goals. We end with outlining future directions for our research and user interface.",Mapping Homer's Catalogue of Ships,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
497,http://dx.doi.org/10.1093/llc/fqu034,"{'string_id': '10.1093/llc/fqu034', 'id_scheme': 'DOI'}","I highlight and illustrate a recent practical strategy for critical argument analysis that I have devised. I refer to it as ‘digital argument deconstruction’. This strategy helps to establish critical purchase on arguments intended for consumption in the public sphere, particularly when readers might not be so knowledgeable about the standpoint being criticized in the argument. Digital argument deconstruction utilizes a recent technological facility—the appending of discussion forums to online arguments, e.g. in online newspapers. This facility allows readers to post responses to an argument and to debate issues raised in it. Taken as a whole, the online comments in the discussion forums can be regarded as supplements to these arguments. I highlight the convenient utility value of this digital supplementation for critical reading of arguments. Integral to this approach to critical reading is corpus linguistic method—the software-based analysis of collections of electronic texts. I show how corpus linguistic analysis of a discussion forum appended to an argument can help illuminate whether or not the argument distorts the standpoint it criticizes. I also show how there can be a penalty for such distortion—the cohesive structure of the argument can be shown to be unstable. Because an argument’s credibility and capacity to persuade are, amongst a number of things, dependent on effective cohesion, showing where an argument’s cohesion falls apart or ‘deconstructs’ diminishes its credibility. A theoretical stimulus for this approach comes from the work of the philosopher, Jacques Derrida.",Deconstructing arguments via digital mining of online comments,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
498,http://dx.doi.org/10.1093/llc/fqu035,"{'string_id': '10.1093/llc/fqu035', 'id_scheme': 'DOI'}","The work of theoretical linguistics rarely focuses on poetry. This is far from surprising, as poetic language, especially in modern and contemporary literature, seems to defy the general rules of syntax and semantics, as observed in ordinary language. This article assumes, however, that linguistic theories should ideally be able to account for creative uses of language, down to their most difficult incarnations. It proposes that at the semantic level, what distinguishes poetry from other uses of language may be its ability to trace conceptual patterns which do not belong to everyday discourse but are latent in our shared language structure. Distributional semantics provides a theoretical and experimental basis for this exploration. First, the notion of a specific ‘semantics of poetry’ is discussed, with some help from literary criticism and philosophy. Then, distributionalism is introduced as a theory supporting the notion that the meaning of poetry comes from the meaning of ordinary language. In the second part of the article, experimental results are provided showing that (i) distributional representations can model the link between ordinary and poetic language, (ii) a distributional model can experimentally distinguish between poetic and randomized textual output, regardless of the complexity of the poetry involved, and (iii) there is a stable, but not immediately transparent, layer of meaning in poetry, which can be captured distributionally, across different levels of poetic complexity.",The semantics of poetry: A distributional reading,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
499,http://dx.doi.org/10.1093/llc/fqu037,"{'string_id': '10.1093/llc/fqu037', 'id_scheme': 'DOI'}","When Robert Coover anointed Michael Joyce the ‘granddaddy’ of hypertext literature in a 1992 New York Times article, it could scarcely have been imagined that this pronouncement would come to define the origin of electronic literature. This short article examines the human and machinic operations obscuring Judy Malloy's Uncle Roger, a hypertext that predates afternoon. Malloy's reputation was stunted because Uncle Roger was algorithmically invisible, a factor that became increasingly important as the Web's commercial capacities matured. afternoon's endurance can be traced to its ISBN, which made afternoon easy for readers to find and united disparate stewards in preserving access to this work. Malloy's programming expertise and the goodwill among hypertext authors were insufficient to protect her against sexist exclusions that, in aggregate, fostered enduring disequilibria. While some male pioneers of hypertext are now full professors, Malloy and other early female hypertext pioneers are adjuncts or are otherwise at a remove from the academic power base. Ironically, Judy Malloy's papers—13,200 items, 15.6 linear feet—are collected at Duke University's Rubenstein Library, but Judy herself still seeks sustained academic employment. This gesture is read in the context of pursuing the digital humanities ‘for love’ in a higher education environment that's increasingly neoliberal in its financial allegiances.",Judy Malloy's seat at the (database) table: A feminist reception history of early hypertext literature,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,3,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
500,http://dx.doi.org/10.1093/llc/fqu039,"{'string_id': '10.1093/llc/fqu039', 'id_scheme': 'DOI'}","Linguistics is a challenging subject to teach online; however, some instructors are doing it with success. In order to investigate the factors involved in teaching successful online courses, the authors investigated literature related to student characteristics, instructor characteristics, and course design and implementation. The investigators then conducted a qualitative case study of actual online linguistics courses. Through observation of courses and rubrics and interviews with teachers and students, the authors analyze the factors involved with teaching a successful online linguistics course.",Teaching online courses in linguistics:,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
501,http://dx.doi.org/10.1093/llc/fqu040,"{'string_id': '10.1093/llc/fqu040', 'id_scheme': 'DOI'}","Stories of several characters, where different characters may engage in separate activities at different locations over the same period, are produced by humans as linear discourses with no difficulty. The present article addresses this issue by engineering a computational model of the relevant task understood as that of composing a narrative discourse for the events in a chess game. The task of narrative composition is modelled as a set of operations that need to be carried out to obtain a span of narrative discourse from a set of events that inspire the narration. The model explores a set of intermediate representations required to capture the structure that is progressively imposed on the material, and connects this content planning task with a classic pipeline for natural language generation. Several strategies are explored for the linearization procedure and for the evaluation of its results. Additionally, the article considers this productive task immersed in a self-evaluation cycle where the produced discourse is validated via the construction of a possible interpretation (based exclusively on the information available in the discourse itself) and a comparison between this interpretation and the original source material.",Composing narrative discourse for stories of many characters: A case study over a chess game,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
502,http://dx.doi.org/10.1093/llc/fqu041,"{'string_id': '10.1093/llc/fqu041', 'id_scheme': 'DOI'}","The article aims to model the verbal and prosodic features of emotional expression in interviews to investigate the potential for synergy between scholarly fields that have the narrative as object of study. Using a digital collection of oral history interviews that contains narrative aspects addressing war and violence in Croatia, we analyzed emotional expression through the words spoken, and through the pitch, vocal effort, and pause duration in the speech signal. The findings were correlated with the linear structure of interviews as well as question type. Our analysis indicates that the weight of emotion words for the overall expressed emotion is stronger in later interview parts as well as after open questions and meaning questions. Similar patterns were found for pitch and pause duration, but not for vocal effort. Although the verbal expression of emotions was somewhat correlated to pause duration, the hypothesized correlation between the verbal and nonverbal features was not confirmed. The research also shows that the various expressive layers in the interviews as well as the relations between them are a suited basis for computational modeling that may help track emotional personal narratives in interview collections. Additional research is needed to further develop the framework for the automated analysis of verbal and nonverbal cues to automatically generate annotations to be used for exploring spoken word collections.",Towards modeling expressed emotions in oral history interviews: Using verbal and nonverbal signals to track personal narratives,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
503,http://dx.doi.org/10.1093/llc/fqu042,"{'string_id': '10.1093/llc/fqu042', 'id_scheme': 'DOI'}","Unexpectedness is a major factor controlling interest in narratives. Emotions, for instance, are felt intensely if they are associated with unexpected events. The problem with generating unexpected situations is that either characters, or the whole story, are at risk of being no longer believable. This issue is one of the main problems that make story design a hard task. Writers face it on a case by case basis. The automatic generation of interesting stories requires formal criteria to decide to what extent a given situation is unexpected and to what extent actions are kept believable. This paper proposes such formal criteria and makes suggestions concerning their use in story generation systems.",Can believable characters act unexpectedly?,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
504,http://dx.doi.org/10.1093/llc/fqu046,"{'string_id': '10.1093/llc/fqu046', 'id_scheme': 'DOI'}","One of the main efforts of recent computational linguistics is to formalize the process of identifying and evaluating similarity between narratives, which is argued to be a key concept for all human behavior. Analyses of the data of 52 adults that participated in our empirical study offered evidence that supports the position that narrative similarity can be equated to the existence of a common summary between the narratives involved. As a further step for applying the above hypothesis, we introduced our own methods for measuring similarity of narratives through the notion of summary and compared them with some of the existing lexical-matching similarity measures. Comparisons of each computational measure with the actual similarity judgments of human participants revealed that methods that merely measure the number of shared words between two stories were unable to capture human similarity judgments, especially for stories of moderate similarity levels. However, the summary-based methods of our approach managed to reproduce human ratings for story pairs across all the range of similarity (from high similarity to low similarity).",Narrative similarity as common summary: Evaluation of behavioral and computational aspects,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
505,http://dx.doi.org/10.1093/llc/fqu049,"{'string_id': '10.1093/llc/fqu049', 'id_scheme': 'DOI'}","In this article, we focus on the problem of verifying Saikaku Ihara’s authorship, especially for Yorozu no humihougu (万の文反古), a posthumous work. Saikaku Ihara (井原西鶴) (c. 1642–93) is one of the most famous writers of the Edo period (1603–1868) in Japan. Saikaku's works are known for their significance for developing Japanese novels today. For a long time, researchers have tried to identify Saikaku’s works by investigating the history, content, format, and other features. However, it remained unclear which works were really written by Saikaku. Meanwhile, the potential of quantitative analysis of textual data has dramatically advanced. To resolve Saikaku’s authorship problem, our laboratory worked with Saikaku researchers to develop a database of works attributed to Saikaku. Based on these new capabilities, we examined Yorozu no humihougu (万の文反古) using quantitative analysis. In this study, we compared Yorozu no humihougu (万の文反古) and Kousyoku ichidai otoko (好色一代男), which has been verified to be a work of Saikaku. We examined six grammatical categories using principal component analysis where significant differences were found. Yorozu no humihougu (万の文反古) was revealed to be characterized as having a higher frequency of verbs and a lower frequency of particles than in Kousyoku ichidai otoko (好色一代男).",Verifying the authorship of Saikaku Ihara’s work in early modern Japanese literature; a quantitative approach,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
506,http://dx.doi.org/10.1093/llc/fqu050,"{'string_id': '10.1093/llc/fqu050', 'id_scheme': 'DOI'}","We continue the study of the reproducibility of Propp’s annotations from Bod et al. (2012). We present four experiments in which test subjects were taught Propp’s annotation system; we conclude that Propp’s system needs a significant amount of training, but that with sufficient time investment, it can be reliably trained for simple tales.",Annotating with Propp’sMorphology of the Folktale: reproducibility and trainability,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
507,http://dx.doi.org/10.1093/llc/fqu051,"{'string_id': '10.1093/llc/fqu051', 'id_scheme': 'DOI'}","This research studies the effect of language orthographic characteristics on the performance of digital word recognition in degraded documents such as historical documents. We provide a rigorous scheme for quantifying the statistical influence of the orthographic characteristics on the quality of word recognition in such documents. We study and compare several orthographic characteristics for four natural languages and measure the effect of each individual characteristic on the digital word recognition process. To this end, we create synthetic languages, for which all characteristics, except the one we examine, are identical, and measure the performance of two word recognition algorithms on synthetic documents of these languages. We examine and summarize the influence of the values of each characteristic on the performance of these word recognition methods.",The influence of language orthographic characteristics on digital word recognition,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,30,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
508,http://dx.doi.org/10.1093/llc/fqu054,"{'string_id': '10.1093/llc/fqu054', 'id_scheme': 'DOI'}","We often try to teach people through stories and narratives instead of giving them explicit facts and rules. But how do these stories influence us, how do they persuade us to change our attitudes? In this paper, we aim to answer these questions by providing a computational model that offers an internal perspective on character motives in stories. This model allows us to represent the deliberations of the main characters and how they weighed their values and motives given their attitudes. We illustrate out model by discussing the well known fable of the Ant and the Grasshopper and the parable of the Prodigal Son.",Arguments as a new perspective on character motive in stories,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
509,http://dx.doi.org/10.1093/llc/fqu055,"{'string_id': '10.1093/llc/fqu055', 'id_scheme': 'DOI'}","Structural similarities across narratives play an important role in many areas of humanities research. In this article, we describe a methodology and an implementation to uncover such similarities automatically in two application scenarios. In both scenarios—ritual and folktale studies—existing research examines similarities of narratives on a structural level and discusses structural principles that govern the combination of individual events to tales or rituals. We present a largely unsupervised and fully automated alignment-based approach for the detection of structural similarities of narratives that allows for data-driven quantitative studies of narrative structure. Our approach makes use of an adaptable, computational linguistic processing architecture that creates integrated discourse representations of events, participants, and their relations. Our contributions are twofold and crucially build on the automatically constructed discourse representations: (1) We examine different ‘semantics-driven cross-document alignment’ algorithms that determine (sequences of) events shared between narratives, to support the search for recurrent elements in their structure. The alignment algorithms are evaluated in two experiments. (2) We develop ‘tools for exploration and interpretation’ that we offer to humanities researchers for investigation of the analyzed data. These include search facilities, visualizations, statistical overviews, and a graph-based algorithm that identifies densely aligned regions across documents for targeted inspection.",An NLP-based cross-document approach to narrative structure discovery,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
510,http://dx.doi.org/10.1093/llc/fqu056,"{'string_id': '10.1093/llc/fqu056', 'id_scheme': 'DOI'}","This article presents an approach to using cognitive models of narrative discourse comprehension to define an explicit computational model of a reader’s comprehension process during reading, predicting aspects of narrative focus and inferencing with precision. This computational model is employed in a narrative discourse generation system to select and sequence content from a partial plan representing story world facts, objects, and events, creating discourses that satisfy comprehension criteria. Cognitive theories of narrative discourse comprehension define explicit models of a reader’s mental state during reading. These cognitive models are created to test hypotheses and explain empirical results about reader comprehension, but do not often contain sufficient precision for implementation on a computer. Therefore, they have not previously been suitable for computational narrative generation. The results of three experiments are presented and discussed, exhibiting empirical support for the approach presented. This work makes a number of contributions that advance the state-of-the-art in narrative discourse generation: a formal model of narrative focus, a formal model of online inferencing in narrative, a method of selecting narrative discourse content to satisfy comprehension criteria, and both implementation and evaluation of these models.",Cognitive models of discourse comprehension for narrative generation,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,29,4,"[{'value': '0268-1145', 'type': 'print'}, {'value': '1477-4615', 'type': 'electronic'}]"
511,http://dx.doi.org/10.1093/llc/fqu061,"{'string_id': '10.1093/llc/fqu061', 'id_scheme': 'DOI'}"," This article presents an approach to citation segmentation that addresses special challenges as typically found in Digital Humanities applications. We perform citation segmentation from Optical Character Recognition (OCR) input obtained from volumes of a printed bibliography, the Turkology Annual . This showcase application features serious difficulties for state-of-the-art techniques in citation segmentation: multilingual citation entries , lack of data redundancy , inconsistencies , and noise from OCR input . Our approach is based on Markov logic networks (MLN) (Richardson and Domingos, Markov logic networks. Machine Learning , 62 (1): 107–36, 2006), a framework of statistical relational learning that combines first-order logic with probabilistic modeling. Formalization in first-order logic offers high expressivity and flexibility, and makes it possible to tailor segmentation to specific conventions of a given bibliography. We show that in face of the specific difficulties found with segmenting references from a digitized bibliography, our MLN formalizations outperform state-of-the-art statistical methods. We obtain 88% F 1 -score for exact field match, a 24.8% increase over a conditional random fields-based system baseline. In contrast to prior work, we address a data set featuring sparse and noisy data. Our method extends Poon and Domingos (Joint Inference in information extraction. In Proceedings of the Twenty-Second National Conference on Artificial Intelligence . Vancouver, Canada: AAAI Press, 2007)’s approach by applying joint inference at the field level . By this move, we are able to cope with the lack of citation redundancy and noise in the data. Our approach can be characterized as knowledge-based and hence does not rely on annotated training data. The rule sets we designed can be adapted to other bibliographies, or further types of digitized sources, such as historical dictionaries or encyclopedias. ",Citation segmentation from sparse & noisy data: A joint inference approach with Markov logic networks,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,31,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
512,http://dx.doi.org/10.1093/llc/fqu062,"{'string_id': '10.1093/llc/fqu062', 'id_scheme': 'DOI'}","Nowadays, a large amount of documents is generated daily. These documents may contain some spelling errors which should be detected and corrected by using a proofreading tool. Therefore, the existence of automatic writing assistance tools such as spell-checkers/correctors could help to improve their quality. Spelling errors could be categorized into five categories. One of them is real-word errors, which are misspelled words that have been wrongly converted into another word in the language. Detection of such errors requires discourse analysis rather than just checking the word in a dictionary. We propose a discourse-aware discriminative model to improve the results of context-sensitive spell-checkers by reranking their resulted n-best list. We augment the proposed reranker into two existing context-sensitive spell-checker systems; one of them is based on statistical machine translation and the other one is based on language model. We choose the keywords of the whole document as contextual features of the model and improve the results of both systems by employing the features in a log-linear reranker system. We evaluated the system on two different languages: English and Persian. The results of the experiments in English language on the Wall street journal test set show improvements of 4.5% and 5.2% in detection and correction recall, respectively, in comparison to the baseline method. The mentioned improvement on recall metric was achieved with comparable precision. We also achieve state-of-the-art performance on the Persian language.",Discriminative reranking for context-sensitive spell–checker,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,31,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
513,http://dx.doi.org/10.1093/llc/fqu063,"{'string_id': '10.1093/llc/fqu063', 'id_scheme': 'DOI'}","Theorists of technology adoption have demonstrated that successful technologies rely on social resources. This article takes up social tensions involved in designing digital resources for scholarship, at the centre of which stand editing tools. These are (1) individualism versus collaboration in relation to credit; (2) the shift towards dynamism versus stability in resources; (3) simplicity versus complexity in data models and interfaces; (4) swift dissemination versus scholarly quality control; and (5) the benefits of standards versus disciplinary resistance to systematization. With particular reference to editing and publication software, it argues that interfaces are the contact zones within which such tensions will necessarily be mediated. The onus is on the scholarly community to address the social implications of new editing environments, but we can navigate future possibilities only through a deep respect for the embodied and socially situated experience of the user or subject of textuality and a profound recognition of the power of our printed past.",Tensions and tenets of socialized scholarship,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
514,http://dx.doi.org/10.1093/llc/fqu066,"{'string_id': '10.1093/llc/fqu066', 'id_scheme': 'DOI'}","In recent years, social networks, microblogs, and short message service have deeply penetrated peoples lives, and thus, chat-style text is a common phenomenon. This chat-style text has many unknown features for linguists, which can be discovered by analyzing a chat-style corpus. The process of constructing a corpus conforms to specific corpus criteria, such as representativeness, sampling, variety, and chronology. Up to now, literature does not provide specific corpus criteria for creating a chat-style-text corpus. In contrast to related work, corpus criteria for creating a chat-style corpus are provided. An exhaustive and reliable Malay chat-style text corpus is still lacking. Thus, the provided criteria are used to demonstrate the process of constructing a Twitter corpus known as the Malay Chat-style Corpus (MCC). The MCC, which has 1 million twitter messages, consists of 14,484,384 word instances, 646,807 terms and metadata, such as posting time, used twitter client application, and type of Twitter message (simple Tweet, Retweet, Reply). Furthermore, the results of the analysis of the MCC reveal characteristics of the corpus including the most frequent terms and collocations, Zipf law diagram, Twitter peak hours, and percentages of message types. Finally, representativeness of the corpus is evaluated by employing cartography and automatic language identification methods. This corpus and the process of corpus creating are valuable for researchers working in linguistics, natural language processing, and data mining.",Twitter corpus creation: The case of a Malay Chat-style-text Corpus (MCC),Oxford University Press (OUP),2014,,Literary and Linguistic Computing,31,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
515,http://dx.doi.org/10.1093/llc/fqu069,"{'string_id': '10.1093/llc/fqu069', 'id_scheme': 'DOI'}","Writers of a best-selling category romance imprint share a common tendency to decrease their deployment of unique words over the span of their novels—a phenomenon of ‘vocabulary decay’. This tendency cannot be found in the novels of Jane Austen, suggesting this drop is not intrinsic to the romance genre itself, and is unlikely to have any true narrative purpose. A study of Charles Dickens shows that vocabulary decay extends beyond the romance genre. Closer examination reveals vocabulary decay is a result of progressive amounts of linguistic chunking—due to author fatigue or a desire to produce a more readable narrative.",Vocabulary decay in category romance,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,31,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
516,http://dx.doi.org/10.1093/llc/fqu072,"{'string_id': '10.1093/llc/fqu072', 'id_scheme': 'DOI'}","The role of human philological judgement in textual criticism, and particularly in stemmatics, has been at times hotly debated and in computational stemmatology tends to be carefully circumscribed. In this context philological judgement is deployed to distinguish ‘significant’ from ‘insignificant’ textual variation—that is, to select those variants that are more or less likely to betray information about the exemplar from which a given text was copied. This article reports on an experiment performed to assess the accuracy of human philological judgement on a set of three artificial traditions, using tools for stemma analysis developed for a prior project and available to the public as the Stemmaweb online service. We show that for most of the artificial traditions, human judgement was not significantly better than random selection for choosing the variant readings that fit the stemma in a text-genealogical pattern, and we discuss some of the implications of these findings.",Analysis of variation significance in artificial traditions using Stemmaweb,Oxford University Press (OUP),2014,,Literary and Linguistic Computing,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
517,http://dx.doi.org/10.1093/llc/fqu073,"{'string_id': '10.1093/llc/fqu073', 'id_scheme': 'DOI'}","There was one trending topic on Twitter in December 2012 that we could have seen coming for a few years now: the New Age prophecy of the End of Times on 21 December 2012—all because some Mayan calendar supposedly ended on this date. For 2 weeks long—a week before the Apocalypse and a week after—we monitored Twitter for Dutch words concerning the End of the World. We caught 52,000 tweets in 2 weeks. When did the stream of rumours peek? How many retweets were involved? Was there much micro-variation? What was the overall content of the tweets? What emotions were expressed in the tweets? How did religious people respond? And finally, how many people confessed they were truly scared because of the prophecy? These are intriguing questions that we can answer by using a few basic computational tools. Although the Apocalypse got a lot of attention in the news media, it turned out most Dutch people on Twitter took the End of Days with a grain of salt.",The apocalypse on Twitter,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
518,http://dx.doi.org/10.1093/llc/fqv001,"{'string_id': '10.1093/llc/fqv001', 'id_scheme': 'DOI'}","In this article we describe the methodology developed for the semiautomatic annotation of EPEC-RolSem, a Basque corpus labeled at predicate level that follows the PropBank-VerbNet model. The methodology presented is the product of detailed theoretical study of the semantic nature of verbs in Basque and of their similarities and differences with verbs in other languages. As part of the proposed methodology, we are creating a Basque lexicon on the PropBank-VerbNet model that we have named the Basque Verb Index (BVI). Our work thus dovetails with the general trend toward building lexicons from tagged corpora that is clear in work conducted for other languages. EPEC-RolSem and BVI are two important resources for the computational semantic processing of Basque; as far as the authors are aware, they are also the first resources of their kind developed for Basque. In addition, each entry in BVI is linked to the corresponding verb-entry in well-known resources like PropBank, VerbNet, WordNet, FrameNet, and Levin’s classification. We have also implemented several automatic processes to aid in creating and annotating the BVI, including processes designed to facilitate the task of manual annotation.","A methodology for the semiautomatic annotation of EPEC-RolSem, a Basque corpus labeled at predicate level following the PropBank-VerbNet model",Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
519,http://dx.doi.org/10.1093/llc/fqv002,"{'string_id': '10.1093/llc/fqv002', 'id_scheme': 'DOI'}"," After his first edition of the Old French text Lai de l’Ombre in 1890, the Romance scholar Joseph Bédier returned to the text in a revised edition in 1913. In the introduction to this edition, he claimed that he had become aware of a strange law: the great majority of stemmata proposed for Old French texts were bifurcating, i.e. they had two main branches. When he once more returned to this question in 1928, he claimed that of 110 stemmata he had encountered, 105 were bifurcating. Arrigo Castellani, another Romance scholar, revised this material in 1957, and came to somewhat lower numbers, but confirmed in general Bédier’s conclusion—of 86 stemmata, Castellani found that 71 were bifurcating. This article is an investigation into another vernacular tradition, the Old Norse one, i.e. Old Icelandic and Old Norwegian. The material presented here is based on the two major series published in Copenhagen, Bibliotheca Arnamagnæana and Editiones Arnamagnæanæ , and it comes very close to the findings of Castellani—of 89 stemmata, 74 turned out to be bifurcating. The two main hypotheses of Bédier are evaluated, and the conclusion is that the most likely explanation for the preponderance of bifurcating stemmata is the force of dichotomy inherent in the procedure of the stemmatic recension. ",Thesilva portentosaof stemmatology: Bifurcation in the recension of Old Norse manuscripts,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
520,http://dx.doi.org/10.1093/llc/fqv003,"{'string_id': '10.1093/llc/fqv003', 'id_scheme': 'DOI'}"," This article combines bibliography, book history, literary and traditional textual criticism with phylogenetic analysis to infer the publishing history and textual descent of a short printed ballad history of England— The Wandering Jew’s Chronicle . Probably first published in 1634, The Wandering Jew’s Chronicle most commonly survives as a ‘broadside ballad’—a cheaply-printed song-sheet—illustrated with woodcut portraits of kings and queens of England. It remained in print until ca.1830, its text and illustrations updated to the present. Although in partial synchrony with English history, much of its publishing history and textual descent is uncertain. This article demonstrates how historical evidence, taken in particular from book trade history, may be usefully combined with textual and bibliographical evidence, and that it is at times essential for understanding the descent of the text. The textual descent of the ballad is visualized in a stemma that summarizes key findings from both traditional and phylogenetic analyses. ",Lines of succession in an English ballad tradition: The publishing history and textual descent ofThe Wandering Jew’s Chronicle,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
521,http://dx.doi.org/10.1093/llc/fqv004,"{'string_id': '10.1093/llc/fqv004', 'id_scheme': 'DOI'}"," Medieval chronicles offer valuable source material for historians studying the Middle Ages. Like all medieval books, also chronicles were written and copied by hand, causing both unintentional errors and intentional alterations to the text. Stemmatologists are trying to recreate the family tree of the different copies. In recent years, many computer-assisted methods have emerged which several editors have already used. Computer-assisted stemmatology can offer valuable tools not only for editors but also for historians. They can also help to test traditional methods of textual criticism, point out errors in earlier historiography and create possibilities for studying cultural links and the history of books. In this article it is demonstrated how the use of different computer-assisted stemmatological methods can reveal new information concerning the manuscript tradition of a Finnish 16th-century chronicle, Paulus Juusten’s Catalogus et ordinaria successio Episcoporum Finlandensium . With the help of these methods, it can also be shown that the stemma put forward in the latest edition of the chronicle should be reconsidered. Using PAUP, RHM, and SplitsTree, it is pointed out that these methods create similar results as traditional textual criticism. These results can be obtained considerably more quickly than with traditional methods and without the subjective decision by a stemmatologist between ‘original readings and errors’. ",Computer-assisted stemmatology in studying Paulus Juusten's 16th-century chronicleCatalogus et ordinaria successio Episcoporum Finlandensium,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
522,http://dx.doi.org/10.1093/llc/fqv005,"{'string_id': '10.1093/llc/fqv005', 'id_scheme': 'DOI'}","This article presents a novel architecture using a hybrid model for developing a Sindhi spellchecker system which has yet not been developed prior to this work. The compound textual forms and glyphs of Sindhi language presents a substantial challenge for developing a Sindhi spellchecker system and generating a similar suggestion list for misspelled words. In order to implement such a system, phonetic-based Sindhi language rules and patterns must be taken into account for increasing the accuracy and efficiency. In this research work, a simple and efficient combinational hybrid system is proposed, using three different algorithms, the Edit Distance algorithm to find the measure of similarity between two Sindhi strings. The phonetic-based SoundEx and ShapeEx algorithms are developed for pattern or glyph matching, generating accurate and an efficient suggestion list for incorrect or misspelled Sindhi words. The proposed system is established with a blend between Phonetic-based SoundEx algorithm and ShapeEx algorithm for pattern or glyph matching, generating accurate and efficient suggestion list for incorrect or misspelled Sindhi words. In this article, a table of phonetically similar-sounding Sindhi characters is presented which are grouped together along with another table containing similar glyph or shape-based character groups. The system has been successfully integrated into a pre-developed Sindhi word processer application. The Sindhi word segmentation methodology and algorithms required for the spellchecker has already been published and so are not discussed in detail in this article.",Phonetic-based Sindhi spellchecker system using a hybrid model,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
523,http://dx.doi.org/10.1093/llc/fqv006,"{'string_id': '10.1093/llc/fqv006', 'id_scheme': 'DOI'}","Recent contributions have highlighted that the advent of digital technologies in the humanities is the object of different and even polarized narratives, which frame the effects of digitization either in utopian or dystopian terms. In this light, technology-centred narratives should be complemented by an intellectual history of technology-related «myths and dystopias», in order to increase our understanding of how digital technologies are presented in the disciplinary and public debates. Whereas this type of analysis has taken its start from library science and the digital humanities, however, it is still in its infancy within the field of museum computing. To this purpose, the article provides an interpretive and critical review of the debate on the automated cataloguing and digitization of museum collections. It highlights a sequence of discursive shifts in time since the 1960s to the present decade are identified and discussed: namely, a phase of «standardization», followed by «the museum of the Information Society» and, most recently, a «post-modernist utopia» which dominates the current debate. Each new paradigm tends at the same time to reverse and «re-mediate» some characteristics of former phases, generating controversies that are likely to proceed until more empirical knowledge is collected about the actual impacts of digital media on museums.","Towards an intellectual history of digitization: Myths, dystopias, and discursive shifts in museum computing",Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
524,http://dx.doi.org/10.1093/llc/fqv008,"{'string_id': '10.1093/llc/fqv008', 'id_scheme': 'DOI'}","This article demonstrates how to automatically build a Latin word stemmer to transform words into their grammatical roots. By using the Wiktionary database as source data, it becomes possible to build such a tool with several hundreds of thousands of words. Our experiments demonstrate that it can then be used to correctly find the root of 78% of the words of Martial’s Epigrams, and can be combined with other linguistic tools such as the Latin WordNet to greatly enhance their language coverage. While our research focuses on the Latin language, the same methodology could be used to build stemmers and other linguistic tools for many other ancient languages represented in Wiktionary, such as Ancient Greek or Old Armenian.",Latin word stemming using Wiktionary:,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
525,http://dx.doi.org/10.1093/llc/fqv009,"{'string_id': '10.1093/llc/fqv009', 'id_scheme': 'DOI'}"," The present article reports on a corpus-based study that explores the relationship between English dual quantifier both and its Spanish equivalents, especially dual quantifier ambos/as , which is perceived as being functionally similar. The aim of this research is to shed some light on the causes of Spanish students’ underuse of both in their writing by analyzing and classifying the translation solutions for each different use of both and by comparing the use of both and its Spanish counterparts in fictional discourse and in non-fictional discourse to search for any possible cross-register differences. A parallel English–Spanish corpus, P-ACTRES, an ad hoc corpus of contemporary English texts and their corresponding translations into Spanish, has been used for the purpose of this study. ",Translation as an aid to ELT: Using an English–Spanish parallel corpus (P-ACTRES) to study Englishbothand its Spanish counterparts,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
526,http://dx.doi.org/10.1093/llc/fqv010,"{'string_id': '10.1093/llc/fqv010', 'id_scheme': 'DOI'}"," This article introduces a new stylometric method that combines supervised machine-learning classification with the idea of sequential analysis. Unlike standard procedures, aimed at assessing style differentiation between discrete text samples, the new method, supported with compact visualization, tries to look inside a text represented as a set of linearly sliced chunks, in order to test their stylistic consistency. Three flavors of the method have been introduced: (1) Rolling SVM, relying on the support vector machines (SVM) classifier, (2) Rolling NSC, based on the nearest shrunken centroids method, and (3) Rolling Delta, using the classic Burrowsian measure of similarity. The technique is primarily intended to assess mixed authorship; however, it can be also used as a magnifying glass to inspect works with unclear stylometric signal. To demonstrate its applicability, three different examples of collaborative work have been briefly discussed: (1) the 13th-century French allegorical poem Roman de la Rose , (2) a 15th-century translation of the Bible into Polish known as Queen Sophia’s Bible , and (3) The Inheritors , a novel collaboratively written by Joseph Conrad and Ford Madox Ford in 1901. ",Rolling stylometry,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
527,http://dx.doi.org/10.1093/llc/fqv011,"{'string_id': '10.1093/llc/fqv011', 'id_scheme': 'DOI'}"," Understanding the relationship between political regimes and language, while a popular theme among historians and linguists, is empirically difficult. This study suggests a preliminary empirical framework. A quantitative analysis of the Google Books 1-Gram German corpus from the year 1870 to 1945 provides empirical evidence consistent with George Orwell’s hypothesis that everyday language deteriorates under dictatorships, through the inversion of words’ underlying meanings. More specifically, this article shows that six non-technical non-Nazi words— Demokratie (democracy), Freiheit (freedom), Frieden (peace), Herrlichkeit (glory), Gerechtigkeit (justice), and Heldentumd (heroism)—are (1) highly correlated with explicitly Nazi words; (2) negatively correlated with Germany’s level of democracy; and (3) negatively correlated with the count of riots, anti-government protests, and government crises, implying that these words were not used as a form of protest. The use of these words increased sharply under the Nazi government, which banned all publications that were critical of the government. These correlations cannot tell us whether the relationship is causal, and we cannot be sure whether the corpus under study is truly representative. Replicating this empirical framework on other corpora, pushing the period under study further back in time, or using 2-gram data sets can all help assess this study’s findings. ",Politics and the German language: Testing Orwell’s hypothesis using the Google N-Gram corpus,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
528,http://dx.doi.org/10.1093/llc/fqv012,"{'string_id': '10.1093/llc/fqv012', 'id_scheme': 'DOI'}","Drawing on Digital Humanities as a specific, reflexive way of using new information technology to enhance learning, teaching, and research, this article describes a tool whose development began in 2011 with the goal of helping students, teachers, and researchers to share, transcribe, and comment on audio/video data without having to experience complicated software installation, regular overlap of windows, or strong versioning. From the definition of the requirements to the evaluation of the impact on teaching and research, the article stresses the importance of conceiving of techno-pedagogical tools as dynamic products, whose plasticity is a way to offset teething problems and apparent weaknesses.","IMPACT: A tool for transcribing and commenting on oral data, for teaching, learning, and research",Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
529,http://dx.doi.org/10.1093/llc/fqv013,"{'string_id': '10.1093/llc/fqv013', 'id_scheme': 'DOI'}","This article presents the ‘Roman City Ruleset’, a suite of procedural rules for creating 3D models of Roman and Hellenistic architecture and urban environments. Unlike traditional 3D modeling software, in which the user directly manipulates polygons to simulate form, procedural modeling entails the use of computer programming languages to write a semantic description of a building that then generates a polygonal model. Procedural modeling has the potential to address a number of issues related to 3D archaeological reconstructions which are of concern to digital humanists. Of particular interest for archaeologists and architectural historians is the ability to test hypothetical reconstructions of ancient architecture in a fully realized urban context. The procedural rules link each iteration of a model to its source material, allowing the degree of certainty present in each model to be accurately defined through the documentation of each step in the process of interpreting a given data set. Procedural modeling enhances the scholarly value of architectural reconstructions by providing a platform for the comparison and refutation of 3D visualizations, and advances the methodology of 3D modeling toward becoming an essential part of the digital humanist’s toolkit.",An Integrated Approach to the Procedural Modeling of Ancient Cities and Buildings,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,30,suppl 1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
530,http://dx.doi.org/10.1093/llc/fqv014,"{'string_id': '10.1093/llc/fqv014', 'id_scheme': 'DOI'}","There exist about 150,000 premodern Arabic documents on papyrus and paper, of which about 2,500 have been edited. Another 10,000 unpublished documents are described or mentioned in papyrological publications. It is the aim of the Arabic Papyrology Database (APD) to give access to published texts and descriptions. For the APD, an entirely new approach of organizing Arabic text was developed. The APD presents texts in five levels that account for the peculiarities of the Arabic script system and the scribal practices. The first level provides a faithful diplomatic edition of the text as found in the document. The following levels document four steps of editorial interventions. On the second level, lines of characters are broken into single words. On the third level, lacking diacritical dots are supplied. On the fourth level, Arabic vowel signs are added, providing a full phonological representation. On the fifth level, a scientific Latin transliteration is given. Each element of the fifth level is connected to a lexicon and a list of grammatical forms. All levels contain variant readings and editorial remarks. At present, the APD contains 1,806 full-text documents and is freely accessible (http://www.naher-osten.lmu.de/apd).",The Arabic Papyrology Database,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,30,suppl 1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
531,http://dx.doi.org/10.1093/llc/fqv015,"{'string_id': '10.1093/llc/fqv015', 'id_scheme': 'DOI'}","This keynote address for the 2014 Digital Humanities conference is a practitioner’s talk, and—though the abstract belies it—an optimistic one. I take as given the evidence that human beings are irrevocably altering the conditions for life on Earth and that, despite certain unpredictabilities, we live at the cusp of a mass extinction. What is the place of digital humanities (DH) practice in the new social and geological era of the Anthropocene? What are the DH community’s most significant responsibilities, and to whom? This talk positions itself in deep time, but strives for a foothold in the vital here-and-now of service to broad publics. From the presentist, emotional aesthetics of Dark Mountain to the arms-length futurism of the Long Now, I dwell on concepts of graceful degradation, preservation, memorialization, apocalypse, ephemerality, and minimal computing. I discuss digital recovery and close reading of texts and artifacts once thought lost forever, and the ways that prosopography, graphesis, and distant reading open new vistas on the longue durée. Can DH develop a practical ethics of resilience and repair? Can it become more humane while working at inhuman scales? Can we resist narratives of progress, and still progress? I wish to open community discussion about the practice of DH, and what to give, in the face of a great hiatus or the end of it all.",Digital Humanities in the Anthropocene,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,30,suppl 1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
532,http://dx.doi.org/10.1093/llc/fqv016,"{'string_id': '10.1093/llc/fqv016', 'id_scheme': 'DOI'}","The evolution of fairy tales often involves complex interactions between oral and literary traditions, which can be difficult to tease apart when investigating their origins. Here, we show how computer-assisted stemmatology can be productively applied to this problem, focusing on a long-standing controversy in fairy tale scholarship: did Little Red Riding Hood originate as an oral tale that was adapted by Perrault and the Brothers Grimm, or is the oral tradition in fact derived from literary texts? We address this question by analysing a sample of twenty-four literal and oral versions of the fairy tale Little Red Riding Hood using several methods of phylogenetic analysis, including maximum parsimony and two network-based approaches (NeighbourNet and TRex). While the results of these analyses are more compatible with the oral origins hypothesis than the alternative literary origins hypothesis, their interpretation is problematized by the fact that none of them explicitly model lineal (i.e. ancestor-descendent) relationships among taxa. We therefore present a new likelihood-based method, PhyloDAG, which was specifically developed to model lineal as well as collateral and reticulate relationships. A comparison of different structures derived from PhyloDAG provided a much clearer result than the maximum parsimony, NeighbourNet or TRex analyses, and strongly favoured the hypothesis that literary versions of Little Red Riding Hood were originally based on oral folktales, rather than vice versa.",Oral fairy tale or literary fake? Investigating the origins ofLittle Red Riding Hoodusing phylogenetic network analysis,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
533,http://dx.doi.org/10.1093/llc/fqv017,"{'string_id': '10.1093/llc/fqv017', 'id_scheme': 'DOI'}","After the sudden occurrence of East Japan Great Earthquake on 11 March 2011, triple disasters crippled the regular life of citizens of East Japan. A lot of people were affected, especially women victims suffered from different problems and worries: they had to care for elders, raise children, and find new jobs. Women also had specific needs of commodities for everyday life. Administrative authorities wanted to recognize women victims’ specific problems and provide them appropriate supports. However, it was difficult to grasp women victims’ requirements properly, because they were really patient and their needs were sometimes neglected under the environmental pressure. Conducting interviews and taking questionnaire from women victims is one way to gauge their needs, but it is time-consuming and labor intensive. This work proposes a framework for the development of an advisory message board for women victims on the web in which women victims can post their messages freely. The computational technologies are used here for analyzing digital media data in order to improve the lives of underserved or underprivileged people in case of disasters like earthquake. Text mining technologies are developed for automatic analysis of the messages to find out the specific needs of the victims and the change of needs with time and support them with proper advice. The proposed method uses latent semantic analysis (LSA) to extract the hidden topics and change of topics over time. As a case study, text messages from several on-line social media over a period of 2 years after the East Japan Great earthquake are collected and analyzed. It has been found that LSA-based technique is more effective in extracting the change of needs over time than graph-based topic extraction method. The final aim of this work is to develop the framework of advisory message board for women which will help the authority to find out the special needs of women victims after any future disaster and support them.",Developing a framework for an advisory message board for female victims after disasters: A case study after east Japan great earthquake,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
534,http://dx.doi.org/10.1093/llc/fqv018,"{'string_id': '10.1093/llc/fqv018', 'id_scheme': 'DOI'}","This article describes a Recommendation System to help photographic historians make connections between widely dispersed and previously unrelated records of photographs held in different heritage institutions, and demonstrates how it has been used to rediscover images of exhibits from the Royal Photographic Society annual exhibitions of over 120 years ago. While the surviving exhibition catalogues are a rich information source, they are largely devoid of illustrations of the exhibits. The FuzzyPhoto project has developed techniques for analyzing a corpus of more than 1.4 million historical photographic records across different galleries, libraries, archives, and museums, in order to identify similarities between them and used the results to offer visitors to those sites links to potentially related items at other sites, thus creating a web of interconnections between them. The article describes techniques used for data acquisition, cleaning, integration, semantic-based data mining, approximate reasoning, and fuzzy algorithm-based similarity metrics. It compares the approach described here with manual searches and more sophisticated computational methods such as linked data and concludes that FuzzyPhoto is an effective method for dealing with the realities of messy collection records that could be extended to other types of archival objects such as paintings, maps, and textiles where record matching is required.","Words, words. They’re all we have to go on: Image finding without the pictures",Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
535,http://dx.doi.org/10.1093/llc/fqv019,"{'string_id': '10.1093/llc/fqv019', 'id_scheme': 'DOI'}","This article presents a profile-based authorship analysis method which first categorizes texts according to social and conceptual characteristics of their author (e.g. Sex and Political Ideology) and then combines these profiles for two authorship analysis tasks: (1) determining shared authorship of pairs of texts without a set of candidate authors and (2) clustering texts according to characteristics of their authors in order to provide an analysis of the types of individuals represented in the data set. The first task outperforms Burrows’ Delta by a wide margin on short texts and a small margin on long texts. The second task has no such benchmark with existing methods. The data set for evaluating the method consists of speeches from the US House and Senate from 1995 to 2013. This data set contains both a large number of texts (42,000 in the test sets) and a large number of speakers (over 800). The article shows that this approach to authorship analysis is more accurate than existing approaches given a data set with hundreds of authors. Further, this profile-based method makes new types of analysis possible by looking at types of individuals as well as at specific individuals.",Profile-based authorship analysis,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
536,http://dx.doi.org/10.1093/llc/fqv021,"{'string_id': '10.1093/llc/fqv021', 'id_scheme': 'DOI'}","We look to unlock the verbal code of President Putin and NATO Secretary-General Rasmussen during the Ukrainian crisis. On 18 March 2014, referring to the medieval history of Russia, Putin expressed a vision that invites to explore the role of ancestral mental images as an instance of people with a direct knowledge of spiritual truth. Looking for spiritual truth is often at the edge of words, forcing the speaker to resort to poetic language to convey an inspired message beyond common understanding. Using a fragment of Martindale’s creativity model, we look for indicators of insight (regressive thought, metaphors, dual-coding of images and emotions) in Putin’s and NATO’s speeches over a short take of recent history, December 2013–September 2014. Among obvious results, the annexing of Crimea to the Russian Federation (18 March 2014) sparked new words in both Putin’s and NATO’s speeches. The May 9 ‘Great Victory Day’ also affected Putin’s speeches. After May 9, a visible discontinuity marks the speeches of both Putin and NATO, but in opposite directions. Higher scores of metaphoric thought in Putin contrasts with lower scores in NATO. Finally, the threat index increases moderately in both cases, yet bears no strong relation to the ups or downs of visionary metaphoric thought in either Putin or NATO. The two corpuses contain the words ‘value’ and ‘border’ in plenty, pushing linguistic detection scholars to explore new frontiers.",Deaf sentences1over Ukraine: Mysticism versus ethics,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
537,http://dx.doi.org/10.1093/llc/fqv022,"{'string_id': '10.1093/llc/fqv022', 'id_scheme': 'DOI'}","The article describes methodology of zonal text processing based on interpretation of Bradford's law in terms of geometric progression. The methodology involves dividing the text into three zones (J0, J1, J2) and finding their composition. To verify the value of Bradford multiplier two methods that evaluate distribution of stop words across the three zones are used. The concept of zonal-correlational processing that implies contrastive analysis of J1 zones of two or more texts for the purpose of authorship attribution and classification is formulated and tested. To address the problem of difference in text sizes the concept of logarithmic equalizing is proposed.",Zonal text processing,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
538,http://dx.doi.org/10.1093/llc/fqv026,"{'string_id': '10.1093/llc/fqv026', 'id_scheme': 'DOI'}","The authorship of the 1924 short story ‘The Loved Dead’ has been contested by family members of Clifford Martin Eddy, Jr. and Sunand Tryambak Joshi, a leading scholar on Howard Phillips Lovecraft. The authors of this article use stylometric methods to provide evidence for a claim about the authorship of the story and to analyze the nature of Eddy’s collaboration with Lovecraft. Further, we extend Rybicki, Hoover, and Kestemont’s (Collaborative authorship: Conrad, Ford, and rolling delta. Literary and Linguistic Computing, 2014; 29, 422–31) analysis of stylometry as it relates to collaborations in order to reveal the necessary considerations for employing a stylometric approach to authorial collaboration.","Stylometry and collaborative authorship: Eddy, Lovecraft, and ‘The Loved Dead’",Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,32,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
539,http://dx.doi.org/10.1093/llc/fqv027,"{'string_id': '10.1093/llc/fqv027', 'id_scheme': 'DOI'}","This article presents the integration of sentiment analysis in ALCIDE, an online platform for historical content analysis. A prior polarity approach has been applied to a corpus of Italian historical texts, and a new lexical resource has been developed with a semi-automatic mapping starting from two English lexica. This article also reports on a first experiment on contextual polarity using both expert annotators and crowdsourced contributors. The long-term goal of our research is to create a system to support historical studies, which is able to analyse the sentiment in historical texts and to discover the opinion about a topic and its change over time.",Towards sentiment analysis for historical texts,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
540,http://dx.doi.org/10.1093/llc/fqv028,"{'string_id': '10.1093/llc/fqv028', 'id_scheme': 'DOI'}","The relationship between two important semantic properties (polysemy and synonymy) of language and one of the most fundamental syntactic network properties (a degree of the node) is observed. Based on the synergetic theory of language, it is hypothesized that a word which occurs in more syntactic contexts, i.e. it has a higher degree, should be more polysemous and have more synonyms than a word which occurs in less syntactic contexts, i.e. it has a lesser degree. Six languages are used for hypotheses testing and, tentatively, the hypotheses are corroborated. The analysis of syntactic dependency networks presented in this study brings a new interpretation of the well-known relationship between frequency and polysemy (or synonymy).",Polysemy and Synonymy in Syntactic Dependency Networks,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
541,http://dx.doi.org/10.1093/llc/fqv030,"{'string_id': '10.1093/llc/fqv030', 'id_scheme': 'DOI'}","Recently, a claim was made, on the basis of the German Google Books 1-gram corpus (Michel et al., Quantitative Analysis of Culture Using Millions of Digitized Books. Science 2010; 331: 176–82), that there was a linear relationship between six non-technical non-Nazi words and three ‘explicitly Nazi words’ in times of World War II (Caruana-Galizia. 2015. Politics and the German language: Testing Orwell’s hypothesis using the Google N-Gram corpus. Digital Scholarship in the Humanities [Online]. http://dsh.oxfordjournals.org/cgi/doi/10.1093/llc/fqv011 (accessed 15 April 2015)). Here, I try to show that apparent relationships like this are the result of misspecified models that do not take into account the temporal aspect of time-series data. The main point of this article is to demonstrate why such analyses run the risk of incorrect statistical inference, where potential effects are both meaningless and can potentially lead to wrong conclusions.",Why the quantitative analysis of diachronic corpora that does not consider the temporal aspect of time-series can lead to wrong conclusions,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
542,http://dx.doi.org/10.1093/llc/fqv031,"{'string_id': '10.1093/llc/fqv031', 'id_scheme': 'DOI'}","A Systematic Literature Review (SLR) identifies, evaluates, and synthesizes the literature available for a given topic. This generally requires a significant human workload and has subjectivity bias that could affect the results of such a review. Automated document classification can be a valuable tool for recommending the selection of studies. In this article, we propose an automated pre-selection approach based on text mining and semantic enrichment techniques. Each document is firstly processed by a named entity extractor. The DBpedia URIs coming from the entity linking process are used as external sources of information. Our system collects the bag of words of those sources and it adds them to the initial document. A Multinomial Naive Bayes classifier discriminates whether the enriched document belongs to the positive example set or not. We used an existing manually performed SLR as benchmark data set. We trained our system with different configurations of relevant documents and we tested the goodness of our approach with an empirical assessment. Results show a reduction of the manual workload of 18% that a human researcher has to spend, while holding a remarkable 95% of recall, important condition for the nature itself of SLRs. We measure the effect of the enrichment process to the precision of the classifier and we observed a gain up to 5%.",Semantic Enrichment for Recommendation of Primary Studies in a Systematic Literature Review,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
543,http://dx.doi.org/10.1093/llc/fqv032,"{'string_id': '10.1093/llc/fqv032', 'id_scheme': 'DOI'}","Word reordering is one of the fundamental problems of machine translation. It is an important factor in the quality and efficiency of machine translations. Tackling the reordering problem can lead to significant improvements in translation quality. A new method is introduced with the objective of solving the reordering problem. It exploits sophisticated syntactic-based features for re-ranking the n-best translation candidates provided by a phrase-based statistical machine translation system. These sophisticated reordering features are based on an innovative structure named the phrasal dependency tree, which is inspired from target-side dependency relations among contiguous non-syntactic phrases. The features benefit from phrase dependencies, translation orientation, and distance. A translation candidate is modelled as a directed and weighted graph built from information provided by the reordering features and is re-scored by the proposed re-ranking system. This system markedly improves the outputs of the machine translation of two syntactically divergent language pairs. The performance is evaluated for Persian→English and German→English translation tasks using the WMT07 benchmark. The results report 0.566/0.95/0.011- and 0.75/0.97/0.024-point improvements in terms of BLEU/TER/LRSCORE metrics on Persian→English and German→English translation tasks, respectively. The superiority of the proposed system in terms of precision and recall measures is demonstrated as well.",Improving Statistical Machine Translation using Syntax-based Learning-to-Rank System,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
544,http://dx.doi.org/10.1093/llc/fqv033,"{'string_id': '10.1093/llc/fqv033', 'id_scheme': 'DOI'}","This article reports on a Digital Humanities research project which is concerned with the automated linguistic and visual analysis of political discourses with a particular focus on the concept of deliberative communication. According to the theory of deliberative communication as discussed within political science, political debates should be inclusive and stakeholders participating in these debates are required to justify their positions rationally and respectfully and should eventually defer to the better argument. The focus of the article is on the novel interactive visualizations that combine linguistic and statistical cues to analyze the deliberative quality of communication automatically. In particular, we quantify the degree of deliberation for four dimensions of communication: Participation, Respect, Argumentation and Justification, and Persuasiveness. Yet, these four dimensions have not been linked within a combined linguistic and visual framework, but each single dimension helps determining the degree of deliberation independently from each other. Since at its core, deliberation requires sustained and appropriate modes of communication, our main contribution is the automatic annotation and disambiguation of causal connectors and discourse particles.",Visual Linguistic Analysis of Political Discussions: Measuring Deliberative Quality,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
545,http://dx.doi.org/10.1093/llc/fqv034,"{'string_id': '10.1093/llc/fqv034', 'id_scheme': 'DOI'}","Weighted Gene Co-Expression Network Analysis (WGCNA) is a technique developed for analysing gene co-expression microarray data. This article demonstrates the adaption of WGCNA to mine an entire genre of literature, describing its dominant features, exposing its evolution through time, and dissecting its plot structures. WGCNA not only finds these large-scale structures, but exposes the fine-grained contribution of individual words. Romance accounts for 16.7% of all novels sold in the USA, and is the most popular form of genre literature (Romance Writers of America, 2013; Industry Statistics. http://www.rwa.org/p/cm/ld/fid=580 (accessed 13 June 2013)). Selling into 111 different markets and thirty-one languages, Harlequin is the largest publisher of romance worldwide, and its most successful category line, Harlequin Presents, offers the reader ‘pure romantic fantasy’ (Harlequin Presents Writing Guidelines)—romance in an almost archetypal sense. Despite the popularity, ubiquity and profitability of category romance, there is relatively little study to date on the phenomenon. Using WGCNA together with all electronically available Harlequin Presents novels—some 1,400 from 1999 to 2013—this article demonstrates that the genre’s fundamental architecture is a choir of authorial voices, that its evolution is dominated by sudden shifts due to financial pressures on the publisher, and that the order in which elements appear—the plot—is largely fixed.",Whole Genre Sequencing,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
546,http://dx.doi.org/10.1093/llc/fqv035,"{'string_id': '10.1093/llc/fqv035', 'id_scheme': 'DOI'}","Two graphical methods of exploring aspects of cutting structure in film using shot-length data are presented. The first of these converts the cumulative frequencies of shot-lengths to tabular form that can be displayed in various ways, including the use of correspondence analysis. The method is used to investigate the differences in cutting rates used by Mack Sennett and Charlie Chaplin while directing films for the Keystone Company in 1912–14. Results suggest that previous analyses based on the average shot-length may over-simplify the contrast, and some evidence for the evolution of Sennett’s style in 1913 is also suggested. The methods used, like much of the literature, do not take into account the time-series structure of the shot-lengths. In the second approach presented, this is allowed for by smoothing the time-series of shot-lengths using non-parametric regression. A computer-intensive way of presenting results graphically is developed, that does not commit the analyst to a particular choice of smoothing level, and is used to investigate an ‘hypothesis’ about D. W. Griffith’s cutting style suggested by a ‘prescription’ for pacing implied in comments made in an article published under his name in the 1920s. It is shown that his major feature films between 1914 and 1921 conform to the prescription, but there is not much evidence for it in his earlier and later work.","Exploring Cutting Structure in Film, with Applications to the Films of D. W. Griffith, Mack Sennett, and Charlie Chaplin",Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
547,http://dx.doi.org/10.1093/llc/fqv036,"{'string_id': '10.1093/llc/fqv036', 'id_scheme': 'DOI'}","Multispectral imaging—a method for acquiring image data over a series of wavelengths across the light spectrum—is becoming a valuable tool within the cultural and heritage sector for the recovery and enhancement of information contained within primary historical texts. However, most applications of this technique, to date, have been bespoke: analysing particular documents of historic importance. There has been little prior work done on evaluating this technique in a structured fashion, to provide recommendations on how best to capture and process images when working with damaged and abraded textual material. This article introduces a new approach for evaluating the efficacy of image processing algorithms in recovering information from multispectral images of deteriorated primary historical texts. We present a series of experiments that deliberately degrade samples cut from a real historical document to provide a set of images acquired before and after damage. These images then allow us to compare, both objectively and quantitatively, the effectiveness of multispectral imaging and image processing for recovering information from damaged text. We develop a methodological framework for the continuing study of the techniques involved in the analysis and processing of multispectral images of primary historical texts, and a dataset which will be of use to others interested in advanced digitisation techniques within the cultural heritage sector.",The value of critical destruction: Evaluating multispectral image processing methods for the analysis of primary historical texts,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
548,http://dx.doi.org/10.1093/llc/fqv037,"{'string_id': '10.1093/llc/fqv037', 'id_scheme': 'DOI'}","The Google Ngram Corpora seem to offer a unique opportunity to study linguistic and cultural change in quantitative terms. To avoid breaking any copyright laws, the data sets are not accompanied by any metadata regarding the texts the corpora consist of. Some of the consequences of this strategy are analyzed in this article. I chose the example of measuring censorship in Nazi Germany, which received widespread attention and was published in a paper that accompanied the release of the Google Ngram data (Michel et al. (2010): Quantitative analysis of culture using millions of digitized books. Science, 331(6014): 176–82). I show that without proper metadata, it is unclear whether the results actually reflect any kind of censorship at all. Collectively, the findings imply that observed changes in this period of time can only be linked directly to World War II to a certain extent. Therefore, instead of speaking about general linguistic or cultural change, it seems to be preferable to explicitly restrict the results to linguistic or cultural change ‘as it is represented in the Google Ngram data’. On a more general level, the analysis demonstrates the importance of metadata, the availability of which is not just a nice add-on, but a powerful source of information for the digital humanities.",The Impact of Lacking Metadata for the Measurement of Cultural and Linguistic Change Using the Google Ngram Data Sets—Reconstructing the Composition of the German Corpus in Times of WWII,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
549,http://dx.doi.org/10.1093/llc/fqv038,"{'string_id': '10.1093/llc/fqv038', 'id_scheme': 'DOI'}"," For some time, scholars have been using computer-assisted methods to produce graphic representations of the relationships between witnesses within a textual tradition. 1 The use of methods originally developed by evolutionary biologists has been called into question on account of the perceived lack of identity between two different disciplines. This view arises from a misunderstanding about how the methods work in relation to texts and how the resulting stemmata should be interpreted. This article refines textual critical terminology, particularly the distinction between textual traditions and manuscript traditions, in the context of the use of computer-assisted stemmatological methods to further our understanding of how these fit within the wider theoretical framework of textual criticism and scholarly editing, and makes explicit the way in which stemmata produced by using evolutionary biology software should be read. ",The genealogy of texts: Manuscript traditions and textual traditions,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
550,http://dx.doi.org/10.1093/llc/fqv039,"{'string_id': '10.1093/llc/fqv039', 'id_scheme': 'DOI'}","Our hypothesis was simple enough: undergraduates would better understand a historic document online if, instead of having a traditional textual introduction, the same information was made available in bite-size balloons invoked by the users clicking on pins distributed throughout the document. Half the students had a pinned edition and half a more traditional one, while they all had several hours to explore the same eight-page crew agreement from the late 19th century. They then filled out quizzes, short answer tests, and went through an extensive debriefing. The results surprised us. Form made no difference whatsoever, none of the students understood the document’s content. We concluded their difficulty stemmed from living in the immediate, rather than in a temporally informed present, and so they could not fathom the profundity of the past. Seeking to understand this dramatic foreshortening of the analytics of existence, an issue of general interest to humanists, we searched for guidance in the digital humanities and educational literatures with little success. Borrowing from more progressive writings on academic literacy, our solution is a Friere-inspired approach that privileges points of entry for our undergraduates that build on their already-existing knowledge, rather than requiring them to acquire a canonical and frankly out-dated learned past. We argue this critical approach allows our students a democratizing experience that permits them to more fully engage the world as informed citizens.",Realizing the Democratic Potential of Online Sources in the Classroom,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
551,http://dx.doi.org/10.1093/llc/fqv040,"{'string_id': '10.1093/llc/fqv040', 'id_scheme': 'DOI'}","We propose a possible solution to one of the major weaknesses in the application of authorship attribution—the absence of clear-cut standards for accurate analytic practice. To address this, we propose a specific practice as a possible standard and present four recent cases applying this standard. The key elements of this protocol are the use of an ad hoc distractor set in conjunction with multiple analyses structured as a set of elimination tests. This protocol (or close variants of it) has been used in at least four separate cases across a wide variety of documents and consumers. It is mathematically supported while still being easy to understand. We are confident that the proposed protocol will provide a relatively straightforward and understandable way to reduce controversy regarding stylometric authorship attribution, and thereby increase its uptake and credibility.",The Rowling Case: A Proposed Standard Analytic Protocol for Authorship Questions,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
552,http://dx.doi.org/10.1093/llc/fqv041,"{'string_id': '10.1093/llc/fqv041', 'id_scheme': 'DOI'}","This research began in a class taught by Matthew Jockers and has continued under his direction as a project of the newly formed Nebraska Literary Lab. Our work focused on mining the fiction and non-fiction works of Willa Cather that are housed in the Willa Cather Archive at the University of Nebraska-Lincoln (Jewell, Andrew. The Willa Cather Archive. University of Nebraska-Lincoln, 2004–2013. Web). Largely regarded as a private person, Cather forbade the publication of her correspondence; only recently have her letters finally been published. With the publication of these letters comes the unique opportunity for scholars to research Cather’s personal thoughts and voice. Our research focused on using stylometrics to explore the ways in which the voice Cather used in her correspondence differs from the voice she used in her public writing; our conclusions point to similarities between Cather’s novel My Mortal Enemy, a work noted for both its economy of style and autobiographical features, and her recently published letters.",Exploring the Intersection of Personal and Public Authorial Voice in the Works of Willa Cather,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,30,suppl 1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
553,http://dx.doi.org/10.1093/llc/fqv042,"{'string_id': '10.1093/llc/fqv042', 'id_scheme': 'DOI'}","This article presents a selection of findings from a survey-based study on the role of software development and programming in the Digital Humanities, disseminated to researchers, teachers, and practitioners from across the community.",Programming in the Digital Humanities,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
554,http://dx.doi.org/10.1093/llc/fqv043,"{'string_id': '10.1093/llc/fqv043', 'id_scheme': 'DOI'}","This article motivates and details the first implementation of a freely available part of speech tag set and tagger for Coptic. Coptic is the last phase of the Egyptian language family and a descendant of the hieroglyphs of ancient Egypt. Unlike classical Greek and Latin, few resources for digital and computational work have existed for ancient Egyptian language and literature until now. We evaluate our tag set in an inter-annotator agreement experiment and examine some of the difficulties in tagging Coptic data. Using an existing digital lexicon and a small training corpus taken from several genres of literary Sahidic Coptic in the first half of the first millennium, we evaluate the performance of a stochastic tagger applying a fine-grained and coarse-grained set of tags within and outside the domain of literary texts. Our results show that a relatively high accuracy of 94–95% correct automatic tag assignment can be reached for literary texts, with substantially worse performance on documentary papyrus data. We also present some preliminary applications of natural language processing to the study of genre, style, and authorship attribution in Coptic and discuss future directions in applying computational linguistics methods to the analysis of Coptic texts.",Computational Methods for Coptic: Developing and Using Part-of-Speech Tagging for Digital Scholarship in the Humanities,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,30,suppl 1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
555,http://dx.doi.org/10.1093/llc/fqv045,"{'string_id': '10.1093/llc/fqv045', 'id_scheme': 'DOI'}","The use of metaphor in popular science is widespread to aid readers’ conceptions of the scientific concepts under discussion. Almost all research in this area has been done by careful close reading of the text(s) in question, but this article describes—for the first time—a digital ‘distant reading’ analysis of popular science, using a system created by a team from Glasgow and Lancaster. This team, as part of the SAMUELS project, has developed semantic tagging software which is based upon the UCREL Semantic Analysis System developed by Lancaster University’s University Centre for Computer Corpus Research on Language, but using the uniquely comprehensive Historical Thesaurus of English (published in 2009 as The Historical Thesaurus of the Oxford English Dictionary) as its knowledge base, in order to provide fine-grained meaning distinctions for use in word-sense disambiguation. In addition to analyzing metaphors in highly abstract book-length popular science texts from physics and mathematics, this article describes the technical underpinning to the system and the methods employed to hone the word-sense disambiguation procedure.","Metaphor, Popular Science, and Semantic Tagging: Distant reading with theHistorical Thesaurus of English",Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
556,http://dx.doi.org/10.1093/llc/fqv046,"{'string_id': '10.1093/llc/fqv046', 'id_scheme': 'DOI'}","Large-scale digitization efforts and the availability of computational methods, including text mining and information visualization, have enabled new approaches to historical research. However, we lack case studies of how these methods can be applied in practice and what their potential impact may be. Trading Consequences is an interdisciplinary research project between environmental historians, computational linguists, and visualization specialists. It combines text mining and information visualization alongside traditional research methods in environmental history to explore commodity trade in the 19th century from a global perspective. Along with a unique data corpus, this project developed three visual interfaces to enable the exploration and analysis of four historical document collections, consisting of approximately 200,000 documents and 11 million pages related to commodity trading. In this article, we discuss the potential and limitations of our approach based on feedback from historians we elicited over the course of this project. Informing the design of such tools in the larger context of digital humanities projects, our findings show that visualization-based interfaces are a valuable starting point to large-scale explorations in historical research. Besides providing multiple visual perspectives on the document collection to highlight general patterns, it is important to provide a context in which these patterns occur and offer analytical tools for more in-depth investigations.",Trading Consequences: A Case Study of Combining Text Mining and Visualization to Facilitate Document Exploration,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
557,http://dx.doi.org/10.1093/llc/fqv047,"{'string_id': '10.1093/llc/fqv047', 'id_scheme': 'DOI'}","There are many ways to model relationships between texts and objects. Creating statements with RDF triples is one of them. In the project ‘Semantic Blumenbach', we have endeavoured to discover and render visible the innate connections of Johann Friedrich Blumenbach's (1752–1840) writings on natural history with the physical objects studied and collected by him. After a careful evaluation of existing frameworks for describing objects and texts, we decided to test the Scientific Communication Infrastructure (WissKI) for this purpose. During the project, new modules have been developed, and a workflow to connect extracted knowledge from Blumenbach’s texts with metadata of objects has been established. The data modelling, the ingest workflow, and project evaluation in the context of the on-going discussion about Linking TEI and CIDOC Conceptual Reference Model are subjects of this article.",Semantic Blumenbach: Exploration of Text–Object Relationships with Semantic Web Technology in the History of Science,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
558,http://dx.doi.org/10.1093/llc/fqv049,"{'string_id': '10.1093/llc/fqv049', 'id_scheme': 'DOI'}","This article describes the development and application of an innovative tool, Text Re-use Alignment Visualization (TRAViz), whose aim is to visualize variation between editions of both historical and modern texts. Reading different editions of a text empowers research in literary studies and linguistics, where one can study a text’s reception or follow the development of its language over time. One of the purposes of a text edition is to trace or reconstruct a possible archetype or something that might be considered to be an original version of the text in order to better understand its evolution over time. To do so, the textual scholar examines and records the similarities and the differences between a number of exemplars in what is known as a ‘critical apparatus’. The result of this variant analysis can be visually represented as a ‘Variant Graph’, where the relationships between these exemplars can be more easily studied. Variant Graphs can be, in turn, visualized in order to facilitate reading and interaction with the source data. Borrowing from existing digital tools, TRAViz assists the scholar in the collation process by specifically focusing on design and user engagement, concurrently seeking to simplify interaction as a means of encouraging humanists to adopt the tool. The article will describe the needs and rationale behind the creation of TRAViz by exploring existing research, describing its functionality through examples, and by finally discussing how its application can influence future development of this tool in particular and of the field in general.",TRAViz: A Visualization for Variant Graphs,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
559,http://dx.doi.org/10.1093/llc/fqv050,"{'string_id': '10.1093/llc/fqv050', 'id_scheme': 'DOI'}","In this essay, the authors present a case study of how an ongoing, multi-faculty, interdisciplinary DH project focused on the Susquehanna Valley in Pennsylvania has created, and continues to explore, ways in which students can excel both inside the classroom and outside. These DH projects involve undergraduates working with faculty on an unfolding expansive research project that affords otherwise unachievable opportunities for undergraduate student engagement, the development of new skills, and meaningful ongoing interaction between the institution and community that have, in turn, furthered the scope and scale of the project.",Digital Learning in an Undergraduate Context: Promoting Long-Term Student–Faculty Place-Based Collaboration,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
560,http://dx.doi.org/10.1093/llc/fqv051,"{'string_id': '10.1093/llc/fqv051', 'id_scheme': 'DOI'}","Research data in the humanities needs to be sustainable, and access to digital resources must be possible over a long period. Only if these prerequisites are fulfilled can research data be used as a source for other projects. In addition, reliability is a fundamental requirement so that digital sources can be cited, reused, and quoted. To address this problem, we present our solution: the Data and Service Center for the Humanities located in Switzerland. The centralized infrastructure is based on flexible and extendable software that is in turn reliant on modern technologies. Such an approach allows for the straightforward migration of existing research project databases with limited life spans in the humanities. We will demonstrate the basic concepts behind this proposed solution and our first experiences in the application thereof.",DASCH: Data and Service Center for the Humanities,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
561,http://dx.doi.org/10.1093/llc/fqv052,"{'string_id': '10.1093/llc/fqv052', 'id_scheme': 'DOI'}","How do humanities scholars make sense of new or otherwise unfamiliar archives? Is there a role for computational text analysis in the process of sensemaking? We propose that topic modeling, when conceived as a process of thematic exploration, can provide a new entry point into this process. To this end, we present research on a new software tool called TOME: Interactive TOpic Model and MEtadata Visualization, designed to support the exploratory thematic analysis of digitized archival collections. TOME is centered around a set of visualizations intended to facilitate the interpretation of the topic model and its incorporation into extant humanities research practices. In contrast to other topic model browsers, which present the model on its own terms, ours is informed by the process of conducting early-stage humanities research. Our article thus also demonstrates the conceptual conversions—in terms of both design and process—that interdisciplinary collaboration necessarily entails. In making these conversions explicit, and exploring the implications of their successes and failures, we take up the call, as voiced by Johanna Drucker (Humanities approaches to graphical display. Digital Humanities Quarterly, 5(1), 2011), to resist the ‘intellectual Trojan horse’ of visualization. We seek to model a new mode of interdisciplinary inquiry, one that brings the methodological emphasis of the digital humanities to bear on the practices of humanities research and computer science alike.",Exploratory Thematic Analysis for Digitized Archival Collections,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,30,suppl 1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
562,http://dx.doi.org/10.1093/llc/fqv053,"{'string_id': '10.1093/llc/fqv053', 'id_scheme': 'DOI'}","One of the important issues in natural language processing and information retrieval is the automatic extraction of the word’s stem. Both statistical and rule-based approaches for stemming have their own advantages and limitations. The statistical stemmers are not accurate and fail to take advantage of some language phenomenon which can be easily expressed by simple rules. On the other hand, handcrafting the stemming rules in the rule-based stemmers is a time-consuming, tedious, and impractical task. In this regard, we propose a new hybrid stemming method based on a combination of affix stripping and statistical techniques for Persian language. The proposed method combines cues from the orthography, word frequency, and syntactic distributions to induce the stemming rules. In general, the proposed method is divided into two main parts. In the first part, all words of the annotated text corpus are used to automatically induce the stemming rules; while in the second part, the rule-based stemmer uses the induced stemming rules to discover the word's stem. We test the performance of the proposed scheme on two different data sets. The encouraging results indicate the superior performance of the proposed method compared with its counterparts.",A New Hybrid Stemming Method for Persian Language,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
563,http://dx.doi.org/10.1093/llc/fqv058,"{'string_id': '10.1093/llc/fqv058', 'id_scheme': 'DOI'}","Over 100 years after the publication of Mark Twain’s The Adventures of Huckleberry Finn, it still remains a highly reputed classic not only in America but also elsewhere around the globe. However, Twain’s representation of linguistic diversity in his native Missouri region has given rise to a heated dispute, still ongoing, over his, according to some detractors, vaudeville-like characterization of Jim. Such controversy has been further spurred by recent voices lampooning Twain for, allegedly, having portrayed Jim as one more ethnic caricature. Translators seem to have paid no heed to Twain’s cautionary words in his preface, stating that he was using several dialects from his region and that his recording of these dialects had not been done in a ‘haphazard fashion’ but ‘painstakingly’. Unfortunately, many translations have either rendered a standardized version in which all characters speak alike or, worse still, a twisted mirror in which Jim appears speaking in the dialect of a far remote region, say Andalusia or Naples. The effect in the target culture is a considerable distortion, and yet, translators still keep claiming that it is utterly impossible to do otherwise. Given the current state of affairs, one may ask, is there a way to legitimize the translation of dialect? Was Twain’s intention parody or was it, on the contrary, authenticity? Determining this is of paramount importance prior to undertaking a translation of this work into a foreign culture. Corpus linguistics, as I would like to prove heretofore, can greatly contribute to gauging whether dialect has been transcribed consistently or accurately. A thorough linguistic inquiry into Jim’s corpus and a comparison with the corpora of other characters can yield very interesting results. As some Chinese translations of this work have shown, drawing from linguistically oriented data can be very helpful in using the right translation strategies.",The Adventures of Huckleberry Finn and Jim in China: A Case of what Corpus Pragmatics can do for the Translation of Dialect,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
564,http://dx.doi.org/10.1093/llc/fqv061,"{'string_id': '10.1093/llc/fqv061', 'id_scheme': 'DOI'}","The aim of this article is to discuss reliability issues of a few visual techniques used in stylometry, and to introduce a new method that enhances the explanatory power of visualization with a procedure of validation inspired by advanced statistical methods. A promising way of extending cluster analysis dendrograms with a self-validating procedure involves producing numerous particular ‘snapshots’, or dendrograms produced using different input parameters, and combining them all into the form of a consensus tree. Significantly better results, however, can be obtained using a new visualization technique, which combines the idea of nearest neighborhood derived from cluster analysis, the idea of hammering out a clustering consensus from bootstrap consensus trees, with the idea of mapping textual similarities onto a form of a network. Additionally, network analysis seems to be a good solution for large data sets.",Visualization in stylometry: Cluster analysis using networks,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,32,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
565,http://dx.doi.org/10.1093/llc/fqv062,"{'string_id': '10.1093/llc/fqv062', 'id_scheme': 'DOI'}","This article discusses two major initiatives tasked with developing tools to improve optical character recognition (OCR) or the mechanical keying of texts that are digitally available only as page images. The two initiatives are the IMProving ACcess to Text Project in Europe and the Early Modern OCR Project in the USA. Because of dealing with a multilayered problem like OCR technologies and having to collaborate with radically interdisciplinary and international team members, the two projects developed techniques that we call Agile Project Management, outlined in this essay with rationales for their use.","Navigating the Storm: IMPACT, eMOP, and Agile Steering Standards",Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
566,http://dx.doi.org/10.1093/llc/fqv063,"{'string_id': '10.1093/llc/fqv063', 'id_scheme': 'DOI'}","As digital humanities (DH) continues to embrace its global dimensions, community members struggle to ascertain frames of reference for understanding and interpreting local contexts for scholarship. This article intervenes in that effort by distinguishing between the local and global contours of DH. It analyzes two projects that map the geographies of DH and identifies the challenge of recognizing DH work. Drawing on postcolonial and linguistic theories of language, this article then proposes that the concept of a ‘DH accent’ provides a lens for mediating between local and global definitions of DH and resolving the ethical challenge of misrecognition. In seeking a global vision, the article suggests, the DH community must begin with the question, ‘What is your DH accent?’","Other Worlds, Other DHs: Notes towards a DH Accent",Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
567,http://dx.doi.org/10.1093/llc/fqv065,"{'string_id': '10.1093/llc/fqv065', 'id_scheme': 'DOI'}","This article surveys how phylogenetics may be applied effectively and productively to the analysis of textual traditions—and, by implication, how it might not be. Examples from two very different traditions (the artificial Julius Caesar ‘Orange Branch’, and Chaucer’s Wife of Bath’s Prologue) are deployed to support four central premises. First, it is an error to understand the results of any quantitative analysis of textual traditions as if they represent exactly what happened in the actual making of these copies. Second, phylogenetic methods can give useful results on uncorrected, unregularized data for vernacular and other traditions, where the spelling of individual words is relatively stable across copies. Third, before any collation, any transcription, any data preparation is started toward investigation of a textual tradition, and before analysis commences, the editor must develop an explicit model of the variation he or she expects to find in the copies which constitute that tradition. Fourth, one can only be as certain, in any reconstruction of any textual tradition, as the data, the model of variation, the methods applied, and other evidence allow. The discussion leads to a final conclusion that scholars should use both traditional qualitative analysis and the new quantitative methods to complement, correct, and complete each other.",Four rules for the application of phylogenetics in the analysis of textual traditions,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
568,http://dx.doi.org/10.1093/llc/fqv066,"{'string_id': '10.1093/llc/fqv066', 'id_scheme': 'DOI'}","Historians of the English language and students of literary style alike have long agreed that a key change took place in American prose style at the end of the late 19th century, when a more informal, ‘democratic' register came to dominate fictional prose. However, despite its historical and critical importance, neither the features nor the precise historical development of this shift has been the subject of systematic analysis. In this essay, we undertake an in-depth analysis of one key feature of what became known as the colloquial style: patterns of linguistic repetition. With the aid of quantitative analysis, we demonstrate that the use of repetition is in itself a reliable metric for automatically detecting the presence of colloquial discourse. We find that colloquial repetition does indeed increase in American fiction over the course of the 19th and early 20th century, but, via comparative analysis, that this phenomenon may not be limited to American prose. Finally, we explore the semantics of these patterns of repetition, demonstrating first that repetition is broadly characteristic of represented speech in writing, and secondly that changes over time in the semantic contents of repetitions demonstrate a clear increase in colloquial, informal language.",Operationalizing the Colloquial Style: Repetition in 19th-Century American Fiction,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
569,http://dx.doi.org/10.1093/llc/fqv067,"{'string_id': '10.1093/llc/fqv067', 'id_scheme': 'DOI'}","I describe the collection and deep annotation of the semantics of a corpus of Russian folktales. This corpus, which I call the ‘ProppLearner’ corpus, was assembled to provide data for an algorithm designed to learn Vladimir Propp’s morphology of Russian hero tales. The corpus is the most deeply annotated narrative corpus available at this time. The algorithm and learning results are described elsewhere; here, I provide detail on the layers of annotation and how they were chosen, novel layers of annotation required for successful learning, the selection of the texts for annotation, the annotation process itself, and the resulting inter-annotator agreement measures. In particular, the corpus comprised fifteen texts totaling 18,862 words. There were eighteen layers of annotation, five of which were developed specifically to support learning Propp’s morphology: referent attributes, context relationships, event valences, Propp’s ‘dramatis personae’, and Propp’s functions. All annotations were created by trained annotators with the Story Workbench annotation tool, following a double-annotation paradigm. I discuss lessons learned from this effort and what they mean for future digital humanities efforts when working with the semantics of natural language text.",ProppLearner: Deeply Annotating a Corpus of Russian Folktales to Enable the Machine Learning of a Russian Formalist Theory,Oxford University Press (OUP),2015,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
570,http://dx.doi.org/10.1093/llc/fqv068,"{'string_id': '10.1093/llc/fqv068', 'id_scheme': 'DOI'}","The contribution presents an ongoing research project that aims at designing a dynamic grammar of Ancient Greek. Relying on a Drupal-based solution, students will be trained to move between the language’s formal, semantic, and syntactic levels, so as to overcome the static character of a traditional grammar. A short description of the shortcomings of traditional Ancient Greek grammars is followed by a description of both the micro-structure and the macro-structure of the grammar. The final section focuses on the implementation of the tool in classes.Benefiting from recent developments and insights in the fields of technology, linguistics, and language didactics alike, the Greek grammar Pedalion (http://www.pedalion.org/—Ancient Greek πηδάλιον means ‘rudder’) seeks to offer a ‘contemporary’ instrument that is tailored to mastering and understanding ‘ancient’ languages.",Reconciling the Dynamics of Language with a Grammar Handbook: The Ongoing Pedalion Grammar Project,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
571,http://dx.doi.org/10.1093/llc/fqv070,"{'string_id': '10.1093/llc/fqv070', 'id_scheme': 'DOI'}","This article will describe social networks and the concepts of social network analysis. It will then move on to describe some of the uses social network analysis has been put to in historical research. This will be followed by a description of the People of Medieval Scotland database, which provides the data for this research. Finally, the social network analysis techniques used in this research will be described and the preliminary results that reveal findings that traditional historical methods had not will be discussed, including identifying an additional role played by Duncan II Earl of Fife, and using network density model for the diffusion of innovations to identify opinion leaders in medieval Scotland.",Using Social Network Analysis to Reveal Unseen Relationships in Medieval Scotland,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
572,http://dx.doi.org/10.1093/llc/fqv071,"{'string_id': '10.1093/llc/fqv071', 'id_scheme': 'DOI'}","The concept of dramatic situation is important in dramaturgy and narratology. In the domain of story generation and interactive digital storytelling, this concept is particularly powerful in creating meaningful story variations from a single core model. Nevertheless, dramatic situations and the related notion of deep narrative structures have been overlooked in the domain of computational models of narrative. This article presents a computational model of dramatic situations. Designed with creative authors in mind, the model consists of a small set of building blocks that, when assembled with specific relations, create narrative structures. Some structures that are described are of particular interest from a dramatic point of view, for they embed a fundamental paradox. These structures are generalized and formalized to allow an exhaustive search and to establish an initial list of dramatic situations that share this property.",Modeling and Representing Dramatic Situations as Paradoxical Structures,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
573,http://dx.doi.org/10.1093/llc/fqv072,"{'string_id': '10.1093/llc/fqv072', 'id_scheme': 'DOI'}","Modernist authors such as Virginia Woolf and James Joyce greatly expanded the use of ‘free indirect discourse’, a form of third-person narration that is strongly influenced by the language of a viewpoint character. Unlike traditional approaches to analyzing characterization using common words, such as those based on Burrows (1987), the nature of free indirect discourse and the sparseness of our data require that we understand the stylistic connotations of rarer words and expressions which cannot be gleaned directly from our target texts. To this end, we apply methods introduced in our recent work to derive information with regards to six stylistic aspects from a large corpus of texts from Project Gutenberg. We thus build high-coverage, finely grained lexicons that include common multiword collocations. Using this information along with student annotations of two modernist texts, Woolf’s To The Lighthouse and Joyce’s The Dead, we confirm that free indirect discourse does, at a stylistic level, reflect a mixture of narration and direct speech, and we investigate the extent to which social attributes of the various characters (in particular age, class, and gender) are reflected in their lexical stylistic profile.",Using Models of Lexical Style to Quantify Free Indirect Discourse in Modernist Fiction,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
574,http://dx.doi.org/10.1093/llc/fqw001,"{'string_id': '10.1093/llc/fqw001', 'id_scheme': 'DOI'}","Pliny the Younger's letter to Trajan regarding the Christians is a crucial subject for the studies on early Christianity. A serious quarrel among scholars concerning its genuineness arose between the end of the 19th century and the beginning of the 20th; per contra, Plinian authorship has not been seriously questioned in the last few decades. After analysing various kinds of internal and external evidence in favour of and against the authenticity of the letter, a modern stylometric method is applied in order to examine whether internal linguistic evidence allows one to definitely settle the debate.The findings of this analysis tend to contradict received opinion among modern scholars, affirming the authenticity of Pliny’s letter, and suggest instead the presence of large amounts of interpolation inside the text of the letter, since its stylistic behaviour appears highly different from that of the rest of Book X.",An Application of a Profile-Based Method for Authorship Verification: Investigating the Authenticity of Pliny the Younger’s Letter to Trajan Concerning the Christians,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
575,http://dx.doi.org/10.1093/llc/fqw002,"{'string_id': '10.1093/llc/fqw002', 'id_scheme': 'DOI'}","Presented here is a geovisual reading of all three volumes of Karl Marx's Capital. Marx's seminal treatise on political economy is normally treated as a work of abstract conceptualization. However, Marx names hundreds of geographic locations in Capital, usually in a highly relational and dynamic fashion. It seemed to me there was enough geographic information contained in the volumes to produce a geovisually rich map, presenting the themes, places, and relationships in this text in a new and revealing way.",Mapping the Geography of Karl Marx’s Capital,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,32,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
576,http://dx.doi.org/10.1093/llc/fqw003,"{'string_id': '10.1093/llc/fqw003', 'id_scheme': 'DOI'}","This article suggests that Jerome McGann’s proposal for social text editing can be applied to editions understood not as one author’s works, but rather as networks of publications by many authors and editors. The ability to create such an edition has been hampered in the past by the inability of HTML to express the semantic richness of TEI XML. However, by adopting the new semantic tags, custom data attributes, and schema.org microdata introduced with HTML5, an interoperable digital social edition can be feasible. The Grub Street Project, an edition of books, pamphlets, and data from 18th-century London, is a test of this premise.",The Grub Street Project: A digital social edition of London in the long 18th century,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
577,http://dx.doi.org/10.1093/llc/fqw004,"{'string_id': '10.1093/llc/fqw004', 'id_scheme': 'DOI'}","Semi-automated extraction of details corresponding to narratological fabula from a corpus of narrative interviews on a single event provides decontextualized building blocks for transversal, or cross-document, narratives. With information extracted from 503 World Trade Center Task Force Interviews comprising 12,000 pages of testimony and novel visualization techniques, this article proposes a computational method for the emergence of narratives that cross beyond the boundaries of one interview. These assembled narratives, in cases like that of Chief Ganci, can document those who did not survive to tell their own story.","Visualizing Computational, Transversal Narratives from the World Trade Towers",Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '1477-4615', 'type': 'electronic'}]"
578,http://dx.doi.org/10.1093/llc/fqw005,"{'string_id': '10.1093/llc/fqw005', 'id_scheme': 'DOI'}","Many complex systems are naturally described through graph theory, and different kinds of systems described as networks present certain important characteristics in common. One of these features is the so-called scale-free distribution for its node’s connectivity, which means that the degree distribution for the network’s nodes follows a power law. Scale-free networks are usually referred to as small-world because the average distance between their nodes do not scale linearly with the size of the network, but logarithmically. Here we present a mathematical analysis on linguistics: the word frequency effect for different translations of the ‘Le Petit Prince’ in different languages. Comparison of word association networks with random networks makes evident the discrepancy between the random Erdös-Rény model for graphs and real-world networks.",The Small-World of ‘Le Petit Prince’: Revisiting the Word Frequency Distribution,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
579,http://dx.doi.org/10.1093/llc/fqw006,"{'string_id': '10.1093/llc/fqw006', 'id_scheme': 'DOI'}","For most of the world’s 7,000 languages, there are few records available via the Internet. Recognizing this digital divide and the consequential underrepresentation of most languages in any linked open data efforts is a motivation for some solutions offered in this article. Efforts to increase the documentation of the world’s small languages have led to the development of tools and repositories over the past decade. However, as not all digital language archives currently provide metadata in standard formats, their collections are invisible to aggregated searches. Other repositories (including many institutional repositories—national libraries and archives, mission archives, and so on) have language content that is not noted in the collection’s catalog, so is impossible to locate at all via a search based on language names. Finally, there are collections still held by their creators and not in a repository at all, completely hidden from other potential users. This article suggests that it is a digital humanities project to make more information about the world’s small languages more freely available, and identifies several means by which this could be accomplished, including a survey to locate more collections; a register to announce their existence; and a documentation index to provide an overview of what is known for each language.","What Remains to be Done—Exposing Invisible Collections in the other 7,000 Languages and Why it is a DH Enterprise",Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
580,http://dx.doi.org/10.1093/llc/fqw007,"{'string_id': '10.1093/llc/fqw007', 'id_scheme': 'DOI'}","Deliberate differences in how authors represent characters has been a core area of literary investigation since the dawn of literary theory. Here, we focus on epistolary literature, where authors consciously attempt to create different character styles through series of documents like letters. Previous studies suggest that the linguistic gestalt of an author’s style—the author’s ‘writeprint’—can be extracted from the various characters of an epistolary novel, but it is unclear whether individual characters themselves also have distinct writeprints. We examine Samuel Richardson’s Clarissa, lauded as a watershed example of the epistolary novel, using a recently developed and highly successful authorship attribution technique to determine (1) whether Richardson can construct distinct character writeprints, and (2) if so, which linguistic features he manipulated to do so. We find that while there are not as many distinct character writeprints as characters, Richardson does appear to have signature features he alters to create distinct character styles—and few of these features are the function word or abstract syntactic features typically comprising author writeprints. We discuss implications for other questions about character identity in Clarissa and character writeprint analysis more generally.",The Character in the Letter: Epistolary Attribution in Samuel Richardson’sClarissa,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
581,http://dx.doi.org/10.1093/llc/fqw008,"{'string_id': '10.1093/llc/fqw008', 'id_scheme': 'DOI'}","The present investigation is an attempt to investigate how the unique linguistic profile of different text types can be reflected in their respective entropy characteristics. With samples from the Lancaster Corpus of Mandarin Chinese and the Freiburg–Brown corpus of American English, the research investigates entropy performances in two dimensions: the relative entropy of words and their part-of-speech (POS) on different sentential positions, and entropy of aspect markers. Our research yields the following results: First, it shows a strikingly similar distribution pattern in Chinese and English concerning the relative entropy of word-forms and POS-forms on different sentential positions. The relative entropy of word-forms in descending order yields: news > essays > official > academic > fiction, and the POS-forms yields: fiction > essays > news > academic > official. The relative entropy of POS-forms may be a more reliable indicator of syntactical differences, which helps to distinguish dichotomous ‘narrative vs. expository’ text types in both Chinese and English. Second, there exists a cross-linguistic difference concerning entropy of aspect markers, namely, Chinese displays higher relative entropy than English. This indicates that aspect-marking in terms of variation is more prominent in Chinese grammar than in English. The ‘narrative vs. expository distinction’ is also identified by entropy of aspect markers in both Chinese and English, though more obviously in Chinese.",Entropy in Different Text Types,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
582,http://dx.doi.org/10.1093/llc/fqw009,"{'string_id': '10.1093/llc/fqw009', 'id_scheme': 'DOI'}","As digital literary collections continue to expand their scope and to broaden their audience, documenting the collaborative editorial work involved in creating these collections—and rendering that documentation transparent for its users—ensures that scholars continue to develop the idea (and reality) of the ‘social text’ envisioned by D. F. McKenzie. By documenting their collaborative editorial practices in a digital environment, scholarly editors help instantiate both the material history and the authorial, literary, and social contexts of a particular text. By rendering these collaborative practices visible, they make digital collections dynamic and usable for a wide range of individuals. The Walt Whitman Archive (WWA) serves as an ideal case study for examining the ways this particular collection intermediates scholarly editorial practices within a collaborative digital environment, as well as conventions of a scholarly edition and an archive. Through its collaborative editorial practices and guidelines, its layout and design, and its detailed documentation about its conditions of use for the general public, the WWA at once renders more visible the iterative process involved in editorial work, and makes the publicly accessible documentation of that process part of its infrastructure.",Documentation for the public: Social editing inThe Walt Whitman Archive,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
583,http://dx.doi.org/10.1093/llc/fqw011,"{'string_id': '10.1093/llc/fqw011', 'id_scheme': 'DOI'}","One of the great advantages the digital medium has to offer the field of scholarly editing is that it makes its products much easier to distribute. No longer bound to a shelf, the Digital Scholarly Edition has the potential to reach a much wider audience than a printed edition could. To a certain extent, however, the nature of the materials textual scholars are working with dictates the perimeters within which this dissemination can take place. When working with modern manuscripts, for instance, copyright restrictions may limit the extent to which a project can distribute its resources. In an academic climate where open access is not only becoming a standard, but in some cases even a requirement for receiving funding, such limitations may be perceived as problematic. In this article, we argue that even within the boundaries of copyright restrictions there can still be room to produce and distribute the results of textual scholarship. Therefore, the article zooms in on the way in which different Digital Scholarly Editions of copyrighted materials deal with this issue, using the Beckett Digital Manuscript Project (BDMP; www.beckettarchive.org) and Woolf Online (www.woolfonline.com) as case studies. To conclude, we investigate other strategies that may be used to share as much research data as we are allowed to, e.g. by sharing metadata and ancillary data, or by using the fair use doctrine to circumvent the problem. Case studies used for this aspect of the article include ModNets (www.modnets.org), the BDMP Encoding Manual (www.beckettarchive.org/encodingmanual), the Lexicon of Scholarly Editing (http://uahost.uantwerpen.be/lse), and the Finnegans Wake Extensible Elucidation Treasury (FWEET; www.fweet.org).",Digital scholarly editing within the boundaries of copyright restrictions,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
584,http://dx.doi.org/10.1093/llc/fqw012,"{'string_id': '10.1093/llc/fqw012', 'id_scheme': 'DOI'}","The role and usage of a certain technology is not imparted wholesale on the intended user community—technology is not deterministic. Rather, a negotiation between users and the designers of the technology will result in its particular form and function. This article considers a side effect of these negotiations. When a certain known technology is used to convey a new technological concept or model, there is a risk that the paradigm associated by the users with the known technology will eclipse the new model and its affordances in part or in whole. The article presents a case study of this ‘paradigmatic regression’ centering on a transcription tool of the Huygens Institute in the Netherlands. It is argued that similar effects also come into play at a larger scale within the field of textual scholarship, inhibiting the exploration of the affordances of new models that do not adhere to the pervasive digital metaphor of the codex. An example of such an innovative model, the knowledge graph model, is briefly introduced to illustrate the point.",The case of the bold button: Social shaping of technology and the digital scholarly edition,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
585,http://dx.doi.org/10.1093/llc/fqw013,"{'string_id': '10.1093/llc/fqw013', 'id_scheme': 'DOI'}","This essay presents a case study that considers the motivations and needs of a scholarly edition of Joseph Furphy’s Australian novel Such is Life in conjunction with the requirements for the development of the Australian Electronic Scholarly Editing Workbench. The latter integrates a suite of eResearch tools to support the collaborative authoring and management of electronic scholarly editions. The discussion focuses on the theoretical and practical implications of building an electronic edition in such an environment and considers the ways in which the product of these activities begins to move beyond the model of the book. Central to the discussion is the idea of an ontology-based electronic edition, not as an end in itself, but as the ongoing activity of one or more human beings contributing to the creation and assembly of constituent parts with digital tools.","Archiving, editing, and reading on the AustESE Workbench: Assembling and theorizing an ontology-based electronic scholarly edition of Joseph Furphy’sSuch is Life",Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
586,http://dx.doi.org/10.1093/llc/fqw014,"{'string_id': '10.1093/llc/fqw014', 'id_scheme': 'DOI'}"," By comparing the results obtained through traditional qualitative stemmatics with those obtained through computer-assisted quantitative stemmatology, when both approaches are applied to two authentic data sets (the Old English Anglo-Saxon Chronicle and a Latin epitome of Marco Polo’s Devisement dou Monde ), this study aims at bringing to the fore the advantages and the disadvantages of some recent methods for the automatic grouping of witnesses, most of which are based on phylogenetic models. The analysis will show that not all the methods provide results which can be considered reliable in light of the evidence offered by a thorough scrutiny of the documentary history of the texts under inspection. In particular, the majority of computer-assisted methods succeed in providing very good information for detecting the grouping of witnesses, as well as for a preliminary evaluation of their variant readings. Yet, few offer some valuable guidance as to define the sub-groups. This limitation becomes crucial when the existence of more than one codex interpositus has to be postulated, i.e. when the historical evidence makes it clear that the actual textual transmission cannot conform to the mathematical ideal of cladistic parsimony. The latter case is particularly evident with ‘closed’ recensions, where mechanical reconstruction plays a heavier role, and the choice of variants follows rigorous stemmatic steps. On the other hand, the application of computer-assisted methods to ‘open’ recensions seems to give better results. ",Open versus closed recensions (Pasquali): Pros and cons of some methods for computer-assisted stemmatology,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
587,http://dx.doi.org/10.1093/llc/fqw015,"{'string_id': '10.1093/llc/fqw015', 'id_scheme': 'DOI'}","Due to the special features of Persian, developing natural language processing tools for it involves an array of challenges. Lack of efficient Persian knowledge sources is another obstacle to research this language. The goal of this article was to overcome these problems by implementing spelling correction task. The main outputs of this study included a parallel corpus, an N-gram language model for Persian, and a semantic-based spelling correction system named Perspell, which made use of extracted language model. Compared to its rival software (including Vafa spellchecker), Perspell could detect and correct nonword and real word errors more successfully. The rate of real word error detection in Perspell was 95%. In fact, its outstanding ability to detect real word errors as well as its significant improvement in terms of F-measure were the two advantages of the proposed system.",Perspell: A New Persian Semantic-Based Spelling Correction System,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
588,http://dx.doi.org/10.1093/llc/fqw016,"{'string_id': '10.1093/llc/fqw016', 'id_scheme': 'DOI'}","This essay discusses the methodological problems faced by researchers in the humanities, and more especially so those in literature. Its aim is to advance the field of telematics research of literary texts. As it is, literary research is already in checkmate for its high degree of subjectivity and lack of an approach to ensure the effectiveness of its results as it happens in the so-called Hard Sciences. Therefore, when a literato launches him/herself to do research with the use of text analysis software, he/she has to learn from science how to report his/her search so that his/her peers can reproduce the experiment and come to similar results.",On the Path to a Methodology for the Critique of Digital Literature,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
589,http://dx.doi.org/10.1093/llc/fqw017,"{'string_id': '10.1093/llc/fqw017', 'id_scheme': 'DOI'}","FarsiTag is a tagging system capable of assigning the most probable part-of-speech (POS) tags to Persian words in a text. In this system, some linguistic rules have been used to select the best POS tag for every Persian word. The present study aims to report the processes during which a robust tagging system—FarsiTag—was designed and implemented on Persian texts. A POS-tagged parallel corpus of English–Persian containing about 5,000,000 words has also been developed as a side-product of the mentioned tagger. An experiment has been conducted to evaluate the performance of the system while tagging unrestricted Persian texts. The highest rate of error traces back to medical and religious genres, while the lowest system error type is related to the scientific texts. The total error rate considering all domains is as low as 1.4%, with the overall system accuracy of 98.6% which is very promising for a language like Persian.",FarsiTag: A Part-of-Speech Tagging System for Persian,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
590,http://dx.doi.org/10.1093/llc/fqw018,"{'string_id': '10.1093/llc/fqw018', 'id_scheme': 'DOI'}","One of Shakespeare’s least performed plays is King John, a drama indicted by E. K. Chambers for being an ‘incoherent patchwork’. Stylometric evidence over an extended period suggests a division of authorship that sheds light on the play’s ambiguous allegiance to its eponymous hero. The play displays a distinctive cross pattern, both contextually and linguistically. A variety of methods are described here. They include the use of the relative frequency of most common words, the distribution of irregular lines (feminine endings), and the average length of all words in modern spelling. There is also a two-fold cluster analysis with R Stylo, a programme which combines variable extraction with statistical analysis in a single process. This is a promising and accessible means for independent replication. The authorship of King John is a question that can progress towards a solution with modern methods.",Is It Time to Re-thinkKing John?,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
591,http://dx.doi.org/10.1093/llc/fqw019,"{'string_id': '10.1093/llc/fqw019', 'id_scheme': 'DOI'}","Edgar Allan Poe has left us with a literary legacy that in part lacks a definitive stamp of authenticity. While some of the ‘possible Poes’ that have been discovered after his death have generated heated discussion among scholars, many others have quickly slipped into obscurity. This paper reevaluates thirty-two of such prose texts as well as ten poems that have been tenuously attributed to Poe in the past, using the ‘classify’ function (employing Nearest Shrunken Centroid, Burrows’ Delta and Support Vector Machines) of the stylo-toolkit for R. It also sheds new light (with the help of rolling Delta) on Poe’s possible contribution to the so-called Paulding-Drayton review, infamous in Poe criticism for its defense of slavery. By contrasting and comparing the ‘author signal’ behind these pieces with unclear authorship, this analysis is able to answer with a high degree of accuracy the question: Poe or not Poe?",Poe or Not Poe? A Stylometric Analysis of Edgar Allan Poe’s Disputed Writings,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
592,http://dx.doi.org/10.1093/llc/fqw021,"{'string_id': '10.1093/llc/fqw021', 'id_scheme': 'DOI'}","Relying on the analysis of a Latin historical corpus, our research aims to study the markers structuring literary texts in general, and focuses on methods which, by extension, should be valid for any text of some length. Our basic assumption is the following: such texts include complex multilevel structures (i.e. those calling upon lexis, semantics, morphology, syntax, etc) which function as heterogeneity indicators (progression to a new episode, focalization on a new point of view, insertion of reported speech, etc.). Additionally, the recurrence of these structures is a factor in textual cohesion. Under certain conditions, they function as topological ‘motifs’, marking the linear progression of the text and ensuring textual unity. We are developing new methods to detect and analyse the distributions of such ‘motifs’ and to support structural comparisons with the objective of contrastive corpus studies (contrasts between genres, authorial styles, etc.). Our methods are based on mathematical models (neighbourhoods, bursts) and combine a qualitative approach with a sequential quantitative analysis to comprehend language in a linear fashion.",A Text Structure Indicator and two Topological Methods: New Ways for Studying Latin Historic Narratives,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
593,http://dx.doi.org/10.1093/llc/fqw022,"{'string_id': '10.1093/llc/fqw022', 'id_scheme': 'DOI'}","Ever since the Universal Networking Language (UNL) programme started in 1996, researchers and computational linguists across the globe have actively participated in this. English, Chinese, Georgian, Greek, Hindi, Punjabi, Portuguese, Russian, Ukrainian, Vietnamese, Slovenian, etc., are some of the languages which are part of the UNL programme. Although previous years have witnessed significant interest and application development in UNL, UNL can be exploited for several different tasks in natural language engineering, such as multilingual document generation, summarization, text simplification, information retrieval, and semantic reasoning. Although UNL is language-independent artificial language, yet, existing UNL-based applications are standalone in the sense that they are language specific which supports the local natural language of the application developer. This article illustrates the cross-domain client application prototype that has been built to support the development of multilingual UNL-based applications where researchers, developers, or computational linguists across the globe can use this common application for UNL-ization and NL-ization. Output of the proposed system can further be utilized by any other UNL-based application. The proposed system is evaluated on the basis of two measures, i.e. accuracy and correctness. The accuracy of the proposed system is 100%. However, correctness of the proposed system depends upon the F-Measure of UNL-ization and NL-ization modules which has been consistently greater than 0.95 (on a scale of 0–1) for Punjabi language.",A Multilingual Cross-Domain Client Application Prototype for UNL-ization and NL-ization for NLP Applications,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
594,http://dx.doi.org/10.1093/llc/fqw023,"{'string_id': '10.1093/llc/fqw023', 'id_scheme': 'DOI'}","Over the past decade large, searchable collections of primary texts have been embraced by virtually all literary scholars and this has led to changes in how scholarship is conducted. This article offers a partial history of the introduction of search to large collections of primary texts, and explores its effects. It did not come automatically or easily, but when it did search broke down barriers to access (no longer requiring background knowledge in history and bibliography), offering a new means of discovering and selecting texts to read. This change was more than a convenience. It was transformative. Database represents a new form of textuality, and scholars have come to rely on database's affordances to develop new ways of reading. In addition to content analysis of claims made about database in various fora, modest bibliometric analysis of two journals (American Literature and English Literary History) suggests trends: more and more diverse primary texts are being read and cited. Interviews with authors of journal articles and journal editors are used to characterize how the databases are used and the effect on scholarship.","Search, Reading, and the Rise of Database: Table 1",Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
595,http://dx.doi.org/10.1093/llc/fqw024,"{'string_id': '10.1093/llc/fqw024', 'id_scheme': 'DOI'}","Analytic interest in comics, graphic novels and similarly visual media is currently experiencing considerable growth. In order to pursue empirical investigation of such media, it is useful to explore how data of this kind can be made accessible for the application of established empirical methods, such as linguistic corpus analysis. Many forms of communication have already benefited from data-driven analytic procedures, and it is logical to consider how this might also be the case for visual media. However, comics and graphic novels raise some unique challenges for this endeavor because a substantial component of their communicative effect is achieved by variations in their visual appearance and spatial organization. Schemes capable of capturing the spatial organization of visual media are to date limited largely to geometric descriptions and so are of limited value for more interpretative analyses. In this article, we set out a detailed classification scheme for the visual appearance of comics, graphic novels, and similar media that focuses particularly on their spatial ‘layout’ to make this facet of their meaning accessible to corpus-based quantitative and qualitative analyses.",An Open Multilevel Classification Scheme for the Visual Layout of Comics and Graphic Novels: Motivation and Design,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
596,http://dx.doi.org/10.1093/llc/fqw025,"{'string_id': '10.1093/llc/fqw025', 'id_scheme': 'DOI'}","The aim of this paper is to present both the main parts of the designing and the implementation of a useful and user-friendly electronic tool, the Greek grammar checker. This tool carries out the function of analyzing morphologically and syntactically sentences, phrases, and words in order to correct syntactic, grammatical, and stylistic errors (Iordanidou, 1999, 2004). Our premise in order to deal with all these issues is the settings of Grammar (adaptation of Little Modern Grammar of Manolis Triantafyllidis), which is the formal grammatical codification of Modern Greek, since 1976 (Triantafyllidis, 1991). This paper also presents the formalism used (the Mnemosyne), a formalism that handles with the particularities of the Greek language that hinder the computational processing. This formalism has already been used to identify multi-word terms and to phrase grammars, aiming to automatically extract information. We tested the Greek grammar checker by giving texts that were to be evaluated both to the grammar checker and to a person. In the majority of cases, the human corrector accuracy is almost equal to the grammar checker one. As far as mistakes that have to do with the coherence of the text or with meaning are concerned, the human corrector was the only accurate corrector, not the grammar checker one (Gakis, 2015).",Design and construction of the Greek grammar checker,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
597,http://dx.doi.org/10.1093/llc/fqw027,"{'string_id': '10.1093/llc/fqw027', 'id_scheme': 'DOI'}","Variation among human translations is usually invisible, little understood, and under-valued. Previous statistical research finds that translations vary most where the source items are most semantically significant or express most ‘attitude’ (affect, evaluation, ideology). Understanding how and why translations vary is important for translator training and translation quality assessment, for cultural research, and for machine translation development. Our experimental project began with the intuition that quantitative variation in a corpus of historical retranslations might be used to project quasi-qualitative annotations onto the translated text. We present a web-based system which enables users to create parallel, segment-aligned multi-version corpora, and provides visual interfaces for exploring multiple translations, with their variation projected onto a base text. The system can support any corpus of variant versions. We report experiments using our tools (and stylometric analysis) to investigate a corpus of forty German versions of a work by Shakespeare. Initial findings lead to more questions than answers.","Multi-Retranslation Corpora: Visibility, Variation, Value, and Virtue",Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '1477-4615', 'type': 'electronic'}]"
598,http://dx.doi.org/10.1093/llc/fqw028,"{'string_id': '10.1093/llc/fqw028', 'id_scheme': 'DOI'}","This article presents a new theoretical framework for computer-assisted decipherment of ancient alphabetic inscriptions. This framework is based on regular expressions, a widely used computer science formalism for encoding text strings with partially unknown characters. We then present a new software called Scrypt, which applies our framework to the Khirbet Qeiyafa ostracon, an important Proto-Canaanite inscription recently discovered in Israel, as a first case study. Several new anthroponymic readings for the Qeiyafa ostracon, found with the help of our software, are presented as part of that case study. The software, freely available online (www.ScryptApp.com), enables users to encode all possible readings for a given grapheme in the ostracon and provides fast automated dictionary searches for lexemes.",Computer experiments on the Khirbet Qeiyafa ostracon,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
599,http://dx.doi.org/10.1093/llc/fqw029,"{'string_id': '10.1093/llc/fqw029', 'id_scheme': 'DOI'}","Why are Romeo and Juliet prominent characters in Shakespeare’s play of the same name? Contrary to what common sense might suggest, the academic literature does not provide a unique answer to this question. Indeed, there is little agreement on who the main character is and which elements of a script contribute to establishing a character’s leading role. The objective of this article is to explore and compare the prominence of characters in Romeo and Juliet by using social network analysis. To this end, we calculate the centralities of several characters in Romeo and Juliet using a method based on Social Network Analysis. Comparing the scores generated by this analysis, we found that Romeo’s centrality is more stable than Juliet’s while hers is lower and supported by the ‘strength of the bonds’ she develops with other characters. Thus, the comparison of different centrality rankings and clusters provides new knowledge about the plays of Shakespeare. We show that the ‘strength’ of the relationships affects the prominence of the characters. This finding opens new directions for analyzing Shakespeare’s scripts and determining who the main character is using weighted centrality measures. Finally, we discuss some theoretical and practical implications of the method used in this study.",Exploring the prominence ofRomeo and Juliet’s characters using weighted centrality measures,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
600,http://dx.doi.org/10.1093/llc/fqw031,"{'string_id': '10.1093/llc/fqw031', 'id_scheme': 'DOI'}","All-words sense tagging is the task of determining the correct senses of all content words in a given text. Many methods utilizing various language resources, such as a machine readable dictionary (MRD), sense tagged corpus, and WordNet, have been proposed for tagging senses to all words rather than a small number of sample words. However, sense tagging methods that require vast resources cannot be used for resource-deficient languages. The conventional sense tagging method for resource-deficient languages, which utilizes only an MRD, suffers from low recall and low precision because it determines senses only when a gloss word in the dictionary exactly matches a context word. In this study, we propose an all-words sense tagging method that is effective for resource-deficient languages in particular. It requires an MRD, which is the essential resource for all-words sense tagging, and a raw corpus, which is easily acquired and freely available. The proposed sense tagging method attempts to find semantically related context words based on the co-occurrence information extracted from the raw corpus and utilizes these words for tagging the senses of the target word. The experimental results of an evaluation of the proposed sense tagging algorithm on a Korean test corpus consisting of approximately 15 million words show that it can tag senses to all contents words automatically with high precision. Furthermore, we also show that a semantic concordancer can be developed based on the automatic sense tagged corpus.",An All-Words Sense Tagging Method for Resource-Deficient Languages,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
601,http://dx.doi.org/10.1093/llc/fqw035,"{'string_id': '10.1093/llc/fqw035', 'id_scheme': 'DOI'}","This article describes and demonstrates a named entity similarity metric developed for, and currently in use by, the FuzzyPhoto project. The presented metric is effective at comparing named entity data in and across syntaxless data schemas such as are often encountered in Gallery, Library, Archive, and Museum collections. The efficiency of the approach was compared to an existing named entity similarity metric and is shown to be a significant improvement when comparing messy named entity data.",An effective named entity similarity metric for comparing data from multiple sources with varying syntax,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
602,http://dx.doi.org/10.1093/llc/fqw036,"{'string_id': '10.1093/llc/fqw036', 'id_scheme': 'DOI'}","This study uses information gleaned from the front matter, or preliminaries, of Spanish Golden Age texts to model the social networks underpinning the early modern publication industry. Using a data-driven approach, we examine the historical and political conditions that influenced the process of approval, censorship, and publication in the Spanish Empire, with a particular focus on the concept of geography, as it relates to the process of community formation and composition. We find that the literary publishing scene was dominated by a small group of authors, generally tied to Madrid, but highly published across Iberian cultural and political capitals. These authors, together with the powerful literary patrons who they relied upon for support, served as local bridges between communities that formed primarily at the local level. Regionally, we find groups of literate bureaucrats, clergymen, printers, and booksellers working together to fulfill the legal requirements for publication as dictated by the Spanish crown. Finally, we see how certain individuals tend to stand out at the regional level as gatekeepers to the publication industry, interacting equally with high- and low-profile individuals to approve and publish texts.","The preliminaries project: Geography, networks, and publication in the Spanish Golden Age",Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
603,http://dx.doi.org/10.1093/llc/fqw037,"{'string_id': '10.1093/llc/fqw037', 'id_scheme': 'DOI'}","This article makes an argument for an open-workshop editorial process for scholarly editions, in which preliminary research products such as manuscript transcriptions are published to the Web, as they are completed. A publicly accessible online working environment for scholarly editions would allow research assistants to receive proper credit for their intellectual labor, and would reach out to the worldwide audience of scholars, students, and amateur enthusiasts, potentially building public support for a kind of scholarly activity that is threatened within the academy. Two ongoing projects in which the author is involved are used as illustrations of the way this can work, the Online Corpus of Old English Poetry and the Cotton Nero A.x. Project.",‘Why don’t we do it in the road?’: The case for scholarly editing as a public intellectual activity,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
604,http://dx.doi.org/10.1093/llc/fqw038,"{'string_id': '10.1093/llc/fqw038', 'id_scheme': 'DOI'}","Stemmatology aims at gaining understanding of the development and copying history of a textual tradition, based on the surviving witnesses of the text. Typically, this includes the task of identifying for each witness the source text from which it was copied. In a textual tradition, every copyist makes alterations, errors, and corrections (that can be either correct or not), which gradually mutate the contents of the text. When the extant versions are placed in a stemma, i.e. a graph representing the copying relationships, a scholar can attempt to reconstruct the earlier textual versions by reversing the process along the branches of the stemma. Anyone who has attempted such a task is well aware of the fact that it is subject to uncertainty arising from a number of sources, and...",Thematic Section on Studia Stemmatologica,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,31,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
605,http://dx.doi.org/10.1093/llc/fqw039,"{'string_id': '10.1093/llc/fqw039', 'id_scheme': 'DOI'}","Various features of R Stylo were applied to quartos Q1 (1600) and Q2 (1619) of Sir John Oldcastle, a play performed by the Lord Admiral’s Men in November 1599 and written by Anthony Munday, Michael Drayton, Robert Wilson, and Richard Hathwaye, according to Henslowe’s diary. Reference texts by Drayton and Hathwaye were not available, but those of Munday and Wilson did not surface stylistically anywhere in the texts. Instead, stylistic features of Shakespeare’s reference texts were abundant and explain why Q2 was subtitled ‘written by William Shakespeare’. Likewise, Dekker’s ‘additions’ were absent. As the Lord Chamberlain’s Men also performed a play that was referred to as Sir John Old Castell, the assumption is that Shakespeare’s play somehow crossed over to Henslowe’s theatrical company. Multiple approaches (rolling delta, rolling classify, bootstrap consensus tree, Craig’s zeta) all come to the same conclusions.",The Two Oldcastles of London,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
606,http://dx.doi.org/10.1093/llc/fqw041,"{'string_id': '10.1093/llc/fqw041', 'id_scheme': 'DOI'}","In recent years, public engagement is increasingly viewed as more than an ‘additional extra’ in academia. In the UK, it is becoming more common for research projects to embrace public engagement with the belief that it informs research, enhances teaching and learning, and increases research impact on society. Therefore, it is becoming increasingly important to consider ways of incorporating public engagement activities into digital humanities research. This article discusses public engagement and digital humanities in practice, highlighting how museums are utilizing digital technology to engage the public. This article describes the development and presents the results of a case study: The QRator project, an application for digital interpretation in the museum and cultural heritage sector. The QRator project took an innovative, multidisciplinary approach to creating new ways for museum visitors to engage with museum objects and discussions. The objective was to understand how digital technologies, such as interactive labels and smartphones, create new ways for users to engage with museum objects; investigate the value and constraints of digital sources and methods involving cultural content; and demonstrate how crowdsourced digital interpretation may be utilized as a research source. This article will use the QRator project as a case study to explore how mobile devices and interactive digital labels can create new models for public engagement, visitor meaning-making (Silverman, L. H. Visitor meaning-making in museums for a new age. Curator, 1995;38(3):161–70), and the construction of multiple interpretations inside museum spaces. This article will also put emphasis on how public engagement can and should be a core consideration of digital humanities projects.",Engaging the museum space: Mobilizing visitor engagement with digital content creation,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
607,http://dx.doi.org/10.1093/llc/fqw042,"{'string_id': '10.1093/llc/fqw042', 'id_scheme': 'DOI'}","Machine translation (MT) has become increasingly important and popular in the past decade, leading to the development of MT evaluation metrics aiming at automatically assessing MT output. Most of these metrics use reference translations to compare systems output, therefore, they should not only detect MT errors but also be able to identify correct equivalent expressions so as not to penalize them when those are not displayed in the reference translations. With the aim of improving MT evaluation metrics a study has been carried out of a wide panorama of linguistic features and their implications. For that purpose a Spanish and an English corpora containing hypothesis and reference translations have been analysed from a linguistic point of view, so that common errors can be detected and positive equivalencies highlighted. This article focuses on this qualitative analysis describing the linguistic phenomena that should be considered when developing an automatic MT evaluation metric. The results of this analysis have been used to develop an automatic MT evaluation metric that takes into account different dimensions of language. A brief review of the metric and its evaluation are also provided.",Guiding automatic MT evaluation by means of linguistic features,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
608,http://dx.doi.org/10.1093/llc/fqw043,"{'string_id': '10.1093/llc/fqw043', 'id_scheme': 'DOI'}","Recent attempts to computer-model the scholarly edition so as to permit the crowd-sourcing of its production have misunderstood its nature. Scholarly editions are, in their methodology and form, not unchanging, nor are their underlying conceptions simple. This essay is, in response, a reflection on the opportunities that the digital form potentially offers editors about how they may gain traction by taking advantage of the capacities and logic of the new medium. The main proposal stems from leaving the representational question on hold (how the edition represents the work and the methodologies used to achieve that) and instead considering the edition primarily as a transaction with its readers—those print-counterparts of the digital crowd. The history of post-war scholarly editions is reviewed for its evolving understandings of the reader-user. Then a conceptual separation between the archive and the edition is proposed so that a new, more reader-oriented definition of editorial responsibilities can be envisaged for digital scholarly editions—something that the logic of the print medium forbade.",The reader-oriented scholarly edition,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
609,http://dx.doi.org/10.1093/llc/fqw044,"{'string_id': '10.1093/llc/fqw044', 'id_scheme': 'DOI'}","If Scholarly Editing means the exercise of textual criticism for the production of digital archives and editions, then crowdsourcing may produce more problems than solutions because a digital archive is a surrogate for material documents and a digital scholarly edition is a precise argument about the archive. But the analysis of the materials in the archive is truly a crowdsourcing task, for which adequate software design is still wanted and careful monitoring and vetting required.",Reliable social scholarly editing,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
610,http://dx.doi.org/10.1093/llc/fqw045,"{'string_id': '10.1093/llc/fqw045', 'id_scheme': 'DOI'}","In this paper we focus on modelling as a creative process to gain new knowledge about material and immaterial objects by generating and manipulating external representations of them. We aim at enriching the current theoretical understanding by contextualising digital humanities practices within a semiotic conceptualisation of modelling. A semiotic approach enables us to contextualise modelling in a scholarly framework well suited to humanistic enquiries, forcing us to investigate how models function as signs within specific contexts of production and use. Kralemann and Lattmann’s semiotic model of modelling complemented by Elleström's theories on iconicity are some of the tools we use to inform this semiotic perspective on modelling. We contextualise Kralemann and Lattmann’s theory within modelling practices in digital humanities by using three examples of models representing components and structure of historical artefacts. We show how their model of models can be used to understand and contextualise the models we study and how their classification of model types clarify important aspects of digital humanities modelling practice.",Modelling in digital humanities: Signs in context,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
611,http://dx.doi.org/10.1093/llc/fqw046,"{'string_id': '10.1093/llc/fqw046', 'id_scheme': 'DOI'}","Zipf’s law is an important linguistics law. In 1940s, Zipf found power law in the distribution of the word frequencies. The word rankings are in descending order of their occurrence frequencies. Through a series of experiments of random input letter sequences, the results show that the power law distribution has existed in not only natural language utterances but also random symbolic sequences.",Pow law in random symbolic sequences,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
612,http://dx.doi.org/10.1093/llc/fqw047,"{'string_id': '10.1093/llc/fqw047', 'id_scheme': 'DOI'}","In this article we discuss the heuristic capabilities that the process of generating, processing, and integrating cultural heritage linked data may afford, including its potential for enhancing arts and humanities research. More specifically, we report on our current work on detecting and assigning gender properties to person entities and semantically enriching a set of Linked Open Data (LOD) in the domain of history of jazz. Linked Jazz—an ongoing project that experiments with the application of LOD principles and techniques to cultural heritage materials—provided the context for this research. Linked Jazz aims to uncover meaningful connections between data and documents from digital archives of jazz history. It employs oral histories as the main source of named entities to be represented as linked data. The entities are then semantically connected and visualized as social graphs. Using the assignment of gender properties, this article describes how the data development process itself offers new and unanticipated paths of research inquiry and engagement with heritage data.","Accidental discovery, intentional inquiry: Leveraging linked data to uncover the women of jazz",Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
613,http://dx.doi.org/10.1093/llc/fqw048,"{'string_id': '10.1093/llc/fqw048', 'id_scheme': 'DOI'}","The adoption of XML and encoding standards such as those developed by the Text Encoding Initiative was accompanied by expectations of easy interoperability which are now widely seen as unfulfilled. The related but distinct concept of ‘interchange’ has received much less attention. This article argues that, particularly for sophisticated digital edition projects using XML, interchange is a more practical goal, and that approached in a specific way, it is highly beneficial not only to potential end users of the project’s data but also to the project itself.The article illustrates specific strategies and approaches to enabling and facilitating interchange, using work undertaken on the Map of Early Modern London (MoEML) project as a case study.",Whatever happened to interchange?,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
614,http://dx.doi.org/10.1093/llc/fqw049,"{'string_id': '10.1093/llc/fqw049', 'id_scheme': 'DOI'}","In 2011, six academics gathered over 90,000 authentic text messages (SMS) in French from the general public, in compliance with French law (http://sud4science.org,Panckhurst et al., 2013). The SMS ‘donors’ were also invited to fill out a sociolinguistic questionnaire (see Figure A1, Moïse, 2013, Panckhurst and Moïse, 2014). The ‘sud4science’ project is part of a vast international initiative, entitled ‘sms4science’ (http://www.sms4science.org/, Fairon et al., 2006, Cougnon and Fairon, 2014, Cougnon, 2015), which aims to build a worldwide database and analyse authentic text messages in different languages. After the ‘sud4science’ SMS data collection, a pre-processing phase of checking and eliminating any spurious information and a three-step semi-automatic anonymization phase were conducted (Accorsi et al., 2014, Patel et al., 2013). Two extracts were transcoded into standardized French (1,000 SMS) and annotated (100 SMS). The finalized digital resource of 88,000 anonymized French text messages, the ‘88milSMS’ corpus, the extracts and the sociolinguistic questionnaire data are currently available for all to download, from the Huma-Num web service (http://88milsms.huma-num.fr, Panckhurst et al., 2014). The 88milSMS corpus has also recently become available via a Creative Commons Attribution 4.0 International licence on the ‘Ortolang’ platform (https://hdl.handle.net/11403/comere/cmr-88milsms/cmr-88milsms-tei-v1, Panckhurst et al., in Chanier (ed), 2016). In this paper, first the authors briefly situate the project and describe the anonymization process. Then, they focus on why they decided to exclude full ‘transcoding’ and linguistic annotation in the first version of the final corpus.",A digital corpus resource of authentic anonymized French text messages: 88milSMS—What about transcoding and linguistic annotation?,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
615,http://dx.doi.org/10.1093/llc/fqw050,"{'string_id': '10.1093/llc/fqw050', 'id_scheme': 'DOI'}","Text mining and information visualization techniques applied to large-scale historical and literary document collections have enabled new types of humanities research. The assumption behind such efforts is often that trends will emerge from the analysis despite errors for individual data points and that noise will be dominated by the signal in the data. However, for some text analysis tasks, the technology is unable to perform as well as domain experts, perhaps because it does not have sufficient world knowledge or metadata available. Yet, the advantage of language processing technology is that it can process at scale, even if not perfectly accurately. Geo-locating literary works is one example where human expert knowledge is invaluable when it comes to distinguishing between candidate works. This was the underlying assumption in Palimpsest, an interdisciplinary digital humanities research project on mining literary Edinburgh. From the outset, the project adopted an assisted curation process whereby the automatic processing of large data collections was combined with manual checking to identify literary works set in Edinburgh. In this article, we introduce the assisted curation process and evaluate how the feedback from literary scholars helped to improve the technology, thereby highlighting the importance of placing humanities research at the core of digital humanities projects.",Palimpsest: Improving assisted curation of loco-specific literature,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
616,http://dx.doi.org/10.1093/llc/fqw051,"{'string_id': '10.1093/llc/fqw051', 'id_scheme': 'DOI'}","In this article we present a web service framework providing automatic document processing methods to the public. Furthermore, an assessment environment and sample applications using this framework are briefly described. Research on Document Image Analysis (DIA) focuses mainly on developing and refining automatic processing steps, e.g. text line extraction, binarization, and layout analysis. While many state-of-the-art methods perform satisfactorily, the algorithms applied to obtain the results are not easily accessible for other researchers. Making the source code available is often not sufficient as it typically requires a cumbersome installation of required libraries and reading long manuals about the usage. We present a new approach for making methods available to researchers in the digital humanities without detailed knowledge of the algorithms.For our approach we propose a RESTful web service architecture, the current state of the art in online web communication. For a developer this reduces the steps needed to access a method to sending and receiving HTTP requests with Java Script Object Notification data, removing all installation steps. We will build on standards such as the Text Encoding Initiative and the International Image Interoperability Framework. Thus, methods hosted on DivaServices can be integrated easily into document processing workflows by any software engineer in computer science, but also the digital humanities without specific knowledge of the mathematical and algorithmic details of DIA.",DivaServices—A RESTful web service for Document Image Analysis methods,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
617,http://dx.doi.org/10.1093/llc/fqw052,"{'string_id': '10.1093/llc/fqw052', 'id_scheme': 'DOI'}",This article presents a study of Leo Tolstoy’s War and Peace by means of automatic syntactic and semantic analysis. Using a parser that extracts syntactic dependencies and semantic roles we were able to compare different characters of the novel in terms of the semantic roles they tend to occupy. Our data show that there are certain dependencies between the apparent personal traits of a character and his or her positions within the predicate structures. We hope that further research will help us gain more insights into the ‘literary technique’ of Tolstoy and enable us to create a semantic mark-up of his works.,Text miningWar and Peace: Automatic extraction of character traits from literary pieces,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
618,http://dx.doi.org/10.1093/llc/fqw053,"{'string_id': '10.1093/llc/fqw053', 'id_scheme': 'DOI'}","This article examines the question of whether Digital Humanities has given too much focus to text over non-text media and provides four major reasons to encourage more non-text-focused research under the umbrella of Digital Humanities. How could Digital Humanities engage in more humanities-oriented rhetorical and critical visualization, and not only in the development of scientific visualization and information visualization?","Digital humanities is text heavy, visualization light, and simulation poor",Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
619,http://dx.doi.org/10.1093/llc/fqw054,"{'string_id': '10.1093/llc/fqw054', 'id_scheme': 'DOI'}","Traduco is a web-based collaborative tool aimed at supporting the translation of texts that pose particular challenging interpretative issues. Nowadays, Computer-Assisted Translation (CAT) tools are mainly applied to the translation of technical manuals or legislative texts and are aimed at speeding up the translation process. Traduco extends most of the standard components of a traditional CAT tool with specific features necessary to support the interpretation and translation of complex texts (like the Babylonian Talmud, that we here present as a case study), which pose particular comprehension issues. Traduco goes beyond the translation and its printing: it includes features for the addition of notes and annotations and the creation of glossaries. Translators, editors, supervisors, and end-users accessing Traduco are able to use components that can ease the translation process through the use of CAT technologies, the supervision and managing of the whole process of translation and publishing, the exporting of translations and notes in standard formats for desktop publishing software and TEI format, and, soon, the possibility to perform automatic linguistic analysis of the text. Moreover, Traduco allows the users to insert notes, comments, annotations, and bibliographical references. The design and development of Traduco required the adoption of a multidisciplinary approach, leveraging on advances in software engineering, computational linguistics, knowledge engineering, and publishing.",Traduco: A collaborative web-based CAT environment for the interpretation and translation of texts,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
620,http://dx.doi.org/10.1093/llc/fqw056,"{'string_id': '10.1093/llc/fqw056', 'id_scheme': 'DOI'}","This essay considers both the promise and perils of ""social editing,"" a term strongly associated with user-generated content. The idea of user-contributed content has been greeted with enthusiasm in some quarters and with skepticism and anxiety in others. We can learn from the crowdsourcing efforts undertaken thus far and can glimpse some of the new possibilities on the horizon. To what extent might users of electronic editions help projects such as the Walt Whitman Archive address the extensive and costly work that stands in the way of the realization of a digital scholarly edition? How can the Archive best negotiate the roles of scholarly specialists and interested users, and in particular, how can quality control be established without discouraging user involvement?",The Walt Whitman Archiveand the prospects for social editing,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
621,http://dx.doi.org/10.1093/llc/fqw057,"{'string_id': '10.1093/llc/fqw057', 'id_scheme': 'DOI'}","The Great Parchment Book of the Honourable the Irish Society is a major surviving historical record of the estates of the county of Londonderry (in modern day Northern Ireland). It contains key data about landholding and population in the Irish province of Ulster and the city of Londonderry and its environs in the mid-17th century, at a time of social, religious, and political upheaval. Compiled in 1639, it was severely damaged in a fire in 1786, and due to the fragile state of the parchment, its contents have been mostly inaccessible since. We describe here a long-term, interdisciplinary, international partnership involving conservators, archivists, computer scientists, and digital humanists that developed a low-cost pipeline for conserving, digitizing, 3D-reconstructing, and virtually flattening the fire-damaged, buckled parchment, enabling new readings and understanding of the text to be created. For the first time, this article presents a complete overview of the project, detailing the conservation, digital acquisition, and digital reconstruction methods used, resulting in a new transcription and digital edition of the text in time for the 400th anniversary celebrations of the building of Londonderry’s city walls in 2013. We concentrate on the digital reconstruction pipeline that will be of interest to custodians of similarly fire-damaged historical parchment, whilst highlighting how working together on this project has produced an online resource that has focussed community reflection upon an important, but previously inaccessible, historical text.",Digitally reconstructing the Great Parchment Book: 3D recovery of fire-damaged historical documents,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
622,http://dx.doi.org/10.1093/llc/fqw059,"{'string_id': '10.1093/llc/fqw059', 'id_scheme': 'DOI'}","It is widely believed that different parts of a classical Chinese poem vary in syntactic properties. The middle part is usually parallel, i.e. the two lines in a couplet have similar sentence structure and part of speech; in contrast, the beginning and final parts tend to be non-parallel. Imagistic language, dominated by noun phrases evoking images, is concentrated in the middle; propositional language, with more complex grammatical structures, is more often found at the end. We present the first quantitative analysis on these linguistic phenomena—syntactic parallelism, imagistic language, and propositional language—on a treebank of selected poems from the Complete Tang Poems. Written during the Tang Dynasty between the 7th and 9th centuries CE, these poems are often considered the pinnacle of classical Chinese poetry. Our analysis affirms the traditional observation that the final couplet is rarely parallel; the middle couplets are more frequently parallel, especially at the phrase rather than the word level. Further, the final couplet more often takes a non-declarative mood, uses function words, and adopts propositional language. In contrast, the beginning and middle couplets employ more content words and tend toward imagistic language.",Syntactic patterns in classical Chinese poems: A quantitative study,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
623,http://dx.doi.org/10.1093/llc/fqw060,"{'string_id': '10.1093/llc/fqw060', 'id_scheme': 'DOI'}","At certain points in history, certain words take on a positive aura that makes it difficult to openly express dissenting or sceptical views about the objects, processes, or qualities they denote. Right now, social has this aura. This word’s role as a modifier to make the noun after it refer to society and other kinds of human association—as in social law and social life—emerged at the end of the sixteenth century (OED ‘social’ adj. 5a, 5b). Most recently, the word has attached itself to a relatively new word, media—first used to denote mass communication in 1927 (OED ‘media’ n.2)—to denote a new kind of technology of communication. Whereas the ordinary media provided only one-way, one-to-many, communication, the social media allow ‘users to create and share...",Afterword,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,31,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
624,http://dx.doi.org/10.1093/llc/fqw061,"{'string_id': '10.1093/llc/fqw061', 'id_scheme': 'DOI'}","Digitizing Lefebvre’s Spatial Triad is conceived as a seed project for an interdisciplinary analysis of the built environment via digital media. Two social housing projects in İzmir are chosen as case studies as an initial step to be developed toward a potentially international digital platform. The theoretical premises of the project are based on the renowned cultural theorist Henri Lefebvre’s Spatial Triad, which distinguishes between representations of space, representational spaces or spaces of representation, and spatial practices. Following this framework, the collected data are organized in three sections, which are reflected in the digital interface. These are respectively titled, ‘implementations’, which contains architectural drawings and visual recordings of interviews with the chief architect of the projects; ‘perceptions’, which includes related texts that are scanned from Web sites, newspapers, journals, and conference proceedings; and ‘lived experiences’, which contains photographs and visual records of on-site interviews with the users of the two housing estates. Users of the digital interface are enabled access to data in each category by means of choosing one of eighty-three related keywords. The latter are derived from the digital analyses of discursive material. By enabling the comparison of the sections of spatial data for each settlement and between the two settlements, the digital platform has the potential to inform decision-making processes in future social housing projects.",Digitizing Lefebvre’s Spatial Triad,Oxford University Press (OUP),2016,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
625,http://dx.doi.org/10.1093/llc/fqw062,"{'string_id': '10.1093/llc/fqw062', 'id_scheme': 'DOI'}","AustLit is a major Australian cultural heritage database and the most comprehensive record of a nation’s literary history in the world. In this article we will present the successful results of a project addressing the challenge of discovering and recording creative writing published in digitized historical Australian newspapers, provided by the National Library of Australia’s Trove service. As a first step in identifying creative writing, we developed an automated method for identifying articles that are likely to be poems by searching for a number of signals embedded in articles. When this work began, AustLit contained more than 10,200 bibliographical records for poems published between 1803 and 1954 (75% prior to 1900) with links to the full text in 115 different newspapers. The aim of the project was to expand this number of bibliographical records in AustLit and provide a foundation for analysing the importance of poetry in newspaper publishing of the period. Taking advantage of Ted Underwood’s (Getting Everything you Want from HathiTrust http://tedunderwood.com/2012/07/27/getting-everything-you-want-from-hathitrust/, and Open Data (http://tedunderwood.com/open-data/): The Stone and the Shell, Underwood blog posts (Both accessed 27 October 2015), 2012) work with seventeenth- and eighteenth-century full text in the HathiTrust collection, we trained a naive Bayesian classifier, modifying code from Daniel Shiffman (Bayesian Filtering. http://shiffman.net/teaching/a2z_2008/bayesian/ (accessed 27 October 2015), 2008) and Paul Graham (A Plan for Spam. http://www.paulgraham.com/spam.html (accessed 27 October 2015), 2002) and improving the quality of Optical Character Recognition (OCR) by using the overProof correction algorithm. We have been able to successfully identify large numbers of poems in the newspapers database, greatly expanding AustLit’s coverage of this important literary form. After suitable training of the classifier, we were able to successfully identify 88% of the newspaper articles that a knowledgeable human would classify as ‘poetry’. Our results have encouraged us to consider enhancing and extending the techniques to aid the identification of other forms of literature and criticism.",‘Searching for My Lady’s Bonnet: discovering poetry in the National Library of Australia’s newspapers database’,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
626,http://dx.doi.org/10.1093/llc/fqw063,"{'string_id': '10.1093/llc/fqw063', 'id_scheme': 'DOI'}","This article describes the research and development (R&D) work done as an extension to the multilingual cross-domain client application prototype for UNL-ization and NL-ization for natural language processing (NLP) application developed by Agarwal and Kumar (A multilingual cross-domain client application prototype for UNL-ization and NL-ization for NLP applications. Digital Scholarship in the Humanities, 2016)1. A common platform has been developed and made live for worldwide users where they can share their Interactive Analyzer and EUGENE (dEep-to-sUrface GENErator) resources and exploit already shared resources by other users for UNL-ization and NL-ization. This article also highlights that how the existing platform can be used by other researchers and developers to make their UNL-based Web applications language-independent and can make it available to computational linguists, researchers, developers, and general audience globally. Such kind of platform is definitely useful for all UNL-based R&D activities being done throughout the geography. The platform gives 100% accuracy. However, the correctness depends on the F-measure of the respective Analysis and Generation module being used.",A public platform for developing language-independent applications,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
627,http://dx.doi.org/10.1093/llc/fqw064,"{'string_id': '10.1093/llc/fqw064', 'id_scheme': 'DOI'}","We present a process for cost-effective transcription of cursive handwritten text images that has been tested on a 1,000-page 17th-century book about botanical species. The process comprised two main tasks, namely: (1) preprocessing: page layout analysis, text line detection, and extraction; and (2) transcription of the extracted text line images. Both tasks were carried out with semiautomatic procedures, aimed at incrementally minimizing user correction effort, by means of computer-assisted line detection and interactive handwritten text recognition technologies. The contribution derived from this work is three-fold. First, we provide a detailed human-supervised transcription of a relatively large historical handwritten book, ready to be searchable, indexable, and accessible to cultural heritage scholars as well as the general public. Second, we have conducted the first longitudinal study to date on interactive handwriting text recognition, for which we provide a very comprehensive user assessment of the real-world performance of the technologies involved in this work. Third, as a result of this process, we have produced a detailed transcription and document layout information (i.e. high-quality labeled data) ready to be used by researchers working on automated technologies for document analysis and recognition.",Transcribing a 17th-century botanical manuscript: Longitudinal evaluation of document layout detection and interactive transcription,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
628,http://dx.doi.org/10.1093/llc/fqx001,"{'string_id': '10.1093/llc/fqx001', 'id_scheme': 'DOI'}","Translating poetry is a very complex process. The paradoxical nature of untranslatability and translatability of poetry has been noticed by Hai An (The translation of poetry by the translator-cum-poet. Chinese Translator Journals 2005; 6: 27–30), citing two famous scholars who are holding totally different opinions toward poetry translation. Robert Frost purports that ‘poetry is what gets lost in translation’, and Susan Bassnet advocates ‘poetry is what we gain in translation’. However, the common ground between these two drastic opinions is that poetry translation is no more a repetition of the original works than a reproduction. There are both similarities and discrepancies between the translated works and the original pieces, or in another word: ‘harmony in diversity’. This study aims to testify the above-mentioned proposal in a clear and objective manner, by comparing the original poetry texts (twenty randomly selected poems from Shakespearean sonnets) with their translated versions (the corresponding Chinese-translated versions by four different translators) from the perspective of vocabulary, word frequency distribution and part-of-speech (POS) frequency distribution. The results have corroborated the previous proposal: first, there is no significant difference in terms of vocabulary size and the text management styles between the translated poems and the original ones. Second, there is a significant difference in the word frequency distribution and POS frequency distribution between translated poems and the original ones. Third, there are also differences in the POS frequency distribution in poems translated by different authors. Furthermore, the translation style could distinguish professional translators from professional poets.",Harmony in diversity: The language codes in English–Chinese poetry translation,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
629,http://dx.doi.org/10.1093/llc/fqx002,"{'string_id': '10.1093/llc/fqx002', 'id_scheme': 'DOI'}","A mediator invited 101 academics from around the world to work together by email, over three rounds, to author a manuscript, in an attempt to establish the first ‘crowd-authored’ paper. Once the paper was finalized, it was submitted for publication to fifty-one accredited journals. However, the journals rejected the paper. The current article offers a critique of this negative experience, in reference to previous research and in consultation with the 101 authors. This critique highlights possible factors that may encourage journals to decline manuscripts authored by a large number of people. An awareness of such possible factors would be beneficial for other academics undertaking crowd-authoring projects. A main contribution of the present article is that it provides a debate about the cultural and political ramifications of crowd-authoring, a phenomenon that is expected to soon enter the academic discourse.",Crowd-authoring versus peer-reviewing: An epistemic clash in the field of educational technology,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
630,http://dx.doi.org/10.1093/llc/fqx003,"{'string_id': '10.1093/llc/fqx003', 'id_scheme': 'DOI'}","English historical linguists have often complained about the scholarly neglect of the phonology of the Late Modern English period; yet, the value of pronouncing dictionaries as rich and reliable evidence has been demonstrated (Beal, J. C., 1999, English Pronunciation in the Eighteenth Century: Thomas Spence’s Grand Repository of the English Language (1775). Oxford: Clarendon Press; Jones, C., 2006, English Pronunciation in the Eighteenth and Nineteenth Centuries. Basingstoke: Palgrave Macmillan). This article presents a new electronic, searchable database of ‘Eighteenth-Century English Phonology’ (ECEP) which aims to facilitate research on the social, regional, and lexical distribution of phonological variants in 18th-century English, as documented in contemporary pronouncing dictionaries. Taking Wells’ (1982, Accents of English. Cambridge: Cambridge University Press) lexical sets for the vowel system of present-day varieties of English as its reference, the database provides unicode IPA transcriptions for the relevant segment of each word given as an example of lexical (sub)set in his account of standard lexical sets, to which we have added some complementary consonant sets. These will be of use for comparative studies with 19th-century and present-day English. First, we describe the methodology and contents of ECEP: primary source selection, data input and annotation, the web-based interface. Second, we report on two case studies that demonstrate the value of evidence that can be systematically extracted from ECEP for the analysis of segmental and suprasegmental phonology; these are variations in the pronunciation of ‘wh’ in the set whale (/hw/∼/w/∼/h/), and the palatalization of alveolar consonants before /uː/. Thus, this article will demonstrate the viability of ECEP for historical phonology, dialectology, and sociolinguistics, and will help to promote the use of databases as key resources in historical linguistics.",‘Proper’ pro-nun-ʃha-ʃhun1 in Eighteenth-Century English: ECEP as a New Tool for the Study of Historical Phonology and Dialectology,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
631,http://dx.doi.org/10.1093/llc/fqx004,"{'string_id': '10.1093/llc/fqx004', 'id_scheme': 'DOI'}","This article explains some aspects of the verbal polysemy of Old English by means of the concept of semantic pole, an area of semantic space that represents a core meaning. It draws on the semantic primes of the natural semantic metalanguage, the theoretical constructs of semantic space, and force dynamics as well as semantic maps based on graph theory. In the semantic map, graphs link poles to definiens and lexical nodes, in such a way that the centrality of the poles is indicated, quantitatively, by the number of edges and, qualitatively, by the distance between the pole that exerts the centrifugal force and the pole to which such a force is directed. The conclusion is reached that the semantic poles MOVE, BE, and SAY constitute the core of the verbal lexicon of Old English, considering the semantic space that they occupy and the centrifugal and centripetal forces that produce polysemy originating in these semantic poles.",The semantic poles of Old English: Toward the 3D representation of complex polysemy,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
632,http://dx.doi.org/10.1093/llc/fqx005,"{'string_id': '10.1093/llc/fqx005', 'id_scheme': 'DOI'}","This article highlights shared methods, questions, and challenges between Research Through Design (RtD) and Digital Humanities (DH) through the discussion of an archival research project. In DH, debates continue e.g. in (Gold, Debates in the Digital Humanities. University of Minnesota Press, 2012) regarding the impact of digital technologies on epistemology, methodology, and our professional identities as researchers, scholars, academics, and teachers. Our reading of this debate is that there is a tripartite relationship between the kind of work we should call DH (and aspire to produce), the nature of DH knowledge, research and scholarship (particularly regarding the role of artefacts produced), and issues of disciplinary orientation or professional identity. We could phrase these as the what, how, and who of DH and, of course, RtD. The discussion of our project is in no sense intended to provide an exclusive answer to those questions, but to give one snapshot of what DH and RtD may look like when they come together. We emphasize that this relationship can and will be productive for both disciplines and point to the lack of significant discussion hereto.","Research through design and digital humanities in practice: What, how and who in an archive research project",Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
633,http://dx.doi.org/10.1093/llc/fqx006,"{'string_id': '10.1093/llc/fqx006', 'id_scheme': 'DOI'}","What is the scholarly nature of code and how do we evaluate the scholarship involved with coding? Our claim is that the humanities need an urgent answer to these questions given the increasing softwarization of both society and scholarship that pushes the boundaries of the methods and objects of study of the humanities. We argue that, as a result, there is a need to develop code criticism as a critical and reflexive tool within the humanities. Code criticism is described and positioned with respect to critical code studies, textual criticism, literary criticism, tool, and interface critique. Finally we outline an approach to code criticism based on ideas of reciprocal inquiry and of a continuum of literacies that connects code, code criticism, textual criticism, and literature.","Code, scholarship, and criticism: When is code scholarship and when is it not?",Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
634,http://dx.doi.org/10.1093/llc/fqx007,"{'string_id': '10.1093/llc/fqx007', 'id_scheme': 'DOI'}","This present article examines the verbal style and rhetoric of the candidates of the 2016 US presidential primary elections. To achieve this objective, this study analyzes the oral communication forms used by the candidates during the TV debates. When considering the most frequent lemmas, the candidates can be split into two groups, one using more frequently the pronoun ‘I’, and the second favoring more the ‘we’ (which corresponds to candidates leaving the presidential run sooner). According to several overall stylistic indicators, candidate Trump clearly adopted a simple and direct communication style, avoiding complex formulation and vocabulary. From a topical perspective, our analysis generates a map showing the affinities between candidates. This investigation results in the presence of three distinct groups of candidates, the first one with the Democrats (Clinton, O’Malley, and Sanders), the second with three Republicans (Bush, Cruz, Rubio), and the last with the duo Trump and Kasich, with, at a small distance, Paul. The over-used terms and typical sentences associated with each candidate reveal their specific topics such as ‘simple flat tax’ for Cruz, ‘balanced budget’ for Kasich, negativity with Trump, or critiques against large corporations and Wall Street for Sanders.",Analysis of the style and the rhetoric of the 2016 US presidential primaries,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
635,http://dx.doi.org/10.1093/llc/fqx008,"{'string_id': '10.1093/llc/fqx008', 'id_scheme': 'DOI'}","Universities around the world have increasingly turned to digital infrastructures as a way to revamp the arts and humanities. This article contributes a fresh understanding by examining the material development of HumlabX, a research laboratory for digital humanities at Umeå University, Sweden. Specifically, we approach the empirical case as a timeline of research funding, projects, events, and deliverables to examine how the research laboratory as an organizational and material space developed and evolved in relation to new technology investments. Based on our analysis, we argue that while digital research infrastructures can, indeed, stimulate innovation in and around research, aimed to produce new knowledge, digital technologies carry social and material implications that affect organizational processes. We show that while knowledge production processes at HumlabX were highly influenced by the infrastructural legacy of the past, they indeed directed scholars toward innovation. By discussing these implications in detail, we move beyond the debate of humanities qua digital, and demonstrate the need for scholars of digital humanities to engage in the development of policies for digital research infrastructures. Using a Swedish case study, we argue that research laboratories for the digital humanities must be scrutinized and should be fully exposed as socio-material organizations that develop, and should develop, over time. In particular, we stress the need to ensure that digital humanities laboratories are sustainable and open for redevelopment.",Beyond humanities qua digital: Spatial and material development for digital research infrastructures in HumlabX1,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
636,http://dx.doi.org/10.1093/llc/fqx009,"{'string_id': '10.1093/llc/fqx009', 'id_scheme': 'DOI'}","In this article an automatic scansion model for fixed-metre Spanish poetry is presented. It is a hybrid model that combines hand-made rules with probabilistic information. Through the set of rules, the model is able to extract the syllabic structure of each word, to classify them as stressed or unstressed and to resolve metrical phenomena such as synaloephas or diaereses. The article is mainly focused on the metrical ambiguities produced by synaloephas: verse lines from which it is possible to derive two or more metrical patterns. This metrical ambiguity is resolved through probabilities, assuming a relation between high probabilities and metricality. The system has been evaluated through more than 1,000 lines extracted from a corpus of Golden-Age Spanish sonnets. An accuracy of 95% has been achieved, resulting in not only considerable progress if we compare it to previous proposals, but also in an adequate way of performing the task when compared to human performance.",A metrical scansion system for fixed-metre Spanish poetry,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
637,http://dx.doi.org/10.1093/llc/fqx010,"{'string_id': '10.1093/llc/fqx010', 'id_scheme': 'DOI'}","The point of departure for this article is the Renderings project (http://trope-tank.mit.edu/renderings/) established in 2014 and developed at the Massachusetts Institute of Technology in a laboratory called the Trope Tank. The goal of the project is to translate highly computational and otherwise unusual digital literature into English. Translating digital works that are implemented as computer programs presents new challenges that go beyond the already difficult ones tackled by translators of more typical forms of literature. It is a type of translation akin to the translation of experimental, conceptual, or constrained works. It is not unusual for this task to require the translator or translators to reinvent the work in a new linguistic and cultural context, and sometimes also to port the original program to another programming language. This article describes an undertaking related to the broadly understood discipline of creative computing and studies the work of the translator as taking place both in code and language, drawing from the methodologies developed by the fields of code studies, platform studies, and expressive processing.",Renderings: Translating literary works in the digital age,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
638,http://dx.doi.org/10.1093/llc/fqx011,"{'string_id': '10.1093/llc/fqx011', 'id_scheme': 'DOI'}","The identification of pseudepigraphic texts—texts not written by the authors to which they are attributed—has important historical, forensic, and commercial applications. Any method for identifying such pseudepigrapha must ultimately depend on some measure of a given document’s similarity to the other documents in a corpus. We show that for this purpose, second-order document similarity measures taken from the authorship verification literature strongly outperform standard document similarity measures commonly used for outlier identification. We apply these improved methods to two famous corpora suspected of including pseudepigrapha: Shakespeare’s plays and Pauline epistles.",Detecting pseudepigraphic texts using novel similarity measures,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
639,http://dx.doi.org/10.1093/llc/fqx012,"{'string_id': '10.1093/llc/fqx012', 'id_scheme': 'DOI'}","Historical documents usually have a complex layout, making them one of the most challenging types of documents for automatic image analysis. In the pipeline of automatic document image analysis (DIA), layout analysis is an important prerequisite for further steps including optical character recognition, script analysis, and image recognition. It aims at splitting a document image into regions of interest such as text lines, background, and decorations. To train a layout analysis system, an essential prerequisite is a set of pages with corresponding ground truth (GT), i.e. existing labels (e.g. text line and decoration) annotated by human experts. Although there exist many methods and tools in GT generation, most of them are not suitable on our specific data sets. In this article, we propose to use Gabor features to generate GT, and based on Gabor features, we developed a web-based interface called DivaDiaWI. DivaDiaWI applies automatic functions using Gabor features to generate GT of text lines. For other region types such as background and decorations, users can manually draw their GT with user-friendly operations. The evaluation shows that (1) DivaDiaWI has two advantages when bringing it into context with state-of-the-art tools, (2) the automatic functions of DivaDiaWI greatly accelerate the GT generation, and (3) DivaDiaWI obtains a high score in a system usability test.",The use of Gabor features for semi-automatically generated polyon-based ground truth of historical document images,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
640,http://dx.doi.org/10.1093/llc/fqx013,"{'string_id': '10.1093/llc/fqx013', 'id_scheme': 'DOI'}","This article deals with Rolling Delta and Rolling Classify authorship attributions in the apocryphal play Sir Thomas More. Conflicting results were overcome by extracting stable information from the tested range of diverse parameter results of Rolling Delta, and by establishing majority attributions of the text chunks with Rolling Classify. Both approaches were applied to the well-recognized 1911 edition of the play, prepared by W. Greg. This followed the given folio sequence 3a–22a and then made the various additions. Each of the additions was not long enough to establish convincing results as to its authorship, but the folio sequences of the original text could be analysed with larger and reliable window sizes, revealing Samuel Rowley and William Shakespeare as authors. The long-standing claim that Munday or Chettle is the author of the original text of the play proved to be unsustainable. As far as the conventional dating is concerned, an earlier analysis of Thomas of Woodstock, largely written by Rowley, and in part by Shakespeare, points, if later revisions are disregarded, to the period 1592–93.",More news on Sir Thomas More,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
641,http://dx.doi.org/10.1093/llc/fqx014,"{'string_id': '10.1093/llc/fqx014', 'id_scheme': 'DOI'}","For several decades, Geographic Information Systems (GISs) have held center stage in archaeological studies of ancient landscapes. Recently, three-dimensional (3D) technologies such as airborne LiDAR and aerial photogrammetry are allowing us to acquire inordinate amounts of georeferenced 3D data to locate, map, and visualize archaeological sites within their surrounding landscapes. GIS offers locational precision, data overlay, and complex spatial analysis. Three-dimensionality adds a ground-based perspective lacking in two-dimensional GIS maps to provide archaeologists a sense of mass and space more closely attuned with human perception. This article uses comparative and iterative approaches ‘tacking back and forth’ between GIS and 3D visualization to explore the role of visibility in conveying sociopolitical and ideological messages at ancient Copan—today a UNESCO World Heritage Site in Honduras. A two-prong approach comprising computational and experiential components explores the potential role of visibility in sending messages that participate in the shaping of social interaction on a daily basis. The organization of built forms within the natural landscape created spatial configurations that sent visual messages targeting specific different groups, subsequently influencing how people negotiated their physical surroundings and the frequency and intensity of social interactions. The ancient Maya belief that sight played a key role in structuring everyday experiences because it triggered perception in the other senses thus serves to bridge the computational and experiential results in this case study.","An iterative 3D GIS analysis of the role of visibility in ancient Maya landscapes: A case study from Copan, Honduras",Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
642,http://dx.doi.org/10.1093/llc/fqx015,"{'string_id': '10.1093/llc/fqx015', 'id_scheme': 'DOI'}","As society continues to embrace advances in digital technologies, a major question that arises is the impact which such technologies have on the concept of an ‘author’ under copyright law. Prior to the advent of the user-generated content (UGC) technology, creative works on the Internet were produced by one or several identifiable authors. The advent of the UGC technology has enabled the active authorial participation of Internet users. This has made it possible for massively collaborative works on the Internet to mushroom where numerous authors’ contributions are incrementally merged into an extensive single work. The concept of an ‘author’ under copyright law is premised on the basis that a work has one or several finite authors. Many UGC works defy this traditional mode of creating works. Taking Malaysian copyright law as the focal point of this study, the research examines whether the concept of an ‘author’ as defined in the Malaysian Copyright Act 1987 is sufficient to address the authorship issue in the light of the UGC technology. It concludes that the current concept of an ‘author’ in the Act is ill-equipped to accommodate Internet-based collaborations. It recommends the introduction of the concept of a ‘deemed author’ in copyright law and suggests that the status of a ‘deemed author’ be conferred on the entity who controls and determines the configuration of the resulting work.",Rethinking the concept of an ‘Author’ in the face of digital technology advances: A perspective from the copyright law of a commonwealth country,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
643,http://dx.doi.org/10.1093/llc/fqx017,"{'string_id': '10.1093/llc/fqx017', 'id_scheme': 'DOI'}","The ongoing dispute in literary studies concerned with gender and writing style is wide and varied. Our preliminary analyses lend evidence to the claims that such gender differences are evident in writing across periods. While we follow in the methodological footsteps of such studies, particular those completed by Hoover (Textual analysis. In Price, K. M. and Siemens, R. (eds), Literary Studies in the Digital Age. Modern Language Association of America, 2013) and Rybicki (2016), we have shifted the focus of our investigation away from style, in the macro-analytical sense, to period and its relation to gender-differentiable terminology. Doing so recognizes the limitations of approaches like Zeta and Delta, while simultaneously benefiting from their affordances. Accepting that one can never have too large or robust a data set for this type of macro-analytic case study, we attempt to build on the foundations set down by Hoover and Rybicki, analyzing gender markers across a selection of male and female authors, and doing so crucially with a concern for the evolution of gender markers over specified canonical literary periods.",The limits of distinctive words: Re-evaluating literature’s gender marker debate,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
644,http://dx.doi.org/10.1093/llc/fqx020,"{'string_id': '10.1093/llc/fqx020', 'id_scheme': 'DOI'}","Although there has been a drive in the cultural heritage sector to provide large-scale, open data sets for researchers, we have not seen a commensurate rise in humanities researchers undertaking complex analysis of these data sets for their own research purposes. This article reports on a pilot project at University College London, working in collaboration with the British Library, to scope out how best high-performance computing facilities can be used to facilitate the needs of researchers in the humanities. Using institutional data-processing frameworks routinely used to support scientific research, we assisted four humanities researchers in analysing 60,000 digitized books, and we present two resulting case studies here. This research allowed us to identify infrastructural and procedural barriers and make recommendations on resource allocation to best support non-computational researchers in undertaking ‘big data’ research. We recommend that research software engineer capacity can be most efficiently deployed in maintaining and supporting data sets, while librarians can provide an essential service in running initial, routine queries for humanities scholars. At present there are too many technical hurdles for most individuals in the humanities to consider analysing at scale these increasingly available open data sets, and by building on existing frameworks of support from research computing and library services, we can best support humanities scholars in developing methods and approaches to take advantage of these research opportunities.","Enabling complex analysis of large-scale digital collections: humanities research, high-performance computing, and transforming access to British Library digital collections",Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
645,http://dx.doi.org/10.1093/llc/fqx021,"{'string_id': '10.1093/llc/fqx021', 'id_scheme': 'DOI'}","In recent years, one of the two fully preserved ancient Greek tragic plays of disputed authorship, Rhesus, traditionally attributed to Euripides, has been the object of a quite lively scholarly interest. The rather extreme number, for the standards of classical philology, of four published commentaries in 10 years, by Athanasios D. Stefanis, [Euripides’]: Rhesus, Athens: Academy of Athens, 2004, Arne Feickert, Euripidis: Rhesus, Frankfurt: Lang, 2005, Vayos Liapis, A Commentary on the Rhesus Attributed to Euripides, Oxford: Oxford University Press, 2012, and Almut Fries, Pseudo-Euripides: Rhesus, Berlin: De Gruyter, 2014 (two of them in English), and one forthcoming by Marco Fantuzzi (also in English), tangibly prove the great vogue this drama enjoys nowadays. Its doubtful nature, as far as the fusion of tragic and comic elements is concerned (see further Burnett, Directions in Euripidean Criticism: A Collection of Essays, Durham: Duke University Press, pp. 13–51, 177–88, 1985), and, most of all, its controversial authorship and date still render it a riddle—even though its non-Euripidean origin is widely accepted. Our research is an attempt to unveil the authorial status of this play, employing traditional and non-traditional authorship attribution techniques. In fact, as it will become clear in the course of this work, the latter kind of methodology allows us to approach the author of Rhesus in a way that was not even thought possible until the present day.",Devising Rhesus: A strange ‘collaboration’ between Aeschylus and Euripides,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
646,http://dx.doi.org/10.1093/llc/fqx022,"{'string_id': '10.1093/llc/fqx022', 'id_scheme': 'DOI'}","Computational stylistics has often analyzed variations in the style of a single author or text, including chronological change, the dialogue or narration of multiple characters or narrators in a single novel, and other perceived shifts in style. Here, I examine intra-textual style variation from a different perspective, and suggest circumstances under which it is appropriate and beneficial to omit some parts of texts from an analysis to eliminate sources of predictable and intrusive variation that would render other kinds of variation or consistency of style invisible. In other cases it may be appropriate to randomize a text to mask a kind of variation that would otherwise disrupt an analysis. One way to do this is by sorting the lines of the text in random order before analyzing it in sections. In analyses in which texts of very different lengths must be compared, it is also beneficial to randomize the longer parts, cut them all to approximately the length of the shortest one, and use the equalized amounts of text to create a word frequency list that is not biased in favor of the longer parts. These techniques can lead to a deeper understanding of style variation.",The microanalysis of style variation,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
647,http://dx.doi.org/10.1093/llc/fqx023,"{'string_id': '10.1093/llc/fqx023', 'id_scheme': 'DOI'}","This article builds on a mathematical explanation of one the most prominent stylometric measures, Burrows’s Delta (and its variants), to understand and explain its working. Starting with the conceptual separation between feature selection, feature scaling, and distance measures, we have designed a series of controlled experiments in which we used the kind of feature scaling (various types of standardization and normalization) and the type of distance measures (notably Manhattan, Euclidean, and Cosine) as independent variables and the correct authorship attributions as the dependent variable indicative of the performance of each of the methods proposed. In this way, we are able to describe in some detail how each of these two variables interact with each other and how they influence the results. Thus we can show that feature vector normalization, that is, the transformation of the feature vectors to a uniform length of 1 (implicit in the cosine measure), is the decisive factor for the improvement of Delta proposed recently. We are also able to show that the information particularly relevant to the identification of the author of a text lies in the profile of deviation across the most frequent words rather than in the extent of the deviation or in the deviation of specific words only.",Understanding and explaining Delta measures for authorship attribution,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
648,http://dx.doi.org/10.1093/llc/fqx024,"{'string_id': '10.1093/llc/fqx024', 'id_scheme': 'DOI'}","Text reuse in early Chinese transmitted texts is extensive and widespread, often reflecting complex textual histories involving repeated transcription, compilation, and editing spanning many centuries and involving the work of multiple authors and editors. In this study, a fully automated method of identifying and representing complex text reuse patterns is presented, and the results evaluated by comparison to a manually compiled reference work. The resultant data are integrated into a widely used and publicly available online database system with browse, search, and visualization functionality. These same results are then aggregated to create a model of text reuse relationships at a corpus level, revealing patterns of systematic reuse among groups of texts. Lastly, the large number of reuse instances identified make possible the analysis of frequently observed string substitutions, which are observed to be strongly indicative of partial synonymy between strings.",Unsupervised identification of text reuse in early Chinese literature,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
649,http://dx.doi.org/10.1093/llc/fqx026,"{'string_id': '10.1093/llc/fqx026', 'id_scheme': 'DOI'}","Through the prism of the comparison, this study examines the dialogism characterizing the discourse of French literary critics in the second half of the 19th century. Using an automatic method, a set of comparisons relying on terms belonging to eleven predetermined hard sciences (anatomy, biology, physics, chemistry, botany, zoology, astronomy, surgery, medicine, geology, and mathematics) was extracted in a corpus of 249 French critical texts as well as in novels, philosophical texts, scientific texts, and texts from the social and human sciences. Apart from confirming the separation of literary critics into two tendencies, one which distances itself from sciences, and the other which wants to emulate it, the retrieved comparative constructions show that despite this division, some sciences such as mathematics are mostly depicted negatively in relation to literature. Furthermore, the comparison plays a crucial role in conferring a scientific dimension to literary criticism, especially because it enables to create an analogical system around some scientific concepts such as ‘espèce’ and therefore, to classify, discuss, and scrutinize literary forms more rigorously. In this literary discourse, the comparison does not merely borrow terms from scientific disciplines but also imbue them with new meaning, so that literary criticism can acquire the same legitimacy as sciences without losing its intrinsic features.",At the crossroads between the scientific and the literary discourse: Comparison as a figure of dialogism,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
650,http://dx.doi.org/10.1093/llc/fqx027,"{'string_id': '10.1093/llc/fqx027', 'id_scheme': 'DOI'}","The article describes First We Feel Then We Fall, a multichannel, interactive video application, which is a multimedia adaptation of James Joyce’s Finnegans Wake. It offers the viewers a portmanteau-like audiovisual experience resembling the experience of reading Joyce’s enigmatic, multilingual dream-like narrative. Through an audiovisual format consisting of simultaneously running streams, it proposes an intermedial translation of hypertextuality and simultaneity of Finnegans Wake. The Wakean imagery, euphonies, rhythms, and polyphonic contexts are rendered into four narrative strands, or ‘plots’. Networks of linguistic, historical, symbolic, and mathematical meanings entailed in Wakean puns are transposed into a dynamic audiovisual structure that the audience can co-shape in the process of interactive viewing. They can switch at will between four simultaneous streams of film clips accompanied by sound (and optional captions with the Finnegans Wake text). The interactive and immersive nature of First We Feel Then We Fall goes beyond previous cinematic adaptations of Joyce’s novel. It is the advance of digital technologies that have enabled us to approach complexity of Finnegans Wake in this novel way.",First We Feel Then We Fall: James Joyce’s Finnegans Wake as an interactive video application,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
651,http://dx.doi.org/10.1093/llc/fqx028,"{'string_id': '10.1093/llc/fqx028', 'id_scheme': 'DOI'}","Maya hieroglyphic analysis requires epigraphers to spend a significant amount of time browsing existing catalogs to identify individual glyphs. Automatic Maya glyph analysis provides an efficient way to assist scholars’ daily work. We introduce the Histogram of Orientation Shape Context (HOOSC) shape descriptor to the Digital Humanities community. We discuss key issues for practitioners and study the effect that certain parameters have on the performance of the descriptor. Different HOOSC parameters are tested in an automatic ancient Maya hieroglyph retrieval system with two different settings, namely, when shape alone is considered and when glyph co-occurrence information is incorporated. Additionally, we developed a graph-based glyph visualization interface to facilitate efficient exploration and analysis of hieroglyphs. Specifically, a force-directed graph prototype is applied to visualize Maya glyphs based on their visual similarity. Each node in the graph represents a glyph image; the width of an edge indicates the visual similarity between the two according glyphs. The HOOSC descriptor is used to represent glyph shape, based on which pairwise glyph similarity scores are computed. To evaluate our tool, we designed evaluation tasks and questionnaires for two separate user groups, namely, a general public user group and an epigrapher scholar group. Evaluation results and feedback from both groups show that our tool provides intuitive access to explore and discover the Maya hieroglyphic writing, and could potentially facilitate epigraphy work. The positive evaluation results and feedback further hint the practical value of the HOOSC descriptor.",Analyzing and visualizing ancient Maya hieroglyphics using shape: From computer vision to Digital Humanities,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
652,http://dx.doi.org/10.1093/llc/fqx029,"{'string_id': '10.1093/llc/fqx029', 'id_scheme': 'DOI'}","We present two new measures of syntactic distance between languages. First, we present the ‘movement measure’ which measures the average number of words that has moved in sentences of one language compared to the corresponding sentences in another language. Secondly, we introduce the ‘indel measure’ which measures the average number of words being inserted or deleted in sentences of one language compared to the corresponding sentences in another language. The two measures were compared to the ‘trigram measure’ which was introduced by Nerbonne & Wiersma (2006, A Measure of Aggregate Syntactic Distance. In Nerbonne, J. and Hinrichs, E. (eds.) Linguistic Distances Workshop at the joint conference of International Committee on Computational Linguistics and the Association for Computational Linguistics, Sydney, July, 2006, pp. 82–90.). We correlated the results of the three measures and found a low correlation between the results of the movement and indel measure, indicating that the two measures represent different kinds of linguistic variation. We found a high correlation between the results of the movement measure and the trigram measure. The results of all of the three measures suggest that English is syntactically a Scandinavian language. Because of our unique database design we were able to detect asymmetric relationships between the languages. All three measures suggest that asymmetric syntactical distances could be part of the explanation why native speakers of Dutch more easily understand German texts than native speakers of German understand Dutch texts (Swarte 2016).",Measuring syntactical variation in Germanic texts,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
653,http://dx.doi.org/10.1093/llc/fqx030,"{'string_id': '10.1093/llc/fqx030', 'id_scheme': 'DOI'}","Understanding people’s online behaviour has traditionally been a field of interest of commercial research agencies. However, academic researchers in a variety of fields are interested in the same type of data to gain insights in the Web behaviour of users. Digital Humanities scholars interested in the use of digital collections are, e.g., interested in the navigation paths of users to these collections. In our case we wanted (1) to analyse the way news consumers visit news websites and (2) understand how these websites fit in their daily news consumption patterns. Until now most common applied scholarly research methods to analyse online user behaviour focus on analyses of log files provided by website owners or recalled user behaviour by survey, diary, or interview methods. Only recently scholars started to experiment with gathering real-world data of Web behaviour by monitoring a group of respondents. In this article we describe the set-up of ‘The Newstracker’, a tool that primarily allowed us to analyse online news consumption of a group of young Dutch news users on their desktop and laptop computers. We demonstrate the workflow of the Newstracker and how we designed the data collection and pre-processing phase. By reflecting on the technical, methodological, and analytical challenges we encountered, we illustrate the potential of online monitoring tools such as the Newstracker. We end our article with discussing its limitations by stressing the need for a multimethod study design when aiming not only to analyse but also to understand online user behaviour.",Analysing and understanding news consumption patterns by tracking online user behaviour with a multimodal research design,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
654,http://dx.doi.org/10.1093/llc/fqx033,"{'string_id': '10.1093/llc/fqx033', 'id_scheme': 'DOI'}","Medieval literary traditions provide a particularly challenging test case for textual alignment and the visualization of variance. Whereas the editors of medieval traditions working with the printed page struggle to illustrate the complex phenomena of textual instability, research in screen-based visualization has made significant progress, allowing for complex textual situations to be captured at the micro- and the macro-level. This article uses visualization and a computational approach to identifying variance to allow the analysis of different medieval poetic works using the transcriptions of how they are found in particular manuscripts. It introduces the notion of a meso-level visualization, a visual representation of aligned text providing for comparative reading on the screen, all the while assembling non-contradictory, intuitive solutions for the visual exploration of multi-scalar variance. Building upon the literary notion of mouvance, it delves into medieval French literature and, in particular, different visualizations of three versions of the Chanson de Roland (the Oxford, the Châteauroux, and the Venice 4 manuscripts). The article presents experimental prototypes for such meso-level visualization and explores how they can advance our understanding of formulaically rich medieval poetry.",Visualizing Mouvance: Toward a visual analysis of variant medieval text traditions,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
655,http://dx.doi.org/10.1093/llc/fqx034,"{'string_id': '10.1093/llc/fqx034', 'id_scheme': 'DOI'}","This article presents an epistemological rationale, intellectual justification, and design outline for a non-representational approach to modeling interpretation in a graphical environment. It begins with a brief critical discussion of the representational approaches that are the common form of information visualizations and suggests that the less familiar non-representational approach could be used to augment these existing visualizations by supporting interpretative work that is closer to the practice of humanistic hermeneutic traditions. Representational display, based on large-scale processing, surrogates, and conventional visualizations, and non-representational modeling at the level of the individual interpretative act operate at very different scales to support intellectual work. In a representational approach, data precede display. Display is a surrogate produced according to automated protocols and algorithms. These cannot be altered or intervened except through rewriting their code, and the display, though interpretative and subject to interpretation, cannot be used as a means by which interpretation is actually modeled. While all visualizations express a model, they do not all provide a modeling environment. In the non-representational approach proposed here, graphical input serves as a primary means of interpretative work. More significantly, a graphical environment that supports direct modeling of interpretation allows traditional humanistic approaches, close reading, and marking of texts, documents, artifacts, or images, to be integrated with computationally produced visualizations. This research was developed as part of the 3DH (three-dimensional/digital humanities) project hosted at the University of Hamburg, between April and June 2016.",Non-representational approaches to modeling interpretation in a graphical environment,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
656,http://dx.doi.org/10.1093/llc/fqx036,"{'string_id': '10.1093/llc/fqx036', 'id_scheme': 'DOI'}","Certain problems in the design of digital systems for use in cultural heritage and the humanities have proved to be unexpectedly difficult to solve. For example, Why is it difficult to locate ourselves and understand the extent and shape of digital information resources? Why is digital serendipity still so unusual? Why do users persist in making notes on paper rather than using digital annotation systems? Why do we like to visit and work in a library, and browse open stacks, even though we could access digital information remotely? Why do we still love printed books, but feel little affection for digital e-readers? Why are vinyl records so popular? Why is the experience of visiting a museum still relatively unaffected by digital interaction? The article argues that the reasons these problems persist may be due to the very complex relationship between physical and digital information and information resources. I will discuss the importance of spatial orientation, memory, pleasure, and multi-sensory input, especially touch, in making sense of, and connections between physical and digital information. I will also argue that, in this context, we have much to learn from the designers of early printed books and libraries, such as the Priory Library and that of John Cosin, a seventeenth-century bishop of Durham, which is part of the collections of Durham University library.",Beauty is truth: Multi-sensory input and the challenge of designing aesthetically pleasing digital resources,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
657,http://dx.doi.org/10.1093/llc/fqx037,"{'string_id': '10.1093/llc/fqx037', 'id_scheme': 'DOI'}","This article presents a model of author dictionary in the field of Latin lexicography. It proposes an organization of the microstructure of its entries following S. C. Dik’s Functionalist Grammar linguistic principles, especially when describing the predicative frameworks. The objective of this article is to provide the user of the lexicon with a suitable tool to disambiguate meanings, thanks to the description of the predicative frameworks and the relationships the different lemmatized units keep. To proceed so, an XML file has been used as the support of the dictionary which is contrasted with its correspondent DTD. This guarantees the adequacy of the principle of hierarchic representation of this metalanguage as well as its lexicographic exploitation in computational linguistics.",A lexicographical model based on the predicative framework theory (functional grammar) for sense disambiguation. An application to Latin author dictionaries,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
658,http://dx.doi.org/10.1093/llc/fqx039,"{'string_id': '10.1093/llc/fqx039', 'id_scheme': 'DOI'}","In this article we aim to provide a minimally sufficient theoretical framework to argue that it is time for a re-conception of the notion of text in the field of digital textual scholarship. This should allow us to reconsider the ontological status of digital text, and that will ground future work discussing the specific analytical affordances offered by digital texts understood as digital texts. Following from the argument of Suzanne Briet regarding documentation, referring to Eco’s understanding of ‘infinite semiosis’, and accounting for the reciprocal effects between carrier technology and meaning observed by McLuhan, we argue that the functions of document and text are realized primarily by their fluid nature and by the dynamic character of their interpretation. To define the purpose of textual scholarship as a ‘stabilisation’ of text is therefore fallacious. The delusive focus on ‘stability’ and discrete ‘philological fact’ gives rise to a widespread belief in textual scholarship that digital texts can be treated simply as representations of print or manuscript texts. On the contrary—digital texts are texts in and of themselves in numerous digital models and data structures which may include, but is not limited to, text meant for graphical display on a screen. We conclude with the observation that philological treatment of these texts demands an adequate digital and/or computational literacy.",Qu’est-ce qu’un texte numérique?—A new rationale for the digital representation of text,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
659,http://dx.doi.org/10.1093/llc/fqx041,"{'string_id': '10.1093/llc/fqx041', 'id_scheme': 'DOI'}","While Gallo-Italic varieties clearly belong to the Romance language family, their subgrouping as either Gallo-Romance or Italo-Romance has been the source of disagreement in the classificatory literature. While earlier analyses tended to classify Gallo-Italic as Gallo-Romance (notably Schmid, 1956; Bec, 1970–1971), later work has either argued for or tacitly assumed a classification of Gallo-Italic as part of the Italo-Romance branch, a view that is both different from as well as irreconcilable with the earlier Gallo-Romance classifications. In this article, we aim to contribute to the development of an empirically based classification of Gallo-Italic through the use of dialectometry applied to atlas corpora, and specifically through the measurement of Levenshtein distance. Using three wordlists (Swadesh 100, Swadesh 200, Leipzig–Jakarta) and comparing twenty-six linguistic varieties across Italy and south-eastern France, we show that Gallo-Italic is best classified as a third subgroup within the Gallo-Romance branch. Our results also clearly identify all the major bundles of isoglosses established through traditional dialectological methods and confirm Gallo-Italic as a relatively homogenous group distinct from Italo-Romance.",Revisiting the classification of Gallo-Italic: a dialectometric approach,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
660,http://dx.doi.org/10.1093/llc/fqx042,"{'string_id': '10.1093/llc/fqx042', 'id_scheme': 'DOI'}","Compared to the epistemic traditions digital palaeography builds on, how is it transformative? In this article I will outline the emergent meanings and possible research directions of digital palaeography by reflecting on the past 15 years of approaches and conceptualizations in the field. By departing from a contextualized take of the term digital coupled with humanities and palaeography, I will show how digital approaches relate to the scholarly tradition of the study of handwriting and writing systems as a whole and how recent approaches of digital palaeography can be defined as critical, self-reflective, multidisciplinary and interdisciplinary. Moving between a formal and a historically situated analysis, I will relate practices of modelling of handwriting in digital palaeography to modelling in digital humanities more generally. Digital palaeography will emerge well positioned to represent the complexity of handwritten objects from the unfamiliar perspective of the substance of the expression of handwriting (text as shape).",Digital palaeography: What is digital about it?,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,32,suppl_2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
661,http://dx.doi.org/10.1093/llc/fqx043,"{'string_id': '10.1093/llc/fqx043', 'id_scheme': 'DOI'}","This article looks at the provenance of the unfinished novel The Dark Tower, generally attributed to C. S. Lewis. The manuscript was purportedly rescued from a bonfire shortly after Lewis’s death by his literary executor Walter Hooper, but the quality of the text is hardly vintage Lewis. Using computer stylometric programs made available by Eder et al.’s (2016: Stylometry with R: A package for computational text analysis. R Journal, 8(1): 107–21) ‘stylo’ package and a word length analysis, samples of each chapter of The Dark Tower were compared with works known to be by Lewis, two books by Hooper and a hoax letter concerning the bonfire by Anthony Marchington. Initial experiments found that the first six chapters of The Dark Tower were stylometrically consistent with Lewis’s known works, but the incomplete Chapter 7 was not. This may have been due to an abrupt change in genre, from narrative to pseudoscientific style. Using principal components analysis, it was found that the first and subsequent components were able to separate genre and individual style, and thus a plot of the second against the third principal components enabled the effects of genre to be filtered out. This showed that Chapter 7 was also consistent with the other samples of C. S. Lewis’s writing.",Computer stylometry of C. S. Lewis’s The Dark Tower and related texts,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
662,http://dx.doi.org/10.1093/llc/fqx045,"{'string_id': '10.1093/llc/fqx045', 'id_scheme': 'DOI'}","‘Authorship attribution’, the problem of determining the author (or the author's attributes, such as gender, age, native language, or other characteristics) by examining the writing style of an unknown work, is an important problem in applied linguistics. The theory of authorship attribution is relatively straightforward: language is an underspecified system, and people can pick and choose among several different ways to describe the same thing. These choices, in turn, become habituated and can be identified as persistent patterns of an individual or group of writers.One important psycholinguistic underpinning to this solution is the universal existence (in natural languages) of so-called “marker words” or ‘function words,’–little, closed-class words that do not carry much semantics but instead denote relationships between content words. Because these words are so lightly processed, writers/speakers can choose among many different near-synonymous forms, and implicitly express their identity in doing so.Do constructed languages have this same degree of near-synonymity? We present the results of a study of authorship attribution using an ad-hoc corpus of fan-written documents in various constructed languages, and show that even artificial languages constructed for artistic purposes, such as Klingon, Na'vi, and Elvish, permit this type of analysis. This indicates that even constructed languages tend to be psycholinguistically plausible.","Authorship attribution, constructed languages, and the psycholinguistics of individual variation",Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
663,http://dx.doi.org/10.1093/llc/fqx046,"{'string_id': '10.1093/llc/fqx046', 'id_scheme': 'DOI'}","Distributed language representation (deep learning) has been applied successfully in different applications in natural language processing. Using this model, we propose and implement two new authorship attribution classifiers. In this perspective, a vector-space representation can be generated for each author or disputed text according to words and their nearby context. To determine the authorship of a disputed text, the cosine similarity between vector representations can be applied. The proposed strategies can be adapted without any difficulty to different languages (such as English and Italian) or genres (essays, political speeches, and newspaper articles). Evaluations using the k-nearest neighbors (k-NNs))and based on four test collections (the Federalist Papers, the State of the Union addresses, the Glasgow Herald, and La Stampa newspapers) indicate that the distributed language representation preforms well, providing sometimes better effectiveness than state-of-the-art methods such as k-NN, nearest shrunken centroids, chi-square, Delta, latent Dirichlet allocation, or multi-layer perceptron classifier.",Distributed language representation for authorship attribution,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
664,http://dx.doi.org/10.1093/llc/fqx047,"{'string_id': '10.1093/llc/fqx047', 'id_scheme': 'DOI'}","In our research, we study mechanisms of knowledge dissemination based on the structural and social networks surrounding the edition history of a single text: the Tractatus de sphaera by Johannes de Sacrobosco. By applying methods from network analysis, we investigate how specific commentaries on the text circulated, which actors were responsible for them and what factors supported or hindered the spread of specific kinds of knowledge. The basis of this investigation is represented by CorpusTracer, a database that stores the required data in a suitable format and with the required level of expressivity. In this article, we present the design of our database and our data model based on CIDOC-CRM and FRBRoo. We discuss the implementation and suitability of the conceptual and technical realization for our research question. We conclude that FRBRoo fits well to the task at hand. We found that the comparatively complex data structure it requires can be sufficiently abstracted through current implementation methods. As the research continues, our data model will have to grow and we expect that the presented methods will be sufficient to accommodate our future requirements.",CorpusTracer: A CIDOC database for tracing knowledge networks,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
665,http://dx.doi.org/10.1093/llc/fqx049,"{'string_id': '10.1093/llc/fqx049', 'id_scheme': 'DOI'}","The article outlines the rationale of the born-digital dossier génétique from a digital forensic perspective in the light of the recent discussion about digital materiality. In its first part, the study addresses theoretical, conceptual, and methodological questions that arise from the specific materiality of the born-digital avant-texte, namely, the dualism of ‘forensic materiality’ and ‘formal materiality’ (M. Kirschenbaum) and the role of distributed materiality (J.-F. Blanchette). The article argues that the born-digital record, consisting of digital objects, temporary files, metadata, and fragmented traces of the writing process scattered across multiple system locations, has to be analyzed with regard to the specific historical computing context, its distributed materiality ensemble of hardware, operating system, and application (multi-evidential perspective, J. L. John). Current challenges for born-digital preservation and philological analysis will be discussed. In the second part, the exemplary analysis of several digital drafts and text fragments found on the hard drives of German poet Thomas Kling (1957–2005) sheds light on digital materiality from a practical digital forensic and critique génétique perspective. The following methods will be demonstrated: analysis of fast save artifacts in Microsoft Word documents; draft text recovery from CHK files; file carving and verification of recovery results (true, false positives); recovery of text fragments from drive slack. Digital forensic methodology is in the focus of this article as a tool in the context of archival studies, philology, genetic criticism, and scholarly editing of born-digital material. As born-digital primary records of cultural, social, and political history (private storage media, cloud storage, world wide web content as well as public and semi-public social media posts) come to the archives in increasing numbers and volumes, the forensic perspective on born-digital material, questions of authentic preservation and analysis, bibliographic citability and stability, materiality and their status as a document and evidence as well as legal and ethical issues of preservation, curation and access in the archives become crucial for all humanities disciplines.",The rationale of the born-digital dossier génétique: Digital forensics and the writing process: With examples from the Thomas Kling Archive,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
666,http://dx.doi.org/10.1093/llc/fqx050,"{'string_id': '10.1093/llc/fqx050', 'id_scheme': 'DOI'}","This article reports a study that compared how Portuguese and Brazilian newspapers covered Japan in the 90s. The research was based on 9,152 texts related to Japan published in a Portuguese and a Brazilian newspaper from that era. This is a much larger sample than what was used in existing text content analysis studies for Portuguese. To treat this large sample, selected concordances and distributions obtained from the corpora were semi-automatically analyzed. Results revealed that the most referred Japanese personalities were politicians. Additionally, in general, there are more texts related to Japan (and naming Japanese personalities) in the Portuguese newspaper than in the Brazilian newspaper. The study focused on personalities for which there are statistically significant frequency differences in the Portuguese and the Brazilian newspapers. A detailed analysis of the texts where these personalities were named revealed events related to Japan with different impact in Portugal and Brazil.",A method for content analysis applied to newspaper coverage of Japanese personalities in Brazil and Portugal,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
667,http://dx.doi.org/10.1093/llc/fqx051,"{'string_id': '10.1093/llc/fqx051', 'id_scheme': 'DOI'}",Digitization has changed the concept of dictionaries from merely alphabetically ordered reference works into lexical databases providing flexible search systems with interconnected lemmas. This article investigates ensuing opportunities and useful design options of digitized historical dictionaries as research tools for the study of texts. It appears that we have arrived at an interesting intersection of digital humanities and historical lexicography. The 14th-century ‘seemly play of Winter and Summer’ serves as a research case.,At the crossroads of digital humanities and historical lexicography: The Middle Dutch ‘seemly play (abel spel) of Winter and Summer’ as a research case,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
668,http://dx.doi.org/10.1093/llc/fqx052,"{'string_id': '10.1093/llc/fqx052', 'id_scheme': 'DOI'}","Normally, software documentations are produced, informally. They are written in the unnatural and non-structural form of the language, such as user manuals, user requirements, design documentation, tutorials, support documentation, and so on. Recent studies show that 61% of software projects are subject to failure or challenges due to an increase in the costs and production time. Various factors may lead to this issue, and one of the major contributing factors is the lack of links between the software's source code and its related documents. The significance of software development and the possibility of making prospective changes by the development team necessitate an understanding of the links between various sections of codes and documentations. Therefore, it is crucial to design a system to link the software codes to their corresponding textual documentation. This article proposes a model for recovering the latent, but traceable links between software source codes and existing documents based on word extraction and function name separation. The contributions in this article include: (1) a model based on word extraction from document and source codes; (2) the proposal of an algorithm for splitting compound words and words that are connected to one another and completing abbreviations used in the names of functions, variables, and output commands; and (3) a new algorithm that is proposed for retrieving traceable latent links between the source code and documents. Two data sets are used in this research and the achieved results will be reported in terms of recall, precision, and F-measure. The experimental results are promising and indicate that the proposed approach significantly outperforms its counterparts.",Mining and discovery of hidden relationships between software source codes and related textual documents,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
669,http://dx.doi.org/10.1093/llc/fqx053,"{'string_id': '10.1093/llc/fqx053', 'id_scheme': 'DOI'}","Dr Anne Luther is a researcher, curator, and software developer whose work examines the contemporary art market and data visualization in qualitative research. She received her PhD from Central Saint Martins College of Art and Design, London, and is currently a researcher at the Department for Modern Art History at the Institute of Art Studies and Historical Urban Studies at TU Berlin and at The Center for Data Arts at The New School in New York. Her PhD research presents a cultural analysis of contemporary art collecting and art production with an illustration of patterns that overlap in collecting and art production practices in contemporary art. The analysis shows how institutions, local context, social strategies, and prestige overlap in their influences on art production as a cause for collecting contemporary art. Her research in data visualization, qualitative methods, and data analysis emerged into the successful implementation of new software for qualitative research, The Entity Mapper.anneluther.info@AnneLutherahttps://github.com/lutheranne",Visual meta-data in qualitative analysis,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
670,http://dx.doi.org/10.1093/llc/fqx054,"{'string_id': '10.1093/llc/fqx054', 'id_scheme': 'DOI'}","Spelling correction is one of the main tasks in the field of Natural Language Processing. Contrary to common spelling errors, real-word errors cannot be detected by conventional spelling correction methods. The real-word correction model proposed by Mays, Damerau, and Mercer showed a great performance in different evaluations. In this research, however, a new hybrid approach is proposed which relies on statistical and syntactic knowledge to detect and correct real-word errors. In this model, Constraint Grammar is used to discriminate among sets of correction candidates in the search space. Mays, Damerau, and Mercer’s trigram approach is manipulated to estimate the probability of syntactically well-formed correction candidates. The approach proposed here is tested on the Wall Street Journal corpus. The model can prove to be more practical than some other models, such as WordNet-based method of Hirst and Budanitsky and fixed windows size method of Wilcox-O’Hearn and Hirst.",Correcting real-word spelling errors: A new hybrid approach,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
671,http://dx.doi.org/10.1093/llc/fqx055,"{'string_id': '10.1093/llc/fqx055', 'id_scheme': 'DOI'}","Text mining techniques were applied to a corpus consisting in the titles of 2,454 documents on Mudejar art, a style unique to Spanish art history. Probabilistic topic modelling was used to analyse the semantic structure underlying the suite of documents studied. Two classifications were obtained, an initial, generic division into five topics followed by a second more refined division into ten. These were compared to the preliminary subject categories found for the corpus with the guidance of an area specialist. The classifications delivered by the automatic and manual procedures were observed to be compatible. The conclusion drawn was that the deployment of digitized data affords the opportunity to conduct humanities studies from new perspectives.",Topic modelling characterization of Mudejar art based on document titles,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
672,http://dx.doi.org/10.1093/llc/fqx056,"{'string_id': '10.1093/llc/fqx056', 'id_scheme': 'DOI'}","The question of translators’ stylistic visibility in translated texts has been a recurring theme in translation studies. Recently, the employment of state-of-the-art stylometric methods such as multivariate statistical analysis or machine learning techniques has enabled important progress to be made in exploring the problem. Nevertheless, studies are conflicting in their findings. Some find evidence of translators’ stylistic presence, while others fail to do so. The lack of agreement in the literature makes one suspect some contextual factors affecting the degree of translators’ visibility. The present study focuses on one such possible factor—language combinations involved in translation. It is hypothesized that the farther the two languages involved are structurally apart, the more likely it is for the translator’s style to gain visibility as the increased distance will allow the translator greater freedom to be creative with his or her choices. The present study applies bootstrap consensus tree analysis and consensus network analysis to 175 samples of contemporary literary translation in two language pairs—the intimate pair of French and English and the distant pair of Korean and English. The analysis supports the hypothesis by showing that the authors completely override their translators to claim authorship in the French group, while authorial presence is significantly diminished in the Korean group, resulting in greater visibility for some translators.",Do language combinations affect translators’ stylistic visibility in translated texts?,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
673,http://dx.doi.org/10.1093/llc/fqx057,"{'string_id': '10.1093/llc/fqx057', 'id_scheme': 'DOI'}","Searching for articles of interest in a digital archive need not be through a free-form text search. In fact, many authors have suggested that the best way to find relevant items in an archive is to browse its contents rather than to search for specific keywords. The University of Central Florida’s Regional Initiative for Collecting Histories, Experiences and Stories (RICHES) project uses a multi-criteria Connections algorithm to make item selection recommendations and browse through the RICHES Mosaic Interface (RICHES MI)—an archive of digitized historical documents, imagery, and audio. The Connections algorithm allows researchers to examine a selected artifact and nearest related items in the archive based on multiple criteria from the metadata contained in the artifact of interest. To determine how effective the Connections algorithm was at presenting relevant material, it was compared to random selections and single criteria keyword searches. In this article we will show that the multi-criteria approach is not only better than randomly selected results it also selects more relevant items than single criteria keyword searches. In addition, the multi-criteria algorithm achieves a secondary benefit: it returns unanticipated relevant results that potentially yield new insights for the researcher.",Evaluating multi-criteria Connection mechanisms: A new algorithm for browsing digital archives,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
674,http://dx.doi.org/10.1093/llc/fqx058,"{'string_id': '10.1093/llc/fqx058', 'id_scheme': 'DOI'}","The article gives a brief outline of the fruitless attempts in the past at finding the author(s) of the Parnassus Plays, an anonymous trilogy performed between 1597 and 1601 at St. John’s College in Cambridge as part of the Christmas festivities, and then moves on to employ the R Stylo features Rolling Delta and Rolling Classify, which in various approaches confirm John Marston and Thomas Nashe as the authors. As collaborative scenarios count among the assets of these relatively new stylometric tools, it became possible to tip the scales towards Glatzer’s view that the plays have two authors, and Leishman’s advocacy of one author, based on the manuscript prologue of Part 2 of The Return, gave way to a ‘battle of the plays’ between Marston and Nashe.",Stylometry approaching Parnassus,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
675,http://dx.doi.org/10.1093/llc/fqx059,"{'string_id': '10.1093/llc/fqx059', 'id_scheme': 'DOI'}","Function word adjacency networks (WANs) are used to study the authorship of plays from the Early Modern English period. In these networks, nodes are function words and directed edges between two nodes represent the relative frequency of directed co-appearance of the two words. For every analyzed play, a WAN is constructed and these are aggregated to generate author profile networks. We first study the similarity of writing styles between Early English playwrights by comparing the profile WANs. The accuracy of using WANs for authorship attribution is then demonstrated by attributing known plays among six popular playwrights. Moreover, the WAN method is shown to outperform other frequency-based methods on attributing Early English plays. In addition, WANs are shown to be reliable classifiers even when attributing collaborative plays. For several plays of disputed co-authorship, a deeper analysis is performed by attributing every act and scene separately, in which we both corroborate existing breakdowns and provide evidence of new assignments.",Stylometric analysis of Early Modern period English plays,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
676,http://dx.doi.org/10.1093/llc/fqx062,"{'string_id': '10.1093/llc/fqx062', 'id_scheme': 'DOI'}","For the first time, historians of higher education have large data sets of primary sources that reflect the complete output of academic institutions at their disposal. To analyze this unprecedented abundance of digital materials, scholars have access to a large suite of computational methods developed in the field of Natural Language Processing. However, when the intention is to move beyond exploratory studies and use the results of such analyses as quantitative evidences, historians need to take into account the reliability of these techniques. The main goal of this article is to investigate the performance of different text mining methods for a specific task: the automatic identification of interdisciplinary works from a corpus of PhD dissertation abstracts. Based on the output of our study, we provide the research community of a new data set for analyzing recent changes in interdisciplinary practices in a large sample of European universities. We show the potential of this collection by tracking the growth in adoption of computational approaches across different research fields, during the past 30 years.",Toward a computational history of universities: Evaluating text mining methods for interdisciplinarity detection from PhD dissertation abstracts,Oxford University Press (OUP),2017,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
677,http://dx.doi.org/10.1093/llc/fqx064,"{'string_id': '10.1093/llc/fqx064', 'id_scheme': 'DOI'}","In recent years, important research on crowdsourcing in the cultural heritage sector has been published, dealing with topics such as the quantity of contributions made by volunteers, the motivations of those who participate in such projects, the design and establishment of crowdsourcing initiatives, and their public engagement value. This article addresses a gap in the literature, and seeks to answer two key questions in relation to crowdsourced transcription: (1) whether volunteers’ contributions are of a high enough standard for creating a publicly accessible database, and for use in scholarly research; and (2) if crowdsourced transcription makes economic sense, and if the investment in launching and running such a project can ever pay off. In doing so, this article takes the award-winning crowdsourced transcription initiative, Transcribe Bentham, which began in 2010, as its case study. It examines a large data set, namely, 4,364 checked and approved transcripts submitted by volunteers between 1 October 2012 and 27 June 2014. These data include metrics such as the time taken to check and approve each transcript, and the number of alterations made to the transcript by Transcribe Bentham staff. These data are then used to evaluate the long-term cost-effectiveness of the initiative, and its potential impact upon the ongoing production of The Collected Works of Jeremy Bentham at UCL. Finally, the article proposes more general points about successfully planning humanities crowdsourcing projects, and provides a framework in which both the quality of their outputs and the efficiencies of their cost structures can be evaluated.",‘Making such bargain’: Transcribe Bentham and the quality and cost-effectiveness of crowdsourced transcription1,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
678,http://dx.doi.org/10.1093/llc/fqx065,"{'string_id': '10.1093/llc/fqx065', 'id_scheme': 'DOI'}","The Whitechapel murders that terrorized London in 1888 are still remembered to this day, thanks to the legend of its unapprehended perpetrator, Jack the Ripper. In addition to the gruesomeness of the murders, the name and the persona of the killer have been popularized by the over 200 letters signed as ‘Jack the Ripper’ that have been received following the murders. The most supported theory on the authorship of these letters is that some of the earliest key texts were written by journalists to sell more newspapers and that the same person is responsible for writing the two most iconic earliest letters. The present article reports on an authorship clustering/verification analysis of the Jack the Ripper letters with a view to detect the presence of one writer for the earliest and most historically important texts. After compiling the ‘Jack the Ripper Corpus’ consisting of the 209 letters linked to the case, a cluster analysis of the letters is carried out using the Jaccard distance of word 2-grams. The quantitative results and the discovery of certain shared distinctive lexicogrammatical structures support the hypothesis that the two most iconic texts responsible for the creation of the persona of Jack the Ripper were written by the same person. In addition, there is also evidence that a link exists between these texts and another of the key texts in the case, the Moab and Midian letter.",An authorship analysis of the Jack the Ripper letters,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
679,http://dx.doi.org/10.1093/llc/fqx066,"{'string_id': '10.1093/llc/fqx066', 'id_scheme': 'DOI'}","This article looks at the case of Elena Ferrante, the (presumed) pseudonym of an internationally successful Italian novelist, and has two objectives: first, to observe how her novels are positioned in the panorama of modern Italian literature (represented by an ad hoc reference corpus—composed of 150 novels by forty different authors) and, second, to attempt to understand whether, amongst the authors in the corpus, there are any that can be considered candidates for involvement in the writing of the novels signed Ferrante. Consistent with these two objectives, the analyses also use two methods: correspondence analysis for the content mapping of the novels and Labbé’s intertextual distances to establish a measure of similarity between the novels. In the results, we do not see the expected similarities with writers from the Naples area as Elena Ferrante distinguishes herself with original literary products that, both in terms of theme and style, show her strong individuality. Amongst the authors included, Domenico Starnone, who has been previously identified by other investigations as the possible hand behind this pen name, is the author who has written novels most similar to those of Ferrante and which, over time, has become progressively more similar.",What is Elena Ferrante? A comparative analysis of a secretive bestselling Italian writer,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,33,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
680,http://dx.doi.org/10.1093/llc/fqy001,"{'string_id': '10.1093/llc/fqy001', 'id_scheme': 'DOI'}","This paper makes use of new tables that provide a comprehensive survey of authorship attributions derived from the Rolling Delta feature of R Stylo. Its main subject is the analysis of the Marlowe corpus and other plays connected to Marlowe. The two Tamburlaines and Locrine turn out to be sole-authored Marlowe plays. The core plays of the Marlowe corpus are largely un-Marlovian in their style and this casts further doubts on his authorship, going well beyond the assessment of Marlowe by Dabbs. But Marlowe was not a phantom. His stylistic presence could be observed in a number of the plays investigated, among them the anonymous Edward III and Shakespeare's Henry V. Contrary to findings of the New Oxford Shakespeare, 3 Henry VI is not found to have been co-authored by Marlowe. Methodological observations offer indicators for the preference of the most frequent character trigrams as variables and the possible provenance of model texts conveyed by the most frequent function words. The latter were often key pieces of evidence in traditional stylometry.",Christopher Marlowe: Hype and Hoax,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,33,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
681,http://dx.doi.org/10.1093/llc/fqy002,"{'string_id': '10.1093/llc/fqy002', 'id_scheme': 'DOI'}","We present the methodological and technical process we adopted to develop DanteSources, a Web application that allows free access to the knowledge about Dante Alighieri’s primary sources, i.e. the works of other authors that Dante cites in his texts. Up to now, this knowledge has been collected in many paper books, making it difficult for the scholars to retrieve it and to produce a complete overview of these data. Using Semantic Web technologies, we developed an ontology expressed in the Resource Description Framework Schema vocabulary providing the terms to represent this knowledge in a machine-readable form. A semi-automatic tool helps the scholars to populate the ontology with the data included in authoritative paper commentaries to Dante’s works. Then, the tool automatically saves the resulting Resource Description Framework graph in a triple store. On top of this graph, we developed DanteSources, a Web application that allows users to extract and display the information stored in the knowledge base in the form of charts and tables. Finally, we report the results of a survey to collect suggestions from end-users on their interactions with DanteSources. The methodology and the tools we developed are easily reusable, e.g. to represent the knowledge about primary sources of other authors of the Italian and the international literature.",A web application for exploring primary sources: The DanteSources case study,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,33,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
682,http://dx.doi.org/10.1093/llc/fqy003,"{'string_id': '10.1093/llc/fqy003', 'id_scheme': 'DOI'}","This article presents Parsa as an open information extraction (OIE) system for Persian. Comparing with advanced English approaches, OIE has just started to develop in other languages. Existing systems apply information about the grammar and syntactic structures of the target language to gain domain independence (which is a key goal in OIE). To improve modeling these complex structures, Parsa introduces a novel set of Patterns based on tree format. The patterns also enable Parsa to define POS tags, and lexical constraints to reduce incorrect matches. Each Tree Pattern is placed inside a Package based on its type and priority. The Packages help Parsa to alleviate some challenges in processing Persian like null-subject problem and uninformative extraction. To make the extraction process simple and coherent, we separate matching template from extraction template. An efficient algorithm for matching patterns inside dependency parse of a sentence is presented as well. Our experiments showed that Parsa achieves better performance than the state of the art systems in Persian, and highly comparable with the existing approaches in English.",Parsa: An open information extraction system for Persian,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,33,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
683,http://dx.doi.org/10.1093/llc/fqy004,"{'string_id': '10.1093/llc/fqy004', 'id_scheme': 'DOI'}","Much of the quantitative work undertaken in stylistic analysis has to do with word frequencies—usually the relative frequencies of an appropriate set of single word-types. Those who have sought to go further by choosing word-types that tend to ‘go together’ have taken sequence and close proximity as their criteria. But many words display similar patterns of frequency without necessarily meeting those criteria: sets of grammatical associates, deictic features, archaisms, colloquialisms, Latinisms, and many others. Such sets, moreover, have negative corollaries, the alternatives consistently not chosen. Across a range of texts appropriate to whatever case may be in hand, both positive resemblances and direct contrasts of frequency can be identified by Spearman’s method of correlation. The coefficients for many of the pairs united in this way show very high levels of statistical significance. These pairs can be gathered in sets embracing all the partners of a given member, with separate subsets for positives and negatives. When, for example, ‘the’ is taken as a ‘headword’, it yields positive and negative sets, ‘THE_p’ and ‘THE_n’. Such ‘rho-sets’ can then be treated as composite variables and employed as data in much the same ways as we customarily use single-word variables. The trials undertaken (and illustrated here) suggest that this approach gives unusually accurate measures of stylistic difference, especially with short texts. Many of the sets themselves are of considerable philological interest and help to explain how the study of word frequencies can be so rich in stylistic information.",Rho-grams and rho-sets: Significant links in the web of words,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,33,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
684,http://dx.doi.org/10.1093/llc/fqy005,"{'string_id': '10.1093/llc/fqy005', 'id_scheme': 'DOI'}","This article presents a text-analytic approach to analysing media content for evidence of gender bias. Irish newspaper content is examined using machine learning and natural language processing techniques. Systematic differences in the coverage of male and female politicians are uncovered, and these differences are analysed for evidence of gender bias. A corpus of newspaper coverage of politicians over a 15-year period was created. Features of the text were extracted and patterns differentiating coverage of male and female politicians were identified using machine learning. Discriminative features were then analysed for evidence of gender bias. Findings showed evidence of gender bias in how female politicians were portrayed, the policies they were associated with, and how they were evaluated. This research also sets out a methodology whereby natural language processing and machine learning can be used to identify gender bias in media coverage of politicians.",Uncovering gender bias in newspaper coverage of Irish politicians using machine learning,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
685,http://dx.doi.org/10.1093/llc/fqy006,"{'string_id': '10.1093/llc/fqy006', 'id_scheme': 'DOI'}",Digital possibilities and the presence of large image collections urged art history to reassess existing methods to study artworks. Big data facilitate new research—allowing to analyze millions of images—but also revealed the insufficiency of existing methods. The collaboration between computer vision and art history has provided tools to access and evaluate large image collections. This article elaborates on the potentials of a collaboration and presents work by the Computer Vision group of Heidelberg University. The group uses computational methods to study art data and performs automatic visual searches to find recurrences and organize data according to notions of similarity. It will be shown that large image collections can be studied efficiently with computer-based methods to assist art historians with iconographic research and that similar approaches already existed in art history at the beginning of the twentieth century.,Attesting similarity: Supporting the organization and study of art image collections with computer vision,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,33,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
686,http://dx.doi.org/10.1093/llc/fqy007,"{'string_id': '10.1093/llc/fqy007', 'id_scheme': 'DOI'}","The article explores the uses of quantitative approaches used in textual scholarship in studying large amounts of medieval hand-written calendars. Calendars are exceedingly numerous among medieval manuscript sources but have been studied surprisingly little in spite of the insights they offer into the values and ideals of the communities using and updating them. Moreover, the study of a large number of calendars helps shape patterns of cultural contacts, for instance. The constant copying and modifying of a medieval calendar is analogous to copying of other manuscripts by hand in the Middle Ages. However, the overall pattern of influences was much more complex than in traditional copying, and new quantitative methods are called for. In this article, we propose three different quantitative methods for the analysis of medieval calendars. They provide a scholar with sound hypotheses on the relationships between a large number of calendars, on the broader context of an individual calendar’s contents as well as on the single feasts that can be indicative of the origin of one or several calendars.",Quantitative methods for the analysis of medieval calendars,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,33,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
687,http://dx.doi.org/10.1093/llc/fqy008,"{'string_id': '10.1093/llc/fqy008', 'id_scheme': 'DOI'}","The article presents the achievements of digital humanities in Poland, draws attention to the needs related to the development of digitization, and points to possible future undertakings aimed to popularize the accomplishments in this field. Apart from digitization, issues such as methods of sharing historic and linguistic sources in the digital form are covered. These sources include historic and scientific dictionaries of the Polish language, records, and texts dating back to before 1945. Additionally, linguistic corpora of historic Polish are presented, both those completed and those underway. The article emphasizes the imperative to create constellations of linguistic data warehouses. The last part, dedicated to the concept of the platform Diachronic Corpora of Polish, constitutes an attempt to catalog, disseminate, and made public the results of activities pertaining to works on the Polish corpora.","Digital Humanities in Poland from the Perspective of the Historical Linguist of the Polish Language: Achievements, Needs, Demands",Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,33,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
688,http://dx.doi.org/10.1093/llc/fqy010,"{'string_id': '10.1093/llc/fqy010', 'id_scheme': 'DOI'}","In the present study 2,180 papers related to embodied cognition in the framework of linguistics were reviewed by using the bibliometric approach. The bibliographic records were collected from the Web of Science (Thomson Reuters) from 1992 to 2016 and were composed of a core data set and an expanded data set by topic searching and citation expansion. Document co-citation analysis, citation burst detection, and betweenness centrality measurement were conducted to explore and determine the thematic patterns, emerging trends, and critical articles of the knowledge domain. The results indicate that the study concerning language comprehension is the most prominent cluster. In addition, the labels as conceptual metaphor and conversational analysis are active clusters in a certain period. Meanwhile, the bursts of detected papers demonstrate that the present focus on language comprehension is a process of mental simulation of sensorimotor and other related experiences, and the topic of meaning construction is the product of interactive embodiment and cognitive processing, highlighting the role of simulation in language comprehension in emerging trends and future directions.",Visualizing the knowledge domain of embodied language cognition: A bibliometric review,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
689,http://dx.doi.org/10.1093/llc/fqy011,"{'string_id': '10.1093/llc/fqy011', 'id_scheme': 'DOI'}","This research examines and contributes to recent work by Matthew Jockers and Gabi Kirilloff on the relationship between gender and action in the nineteenth-century novel. Jockers and Kirilloff use dependency parsing to extract verb and gendered pronoun pairs (‘he said’, ‘she walked’, etc.). They then build a classification model to predict the gender of a pronoun based on the verb being performed. This present study examines the novels that were categorized as outliers by the classification model to gain a better understanding of the way the observed trends function at the level of individual narratives. We argue that while the classifier successfully categorized and identified novels in which characters behave unconventionally—that is, in ways not typical to the corpus as a whole—the rhetorical effects of these unconventional novels (and the extent to which their authors openly question nineteenth-century gender norms) vary based on other factors of characterization and narration. We propose that the combination of machine and human reading that this essay utilizes provides a productive model for allowing distant reading to guide and provoke traditional humanities scholarship.","From a distance ‘You might mistake her for a man’: A closer reading of gender and character action in Jane Eyre, The Law and the Lady, and A Brilliant Woman1",Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,33,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
690,http://dx.doi.org/10.1093/llc/fqy012,"{'string_id': '10.1093/llc/fqy012', 'id_scheme': 'DOI'}","While studies on diachronic Chinese syntax have identified a number of linguistic changes in Medieval Chinese, they have mostly been underpinned by qualitative analyses. In the most large-scale quantitative analysis to-date, this article investigates changes in the use of classifiers, demonstratives, and copulae. Our analysis, based on the Chinese Buddhist Canon, examines over 40 million characters in texts spanning a millennium. Results suggest that from the late Eastern Han period (circa 150 CE) onwards, the vernacular style became increasingly widespread, at the expense of the literary style, as reflected by changes in the use of classifiers and demonstratives, and in the construction of nominal sentences. However, the vernacular style became less frequently used in the Northern Sung period (960–1127 CE). This reversal may shed light on the work of the Stylists, editors appointed by the Sung court to polish Buddhist texts with more literary elements.","Vernacularization in Medieval Chinese: A quantitative study on classifiers, demonstratives, and copulae in the Chinese Buddhist Canon",Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
691,http://dx.doi.org/10.1093/llc/fqy013,"{'string_id': '10.1093/llc/fqy013', 'id_scheme': 'DOI'}","Based on n text excerpts, the authorship linking task is to determine a way to link pairs of documents written by the same person together. This problem is closely related to authorship attribution questions, and its solution can be used in the author clustering task. However, no training information is provided and the solution must be unsupervised. To achieve this, various text representation strategies can be applied, such as characters, punctuation symbols, or letter n-grams as well as words, lemmas, Part-Of-Speech (POS) tags, and sequences of them. To estimate the stylistic distance (or similarity) between two text excerpts, different measures have been suggested based on the L1 norm (e.g. Manhattan, Tanimoto), the L2 norm (e.g. Matusita), the inner product (e.g. Cosine), or the entropy paradigm (e.g. Jeffrey divergence). From those possible implementations, it is not clear which text representation and distance functions produce the best performance, and this study provides an answer to this question. Three corpora, extracted from French and English literature, have been evaluated using standard methodology. Moreover, we suggest an additional performance measure called high precision (HPrec) capable of judging the quality of a ranked list of links to provide only correct answers. No systematic difference can be found between token- or lemma-based text representations. Simple POS tags do not provide an effective solution but short sequences of them form a good text representation. Letter n-grams (with n = 4–6) give high HPrec rates. As distance measures, this study found that the Tanimoto, Matusita, and Clark distance measures perform better than the often-used Cosine function. Finally, applying a pruning procedure (e.g. culling terms appearing once or twice or limiting the vocabulary to the 500 most frequent words) reduces the representation complexity and might even improve the effectiveness of the attribution scheme.",Evaluation of text representation schemes and distance measures for authorship linking,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
692,http://dx.doi.org/10.1093/llc/fqy015,"{'string_id': '10.1093/llc/fqy015', 'id_scheme': 'DOI'}","The widespread use of English in science and scholarship has stressed the increasing need for reference tools which provide non-native, especially junior, researchers with useful information about the collocational patterns as well as conventionalized phraseology of non-technical words prototypical of specialized discourses. Within this lexicographic trend, the GRELIC research group research (GRELIC) has developed SciE-Lex, a lexicographic tool which includes morphosyntactic and contextual information about the combinatory potential of general words commonly used in biomedical discourse. This article serves the purpose of checking the validity of SciE-Lex by means of a qualitative survey distributed among a group of experts who were asked to explore and rate the dictionary by highlighting its weaknesses and strengths. The analysis of their reports have revealed interesting findings with respect to the adequacy of SciE-Lex as a lexicographic tool addressed to the biomedical community as well as to the appropriateness of the inclusion of contextual information of non-specialized terms prototypical of biomedical discourse. This pre-evaluation has informed the revision process of the dictionary and thus has greatly contributed to verifying its usefulness.",Research report on the adequacy of SciE-Lex as a lexicographic tool for the writing of biomedical papers in English,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
693,http://dx.doi.org/10.1093/llc/fqy016,"{'string_id': '10.1093/llc/fqy016', 'id_scheme': 'DOI'}","Elena Ferrante is a pen name known worldwide, authoring novels such as the bestseller My Brilliant Friend. A recent study indicates that the true author behind these books is probably Domenico Starnone. This study aims to select a set of approved authorship methods and appropriate feature sets to prove, with as much certainty as possible, that this conclusion is correct. To achieve this, a corpus of contemporary Italian novels has been generated, containing 150 books written by forty authors (including seven by Ferrante). Six authorship identification models have been applied to this data set (Delta, Labbé’s distance, nearest shrunken centroids (NSC), naïve Bayes, k-nearest neighbors, and character n-grams). Using either an instance- or profile-based matching technique, the same result (Starnone) appears very often in first place. Modifying the feature set to include between 50 and 2,000 of the most frequent tokens or lemmas does not change this result. When removing Starnone’s novels from the corpus, all approved attribution methods tend to indicate different names as the most probable author. This result confirms not only that the outputs of these methods are independent but also that the true author is certainly Starnone. Finally, a lexical analysis reveals the reasons justifying this conclusion.",Is Starnone really the author behind Ferrante?,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,33,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
694,http://dx.doi.org/10.1093/llc/fqy018,"{'string_id': '10.1093/llc/fqy018', 'id_scheme': 'DOI'}","Cultural analysts are currently faced with an ecosystem of heterogeneous, globally distributed, and mass-scale sources that cannot be ignored in the research process. In addition to that, cultural analysts need to deal with the emergence of a knowledge economy which is no longer based on the value of the information entities contained in one or more documents, but on the potential for heterogeneous data to be recombined to generate previously unknown knowledge. Therefore, the challenge of accessing a set of heterogeneous, mass-scale, dynamic, and globally distributed sources has been joined to that of transforming the content of these sources into reusable data for the creation of knowledge and value. To the traditional question—What sources should be consulted or selected?—must now be added: What tools should be built and what work procedures should be designed to access those sources on a mass basis and analyze them as data? This article aims to provide some possible answers through the work carried out within the Exhibitium Project. This included the design and implementation of the Expofinder system, which is a technological device meant to examine the mechanisms for dissemination of digital information about art exhibitions and reuse this information as data to generate new knowledge and new interpretations about them. We believe that one of the best ways to strengthen the paths of digital research is to make the methodologies and mechanisms that govern decision-making transparent in order to be discussed or to be adopted by other projects. This is the ultimate goal of this article.",Development of technological ecosystems for cultural analysis: The case of Expofinder system and art exhibitions,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
695,http://dx.doi.org/10.1093/llc/fqy019,"{'string_id': '10.1093/llc/fqy019', 'id_scheme': 'DOI'}","The integration of context-aware ubiquitous learning (CAUL) into English for Specific Purposes (ESP) learning has become increasingly widespread due to the enhanced interaction between learners and situated contexts. Using an authentic learning site on environmental protection and its related discourse-specific English as target knowledge, this study analyzed the learning needs for developing ESP knowledge and skills through the use of smartphones and QR codes. Different need choices of the four language skills, requirements of CAUL system design, and effective audio–visual materials for social interaction are discussed. Based on previous reviewed arguments and current empirical arguments, the quantitative results reveal significant findings in ESP material design, content knowledge, and effective methods for CAUL. Further, qualitative interview results are classified into technological viewpoints and practical infield viewpoints. Consequently, identified themes and contradictions among three target groups—experts, students, and guides—are explained in terms of different pursuits of context-specific English learning needs.",Needs analysis for an ESP case study developed for the context-aware ubiquitous learning environment,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
696,http://dx.doi.org/10.1093/llc/fqy020,"{'string_id': '10.1093/llc/fqy020', 'id_scheme': 'DOI'}","Digital technology is drawing more and more research attention in many areas of humanities because of its advantages in objectivity and automation. This work was an attempt to analyze two protagonists’ personality traits and development, through their dialogues in a Chinese novel Ordinary World. We used language technology LTP, a Simplified Chinese segmentation software, to segment dialogues. Then, on the basis of eighty-eight LIWC (Linguistic Inquiry and Word Count) features, personality predictive models were recruited to calculate the Big Five personality. We obtained two sets of predicted Big Five personality scores through dialogues in general and those before versus after the protagonist’s life events (high-school graduation and marriage). As expected, the two protagonists’ (Shaoping Sun and Shaoan Sun) Big Five personality traits were coincided with the portraits in the novel, and the two protagonists showed observable personality development after their life events, which was consistent with previous studies on personality change. This work demonstrates the applicability and validity of literary intelligence analysis in humanistic texts, suggesting a reliable approach to analyze novel protagonists’ personality in an objective manner.",Literary intelligence analysis of novel protagonists’ personality traits and development,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
697,http://dx.doi.org/10.1093/llc/fqy021,"{'string_id': '10.1093/llc/fqy021', 'id_scheme': 'DOI'}","Academic writing training in various forms has been developed to enhance writing knowledge and skills for graduate students at universities. However, few studies have targeted comparative learning analysis of the Introduction and Method sections in terms of genre structure and language use with the support of technology in the humanities and social sciences contexts. The present study designed a 13-week blended English genre-based writing instruction (GBWI) workshop with an online writing tutorial system—EJP-Write, in conjunction with the peer review and discovery-based learning approaches. This GBWI workshop was designed to help twenty-five graduate students from nine fields of the humanities and social sciences disciplines enhance their genre structure and language use of the Introduction and Method sections in academic writing. The results showed that although the participants benefited from GBWI, their improvements were not significant. The students further reported that the EJP-Write coupled with the peer review and discovery-based learning activities did not completely meet their requirements and anticipations. Overall, this study contributes to aid scholars and educators in better understanding how English as a foreign language (EFL) graduate students respond to GBWI with an integrated system for developing the knowledge and skills of academic writing. Moreover, these preliminary results can be used as guidance to further strengthen the effectiveness and efficiency of future GBWI research.",Genre-based writing instruction blended with an online writing tutorial system for the development of academic writing,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
698,http://dx.doi.org/10.1093/llc/fqy023,"{'string_id': '10.1093/llc/fqy023', 'id_scheme': 'DOI'}","The R Stylo program features, Rolling Delta and Rolling Classify, were applied to Thomas Kyd’s closet drama Cornelia. After the elimination of a large number of unsuitable reference texts, Marlowe’s Tamburlaine 1 turned out to be the play with the lowest delta values; that is it showed the smallest stylistic difference from Cornelia. In previous investigations the anonymous play The Tragedy of Locrine had been identified as a play by Christopher Marlowe (see Appendix). In a double check the procedures were repeated with Locrine, and it was Locrine in particular that came to the foreground. This was confirmed by traditional stylometric measurements like bootstrap consensus trees, multivariate analyses, and multidimensional scaling. Craig’s Zeta located Cornelia’s preferred vocabulary in the vicinity of the Marlowe compound. The safe conclusion is that Cornelia’s real provenance is Christopher Marlowe, who had shared lodgings with Kyd before he was killed by fellow government agent Frizer in Deptford on 30 May 1593. ",Forensic stylometry,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
699,http://dx.doi.org/10.1093/llc/fqy024,"{'string_id': '10.1093/llc/fqy024', 'id_scheme': 'DOI'}","Love is the most significant subject of mystical path. This study explores all lines of Saadi’s lyric poems. Different words applied as alternatives of love were classified in twelve categories. To compare the frequency of different categories and words that were used as alternatives of love, the chi-square goodness-of-fit test was separately used. Then, using K-means clustering method, these alternatives were clustered in three categories (high frequency, medium frequency, and low frequency). The results indicated that the words Fire and Pain and the categories of Sickness, Human, and Heat had the highest uses as the alternatives of love in Saadi's lyric poems.",On comparing and clustering the alternatives of love in Saadi's lyric poems (Ghazals),Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
700,http://dx.doi.org/10.1093/llc/fqy025,"{'string_id': '10.1093/llc/fqy025', 'id_scheme': 'DOI'}","We propose a novel way to create categorized discourse lexicons for multiple languages. We combine information from the Penn Discourse Treebank with statistical machine translation techniques on the Europarl corpus. Using gender profiling as an application, we evaluate our approach by comparing it with an approach using features from a knowledge-based lexicon and with an Rhetorical structure theory (RST) discourse parser. Our experiments are performed on corpora for three languages (English, Dutch, and German) in two genres (news and blogs). We include a feature analysis in which we look for (in)consistencies of discourse features related to male and female authors between the different experimental settings.",Discourse lexicon induction for multiple languages and its use for gender profiling,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
701,http://dx.doi.org/10.1093/llc/fqy026,"{'string_id': '10.1093/llc/fqy026', 'id_scheme': 'DOI'}","The present study exemplifies an action research-based approach to developing learner autonomy in learning productive vocabulary in an English as a foreign language (EFL) setting. We conducted two cycles of teaching actions as interventions to solve immediate learning problems. These actions involved Evernote-aided learning and activities of word guessing, gap noticing, and phonetic drilling. The results of vocabulary tests and interviews were analysed to measure and verify the outcomes of the interventions. Our results reveal that, by the end of the research, learners took initiative in learning productive vocabulary. They became more self-conscious of the knowledge involved in a productive word and more capable of managing vocabulary learning independently. Reflections upon the teachers’ role, their actions in the teaching practice, and the relationship between learner autonomy and vocabulary learning are discussed.",Autonomous learning of productive vocabulary in the EFL context: An action research approach,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
702,http://dx.doi.org/10.1093/llc/fqy028,"{'string_id': '10.1093/llc/fqy028', 'id_scheme': 'DOI'}","In this article, we describe and contrast relevant properties of two electronic registers of Jakarta Indonesian instant messaging, or SMS, and Twitter against a continuum of conversation and writing features. Many linguists (Crystal, 2008, Txting: the Gr8 Db8. Oxford University Press; Carter and McCarthy, 2015, Applied Linguistics, 38(1): 1–20; McWhorter, 2013; Sindoni, 2013, Spoken and Written Discourse in Online Interactions. New York, NY; London: Routledge) have turned their attention to the properties of language evidenced in various social media. Taken as a whole, electronic media have become the predominant channel of non-speech language interaction (CNN.com reports (11/3/2015) that teens spend nine hours a day on electronic media: www.cnn.com/2015/11/03/health/). However, the study of these media is still in its early stages and, in our opinion, suffers from two flaws: instant messaging is relatively understudied, given its prevalence; and contrastive studies of different media are also rare (but cf. Danet and Herring’s, 2007, Handbook of Language and Communication: Diversity and Change. Handbook of Applied Linguistics, vol. 9. Berlin: Mouton De Gruyter, review of genre-specific analyses). Similarly, most linguistic studies we are aware of take European languages as the object of study; since any contrastive or variationist study takes the grammatical features of some standard as its point of departure, insights into register-based variation are limited by the typological features of the language. As a highly isolating, free word order language, Indonesian represents a typologically very different basic grammar from which to view register-based variation. This study contributes to the broadening of linguistic study of social media by contrasting rather than conflating two media, demonstrating that different register properties are associated with distinct social media platforms.",Distinguishing properties of SMS and Twitter in Indonesian: A contrastive study,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
703,http://dx.doi.org/10.1093/llc/fqy029,"{'string_id': '10.1093/llc/fqy029', 'id_scheme': 'DOI'}","MOOC appearance has produced, in a first phase, more discussions than contributions. Despite pessimistic opinions or those catastrophic foreseeing the end of the classic education by accepting MOOC, the authors consider that, as it is happening in all situations when a field is reformed, instead of criticism or catastrophic predictions, an assessment should be simply made. MOOC will not be better or worse if it is discussed and dissected but can be tested in action, perfected by results, or abandoned if it has no prospects. Without testing, no decision is valid. A similarity between the MOOC appearance and the appearance of the idea of flying machines heavier than air can be made. In the flight case, the first reaction was a strong negation (including at Academies level) and only performing the first independent flight with an apparatus heavier than air has shifted orientation from denial to contributions. So, practical tests clarified the battle between ideas. The authors of this article encourage the idea of testing–assessment and, therefore, imagined and proposed one software for quickly assess whether MOOC produces changes in knowledge, by simply transferring courses from ‘face-to-face’ environment into the virtual one. Among the methods of statistical analysis for student behavioral changes was chosen the Keppel method. It underpins the assessment method of this work being approached using both the version with one variable and also with three variables. It is intended that this attempts to pave the way for other series of rapid assessment regarding MOOC effects (using other statistical methods). We believe, that this is the only approach that can lead either to improve the system or to renunciation.",Evaluation software for effects produced by MOOC in mediums with different linguistically levels,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
704,http://dx.doi.org/10.1093/llc/fqy030,"{'string_id': '10.1093/llc/fqy030', 'id_scheme': 'DOI'}","Statistics and data analysis provide exciting approaches for extracting knowledge from data. Recently, statistics and data analysis were sought to be exploited in many research fields. In this study, statistics is applied to religious studies. Thirteen different orders of Quran’s revelation are considered. To start with, by using regression analysis, the similarity between these orders is investigated. The results show that all of the orders are highly related (R2 > 0.7, P < 0.001). Then, the hierarchical clustering method is used to cluster the orders. The results indicate that based on the similarity, different orders can be clustered in two groups; Cluster 1: ‘Ebn Abbas’, ‘Al-Azhar’, ‘Jaber’, ‘Ebn Nadim’, ‘Khazan’, and ‘Hazrat Ali’; Cluster 2: ‘Blachère’, ‘Bazargan’, ‘Nöldeke’, ‘Grimm’, ‘Muir’, ‘E'temad-al-Saltane’, and ‘Davood’. The results also determined that all of the orders are highly similar (mean of similarity >80%).",Statistical analysis about the order of Quran’s revelation,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
705,http://dx.doi.org/10.1093/llc/fqy031,"{'string_id': '10.1093/llc/fqy031', 'id_scheme': 'DOI'}","Recent research has demonstrated the potential of corpus linguistics as a solid aid in children’s understanding of how language works. However, the availability of data from the UK is still somewhat limited. Most corpora are either based on a small number of schools, synchronic in nature, or focused on the post-National Curriculum era (cf. Lancaster Corpus of Children’s Project Writing, the Oxford Children’s Corpus, the Growth in Grammar Corpus); on the other hand, historical corpora are, unfortunately, not publicly available in electronic format (cf. the Child Language Survey or the Aspects of Writing in 16+ English Examinations, Cambridge Assessment). This article introduces the APU Writing and Reading Corpus 1979–1988, a new large electronic data set of historical materials which are linguistically annotated. The aim is two-fold. First, to describe the contents of the corpus and its compilation procedure. Second, to illustrate its potential as a research and pedagogical tool by presenting a number of research case studies and teaching materials that are currently being developed based on the corpus data. All in all, the Assessment Performance Unit (APU) Corpus contributes to both Corpus Linguistics and Educational Linguistics by presenting itself as a new resource tool with replicable methodology and objective empirical evidence, which will be of interest to academics and school teachers as well as to school material developers and policymakers.","‘he liked to read, write, and whatch televishon’—The APU Writing and Reading Corpus (1979–1988)",Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
706,http://dx.doi.org/10.1093/llc/fqy032,"{'string_id': '10.1093/llc/fqy032', 'id_scheme': 'DOI'}","Purpose—To identify new information barriers created in the process related to the formation and transfer of humanistic scientific knowledge. Methodology—The reanalysis of the data collected during the earlier research projects. Results—Information barriers were identified, which are created by incomplete bibliographic databases, some solutions used in publishing on the Internet, the low level of information competencies of many humanists, their adverse attitude to open access, and by the education system that prefers knowledge transmission in the linear and text form. Conclusion—New information barriers effectively reduce research capabilities of many humanists, thereby creating a communication barrier between traditional and digital humanists.",Information infrastructure of contemporary humanities and the digital humanities development as a cause of creating new information barriers. A Polish case,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
707,http://dx.doi.org/10.1093/llc/fqy033,"{'string_id': '10.1093/llc/fqy033', 'id_scheme': 'DOI'}","Man is a Tool-making Animal (Attributed to Benjamin Franklin by Samuel Johnson) Although digital tools built by those outside the Digital Humanities (DH) community (particularly tools for big data analysis or social network analysis) currently take a significant amount of DH attention these days, there has been toolmaking inside the DH for about 70 years. This article’s author has acted as a tool developer from time to time over many years, and this work has been part of his personal experience of DH from the 1970s to the present day (some history of his involvement in DH can be found in Bradley and Nylan, 2016). Text Analysis...",Digital tools in the humanities: Some fundamental provocations?,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
708,http://dx.doi.org/10.1093/llc/fqy034,"{'string_id': '10.1093/llc/fqy034', 'id_scheme': 'DOI'}","Nowadays, text summarization is one of the most important active research fields in information retrieval. The most of the supervised extractive summarization systems utilize learning-to-rank methods to score sentences according to their importance. They need a high-quality comprehensive summarization corpus, which is labeled manually by human experts. Unfortunately, this sort of corpus is not available for most low-resource languages such as Persian. In this study, first of all, a comprehensive human-labeled summarization corpus (called Bistoon) collected by the crowdsourcing approach is introduced, and then a Persian summarizer based on a novel semi-supervised summarization approach, which is a combination of co-training and self-training, is presented to overcome the absence of sufficient data. During an iterative process, the proposed system is learned by Bistoon corpus and applied to unlabeled texts to generate the most confident summaries. These summaries are added to Bistoon for more iterations. During iterations, the training corpus is grown and the quality of the summarizer is simultaneously improved. The proposed system has been compared to other well-known Persian summarizers over the Pasokh and Bistoon standard test data sets. The evaluation results show the superiority of our methods in terms of precision, F-measure, Rouge metrics, and also human judgments.",Katibeh: A Persian news summarizer using the novel semi-supervised approach,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
709,http://dx.doi.org/10.1093/llc/fqy035,"{'string_id': '10.1093/llc/fqy035', 'id_scheme': 'DOI'}","We compare the scope of museum digitization in the Russian Federation, a country with diverse cultural heritage and over 2,300 museums, with the scope of digitization in Europe as measured by the Enumerate Survey of 355 museums from twenty European countries initiated by the Collections Trust, UK, in 2011. Our article shows that the reach and scope of digitization in Russia is lesser than that of European museums. Digitization is mainly done in Russia for inventory purposes. The share of digitized objects published online is comparable to that in Europe if we consider images published on museum websites; however, much content from Russia is not licensed as reusable, partly due to the different legal framework that exists there. The article challenges the perceptions that global heritage collections are becoming more visible and accessible. It shows that future digital analysis of cultural heritage may be only possible with corpora of images provided by museums that publish numerous images from their digital collections online while pursuing the policies of free image reuse alongside open licensing. Such corpora may not be found beyond a limited number of Western collections, which may result in excluding many cultures from humanities research.",Accessing Russian culture online: The scope of digitization in museums across Russia,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
710,http://dx.doi.org/10.1093/llc/fqy036,"{'string_id': '10.1093/llc/fqy036', 'id_scheme': 'DOI'}","Some argue that some religious books, Prophetic Traditions in particular, are adulterated, but this was sorted out by introducing a manual methodology for identifying authentic and non-authentic texts. The methodology is not precise and has become time-consuming and a daunting process today. A general motivation for many recent studies has been the desire to automate some descriptive processes and employ scientific observation in authorship identification/verification. Using methods of Authorship Verification to examine sensitive texts, like established religious texts, could challenge the existing paradigms in Islamic discourse and any religion that has sacred texts. However, it could lead to the refining of these texts and eliminate any contested belief/s. The selected linguistic features are used to measure the validity of morphological structures (word length), syntactic structure (word type), and lexical richness across the disputed and undisputed Hadiths. Using the three linguistic measures together enabled us to draw each Hadith in a three-dimensional space, with word length plotted along lexical richness and word type. The results for the selected Hadiths that have been measured showed marked differences that could be augmented by examining further features.",Authorship verification of disputed Hadiths in Sahih al-Bukhari and Muslim,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
711,http://dx.doi.org/10.1093/llc/fqy037,"{'string_id': '10.1093/llc/fqy037', 'id_scheme': 'DOI'}","In this article, an innovative approach to perform the sentiment analysis (SA) has been presented. The proposed system handles the issues of Romanized or abbreviated text and spelling variations in the text to perform the sentiment analysis. The training data set of 3,000 movie reviews and tweets has been manually labeled by native speakers of Hindi in three classes, i.e. positive, negative, and neutral. The system uses WEKA (Waikato Environment for Knowledge Analysis) tool to convert these string data into numerical matrices and applies three machine learning techniques, i.e. Naive Bayes (NB), J48, and support vector machine (SVM). The proposed system has been tested on 100 movie reviews and tweets, and it has been observed that SVM has performed best in comparison to other classifiers, and it has an accuracy of 68% for movie reviews and 82% in case of tweets. The results of the proposed system are very promising and can be used in emerging applications like SA of product reviews and social media analysis. Additionally, the proposed system can be used in other cultural/social benefits like predicting/fighting human riots.",A sentiment analysis system for social media using machine learning techniques: Social enablement,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
712,http://dx.doi.org/10.1093/llc/fqy038,"{'string_id': '10.1093/llc/fqy038', 'id_scheme': 'DOI'}","Zeta has been described as the most powerful general-purpose authorship tool currently available. It is therefore of the utmost importance that Zeta test results be correctly interpreted, because incorrect interpretations can lead to incorrect authorship attributions. This article argues that the current method of interpreting Zeta results, pioneered by Craig and Kinney (2009) in Shakespeare, Computers, and the Mystery of Authorship and used in the Authorship Companion to The New Oxford Shakespeare, is unsound. The article provides theoretical arguments and a counterexample to demonstrate this. Moreover, the article argues that the validation of the Zeta method that has so far been performed is less strong than it appears, being based on a misunderstanding of what the results are telling us. It suggests that conclusions apparently drawn from Zeta, that shorter n-grams and function word skip bigrams make better authorial markers, are unwarranted. Finally, the article presents the results of new Zeta tests, which show that the method is less reliable than has been supposed.",The interpretation of Zeta test results,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
713,http://dx.doi.org/10.1093/llc/fqy039,"{'string_id': '10.1093/llc/fqy039', 'id_scheme': 'DOI'}","Zeta has been described as ‘the most powerful general-purpose authorship tool currently available.’ It has been used to attribute parts of Arden of Faversham to Shakespeare and parts of 3 Henry VI to Marlowe, among other uses. The method was invented by John Burrows, but it is currently used in an adapted form developed by Hugh Craig. This article demonstrates that the method has not been adapted into its simplest form, thereby obscuring a true understanding of what it does. The article proposes an improvement to the method, which makes it easier to implement and may also help to improve researchers’ understanding of it, without affecting the results of Zeta tests already completed.",An improvement to Zeta,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
714,http://dx.doi.org/10.1093/llc/fqy040,"{'string_id': '10.1093/llc/fqy040', 'id_scheme': 'DOI'}","In ‘Christopher Marlowe: Hype and Hoax’(2018), Hartmut Ilsemann implies that his application of the Rolling Delta feature of R Stylo is sufficiently robust that a century and a half of traditional scholarship should be overturned, and Marlowe stripped of the majority of his canon, including Doctor Faustus and Edward II. The article concludes that ‘Marlowe is totally overrated in his influence on modern English drama’ (p. 26), the natural consequence of stripping away 5/7ths of his canon. In this response, I demonstrate that the assumptions underlying this application of the Delta method, and the application itself, are fundamentally flawed, leading to predictably erroneous conclusions. Problems with the study include a poorly designed test environment, incorrect preparation of texts, assuming that ‘Marlowe’s style’ can be determined by a single early play, selecting and constructing Shakespeare’s comparison texts in a manner likely to prejudice results, ignoring the effect upon style of a play’s date and genre, failing to consider the effect of different-length comparison texts, and dismissing external evidence of authorship that conflicts with the test outcomes. I argue that in the light of these issues, the results and conclusions must be dismissed. Further, the question is raised as to whether the current methods of computational stylistics, even when more rigorously applied, are equipped to challenge the attribution of the accepted Marlowe canon.1",Marlowe and overreaching: A misuse of stylometry,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
715,http://dx.doi.org/10.1093/llc/fqy042,"{'string_id': '10.1093/llc/fqy042', 'id_scheme': 'DOI'}","There is a long-standing debate about the authorship of the Bixby Letter, one of the most famous pieces of correspondence in American history. Despite being signed by President Abraham Lincoln, some historians have claimed that its true author was John Hay, Lincoln’s personal secretary. Analyses of the letter have been inconclusive in part because the text totals only 139 words and is thus far too short to be attributed using standard methods. To test whether Lincoln or Hay wrote this letter, we therefore introduce and apply a new technique for attributing short texts called ‘n-gram tracing’. After demonstrating that our method can distinguish between the known writings of Lincoln and Hay with a very high degree of accuracy, we use it to attribute the Bixby Letter. We conclude that the text was authored by John Hay—rewriting this one episode in the history of the USA, while offering a solution to one of the most persistent problems in authorship attribution.",Attributing the Bixby Letter using n-gram tracing,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
716,http://dx.doi.org/10.1093/llc/fqy043,"{'string_id': '10.1093/llc/fqy043', 'id_scheme': 'DOI'}","In the summer of 2012, the Institute of Fine Arts, New York University Selinunte Mission began to explore the interior of the cella of Temple R. This excavation showed that the classical and archaic layers had been sealed by a deep fill of the Hellenistic period and left untouched by earlier archaeological research at the site. Among the discoveries were a series of votive depositions positioned against the walls, dating to the sixth century BCE. One of the most striking finds among those votive depositions was the discovery of two parts of a bone aulos, which can be dated to 570 BCE. The virtual reconstruction of the aulos found in Temple R at Selinunte aims to increase and improve its scientific investigation, overcoming the limitations caused by the fragility of the instrument. Digital technology has allowed us to produce a three-dimensional (3D) model of the aulos. This digital model has been translated into a 3D artificial copy, using polymer as a material. Our goal is to reconstruct the aulos, after analysing its organological characteristics. We also hope that this new study of the aulos will increase our knowledge of Ancient Greek music.",Towards a new approach in the study of Ancient Greek music: Virtual reconstruction of an ancient musical instrument from Greek Sicily,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
717,http://dx.doi.org/10.1093/llc/fqy044,"{'string_id': '10.1093/llc/fqy044', 'id_scheme': 'DOI'}","To assist legal professionals with more effective information processing and evaluation, we aim to develop software to identify and visualize the key information dispersed in the unstructured language data of a criminal case. A preliminary model of the software, Worldbuilder, is described in Wang et al. (2016a, b). The present article focuses on explaining the theory and vision behind the computational development of the software, which has involved establishing a means to annotate discourse for visualization purposes. The design of the annotation scheme is based on a cognitive model of discourse processing, Text World Theory (TWT), which describes and tracks how language users create a dynamic representation of events (i.e. text-worlds) in their minds as they communicate. As this is the first time TWT has informed the computational analysis of language, the model is augmented with Contextual Frame Theory, among other linguistic apparatus, to account for the complexities in the data and its translation from text to visualization. Using a statement from the Meredith Kercher murder trial as a case study, we illustrate the efficacy of the augmented TWT framework in the careful and purposeful preparation of linguistic data for computational visualization. Ultimately, this research bridges Cognitive and Computational Linguistics, improves the TWT model’s analytical accuracy, and yields a potentially useful tool for forensic work.",Text-world annotation and visualization for crime narrative reconstruction,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
718,http://dx.doi.org/10.1093/llc/fqy047,"{'string_id': '10.1093/llc/fqy047', 'id_scheme': 'DOI'}","Four letters in the Adventurer are currently attributed to Johnson, who allegedly disguised his style so that they could be plausibly ascribed to his friend Richard Bathurst. A stylometric analysis, supported by internal evidence, finds the case for disguise implausible, and suggests that the letters are a collaboration between Johnson and Bathurst.","Johnson, ‘Misargyrus’, and Richard Bathurst",Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
719,http://dx.doi.org/10.1093/llc/fqy048,"{'string_id': '10.1093/llc/fqy048', 'id_scheme': 'DOI'}","In the past decade, an increasing set of digital tools has been developed with which digital sources can be selected, analyzed, and presented. Many tools go beyond key word search and perform different types of analysis, aggregation, mapping, and linking of data selections, which transforms materials and creates new perspectives, thereby changing the way scholars interact with and perceive their materials. These tools, together with the massive amount of digital and digitized data available for humanities research, put a strain on traditional humanities research methods. Currently, there is no established method of assessing the role of digital tools in the research trajectory of humanities scholars. There is no consensus on what questions researchers should ask themselves to evaluate digital sources beyond those of traditional analogue source criticism. This article aims to contribute to a better understanding of digital tools and the discussion of how to evaluate and incorporate them in research, based on findings from a digital tool criticism workshop held at the 2017 Digital Humanities Benelux conference. The overall goal of this article is to provide insight in the actual use and practice of digital tool criticism, offer a ready-made format for a workshop on digital tool criticism, give insight in aspects that play a role in digital tool criticism, propose an elaborate model for digital tool criticism that can be used as common ground for further conversations in the field, and finally, provide recommendations for future workshops, researchers, data custodians, and tool builders.",Toward a model for digital tool criticism: Reflection as integrative practice,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
720,http://dx.doi.org/10.1093/llc/fqy049,"{'string_id': '10.1093/llc/fqy049', 'id_scheme': 'DOI'}","This article explores the linguistic landscape of social media posts associated with specific geographic locations using computational methods. Because physical and virtual spaces have become increasingly intertwined due to location-aware mobile devices, we propose extending the concept of linguistic landscape to cover both physical and virtual environments. To cope with the high volume of social media data, we adopt computational methods for studying the richness and diversity of the virtual linguistic landscape, namely, automatic language identification and topic modelling, together with diversity indices commonly used in ecology and information sciences. We illustrate the proposed approach in a case study covering nearly 120,000 posts uploaded on Instagram over 4.5 years at the Senate Square in Helsinki, Finland. Our analysis reveals the richness and diversity of the virtual linguistic landscape, which is also shown to be susceptible to continuous change.",Exploring the linguistic landscape of geotagged social media content in urban environments,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
721,http://dx.doi.org/10.1093/llc/fqy050,"{'string_id': '10.1093/llc/fqy050', 'id_scheme': 'DOI'}","Statistical approaches have become the mainstream in machine translation (MT), for their potential in producing less rigid and more natural translations than rule-based approaches. However, on closer examination, the uses of function words between statistical machine-translated Chinese and the original Chinese are different, and such differences may be associated with translationese as discussed in translation studies. This article examines the distribution of Chinese function words in a comparable corpus consisting of MTs and the original Chinese texts extracted from Wikipedia. An attribute selection technique is used to investigate which types of function words are significant in discriminating between statistical machine-translated Chinese and the original texts. The results show that statistical MT overuses the most frequent function words, even when alternatives exist. To improve the quality of the end product, developers of MT should pay close attention to modelling Chinese conjunctions and adverbial function words. The results also suggest that machine-translated Chinese shares some characteristics with human-translated texts, including normalization and being influenced by the source language; however, machine-translated texts do not exhibit other characteristics of translationese such as explicitation.",Function words in statistical machine-translated Chinese and original Chinese: A study into the translationese of machine translation systems,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
722,http://dx.doi.org/10.1093/llc/fqy051,"{'string_id': '10.1093/llc/fqy051', 'id_scheme': 'DOI'}","Although recent research acknowledges the potential of visualization methods in digital humanities (DH), the predominant terminology used to describe visualizations (prototypes and tools) focuses on their use as a means to an end and, more importantly, as an instrument in the service of humanities research. We introduce the sandcastle as a metaphorical lens and provocative term to highlight visualization as a research process in its own right. We argue that building visualization sandcastles provides a holistic approach to cross-disciplinary knowledge generation that embraces visualization as (1) an aesthetic provocation to elicit critical insights, interpretation, speculation, and discussions within and beyond scholarly audiences, (2) a dynamic process wherein speculation and re-interpretation advance knowledge within all disciplines involved, and (3) a mediator of ideas and theories within and across disciplines. Our argument is grounded in critical theory, DH, design, human–computer interaction, and visualization, and based on our own research on an exceptional literary collection. We argue that considering visualizations as sandcastles foregrounds valuable insights into the roles of visualization as a mindset, methodology, and praxis within humanities research and beyond.",In defense of sandcastles: Research thinking through visualization in digital humanities,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,Supplement_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
723,http://dx.doi.org/10.1093/llc/fqy052,"{'string_id': '10.1093/llc/fqy052', 'id_scheme': 'DOI'}","Library-based publishing initiatives are on the rise in a rapidly diversifying scholarly publishing ecosystem. This article presents selected results from a US-based survey on the needs of humanities scholars in a contemporary publishing environment, emphasizing survey responses that shed light on key aspects of access for scholars seeking to publish: access to support services, access to content, and access to audience. Survey responses suggest a profile of the authors for whom libraries are poised to offer attractive publishing solutions: (1) those whose scholarship is not sufficiently represented in the print medium and (2) those who place a high value on the technological affordances provided by open-access digital scholarship to reach their intended audiences. Compared to other publishing models, situating support for scholarly communication in the research library creates opportunities for addressing challenges related to access and sustainability in the digital scholarly publishing.",Informing library-based digital publishing: Selected findings from a survey of scholars' needs in a contemporary publishing environment,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,Supplement_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
724,http://dx.doi.org/10.1093/llc/fqy053,"{'string_id': '10.1093/llc/fqy053', 'id_scheme': 'DOI'}","A language-independent stemmer has always been looked for. Single N-gram tokenization technique works well; however, it often generates stems that start with intermediate characters, rather than initial ones. We present a novel technique that takes the concept of N-gram stemming one step ahead and compare our method with an established algorithm in the field, say, Porter’s stemmer for English, Spanish, and Portuguese languages. Results indicate that our N-gram stemmer is comparable with the Porter’s linguistic stemmer.","Generation, implementation, and appraisal of an N-gram-based stemming algorithm",Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
725,http://dx.doi.org/10.1093/llc/fqy055,"{'string_id': '10.1093/llc/fqy055', 'id_scheme': 'DOI'}","During World War I (WWI), between 1916 and 1917, Robert Musil was the chief editor of the Tiroler Soldaten-Zeitung in Bozen. This activity probably also involved authorship of articles and has posed a philological problem to scholars, who have not been able to attribute with certainty a range of relatively short texts to Musil. With this article, we present a new approach that combines philological research with stylometric methods. Exploration of WWI archives and digitization of historical documents were paired with application of authorship attribution techniques, following extensive evaluation. To build the training set, we adapted the ‘impostors method’ by grouping three ‘distractor authors’ (similar to Musil in terms of style) and three actual candidates for authorship. In the test set, we developed two designs for tackling the issue of text length: a combinatory design, where longer chunks were composed by the juxtaposition of short texts; a simplified design, where the texts for attribution were merged with already attributed texts. Results of our experiment suggest that Musil attribution may be disproved with a high level of confidence for ten texts that were more probably written by a less well-known author, Albert Ritter. We carried out a keyness analysis on the specific words preferred or avoided by the two authors, which not only corroborated the results of the quantitative analysis but also findings from Musil philology. Our study showcases the potentialities of using mixed methods in stylometry.","Robert Musil, a war journal, and stylometry: Tackling the issue of short texts in authorship attribution",Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
726,http://dx.doi.org/10.1093/llc/fqy059,"{'string_id': '10.1093/llc/fqy059', 'id_scheme': 'DOI'}","We have designed an ontology to index a corpus of digital literature works. We have given this ontology the shape of a memory island, a navigable virtual territory where categories are regions and descriptor places, and where archives of these ephemeral works are made accessible.",An ontology and a memory island to give access to digital literature works,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,Supplement_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
727,http://dx.doi.org/10.1093/llc/fqy060,"{'string_id': '10.1093/llc/fqy060', 'id_scheme': 'DOI'}","This article describes methods that can be used to construct a Latin morphological and syntactic parser, including resources that enable reordering the sentence according to the SVO (subject + verb + object) standard order, aiming to facilitate the translation of Latin phrases by users. The article also addresses the solutions that can be implemented to overcome some of the problems found in this context, namely, the difficulties in accessing different forms of dictionarization, the lack of lexicographic resources, and a methodology for processing the ‘special constructions’ required by adverbs and adjectives.",Deciphering Latin sentences using traditional linguistic resources,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
728,http://dx.doi.org/10.1093/llc/fqy061,"{'string_id': '10.1093/llc/fqy061', 'id_scheme': 'DOI'}","In the late evening of 18 March 2014, students and activists stormed into and occupied the main chamber of Taiwan's Legislature. The event set off the Sunflower Movement, signifying a turning point in Taiwan's recent history. Researchers at Academia Sinica arranged to acquire all the supporting artifacts and documentary materials in the chamber before the protest came to a peaceful end. In this article, we discuss the issues in archiving and making available to the public a large collection of artifacts created by thousands of participants during a contemporary event. We demonstrate systems designed to encourage people to identify objects of their own in the archive. We show how an accessible catalog to the archive can help people tell their stories, hence collectively may strengthen the public's recollections about the movement.",Remembrance of contemporary events: On setting up the Sunflower Movement Archive,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,Supplement_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
729,http://dx.doi.org/10.1093/llc/fqy062,"{'string_id': '10.1093/llc/fqy062', 'id_scheme': 'DOI'}","Recent advances in data science and machine learning have enhanced our ability to analyze and understand the structure of social interactions in fictional stories by using formal and quantitative approaches. However, an objective assessment of these aspects of fictional stories remains a relatively new and technically difficult field. In this brief report, we introduce our study in which we modeled story dynamics from a novel perspective. By implementing a relational event model based on a two-mode network framework in an analytical system for movie scripts, we examined the interdependence of character activities and their participation in a given plot on the basis of an analysis of more than 900 scripts from the Internet Movie Script Database.",Representing stories as interdependent dynamics of character activities and plots: A two-mode network relational event model,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
730,http://dx.doi.org/10.1093/llc/fqy063,"{'string_id': '10.1093/llc/fqy063', 'id_scheme': 'DOI'}","Scattered throughout the International Tracing Service (ITS) digital archive, one of the largest and most heterogeneous collections of Holocaust-related material, are hundreds of thousands of reference cards to official death certificates recording a fraction of individuals who perished within concentration camps. These cards represent the most comprehensive collection of digital material pertaining to these death certificates issued by Sonderstandesamt Arolsen, a German civil registry office. However, the reference cards can only be found dispersed throughout the Central Name Index (CNI), ITS’s 46+ million-card finding aid that is indexed only by name. Consequently, aggregating the death certificate reference cards for research requires an intractable manual search. I adopt template matching and machine learning to automate the retrieval of these cards from the ITS digital archive. I demonstrate the efficacy of my method on a test set of 22,117 hand-classified cards, reporting 100% precision and 100% recall. Running this algorithm on 39,967,358 scans of cards from the CNI, I identify 312,183 death certificate reference cards in 13.75 days of elapsed real runtime on a personal computer with only a single, $600 Intel processor. Finally, I demonstrate that this approach can be generalized to many different card types within the CNI, showing great promise for application to other archives.","Machine learning, template matching, and the International Tracing Service digital archive: Automating the retrieval of death certificate reference cards from 40 million document scans",Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
731,http://dx.doi.org/10.1093/llc/fqy064,"{'string_id': '10.1093/llc/fqy064', 'id_scheme': 'DOI'}","The recent digital-born electronic literature has heterogeneous components such as kinetic texts, kinetic images, graphical designs, sounds, and videos. These digital components are embedded with the main text as the paratext of print and digital works such as preface, author’s name, illustrations, and title. However, the comparative study between paratext and embedded paratext of electronic literature shows the different strategic patterns and functions of these entities. We discuss the conceptual framework of illuminant devices of paratexts and propose a new term technoeikon to recognize the functions of embedded literary artifact in digital literary works. We examine the critical construction of new term technoeikon which has a unique characteristic that makes electronic literary works different from print literature. This essay reviews the cyclical process of technoeikon from the historical perspective of pre-print culture and print culture and acknowledges technoeikon as inherited from our tradition. Due to digital contrivances, technoeikon takes a new expression as performing in digital ecology which is different from our traditional analog. This article presents a case study on Andy Campbell's (2007b) Dim O'Gauble. Also, Campbell responds to the interpretation of new term technoeikon in the fourth section of the essay.",An introduction to the functioning process of embedded paratext of digital literature: Technoeikon of digital poetry,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
732,http://dx.doi.org/10.1093/llc/fqy065,"{'string_id': '10.1093/llc/fqy065', 'id_scheme': 'DOI'}","In this research we devised and implemented a semi-automatic approach for building a SageBook–a cross-generational social network of the Jewish sages from the Rabbinic literature. The proposed methodology is based on a shallow argumentation analysis leading to detection of lexical–syntactic patterns which represent different relationships between the sages in the text. The method was successfully applied and evaluated on the corpus of the Mishna, the first written work of the Rabbinic Literature which provides the foundation to the Jewish law development. The constructed prosopographical database and the network generated from its data enable a large-scale quantitative analysis of the sages and their related data, and therefore might contribute to the research of the Talmudic literature and evolution of the Jewish thought throughout the two last millennia.",SageBook: Toward a cross-generational social network for the Jewish sages’ prosopography,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
733,http://dx.doi.org/10.1093/llc/fqy066,"{'string_id': '10.1093/llc/fqy066', 'id_scheme': 'DOI'}","Microattribution is the name of a method which has recently started to be used in the attribution of parts of early modern plays. The method seeks to make authorship attributions by using samples of writing consisting of less than two hundred words. This article argues that the method should not be used, fundamentally because it flouts the well-founded scientific insistence on the sufficiency of sample sizes. The article considers two recent applications of the method, showing that huge amounts of evidence were overlooked which would have invalidated the conclusions drawn. Moreover, the article demonstrates that the method is biased in favour of authors with large surviving canons, such as Shakespeare, and it cannot therefore be relied upon.",The problem of microattribution,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
734,http://dx.doi.org/10.1093/llc/fqy067,"{'string_id': '10.1093/llc/fqy067', 'id_scheme': 'DOI'}","We describe the first wide results of the linguistic profiling of the Common European Framework of Reference (CEFR)-levelled English Corpus (CLEC), a corpus built up for Natural Language Processing purposes. The CLEC is a proficiency-levelled English corpus that covers A1, A2, B1, B2, and C1 CEFR levels and that has been built up to train statistic models for automatic proficiency assessment. We describe not only the main aspects of the corpus development but also display the linguistic features and the statistic results for levels A2, B1, and B2 written examples, carried out automatically. We show how raw text, lexical, morphosyntactic, or syntactic statistic outcomes can help to identify levels of proficiency, to test teaching materials accurate proficiency classification, to provide computable support to new text proficiency validation, and to specify level boundaries. In fact, upper levels strengthen proficiency by showing higher outcomes of lexical and syntactic complexity. This analysis validates the use of automatic tools for proficiency level identification based on lexical and syntactic data, whereas morphosyntactic features strengthen competence-level distinctions. Finally, we suggest that these results are a first step onto the CEFR-levelled automatic assessment of new texts.",Automatic profiling of L2-simplified texts: Identifying discriminate features of linguistic proficiency,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
735,http://dx.doi.org/10.1093/llc/fqy068,"{'string_id': '10.1093/llc/fqy068', 'id_scheme': 'DOI'}","Although music is an important part of cremation rituals, there is hardly any research regarding music and cremations. This lack of research has inspired the authors to conduct a long-term research project, focusing on musical and linguistic aspects of music played during cremations. This article presents the analysis of a playlist consisting of twenty-five sets of music, each consisting of three tracks, used in a crematorium in the south of The Netherlands from 1986 onward. The main objective is to identify the differences and similarities of the twenty-five sets of musical tracks regarding content and musical properties. Consequently, we aim to provide insight in the history of (music played during) cremation rituals in The Netherlands. To analyze the musical properties of the sets, the authors use both a qualitative approach (close reading and musical analysis) and a computational analysis approach. The article demonstrates that a combination of a close reading and musical analysis and a computational analysis is necessary to explain the differences in properties of the sets. The presented multi-method approach may allow for comparisons against musical preferences in the context of current cremations, which makes it possible to trace the development of music and cremation rituals.",Music and cremation rituals in The Netherlands: A fine-grained analysis of a crematorium’s playlist,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
736,http://dx.doi.org/10.1093/llc/fqy069,"{'string_id': '10.1093/llc/fqy069', 'id_scheme': 'DOI'}","Statistics and data mining techniques provide exciting approaches for extracting knowledge from data. Recently, using statistics and data mining has sought to be exploited in many research fields. In this study, it was demonstrated that how statistics can be applied to literary studies. First, all the lines in Khaghani’s divan are classified and coded into three categories (mystical, non-mystical, and borderline). Then a set of chi-square goodness-of-fit tests are used to investigate and compare the frequency of different line’s categories for all lines and all odes, separately. Finally, the chi-square independence test (crosstabs) is employed to investigate the existence of trend in the lines.",How statistics and text mining can be applied to literary studies?,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,3,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
737,http://dx.doi.org/10.1093/llc/fqy070,"{'string_id': '10.1093/llc/fqy070', 'id_scheme': 'DOI'}","Identifying the stylistic signatures characteristic of different genres is of central importance to literary theory and criticism. In this article we report a large-scale computational analysis of Latin prose and verse using a combination of quantitative stylistics and supervised machine learning. We train a set of classifiers to differentiate prose and poetry with high accuracy (>97%) based on a set of twenty-six text-based, primarily syntactic features and rank the relative importance of these features to identify a low-dimensional set still sufficient to achieve excellent classifier performance. This analysis demonstrates that Latin prose and verse can be classified effectively using just three top features. From examination of the highly ranked features, we observe that measures of the hypotactic style favored in Latin prose (i.e. subordinating constructions in complex sentences, such as relative clauses) are especially useful for classification.",A small set of stylometric features differentiates Latin prose and verse,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
738,http://dx.doi.org/10.1093/llc/fqy071,"{'string_id': '10.1093/llc/fqy071', 'id_scheme': 'DOI'}","The Guidelines of the Text Encoding Initiative are generally recognized in the digital humanities as important and foundational standards for many types of research in the field. The TEI Guidelines are generalistic, seeking to enable the largest possible user base encoding digital texts for a wide range of purposes. Consulting on many TEI-based projects, teaching TEI workshops, and volunteering as part of the TEI Technical Council, I have encountered many myths, misconceptions, and misunderstandings about the TEI. Indeed, one plenary lecturer once claimed ‘the problem with the TEI is it has too many tags and there is no way to change it’. Inspired by myths such as this, this article will detail common misconceptions about the TEI that I have encountered, concentrating on those technical myths that will help increase knowledge about the TEI misconceptions along the way. The article ends with a consideration of why these myths might have arisen, and what might be able to be done about them.",A world of difference: Myths and misconceptions about the TEI,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
739,http://dx.doi.org/10.1093/llc/fqy072,"{'string_id': '10.1093/llc/fqy072', 'id_scheme': 'DOI'}","The article proposes, justifies, and tests a new methodological framework to measure museum ‘soft power’ by employing geo-visualization as a new method empowered by the rapid development of digital humanities. This research not only demystifies the buzz term of ‘soft power’ that is frequently applied in relation to contemporary museums and their international cultural engagements but also develops an evaluation framework to assess museum capacities to exert global impacts. Specifically, the article draws on the academic scholarship outlining a plethora of approaches for ‘soft power’ evaluation, including Resources, Outputs, Perceptions, and Networks evaluation models. It argues for a new integrative approach that can comprehensively combine different methods to construct a more advanced tool to measure museum ‘soft power’. The article draws on preliminary results of developing a digital mapping system to assess museum soft power. It shares findings from the pilot project, Australian Center of the Moving Image (ACMI) on the Global Map, designed in collaboration with the ACMI in Melbourne.",Mapping museum ‘Soft Power’: Adding geo-visualization to the methodological framework,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
740,http://dx.doi.org/10.1093/llc/fqy073,"{'string_id': '10.1093/llc/fqy073', 'id_scheme': 'DOI'}","While the challenge of historical reconstruction of past musical performances is not a fully solved problem, not all the elements are equally unknown, or of equal magnitude. Despite some uncertainty about the interpretation of individual performers on specific dates, scholarship can still inform other factors of greater perceptual importance, leading to a good approximation of historical performances. In addition to performance style and period instruments, computer simulations make it possible to also account for the acoustics of the period performance space. In addition, the most accurate reconstruction should simulate the room’s acoustics in real time for the performers, thus retaining the feedback mechanisms of room response on performance practice.",Computational acoustic musicology,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
741,http://dx.doi.org/10.1093/llc/fqy074,"{'string_id': '10.1093/llc/fqy074', 'id_scheme': 'DOI'}","In this article, we introduce the first Kurdish text corpus for Central Kurdish (Sorani) branch, called AsoSoft text corpus. Kurdish language, which is spoken by more than 30 million people, has various dialects. As one of the two main branches of Kurdish, Central Kurdish is the formal dialect of Kurdish literature. AsoSoft text corpus is of size 188 million tokens and has been collected mostly from Web sites, published books, and magazines. The corpus has been normalized and converted into Text Encoding Initiative XML format. In both collecting and processing the text, we have faced several challenges and have proposed solutions to them. About 22% of the corpus is topic annotated with six topic tags, and a topic identification task has been done to evaluate the correctness of annotation. The computational experiments of the Central Kurdish text processing are also presented with the support of related supplementary statistics. For the first time, the validity of Zipf’s law for Central Kurdish is presented and also perplexity of this language is calculated using standard N-gram language models. The perplexity of Central Kurdish is 276 for a tri-gram language model.",Toward Kurdish language processing: Experiments in collecting and processing the AsoSoft text corpus,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
742,http://dx.doi.org/10.1093/llc/fqy076,"{'string_id': '10.1093/llc/fqy076', 'id_scheme': 'DOI'}","Semantic Textual Similarity (STS), which measures the equivalence of meanings between two textual segments, is an important and useful task in Natural Language Processing. In this article, we have analyzed the datasets provided by the Semantic Evaluation (SemEval) 2012–2014 campaigns for this task in order to find out appropriate linguistic features for each dataset, taking into account the influence that linguistic features at different levels (e.g. syntactic constituents and lexical semantics) might have on the sentence similarity. Results indicate that a linguistic feature may have a different effect on different corpus due to the great difference in sentence structure and vocabulary between datasets. Thus, we conclude that the selection of linguistic features according to the genre of the text might be a good strategy for obtaining better results in the STS task. This analysis could be a useful reference for measuring system building and linguistic feature tuning.",Linguistic analysis of datasets for semantic textual similarity,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
743,http://dx.doi.org/10.1093/llc/fqy077,"{'string_id': '10.1093/llc/fqy077', 'id_scheme': 'DOI'}","Recent studies have shown that macroscopic patterns of continuity and change over the course of centuries can be detected through the analysis of time series extracted from massive textual corpora. Similar data-driven approaches have already revolutionized the natural sciences and are widely believed to hold similar potential for the humanities and social sciences, driven by the mass-digitization projects that are currently under way, and coupled with the ever-increasing number of documents which are ‘born digital’. As such, new interactive tools are required to discover and extract macroscopic patterns from these vast quantities of textual data. Here we present History Playground, an interactive web-based tool for discovering trends in massive textual corpora. The tool makes use of scalable algorithms to first extract trends from textual corpora, before making them available for real-time search and discovery, presenting users with an interface to explore the data. Included in the tool are algorithms for standardization, regression, change-point detection in the relative frequencies of n-grams, multi-term indices, and comparison of trends across different corpora.",History playground: A tool for discovering temporal trends in massive textual corpora,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
744,http://dx.doi.org/10.1093/llc/fqy078,"{'string_id': '10.1093/llc/fqy078', 'id_scheme': 'DOI'}","The question of why Pablo Picasso dedicated a considerable amount of his time to writing around 1935 is open to speculation. Many have cited, among possible causes: the Spanish artist’s emotional crisis, the political turmoil in Europe in the period between the two wars, and the menace of a confrontation in Spain. All of these views are predicated on an assumed irreducible conflict between visual composition and verbal expression. However, we cannot forget that Picasso’s interest in alternative methods of expression might have started with his fascination for linguistic structure as a whole during his cubist period. In this article, we explore the possibility that the transition into poetry that we observe in Picasso is simply one more manifestation of his pursuit of alternative approaches to language as a means of representation. In this sense, one thing that remained to be determined was how concrete concepts in both languages cluster into representative semantic categories and how these categories interact with each other in semantic networks.",Semantic domains in Picasso’s poetry,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,Supplement_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
745,http://dx.doi.org/10.1093/llc/fqy079,"{'string_id': '10.1093/llc/fqy079', 'id_scheme': 'DOI'}","A large portion of the research carried out in the digital humanities has an online digital object (usually referred as a project) as one of its components. In turn, these online digital objects can be catalogued as distributed resources, which implies that the administrative control of information related to a topic may be spread across online resources and/or collections maintained by multiple scholars in different institutions. This administrative decentralization can lead to changes in content that are often unexpected by a researcher, which can be caused by different factors or circumstances. This reasoning led us to formulate the following question: When can online digital humanities projects be considered abandoned? In this article, we carry out a study on the persistence and average life span of online projects in the digital humanities. More specifically, we will elaborate on their reliance on distributed resources and methods for measuring their shelf life: the average length of time that a digital project can endure without updates until it can ultimately be considered abandoned by its researcher.",Shelf life: Identifying the abandonment of online digital humanities projects,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,34,Supplement_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
746,http://dx.doi.org/10.1093/llc/fqy081,"{'string_id': '10.1093/llc/fqy081', 'id_scheme': 'DOI'}","The course of reprocessing knowledge and information about social sciences and humanities using digital technology is taking root as a new field of academia called the ‘digital humanities’ (DH). While the social sciences and humanities in South Korea have shown a marked reluctance toward the integration of digital technology, the perception of its necessity as a new methodology for developing these fields in the digital age is growing. Until recently, analytical studies on the status and contents of DH were conducted on data from the western world. Despite their late start, however, Asian countries have begun conducting research on DH with enthusiasm. In order for DH to be properly established in each country, it is essential to set the direction by investigating the pre-requisites for DH studies in that country, as well as the current and future demands. As such, this study discusses the current status and issues regarding DH in South Korea by analyzing the trends of DH research published in South Korea, as well as by examining the status and perception of DH among actual scholars. Based on this study’s findings, we present strategies for improving education programs on DH in South Korea and promulgate the necessity of using DH methodologies in the study of social sciences and humanities to develop global networks and academic communication.",Digital humanities and new directions in South Korea,Oxford University Press (OUP),2018,,Digital Scholarship in the Humanities,34,4,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
747,http://dx.doi.org/10.1093/llc/fqy082,"{'string_id': '10.1093/llc/fqy082', 'id_scheme': 'DOI'}","This article addresses an important challenge in artificial intelligence research in the humanities, which has impeded progress with supervised methods. It introduces a novel method to creating test collections from smaller subsets. This method is based on what we will introduce as distant supervision’ and will allow us to improve computational modelling in the digital humanities by including new methods of supervised learning. Using recurrent neural networks, we generated a training corpus and were able to train a highly accurate model that qualitatively and quantitatively improved a baseline model. To demonstrate our new approach experimentally, we employ a real-life research question based on existing humanities collections. We use neural network based sentiment analysis to decode Holocaust memories and present a methodology to combine supervised and unsupervised sentiment analysis to analyse the oral history interviews of the United States Holocaust Memorial Museum. Finally, we employed three advanced methods of computational semantics. These helped us decipher the decisions by the neural network and understand, for instance, the complex sentiments around family memories in the testimonies.",Understanding memories of the Holocaust—A new approach to neural networks in the digital humanities,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
748,http://dx.doi.org/10.1093/llc/fqy083,"{'string_id': '10.1093/llc/fqy083', 'id_scheme': 'DOI'}","This article explores the intersection of biomechanics and culturally situated dance scholarship. We focus on ‘Sendratari Ramayana’, a 50-year-old dance form heavily influenced by classical Javanese dance traditions dating back to the 19th century. We used a full-body plug-in gait model to record differences in character typology—a key concern of Javanese dance scholarship. The results are presented through online visualizations and analyzed quantitatively. This approach outlines a digital stylometry of movement that contributes to the long tradition of formalist analysis of Javanese dance.",Digital dance scholarship: Biomechanics and culturally situated dance analysis,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
749,http://dx.doi.org/10.1093/llc/fqy084,"{'string_id': '10.1093/llc/fqy084', 'id_scheme': 'DOI'}","In 2005, the National Library of Australia (NLA) began a pilot project to selectively digitize back issues of major Australian newspapers to provide free public access to over 60 million digitized newspaper articles, dating from the first years of Australian colonization to the early 1960s. Trove, a faceted search engine maintained by NLA, provides access to this very large collection. Unfortunately, Trove lacked any means to filter by location, which raised the tantalizing possibility of using advanced computational techniques to identify long-term patterns and trends in newspaper reportage of people, events, concepts, and many other historical entities. PaperMiner, which utilizes text mining techniques for extracting metadata information, was developed that enabled the inclusion of geolocations of the places cited in the newspaper articles and supported the searching of articles by location and visualizing the results of searches using both location and time using a map of Australia. Using PaperMiner, researchers could see when and where the anti-Chinese leagues movement started in Australia and how it spread, to better focus their subsequent research. PaperMiner can be used as a digital humanities tool to assist in research by replacing the tedium of a shallow scan through thousands of Trove search results with a more efficient method that draws the researchers’ attention to more significant times and places where their time can be better spent in deeper analysis. In this article, we describe the techniques utilized in creating PaperMiner and discuss its usability testing with a group of leading researchers in Australian history.",PaperMiner—a real-time spatiotemporal visualization for newspaper articles,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
750,http://dx.doi.org/10.1093/llc/fqy085,"{'string_id': '10.1093/llc/fqy085', 'id_scheme': 'DOI'}","Digital humanities research has focused primarily on the analysis of texts. This emphasis stems from the availability of technology to study digitized text. Optical character recognition allows researchers to use keywords to search and analyze digitized texts. However, archives of digitized sources also contain large numbers of images. This article shows how convolutional neural networks (CNNs) can be used to categorize and analyze digitized historical visual sources. We present three different approaches to using CNNs for gaining a deeper understanding of visual trends in an archive of digitized Dutch newspapers. These include detecting medium-specific features (separating photographs from illustrations), querying images based on abstract visual aspects (clustering visually similar advertisements), and training a neural network based on visual categories developed by domain experts. We argue that CNNs allow researchers to explore the visual side of the digital turn. They allow archivists and researchers to classify and spot trends in large collections of digitized visual sources in radically new ways.",The visual digital turn: Using neural networks to study historical images,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
751,http://dx.doi.org/10.1093/llc/fqy087,"{'string_id': '10.1093/llc/fqy087', 'id_scheme': 'DOI'}","Vossian antonomasia is a prolific stylistic device, in use since antiquity. It can compress the introduction or description of a person or another named entity into a terse, poignant formulation and can best be explained by an example: When Norwegian world champion Magnus Carlsen is described as ‘the Mozart of chess’, it is Vossian antonomasia we are dealing with. The pattern is simple: A source (Mozart) is used to describe a target (Magnus Carlsen), the transfer of meaning is reached via a modifier (‘of chess’). This phenomenon has been discussed before (as ‘metaphorical antonomasia’ or, with special focus on the source object, as ‘paragons’), but no corpus-based approach has been undertaken as yet to explore its breadth and variety. We are looking into a full-text newspaper corpus (The New York Times, 1987–2007) and describe a new method for the automatic extraction of Vossian antonomasia based on Wikidata entities. Our analysis offers new insights into the occurrence of popular paragons and their distribution.","‘The Michael Jordan of greatness’—Extracting Vossian antonomasia from two decades ofThe New York Times, 1987–2007",Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
752,http://dx.doi.org/10.1093/llc/fqz001,"{'string_id': '10.1093/llc/fqz001', 'id_scheme': 'DOI'}","In this article, we analyze the social networks of Paradise Lost at multiple scales of analysis: from the whole poem, down to the level of the individual book and character, and also through quantitative analysis of social network analysis metrics, to understand from multiple angles how network structure can inflect the plot and the effects of relationships between characters. In the case of Eve, our multiscaled analysis reveals her complex role binding together her social world in ways not previously considered. Only through examining the network do we see that Milton has placed her at the very center of the human universe, providing the primary connection between the reader and the world of God. This example shows promise that our multiscaled network analysis can come to important conclusions about the role of gender in a text, and the method can be expanded outward to come to conclusions about other social identities that likewise remain largely overlooked in current implementations of digital humanities scholarship.",Epic social networks and Eve's centrality in Milton’sParadise Lost,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
753,http://dx.doi.org/10.1093/llc/fqz003,"{'string_id': '10.1093/llc/fqz003', 'id_scheme': 'DOI'}","Selecting effective features from data sets is a particularly important part in text classification, data mining, pattern recognition, and artificial intelligence. Feature selection (FS) is capable of excluding irrelevant features for the classification task and reducing the dimensionality of data sets, which help us better understand data. Through FS selection, the performance of machine learning techniques is improved, and computation requirement is minimized. Thus far, a large number of FS methods have been proposed, whereas the most practically effective one has not been found. Although it is conceivable that different categories of FS methods follow different criteria for evaluating variables, rare studies have focused on evaluating various categories of FS methods. This article first lists thirteen superior FS methods under five different categories and focuses on evaluating and comparing the effectiveness and general versatility of these methods. The thirteen FS methods were ranked using rank aggregation method. Subsequently, the best five FS methods were elected to perform multi-class classifications. Support vector machine served as the classifier. Different languages, different numbers of selected features, and different performance measures were employed to measure the effectiveness and general versatility of these methods together. The analysis results suggest that Mahalanobis distance is the best method on the whole.",Comparing multiple categories of feature selection methods for text classification,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
754,http://dx.doi.org/10.1093/llc/fqz004,"{'string_id': '10.1093/llc/fqz004', 'id_scheme': 'DOI'}","This article describes a first attempt to annotate the full Greek papyrus corpus automatically for linguistic information. It gives an overview of existing work on Ancient Greek and analyzes the typical problems one encounters when using natural language processing techniques on (1) a historical corpus of (2) a highly inflectional language (as opposed to the more analytic present-day English) and offers solutions to them, testing several different approaches. The focus is on part-of-speech/morphological tagging and lemmatization; some syntactic parsing experiments are also briefly discussed. The conclusion discusses the strengths and shortcomings of the examined techniques and suggests possible ways to further improve tagging and parsing accuracy.",Creating a richly annotated corpus of papyrological Greek: The possibilities of natural language processing approaches to a highly inflected historical language,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
755,http://dx.doi.org/10.1093/llc/fqz005,"{'string_id': '10.1093/llc/fqz005', 'id_scheme': 'DOI'}","This article explores the linguistic features of different registers in Chinese through text clustering driven by the Menzerath–Altmann (MA) law. We propose to calculate the average word length distribution according to clause length. The MA law predicts that texts from different registers will show differences in terms of average word length distribution in texts. As predicted by the MA law, analysis result demonstrates that average word length decreases with the increase of clause length in each register and that their relationship can be fitted by the formula y = axbe-cx. We hypothesize that it is the situation type, i.e. whether the text is dialectic or monologue, that is the linguistic characteristic behind the dichotomy of word length distribution. To confirm these register-distinguishing linguistic features, texts were represented by the average word length distribution and the fitted parameters using the vector space model and clustered according to their register categories. Good clustering results show that average word length distribution in certain length clauses and their fitted parameters can be used as the distinctive characteristics of these three registers.",Linguistic characteristics of Chinese register based on the Menzerath—Altmann law and text clustering,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
756,http://dx.doi.org/10.1093/llc/fqz007,"{'string_id': '10.1093/llc/fqz007', 'id_scheme': 'DOI'}","The Territorial Papers of the United States are a valuable and underused resource containing almost 10,000 documents written between 1789 and 1848 about the formation of new sovereign states from US territory. These communications between the federal government and frontier settlers comprise the actual discourse of the nation’s expansion over six decades. Digitizing the Territorial Papers permits the possibility of analyzing the entire corpus globally. Text mining and topic modeling methods give us a lens on the language patterns through which new state governments and the expanding nation were formed. An initial statistical analysis of the textual information provides a visualization of content, helps discern how ideals about governance emerged, and lays the foundation for developing more sophisticated hypotheses and theoretical constructs.",A case study in text mining: Textual analysis of the Territorial Papers,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
757,http://dx.doi.org/10.1093/llc/fqz009,"{'string_id': '10.1093/llc/fqz009', 'id_scheme': 'DOI'}","This article describes work undertaken at the Warburg Institute in London into the definition of machine-readable ontologies for the identification of iconographic subjects. Iconography, a descriptive discipline concerned with the identification of the content or subject of an image, is a core component of the wider discipline of iconology, the study of the meanings of images in their cultural or historical contexts. The research detailed here attempts to define the core of an ontology for the indicators of an iconographic subject that would be employed by an art historian in making an identification: these are encoded in OWL, the Web Ontology Language. The article demonstrates how such an ontology may be queried in XML format using simple XQUERY queries. Future directions for this research are discussed, including its possible integration with image recognition technologies to facilitate more automated approaches to iconographic identification.",Towards an ontology-based iconography,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
758,http://dx.doi.org/10.1093/llc/fqz011,"{'string_id': '10.1093/llc/fqz011', 'id_scheme': 'DOI'}","Digital humanities projects have long relied on various schema languages—chiefly, RELAX NG and Schematron—for validating the XML documents in their data collections; however, these languages are limited in their ability to check for consistency, coherence, and completeness across the entire project. In our work as part of “Endings”, an umbrella project that comprises four diverse digital edition projects from different fields, we have developed a methodology for checking and enforcing correctness, completeness, and coherence across the entire document set. The following article describes the various stages (what we term “levels”) of our diagnostics process, all of which are driven by XSLT (Extensible Stylesheet Language Transformations) stylesheets, and produce a human readable report. These levels include checks for referential integrity, correct entity tagging, and potential duplicates in the data set. Using examples from the Endings projects, we show how diagnostic processes not only ensure correctness in the data set, but can also aid in determining project milestones and completion dates. Diagnostics, we argue, are thus a crucial extension to schema-based validation for complex digital projects and can provide concrete ways for digital humanities projects to enforce coherence and consistency and track their progress toward completion.","Beyond validation: Using programmed diagnostics to learn about, monitor, and successfully complete your DH project",Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,34,Supplement_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
759,http://dx.doi.org/10.1093/llc/fqz014,"{'string_id': '10.1093/llc/fqz014', 'id_scheme': 'DOI'}","Nudging is simply guiding people behaviors by the use of user-interface and design elements in digital environments. Today, many decisions are made in online environments. Gaining insights about digital nudging can greatly help communicators, policy makers, and designers lead users to make the most desirable choice for them and/or for the wealth of the society as well. Digital nudges can be used in many digital environments like e-mail, SMS, push notifications, mobile apps, social media, gamification, e-commerce, e-government, location services, corporate digital information systems, and many other digital interfaces that include any kind of decision-making processes. This study is a descriptive study and more of a qualitative nature and aims to identify the digital nudging concept, dark patterns, and usage of digital nudges in real-life applications. It also proposes a brief digital nudging process schema to be used for designing behavioral digital interventions. ",Digital nudges and dark patterns: The angels and the archfiends of digital communication,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
760,http://dx.doi.org/10.1093/llc/fqz015,"{'string_id': '10.1093/llc/fqz015', 'id_scheme': 'DOI'}","Text analysis provides an exciting approach for extracting knowledge from text data. Recently, text analysis has been applied in many research fields. In this study, it was demonstrated that how text analysis can be applied to literary researches. All the lines of Khaghani’s Divan have been considered using different text analysis methods. Then the accuracy of the applied methods is compared.",Analysis of mystical concepts in Khaghani’s Divan,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
761,http://dx.doi.org/10.1093/llc/fqz016,"{'string_id': '10.1093/llc/fqz016', 'id_scheme': 'DOI'}","Reliable high-quality transcription and/or annotation (a.k.a. ‘coding’) is essential for research in a variety of areas in Humanities and Social Sciences which make use of qualitative data such as interviews, focus groups, classroom observations, or any other audio/video recordings. A good tool can facilitate the work of transcription and annotation because the process is notoriously time-consuming and challenging. However, our survey indicates that few existing tools can accommodate the requirements for transcription and annotation (e.g. audio/video playback, spelling checks, keyboard shortcuts, adding tags of annotation) in one place so that a user does not need to constantly switch between multiple windows, for example, an audio player and a text editor. ‘Transcribear’ (https://transcribear.com) is therefore developed as an easy-to-use online tool which facilitates transcription and annotation on the same interface while this web tool operates offline so that a user’s recordings and transcripts can remain secure and confidential. To minimize human errors, the functionality of tag validation is also added. Originally designed for a multimodal corpus project UNNC CAWSE (https://www.nottingham.edu.cn/en/english/research/cawse/), this browser-based application can be customized for individual users’ needs in terms of the annotation scheme and corresponding shortcut keys. This article will explain how this new tool can make tedious and repetitive manual work faster and easier and at the same time improve the quality of outputs as the process of transcription and annotation tends to be prone to human errors. The limitations of Transcribear and future work will also be discussed.",Transcribear – Introducing a secure online transcription and annotation tool,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
762,http://dx.doi.org/10.1093/llc/fqz017,"{'string_id': '10.1093/llc/fqz017', 'id_scheme': 'DOI'}","‘And Quiet Flows the Don’ is an epic novel, considered one of the most significant works of Russian and world literature. The debate on the authorship of ‘And Quiet Flows the Don’ had been surrounding the novel since its first release in 1928 by Mikhail Sholokhov, who was repeatedly accused of plagiarism. The supporters of the plagiarism theory often indicate that the real author of the novel is the Cossack writer, Fyodor Kryukov, who died before ‘And Quiet Flows the Don’ was published. In the present study we applied the information-based similarity analysis (Yang et al., 2003a, Linguistic analysis of human heartbeats using frequency and rank order statistics. Physical Review Letters, 90: 108103; Yang et al., 2003b, Information categorization approach to literary authorship disputes. Physica A, 329, 473) and Burrows's Delta (Burrows, 2002, ‘Delta’: a measure of stylistic difference and a guide to likely authorship. Literary and Linguistic Computing, 17(3):267–87) to a corpus of Russian literature of XIX and XX centuries. We next used these two methods to compare ‘And Quiet Flows the Don’ to Sholokhov’s and Kryukov’s writings. It was found that Fyodor Kryukov writings are distinct from ‘And Quiet Flows the Don’, whilst Sholokhov’s writings being close to the Don novel. The results also highlight how both information similarity analysis and Delta analysis can be used Russian language.",And Quiet Flows the Don: the Sholokhov-Kryukov authorship debate,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
763,http://dx.doi.org/10.1093/llc/fqz018,"{'string_id': '10.1093/llc/fqz018', 'id_scheme': 'DOI'}","This research starts from the observation that Bernard Stiegler’s general organology draws from the philosophical rethinking of the original practice of organology in musicology. Stiegler’s main philosophical concepts that led to the establishment of general organology, as well as the trajectory of development of Stiegler’ general organology from his musical/musicological experience are discussed and explained. The main claim of this article is that the philosophical platform of general organology has an activist potential for the revitalization of the contemporary humanities and the transformation of the humanities into digital studies. This transformation takes place in the manner of two-level transcontextualization. The first level concerns the transcontextualization of the musicological organology into the general organology as a philosophical platform for understanding the phenomenological and ontological questions of a human being in the world mediated by digital technology. The second level concerns the activist potential of the general organology in relation to the humanities. The transformation of the humanities into digital studies is enabled through the transcontextualization of the general organology as the new, revitalized philosophical ground of the humanities that are dealing with the conditions of humanity in our contemporaneity. This means that digital studies involve not only the digital mediation of the knowledge, but researching that should be implemented in improving the humans' skills, knowledge, attention, and perceptive capabilities through digital technologies.",Digital studies and transcontextualization of the humanities: The case of organology,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
764,http://dx.doi.org/10.1093/llc/fqz019,"{'string_id': '10.1093/llc/fqz019', 'id_scheme': 'DOI'}","Humour relies on dominant cultural values and outlooks in its environment in enacting its comic content. However, since humour sometimes encompasses global experiences, it often transcends artificial human barriers. Its transcendence has been facilitated through online digitized content, in the present case—Internet memes. In exploring transnationalization, fifty purposively selected memes are culled from the Facebook group Robert Mugabe Quotes and subjected to critical linguistic analysis. In the evaluation of the data which is achieved through the linguistic examination of cross-cultural themes in the formulation of identity, didactics, and ideology, sixteen memes, representative of the overarching tripartite concerns, are used as exemplifications. Reliant on the bipartite postulations of Critical Linguistic Stylistics—a linguistic theory that examines the style and peculiarities of linguistic data—and Relief Theory of Humour—which considers humour as a platform for the assuagement of tension and emotions, the memes are testamental of prevailing concerns— politics, technology, social/international relations, sex, male–female relations, etc.—in the human society at large. Linguistic markers such as oppositional expressions, capitalization, and other graphological features are annexed in meaning-formation. I conclude that although humour is generally regarded as a light-hearted routine geared towards the provision of momentary relief, a close scrutiny reveals that deep messages targeted at stimulating consciousness and social transformation find provenance in these artefacts.","Transnationalizing humour on social media: A linguistic analysis of ideology, identity and didactics in Robert Mugabe Quotes memes1",Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
765,http://dx.doi.org/10.1093/llc/fqz022,"{'string_id': '10.1093/llc/fqz022', 'id_scheme': 'DOI'}","Although the digital humanities have traditionally been conceived as a text-based discipline, both digital visualization techniques as well as visual analysis are increasingly used for research in various humanities disciplines. Since there are several overlaps in epistemic cultures of visually oriented and digitally supported research in art and architectural history studies, museology, and archaeology, as well as cultural heritage, we introduce ‘visual digital humanities’ as novel ‘umbrella’ term to cover research approaches in the digital humanities that are dependent on both consuming and producing pictorial, rather than textual, information to answer their humanities research questions. This article aims to determine this particular field of research in terms of (1) research topics, (2) disciplinary standards, and (3) a scholarly culture as well as (4) researchers’ habits and backgrounds. This study is intended to highlight a scope of phenomena and aspects of relevance. Information is gathered by interviews with researchers at London universities and workshops held in Germany and Sweden.","The visual side of digital humanities: a survey on topics, researchers, and epistemic cultures",Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
766,http://dx.doi.org/10.1093/llc/fqz023,"{'string_id': '10.1093/llc/fqz023', 'id_scheme': 'DOI'}","Digital Humanities (DH) within Coptic Studies, an emerging field of development, will be much aided by the digitization of large quantities of typeset Coptic texts. Until recently, the only Optical Character Recognition (OCR) analysis of printed Coptic texts had been executed by Moheb S. Mekhaiel, who used the Tesseract program to create a text model for liturgical books in the Bohairic dialect of Coptic. However, this model is not suitable for the many scholarly editions of texts in the Sahidic dialect of Coptic which use noticeably different fonts. In the current study, DH and Coptological projects based in Göttingen, Germany, collaborated to develop a new Coptic OCR pipeline suitable for use with all Coptic dialects. The objective of the study was to generate a model which can facilitate digital Coptic Studies and produce Coptic corpora from existing printed texts. First, we compared the two available OCR programs that can recognize Coptic: Tesseract and Ocropy. The results indicated that the neural network model, i.e. Ocropy, performed better at recognizing the letters with supralinear strokes that characterize the published Sahidic texts. After training Ocropy for Coptic using artificial neural networks, the team achieved an accuracy rate of >91% for the OCR analysis of Coptic typeset. We subsequently compared the efficiency of Ocropy to that of manual transcribing and concluded that the use of Ocropy to extract Coptic from digital images of printed texts is highly beneficial to Coptic DH.",Optical character recognition of typeset Coptic text with neural networks,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,34,Supplement_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
767,http://dx.doi.org/10.1093/llc/fqz027,"{'string_id': '10.1093/llc/fqz027', 'id_scheme': 'DOI'}","Eye-tracking—the process of capturing and measuring human eye movement—is becoming an increasingly prevalent tool in the cultural heritage sector to understand visual processing and audience behaviours. Yet, most applications to date have focused on individual artworks and distinctions between representative/non-representative topics, with little prior work on the effects of differing written interpretations on the visual exploration of collections of artworks, particularly with devotional themes. This article reports on an eye-tracking study that explored responses to the unique collection of Francisco de Zurbarán paintings in County Durham. Using eye-tracking technology in a laboratory setting, we evaluated the viewing behaviour of three participant groups to determine whether the accompanying written context influences how digital reproductions are experienced. In addition to demonstrating statistically significant variations in aesthetic appreciation, the experiments showed that the gaze can be redirected towards areas of conceptual significance. Most importantly, we were able to challenge the assumption that viewers always look at faces (Bindemann et al., 2005). Our findings make an important new contribution to the scholarly understanding of how audiences view, appreciate, and understand artworks and to museum and heritage practices relevant to the display of art.",Aesthetic appreciation and Spanish art: insights from eye-tracking,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,34,Supplement_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
768,http://dx.doi.org/10.1093/llc/fqz028,"{'string_id': '10.1093/llc/fqz028', 'id_scheme': 'DOI'}","The judgments by members of the US Supreme Court in the 2000 case of Bush versus Gore remain controversial to the present. We use text mining and machine learning methods to compare the word usage patterns of Supreme Court Justices in order to explore the likely authorship of both the anonymous 5-4 per curiam decision in this case and the concurrence that is attributed to Chief Justice Rehnquist, with Scalia and Thomas joining. An analysis of high and medium frequency words suggests that Justice Kennedy was likely the main contributor to the per curiam decision. A similar analysis of the concurrence, however, suggests that Justice Scalia may have played a more central role than the document’s purported author, Justice Rehnquist. Our analysis indicates that while Chief Justice Rehnquist was likely to have been the crafter of the document, much of the more forceful language of the concurrence resonates more clearly with a vocabulary that is indicative of Justice Scalia.",Judging style: The case of Bush versus Gore,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
769,http://dx.doi.org/10.1093/llc/fqz029,"{'string_id': '10.1093/llc/fqz029', 'id_scheme': 'DOI'}","Many classical texts are available in multiple versions that almost always differ from each other due to transcription error and editorial discretion. One of the central challenges in the study of such texts is the preparation of a ‘synoptic’ text: an aligned presentation of the various versions in which corresponding words or phrases, even if not identical, are mapped to each other. Multiple text alignment of this sort must take into account orthographic and conceptual relationships between words. In this article, we define this text alignment problem as an optimization problem by providing a formal measure of alignment quality. Unlike previous measures, our measure uses word embeddings to take into account conceptual similarity between aligned words. We propose an efficient and scalable alignment method in accordance with the proposed criteria. This method splits the texts to be aligned into smaller subtexts, thus improving both efficiency and accuracy. Empirical comparisons on sample data indicate our method is significantly faster than existing methods, often rendering intractable problems tractable, and that the alignment obtained by our method is considerably better than that obtained by other methods.",FAST: Fast and Accurate Synoptic Texts,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
770,http://dx.doi.org/10.1093/llc/fqz030,"{'string_id': '10.1093/llc/fqz030', 'id_scheme': 'DOI'}","Ever since the beginnings of the modern historiography, the court books have posed a challenge for editors in Poland, both due to their number and variety. They constitute one of the richest sources enabling a variety of historical research. The publication of the sources’ content can be shared owing to new approaches stemming from constant development of IT tools and their application in the humanities. The solution proposed in our article is a digital indexing based on a relational database enabling access to the sources’ scans. The characterization of the method is preceded by a description of the theoretical foundations of the presented method. The assumed principals are implemented by the use of a dedicated to this project online INDXR application which functionalities is thoroughly described. Using the INDXR application, the data acquired from the sources are collected and stored in the database which structure is also illustrated along with its theoretical foundations. The database is established in order to better reflect the typical elements comprising the court books as well as to store the acquired information. The issues stemming from the process of indexing the court books, such as categorizing of the entries, their spatial context, and the problem of how to describe the persons appearing in the manuscript are also presented.",Technical and methodological foundations of digital indexing of medieval and early modern court books,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
771,http://dx.doi.org/10.1093/llc/fqz031,"{'string_id': '10.1093/llc/fqz031', 'id_scheme': 'DOI'}","Determining the date of writing is a practical problem often encountered in the study of ancient and medieval texts. The problem is compounded by differences in genre, register and style, but in particular by our frequent dependence on much later copies of lost originals. This article examines how a method of classification with flexible time intervals has been developed for a corpus of medieval Irish annals (c.700–c.1600). It is shown that the method can deal successfully with an unparsed, complex corpus containing contaminated data. A method of extending the model from good quality, uncontaminated data to more complex texts is also demonstrated by identifying temporal characteristics and typical entries in the high-quality source.",Dating medieval texts by classification with flexible time intervals,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
772,http://dx.doi.org/10.1093/llc/fqz033,"{'string_id': '10.1093/llc/fqz033', 'id_scheme': 'DOI'}","Data driven approaches for machine translation, such as statistical and neural machine translation, suffer from sparsity when dealing with low-resource languages. In these cases, using other sources of information including linguistic information could alleviate the problem. In this article, we focus on the problem of word ordering in translation from a high-resource to a low-resource language and try to improve the quality by using syntactic information from the high-resource side. We propose some syntactic features based on Tree Adjoining Grammar (TAG) to be employed in a phrase-based SMT model in order to improve the word ordering. In this work, a set of synchronous TAG rules is extracted and used to estimate the probability of the phrase orders suggested by the phrase-based model. The main idea of the article is to handle the word ordering by using the extended domain of locality property of TAG and abstracting the long distance dependencies into a local view, which is a TAG elementary tree. The experiments on English–Persian and English–German translation showed that, by combining the proposed TAG-based reordering features with lexical and hierarchical reordering models, we gain significant improvements over the baseline and in comparison with a neural reordering model and a pre-reordering model.",Using syntax for improving phrase-based SMT in low-resource languages,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
773,http://dx.doi.org/10.1093/llc/fqz035,"{'string_id': '10.1093/llc/fqz035', 'id_scheme': 'DOI'}","In this study, I frame the concept of techno-poetics by analyzing Alex Epstein’s micro-stories and by examining the development of the micro-fiction genre throughout the world. Epstein is a contemporary Israeli author whose universal micro-stories have been translated into several languages (English, Spanish, French, and Russian). Epstein uses a dual language: given his career as a computer programmer in a high-tech company, the language of his thoughts is conveyed through the logic of technology, whereas, as an artist, he is loyal to the language of poetics. Is Epstein a “programmer” of micro-stories? Within the framework of this study, I analyze the dynamic relationship between the polar opposites of technology and poetics, as it is revealed in Epstein’s micro-stories, taking into account the genre’s characteristics as well as the unique features with which Epstein—as a contemporary author—imbues his works. More specifically, I analyze six categories that describe the relationship between digital technology and the world of art, a relationship that informs Epstein's micro-stories. Epstein's work was not created in a void; nevertheless, his micro-stories differ not only from the works of previous authors of the genre, but also from those of his contemporaries, whose work, likewise, deals with the tension between technology and poetics. A major difference is the methods of publication that Epstein uses, which form part of the techno-poetical process. In this sense, the themes, the conception of art, and the method of publication are all indications of a unique artistic phenomenon.",Techno-poetics in micro-stories of the digital age: The case of Alex Epstein,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
774,http://dx.doi.org/10.1093/llc/fqz038,"{'string_id': '10.1093/llc/fqz038', 'id_scheme': 'DOI'}","In this article, we utilized large-scale statistical analysis and data visualization techniques of the greatest collection in the world of Hebrew manuscript metadata records to develop a new methodology for quantitative investigation of the palaeographic, geographic, and temporal characteristics of historical manuscripts. The study aims to explore whether and to what extent the script type of the manuscript and its changes over time can be used to automatically predict and complete missing geospatial data of the manuscripts. To this end, various ontological entities were used as features to train supervised machine-learning algorithms to predict the places of writing of manuscripts which were often absent in the catalogue records. The obtained results show that while the script type as an only feature might not be sufficient for prediction of the location of the manuscript’s writing, its combination with temporal data of the manuscript yielded about 80% accuracy. Eventually, our system was able to complete the missing places of writing for over 60% of the manuscripts in the corpus. Moreover, we found that through typical and marginal script types in different regions and their changes over time, it is possible to draw the migration map of the Jewish communities over the centuries. This reinforces the findings of historical research on Jewish migration patterns and communal formation. For example, the waves of immigration from Western Europe can be seen clearly from the second half of the 13th century, which continued until the 17th century and greatly increased the Eastern European Jewish community.",A new analytic framework for prediction of migration patterns and locations of historical manuscripts based on their script types,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,35,2,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
775,http://dx.doi.org/10.1093/llc/fqz039,"{'string_id': '10.1093/llc/fqz039', 'id_scheme': 'DOI'}","This study investigates ‘otherness’ in gender identity creation in social media memes. Nineteen ‘Correct Bro’ and ‘Correct Bae’ memes from Facebook are purposively selected for the study. The analysis is qualitative in approach and is anchored on Incongruity Theory of humour and Gender Social Theory. Linguistic tools from Critical Stylistics are employed for the linguistic interpretation of the data. Specific gender otherness identified are the dependent other, irrational other, opportunistic other, weak other, and the unreal other. Humorous forms identified are parody, teasing, pun, and sarcasm among others. Otherness is constructed within these humorous forms with linguistic and rhetorical tools, such as structural opposition, lexical absence, prioritizing, repetition, and so on. The study identified that gender ‘otherness’ is expressed in humorous memes to either reinforce existing gender ideologies or challenge them. Also, humorous memes are not just created to evoke laughter but are tools for expressing new tendencies in gender ideological orientation in the society.",Gender ideology and identity in humorous social media memes,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
776,http://dx.doi.org/10.1093/llc/fqz042,"{'string_id': '10.1093/llc/fqz042', 'id_scheme': 'DOI'}","The article offers a state-of-the-art overview of a number of Digital Humanities (DH) initiatives that have emerged in Sweden over the past decade. We identify two major developments that seem to be taking place within DH, with a specific focus on the infrastructural aspects of the development: (1) a strive to open up and broaden the research output and (2) multi-disciplinary collaboration and its effects. The two major components accentuate the new infrastructural patterns that are developing and the challenges these infer on universities. While current research is at large multi-disciplinary, developing infrastructures also enable the move towards post-disciplinarity, bringing the universities closer to the surrounding society. At five universities in Sweden, individual-sited infrastructures supporting DH research have been built today. They are complemented by national and international infrastructures, thus supporting developments and tackling some of the major challenges. In the article, the relations between individual disciplines, the question of multi- and post-disciplinarity, and the field of Digital Humanities are discussed, while stressing the factors necessary—sine qua non—for a fruitful development of the scholarly infrastructures.",Digital humanities in Sweden and its infrastructure: Status quo and the sine qua non,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
777,http://dx.doi.org/10.1093/llc/fqz043,"{'string_id': '10.1093/llc/fqz043', 'id_scheme': 'DOI'}","Experience through sight has been recognized as a contributing factor in the shaping of historic landscapes, where humans could express themselves in response to their aesthetic and intellectual qualities. What was experienced, however, was not only dependent on the ‘prospect’, or landscape view, but also on the individual ‘perspective’ of the spectator. Three-dimensional Geographical Information Systems (3D GIS) has enabled investigations into landscape visibility within digitized historic environments and can therefore assist the analysis and understanding of this phenomenon. This article applies 3D GIS to a currently under-researched area of landscape history, English designed landscapes of the 16th and 17th centuries. From country houses and gardens to parks and working estates, these landscapes were manipulated in response to the landowners’ ‘perspectives’ towards the landscape, which subsequently determined the visibility or invisibility of features within certain ‘prospects’. This concept was dubbed ‘The Royaltie of Sight’ by Henry Wotton in 1624. By using 3D GIS to recreate a designed landscape that poses challenges which have previously hindered its analysis, the characteristics of ‘prospects’ can be ascertained using viewshed analysis and the individual ‘perspective’ of the landowner interpreted using phenomenology and reception theory. The results presented in this article demonstrate how 3D GIS has benefited studies into English designed landscapes and improved knowledge of how perception influenced landscape change. ",3D GIS and ‘The Royaltie Of Sight’: Recreating ‘Prospects’ and ‘Perspectives’ within an English designed landscape c. 1550–1660,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
778,http://dx.doi.org/10.1093/llc/fqz045,"{'string_id': '10.1093/llc/fqz045', 'id_scheme': 'DOI'}","Although in the digital humanities, researchers use software tools to conduct their research, and often apply these tools to data the software was not developed for, there has been little attention for investigating tool performance on this data. This is strange because in order to be able to appraise the results of digital humanities research, it is important to understand to what extent the tool output is correct. To illustrate the importance of the validation of tools, this article presents a case study of validating Arabic root extraction tools. Arabic words are based on root letters; three root letters usually demarcate a semantic field. Thus, roots can be used for studying semantic fields. For example, researchers can gain insight into the relative importance of the different senses (i.e. seeing, hearing, touching, smelling, and tasting) in Arabic jurisprudence (fiqh) by extracting and counting roots. A problem is that there are only a few usable tools available. We take three root extraction tools, Khoja (Khoja and Garside, 1999, Stemming Arabic Text. Lancaster, England: Lancaster University), ISRI (Taghva et al., 2005, Arabic stemming without a root dictionary. In International Conference on Information Technology: Coding and Computing (ITCC’05). Vol. 2. Las Vegas, NV, April 2005 pp. 152–57), and AlKhalil (Boudlal et al., 2010, Alkhalil morpho sys1: a morphosyntactic analysis system for Arabic texts. In International Arab Conference on Information Technology. New York, NY: Elsevier Science Inc., April 2017, pp. 1–6), and create manually annotated gold standard data consisting of three samples of approximately 1,000 words from important books of Islamic jurisprudence. We show that Khoja is the best root extraction tool for our data. We also demonstrate that the relative counts of individual roots differ among tools, which leads to a different interpretation depending on which tool is chosen. This means that findings based on automatically extracted roots should always be interpreted with care.",Are you sure your tool does what it is supposed to do? Validating Arabic root extraction,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
779,http://dx.doi.org/10.1093/llc/fqz046,"{'string_id': '10.1093/llc/fqz046', 'id_scheme': 'DOI'}","This article presents technical approaches and innovations in digital library design developed during the design and implementation of the Chinese Text Project, a widely-used, large-scale full-text digital library of premodern Chinese writing. By leveraging a combination of domain-optimized Optical Character Recognition, a purpose-designed crowdsourcing system, and an Application Programming Interface (API), this project simultaneously provides a sustainable transcription system, search interface and reading environment, as well as an extensible platform for transcribing and working with premodern Chinese textual materials. By means of the API, intentionally loosely integrated text mining tools are used to extend the platform, while also being reusable independently with materials from other sources and in other languages.",Chinese Text Project: a dynamic digital library of premodern Chinese,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
780,http://dx.doi.org/10.1093/llc/fqz048,"{'string_id': '10.1093/llc/fqz048', 'id_scheme': 'DOI'}","Word sense disambiguation (WSD) is the task of selecting correct sense for an ambiguous word in its context. Since WSD is one of the most challenging tasks in various text processing systems, improving its accuracy can be very beneficial. In this article, we propose a new unsupervised method based on co-occurrence graph created by monolingual corpus without any dependency on the structure and properties of the language itself. In the proposed method, the context of an ambiguous word is represented as a sub-graph extracted from a large word co-occurrence graph built based on a corpus. Most of the words are connected in this graph. To clarify the exact sense of an ambiguous word, its senses and relations are added to the context graph, and various similarity functions are employed based on the senses and context graph. In the disambiguation process, we select senses with highest similarity to the context graph. As opposite to other WSD methods, the proposed method does not use any language-dependent resources (e.g. WordNet) and it just uses a monolingual corpus. Therefore, the proposed method can be employed for other languages. Moreover, by increasing the size of corpus, it is possible to enhance the accuracy of WSD. Experimental results on English and Persian datasets show that the proposed method is competitive with existing supervised and unsupervised WSD approaches.",Co-occurrence graph-based context adaptation: a new unsupervised approach to word sense disambiguation,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
781,http://dx.doi.org/10.1093/llc/fqz050,"{'string_id': '10.1093/llc/fqz050', 'id_scheme': 'DOI'}","Psychological analysis of characters in ordinary novels is mainly a qualitative analysis, which is easily affected by the researchers’ reading level, theoretical literacy, subjective experience, and other factors. With the development of computer technology and big data, stable and systematic personality can more accurately describe the psychology of text characters. This article adopts the method of literary intelligence analysis based on data mining and statistics, through the Chinese psychological analysis system, the language of the characters in the novel of ordinary world can be counted, processed, and disposed, and then obtains the big five personality prediction scores of the characters. Furthermore, the validity of the intelligent analysis method is confirmed by examining the verification of the predictive scores in the text and literature. After verification by many parties, the predicted results of this article are supported by the text and literature, which shows that literary intelligence analysis of novel characters’ personalities is effective.",The intelligence analysis of personal characters about ordinary world,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
782,http://dx.doi.org/10.1093/llc/fqz051,"{'string_id': '10.1093/llc/fqz051', 'id_scheme': 'DOI'}","The fact about the way God has described himself or how Muslims regard God’s traits is a significant point because that is the path to know the truth about God in his own words, and the verification of Muslims understanding of God through their thoughts and behaviors in accordance with Quran can be studied. In Islamic mysticism, the names and traits of God are categorized into two groups: beauty and divine glory. Although there have been widespread studies in regard to God’s traits, casting a statistical view on these traits can help with the understanding of God, because it would ease the recognition of the way God has introduced himself or envisages traits he most used for himself which in its turn will enlighten the path a Muslim should take. Therefore, with regard to statistics in this work, we would like to study the idea that which group of God’s traits (beauty or divine grace) is more repeated or what the proportion of the two is, and also what difference there is between the Meccan and Medinan suras in the description of God.",Statistical analysis about the God’s traits in Quran,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
783,http://dx.doi.org/10.1093/llc/fqz052,"{'string_id': '10.1093/llc/fqz052', 'id_scheme': 'DOI'}","Over the last few decades, there has been tremendous growth in online communication through different types of media. Communication via the Internet is anonymous, which causes a critical issue regarding identity tracing. Authorship identification can apply to tasks such as identifying an anonymous author, detecting plagiarism, or finding a ghostwriter. Previous research has outlined the various methods and their improvements for the identification of anonymous authors based on stylometry. However, changes in the writing style of an author over a long period has not been addressed. In this article, we propose a methodology for author identification where the writing style of an author changes. The proposed methodology consists of two phases: the first will show the change in writing style of the author and in another phase the change is mitigated by a new feature normalization technique. A novel Transform Feature to Current Time function is proposed for normalization, where features are shifted to current time and made available for further classification. A machine-learning algorithm is used to identify an author candidate. The experiments of the proposed methodology conducted on a set of text samples by several authors were collected over a different time period and the results show an improvement in performance.",Author identification with feature transformation method,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
784,http://dx.doi.org/10.1093/llc/fqz053,"{'string_id': '10.1093/llc/fqz053', 'id_scheme': 'DOI'}","Over the past few years, research carried out at large-scale materials science facilities in the USA and elsewhere has undergone a phase transition that affected its character and culture. Research cultures at these facilities now resemble ecosystems, comprising of complex and evolving interactions between individuals, institutions, and the overall research environment. The outcome of this phase transition, which has been gradual and building since the 1980s, is known as the New (or Ecologic) Big Science [Crease, R. and Westfall, C. (2016). The new big science. Physics Today, 69: 30–6]. In this article, we describe this phase transition, review the practical challenges that it poses for historians, review some potential digital tools that might respond to these challenges, and then assess the theoretical implications posed by “database history’. ",Database thinking and deep description: designing a digital archive of the National Synchrotron Light Source,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,34,Supplement_1,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
785,http://dx.doi.org/10.1093/llc/fqz054,"{'string_id': '10.1093/llc/fqz054', 'id_scheme': 'DOI'}","Multispectral (MSI) imaging of historical documents can recover lost features, such as text or drawings. This technique involves capturing multiple images of a document illuminated using different wavelengths of light. The images created must be registered in order to ensure optimal results are produced from any subsequent image processing techniques. However, the images may be misaligned due to the presence of optical elements such as filters, or because they were acquired at different times or because the images were captured from different copies of the documents . There is little prior work or information available about which image registration techniques are most appropriate. Image registration of multispectral images is challenging as the illumination changes for each image and the features visible in images captured at different wavelengths may not appear consistently throughout the image sequence. Here, we compare three image registration techniques: two based on similarity measures and a method based on phase correlation. These methods are characterized by applying them to realistic surrogate images and then assessed on three different sets of real multispectral images. Mutual information is recommended as a measure for affine image registration when working with multispectral images of documentary material as it was proven to be more robust than the other techniques tested.",Affine registration of multispectral images of historical documents for optimized feature recovery,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
786,http://dx.doi.org/10.1093/llc/fqz056,"{'string_id': '10.1093/llc/fqz056', 'id_scheme': 'DOI'}","In 1985, Italo Calvino wrote a series of lectures (later published as ‘memos’) in which he proposed six values he deemed crucial to literature as it moved into the next millennium: lightness, quickness, ‘crystal’ exactitude, visibility, multiplicity, and consistency. Though never a writer of electronic literature, Calvino has frequently been associated or referenced in relation to digital works. J.R. Carpenter’s web-based work The Gathering Cloud (2016) (hereafter TGC) exhibits Calvino’s values. TGC is informed by Howard’s 1803 Essay on the Modifications of Clouds. Howard’s ‘frontispiece’ and five ‘plates’ are used in Carpenter’s web-based work. Poetry is then superimposed on these repurposed illustrations. Situated ‘within’ the poetry, animated gif collages play. Where Calvino in his memos writes that he considers the virtues of the binary opposites of his values (i.e., weight, lingering, ‘flame’ exactitude, ambiguity, singularity, and inconsistency) no less compelling, Carpenter’s work suggests that Calvino’s values (or rather the absence or removal of their binary opposites) are not only preferable in terms of contemporary literary challenges, but an ethical imperative in relation to environmental impact as it relates to contemporary media, dissemination, and indeed everyday life. In this analysis of TGC, Calvino’s values will be discussed in relation to each of the work’s six sections (i.e., the ‘frontispiece’ and five ‘plates’).",Italo Calvino’s Six Memos as ethical imperative in J.R. Carpenter’s The Gathering Cloud,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
787,http://dx.doi.org/10.1093/llc/fqz058,"{'string_id': '10.1093/llc/fqz058', 'id_scheme': 'DOI'}","In this paper, we present a new semi-automatic methodology for construction of event-based ontology from the library catalogue of the largest collection in the world of metadata records of historical Hebrew manuscripts. Based on the constructed ontology, we developed and implemented a new framework for catalogue data enrichment, correction, and its systematic quantitative analysis. Finally, we demonstrate the results of the proposed large-scale analysis of three most prominent event types in the corpus, as well as a few cross-event relations and trends.",Ontology-based analysis of the large collection of historical Hebrew manuscripts,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
788,http://dx.doi.org/10.1093/llc/fqz059,"{'string_id': '10.1093/llc/fqz059', 'id_scheme': 'DOI'}","Despite the commonly reported underuse of linking adverbials of contrast and concession (such as yet, nevertheless) by English as a Foreign Language (EFL) learners in writing, relatively little is known about the use of structural conjunctions in this regard. The present work uses a corpus approach to investigate the use of while, a polysemous conjunction of contrast and concession, in the writing of Chinese EFL learners as compared with their British native-speaker counterparts. The analysis of while-clauses is informed by clause-complexing and textual descriptions of clause in Systemic Functional Linguistics (Halliday and Matthiessen, 2013). Preference for initial concessive while-clauses by native-speaker students was found, in sharp contrast to the dominant use of final adversative while-clauses by Chinese EFL learners. Analysis of the native-speaker data revealed that initial concessive while-clause is characterized by equivalence or relatedness of topical themes of while-clause and its main clause, confirming the discourse-organizing function of thematic hypotactic clauses. In addition, the pattern of non-human subjects and low-value modal operators (e.g. While this … may …) associated dominantly and exclusively with initial concessive while-clauses in the native corpus serves further evidence of distinctive features of concessive while-clauses. This study adds to a growing body of literature on the SFL approach to second language writing and is among the first to combine corpus-based methodologies and SFL theoretical framework to analyze logico-semantic relations. The study concludes with some pedagogical implications for teaching adversative and concessive while-clauses to EFL learners.",Adversative versus concessive while-clauses in native and learner English texts: A corpus-based systemic functional description,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
789,http://dx.doi.org/10.1093/llc/fqz060,"{'string_id': '10.1093/llc/fqz060', 'id_scheme': 'DOI'}","I model a critical posthumanist pedagogy that uses text analysis software and is aimed at higher education students. A key purpose of the pedagogy is to help students enhance empathetic, critical and independent thinking. For their project assignment, the student chooses an unfamiliar campaign seeking to eliminate suffering and extend rights. They gather all texts from the campaign website into a corpus, which thus represents the campaign writ large. Then they use appropriate software to ascertain, efficiently and rigorously, common campaign concerns across this corpus. This puts students in a position to discern any significant concerns in the campaign corpus that are not addressed in text(s) supporting the status quo which the campaign opposes. Should significant omissions be found, students critically evaluate the status quo text(s) from the campaign’s perspective. Since this perspective derives from the student identifying (at least temporarily) with software generated data, it is a posthuman subjectivity. Engaging digitally and empathetically with a campaign’s data at scale for creation of a posthuman subjectivity can broaden awareness of disadvantage, discrimination, and suffering as well as expand horizons. Moreover, at the end of the assignment, the student is expected to formulate their own position vis-à-vis the previously unfamiliar campaign. Conditions have been created then for the student to enhance independent thinking too.",A posthumanist pedagogy using digital text analysis to enhance critical thinking in higher education,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
790,http://dx.doi.org/10.1093/llc/fqz063,"{'string_id': '10.1093/llc/fqz063', 'id_scheme': 'DOI'}","Our article uses text mining techniques to examine confidential letters sent from the Bank of England’s Prudential Regulation Authority (PRA) to financial institutions it supervises. These letters are a ‘report card’ written to firms annually, and are the most important, regularly recurring written communication sent from the PRA to firms it supervises. Using two complementary machine learning techniques—random forests and logistic ridge regression—we explore whether the letters vary in substance and style depending on the size and importance of the firm to whom the PRA is writing. We find that letters to high impact firms use more evaluative, judgment-based language, and adopt a more forward-looking perspective. We also examine how PRA letters differ from similarly purposed letters written by its predecessor, the Financial Services Authority. We find evidence that PRA letters are different, with a greater degree of forward-looking language and directiveness, reflecting the shift in supervisory approach that has occurred in the UK following the financial crisis of 2007–09.",Text mining letters from financial regulators to firms they supervise,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
791,http://dx.doi.org/10.1093/llc/fqz070,"{'string_id': '10.1093/llc/fqz070', 'id_scheme': 'DOI'}","How to classify short texts effectively remains an important question in computational stylometry. This study presents the results of an experiment involving authorship attribution of ancient Greek texts. These texts were chosen to explore the effectiveness of digital methods as a supplement to the author’s work on text classification based on traditional stylometry. Here it is crucial to avoid confounding effects of shared topic, etc. Therefore, this study attempts to identify authorship using only morpho-syntactic data without regard to specific vocabulary items. The data are taken from the dependency annotations published in the Ancient Greek and Latin Dependency Treebank. The independent variables for classification are combinations generated from the dependency label and the morphology of each word in the corpus and its dependency parent. To avoid the effects of the combinatorial explosion, only the most frequent combinations are retained as input features. The authorship classification (with thirteen classes) is done with standard algorithms—logistic regression and support vector classification. During classification, the corpus is partitioned into increasingly smaller ‘texts’. To explore and control for the possible confounding effects of, e.g. different genre and annotator, three corpora were tested: a mixed corpus of several genres of both prose and verse, a corpus of prose including oratory, history, and essay, and a corpus restricted to narrative history. Results are surprisingly good as compared to those previously published. Accuracy for fifty-word inputs is 84.2–89.6%. Thus, this approach may prove an important addition to the prevailing methods for small text classification.",Author identification of short texts using dependency treebanks without vocabulary,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
792,http://dx.doi.org/10.1093/llc/fqz073,"{'string_id': '10.1093/llc/fqz073', 'id_scheme': 'DOI'}","This article presents a digitally assisted mode of close listening as an innovative way of analysing poetry, through the implementation of a recently developed web-based tool called Visualising Voice, initially conceived to facilitate performance studies of French poetry. This article begins by establishing the status of close listening practices and their importance as a means of studying poetry in French, as well as considering the possibilities afforded by applying these practices to studying poetry in other languages. It then goes on to examine how the Visualising Voice tool can be applied to case studies of two poems—Charles Baudelaire’s ‘L’Albatros’ (‘The Albatross’) and Paul Verlaine’s ‘Green’—each performed by three different speakers. This article argues that close listening using the Visualising Voice tool reveals subtle differences in the handling of metrical features and differences in performance styles of the same poem, which would be unlikely to be perceived by traditional listening methods. This article thus contends that close listening practices not only take the study of poetry beyond traditional modes of textual analysis but also that facilitating these practices through digital methodologies—such as those offered by the Visualising Voice tool—can transform the way in which poetry is read and understood beyond the academic sphere, in particular by general and younger audiences.",Visualising Voice: Analysing spoken recordings of nineteenth-century French poetry,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
793,http://dx.doi.org/10.1093/llc/fqz074,"{'string_id': '10.1093/llc/fqz074', 'id_scheme': 'DOI'}","This study examines whether a qualitative analysis of news headlines produces complementary, convergent, or dissonant findings with a quantitative analysis of the full news story. Headlines are among the most important parts of a news story and its summary. This study investigates the construction of Qaddafi in the headlines of two newspapers before and during the 2011 Libyan civil war. This is based on a sub-corpus of headlines that was taken from a 6.5-million-word corpus of two newspapers; one published in English; The Guardian, and the other in Arabic; Asharq Al-Awsat from 2009 to 2011. The analysis of the headlines has produced complementary and convergent findings with the corpus analysis and suggests that the 2011 Libyan civil war represents a turning point on how Qaddafi is represented in the investigated newspapers. This study concludes that analysing headlines proves to be a good down-sampling option to reduce large news corpora to a workable amount of data.",Analysing headlines as a way of downsizing news corpora: Evidence from an Arabic–English comparable corpus of newspaper articles,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
794,http://dx.doi.org/10.1093/llc/fqz075,"{'string_id': '10.1093/llc/fqz075', 'id_scheme': 'DOI'}","Besides the Holly Quran, Nahj al-Balagha is the main source of literature in Arabian nations, especially for Shia as one of the main branches of the Muslims. Along with literary brilliance, the text of this book covers the broad topics. This research deals with the application of the statistical text and data analysis to extract knowledge from the aphorisms in Nahj al-Balaghah. First, we classify these aphorisms in seven topics. Then, the count of the aphorisms in each category is computed. Finally, the counts of the aphorisms of the categories are compared using the chi-square test and the cluster analysis.",A statistical view to study the aphorisms in Nahj al-Balaghah,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
795,http://dx.doi.org/10.1093/llc/fqz076,"{'string_id': '10.1093/llc/fqz076', 'id_scheme': 'DOI'}","An empirical study on about 1.7 million dictionary words from seven languages viz. English, French, Dutch, Spanish, Italian, Hindi, and German has been conducted. Three intriguing characteristic features have been analyzed. First, the alphabet usage pattern in a language was determined which can be used to give an idea on how alphabets have been employed. For instance, the alphabet ‘e’ is highly used in English, while ‘q’ is least used. Second, the average and range of word lengths in the languages were computed and seen to vary from 1 to 37. Average word lengths were computed in the range (6.665–11.14). For comparison, word lengths have been fitted using Gaussian distribution. Third, a new measure was derived; which we termed ‘Language Sparsity’; computed as one minus ratio of number of words of a particular length already existing to the total number of possible words that can be formed. Sparsity hence gives a measure of the scope of fruition in languages. Two such measures have been defined: a weighted and a nonweighted sparsity. Nonweighted sparsity was found to be minimum (0.877) for English and maximum (0.982) for Dutch. The results obtained can play a significant role in propagating the synergy of language evolution.","Alphabet usage pattern, word lengths, and sparsity in seven Indo-European languages",Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
796,http://dx.doi.org/10.1093/llc/fqz079,"{'string_id': '10.1093/llc/fqz079', 'id_scheme': 'DOI'}","Is there a way to ensure older adults can bridge the digital divide and engage with online cultural heritage? How can cinema-going memories encourage cross-generational engagement? This article proposes to address these issues by using the Italian Cinema Audiences research project as a case study, and specifically cinema-going memories as intangible cultural heritage (Ercole et al., 2016, Cinema heritage in Europe: preserving and sharing culture by engaging with film exhibition and audiences. Editorial. Alphaville: Journal of Film and Screen Media, 11(Summer): 1–12. Web. ISSN: 2009-4078). It aims to tackle the difficulty of engaging the older generation with the digital world, by proposing and testing new ways to resolve it. Through a mixed-methods ethnographic approach, this article investigates different strategies: the use of social media platforms; a cross-generational activity involving Historypin, a digital, user-generated archive of crowdsourced historical material; an online dedicated archive built in collaboration with the older adults involved in the project. These different solutions aim not only at increasing digital engagement among older adults, but also at furthering younger generations’ involvement in shared cultural heritage in an online context. By focusing on the memories of cinema-going in 1950s Italy, the article explores the implications of the advantages and disadvantages of these different approaches. It also tests Anja K. Leist’s research findings (2013, Social media use of older adults: a mini-review. Gerontology, 59(4): 378–84) on the key role of moderators (the younger generation) to help novice users (the older generation) in the ‘continuous engagement’ in digital environments. We conclude that in order to bridge the digital divide two components are necessary simultaneously: the creation of digital platforms in which the older generations are both curators and users, and the support of and interaction with younger generations.",Bridging the digital divide: Older adults’ engagement with online cinema heritage,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
797,http://dx.doi.org/10.1093/llc/fqz081,"{'string_id': '10.1093/llc/fqz081', 'id_scheme': 'DOI'}","This article reports on a study of interfaces to long-lived digital humanities (DH) resources using an innovative combination of research methods from book history, interface design, and digital preservation and curation to investigate how interfaces to DH resources have changed over time. To do this, we used the Internet Archive’s Wayback machine to investigate the original presentation and all subsequent changes to the interfaces of a small sample of projects. The study addresses the following questions: What can we learn from a study of interfaces to DH material? How have interfaces to DH materials changed over the course of their existence? Do these changes affect the way the resource is used, and the way it conveys meaning? Should we preserve interfaces for future scholarship? We show that a valuable information may be derived from the interfaces of long-lived projects. Visual design can communicate subtle messages about the way the resource was originally conceived by its creators and subsequent changes show how knowledge of user behaviour developed in the DH community. Interfaces provide information about the intellectual context of early digital projects. They can also provide information about the changing place of DH projects in local and national infrastructures, and the way that projects have sought to survive in challenging funding environments.","Interfaces, ephemera, and identity: A study of the historical presentation of digital humanities resources",Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
798,http://dx.doi.org/10.1093/llc/fqz083,"{'string_id': '10.1093/llc/fqz083', 'id_scheme': 'DOI'}","Cultural data and information on the web are continuously increasing, evolving, and reshaping in the form of big data due to globalization, digitization, and its vast exploration, with common people realizing the importance of ancient values. Therefore, before it becomes unwieldy and too complex to manage, its integration in the form of big data repositories is essential. This article analyzes the complexity of the growing cultural data and presents a Cultural Big Data Repository as an efficient way to store and retrieve cultural big data. The repository is highly scalable and provides integrated high-performance methods for big data analytics in cultural heritage. Experimental results demonstrate that the proposed repository outperforms in terms of space as well as storage and retrieval time of Cultural Big Data.",CBDR: An efficient storage repository for cultural big data,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
799,http://dx.doi.org/10.1093/llc/fqz084,"{'string_id': '10.1093/llc/fqz084', 'id_scheme': 'DOI'}","A growing body of critical works on the digital expressions of African literature confirms the importance of digital literary studies in Africa. Examples of this growing scholarship include those of Shola Adenekan (2012) and Stephanie Santana (2018). Adenekan’s work focuses on ‘the Internetting of African literature’, while Santana's essay uses digital fiction from South Africa to offer a brilliant treatment of the connections between national spaces and digital networks. Several questions emerge, though, in considering how digital technologies reformulate the form, function, and audience of African literature. How are we, for instance, to understand the role of digital publics? What kind of ideological terrains emerge in online literary representations? What mediations do digital technologies bring to the changing forms and publics of contemporary writing in Africa? How does the ontology of the digital reconfigure the behaviour of the audience in the interpretation of literary meaning and how might previous scholarship such as those of Karin Barber’s on publics and audiences be read in the context of digitality? This article aims to answer these questions by examining some articulations of reader agency to Chinua Achebe’s There was a Country on digital avenues such as Facebook. Aside from an exploration of the intersection of digital culture and African literary forms, I hope to use online responses to Achebe’s memoir to track the trajectories of the new publics of African literature, showing how the ethnopolitics that greeted the publication of Achebe’s wartime narrative explicates the nature of digital publics",Chinua Achebe’s There was a Country and the digital publics of African literature,Oxford University Press (OUP),2019,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
800,http://dx.doi.org/10.1093/llc/fqz085,"{'string_id': '10.1093/llc/fqz085', 'id_scheme': 'DOI'}","Computational stylistics has developed various methods for investigating and attributing authorship of collaborative literary texts. This article investigates ‘precursory authorship’ (Love, 2002): that is, the authorial traces of a source text that inform—to a greater or lesser degree—a subsequent literary output, in order to establish its relevance for our approach to and understanding of the linguistic properties of literary style. Precursory authorship and derivative adaptations are common features of early modern English drama, and the study focusses on two case studies relating to the plays of Restoration playwright, Aphra Behn (c. 1640–89). Using a combination of quantitative methods (Rolling Delta (RD), principal components analysis (PCA), Delta, and Hierarchical Cluster Analysis), the investigation highlights the presence of precursory authorial style in Behn’s The Rover and an anonymous work associated with Behn, The Counterfeit Bridegroom. The results suggest that precursory authorial style is identifiable in both cases, not only through a similarity with the source text but, to a lesser degree, other texts by the precursory author as well. The anonymous play yields complex and non-confirmatory evidence for Behn’s authorship. Methodologically, RD is most sensitive to precursory collaboration. Collectively, the findings highlight the importance of stylistic factors when describing and interpreting literary linguistic quantitative data: precursory authorial style is another facet that intersects with properties such as time period and genre. The article urges a more critical and theoretically informed view of authorially aligned linguistic style.",Stylistic palimpsests: Computational stylistic perspectives on precursory authorship in Aphra Behn’s drama,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
801,http://dx.doi.org/10.1093/llc/fqz086,"{'string_id': '10.1093/llc/fqz086', 'id_scheme': 'DOI'}","Situated within the debate that has taken place in the recent years on how Digital Humanities can break down barriers between countries of the Global North and South (Intersectionality in Digital Humanities Conference, 2016), and how materials in minority langauges can have presence in the network for the generation of new knowledge (Thieberger, 2017; Rodríguez-Ortega and Cruces Rodríguez, 2018), the objective of this article is to explain how (and if) digitalization and Digital Humanities can facilitate research in the Philippines, as well as make it visible, and how this can be facilitated by cooperation projects, citing the example of the project Philperiodicals, carried out by the University of Antwerp and the University of the Philippies. What opportunities and difficulties were encountered upon proposing a project with such characteristics? What problems (ethical, at times) do we encounter when subsidizing projects in the South from the North? We shall address these questions based on the current status of digitalization and Digital Humanities in the country. Lastly, we offer a series of good practices concluded from debates and experiences from the project Philperiodicals, in the hopes that our previous difficulties and discussions may be of use for the development of similar projects in what has been called the Global South. ","Humanidades Digitales en Filipinas: proyectos, dificultades y oportunidades de la colaboración Norte-Sur",Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
802,http://dx.doi.org/10.1093/llc/fqz087,"{'string_id': '10.1093/llc/fqz087', 'id_scheme': 'DOI'}","While the field of digital humanities continues to evolve and expand, the affordances of the digital medium are becoming increasingly applicable to research in the field of education. This article provides an overview of some of the issues involved with publishing educational research as a digital web-based thesis. It also introduces the term multimodal theses and dissertations (MTDs) and reports on a Ph.D. research project which is an early example of an MTD in the field of education. The purpose of this article is three-fold: (1) to stand on the shoulders of those in the digital humanities to expand the growing field of digital scholarship to include education; (2) to report on a range of unanticipated affordances arising from the MTD format in addition to the obvious benefits of ease of use, embedded media, and functionality; (3) to propose a transdisciplinary protocol for digital scholarship to assist researchers, librarians, and graduate school administrators in various disciplines.",A transdisciplinary protocol for digital scholarship,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
803,http://dx.doi.org/10.1093/llc/fqz088,"{'string_id': '10.1093/llc/fqz088', 'id_scheme': 'DOI'}","Stylometric methods can be used to reveal similarities between texts and, combined with network analysis, to depict the stylistic relations between those texts. The research conducted here focuses on a corpus of letters written by Jacob and Wilhelm Grimm. Using stylometric analysis, we model the writing styles of the brothers depending on the addressees and chronology. The brothers have individual styles: Wilhelm has a more friendly and personal tone independent on addresses, while Jacob has a more impersonal style, unless he was writing to Wilhelm. Their styles merge at the interactions of their career or personal development.",The Grimm Brothers: A stylometric network analysis,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
804,http://dx.doi.org/10.1093/llc/fqz091,"{'string_id': '10.1093/llc/fqz091', 'id_scheme': 'DOI'}","Cultural heritage connects the past with the future by forming an integral part of the identities of societies. Thus, countries must protect their cultural heritage and create policies to ensure that people benefit today and pass the heritage on to future generations. Beyond physical protection in the face of globalization, increasing access to cultural heritage is necessary to ensure participation and evaluate practices from an international perspective. Advances in computer and communication technologies are being exploited for these requirements. Acquiring knowledge about and interacting with cultural objects in any part of the world today is possible through the digital humanities approach. In this study, a conceptual model was formed to increase visibility and usage of Kandilli Observatory and Earthquake Research Institute Manuscripts, the Hittite cuneiform tablets from Bogazköy, the works of Ibn Sina at the Süleymaniye Manuscript Library, Evliya Çelebi’s ‘Book of Travels’, and the Old Assyrian Merchant Archives of Kültepe registered in UNESCO’s Memory of the World List. In this model, which is prepared with the digital humanities approach, best practices applied in the field are considered as examples in the literature. Suggestions are made on how to utilize digital humanities tools to increase access and visibility by revealing the economic, social, and cultural values of the works based on the model.",A conceptual model to increase the visibility and usage of cultural heritage objects: The case of UNESCO’s Memory of the World list1,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
805,http://dx.doi.org/10.1093/llc/fqz092,"{'string_id': '10.1093/llc/fqz092', 'id_scheme': 'DOI'}","The moods, feelings, and attitudes represented in a novel will resonate in the reader by activating similar sentiments. It is generally accepted that sentiment analysis can capture aspects of such moods, feelings, and attitudes and can be used to summarize a novel’s plot in a story arc. With the availability of a number of algorithms to automatically extract sentiment-based story arcs, new approaches for their utilization becomes pertinent. We propose to use nonlinear adaptive filtering and fractal analysis in order to analyze the narrative coherence and dynamic evolution of a novel. Using Never Let Me Go by Kazuo Ishiguro, the winner of the 2017 Nobel Prize for Literature as an illustrative example, we show that: (1) nonlinear adaptive filtering can extract a story arc that reflects the tragic trend of the novel; (2) the story arc displays persistent dynamics as measured by the Hurst exponent at short and medium timescales; (3) the plot’s dynamic evolution is reflected in the time-varying Hurst exponent. We argue that these findings are indicative of the potential that multifractal theory has for computational narratology and large-scale literary analysis. Specifically that the global Hurst exponent of a story arc is an index of narrative coherence that can identify bland, incoherent, and coherent narratives on a continuous scale. And, further, that the local time-varying Hurst exponent captures variation of a novel’s plot such that the extrema have specific narratological interpretations.",Dynamic evolution of sentiments in Never Let Me Go: Insights from multifractal theory and its implications for literary analysis,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
806,http://dx.doi.org/10.1093/llc/fqz094,"{'string_id': '10.1093/llc/fqz094', 'id_scheme': 'DOI'}","Color preference in Chinese folksongs is examined from the perspectives of themes, ethnicity, and geographical environment. The results yield that self-organization property of language system plays the role in color use and color preference varies with theme, ethnicity, and geographical environment. Specifically, the color of white is preferred by twenty-three ethnic minorities and the color of red is much more popular among the Han. Only in love songs, the preference for white and red exhibits an approximate north and south dimension. The study shows that digital approaches related to colors in folklore are an effective and promising tool to explore human’s response to colors.",Red or white? Color in Chinese folksongs,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
807,http://dx.doi.org/10.1093/llc/fqz096,"{'string_id': '10.1093/llc/fqz096', 'id_scheme': 'DOI'}","Deep neural networks have been widely used in various language processing tasks. Recurrent neural networks (RNNs) and convolutional neural networks (CNN) are two common types of neural networks that have a successful history in capturing temporal and spatial features of texts. By using RNN, we can encode input text to a lower space of semantic features while considering the sequential behavior of words. By using CNN, we can transfer the representation of input text to a flat structure to be used for classifying text. In this article, we proposed a novel recurrent CNN model to capture not only the temporal but also the spatial features of the input poem/verse to be used for poet identification. Considering the shortcomings of the normal RNNs, we try both long short-term memory and gated recurrent unit units in the proposed architecture and apply them to the poet identification task. There are a large number of poems in the history of literature whose poets are unknown. Considering the importance of the task in the information processing field, a great variety of methods from traditional learning models, such as support vector machine and logistic regression, to deep neural network models, such as CNN, have been proposed to address this problem. Our experiments show that the proposed model significantly outperforms the state-of-the-art models for poet identification by receiving either a poem or a single verse as input. In comparison to the state-of-the-art CNN model, we achieved 9% and 4% improvements in f-measure for poem- and verse-based tasks, respectively.",Recurrent convolutional neural networks for poet identification,Oxford University Press (OUP),2020,,Digital Scholarship in the Humanities,,,"[{'value': '2055-7671', 'type': 'print'}, {'value': '2055-768X', 'type': 'electronic'}]"
808,http://dx.doi.org/10.1145/1367080.1367082,"{'string_id': '10.1145/1367080.1367082', 'id_scheme': 'DOI'}","Any application of three-dimensional computer graphics in the Cultural Heritage (CH) field requires availability of a digital model of the artifact(s) treated. Detailed and accurate digital 3D models can be produced with 3D scanning devices, which allow conversion of reality into digital form in a cost and time-effective manner. We present the capabilities of this technology and the main issues which are preventing its wider use in contemporary applications, highlighting some open problems and a few promising new approaches for 3D model construction. We also briefly review some CH applications which could boost the diffusion and evolution of 3D scanning technology.",Sampled 3D models for CH applications,Association for Computing Machinery (ACM),2008,,Journal on Computing and Cultural Heritage,1,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
809,http://dx.doi.org/10.1145/1367080.1367084,"{'string_id': '10.1145/1367080.1367084', 'id_scheme': 'DOI'}","This article describes an approach to transdisciplinary information integration employing a core ontology. Information is modeled here with an ontology based on the CIDOC-Conceptual Reference Model (ISO 21127). When instantiated with some realistic examples taken from the field of biodiversity (collecting, determination, type creation, expedition, and observation events), the formal specification of semantic concepts makes scientific activities commonly understandable. Ontologies not only allow one to describe the results of scientific activities, such as a description of a biological species, but they can help to clarify the path by which the goal was reached. In particular, they provide a high-level uniform representation of transdisciplinary research activities and results. Ontologies as knowledge representation tools will therefore have strong impact on methodological questions and research strategies for different domains such as biology, archaeology, art history, and socio-economy. They can be regarded as semantic glue between and within scientific and scholarly domains as demonstrated in a series of examples. Following this approach, some requirements for research and development of integrated IT environments between memory instutions are derived.",Research between natural and cultural history information,Association for Computing Machinery (ACM),2008,,Journal on Computing and Cultural Heritage,1,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
810,http://dx.doi.org/10.1145/1367080.1367085,"{'string_id': '10.1145/1367080.1367085', 'id_scheme': 'DOI'}","Decades of research have been devoted to the goal of creating systems which integrate information into a global knowledge network, yet we still face problems of cross-repository interoperability, lack of public infrastructure, and a coherent research agenda—both theoretical and practical—to face these challenges. Interest in the semantic Web has revived the dream, but many are sceptical. This article offers a breakthrough to problems of semantic interoperability and defends the feasibility of a global knowledge network against traditional counterarguments. It offers a new approach based on (i) interdisciplinary research of scholarly and scientific discourse, (ii) a generic global ontological model based on relations and co-reference rather than objects, (iii) semi-automatic maintenance of co-reference links, and (iv) public engagement in the creation and development of the network.",The dream of a global knowledge network—A new approach,Association for Computing Machinery (ACM),2008,,Journal on Computing and Cultural Heritage,1,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
811,http://dx.doi.org/10.1145/1367080.1367086,"{'string_id': '10.1145/1367080.1367086', 'id_scheme': 'DOI'}","Centers for electronic music creation now face strong difficulties to reperform important works considered as belonging to their repertoire. They realize the fragility of works based on electronic modules, subject to technology changes. Sustainability can be achieved in four different ways that are examined in this article: preservation, emulation, migration and virtualization. We discuss these issues, showing the first steps towards solutions at Ircam in the framework of the European project Caspar.","The preservation, emulation, migration, and virtualization of live electronics for performing arts",Association for Computing Machinery (ACM),2008,,Journal on Computing and Cultural Heritage,1,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
812,http://dx.doi.org/10.1145/1434763.1434766,"{'string_id': '10.1145/1434763.1434766', 'id_scheme': 'DOI'}","This research addresses the question of what is an acceptable period between conservation treatments. Taking the cleaning of impressionist paintings as a case study, survival analysis is carried out on metadata from conservation records for the Tate, UK, the National Gallery in Oslo, Norway, and the National Trust, UK. Currently the only guidance available is offered by the National Trust. Based on cost-benefit analysis, it suggests an interval of 25 years for light surface cleaning and 100 years for major treatment. The majority of paintings held by the National Trust are pre-20th century, with a varnish layer protecting the paint surface. Arguments are put forward that this guidance cannot be applied to unvarnished paintings with exposed ground and a fragile paint layer. The cleaning process for these types of paintings poses a greater risk as it inevitably causes some loss of paint. Any cleaning of paintings in this category is more akin to a major conservation treatment. The Kaplan Meier Product Limit estimator uses metadata collected from the conservation records to estimate an average inter-cleaning period for the Tate, UK, and National Gallery, Oslo. The result of the theoretical analysis, although highly dependent on the quality of conservation records available, is supplemented by evidence from practical experience. Current views on acceptable frequency of cleaning and the associated risks of treatment are sought using a survey of conservators with experience of treating impressionist and post-impressionist paintings. This is applied to a case study of the Munch paintings in the University Aula, Oslo, that are currently unacceptably dirty. In the past, these paintings have been cleaned on average once every 14 years, which is clearly too often. The results suggest that the intercleaning period should be increased by a factor of three. Criteria are discussed for improving the environment within the University Aula, Oslo, to achieve this goal.",Using survival analysis on conservation metadata to benchmark treatment frequency,Association for Computing Machinery (ACM),2008,,Journal on Computing and Cultural Heritage,1,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
813,http://dx.doi.org/10.1145/1434763.1434767,"{'string_id': '10.1145/1434763.1434767', 'id_scheme': 'DOI'}","In this article, we propose a general computational model for the extraction of symmetry features of Islamic geometrical patterns' (IGP) images. We describe IGP images using the discrete symmetry groups theory. Our model contains the three following steps. (1) By noting that these patterns fall into three major categories, we begin our indexation process by classifying every pattern into one of these categories. The first pattern category describes all the patterns generated by translation along one direction. Every pattern of this category can be classified into one of the seven Frieze groups. The second type of pattern contains translational symmetries in two independent directions. Patterns of this category can be classified into one of the seventeen Wallpaper groups. The last type, called rosettes, describes patterns which begin at a central point and grow radially outward. We use rosette symmetry groups to classify patterns of this latter category. (2) For every pattern, we extract the symmetry features, namely, the symmetry group and the fundamental region, which is a representative region in the image from which the whole image can be regenerated. But for rosette groups, we can also compute the number of folds. (3) Finally, we describe the fundamental region by a simple color histogram and build the feature vector which is a combination of the symmetry feature (defined in the second step) and histogram information. Experiments show promising results for either IGP images' classification or indexing. Efforts for the subsequent task of classifying Islamic geometrical patterns' images can be significantly reduced.",Islamic geometrical patterns indexing and classification using discrete symmetry groups,Association for Computing Machinery (ACM),2008,,Journal on Computing and Cultural Heritage,1,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
814,http://dx.doi.org/10.1145/1434763.1434768,"{'string_id': '10.1145/1434763.1434768', 'id_scheme': 'DOI'}","Traveling through a single virtual environment only tells part of the story; a particularly interesting aspect is to illustrate how an area has developed over time. This article presents a unified approach to illustrating four-dimensional data concerning a cultural heritage site. The proposed framework provides a semi-automatic approach to both reconstructing the environment and bringing all the time-dependent models into an intuitive visualization package. For each time period considered for reconstruction, the system requires a set of building footprint maps depicting the layout of the environment plus a few statistics. The statistics govern the construction of three-dimensional building models, allowing each building's architectural style, typical building height, and roof style to be altered. This information is automatically processed and converted into a form that can be visualized. By integrating high quality landmark buildings from laser scanning or interactive modelling packages into the automatically generated scene, the cultural heritage site is realized both in a spatial and temporal context. The visualization is achieved via a 4D navigable movie which is presented using two concrete implementations written using Flash and OpenGL. The OpenGL-based implementation allows a collection of 3DS Max scenes to be automatically visualized requiring only a set of camera paths identified by the user.",Exploring cultural heritage sites through space and time,Association for Computing Machinery (ACM),2008,,Journal on Computing and Cultural Heritage,1,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
815,http://dx.doi.org/10.1145/1434763.1434769,"{'string_id': '10.1145/1434763.1434769', 'id_scheme': 'DOI'}","The current methods to describe the shape of three-dimensional objects can be classified into two groups: methods following the composition of primitives approach and descriptions based on procedural shape representations. As a 3D acquisition device returns an agglomeration of elementary objects (e.g. a laser scanner returns points), the model acquisition pipeline always starts with a composition of primitives. Due to the semantic information carried with a generative description, a procedural model provides valuable metadata that make up the basis for digital library services: retrieval, indexing, and searching.An important challenge in computer graphics in the field of cultural heritage is to build a bridge between the generative and the explicit geometry description combining both worlds—the accuracy and systematics of generative models with the realism and the irregularity of real-world data. A first step towards a semantically enriched data description is a reconstruction algorithm based on decreasing exponential fitting. This approach is robust towards outliers and multiple dataset mixtures. It does not need a preceding segmentation and is able to fit a generative shape template to a point cloud identifying the parameters of a shape.",Semantic fitting and reconstruction,Association for Computing Machinery (ACM),2008,,Journal on Computing and Cultural Heritage,1,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
816,http://dx.doi.org/10.1145/1551676.1551677,"{'string_id': '10.1145/1551676.1551677', 'id_scheme': 'DOI'}","Anastylosis is the archaeological and architectural reconstruction of a ruined monument at the historic site after careful study of the remaining original elements. We present research results concerning 3D technologies that are used in the digital anastylosis of cultural heritage monuments. Based on current state-of-the-art research, we present 3D data collection, digital artifacts reconstruction, and digital reassembly of existing fragments illustrated by means of the Octagon monument in Ephesos, Turkey. Our focus is on methods that belong to geometry processing.",Digital anastylosis of the Octagon in Ephesos,Association for Computing Machinery (ACM),2009,,Journal on Computing and Cultural Heritage,2,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
817,http://dx.doi.org/10.1145/1551676.1551678,"{'string_id': '10.1145/1551676.1551678', 'id_scheme': 'DOI'}","The article reports the interdisciplinary project of the virtualization of the Great Inscription of Gortyna, Crete, for 3D documentation, structural studies, and physical replica purposes. The digitization of the longest epigraphic text of the Greek civilization (6 m long and 1.75 m high, with approximately 2--3 mm-depth engraved letters) and its surrounding heritage area (around 30 × 30 m), required long planning and the construction of a dedicated acquisition system to speed up the surveying time, limited to few hours per day. Primarily, range sensors were employed in a multi-resolution way, digitizing detailed parts in high resolution and less smoothed areas with lower geometric resolution. Some selected areas were also modeled with our multiphoto geometrically constrained image matching approach to demonstrate that the same accuracy and details can be achieved using either scanners or photogrammetry. The derived 3D model of the heritage is now the basis for further archaeological studies on the incision techniques and a deeper structural analysis on the monument. The challenges of the work stay in the acquisition, processing, and integration of the multi-resolution data as well as their interactive visualization.",3D modeling of complex and detailed cultural heritage using multi-resolution data,Association for Computing Machinery (ACM),2009,,Journal on Computing and Cultural Heritage,2,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
818,http://dx.doi.org/10.1145/1551676.1551679,"{'string_id': '10.1145/1551676.1551679', 'id_scheme': 'DOI'}","In the last few years, the number of projects involving historical reconstruction has increased significantly. Recent technologies have proven a powerful tool for a better understanding of our cultural heritage through which to attain a glimpse of the environments in which our ancestors lived. However, to accomplish such a purpose, these reconstructions should be presented to us as they may really have been perceived by a local inhabitant, according to the illumination and materials used back then and, equally important, the characteristics of the human visual system.The human visual system has a remarkable ability to adjust itself to almost all everyday scenarios. This is particularly evident in extreme lighting conditions, such as bright light or dark environments. However, a major portion of the visible spectra captured by our visual system cannot be represented in most display devices. High dynamic range imagery is a field of research which is developing techniques to correct such inaccuracies. This new viewing paradigm is perfectly suited for archaeological interpretation, since its high contrast and chromaticity can present us with an enhanced viewing experience, closer to what an inhabitant of that era may have seen.In this article we present a case study of the reconstruction of a Roman site. We generate high dynamic range images of mosaics and frescoes from one of the most impressive monuments in the ruins of Conimbriga, Portugal, an ancient city of the Roman Empire. To achieve the requisite level of precision, in addition to having a precise geometric 3D model, it is crucial to integrate in the virtual simulation authentic physical data of the light used in the period under consideration. Therefore, in order to create a realistic physical-based environment, we use in our lighting simulations real data obtained from simulated Roman luminaries of that time.",High dynamic range—a gateway for predictive ancient lighting,Association for Computing Machinery (ACM),2009,,Journal on Computing and Cultural Heritage,2,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
819,http://dx.doi.org/10.1145/1551676.1551680,"{'string_id': '10.1145/1551676.1551680', 'id_scheme': 'DOI'}","It is still a big technical problem to establish a relation between a shape and its meaning in a sustainable way. We present a solution with a markup method that allows for labeling parts of a 3D object very much like labeling parts of a hypertext. A 3D markup can serve both as hyperlink and as link anchor, which is the key to bidirectional linking between 3D objects and Web documents. Our focus is on a sustainable 3D software infrastructure for application scenarios ranging from email and Internet over authoring and browsing semantic networks to interactive museum presentations. We demonstrate the workflow and the effectiveness of our tools by redoing the Arrigo 3D Showcase. We are working towards a best practice example for information modeling in cultural heritage.",The arrigo showcase reloaded—towards a sustainable link between 3D and semantics,Association for Computing Machinery (ACM),2009,,Journal on Computing and Cultural Heritage,2,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
820,http://dx.doi.org/10.1145/1613672.1613673,"{'string_id': '10.1145/1613672.1613673', 'id_scheme': 'DOI'}","This article presents a digital simulation of the intonarumori, musical instruments invented by the Italian Futurist composer and painter Luigi Russolo. By building the physical models of different members of the intonarumori family and a physical interface, a preservation of an important contribution to the musical heritage of the beginning of the 20th century is achieved.",The musical heritage of futurism,Association for Computing Machinery (ACM),2009,,Journal on Computing and Cultural Heritage,2,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
821,http://dx.doi.org/10.1145/1613672.1613674,"{'string_id': '10.1145/1613672.1613674', 'id_scheme': 'DOI'}","In this article, the wind flow over one of the most important Egyptian historical heritage sites, the Giza Plateau, was investigated using the Computational Fluid Dynamics (CFD) state-of-the-art techniques. The present study addresses the influences of wind flow structure, as an important denudation factor, on the site and its famous monuments: the Pyramids and the Great Sphinx. Three-dimensional CFD simulations have been performed based on the Reynolds Averaged Navier-Stokes (RANS) equations for the cases of the northwest wind, at the average wind speed over the year, and the southwest windstorms. In addition, the wind-driven sand was considered for the same cases. Particular attention was paid to the Great Sphinx and the Pyramids to investigate their parts most vulnerable to the wind, which is contributing to the erosion of these monuments. The Great Sphinx was buried by sand and it was cleared several times throughout its history. In this study, we also address the less understood, yet important, burial mechanism of the Great Sphinx. The present work may give more insight to the effect of wind around the Giza Plateau when developing a global plan for conserving and protecting the site.",Wind flow modeling and simulation over the Giza Plateau cultural heritage site in Egypt,Association for Computing Machinery (ACM),2009,,Journal on Computing and Cultural Heritage,2,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
822,http://dx.doi.org/10.1145/1658346.1658347,"{'string_id': '10.1145/1658346.1658347', 'id_scheme': 'DOI'}","The increasing creation of 3D cultural heritage models has resulted in a need for the establishment of centralized digital archives. We advocate open repositories of scientifically authenticated 3D models based on the example of traditional scholarly journals, with standard mechanisms for preservation, peer review, publication, updating, and dissemination of the 3D models. However, fully realizing this vision will require addressing a number of related research challenges.In this article, we first give a brief background of the virtual heritage discipline, and characterize the need for centralized 3D archives, including a preliminary needs assessment survey of virtual heritage practitioners. Then we describe several existing 3D cultural heritage repositories, and enumerate a number of technical research challenges that should be addressed to realize an ideal archive. These challenges include digital rights management for the 3D models, clear depiction of uncertainty in 3D reconstructions, version control for 3D models, effective metadata structures, long-term preservation, interoperability, and 3D searching. Other concerns are provision for the application of computational analysis tools, and the organizational structure of a peer-reviewed 3D model archive.",Research challenges for digital archives of 3D cultural heritage models,Association for Computing Machinery (ACM),2009,,Journal on Computing and Cultural Heritage,2,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
823,http://dx.doi.org/10.1145/1658346.1658348,"{'string_id': '10.1145/1658346.1658348', 'id_scheme': 'DOI'}","This article presents Ottoman Archives Explorer, a Content-Based Retrieval (CBR) system based on character recognition for printed and handwritten historical documents. Several methods for character segmentation and recognition stages are investigated. In particular, sliding-window and histogram segmentation methods are coupled with recognition approaches using spatial features, neural networks, and a graph-based model. The prototype system provides CBR of document images using both example-based queries and a virtual keyboard to construct query words.",Ottoman archives explorer,Association for Computing Machinery (ACM),2009,,Journal on Computing and Cultural Heritage,2,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
824,http://dx.doi.org/10.1145/1709091.1709092,"{'string_id': '10.1145/1709091.1709092', 'id_scheme': 'DOI'}","Flash light of digital cameras is a very useful way to picture scenes with low-quality illumination. Nevertheless, especially in low-end cameras integrated flash lights are considered as not reliable for high-quality images, due to known artifacts (sharp shadows, highlights, uneven lighting) generated in images. Moreover, a mathematical model of this kind of light is difficult to create. In this article we present a color correction space which, given some information about the geometry of the pictured scene, is able to provide a space-dependent color correction for each pixel of the image. The correction space can be calculated once in a lifetime using a quite fast acquisition procedure; after 3D spatial calibration, the obtained color correction function can be applied to every image where flash is the dominant light source. We developed this approach to produce better color samples in the application framework of color mapping on 3D scanned models. The correction space proposed presents several advantages: it is independent from the kind of light used (provided that it is bound to the camera), it gives the possibility to correct some artifacts (for example, color deviation) introduced by flash light, and it has a wide range of possible applications, from image enhancement to material color estimation. Moreover, once that the inverse photo-to-geometry transformation is known, it allows the easy estimation of the flash light position and permits to identify and remove other annoying artifacts, like highlights and shadows. The resulting approach allows to gather in an easy manner a better and more consistent color information and to produce higher-quality 3D models.",Improved color acquisition and mapping on 3D models via flash-based photography,Association for Computing Machinery (ACM),2010,,Journal on Computing and Cultural Heritage,2,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
825,http://dx.doi.org/10.1145/1805961.1805962,"{'string_id': '10.1145/1805961.1805962', 'id_scheme': 'DOI'}","In this article we present a general classification of the different approaches that might be employed to constitute a visual representation of a cultural heritage item, including the ones featuring the use of traditional tools as the ones exhibiting the inclusion of modern 2D and 3D digital technologies. In order to establish a coherent taxonomy, specific elements characterizing such approaches will be discussed and employed to assist the definition of a general conceptual framework that will enable the classification of the possible design choices according to their specific characteristics. Finally, a selection of modeling and simulation techniques which are specifically related to the creation process underlying the virtual representation of heritage items by means of modern visualization technologies will be explored alongside the main areas of preferences linked to the major actors active in the cultural heritage field.",A taxonomy of visualization strategies for cultural heritage applications,Association for Computing Machinery (ACM),2010,,Journal on Computing and Cultural Heritage,3,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
826,http://dx.doi.org/10.1145/1805961.1805963,"{'string_id': '10.1145/1805961.1805963', 'id_scheme': 'DOI'}","Even decades after the advent of computer technology, preserving information in digital archives remains a challenging task. Technological progress on all levels calls for migration as an essential component of any digital archive. Migration, however, is not only expensive, but also makes it difficult to ensure the authenticity of digital documents. The University of Basel's Imaging and Media Lab has developed PEVIAR (Permanent Visual Archive), an archiving solution addressing one of the fundamental challenges of digital archiving, migration, and introducing the notion of digital originals. Essentially, digital documents are inseparably bound to a migration-free medium, thereby overcoming the transient nature of today's state-of-the-art storage systems.",PEVIAR,Association for Computing Machinery (ACM),2010,,Journal on Computing and Cultural Heritage,3,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
827,http://dx.doi.org/10.1145/1841317.1841318,"{'string_id': '10.1145/1841317.1841318', 'id_scheme': 'DOI'}","In this article, an integrated conjecture about the method of drawing of monumental prehistoric wall-paintings is presented and supported. Specifically, the article deals with paintings that initially decorated the internal walls of the highest floor of a building, called “Xeste 3”, at Akrotiri of the Greek island of Thera circa. 1650 B.C. It is argued that these wall-paintings could had been drawn while the brush was guided by an apparatus, which corresponds to advanced for the era of geometric prototypes with impressive precision. A set of assumptions concerning the actions the artists might have taken in order to create the spiral themes is stated and supported. These assumptions refer to the existence of a draft plan, the sequence of brush strokes, the placement of the brush on the wall, as well as the possible form of the apparatus. These conjectures are evaluated and tested by means of curve fitting and image analysis methods developed by the authors. The results indicate that all drawn contour parts optimally fit along a single prototype linear spiral with fitting error of less than 0.4mm, supporting existence of a very advanced culture for the era of geometric guide. It is statistically rejected that this guide could have the form of a stamp. Moreover, there is strong evidence that the painter might have used a draft plan of the spiral themes to prepare the final drawing and that the linear spiral guide has been used by alternating its placements in order to form the internal and external spiral contour.",Image and pattern analysis for the determination of the method of drawing celebrated thera wall-paintings circa 1650 B.C.,Association for Computing Machinery (ACM),2010,,Journal on Computing and Cultural Heritage,3,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
828,http://dx.doi.org/10.1145/1841317.1841319,"{'string_id': '10.1145/1841317.1841319', 'id_scheme': 'DOI'}","ArcheoTUI is a new tangible user interface for the efficient assembly of the 3D scanned fragments of fractured archeological objects. An efficient user interaction for the complex task to orientate or position two 3D objects relative to each other is essential, eventually in addition to automatic matching techniques.Our key idea is to use tangible props for the manipulation of the virtual fragments. In each hand, the user manipulates an electromagnetically tracked prop, and the translations and rotations are directly mapped to the corresponding virtual fragments on the display.For each hand, a corresponding foot pedal is used to clutch the movements of the hands. Hence, the user's hands can be repositioned, or the user can be switched. The software of ArcheoTUI is designed to easily change assembly hypotheses, beyond classical undo/redo, by using a scene graph.We designed ArcheoTUI on the demand of archeaologists and in a direct collaboration with them, and we conducted two user studies on site at their workplace. The first user study revealed that the interface, and especially the foot pedal, was accepted, and that all the users managed to solve simple assembly tasks. In a second user study, we compare a different clutching mechanism with buttons on the props to the foot pedal mechanism. This second user study revealed that the movement of the hands is more similar to real-world assembly scenarios when using the foot pedals, and that the users can keep on concentrating on the actual assembly task.Finally, we show how the virtual assembly is used for a fractured archeological finding.",ArcheoTUI—Driving virtual reassemblies with tangible 3D interaction,Association for Computing Machinery (ACM),2010,,Journal on Computing and Cultural Heritage,3,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
829,http://dx.doi.org/10.1145/1841317.1841320,"{'string_id': '10.1145/1841317.1841320', 'id_scheme': 'DOI'}","The Chinese zither, called guqin, has existed for over 3,000 years and always played an important role in Chinese social history. An interesting but unfortunate fact is that the traditional notation of guqin music does not provide the duration information for each music note which requires the player to learn from his teacher and memorize. As a result, among several thousands of compositions that have been created and recorded with guqin music notation, only around 100 of them are still being played today. In this article we use a machine learning method to study the guqin music recovery problem which tries to use the guqin music notation to recover the duration of each music note. Information provided by the music note is used as features to predict the duration information with a support vector machine. The experimental result shows that our system can predict with fair accuracy, and can be used as a valuable reference for human guqin masters to recover guqin music.",Ancient Chinese zither (guqin) music recovery with support vector machine,Association for Computing Machinery (ACM),2010,,Journal on Computing and Cultural Heritage,3,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
830,http://dx.doi.org/10.1145/1841317.1841321,"{'string_id': '10.1145/1841317.1841321', 'id_scheme': 'DOI'}","We propose a set of dynamic shading enhancement techniques for improving the perception of details, features, and overall shape characteristics from images created with Reflectance Transformation Imaging (RTI) techniques. Selection of these perceptual enhancement filters can significantly improve the user's ability to interactively inspect the content of 2D RTI media by zooming, panning, and changing the illumination direction. In particular, we present two groups of strategies for RTI image enhancement based on two main ideas: exploiting the unsharp masking methodology in the RTI-specific context; and locally optimizing the incident light direction for improved RTI image sharpness and illumination of surface features. The Result section will present a number of datasets and compare them with existing techniques.",Dynamic shading enhancement for reflectance transformation imaging,Association for Computing Machinery (ACM),2010,,Journal on Computing and Cultural Heritage,3,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
831,http://dx.doi.org/10.1145/1841317.1841322,"{'string_id': '10.1145/1841317.1841322', 'id_scheme': 'DOI'}","The time restrictions that apply in museum learning increase the need for adaptive and/or adaptable technologies. However, deriving a visitor's profile is not an easy task, since most common ways (asking direct questions, recording user actions) are either intrusive or time consuming. Observing the visitors' movement (visiting style) within the museum could provide valuable information regarding adaptivity. In the present study, issues of visiting style were explored and statistical significance was found once different factors were analyzed. Most importantly, there seems to be a connection between the way people move in a museum and the way they prefer to approach and process information cognitively. Environmental factors that can affect the expression of visiting style were also identified.",Modeling visitors' profiles,Association for Computing Machinery (ACM),2010,,Journal on Computing and Cultural Heritage,3,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
832,http://dx.doi.org/10.1145/1921614.1921615,"{'string_id': '10.1145/1921614.1921615', 'id_scheme': 'DOI'}","Modeling human argumentation should shed light on how knowledge described in information systems could be better accessed, structured, and used for real life research purposes. Current argumentation models are either not analytical enough or restricted to formal logic. For that purpose, we seek a model of human argumentation in which reasoning may not only consist of falsification or verification but more generally of strengthening or weakening hypotheses, and a way to connect this model to an ontology of the domain of discourse. We have studied examples of factual argumentation in empirical research in archaeology. Based on this and other empirical material, we propose an innovative integrated model of factual argumentation that includes evolution, composition, and revision of arguments. It makes explicit both the processes of argument-making and the states of belief at a particular point in time in a composite inference, and connects explicitly to a domain ontology, free of tacit background knowledge. We have implemented the model in a more restricted form and tested it with published archaeological examples. Future work may generalize the model to other kinds of argumentation.",Factual argumentation—a core model for assertions making,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,3,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
833,http://dx.doi.org/10.1145/1921614.1921616,"{'string_id': '10.1145/1921614.1921616', 'id_scheme': 'DOI'}","The guqin is an ancient Chinese musical instrument that has secured a key position in Chinese elite culture. The guqin has its own special notation composed of reduced characters. The input system of the guqin notation is necessary for digital storage and analysis of guqin notation. This article presents the first handwriting input system of guqin notation. This system adopts a radical-based approach. In this approach, the radicals of the input reduced character are extracted first. Next the layout template that best matches the character is found. The name of the character can then be interpreted according to the radicals and the matching layout template. Radical extraction is the key step of any radical-based approach. In this article a method of radical extraction for online cursive characters is proposed. Experimental results show that the approach is efficient and that the input system performs well.",Handwriting input system of chinese guqin notation,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,3,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
834,http://dx.doi.org/10.1145/1921614.1921617,"{'string_id': '10.1145/1921614.1921617', 'id_scheme': 'DOI'}","Exhibits within cultural heritage collections such as museums and art galleries are arranged by experts with intimate knowledge of the domain, but there may exist connections between individual exhibits that are not evident in this representation. For example, the visitors to such a space may have their own opinions on how exhibits relate to one another. In this article, we explore the possibility of estimating the perceived relatedness of exhibits by museum visitors through a variety of ontological and document similarity-based methods. Specifically, we combine the Wikipedia category hierarchy with lexical similarity measures, and evaluate the correlation with the relatedness judgements of visitors. We compare our measure with simple document similarity calculations, based on either Wikipedia documents or Web pages taken from the Web site for the museum of interest. We also investigate the hypothesis that physical distance in the museum space is a direct representation of the conceptual distance between exhibits. We demonstrate that ontological similarity measures are highly effective at capturing perceived relatedness and that the proposed RACO (Related Article Conceptual Overlap) method is able to achieve results closest to relatedness judgements provided by human annotators compared to existing state-of-the art measures of semantic relatedness.",Using ontological and document similarity to estimate museum exhibit relatedness,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,3,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
835,http://dx.doi.org/10.1145/1921614.1921618,"{'string_id': '10.1145/1921614.1921618', 'id_scheme': 'DOI'}","Technology can play a crucial role in supporting museum visitors and enhancing their overall museum visit experiences. Visitors coming to a museum do not want to be overloaded with information, but to receive the relevant information, learn, and have an overall interesting experience. To serve this goal, a user-friendly and flexible system is needed. The design of such a system poses several challenges that need to be addressed in parallel. The user interface should be intuitive and let the visitors focus on the exhibits, not on the technology. Content and delivery must provide relevant information and at the same time allow visitors to get the level of detail and the perspectives in which they are interested. Personalization may play a key role in providing relevant information to individuals. Yet, since visitors tend to visit the museum in small groups, technology should also contribute to and facilitate during-the-visit communication or post-visit group interaction. The PIL project applied at the Hecht museum extended the research results of the PEACH project and tried to address all of these considerations. Evaluation involving users substantiated several aspects of the design.",A visitor's guide in an active museum,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,3,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
836,http://dx.doi.org/10.1145/1957825.1957826,"{'string_id': '10.1145/1957825.1957826', 'id_scheme': 'DOI'}","We have created a tool called ImaNote (Image Map Annotation Notebook) for viewing, zooming, and annotating arbitrary-sized images using a Web browser. Multiple users can work with the tool simultaneously, being able to almost instantly see and edit each other's modifications. In this article we describe the design, implementation, and user interface of ImaNote focusing on issues arising from the Web-based nature of the software, as well as from its intended use as a tool for visualization and interaction with digital surrogates, facsimiles, and replicas of cultural heritage objects.",ImaNote,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,3,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
837,http://dx.doi.org/10.1145/1957825.1957827,"{'string_id': '10.1145/1957825.1957827', 'id_scheme': 'DOI'}","The dichotomy between full detail representation and the efficient management of data digitization is still a big issue in the context of the acquisition and visualization of 3D objects, especially in the field of the cultural heritage. Modern scanning devices enable very detailed geometry to be acquired, but it is usually quite hard to apply these technologies to large artifacts. In this article we present a project aimed at virtually reconstructing the impressive (7×11 m.) portal of the Ripoll Monastery, Spain. The monument was acquired using triangulation laser scanning technology, producing a dataset of 2212 range maps for a total of more than 1 billion triangles. All the steps of the entire project are described, from the acquisition planning to the final setup for dissemination to the public. We show how time-of-flight laser scanning data can be used to speed-up the alignment process. In addition we show how, after creating a model and repairing imperfections, an interactive and immersive setup enables the public to navigate and display a fully detailed representation of the portal. This article shows that, after careful planning and with the aid of state-of-the-art algorithms, it is now possible to preserve and visualize highly detailed information, even for very large surfaces.",Multiscale acquisition and presentation of very large artifacts,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,3,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
838,http://dx.doi.org/10.1145/2001416.2001417,"{'string_id': '10.1145/2001416.2001417', 'id_scheme': 'DOI'}","Digital documentation and reconstruction is a comparatively recent phenomenon in the history of architectural heritage documentation and conservation. It has emerged as a result of technological development, especially in computer applications that have extended our capabilities to cope up with the knowledge systems of increasing broad range and complexity.For most people, including architects, heritage and conservation mean historical buildings and their conservation. However, there is another facet to it that is not well recognized; the area of “literary architectural heritage, ” that is, the literature on historical architecture. In the literary architectural heritage of India, many architectural treatises are available for graphical documentation of the built examples described in them. This article aims to present digital graphical documentation of the pavilions described in the Mayamatam, an architectural treatise of ancient India.Research in this area will enrich the knowledge base of architecture. Having this information in a digital format like CAD drawings, virtual 3D models would result in better communication, storage, and retrieval for editing, modifications, and analysis. This digital documentation and representation provides the possibility of being used as a teaching aid for students and architects and also for recovering lost knowledge in the context of partially destroyed structures. For architects and designers, who constantly search for new ways, the digital presentation of ancient Indian pavilions provide a design tool for discovering new patterns to be used in contemporary architecture.",Digital reconstruction of pavilions described in an ancient Indian architectural treatise,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,4,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
839,http://dx.doi.org/10.1145/2001416.2001418,"{'string_id': '10.1145/2001416.2001418', 'id_scheme': 'DOI'}","This article introduces a model and a system for embodied social active listening to sound and music content. The model is based on the simultaneous navigation/exploration of multiple maps, starting from a low-level physical map, up to a high-level emotional, affective map. The paper discusses the concepts underlying such a model, the system implementing it, and two concrete examples: The Orchestra Explorer designed for the Museo degli Strumenti Musicali at Accademia Nazionale di Santa Cecilia, Roma, Italy, and the interactive dance and music performance The Bow Is bent and Drawn (composer Nicola Ferrari), presented at Casa Paganini, Genova, Italy, in occasion of the opening concert of the 8th Intl. Conference on New Interfaces for Musical Expression (NIME08), June 4, 2008, and in museum interactive experiences at Festival della Scienza, Genova. This work is part of current research at Casa Paganini—InfoMus on embodied social active listening to sound and music content through the analysis and processing of expressiveness in human full-body movement and gesture. A user-centric interactive multimedia system architecture is proposed, operating on prerecorded music. From the perspective of valorization of cultural heritage, this research provides engaging paradigms of interaction with prerecorded music content, enabling a large number of nonexpert users to rediscover the musical heritage (e.g., classical and contemporary music) they may not be familiar with. Research was carried out in the framework of the EU-ICT Project SAME (Sound and Music for Everyone, Everyday, Everywhere, Every Way, www.sameproject.eu, 2008–2010) and has been recently extended to active experience of audiovisual content, and in particular to cultural heritage and museum scenarios: the novel interactive permanent museum exhibition Viaggiatori di Sguardo has been designed and developed at Palazzo Ducale, Genova, enabling visitors to explore virtually the UNESCO Heritage Palazzi dei Rolli in Genova.",A system for embodied social active listening to sound and music content,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,4,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
840,http://dx.doi.org/10.1145/2001416.2001419,"{'string_id': '10.1145/2001416.2001419', 'id_scheme': 'DOI'}","In this article, a general framework for the retrieval of artistic line-drawings is introduced. It relies on the pictorial content, defined as a combination of the stylistic content and the visual features of the represented subject. First, we propose an automatic method for the extraction of stroke contours in line drawings, relying on a filtering of the level lines of images. Next, the radius of the drawing tool is estimated from these segmented strokes. This information then efficiently tunes the extraction of several geometric features, including the distribution of curvature, endpoints, junctions and corners of strokes. The efficiency of the proposed method is illustrated with several experiments on two classified databases of artistic line-drawings, and compared with an approach based on the curvature scale space (CSS). Retrieval experiments suggest that the proposed framework is able to handle the pictorial effect delivered by line drawings to a human observer.",Artistic line-drawings retrieval based on the pictorial content,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,4,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
841,http://dx.doi.org/10.1145/2037820.2037822,"{'string_id': '10.1145/2037820.2037822', 'id_scheme': 'DOI'}","The aim of this work is to bring the cultural heritage of two-dimensional art closer to being accessible by blind and visually impaired people. We present a computer-assisted workflow for the creation of tactile representations of paintings, suitable to be used as a learning tool in the context of guided tours in museums or galleries. Starting from high-resolution images of original paintings, our process allows an artist to quickly design the desired form, and generate data suitable for rapid prototyping machines to produce the physical touch tools. Laser-cut layered depth diagrams convey not only the individual objects in the painting and their spatial layout, but also augment their depth relations. CNC-milled textured reliefs additionally render fine details like brush strokes and texture suitable for the sense of touch. Our methods mimic aspects of the visual sense, make sure that the haptic output is quite faithful to the original paintings, and do not require special manual abilities like sculpting skills.",High-quality tactile paintings,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,4,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
842,http://dx.doi.org/10.1145/2037820.2037823,"{'string_id': '10.1145/2037820.2037823', 'id_scheme': 'DOI'}","We present an efficient scalable streaming technique for mapping highly detailed color information on extremely dense point clouds. Our method does not require meshing or extensive processing of the input model, works on a coarsely spatially reordered point stream, and can adaptively refine point cloud geometry on the basis of image content. Seamless multiband image blending is obtained by using GPU-accelerated screen-space operators, which solve point set visibility, compute a per-pixel view-dependent weight, and ensure a smooth weighting function over each input image. The proposed approach works independently on each image in a memory-coherent manner, and can be easily extended to include further image-quality estimators. The effectiveness of the method is demonstrated on a series of massive real-world point datasets.",Fast low-memory seamless photo blending on massive point clouds using a streaming framework,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,4,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
843,http://dx.doi.org/10.1145/2037820.2037824,"{'string_id': '10.1145/2037820.2037824', 'id_scheme': 'DOI'}","One of the main problems faced during reconstruction of fractured archaeological artifacts is sorting through a large number of candidate matches between fragments to find the relatively few that are correct. Previous computer methods for this task provided scoring functions based on a variety of properties of potential matches, including color and geometric compatibility across fracture surfaces. However, they usually consider only one or at most a few properties at once, and therefore provide match predictions with very low precision. In this article, we investigate a machine learning approach that computes the probability that a match is correct based on the combination of many features. We explore this machine learning approach for ranking matches in three different sets of fresco fragments, finding that classifiers based on many match properties can be significantly more effective at ranking proposed matches than scores based on any single property alone. Our results suggest that it is possible to train a classifier on match properties in one dataset and then use it to rank predicted matches in another dataset effectively. We believe that this approach could be helpful in a variety of cultural heritage reconstruction systems.",Learning how to match fresco fragments,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,4,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
844,http://dx.doi.org/10.1145/2050096.2050097,"{'string_id': '10.1145/2050096.2050097', 'id_scheme': 'DOI'}","The article reports the results of a usability test applying digital smartpen technology and interactive audio booklets to assist the Miami Tribe of Oklahoma in revitalization of the Myaamia language and culture. The usability test was conducted to determine if computer technology such as smartpens can provide an effective tool in the Tribe's home-based education initiative. Previous efforts in the use of computer technology language revitalization are reviewed and found to offer promise. The smartpen technology is reviewed, the interactive booklets and programmed smartpen are described, and usability test results are analyzed. Encouraging findings have led to interest in further development of additional technological tools that could be incorporated into “home learning kits.” Additionally, by interchanging sound files, the smartpen software can be adapted easily to support language revitalization work by other communities.",Digital smartpen technology and revitalization of the Myaamia language,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,4,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
845,http://dx.doi.org/10.1145/2050096.2050098,"{'string_id': '10.1145/2050096.2050098', 'id_scheme': 'DOI'}","The aim of our project, Last House on the Hill (LHotH), is to holistically reconstitute the rich multimedia and primary research data with the impressive texts of the monograph, the printed final report of the Berkeley Archaeologists at Çatalhöyük (BACH) project, in which a team from UC Berkeley excavated a group of Neolithic 9000-year old buildings at this famous cultural heritage location in Central Anatolia, Turkey. The Last House on the Hill brings together the published text, complete project database (including all media formats such as photographs, videos, maps, line drawings), related Web sites, data and media outside the direct domain of the BACH project, and recontextualized presentations of the data as remixes, movies, and other interpretive works by BACH team members and many others. We are achieving this through an event-centered, CIDOC-CRM-compatible implementation ontology, expressed through an open-source Web publishing platform, providing open access, transparency and open-endedness to what is normally the closed and final process of monograph publication.The idea of embedding, interweaving, entangling, and otherwise linking the data and media from archaeological excavations with their interpretation and meaningful presentation in an open access sharable platform has long been an ambition of those of us working in the digital documentation of archaeological research and the public presentation of cultural heritage. Formidable barriers still exist to making it possible for projects to achieve these aims, ranging from intellectual property concerns to providing commitments to the long-term sustainability of the digital content. We believe that our event-centered implementation ontology will make it far easier for archaeologists and researchers in other disciplines to organize, manage, and share their data while gaining the significant benefits of the CIDOC-CRM framework.This article describes the strategy, goals, architecture, and implementation for the project, emphasizing the novel and innovative approaches that were required to make the project successful.",Last House on the Hill,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,4,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
846,http://dx.doi.org/10.1145/2069276.2069278,"{'string_id': '10.1145/2069276.2069278', 'id_scheme': 'DOI'}","In this article, we present the results of the photogrammetric surveys of two important monuments, the Roman Theatre in Sabratha, Libya, and San Galgano Abbey, in Italy. The surveys were performed with a new photogrammetric technique, Spherical Photogrammetry, developed by Gabriele Fangi [2007, 2008, 2009, 2010]. The method is based on so-called spherical panoramas. These are obtained by stitching together several pictures taken from the same point and covering 360°, which are then mapped in a plane with an equi-rectangular projection. This technique is normally used to produce QuickTime movies which have already proven to be very useful for the documentation of cultural heritage. One panorama can replace many normal photographic images. Ease, rapidity, low cost, and completeness of the documentation are the main advantages of this technique.The Abbey of San Galgano is an important example of Gothic architecture in Italy. The church is empty and without its roof, which fell towards the end of the 18th century. The photogrammetric survey was done in November 2009. Thirty-four panoramas were taken outside, around the perimeter of the complex, and 22 were taken inside the church. The Roman theatre of Sabratha, built in the 2nd century AD, was rebuilt in the years between 1930-1940 at the command of the Italian governor of Libya, Italo Balbo, as a rememberance of the past Roman Empire. The project was handled by two Italian archaeologists Caputo [1953] and Guidi [1935]. The photogrammetric survey was done in August 2009 with 22 panoramas taken around and inside the monument. These two projects are limited in their scope, not covering the geometry of the entire monuments. The time and the available resources did not permit a complete survey. The purpose of the two projects was not to satisfy the needs of a client, but only to demonstrate the feasibility and the efficiency of the approach. In these two projects accuracy tests were not made, but in other projects accuracy was found to be on the order of 1/1000 the object distance, say one centimeter every ten meters of distance from the object [Fangi 2007a, 2007b, 2007c, 2009, 2010].","Spherical photogrammetry for cultural heritage—San Galgano Abbey and the Roman Theater, Sabratha",Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,4,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
847,http://dx.doi.org/10.1145/2069276.2069279,"{'string_id': '10.1145/2069276.2069279', 'id_scheme': 'DOI'}","The segmentation of films and videos into shots requires the detection of gradual transitions such as dissolves and fades. There are two types of approaches: unified approaches, that is, one detector for all gradual transition types, and approaches that use specialized detectors for each gradual transition type. We present an overview on existing methods and extend an existing unified approach for the detection of gradual transitions in historic material. In an experimental study, we evaluate the proposed approach on complex and low-quality historic material as well as on contemporary material from the TRECVid evaluation. Additionally, we investigate different features, feature combinations, and fusion strategies. We observe that the historic material requires the use of texture features, in contrast to the contemporary material that, in most of the cases, requires the use of color and luminance features.",Gradual transition detection in historic film material—a systematic study,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,4,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
848,http://dx.doi.org/10.1145/2069276.2069280,"{'string_id': '10.1145/2069276.2069280', 'id_scheme': 'DOI'}","Preservation of intangible cultural heritage, such as music and dance, requires encoding of background knowledge together with digitized records of the performances. We present an ontology-based approach for designing a cultural heritage repository for that purpose. Since dance and music are recorded in multimedia format, we use Multimedia Web Ontology Language (MOWL) to encode the domain knowledge. We propose an architectural framework that includes a method to construct the ontology with a labeled set of training data and use of the ontology to automatically annotate new instances of digital heritage artifacts. The annotations enable creation of a semantic navigation environment in a cultural heritage repository. We have demonstrated the efficacy of our approach by constructing an ontology for the cultural heritage domain of Indian classical dance, and have developed a browsing application for semantic access to the heritage collection of Indian dance videos.",Nrityakosha,Association for Computing Machinery (ACM),2011,,Journal on Computing and Cultural Heritage,4,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
849,http://dx.doi.org/10.1145/2160165.2160167,"{'string_id': '10.1145/2160165.2160167', 'id_scheme': 'DOI'}","With the rise of large open digitization projects such as the Internet Archive and Google Books, we are witnessing an explosive growth in the number of source texts becoming available to researchers in historical languages. The Internet Archive alone contains over 27,014 texts catalogued as Latin, including classical prose and poetry written under the Roman Empire, ecclesiastical treatises from the Middle Ages, and dissertations from 19th-century Germany written—in Latin—on the philosophy of Hegel. At one billion words, this collection eclipses the extant corpus of Classical Latin by several orders of magnitude. In addition, the much larger collection of books in English, German, French, and other languages already scanned contains unknown numbers of translations for many Latin books, or parts of books.The sheer scale of this collection offers a broad vista of new research questions, and we focus here on both the opportunities and challenges of computing over such a large space of heterogeneous texts. The works in this massive collection do not constitute a finely curated (or much less balanced) corpus of Latin; it is, instead, simply all the Latin that can be extracted, and in its reach of twenty-one centuries (from approximately 200 BCE to 1922 CE) arguably spans the greatest historical distance of any major textual collection today. While we might hope that the size and historical reach of this collection can eventually offer insight into grand questions such as the evolution of a language over both time and space, we must contend as well with the noise inherent in a corpus that has been assembled with minimal human intervention.",Extracting two thousand years of latin from a million book library,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
850,http://dx.doi.org/10.1145/2160165.2160168,"{'string_id': '10.1145/2160165.2160168', 'id_scheme': 'DOI'}","More than a century of modern Classical scholarship has created a vast archive of journal publications that is now becoming available online. Most of this work currently receives little, if any, attention. The collection is too large to be read by any single person and mostly not of sufficient interest to warrant traditional close reading. This article presents computational methods for identifying patterns and testing hypotheses about Classics as a field. Such tools can help organize large collections, introduce younger scholars to the history of the field, and act as a “survey,” identifying anomalies that can be explored using more traditional methods.",Computational historiography,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
851,http://dx.doi.org/10.1145/2160165.2160169,"{'string_id': '10.1145/2160165.2160169', 'id_scheme': 'DOI'}","Our article explores the possibilities of using deeply annotated, incrementally evolving comparable corpora for the study of language change, in this case for different stages from Old High German to New High German. Using the example of the evolution of German past tenses, we show how a variety of categories ranging from low to high complexity interact with the choice between competing linguistic variants. To adequately explore the influence of these categories, we use a multilayer corpus architecture that develops together with our study. We show that a combination of quantitative and qualitative analyses can recognize relevant contextual factors, which feed into the addition of new annotation layers applying to the same data. By making our categorizations explicit as corpus annotations and our data available to other researchers, we promote an open, extensible, and transparent mode of research, where both raw data and the inferential process are exposed to other researchers.",Measuring and coding language change,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
852,http://dx.doi.org/10.1145/2307723.2307725,"{'string_id': '10.1145/2307723.2307725', 'id_scheme': 'DOI'}","We propose a new method of annotating a masterpiece of traditional Chinese painting with voice dubbings and environmental sounds. The painting was created with moving focus drawing technique without rigorous perspective. A novel algorithm is proposed to infer the 3D space of the painting according to its layout and embed the audio annotations. For exhibition, the masterpiece is scanned into a high-resolution gigapixel image for presenting the drawing details, and we develop an interactive multimedia system with a panning and zooming interface to enable smooth navigation on the giant painting and exploring the historical culture. During the navigation, the system estimates the 3D position of the user's viewpoint from his/her actions, and subsequently synthesizes a realistic stereo audio field according to the viewer's orientation and distance from the annotations in the 3D space. The proposed system provides an immersive user experience by rendering a visual-audio consistent perception.",Annotating traditional Chinese paintings for immersive virtual exhibition,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
853,http://dx.doi.org/10.1145/2307723.2307726,"{'string_id': '10.1145/2307723.2307726', 'id_scheme': 'DOI'}","Analysis of visual art is a highly complex cognitive task that depends on the very many aspects of the art as well as complex brain connectivity, and the examination of visual art and the analysis of influential links between artists and artistic movements require the trained eye of knowledgeable art historians. However, while the human eye and brain can perceive visual art and notice the differences, similarities, and influential links between painters, computers employing artificial intelligence find this task far more challenging. In this article we show that computers can automatically analyze paintings of different artists and different schools of art in an unsupervised fashion. Experimental results show that the automatic computer analysis can group artists by their artistic movements, and provide a map of similarities and influential links that is largely in agreement with the analysis of art historians. These results demonstrate that machine vision and pattern recognition algorithms are able to mimic the complex cognitive task of the human perception of visual art, and can be used to measure and quantify visual similarities between paintings, painters, and schools of art.",Computer analysis of art,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
854,http://dx.doi.org/10.1145/2307723.2307727,"{'string_id': '10.1145/2307723.2307727', 'id_scheme': 'DOI'}","In this article we evaluate three modeling techniques coming from information system engineering to represent cultural heritage domain concepts. Evaluations of the modeling techniques were performed by carrying out experiments with cultural heritage specialists. The first experiment consisted in evaluating UML class diagrams and a conceptual graph. The second experiment evaluated a value cluster modeling approach. These qualitative evaluations were conducted using focus groups composed of cultural heritage specialists. We evaluated the perceived ease of use, the perceived usefulness, and the intention to use of the three techniques (UML class diagrams, conceptual graphs, and the value cluster approach). Thanks to these evaluations, we will be able to propose a combined approach using value cluster modeling and UML class diagraming techniques to describe cultural heritage information.",Qualitative evaluation of cultural heritage information modeling techniques,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
855,http://dx.doi.org/10.1145/2362402.2362403,"{'string_id': '10.1145/2362402.2362403', 'id_scheme': 'DOI'}","This article describes recent field work undertaken by Microsoft Research Asia and the Dunhuang Academy to capture high-resolution images of Buddhist art at the UNESCO world heritage site, the Mogao Caves. This project is intended as a feasibility study examining the use of a digital large-format gigapixel camera to capture high-resolution images in a cultural heritage setting. In particular, we report on the current challenges faced by the Dunhuang Academy in their imaging efforts and how the use of a digital large-format camera can improve the quality of the imaging process while reducing time and effort. We also describe lessons learned from this field study as well as remaining challenges inherent to such projects.",Imaging buddhist art with a digital large-format camera,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
856,http://dx.doi.org/10.1145/2362402.2362405,"{'string_id': '10.1145/2362402.2362405', 'id_scheme': 'DOI'}","Museums have recognized the need for supporting visitors in fulfilling a personalized experience when visiting artwork collections, and they have started to adopt recommender systems as a way to meet this requirement. Content-based recommender systems analyze features of artworks previously rated by a visitor and build a visitor model or profile, in which preferences and interests are stored, based on those features. For example, the profile of a visitor might store the names of his or her favorite painters or painting techniques, extracted from short textual descriptions associated with artworks. The user profile is then matched against the attributes of new items in order to provide personalized suggestions. The Web 2.0 (r)evolution has changed the game for personalization from “elitist” Web 1.0, written by few and read by many, to Web content potentially generated by everyone (user-generated content - UGC). One of the forms of UGC that has drawn most attention from the research community is folksonomy, a taxonomy generated by users who collaboratively annotate and categorize resources of interests with freely chosen keywords called tags.In this work, we investigate the problem of deciding whether folksonomies might be a valuable source of information about user interests in the context of recommending digital artworks. We present FIRSt (Folksonomy-based Item Recommender syStem), a content-based recommender system which integrates UGC through social tagging in a classic content-based model, letting users express their preferences for items by entering a numerical rating as well as by annotating items with free tags. Experiments show that the accuracy of recommendations increases when tags are exploited in the recommendation process to enrich user profiles, provided that tags are not used as a surrogate for the item descriptions, but in conjunction with them. FIRSt has been developed within the CHAT project “Cultural Heritage fruition & e-learning applications of new Advanced (multimodal) Technologies”, and it is the core of a bouquet of Web services designed for personalized museum tours.",A folksonomy-based recommender system for personalized access to digital artworks,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
857,http://dx.doi.org/10.1145/2362402.2362406,"{'string_id': '10.1145/2362402.2362406', 'id_scheme': 'DOI'}","During the construction of an underground museum in the historic city center of Cologne, Germany, large parts of the Roman and medieval city are being excavated. The newly excavated remains as well as remains of the Roman city, which had already been excavated in 1954, exhibit structural damages. While at first deficiencies in the construction were assumed to be the cause of the damages, in 2003 a seismogenic origin was suggested. To further test this hypothesis of seismically induced slope movements and other possible causes, a multidisciplinary project was started. One step in this project is the documentation of the damages using a 3D laser scanner, followed by a quantitative damage analysis. This article presents the 3D documentation and the quantitative damage analysis of a recently excavated medieval cesspit. The 8.3m-deep cesspit was mapped during 11 campaigns using a phase-based 3D laser scanner. Due to the static conditions of the cesspit, the structure could not be excavated in its entirety. After the excavation of every 1-2m-section, restoration work had to be done to avoid a collapse of the construction. The laser scanning technique offered the possibility of working parallel to the excavation so the original conditions of each section could be documented before the restoration. The resulting models were used to identify, classify, and quantify the structural damages of the cesspit.","Excavation-parallel laser scanning of a medieval cesspit in the archaeological zone cologne, germany",Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
858,http://dx.doi.org/10.1145/2399180.2399181,"{'string_id': '10.1145/2399180.2399181', 'id_scheme': 'DOI'}","Laser scanning has been widely adopted for the digital preservation and study of cultural heritage artifacts. However, surface scans alone are not sufficient for a variety of intricate objects such as the Cantonese chess pieces that were delicately carved from ivory throughout the 19th Century. These pieces incorporate a puzzle ball base, which is comprised of several balls, one inside the other. In this project we explore the use of x-ray micro-CT technology to scan the piece with a voxel separation of less than 9 μm, By using masking and editing software we generate 3D surface models of the separate parts to enable close inspection of both exterior and interior sections. Visualizing the dataset at a much larger scale enables the viewer to appreciate the fine surface details imperceivable in the original piece. Finally, 3D scaled versions are printed where the small details are also easily observed and the inner balls within the puzzle ball base move freely. The work flow demonstrated in this article has important possibilities in digital preservation, documentation, and presentation.",Combining X-ray micro-CT technology and 3D printing for the digital preservation and study of a 19th century cantonese chess piece with intricate internal structure,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
859,http://dx.doi.org/10.1145/2399180.2399182,"{'string_id': '10.1145/2399180.2399182', 'id_scheme': 'DOI'}","Digital heritage archives often lack engaging user interfaces that strike a balance between providing narrative context and affording user interaction and exploration. It seems nevertheless feasible for metadata tagging and a “joined up” workflow to provide a basis for such rich interaction. After outlining relevant research from within and outside the heritage domain, we present our project, FINE (Fluid Interfaces for Narrative Exploration), an effort to develop such a system. Based on content from Wendy James' archive of anthropological research material from the Sudan/Ethiopian borderlands, the FINE project attempts to use structural and thematic metadata to drive exploratory interfaces which link video, images, audio, and text to relevant narrative units. The interfaces also benefit from the temporal and spatial variety of the collection to provide opportunities to discover contrasts and juxtaposition in the material across place and time.",Interactive multimedia ethnography,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
860,http://dx.doi.org/10.1145/2399180.2399183,"{'string_id': '10.1145/2399180.2399183', 'id_scheme': 'DOI'}","This article presents a novel technique for automatic archaeological sherd classification. Sherds that are found in the field usually have little to no visible textual information such as symbols, graphs, or marks on them. This makes manual classification an extremely difficult and time-consuming task for conservators and archaeologists. For a bunch of sherds found in the field, an expert identifies different classes and indicates at least one representative sherd for each class (training sample). The proposed technique uses the representative sherds in order to correctly classify the remaining sherds. For each sherd, local features based on color and texture information are extracted and are then transformed into a global vector that describes the whole sherd image, using a new bag of words technique. Finally, a feature selection algorithm is applied that locates features with high discriminative power. Extensive experiments were performed in order to verify the effectiveness of the proposed technique and show very promising results.",Automatic classification of archaeological pottery sherds,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
861,http://dx.doi.org/10.1145/2399180.2399184,"{'string_id': '10.1145/2399180.2399184', 'id_scheme': 'DOI'}","Large amounts of cultural heritage content have now been digitized and are available in digital libraries. However, these are often unstructured and difficult to navigate. Automatic techniques for identifying similar items in these collections could be used to improve navigation since it would allow items that are implicitly connected to be linked together and allow sets of similar items to be clustered. Europeana is a large digital library containing more than 20 million digital objects from a set of cultural heritage providers throughout Europe. The diverse nature of this collection means that the items do not have standard metadata to assist navigation.A range of methods for computing the similarity between pairs of texts are applied to metadata records in Europeana in order to estimate the similarity between items. Various methods for computing similarity have been proposed and can be classified into two main approaches: (1) knowledge-based, which make use of external knowledge sources and (2) corpus-based approaches, which rely on analyzing the frequency distributions of words in documents. Both techniques are evaluated against manual judgements obtained for this study and a multiple-choice test created from manually generated categories in cultural heritage collections. We find that a combination of corpus and knowledge-based approaches provide the best results in both experiments.",Computing similarity between items in a digital library of cultural heritage,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
862,http://dx.doi.org/10.1145/2399180.2399185,"{'string_id': '10.1145/2399180.2399185', 'id_scheme': 'DOI'}","Serious games present a promising opportunity for learning, but the genre still lacks methodologies and tools for efficient and low-cost production, particularly for teacher and domain experts. This article gives an authoring framework that aims to provide structured support, from content design to final implementation. In particular, we have abstracted a conceptual model—the SandBox Serious Game - which relies on a generalization of task-based learning theory. The model invites players to perform cognitive tasks contextually while exploring information-rich virtual environments. We consider it particularly suited for cultural heritage entertainment applications. The model defines games that are set in realistic virtual worlds enriched with embedded educational tasks, which we have implemented as minigames. This approach simplifies the authoring work, which can easily be supported by visual authoring tools for ontology-based urban 3D modeling and implementation tasks, thus allowing an approach similar to the mind-maps concept. We propose a top-down methodology for content preparation, starting from a city-level analysis down to the single points of interest and associated tasks, which are instances of simple predefined minigame/quiz typologies. We provide examples and discuss criteria for selecting task typologies according to the authors' cognitive targets. Finally, we discuss the results of a user test, which took place in a lab, aimed at verifying the acquisition of cultural heritage knowledge in a pleasant and engaging way. Games appear particularly suited for supporting the study of images, especially of iconography. Compared to reading text, a game forces the player to focus more strongly on problems, which favors knowledge acquisition and retention. Learning complex concepts requires an investigative attitude, which can be spurred by well-designed games. Good design involves usability, graphic appeal, appropriate content, and the presence of connections which a player must discover in the content. Players should be asked to pay attention to and reason about their whole game activity - including the relationships between the game content, the brief introduction, and concluding texts. More comprehensive tests are needed to better investigate the educational effectiveness—however, the first results are promising, especially in terms of user motivation and creation of new opportunities for learning about CH.",A serious game model for cultural heritage,Association for Computing Machinery (ACM),2012,,Journal on Computing and Cultural Heritage,5,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
863,http://dx.doi.org/10.1145/2442080.2442082,"{'string_id': '10.1145/2442080.2442082', 'id_scheme': 'DOI'}","DirectorNotation (DN) is a symbolic language intended to express the content of film (motion pictures), much as notes provide a language for the writing of music. It thus constitutes a new approach to the creative process of filmmaking. Musicians, and also choreographers, have long been able to express their creative choices using logical symbolic structures (music notation and dance notation), yet those working in the movie industry have to rely on cartoons and verbal description. Development of a successful notation becomes appropriate today because of its dependence upon the parallel development of effective notation-based software tools such as visualisation (automatic animated storyboard generation), production budget estimation, and automated rough editing of dailies. Directors maintain complete control of their creative decisions when using DN. It is an artistic language supported by technical tools for planning and analysis. The notation is not merely a graphical user interface for these tools, and the tools are never intended to make decisions for the director. This article introduces DN, argues its great cultural significance, provides market research results showing directors' interest in using it, and reports on two controlled experiments confirming its effectiveness—including its usability in a practical context and its ability to represent and communicate the necessary information.",DirectorNotation,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
864,http://dx.doi.org/10.1145/2442080.2442083,"{'string_id': '10.1145/2442080.2442083', 'id_scheme': 'DOI'}","This article addresses the problem of automatic reconstruction of a 3D relief object from a line drawing. Our main application is reconstruction of archaeological artifacts based on line drawings. The problem is challenging due to five reasons: the small number of orthogonal views of the object, the sparsity of the strokes, their ambiguity, their large number, and their interrelations. We partition the reconstruction problem into two subproblems. First, we reconstruct the underlying smooth base of the object from the silhouette. Assuming that the variation of bases belonging to the same class of objects is relatively small, we create the base by modifying a similar base retrieved from a database. Second, we reconstruct the relief on top of the base. Our approach can reconstruct the relief from a complex drawing that consists of many interrelated strokes. Rather than viewing the interdependencies as a problem, we show how they can be exploited to automatically generate a good initial interpretation of the line drawing. Even though our algorithm is generic, its strength is demonstrated by the reconstruction of artifacts from manual drawings taken from real archaeological reports. These drawings are highly challenging, since artists created very complex and detailed descriptions of artifacts regardless of any considerations concerning their future use for shape reconstruction.",Reconstruction of relief objects from archeological line drawings,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
865,http://dx.doi.org/10.1145/2442080.2442085,"{'string_id': '10.1145/2442080.2442085', 'id_scheme': 'DOI'}","Computer vision techniques have been applied for rapid and accurate structure recovery in many fields. Most methods perform poorly in areas containing little or no texture and in the presence of repetitive patterns. We present a portable, cost-effective pattern projector system powered by the flash of a camera, to aid the reconstruction of such areas. No calibration is required between the camera-projector, projector-scene, or pattern. We demonstrate the effectiveness of our system on various representative surfaces like stone, metal, clay, porcelain, and natural fibers, with different inherent colors/textures. A pipeline is presented to automatically generate textured, true-scale metric models, that can be used for quantitative studies or visualization. The practicability of our system is explored in the specific area of digital archiving of historically significant objects. We show results from field trips to 12th century temples at Belur and Halebidu in South India and objects from the Wintherthur museum, Delaware, USA.",A camera flash projector-based reconstruction system for digital preservation of artifacts,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
866,http://dx.doi.org/10.1145/2460376.2460378,"{'string_id': '10.1145/2460376.2460378', 'id_scheme': 'DOI'}","Games have become an integral part of today's culture, most obviously among younger people. At the same time, learning games have proved that they can be a source of enjoyment and are, if well-made, powerful tools for communicating knowledge. For cultural heritage projects targeted to raising the awareness of the general public, the integration of interactivity and innovative storytelling techniques can be supporting elements to capture their target audiences' enthusiasm. In particular, games that deal with art history have particular flaws in integrating motivating elements. This observation led us to the development of the 3D online learning game, ThIATRO, that immerses the player an exhibition and, as a side-effect communicates knowledge of art history concepts to the player. Its playful approach not only increases motivation to learn but also raises interest in art history and cultural heritage in general. We provide an overview of the design ideas behind ThIATRO and summarize the results of evaluations conducted with a group of 14-year-old pupils in a classroom environment. The results indicate that ThIATRO changes the player's aesthetic response and allows him or her to perceive art on a deeper level.",Art history concepts at play with ThIATRO,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
867,http://dx.doi.org/10.1145/2460376.2460379,"{'string_id': '10.1145/2460376.2460379', 'id_scheme': 'DOI'}","This article describes a case study on MuseUs, a pervasive serious game for use in museums, running as a smartphone app. During the museum visit, players are invited to create their own exposition and are guided by the application in doing so. The aim is to provide a learning effect during a visit to a museum exhibition. Central to the MuseUs experience is that it does not necessitate a predefined path trough the museum and that it does not draw the attention away from the exposition itself. Also, the application stimulates the visitor to look at cultural heritage elements in a different way, permitting the construction of personal narratives while creating a personal exposition. Using a methodology derived from action research, we present recommendations for the design of similar applications and conclude by proposing a high-level architecture for pervasive serious games applied to cultural heritage.",MuseUs,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
868,http://dx.doi.org/10.1145/2460376.2460380,"{'string_id': '10.1145/2460376.2460380', 'id_scheme': 'DOI'}","This study utilizes the characteristics of the Annales School, including their emphasis on total history and space and time integration, their focus on the economic world, and their use of structural analysis to develop a creation framework for a serious video game related to the cultural and life history of Taiwan's indigenous people. Game development comprised the tiers of data, logic, and presentation. During the data tier, we used taxonomy to extract the cultural components of Taiwan's Atayal tribe. During the logic tier, we employed an analysis of cultural characteristics and comparisons of historical education goals to confirm the game's framework and storytelling engine, selecting a construction management simulation game genre to present tribal life and economic operations. Finally, in the presentation tier, we converted cultural components into elements in the game's user interface.The game evaluation results showed that “Papakwaqa” (our serious game) had positive benefits for enhancing schoolchildren's learning motivation and performance regarding indigenous life and history. Our research verified that integrating the cross-disciplinary methods of anthropologists, history teachers, tribal elders, and game designers to establish instructional goals and design foci for game development and to create a cultural heritage serious game-creation framework warrants further study and effort.",An annales school-based serious game creation framework for taiwanese indigenous cultural heritage,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
869,http://dx.doi.org/10.1145/2460376.2460381,"{'string_id': '10.1145/2460376.2460381', 'id_scheme': 'DOI'}","This article describes an architecture for a dialogue management system to be employed in serious games for natural language interaction with nonplayer characters. The aim of this work is to improve the learning experience by enhancing the immersiveness felt by the player. The system is implemented in a Service Oriented Architecture perspective and it exposes its functionalities through Web services. It also provides an easy to use authoring tool, which allows cultural heritage experts to define the character's knowledge without the need for learning a scripting language. We tested the system by embedding it in a simple serious game, where the user could interact with the virtual representation of a XVI century Geneoese artist, Luca Cambiaso, in order to acquire knowledge about his life and artworks. These early lab test results showed a high level of appreciation for the usability of the system and for the user engagement. Tests also showed that the system is a good didactic tool, since players demonstrated a similar level of knowledge acquisition to that achieved by reading a text.",An easy to author dialogue management system for serious games,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
870,http://dx.doi.org/10.1145/2499931.2499932,"{'string_id': '10.1145/2499931.2499932', 'id_scheme': 'DOI'}","Museums and Cultural Heritage institutions have a growing interest in presenting their collections to a broader community via the Internet. The photo-realistic presentation of interactively inspectable virtual surrogates is one of the most challenging problems in this field. For this purpose, we seek to employ not only a 3D geometry but also a powerful material representation capable of reproducing the full visual appeal of an object. In this article, we propose a WebGL-based presentation framework in which reflectance information is represented via Bidirectional Texture Functions (BTF). Our approach works out-of-the-box in modern Web browsers and allows for the progressive transmission and interactive rendering of digitized artifacts consisting of 3D geometry and reflectance information. We handle the huge amount of data needed for this representation by employing a novel progressive streaming approach for BTFs, which allows for the smooth interactive inspection of a steadily improving version during the download. We demonstrate an interesting use-case of this technique at a cross section of Cultural Heritage, medical education, and research and provide an evaluation of the capabilities of our framework in the scope of BTF compression and transmission.",WebGL-based streaming and presentation of objects with bidirectional texture functions,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
871,http://dx.doi.org/10.1145/2499931.2499933,"{'string_id': '10.1145/2499931.2499933', 'id_scheme': 'DOI'}","Digital gaming and digital technologies have their own unique cultural history while at the same time, the cultural heritage of digital technology is emerging. Digital technology has been understood as merely an apparatus that can be utilized for transferring nondigital historical content to novel digital products. These products, including types such as multimedia shows, games, Web sites, and online course environments, are targeted at juvenile audiences, who are typically considered to be the primary users of such new media forms. For decades, the changes and new continuities in both mediated content and the technology of mediation were mostly hidden in the shadow of educational goal-attainment. This article draws inspiration from ideas on media archaeology and the cultures of history. In this article we suggest an approach of internal and external cultural heritage of games cultures. We introduce a four-fold table regarding the relationship between cultural heritage (or history) and digital technology. The four-fold table consists of the dimension of a researcher's comprehensive/applied goal-attainment and the dimension of the internality/externality of history and cultural heritage in regard to the digital game cultural context. Within these cultures, there are several alternative ways of discussing the relationship between history, cultural heritage, and digital technology, separate from the traditional edutainment perspective. The dimensions are illustrated with practical examples, including a typologization of historiographical computer games, retrogaming, and educational workshops on game classics.",Gaming legacy? four approaches to the relation between cultural heritage and digital technology,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
872,http://dx.doi.org/10.1145/2499931.2499934,"{'string_id': '10.1145/2499931.2499934', 'id_scheme': 'DOI'}","This article describes an interactive platform for the semantic annotation of oriented iconographic sources based on an accurate 3D-model structured according to spatial and temporal features. This platform, called LOCUS IMAGINIS, provides access to cultural information about monuments by collecting personal snapshots taken by visitors. In particular, the platform enables to perform two actions. First, pictures are added into the database: visitors manually align images to the digital model of the monument and than, by means of automatic procedures, images are enriched with 2D semantic annotations that are projected from the 3D model. Second, the collection of semantically-enriched data can be queried: when visitors select 2D images of the database, these are oriented in the 3D scene and superimposed onto the 3D-model. In this way, this tool provides access to textual and graphic information describing the photographed site (semantic, spatial, temporal features, etc.). Moreover, visitors can interact both with images and the 3D-model to highlight building parts by color. Accessible onsite and online, this tool can create awareness about conservation and is adapted to all kinds of audience: Students involved in educational workshops, visitors needing to extend their visit, and documentalists interested in the consultation of the iconographic corpus.",A web platform for the consultation of spatialized and semantically enriched iconographic sources on cultural heritage buildings,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
873,http://dx.doi.org/10.1145/2499931.2499935,"{'string_id': '10.1145/2499931.2499935', 'id_scheme': 'DOI'}","Laser range scanning is commonly used in cultural heritage to create digital models of real-world artefacts. A large scanning campaign can produce billions of point samples—too many to be manipulated in memory on most computers. It is thus necessary to spatially partition the data so that it can be processed in bins or slices. We introduce a novel compression mechanism that exploits spatial coherence in the data to allow the bins to be computed with only 1.01 bytes of I/O traffic for each byte of input, compared to 2 or more for previous schemes. Additionally, the bins are loaded from the original files for processing rather than from a sorted copy, thus minimizing disk space requirements. We demonstrate that our method yields performance improvements in a typical point-processing task, while also using little memory and guaranteeing an upper bound on the number of samples held in-core.",Fast in-place binning of laser range-scanned point sets,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
874,http://dx.doi.org/10.1145/2532630.2512883,"{'string_id': '10.1145/2532630.2512883', 'id_scheme': 'DOI'}","This article presents the process of reconstructing three facade designs for the east wing of the Louvre using procedural modeling. The first proposal reconstructed is Louis Le Vau's 1662 scheme, the second is Gian Lorenzo Bernini's first design, and the third is the 1668 petit conseil design that still stands today. The results show how such reconstructions may aid both a general and an expert understanding of the three designs. It is proposed that by formalizing the facade description into a shape grammar with procedural modeling, a systematized approach to a stylistic analysis is possible. It is also asserted that such an analysis is still best understood within the historical context of what is known about the contemporary design intentions of the building creators and commissioners.",Using procedural modeling to explore alternative designs for the louvre,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
875,http://dx.doi.org/10.1145/2532630.2532631,"{'string_id': '10.1145/2532630.2532631', 'id_scheme': 'DOI'}","The information about cultural heritage artifacts that archeologists must manage is usually very heterogenous, and, due to its spatial nature, cannot be easily represented using conventional data management frameworks. The strong spatial dependence of this data suggests that the information should be linked to a 3D model of the artifact. This article presents a 3D information system that has been designed to manage cultural heritage information. The system allows information layers to be associated with the surface of the artifact, following an approach similar to that used in geographical information systems. This permits relationships between the different elements to be ascertained, and allows both specialists and the layperson to more easily understand the information. We describe here the structure and functionality of the system.",Design of cultural heritage information systems based on information layers,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
876,http://dx.doi.org/10.1145/2532630.2532632,"{'string_id': '10.1145/2532630.2532632', 'id_scheme': 'DOI'}","Experimental archaeology has long yielded valuable insights into the tools and techniques that were featured in past peoples’ relationships with the material world around them. However, experimental archaeology has, until now, confined itself to rigid, empirical, and quantitative questions. This article applies principles of experimental archaeology and serious gaming tools in the reconstructions of a British Iron Age round house. This article explains a number of experiments conducted to look for quantitative differences in movement in virtual versus material environments, using both “virtual” studio reconstruction as well as material reconstruction. The data from these experiments was then analysed to look for differences in movement that could be attributed to artefacts and/or environments. This article also explains the structure of the experiments, how the data was generated, what theories may make sense of the data, what conclusions have been drawn, and how serious gaming tools can support the creation of new experimental heritage environments.",Experimental archaeology and games,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
877,http://dx.doi.org/10.1145/2532630.2532633,"{'string_id': '10.1145/2532630.2532633', 'id_scheme': 'DOI'}","The unique aspects of different places seem to require games of different characteristics. This paper describes the initial steps of an attempt to design games for different places of cultural heritage and consists of three parts. In the first part, the descriptive model to be used as a classification method for games of different characteristics for cultural heritage sites is presented. In the second part, the model is used for the creation of different cultural heritage games. Finally, the third part presents a detailed case study of one of the developed games, showing the implementation and user testing processes as well as its efficiency in terms of education and visit motivation.",An approach for serious game development for cultural heritage,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
878,http://dx.doi.org/10.1145/2532630.2532634,"{'string_id': '10.1145/2532630.2532634', 'id_scheme': 'DOI'}","Understanding the similarities, differences, and relationships between cultural heritage artifacts is critical for determining their significance and their provenance. It also provides valuable information for ensuring the long-term preservation of cultural heritage artifacts. Consequently, as more museums develop online three-dimensional (3D) collections, curators and scholars are demanding online tools that enable them to document and interpret variances and similarities between related 3D digital objects. This article describes a system that was developed to enable museum curators and/or scholars to document relationships between multiple 3D digital representations of museum objects using web-based annotation tools. The 3D Semantic Association (3DSA) system enables users to annotate relationships between multiple whole objects, parts of objects, or features on objects (surface features or volumetric segments). The annotations are stored on a server in an interoperable format that can be shared, discovered, browsed, and retrieved through a web browser interface. This approach not only improves scholars’ capabilities to undertake cultural heritage research but also enables researchers to document, share, discuss, and compare alternative hypotheses about the relationships between artifacts.",Documenting and sharing comparative analyses of 3D digital museum artifacts through semantic web annotations,Association for Computing Machinery (ACM),2013,,Journal on Computing and Cultural Heritage,6,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
879,http://dx.doi.org/10.1145/2535937,"{'string_id': '10.1145/2535937', 'id_scheme': 'DOI'}","We propose an application that allows a museum audience to interact with the past and to appreciate the value of antique objects through multimedia installations. Discussions in this article are based on our experiences in developing the Mao-Kung Cauldron time perception journey multimedia application. This installation was inspired by the Mao-Kung Cauldron, an ancient bronze cauldron in the collection of the National Palace Museum known for its historical significance. The question, however, lies in how to create an experience in which the audiences can interact with the past. To address this issue, the research team used the Kinect-based breath detection and deterioration/recovery simulation technologies to develop the application. This installation is the first application linked to Kinect-based breath detection and deterioration/recovery simulation technologies. The main contribution of this work is the analysis of design concepts, design decisions, and evaluations in a museum setting. We conduct a control group study to compare the outcomes between the experimental group (Mao-Kung Cauldron time perception journey multimedia application) and control group (asynchronous web-based kiosk). Both the quantitative (questionnaire survey) and qualitative methods (observation) are used to analyze the collected data. This work proposes a feasibility design to let museum audiences experience the features of cultural object by the virtual time perception journey application.",Interacting with the past,Association for Computing Machinery (ACM),2014,,Journal on Computing and Cultural Heritage,7,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
880,http://dx.doi.org/10.1145/2558306,"{'string_id': '10.1145/2558306', 'id_scheme': 'DOI'}","In this article, a method of merging point clouds using the modified Harris corner detection algorithm for extracting interest points of textured 3D point clouds is proposed. A new descriptor characterizing point features for identifying corresponding points in datasets is presented. The merging process is based on the Random Sample Consensus (RANSAC) algorithm, which enables calculation of the geometric transformation between point clouds based on a set of interest points that includes incorrect samples, called outliers. The proposed processing path is designed to integrate many directional measurements, which are acquired with a 3D scanner and are represented as unsorted point clouds (x, y, z) with color information (R, G, B). Exemplary measurements shown in this article represent sections of ceiling in the King's Chinese Cabinet of the Museum of King Jan III's Palace at Wilanow in Warsaw, Poland, as well as some more complex objects. Experimental verification confirms the effectiveness of the proposed method in integrating directional measurements of objects with detailed texture, particularly if they have no unique geometric features.",Color-Based Algorithm for Automatic Merging of Multiview 3D Point Clouds,Association for Computing Machinery (ACM),2014,,Journal on Computing and Cultural Heritage,7,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
881,http://dx.doi.org/10.1145/2567652,"{'string_id': '10.1145/2567652', 'id_scheme': 'DOI'}","Art conservators now have access to a wide variety of digital imaging techniques to assist in examining and documenting physical works of art. Commonly used techniques include hyperspectral imaging, 3D scanning, and medical computed tomography imaging. However, viewing most of this digital image data frequently requires both specialized software, which is often associated with a particular type of acquisition device, and professional knowledge of and experience with each type of data. In addition, many of these software packages are focused on particular applications (such as medicine or remote sensing) and do not permit users to access and fully exploit all the information contained in the data. In this paper, we address two practical barriers to using high-tech digital data in art conservation. First, users must deal with a wide variety of interfaces specialized for applications besides conservation. We provide an open-source software tool with a single intuitive interface consistent with conservators' needs that handles various types of 2D and 3D image data and preserves user-generated metadata and annotations. Second, previous software has largely allowed visualizing a single type or only a few types of data. The software we present is designed and structured to accommodate multiple types of digital imaging data, including as yet unspecified or unimplemented formats, in an integrated environment. This allows conservators to access different forms of information and to view a variety of image types simultaneously.",Hyper3D,Association for Computing Machinery (ACM),2014,,Journal on Computing and Cultural Heritage,7,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
882,http://dx.doi.org/10.1145/2583114,"{'string_id': '10.1145/2583114', 'id_scheme': 'DOI'}","This article introduces the study contents and some research findings regarding digital preservation methods for Chinese Kunqu opera libretto historical literature, including historical literature electronic libretto transformation, libretto musical score image segmentation, musical information recognition, musical score information representation, musical score information storage, and libretto reconstruction on the Web. It proposes a novel editable text method to represent the multidimensional tree-like information structure of the Kunqu libretto literature and a musical semantic annotation method based on numbered musical notation to accommodate the musical features of Kunqu librettos. To maintain the characteristics of the original Kunqu musical notation, it proposes a method to reconstruct Kunqu libretto on the Web based on scalable vector graphics. Some Kunqu librettos were randomly selected for experiments, and the results demonstrated that the editable text method and the musical semantic annotation method were able to fully represent the effective information of the Kunqu libretto literature and that the method to reconstruct librettos on the Web was able to reflect the writing characteristics of the musical notation in the original librettos. Finally, it discusses the primary future research directions related to digital Kunqu, including Kunqu libretto metadata research, corpus construction for the librettos and Qupai (the unique ancient Chinese tune mode), libretto music information disambiguation research, libretto image segmentation and pattern recognition, digital Kunqu roles, digital Kunqu stages, digital Kunqu costume suitcases, virtual Kunqu, digitization and restoration of Kunqu cultural relics, and Kunqu's application prospects in conventional media such as animation, anime, and movies.",Intangible cultural heritage preservation,Association for Computing Machinery (ACM),2014,,Journal on Computing and Cultural Heritage,7,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
883,http://dx.doi.org/10.1145/2583115,"{'string_id': '10.1145/2583115', 'id_scheme': 'DOI'}","Coin classification is one of the main aspects of numismatics. The introduction of an automated image-based coin classification system could assist numismatists in their everyday work and allow hobby numismatists to gain additional information on their coin collection by uploading images to a respective Web site. For Roman Republican coins, the inscription is one of the most significant features, and its recognition is an essential part in the successful research of an image-based coin recognition system. This article presents a novel way for the recognition of ancient Roman Republican coin legends. Traditional optical character recognition (OCR) strategies were designed for printed or handwritten texts and rely on binarization in the course of their recognition process. Since coin legends are simply embossed onto a piece of metal, they are of the same color as the background and binarization becomes error prone and prohibits the use of standard OCR. Therefore, the proposed method is based on state-of-the-art scene text recognition methods that are rooted in object recognition. Sift descriptors are computed for a dense grid of keypoints and are tested using support vector machines trained for each letter of the respective alphabet. Each descriptor receives a score for every letter, and the use of pictorial structures allows one to detect the optimal configuration for the lexicon words within an image; the word causing the lowest costs is recognized. Character and word recognition capabilities of the proposed method are evaluated individually; character recognition is benchmarked on three and word recognition on different datasets. Depending on the Sift configuration, lexicon, and dataset used, the word recognition rates range from 29% to 67%.",Reading the legends of Roman Republican coins,Association for Computing Machinery (ACM),2014,,Journal on Computing and Cultural Heritage,7,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
884,http://dx.doi.org/10.1145/2594443,"{'string_id': '10.1145/2594443', 'id_scheme': 'DOI'}","This article is devoted to a novel application of the micro-3D modeling based on shape from focus. A 3D portable digital microscope prototype has been used for the first time in order to analyze gold punchwork on medieval panel paintings. In general, the 3D domain provides a more flexible and complete characterization of these decorative elements than traditional photographic documentation. Low-magnification 3D digital microscopy is well suited for analyzing morphologies, depths, and profiles of different punch marks. Here, we used these parameters for interpreting the punching process and recognizing sliding and bouncing effects. The 3D reconstruction of the surface engraved also allowed identifying types of anomalies that could be useful as possible authentication markers. The advantages of the present approach with respect to the photographic documentation are of general valence and can be exploited in order to document, parameterize, and interpret a variety of engraving works on panel paintings and metal artifacts.",3D Digital Microscopy for Characterizing Punchworks on Medieval Panel Paintings,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,7,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
885,http://dx.doi.org/10.1145/2597178,"{'string_id': '10.1145/2597178', 'id_scheme': 'DOI'}","This article addresses the problem of automatic reconstruction of ancient artifacts from archaeological fragments. The technique described here focuses on pairwise matching of flat fragments (typically fresco fragments), and it is intended to be the core of a larger system for artifact reconstruction. Global registration techniques are challenging due to the combinatory explosion that happens in the solution space: the goal is to find the best alignment among all possible ones without an initialization. This fact defines the duality between performance and correction that we face in this work. The proposed technique defines a cost function to evaluate the quality of an alignment based on a discrete sampling of the fragments that ensures data alignment. Starting from an exhaustive search strategy, the technique progressively incorporates new features that lead to a hierarchical search strategy. Convergence and correction of the resulting technique are ensured using an optimistic cost function. Internal search calculations are optimized so the only operations performed are additions, subtractions, and comparisons over aligned data. All heavy geometric operations are carried out by the GPU on a preprocessing stage that only happens once per fragment.",A Discrete Approach for Pairwise Matching of Archaeological Fragments,Association for Computing Machinery (ACM),2014,,Journal on Computing and Cultural Heritage,7,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
886,http://dx.doi.org/10.1145/2597179,"{'string_id': '10.1145/2597179', 'id_scheme': 'DOI'}","We describe a system that automatically notates a comparative visualization of multiple recorded performances of the same musical work. Written musical scores have transmitted basic performance information to musicians over the ages; however, these scores only provide skeletal instructions that must be fleshed out in performance, as musical notation describes phrasing, articulation, dynamics, accentuation, and other ornamentations in generalized and ambiguous forms. Consequently, musical performances derived from the same notation can vary widely from each other in the same manner that a written text may be spoken with intense emotion or in flat monotone. Prior to the advent of recording technology, musical performances were ephemeral, only occurring once, never to be heard again in exactly the same rendition. As a result, musical interpretations were informed only by live listening. Now, with more than a century of recorded performance practice, musicians can delve deeper into the history of their aural art to gain inspiration and insight from sources that would otherwise have been inaccessible. Performers have become interested in giving performances inspired by recordings of the past, which often obey a musical common sense alien to the standards of modern practice, and it is useful for historically informed performers to describe, analyze, emulate, and internalize the performance styles of the past through the detailed study of recordings. Although much can be learned by listening, a visual interface may reveal potentially inaudible details of a recording. Because performers interact daily with traditional musical notation—a sophisticated, if ambiguous, multidimensional visualization of musical information—one approach to the design of such an interface leverages performers' existing knowledge by reducing the gap between data visualization and traditional musical notation as much as possible. Using Abjad, a Python-based tool for musical composition, the symbols of conventional staff notation are augmented to illustrate the intensity and temporal proximity of performed musical events graphically, thus facilitating the comparison of individual performances and the study of changes in performance aesthetics over time.",Automated Notation of Piano Recordings for Historic Performance Practice Study,Association for Computing Machinery (ACM),2014,,Journal on Computing and Cultural Heritage,7,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
887,http://dx.doi.org/10.1145/2597183,"{'string_id': '10.1145/2597183', 'id_scheme': 'DOI'}","Intangible cultural heritage represents the cultural identities and diversity of mankind, and should be preserved and passed on to the current and future generations. However, in digital cultural heritage research, intangible cultural heritage has been relatively less studied, and the focus has been placed on creating tools and applications for professionals rather than for the general public. In this paper, we present our research effort on creating an interactive system for conveying traditional Chinese culture through natural language conversation. We propose a systematic method for the domain experts to construct, with minimal effort, the knowledge base from a set of unstructured philosophy texts, and we design an algorithm to build a conversational agent emulating the conversation ability of a famous Chinese philosopher. We also report two evaluation studies on the prototype we developed, showing encouraging results on the feasibility and benefits of our approach. By automating part of the answer-finding task using natural language processing and information retrieval technology, the system is able to find answers dynamically, without the need to manually author large amounts of question and answer pairs. Our proposed method could potentially be used to create other conversational agents for educating and promoting cultural values to the general public in a natural and appealing way.",Interacting with Traditional Chinese Culture through Natural Language,Association for Computing Machinery (ACM),2014,,Journal on Computing and Cultural Heritage,7,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
888,http://dx.doi.org/10.1145/2597894,"{'string_id': '10.1145/2597894', 'id_scheme': 'DOI'}","Today’s information society needs efficient and economic solutions for the digital restoration of the photographic heritage. Different methods have been adopted up to now for the automatic detection of dust and scratches; each method has pros and cons, and a limited field of effectiveness. The use of infrared radiation and the spatiotemporal image analysis are among the most effective methods, although they have their limits. The infrared radiation only works for dye-based material, while the spatiotemporal image analysis is not applicable for still images and is limited due to motion in the scene. The present work defines in detail a set of methods for optical dust and scratches detection applicable on any type of transparent photographic material (silver-based as well as dye-based material, still images as well as moving images). The term “optical” refers to the fact that the considered methods seek physical evidence of the presence of foreign bodies or irregularities on the film; this allows avoiding the typical digital artifacts produced by “nonoptical” methods, for which certain elements of the scenes are erroneously obliterated because they resemble dust grains or scratches. “PDD” (Polarized Dark-field Detection) detects the flaws using an image acquired in a polarized dark-field setup; “DCD” (Dual Collimation Detection) takes advantage of the Callier effect to locate the flaws; “n-MDD” (Multiple Direction Detection) entails the acquisition of n images in dark-field setups with different directions of illumination, and the extraction of the differences between the images through multivariate analysis. A numerical evaluation of the performances of the MDD method with an eightfold acquisition (8-MDD) is carried out by comparing its flaw detection with the flaw detection provided by commercial software based on spatiotemporal image analysis.",Optical Detection of Dust and Scratches on Photographic Film,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
889,http://dx.doi.org/10.1145/2611374,"{'string_id': '10.1145/2611374', 'id_scheme': 'DOI'}","This article describes the evolution of an information system (IS) of Centro Internazionale di Studi di Architettura Andrea Palladio di Vicenza and the related interfaces from the first standalone on a CD-ROM system to the current web-based and georeferenced solution. The goal of these ISs was manifold: the dissemination, study, and analysis of architectural history. The IS was designed for many types of users. Mainly, we present the interface and the interaction techniques that were conceived and developed for large audiences during expositions, in the museum, or at home. This article presents a special focus on communicative language changes and on the level of information provided, which are determined by the different interfaces adopted over time.",Interacting with the Andrea Palladio Works,Association for Computing Machinery (ACM),2014,,Journal on Computing and Cultural Heritage,7,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
890,http://dx.doi.org/10.1145/2611375,"{'string_id': '10.1145/2611375', 'id_scheme': 'DOI'}","Starting from our experience in this domain, we discuss some fundamental concepts about the potentialities of the virtual reconstructions of cultural sites inside museums, with a specific focus on the communication needs, the design, the combination of media, the interaction interfaces, and the embodiment. We conceive a virtual reconstruction as a digital ecosystem, whose main peculiarities are (1) 3D reconstruction, (2) inclusivity, and (3) interactivity. A virtual reconstruction, in a wide sense, should integrate different levels of visualization, both realistic and symbolic; 3D models; metadata; storytelling; behaviors; and tools of visualization and interaction, in order to “reconstruct” and communicate a cultural context, an ecosystem where all the information is integrated. Despite the great advancements of the last years in the digitization process, computer graphics techniques, and archiving strategies, a basic limit of most of virtual museums is that they do not fire up the attention and the involvement of the public: they lack stimulating activities for visitors, narratives metaphors, and emotional impact. The interaction interfaces are not always simple to understand and to control in a few minutes, and they can generate a sense of frustration that causes users to abandon the application after a short and superficial approach. No gap should exist between knowledge and communication. But how can we translate the complexity of the knowledge in appealing to users and into simple applications that fit with the public's need? This article focuses on some communication rules and criteria that are often considered of minor importance by the researchers working in the field of digital cultural heritage but that are really essential to cultural transmission, especially inside museums. We believe that a stronger collaboration between research institutions and museums and among different disciplines would be recommended. Given this premise, we present the Etruscanning EU project, developed in 2011--2013, focused on the virtual reconstruction of two important Etruscan tombs of the Orientalizing period: the Regolini-Galassi tomb in Cerveteri and the tomb n.5 of Monte Michele in Veii.",Interacting with Virtual Reconstructions in Museums,Association for Computing Machinery (ACM),2014,,Journal on Computing and Cultural Heritage,7,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
891,http://dx.doi.org/10.1145/2627729,"{'string_id': '10.1145/2627729', 'id_scheme': 'DOI'}","This research has been conducted in the context of the ArtiMuse project that aims at the modeling and renewal of rare gestural knowledge and skills involved in the traditional craftsmanship and more precisely in the art of wheel-throwing pottery. These knowledge and skills constitute intangible cultural heritage and refer to the fruit of diverse expertise founded and propagated over the centuries thanks to the ingeniousness of the gesture and the creativity of the human spirit. Nowadays, this expertise is very often threatened with disappearance because of the difficulty to resist globalization and the fact that most of those “expertise holders” are not easily accessible due to geographical or other constraints. In this article, a methodological framework for capturing and modeling gestural knowledge and skills in wheel-throwing pottery is proposed. It is based on capturing gestures using wireless inertial sensors and statistical modeling. In particular, we used a system that allows for online alignment of gestures using a modified Hidden Markov Model. This methodology is implemented into a human--computer interface, which permits both the modeling and recognition of expert technical gestures. This system could be used to assist in the learning of these gestures by giving continuous feedback in real time by measuring the difference between expert and learner gestures. The system has been tested and evaluated on different potters with rare expertise, which is strongly related to their local identity.","Capture, Modeling, and Recognition of Expert Technical Gestures in Wheel-Throwing Art of Pottery",Association for Computing Machinery (ACM),2014,,Journal on Computing and Cultural Heritage,7,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
892,http://dx.doi.org/10.1145/2629484,"{'string_id': '10.1145/2629484', 'id_scheme': 'DOI'}","Despite their temporal proximity, the technologies of the early computers are far from us. Yet, they are part of the history of science and technology, and they deserve to be studied and popularized. Being machines made to run software programs, they should be exhibited running. Unfortunately, old machines still in working condition are extremely rare. Restoring or rebuilding an old computer is a hard, expensive task: the original components are rare, and the technology is forgotten and sometimes lost. The research needed to re-understand those computers has to adopt experimental archaeology methods: rebuilding old hardware/software requires proceeding by hypotheses and experiments.However, a rebuilt or restored computer is a unique exemplar and a precious specimen: it is not suitable to let people interact with it. A more flexible solution is to use software simulations. First of all, simulation is a valuable tool to carry out the experiments needed to study past technology. Second, the simulators are virtual replicas that let people fully understand the old machines by interacting with them without jeopardizing those precious relics of the past.This article presents the virtual rebuilding of the first computer made in Italy: the Macchina Ridotta (MR) of the University of Pisa. The MR was dismantled after few months of intensive usage to cannibalize the materials for a second computer. As a consequence, the MR disappeared from later chronicles and for many years was ignored by historians. When we attempted to reconstruct the MR history, we found that the survived documentation was far from complete. Simulation proved to be the key tool to support the experimental approach adopted for understanding the MR technology, rebuilding it, and assessing its achievements. The MR simulator is now used at the Museum of Computing Machinery of Pisa as a mean to truly experience a working session on the MR—a typical computer from the 1950s. The exhibit and the workshops, by exploiting the accurately reproduced characteristics of the MR, address popularization of computer science from several perspectives: from technological mechanisms to scientific foundations, passing through the representation of computers in popular culture.",A Virtual Experience on the Very First Italian Computer,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,7,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
893,http://dx.doi.org/10.1145/2629513,"{'string_id': '10.1145/2629513', 'id_scheme': 'DOI'}","Studying Greek and Latin cultural heritage has always been considered essential to the understanding of important aspects of the roots of current European societies. However, only a small fraction of the total production of texts from ancient Greece and Rome has survived up to the present, leaving many gaps in the historiographic records. Epigraphy, which is the study of inscriptions (epigraphs), helps to fill these gaps. In particular, the goal of epigraphy is to clarify the meanings of epigraphs; to classify their uses according to their dating and cultural contexts; and to study aspects of the writing, the writers, and their “consumers.” Although several research projects have recently been promoted for digitally storing and retrieving data and metadata about epigraphs, there has actually been no attempt to apply data mining technologies to discover previously unknown cultural aspects. In this context, we propose to exploit the temporal dimension associated with epigraphs (dating) by applying a data mining method for novelty detection. The main goal is to discover relational novelty patterns—that is, patterns expressed as logical clauses describing significant variations (in frequency) over the different epochs, in terms of relevant features such as language, writing style, and material. As a case study, we considered the set of Inscriptiones Christianae Vrbis Romae stored in Epigraphic Database Bari, an epigraphic repository. Some patterns discovered by the data mining method were easily deciphered by experts since they captured relevant cultural changes, whereas others disclosed unexpected variations, which might be used to formulate new questions, thus expanding the research opportunities in the field of epigraphy.",Discovering Novelty Patterns from the Ancient Christian Inscriptions of Rome,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,7,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
894,http://dx.doi.org/10.1145/2629682,"{'string_id': '10.1145/2629682', 'id_scheme': 'DOI'}","As scanning technologies improve new approaches to digitizing cultural heritage artefacts emerge. The use of micro--computed tomography (micro-CT) presents an interesting approach as it is capable of high-resolution scans of not just the surface of the artifact but also the intricate internal structures and volumes leading to more complete digitization than is possible with traditional surface laser scanning techniques. In this work, we investigate the potential to digitize heritage artifacts using micro-CT and focus on the methods of interacting with the datasets to create digital restorations of broken and incomplete pieces before printing physical replicas using 3D printing technology. We visualize the virtual models with clear identification of the new portions added by a digital artist with reference to existing pieces and archives.","Using a Combination of Micro--Computed Tomography, CAD and 3D Printing Techniques to Reconstruct Incomplete 19th-Century Cantonese Chess Pieces",Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,7,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
895,http://dx.doi.org/10.1145/2629693,"{'string_id': '10.1145/2629693', 'id_scheme': 'DOI'}","Effective documentation and display of ancient objects is an essential task in the field of cultural heritage conservation. Digitization plays an important role in the process of creating, preserving, and accessing objects in digital space. Up to the present day, industrial scanners are used for this task, which focus mainly on the detailed reconstruction of the object’s geometry only. However, particularly important for a faithful digital presentation of the object is the appearance information—that is, a description of the used materials and how they interact with incident light. Using the world’s first full-spherical scanner, we propose a user-friendly reconstruction process that is specifically tailored to the needs of digitizing and representing cultural heritage artifacts. More precisely, our hardware specifically addresses the problem that invaluable or fragile artifacts may not be turned over during acquisition. Nevertheless, we can digitize the object completely, including its bottom. Further, by integrating appearance information into our digitization, we achieve a far more faithful digital replica with a quality comparable to a real picture of the object. But in contrast to a static picture, our representation allows one to interactively change the viewing and lighting directions freely. In addition, the results are very memory efficient, consuming only several megabytes per scanned object. In cooperation with museums and a private collector, we digitized several cultural heritage artifacts to demonstrate the feasibility of the proposed process.","Fully Automatic, Omnidirectional Acquisition of Geometry and Appearance in the Context of Cultural Heritage Preservation",Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
896,http://dx.doi.org/10.1145/2637485,"{'string_id': '10.1145/2637485', 'id_scheme': 'DOI'}","This article deals with the design and implementation of an immersive installation where users could gesturally navigate around the wreck of Vrouw Maria, a Dutch merchant ship that sank near the Finnish coast in 1771 and was rediscovered in 1999. The installation was built for the Maritime Museum in Kotka, Finland, and is part of the preservation efforts of the wreck, which still remains underwater. In addition to the cultural heritage aspect, the project was an experiment in holistic user-centered design, where several design methods, such as scenarios, role playing and informance, storyboards, and prototyping, were employed throughout the process in order to envision the final product as well as assess their utility in the scope of immersive installations. The approach we have taken and documented here can be used as a starting point for similar projects where archaeological sites are reconstructed virtually and presented, for example, in a museum setting.",A Holistic User-Centered Approach to Immersive Digital Cultural Heritage Installations,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,7,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
897,http://dx.doi.org/10.1145/2644822,"{'string_id': '10.1145/2644822', 'id_scheme': 'DOI'}","In 2010, Naturalis Biodiversity Center started one of the largest and most diverse programs for natural history collection digitization to date. From a total collection of 37 million specimens and related objects, 7 million relevant objects are to be digitized in a 5-year period. This article provides an overview of the program and discusses the chosen industrial production line approach, the applied method for prioritization of collections that are to be digitized, and some preliminary results.",The Natural History Production Line,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
898,http://dx.doi.org/10.1145/2644823,"{'string_id': '10.1145/2644823', 'id_scheme': 'DOI'}","We propose an approach for improving the digitization of shape and color of 3D artworks in a cluttered environment using 3D laser scanning and flash photography. To separate clutter from acquired material, semiautomated methods are employed to generate masks used to segment the range maps and the color photographs. This approach allows the removal of unwanted 3D and color data prior to the integration of acquired data in a 3D model. Sharp shadows generated by flash acquisition are easily handled by this masking process, and color deviations introduced by the flash light are corrected at the color blending step by taking into account the geometry of the object. The approach has been evaluated in a large-scale acquisition campaign of the Mont’e Prama complex. This site contains an extraordinary collection of stone fragments from the Nuragic era, which depict small models of prehistoric nuraghe (cone-shaped stone towers), as well as larger-than-life archers, warriors, and boxers. The acquisition campaign has covered 37 statues mounted on metallic supports. Color and shape were acquired at a resolution of 0.25mm, which resulted in more than 6,200 range maps (about 1.3G valid samples) and 3,817 photographs.",Mont’e Scan,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
899,http://dx.doi.org/10.1145/2659020,"{'string_id': '10.1145/2659020', 'id_scheme': 'DOI'}","Massive digital acquisition and preservation of deteriorating historical and artistic documents is of particular importance due to their value and fragile condition. The study and browsing of such digital libraries is invaluable for scholars in the Cultural Heritage field but requires automatic tools for analyzing and indexing these datasets. We present two completely automatic methods requiring no human intervention: text height estimation and text line extraction. Our proposed methods have been evaluated on a huge heterogeneous corpus of illuminated medieval manuscripts of different writing styles and with various problematic attributes, such as holes, spots, ink bleed-through, ornamentation, background noise, and overlapping text lines. Our experimental results demonstrate that these two new methods are efficient and reliable, even when applied to very noisy and damaged old handwritten manuscripts.",ATHENA,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
900,http://dx.doi.org/10.1145/2660776,"{'string_id': '10.1145/2660776', 'id_scheme': 'DOI'}","Besides being a medium for exhibiting three-dimensional heritage objects, the design and development of virtual heritage environments could also facilitate cultural learning. The aim of this article is to present some consideration on factors that could facilitate cultural learning in virtual heritage environment. Cultural learning in the context of this study refers to the acquisition of knowledge that encourages cultural awareness and appreciation. In this study, we explore virtual heritage environment design features, and cultural information characteristics and presentation formats that are important to facilitate cultural learning, from the casual user's perspective. We conducted experiments that required users to explore virtual heritage environments (focusing on architectural heritage) from selected heritage-related websites, followed by interview sessions. Four contributing factors were identified: information design, information presentation, navigation mechanism, and environment setting. The results suggest the need to reduce users’ cognitive load when exploring virtual heritage environments while simultaneously presenting information that adds value to their understanding. The results also suggest that the inclusion of cultural information in virtual heritage environments is essential for cultural learning to take place.",Factors Facilitating Cultural Learning in Virtual Architectural Heritage Environments,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
901,http://dx.doi.org/10.1145/2665072,"{'string_id': '10.1145/2665072', 'id_scheme': 'DOI'}","The understanding and reconstruction of a wrecks formation process can be a complicated procedure that needs to take into account many interrelated components. The team of the University of Cyprus investigating the 4th-century BC Mazotos shipwreck are unable to interact easily and intuitively with the recorded data, a fact that impedes visualization and reconstruction and subsequently delays the evaluation of their hypotheses. An immersive 3D visualization application that utilizes a VR CAVE was developed, with the intent to enable researchers to mine the wealth of information this ancient shipwreck has to offer. Through the implementation and evaluation of the proposed application, this research seeks to investigate whether such an environment can aid the interpretation and analysis process and ultimately serve as an additional scientific tool for underwater archaeology.",Visualizing and Assessing Hypotheses for Marine Archaeology in a VR CAVE Environment,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
902,http://dx.doi.org/10.1145/2677083,"{'string_id': '10.1145/2677083', 'id_scheme': 'DOI'}","Visitor studies explore human experiences within museums, cultural heritage sites, and other informal learning settings to inform decisions. Smartphones offer novel opportunities for extending the depth and breadth of visitor studies while considerably reducing their cost and their demands on specialist human resources. By enabling the collection of significantly higher volumes of data, they also make possible the application of advanced machine-learning and visualization techniques, potentially leading to the discovery of new patterns and behaviors that cannot be captured by simple descriptive statistics. In this article, we present a principled approach to the use of smartphones for visitor studies, in particular proposing a structured methodology and associated methods that enable its effective use in this context. We discuss specific methodological considerations that have to be addressed for effective data collection, preprocessing, and analysis and identify the limitations in the applicability of these tools using family visits to the London Zoo as a case study. We conclude with a discussion of the wider opportunities afforded by the introduction of smartphones and related technologies and outline the steps toward establishing them as a standard tool for visitor studies.",Conducting Visitor Studies Using Smartphone-Based Location Sensing,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
903,http://dx.doi.org/10.1145/2686873,"{'string_id': '10.1145/2686873', 'id_scheme': 'DOI'}","The Dream of the Rood is one of the earliest Christian poems in Old English and an example of the genre of dream poetry. While a complete text can be found in the 10th-century “Vercelli Book,” the poem is considerably older, and its oldest occurrence is carved (in runes) on the 7- to 8th-century Ruthwell Stone Cross. In this article, we present the work done in the framework of the “Visionary Cross” project, starting from the digitization of the Ruthwell Cross to the creation of a web-based digital edition of The Dream of the Rood, as it is carved on the Cross. The 3D data has been collected and processed with the explicit aim of creating a multimedia framework able to present the highly detailed digital model acquired with 3D scanning technology, together with the transcription and translation of the runes that can be found on its surface. The textual and spatial information are linked through a system of bidirectional links called Spots, which allow the users to navigate freely over the multimedia content, keeping the 3D and textual data synchronized. The present work discusses the different issues that arose during the work, from digitization and processing to the design of a tool for the integration of three-dimensional content in the context of the presentation on the web platform of heterogeneous multimedia data. We end with the difficulties involved in the creation of an XML encoding that could account for the necessities of the visualization system but remain within the scholarly encoding standards of the relevant disciplinary community.",The Dream and the Cross,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
904,http://dx.doi.org/10.1145/2693847,"{'string_id': '10.1145/2693847', 'id_scheme': 'DOI'}","The literature on information retrieval shows the importance of information reliability as a key criterion for relevance judgment. However, information reliability evaluation is discussed in many disciplines such as history, Arabic storytelling, and computer science. Although these disciplines share common principles, they differ in many aspects, which are studied in this article. However, we mainly focus on two disciplines. On the one hand, Arabic storytelling stands by rigid rules for transmitting information and inspecting sources and contents. On the other hand, the characteristics of the Web as a collaborative, open and vast area for information sharing has caused changes in our evaluation of information. This article studies related works to enumerate the main principles and steps that constitute guidelines for automatic information reliability evaluation. Finally, these guidelines are applied to Arabic storytelling, and experimental results are presented.",Information Reliability Evaluation,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
905,http://dx.doi.org/10.1145/2700422,"{'string_id': '10.1145/2700422', 'id_scheme': 'DOI'}","The microscopic description of ancient pottery is widely used for the fabric definition, classification and provenance assessment. In most cases, however, the description is qualitative. An improvement of the study of archaeological pottery needs a more objective approach with quantitative analysis. In classical scientific literature, the structural features and mineralogical composition of pottery are carried out on thin sections by means of transmitted polarized light microscope. The determination were obtained through observations with and without cross polarizator (nicols). The quantitative measurements are normally achieved with tedious and time consuming table with point counter. In this article the attention has been focused on the automatic identification of structural and textural components of the potteries through optical microscopy. Image analysis techniques have been then used to automatically classify the image components. Results confirm the effectiveness of the proposed approach: petrographic data collection becomes faster with respect to the traditional method providing also quantitative information useful for fabric recognition.",Automatic Extraction of Petrographic Features from Pottery of Archaeological Interest,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
906,http://dx.doi.org/10.1145/2700425,"{'string_id': '10.1145/2700425', 'id_scheme': 'DOI'}","Over the past three decades, the introduction of digital technologies in the field of architectural documentation has profoundly changed tools and acquisition techniques. Most of the developments concern metrical and colorimetric characteristics of the objects studied. These developments, surrounding the practice of architectural survey, tend to respond primarily to the requirements of completeness. In this context, it seems necessary to assess the impact of these instruments on the cognitive value of architectural representation. With a strong technological presence, the study of the built heritage is facing a problem of “information overload.” Indeed, this strong technological presence fails to strengthen representation in its role as a vehicle of knowledge. Confronted with the intelligibility deficit, this article proposes an original approach for reading morphological features of an artifact by using a bottom-up approach: the meaning of elements (i.e., their semantic layouts) come from a statistical analysis of the major shape discontinuities of a collection of instances. The idea is to rely on data accumulation to render apparent high-level semantic structures from the comparative analysis of common low-level geometric features. The principles introduced are illustrated by the study of 31 columns of the cloister of the abbey of Saint-Michel-de-Cuxa. To summarize, the first objective is to understand how digital technologies can help us in the analysis of artistic and technical production of Romanesque columns. The second objective is to automatically identify the common semantic articulations of the entire collection to build a reference model for the future assessment of each artifact.",What Do Thirty-One Columns Say about a “Theoretical” Thirty-Second?,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
907,http://dx.doi.org/10.1145/2700427,"{'string_id': '10.1145/2700427', 'id_scheme': 'DOI'}","Andean textiles are products of one of the richest, oldest and continuous weaving traditions in the world. Understanding the knowledge and practice of textile production as a form of cultural heritage is particularly relevant in the Andean context due to erosion of clothing traditions, reuse of traditional textiles on commodities targeted at the tourism market, and loss of knowledge embedded in textile production. “Weaving Communities of Practice” was a pilot project that aimed to create a knowledge base of Andean weaving designed to contribute to curatorial practice and heritage policy. The research team gathered data on the chain of activities, instruments, resources, peoples, places and knowledge involved in the production of textiles, relating to over 700 textile samples. A major part of the project has been the modelling and representation of the knowledge of domain experts and information about the textile objects themselves in the form of an OWL ontology, and the development of a suite of search facilities to be supported by the ontology. This paper describes the research challenges faced in developing the ontology and search facilities, the methodology adopted, the design and implementation of the system, and the design and outcomes of a user evaluation of the system undertaken with a group of domain experts.",An Ontological Approach to Creating an Andean Weaving Knowledge Base,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
908,http://dx.doi.org/10.1145/2700428,"{'string_id': '10.1145/2700428', 'id_scheme': 'DOI'}","This article describes a Mixed-Reality (MR) application that superimposes lost buildings of a historical site onto real scenes virtualized using spherical aerial images. The proposed application is set at a UNESCO World Heritage site in Japan, and is based on a novel framework that supports the photorealistic superimposition of virtual objects onto virtualized real scenes. The proposed framework utilizes Image-Based Rendering (IBR), which enables users to freely change their viewpoint in a real-world virtualization constructed using precaptured images. This framework combines the offline rendering of virtual objects and IBR to take advantage of the higher quality of offline rendering without the additional computational cost of online processing; that is, it incurs only the cost of online lightweight IBR, which is simplified through the pregeneration of structured viewpoints (e.g., at grid points).",Mixed-Reality World Exploration Using Image-Based Rendering,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
909,http://dx.doi.org/10.1145/2716324,"{'string_id': '10.1145/2716324', 'id_scheme': 'DOI'}","This article deals with storage and transportation of fragile heritage objets. The main goal is to create an ad hoc packaging software solution by combining the use of 3D scanning and 3D milling technologies. Traditionally, packaging for this kind of object is made manually, creating custom supports and boxes adapted to the specific needs and particularities of each case. This process is time consuming and fully depends on the conservator’s skills. On the other hand, the proposed solution provides a faster execution and a standard solution adapted to every object. The process starts with the acquisition of 3D digital models of the original artifacts and uses an interactive GPU-accelerated algorithm to assist the conservator in the construction of the package. The final result is a CNC program that is sent to a low-cost milling machine to produce the housing for the selected artifact. Unlike 3D printers, milling machines allow working with lots of different materials, which is a key aspect for this kind of package: chemical stability and shock absorption are mandatory. Given that during the design process original artifacts are only manipulated in the acquisition stage, risks of damaging them are also reduced. Since the whole process is considered in the same application, intermediate calculations are performed considering manufacturing and user-defined restrictions. This way, together with a GPU implementation, achieved results are very fast, providing a real-time application with visual feedback.",Automatic Production of Tailored Packaging for Fragile Archaeological Artifacts,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
910,http://dx.doi.org/10.1145/2724723,"{'string_id': '10.1145/2724723', 'id_scheme': 'DOI'}","Location-based systems currently represent a suitable solution to enhance cultural experiences inside museums, as they can satisfy visitors’ needs through the provision of contextualized contents and services. In this framework, a promising approach to captivate the attention of teenagers—a hard to please target audience—is represented by mobile serious games, such as playful activities aiming to primarily fulfil educational purposes. The use of a mobile digital tool during the visit definitely discloses new opportunities for contextual learning scenarios; however, so far, only a few studies have analysed the impact of different communication approaches on visitors’ degree of exploration and acquisition of knowledge. This work aims to enrich this field of research, presenting the conceptual framework; the design principles; and the evaluation results of “Gossip at palace,” a location-based mobile game integrating a storytelling approach. The game was developed for an Italian historical residence to communicate its 18th-century history to teenagers, capitalizing on narrative and game mechanics to foster young visitors’ motivations to explore the museum and facilitate their meaning-making process. Following a mixed-methods perspective, the article firstly describes to what extent the components of the application were appreciated by teenagers as well as by other visitor segments. Secondly, it provides an insight on the effectiveness of the game in facilitating the acquisition of historical knowledge by participants, enriched by considerations on the methods to be adopted when evaluating mobile learning in informal educational settings. Thirdly, players’ degree of use of the digital game throughout the visit is compared to analogous patterns registered for people using a multimedia mobile guide in the same venue. On the one hand, the study pointed out that the game facilitated a wider exploration of the museum; on the other, it highlighted that players mainly gained a superficial knowledge of the proposed contents.",Integrating a Location-Based Mobile Game in the Museum Visit,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
911,http://dx.doi.org/10.1145/2739049,"{'string_id': '10.1145/2739049', 'id_scheme': 'DOI'}","Digital reconstruction of ancient polychromy is a relatively recent phenomenon in the history of archaeological and architectural heritage documentation. It has emerged as a result of new interest in the experimental archaeology and the technological development of computer graphic tools. Therefore, this article presents a project aimed at the scientific reconstruction of the original polychromy and the realistic rendering of an ancient sarcophagus, obtained with the use of some open source tools. In particular, starting from the accurate study of a polychrome Roman sarcophagus (National Roman Museum in Rome, inv. no. 125891), we show how the MeshLab tool was used to support the polychrome reconstruction stage and how Blender (or, rather, a combination of MeshLab and Blender) was used to achieve a more sophisticated visual presentation of the current and reconstruction ancient colour. A central role has been given to the rendering of different and overlapped layers of paint.",Ancient Polychromy,Association for Computing Machinery (ACM),2015,,Journal on Computing and Cultural Heritage,8,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
912,http://dx.doi.org/10.1145/2770877,"{'string_id': '10.1145/2770877', 'id_scheme': 'DOI'}","The 3D reconstruction of archeological sites is still an expensive and time-consuming task. In this article, we present a novel interactive, low-cost approach to 3D reconstruction and compare it to a standard photogrammetry pipeline based on high-resolution photographs. Our novel real-time reconstruction pipeline is based on a low-cost, consumer-level hand-held RGB-D sensor. While scanning, the user sees a live view of the current reconstruction, allowing the user to intervene immediately and adapt the sensor path to the current scanning result. After a raw reconstruction has been acquired, the digital model is interactively warped to fit a geo-referenced map using a handle-based deformation paradigm. Even large sites can be scanned within a few minutes, and no costly postprocessing is required. The quality of the acquired digitized raw 3D models is evaluated by comparing them to actual imagery, a geo-referenced map of the excavation site, and a photogrammetry-based reconstruction. We made extensive tests under real-world conditions on an archeological excavation in Metropolis, Ionia, Turkey. We found that the reconstruction quality of our approach is comparable to that of photogrammetry. Yet, both approaches have advantages and shortcomings in specific setups, which we analyze and discuss.",Low-Cost Real-Time 3D Reconstruction of Large-Scale Excavation Sites,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
913,http://dx.doi.org/10.1145/2786760,"{'string_id': '10.1145/2786760', 'id_scheme': 'DOI'}","This article reports on a successful project for 3D imaging research, digital applications, and use of new technologies in the museum. The article will focus on the development and implementation of a viable workflow for the production of high-quality 3D models of museum objects, based on the 3D laser scanning and photogrammetry of selected ancient Egyptian artefacts. The development of a robust protocol for the complete process chain for imaging cultural heritage artefacts, from the acquisition of 2D and/or 3D images to the development of interactive applications for the public audience, was a specific objective of the project. The workflow devised by the university museum team combines reference photography and 3D imaging with a curatorial review of the actual object to its digital counterpart. It also integrates methodologies for managing the accompanying metadatasets to record these activities. As final stage deliverables from the process, the museum is making high-quality 3D images of artefacts from its collection available through creation and dissemination of digital 3D multiplatform interactive applications in order to allow remote access and to enhance the museum's public engagement. This short article concludes with practical considerations for a 3D imaging workflow such as time and skills needed, 3D model quality, and expectation management.",Developing 3D Imaging Programmes--Workflow and Quality Control,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
914,http://dx.doi.org/10.1145/2795233,"{'string_id': '10.1145/2795233', 'id_scheme': 'DOI'}","By collecting images of heritage assets from members of the public and processing them to create 3D-reconstructed models, the HeritageTogether project has accomplished the digital recording of nearly 80 sites across Wales, UK. A large amount of data has been collected and produced in the form of photographs, 3D models, maps, condition reports, and more. Here we discuss some of the different methods used to realize the potential of this data in different formats and for different purposes. The data are explored in both virtual and tangible settings, and—with the use of a touch table—a combination of both. We examine some alternative representations of this community-produced heritage data for educational, research, and public engagement applications.",Alternative Representations of 3D-Reconstructed Heritage Data,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
915,http://dx.doi.org/10.1145/2810040,"{'string_id': '10.1145/2810040', 'id_scheme': 'DOI'}","We present an approach to shorten Ancient Greek sentences by using morpho-syntactic information attached to each word in a sentence. This work underpins the content of our eLearning application, AncientGeek, whose unique teaching technique draws from primary Greek sources. By applying a technique that skips the clausal dependents of a main verb, we reached a well-formed rate of 89% of the sentences.",Sentence Shortening via Morpho-Syntactic Annotated Data in Historical Language Learning,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
916,http://dx.doi.org/10.1145/2812814,"{'string_id': '10.1145/2812814', 'id_scheme': 'DOI'}","Storytelling, especially in the form called drama, is pervasive across cultures and ages. Though much attention has been devoted to the preservation of the physical supports of drama (e.g., films and tapes), there is a widespread acknowledgment that such tangible heritage is the expression of an intangible notion of drama. This article introduces drama as a form of intangible cultural heritage and presents a solution for its preservation in terms of a formal encoding through a computational ontology. After the review of a formal representation of drama, called Drammar, developed in previous works, we show how an abstraction of drama can be encoded into a digital item. We also show how the method proposed is compliant with the major initiatives for the documentation of cultural heritage, namely CIDOC-CRM and FRBR. Finally, we test the applicability of our solution by showing how the major tenets of two well-known theories of drama can be encoded in Drammar and presenting the results of a focus group of drama scholars and practitioners who have accessed the encoding through a visualization system. The encoding and visualization system have turned out to be a promising support for teaching and investigating drama.",Safeguarding and Accessing Drama as Intangible Cultural Heritage,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
917,http://dx.doi.org/10.1145/2822899,"{'string_id': '10.1145/2822899', 'id_scheme': 'DOI'}","We have experienced rapid development in Augmented Reality (AR) systems and platforms in the world of cultural heritage, namely in cultural settings and historical museums. However, we still face a range of challenges to design an AR system that meets the requirements for an AR installation working autonomously in a cultural heritage setting for an extended duration. This article describes the development of two installations for the visualization of a 3D reconstruction of a castle chapel, running autonomously during open hours in the location of a castle museum. We present a convincing 3D visualization running at interactive frame-rates on modern tablets. In one installation, the tablet is connected to a large screen TV for an immersive experience, and, in another, the tablet is hand-held, thus facilitating translational freedom in the chapel. Both installations allow unsupervised usage during museum visiting hours. Based on in-field observations and on-device logging of application usage, user behavior is analyzed and evaluated. Results indicate that users spent a limited amount of time using the application and did not fully explore the visual area of the chapel. In order for the user to spend more time with the application, additional information must be presented to the user.",Handheld Visual Representation of a Castle Chapel Ruin,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
918,http://dx.doi.org/10.1145/2827856,"{'string_id': '10.1145/2827856', 'id_scheme': 'DOI'}","The need to preserve and live in cities with value and care for cultural heritage makes information technologies a tool to locate, document, describe, and visualize important urban contexts. With technology resources and the convergence of media such as text, data, maps, charts, pictures, and virtual models, among others, cultural heritage can be celebrated, contemplated, and admired more easily. However, creating an application for reactivation of cultural heritage by citizens remains a challenge because there is no clear methodology to be followed for its development. As far as we know, no one has proposed a methodological framework for applications in this field to determine which aspects should be considered and which tools should be used to successfully develop a cultural heritage application. This article proposes a methodological framework that allows a developer to categorize the type of cultural heritage application to develop and determine which types and quality of resources should be used in the application to provide good usability while fitting project requirements. The methodology is applied for the reactivation of Cisneros marketplace at Medellin, in Colombia, as a case study for testing the framework proposed. This article is part of a project called Reactivation of Cultural Heritage in Colombia using Information and Communication Technologies.",Methodological Framework for the Design and Development of Applications for Reactivation of Cultural Heritage,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
919,http://dx.doi.org/10.1145/2835495,"{'string_id': '10.1145/2835495', 'id_scheme': 'DOI'}","Ancient Chinese architecture from the Song dynasty is a prominent example of the ancient oriental architectures. The cai-fen system was a module system used for the carpentry of Song architectures, which was specified by the governmental manual, the Yingzao Fashi (State Building Standards) compiled by Li Jie [1103]. We present a rule-based approach for generation of ancient Chinese architectures from the Song dynasty. Based on the special module system and the hierarchical topology of structural patterns in traditional Chinese architectures, the approach parameterizes the wooden elements of buildings and formalizes the construction rules for different architecture styles. In the approach, XML-based description files are generated for displaying the construction process. What the approach generates are standard architectures that strictly follow the ancient Chinese governmental manual. To demonstrate the efficiency of our approach, architectures in different styles have been generated based on their corresponding rules. The fundamental difference between our approach and previous works is that we apply and implement the module system in digitalization of ancient Chinese architecture.",Rule-Based Generation of Ancient Chinese Architecture from the Song Dynasty,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
920,http://dx.doi.org/10.1145/2875428,"{'string_id': '10.1145/2875428', 'id_scheme': 'DOI'}","Flamenco is a music tradition from Southern Spain that attracts a growing community of enthusiasts around the world. Its unique melodic and rhythmic elements, the typically spontaneous and improvised interpretation, and its diversity regarding styles make this still largely undocumented art form a particularly interesting material for musicological studies. In prior works, it has already been demonstrated that research on computational analysis of flamenco music, despite it being a relatively new field, can provide powerful tools for the discovery and diffusion of this genre. In this article, we present corpusCOFLA, a data framework for the development of such computational tools. The proposed collection of audio recordings and metadata serves as a pool for creating annotated subsets that can be used in development and evaluation of algorithms for specific music information retrieval tasks. First, we describe the design criteria for the corpus creation and then provide various examples of subsets drawn from the corpus. We showcase possible research applications in the context of computational study of flamenco music and give perspectives regarding further development of the corpus.",Corpus COFLA,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
921,http://dx.doi.org/10.1145/2891416,"{'string_id': '10.1145/2891416', 'id_scheme': 'DOI'}","This article presents an observational study of eight families engaging with a bespoke tablet experience produced for a space science centre. It documents the various ways in which family members orientate themselves to the usage of technology in this environment, with a particular focus on the work done to manage the tablet and facilitate the engagement of younger children with the narrative of the experience. These findings are considered in the broader context of the need to design experiences that cater to engagement by families as a whole. We conclude by motivating the need for technologies that are robust in light of regular disengagement and by family members, and which provide functionality to directly support facilitation work.",Families and Mobile Devices in Museums,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
922,http://dx.doi.org/10.1145/2901297,"{'string_id': '10.1145/2901297', 'id_scheme': 'DOI'}","We describe a common problem in the curation and analysis of archaeological materials: restoring the orientation and dimensions of damaged objects. Our focus is a common architectural type in Mediterranean sites, the Doric column drum, which we investigate at one of the earliest Doric temples in the Greek world, the Hera temple at Olympia. The 3D modeling and analysis of this building by the Digital Architecture Project since 2013 has revealed new insights into the construction history of its stone colonnades. This paper concerns the analysis of the 3D models of the in situ material, using the almost 100 fallen drums and capitals to reconstruct the colonnade digitally. In order to accomplish this, we propose two novel methods for training the machine to estimate the dimensions of a fragmentary column drum. One approach is a modification of ICP, where the fragment is compared to an ideal model of an intact drum, which is resized iteratively until concluding with a satisfactory fit. Another approach recasts the scan data into polar coordinates and uses RANSAC to identify the exterior profiles of the piece and remove points likely to belong to damaged areas. The filtered points are then examined by the algorithm to estimate the radii and taper of the drum. Besides saving a great deal of time in the field, these methods are also accurate to within 0.2% of the total radius for well-preserved material, and 1% for even the most fragmentary drums at Olympia. These data have allowed the digital reconstruction of 80% of the displaced drums and all of the capitals from the temple. Our algorithms can be used to measure any fluted column drums, and we discuss the potential value of our approach for other categories of archaeological artifacts.",Pattern Matching and the Analysis of Damaged Ancient Objects,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
923,http://dx.doi.org/10.1145/2905369,"{'string_id': '10.1145/2905369', 'id_scheme': 'DOI'}","Shape representations are critical for visual analysis of cultural heritage materials. This article studies two types of shape representations in a bag-of-words-based pipeline to recognize Maya glyphs. The first is a knowledge-driven Histogram of Orientation Shape Context (HOOSC) representation, and the second is a data-driven representation obtained by applying an unsupervised Sparse Autoencoder (SA). In addition to the glyph data, the generalization ability of the descriptors is investigated on a larger-scale sketch dataset. The contributions of this article are four-fold: (1) the evaluation of the performance of a data-driven auto-encoder approach for shape representation; (2) a comparative study of hand-designed HOOSC and data-driven SA; (3) an experimental protocol to assess the effect of the different parameters of both representations; and (4) bridging humanities and computer vision/machine learning for Maya studies, specifically for visual analysis of glyphs. From our experiments, the data-driven representation performs overall in par with the hand-designed representation for similar locality sizes on which the descriptor is computed. We also observe that a larger number of hidden units, the use of average pooling, and a larger training data size in the SA representation all improved the descriptor performance. Additionally, the characteristics of the data and stroke size play an important role in the learned representation.",Evaluating Shape Representations for Maya Glyph Classification,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
924,http://dx.doi.org/10.1145/2915919,"{'string_id': '10.1145/2915919', 'id_scheme': 'DOI'}","We present and evaluate a scalable interactive system for the exploration of large collections of detailed three-dimensional digital models of sculptures. The system has been applied to the valorization of the Mont’e Prama complex, an extraordinary collection of protostoric Mediterranean sculptures, which depict models of cone-shaped stone towers, as well as larger-than-life human figures. The software architecture is based on scalable components for efficient distribution and adaptive rendering of extremely detailed surface meshes with overlaid information. The user interface, based on a simple and effective interactive camera controller tailored for touch interaction, has been designed for targeting both small screens and large display systems. The system components have been integrated in different interactive applications, ranging from large-screen museum setups and low-end mobile devices both with very high visual quality. The large-scale system has been installed in a variety of temporal and permanent exhibitions and has been extensively used by tens of thousands of visitors. We provide an early analysis in this article of the data gathered during a 20-month period in the National Archaeological Museum in Cagliari and a 6-month period in the Civic Museum in Cabras, for a total of over 67,000 exploration sessions.",Digital Mont’e Prama,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
925,http://dx.doi.org/10.1145/2940332,"{'string_id': '10.1145/2940332', 'id_scheme': 'DOI'}","An author's literary style is influenced by the cultural time period in which the author lives. The author's ideas, and the words chosen to express them, can help identify the cultural time period that most influenced the author.Ideas are expressed in language through sequences of words called n-grams. Over the past several years, Google has been engaged in digitizing millions of books. As part of this endeavor, Google has created a database of n-grams extracted from these digitized books and has made the database available to researchers online. This is the first time ever that such an extensive repository of cultural data has been made available.This study develops and tests an original method for utilizing Google's database to identify the cultural time period that most influenced the author of a published work. Several undisputed literary works are examined, from which sets of n-grams are extracted and compared against the Google database. The frequency and distribution of n-gram matches allow us to determine the cultural time period that most influenced the author. The method is also tested against several literary works having uncertain or disputed authorship and period of composition.The results suggest that the method developed provides a reasonable approximation of the time period of greatest cultural influence for each book. Unexpectedly, the results tend to support conclusions reached by another researcher with regard to prior literary influences on the Ern Malley Poems. In addition, they lend support to early 19th-century origins for authorship of Book of Mormon",Using n-Grams to Identify Time Periods of Cultural Influence,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
926,http://dx.doi.org/10.1145/2943778,"{'string_id': '10.1145/2943778', 'id_scheme': 'DOI'}","As technologies for 3D acquisition become widely available, it is expected that 3D content documenting heritage artifacts will become increasingly popular. Nevertheless, to provide access to and enable the creative use of this content, it is necessary to address the challenges to its access. These include the automatic enrichment of 3D content with suitable metadata so that content does not get lost. To address these challenges, this article presents research on developing technologies to support the organization and discoverability of 3D content in the Cultural Heritage (CH) domain. This research takes advantage of the fact that heritage artifacts have been designed throughout the centuries with distinctive design styles. Hence, the shape and the decoration of an artifact can provide significant information on the history of the artifact. The main contributions of this article include an ontology for documenting 3D representations of heritage artifacts decorated with ornaments such as architectural mouldings. In addition, the article presents a complementary shape retrieval method based on shape saliency to improve the automatic classification of the artifact’s semantic information based on its 3D shape. This method is tested on a collection of Regency ornament mouldings found in domestic interiors. This content provides a rich dataset on which to base the exploration of issues common to many CH artifacts, such as design styles and decorative ornament.",Analyzing the Decorative Style of 3D Heritage Collections Based on Shape Saliency,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
927,http://dx.doi.org/10.1145/2948069,"{'string_id': '10.1145/2948069', 'id_scheme': 'DOI'}","We address the problem of the statistical description of 3D surfaces with the purpose of automatic classification and retrieval of archaeological potsherds. These are particularly interesting problems in archaeology, as pottery comprises a great volume of findings in archaeological excavations. Indeed, the analysis of potsherds brings relevant cues for understanding the culture of ancient groups. In particular, we develop a new local shape descriptor for 3D surfaces, called the histogram of spherical orientations (HoSO), which we use in combination with a bag-of-words approach to compute visual similarity between 3D surfaces. Given a point of interest on a 3D surface, its local shape descriptor (HoSO) captures the distribution of the spherical orientations of its neighboring points. In turn, those spherical orientations are computed with respect to the point of interest itself, both in the azimuth and the zenith axis. The proposed HoSO is invariant to scale transformations and highly robust to rotation and noise. In addition, it is efficient, as it only exploits the information of the position of the 3D points and disregards other types of information like faces or normals. We performed experiments on a set of 3D surfaces representing potsherds from the Teotihuacan civilization and further validations on a set of 3D models of generic objects. Our results show that our methodology is effective for describing 3D models and that it improves classification performance with respect to previous local descriptors.",Classification and Retrieval of Archaeological Potsherds Using Histograms of Spherical Orientations,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
928,http://dx.doi.org/10.1145/2964911,"{'string_id': '10.1145/2964911', 'id_scheme': 'DOI'}","By bringing together the most prominent European institutions and archives in the field of Classical Latin and Greek epigraphy, the EAGLE project has collected the vast majority of the surviving Greco-Latin inscriptions into a single readily-searchable database. Text-based search engines are typically used to retrieve information about ancient inscriptions (or about other artifacts). These systems require that the users formulate a text query that contains information such as the place where the object was found or where it is currently located. Conversely, visual search systems can be used to provide information to users (like tourists and scholars) in a most intuitive and immediate way, just using an image as query. In this article, we provide a comparison of several approaches for visual recognizing ancient inscriptions. Our experiments, conducted on 17, 155 photos related to 14, 560 inscriptions, show that BoW and VLAD are outperformed by both Fisher Vector (FV) and Convolutional Neural Network (CNN) features. More interestingly, combining FV and CNN features into a single image representation allows achieving very high effectiveness by correctly recognizing the query inscription in more than 90% of the cases. Our results suggest that combinations of FV and CNN can be also exploited to effectively perform visual retrieval of other types of objects related to cultural heritage such as landmarks and monuments.",Visual Recognition of Ancient Inscriptions Using Convolutional Neural Network and Fisher Vector,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
929,http://dx.doi.org/10.1145/2966985,"{'string_id': '10.1145/2966985', 'id_scheme': 'DOI'}","In this article, we present a method to carry out a computerized epigraphic survey of historic stones and their engraved epigraphy. In fact, the preservation of archaeological objects has always been an issue of concern for the research community. On the one hand, the fragility of the objects limits their study. On the other hand, such objects are housed in museums, libraries, and institutions worldwide, locations that significantly limit their accessibility. Different survey methods have been carried out in these attempts to overcome the aforementioned limitations. Among them, the MicMac open source software was used in this work to survey and process the dense correlation of the hieroglyphics inscriptions engraved on archaeological stones. Relatively standard workflows have led to depth maps, which can be represented either as 3D point clouds or shaded relief images. This article shows the application of this method in two epigraphic survey case studies.",Close-Range Photogrammetric Tools for Epigraphic Surveys,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
930,http://dx.doi.org/10.1145/2983917,"{'string_id': '10.1145/2983917', 'id_scheme': 'DOI'}","Traditionally, the preservation of archaeological data has been limited by the cost of materials and the physical space required to store them, but for the last 20 years, increasing amounts of digital data have been generated and stored online. New techniques in digital photography and document scanning have dramatically increased the amount of data that can be retained in digital format, while at the same time reducing the physical cost of production and storage. Vast numbers of hand written notes, grey literature documents, images of assemblages, contexts, and artefacts have been made available online. However, accessing these repositories is not always straightforward. Superficial interaction design, sparsely populated metadata, and heterogeneous schemas may prevent users from working the data that they need within archaeological archives. In this article, we present the work of the Digging into Archaeological Data and Image Search Metadata project (DADAISM), a multidisciplinary project that draws together the work of researchers from the fields of archaeology, interaction design, image processing and text mining to create an interactive system that supports archaeologists in their tasks in online archives. By adopting a user-centred approach with techniques grounded in contextual design, we identified the phases of archaeologists work in online archives, which are distinctive to this user group. The insights from this work drove the design and evaluation of an interactive system that successfully integrates content-based image based retrieval and improved metadata searching to deliver a positive user experience when working with online archives.",Improving Archaeologists’ Online Archive Experiences Through User-Centred Design,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
931,http://dx.doi.org/10.1145/2983918,"{'string_id': '10.1145/2983918', 'id_scheme': 'DOI'}","In musicology and music research generally, the increasing availability of digital music, storage capacities, and computing power enable and require new and intelligent systems. In the transition from traditional to digital musicology, many techniques and tools have been developed for the analysis of individual pieces of music, but large-scale music data that are increasingly becoming available require research methods and systems that work on the collection-level and at scale. Although many relevant algorithms have been developed during the past 15 years of research in Music Information Retrieval, an integrated system that supports large-scale digital musicology research has so far been lacking. In the Digital Music Lab (DML) project, a collaboration among music librarians, musicologists, computer scientists, and human-computer interface specialists, the DML software system has been developed that fills this gap by providing intelligent large-scale music analysis with a user-friendly interactive interface supporting musicologists in their exploration and enquiry. The DML system empowers musicologists by addressing several challenges: distributed processing of audio and other music data, management of the data analysis process and results, remote analysis of data under copyright, logical inference on the extracted information and metadata, and visual web-based interfaces for exploring and querying the music collections. The DML system is scalable and based on Semantic Web technology and integrates into Linked Data with the vision of a distributed system that enables music research across archives, libraries, and other providers of music data. A first DML system prototype has been set up in collaboration with the British Library and I Like Music Ltd. This system has been used to analyse a diverse corpus of currently 250,000 music tracks. In this article, we describe the DML system requirements, design, architecture, components, and available data sources, explaining their interaction. We report use cases and applications with initial evaluations of the proposed system.",The Digital Music Lab,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
932,http://dx.doi.org/10.1145/2994151,"{'string_id': '10.1145/2994151', 'id_scheme': 'DOI'}","A cross-period (diachronic) thesaurus enables users to search for information using modern terminology and obtain semantically related terms from earlier historical periods. The complex task of supporting the construction of a diachronic thesaurus by a domain expert lexicographer has hardly been addressed computationally until now. In this article, we introduce a semiautomatic iterative Query Expansion (QE) scheme for supporting diachronic thesaurus construction, which identifies candidate related terms based on statistical corpus-based measures. We use ancient-modern period classification to increase the performance of the statistical cooccurrence measures and extend our methods to deal with Multi-Word Expressions (MWEs). We demonstrate the empirical benefit of our scheme for a Jewish cross-period thesaurus and evaluate its impact on recall and on the effectiveness of the lexicographer’s manual efforts.",Semiautomatic Construction of Cross-Period Thesaurus,Association for Computing Machinery (ACM),2016,,Journal on Computing and Cultural Heritage,9,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
933,http://dx.doi.org/10.1145/2996469,"{'string_id': '10.1145/2996469', 'id_scheme': 'DOI'}","We propose three automatic algorithms for analyzing digitized medieval manuscripts, text block computation, text line segmentation, and special component extraction, by taking advantage of previous clustering algorithms and a template-matching technique. These three methods are completely automatic, so no user intervention or input is required to make them work. Moreover, they are all per-page based; that is, unlike some prior methods—that need a set of pages from the same manuscript for training purposes—they are able to analyze a single page without requiring any additional pages for input, eliminating the need for training on additional pages with similar layout. We extensively evaluated the algorithms on 1,771 images of pages of six different publicly available historical manuscripts, which differ significantly from each other in terms of layout structure, acquisition resolution, writing style, and so on. The experimental results indicate that they are able to achieve very satisfactory performance, that is, the average precision and recall values obtained by the text block computation method can reach as high as 98% and 99%, respectively.",Automatic Single Page-Based Algorithms for Medieval Manuscript Analysis,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
934,http://dx.doi.org/10.1145/2996859,"{'string_id': '10.1145/2996859', 'id_scheme': 'DOI'}","We present a system for automatically extracting hieroglyph strokes from images of degraded ancient Maya codices. Our system adopts a region-based image segmentation framework. Multi-resolution super-pixels are first extracted to represent each image. A Support Vector Machine (SVM) classifier is used to label each super-pixel region with a probability to belong to foreground glyph strokes. Pixelwise probability maps from multiple super-pixel resolution scales are then aggregated to cope with various stroke widths and background noise. A fully connected Conditional Random Field model is then applied to improve the labeling consistency. Segmentation results show that our system preserves delicate local details of the historic Maya glyphs with various stroke widths and also reduces background noise. As an application, we conduct retrieval experiments using the extracted binary images. Experimental results show that our automatically extracted glyph strokes achieve comparable retrieval results to those obtained using glyphs manually segmented by epigraphers in our team.",Extracting Maya Glyphs from Degraded Ancient Documents via Image Segmentation,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
935,http://dx.doi.org/10.1145/3003435,"{'string_id': '10.1145/3003435', 'id_scheme': 'DOI'}","Ancient paintings can provide valuable information for historians and archeologists to study the history and humanity of the corresponding eras. How to determine the era in which a painting was created is a critical problem, since the topic of a painting cannot be used as an effective basis without an era label. To address this problem, this article proposes a novel computational method by using multi-view local color features extracted from the paintings. First, we extract the multi-view local color features for all training images using a novel descriptor named Affine Lab-SIFT. Then we can learn the codebook from all these features by k-means clustering. Afterwards, we create a feature histogram for each image in the form of bag-of-visual-words and use a supervised fashion to train a classifier, which is used for further painting classification. Experimental results from two different datasets show the effectiveness of the proposed classification system and the advantage of the proposed features, especially in the case of small-size training samples.",Multi-View Feature Combination for Ancient Paintings Chronological Classification,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
936,http://dx.doi.org/10.1145/3004457,"{'string_id': '10.1145/3004457', 'id_scheme': 'DOI'}","Over the course of the past century, there have been significant changes in the practices of archives driven by the massive increase in the volume of records for archiving, a larger and more diverse user base, and the digital turn. This paper analyses work undertaken by the European Holocaust Research Infrastructure project (EHRI) to develop heritage archives into research infrastructures by connecting their knowledge and making it relevant for research. In the article, we focus on EHRI’s work on an integrated collection portal, acting as a central gateway to the rich information on Holocaust-related sources. At the time of writing, the portal contains over 150,000 descriptions of over 1,850 institutions that hold Holocaust-related archival material in 51 countries. In addition, it hosts concise reports that provide in-depth per-country information about the Holocaust history and archival situation in 47 countries, topic-focused research guides, and a range of other services. The article presents how the EHRI portal work connects to the state of the art of heritage portals and the novel solutions we had to develop to align the portal with the requirements of a research infrastructure.",The European Holocaust Research Infrastructure Portal,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
937,http://dx.doi.org/10.1145/3009905,"{'string_id': '10.1145/3009905', 'id_scheme': 'DOI'}","The problem of the restoration of broken artifacts, where large parts could be missing, is of high importance in archaeology. The typical manual restoration can become a tedious and error-prone process, which also does not scale well. In recent years, many methods have been proposed for assisting the process, most of which target specialized object types or operate under very strict constraints. We propose a digital shape restoration pipeline consisting of proven, robust methods for automatic fragment reassembly and shape completion of generic three-dimensional objects of arbitrary type. In this pipeline, first we introduce a novel unified approach for handling the reassembly of objects from heavily damaged fragments by exploiting both fracture surfaces and salient features on the intact sides of fragments, when available. Second, we propose an object completion procedure based on generalized symmetries and a complementary part extraction process that is suitable for driving the fabrication of missing geometry. We demonstrate the effectiveness of our approach using real-world fractured objects and software implemented as part of the European Union--funded PRESIOUS project, which is also available for download from the project site.",From Reassembly to Object Completion,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
938,http://dx.doi.org/10.1145/3012284,"{'string_id': '10.1145/3012284', 'id_scheme': 'DOI'}","The digitization initiatives in the past decades have led to a tremendous increase in digitized objects in the cultural heritage domain. Although digitally available, these objects are often not easily accessible for interested users because of the distributed allocation of the content in different repositories and the variety in data structure and standards. When users search for cultural content, they first need to identify the specific repository and then need to know how to search within this platform (e.g., usage of specific vocabulary). The goal of the EEXCESS project is to design and implement an infrastructure that enables ubiquitous access to digital cultural heritage content. Cultural content should be made available in the channels that users habitually visit and be tailored to their current context without the need to manually search multiple portals or content repositories. To realize this goal, open-source software components and services have been developed that can either be used as an integrated infrastructure or as modular components suitable to be integrated in other products and services. The EEXCESS modules and components comprise (i) Web-based context detection, (ii) information retrieval-based, federated content aggregation, (iii) metadata definition and mapping, and (iv) a component responsible for privacy preservation. Various applications have been realized based on these components that bring cultural content to the user in content consumption and content creation scenarios. For example, content consumption is realized by a browser extension generating automatic search queries from the current page context and the focus paragraph and presenting related results aggregated from different data providers. A Google Docs add-on allows retrieval of relevant content aggregated from multiple data providers while collaboratively writing a document. These relevant resources then can be included in the current document either as citation, an image, or a link (with preview) without having to leave disrupt the current writing task for an explicit search in various content providers’ portals.",Ubiquitous Access to Digital Cultural Heritage,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
939,http://dx.doi.org/10.1145/3012285,"{'string_id': '10.1145/3012285', 'id_scheme': 'DOI'}","The online encyclopedia Wikipedia is both a cultural reference to store, refer to, and organize digitized and digital information, as well as a key contemporary digital heritage endeavor in itself. Capitalizing on this dual nature of the project, this article introduces Wikipedia as a digital gateway to and site of an active engagement with cultural heritage. We have developed the open source and freely available analysis architecture Contropedia to examine already existing volunteer user-generated participation around cultural heritage and to promote further engagement with it. Conceptually, we employ the notion of memory work, as it helps to treat Wikipedia's articles, edit histories, and discussion pages as a rich resource to study how cultural heritage is received and (re)worked in and across languages and cultures. Contropedia's architecture allows for the study of the negotiations around and appreciation of cultural heritage without assuming an unchallenged and universal understanding of cultural heritage. The analysis facilitated by Contropedia thus sheds light on the contentious articulation of perspectives on tangible and intangible heritage grounded by conflicting conceptions of events, ideas, places, or persons. Technologically, Contropedia combines techniques based on mining article edit histories and analyzing discussion patterns in talk pages to identify and visualize heritage-related disputes within an article, and to compare these across language versions. In terms of digital heritage, Contropedia presents a powerful tool that opens up a core resource to cultural heritage studies. Moreover, it can form part of a conceptually grounded, technically advanced, and practically enrolled infrastructure for public education that opens up the dynamic formation of both knowledge about cultural heritage and new forms of digital cultural heritage that show a considerable amount of friction.",Digging Wikipedia,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
940,http://dx.doi.org/10.1145/3012286,"{'string_id': '10.1145/3012286', 'id_scheme': 'DOI'}","Information and Communication Technologies have radically changed the modern Cultural Heritage scenery: Simple traditional Information Systems supporting the management of cultural artifacts have left the place to complex systems that expose rich information extracted from heterogeneous data sources—like Sensor Networks, Social Networks, Digital Libraries, Multimedia Collections, Web Data Service, and so on—by means of sophisticated applications that enhance the users’ experience. In this article, we describe SCRABS, a Smart Context-awaRe Browsing assistant for cultural EnvironmentS. SCRABS has been developed during the Cultural Heritage Information Systems national project and promoted by DATABENC, the Cultural Heritage Technological District of the Campania Region, in Italy. SCRABS has been designed on top of a Big Data technological stack as the result of a multidisciplinary project carried out by a heterogeneous team of computer scientists, archeologists, architects, and experts in humanities. We describe the main ideas that support the system, showing its use in some real application scenarios located in the Paestum Archeologica Sites.",Big Data Meets Digital Cultural Heritage,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
941,http://dx.doi.org/10.1145/3012287,"{'string_id': '10.1145/3012287', 'id_scheme': 'DOI'}","Annotations provide a valuable perspective on the semantic information present in digital heritage collections, and in recent years they've been employed in a number of innovative, user-centric techniques that can personalise a user's experience of heritage materials, such as by actively adapting exhibits as a user reveals their interests, or by guiding users to explore collections which are meaningfully linked to what they have previously encountered. Despite the captivating opportunities offered by these techniques, collecting annotations for a large heritage collection is no trivial task. A significant amount of work is required to manually annotate large quantities of heritage materials, and automated, computational approaches leave much to be desired regarding the level of insight and semantic richness that they can currently provide. By analysing the emergent relationships between the initial annotations in a collection, we propose a metadata-driven algorithm for assisting and augmenting the annotation process. This algorithm, called SAGA (Semantically-Annotated Graph Analysis), allows for semi-automatic annotation, which balances the value of the contributions of human annotators with the time and effort-saving benefits of an automatic, suggestion-driven process. SAGA uses an entity relationship-driven approach to make annotation suggestions. It is used in the context of a web-based infrastructure called SAGE (Semantic Annotation by Group Exploration), a multiagent environment which assists groups of experts in creating comprehensive annotation sets for heritage collections. SAGA and SAGE are evaluated from the perspectives of suggestion accuracy, explicit user acceptance and implicit user acceptance, and demonstrate strong results in each evaluation.",A Web-Based Infrastructure for the Assisted Annotation of Heritage Collections,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
942,http://dx.doi.org/10.1145/3012288,"{'string_id': '10.1145/3012288', 'id_scheme': 'DOI'}","Using the Chaco Research Archive (CRA) as a case study, in this article, we discuss the spectrum of intellectual decisions: conceptualization, design, and development, required to make legacy records (accumulated over many years through numerous archaeological expeditions) publicly accessible. Intellectual and operational choices permeated the design and implementation of the digital architecture to provide internet access to the vast information structures inherent in legacy records for the cultural heritage of Chaco Canyon, New Mexico. We explore how an expansive but focused repository can enable opportunities for research and foster communities of co-creation. We also use the CRA as a case study to outline some of the pitfalls of conventional academic metrics for scholarly impact and provide some alternative means to assess the value of digital heritage resources.",Innovation through Large-Scale Integration of Legacy Records,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
943,http://dx.doi.org/10.1145/3012289,"{'string_id': '10.1145/3012289', 'id_scheme': 'DOI'}","The increasing demand of technological facilities for galleries, museums, and archives has led to the need for designing practical and effective solutions for managing the digital life cycle of cultural heritage collections. These facilities have to support users in addressing several challenges directly related to the creation, management, preservation, and visualization of digital collections. Such challenges include, for example, the support for a collaborative management of the produced information, their curation from a multilingual perspective to break the language barriers and make collections available to different stakeholders, and the development of services for exposing structured version of data both to users and machines. Platforms satisfying all of these requirements have to support curators activities and, at the same time, provide facilities for engaging the virtual consumers of the produced data. In this article, we propose a description of an abstract architecture for managing digital collections built on a set of components, services, and APIs able to address the challenges mentioned previously. An instantiation of this architecture is discussed, and we present a use case concerning the management of a digital archive of verbo-visual art. Lessons learned from this experience are reported to outline future activities.",A Knowledge Management Architecture for Digital Cultural Heritage,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
944,http://dx.doi.org/10.1145/3040200,"{'string_id': '10.1145/3040200', 'id_scheme': 'DOI'}","In recent years, thematic route planning is gaining popularity in recreational navigation. A growing number of people start to use route-planning services to prepare, ride, explore, and log their activities, with a particular focus on where they want to ride and what they want to see. In the context of cultural heritage, however, route planners still suffer from lack of data and route weighting/scoring mechanisms to achieve end-user satisfaction. In this article, we take advantage of mobile sensing and geotagging (r)evolution to tackle both issues and propose a novel framework for cultural heritage routing on top of RouteYou’s existing recreational navigation platform. Our first improvement focuses on the automatic collection and multimodal enrichment of thematic cultural heritage points of interest. Second, we introduce a weighting procedure for these points of interest and analyze their meta(data) quality and spatial coverage in our route databases. Finally, we present a novel routing algorithm targeted to cultural heritage exploration. Experimental results show that the proposed framework improves cultural heritage POI coverage and quality with respect to traditional recreational navigation routing algorithms. Furthermore, the proposed framework can easily be used in other thematic routing applications due to its generic architecture, making it a widely applicable approach.",Cultural Heritage Routing,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
945,http://dx.doi.org/10.1145/3051480,"{'string_id': '10.1145/3051480', 'id_scheme': 'DOI'}","Cultural heritage artifacts connect us to past generations and provide links to previous worlds that are beyond our reach. We developed TombSeer, an augmented reality application that aims to immerse the wearer in a museum space engaging two senses (seeing and gesturing) through a holographic heads-up interface that brings virtual, historical artifacts “back to life” through gestural interactivity. This article introduces the TombSeer software prototype and highlights the application of embodied interaction to museum visits using an emerging hardware platform for 3D interactive holographic images (e.g., Meta head-mounted display). This article discusses the TombSeer prototype's development and functionality testing with the Tomb of Kitines exhibit, which was conducted at The Royal Ontario Museum in Toronto, Canada. TombSeer's embodied gestural and visual augmented reality experience functions to aesthetically enhance museum exhibits, cultural heritage sites, and galleries.",More than Meets the Eye,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
946,http://dx.doi.org/10.1145/3051487,"{'string_id': '10.1145/3051487', 'id_scheme': 'DOI'}","Describing cultural heritage objects from the perspective of Linked Open Data (LOD) is not a trivial task. The process often requires not only choosing pertinent ontologies but also developing new models that preserve the most information and express the semantic power of cultural heritage data. Indeed, data managed in archives, libraries, and museums are complex objects themselves, which require a deep reflection on even nonconventional conceptual models. Starting from these considerations, this article describes a research project: to expose the vastness of one of the most important collections of European cultural heritage, the Zeri Photo Archive, as LOD. We describe here the steps we undertook to this end. First, we developed two ad hoc ontologies for describing all issues not completely covered by existent models (the F Entry and the OA Entry Ontology). Then we mapped into RDF the descriptive elements used in the current Zeri Photo Archive catalog, converting into CIDOC CRM and into the two new aforementioned models the source data based on the Italian content standards Scheda F (photography entry, in English) and Scheda OA (work of art entry, in English). Finally, we created an RDF dataset of the output of the mapping that could show a result capable of demonstrating the complexity of our scenario.",Enhancing Semantic Expressivity in the Cultural Heritage Domain,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
947,http://dx.doi.org/10.1145/3057929,"{'string_id': '10.1145/3057929', 'id_scheme': 'DOI'}","Archaeological data are characterized by both spatial and temporal dimensions that are often related to each other and are of particular interest during the interpretation process. For this reason, several attempts have been performed in recent years to develop a geographical information system (GIS) tailored for archaeological data. However, despite the increasing use of information technologies in the archaeological domain, the actual situation is that any agency or research group independently develops its own local database and management application that is isolated from the others. Conversely, the sharing of information and the cooperation between different archaeological agencies or research groups can be particularly useful to support the interpretation process by using data discovered in similar situations with respect to spatio-temporal or thematic aspects. In the geographical domain, the INSPIRE initiative of the European Union tries to support the development of a spatial data infrastructure (SDI) through which several organizations, such as public bodies or private companies, with overlapping goals can share data, resources, tools, and competencies in an effective way. The aim of this article is to lay the basis for the development of an archaeological SDI starting from the experience acquired during the collaboration among several Italian organizations. In particular, the article proposes a spatio-temporal conceptual model for archaeological data based on the ISO Standards of the 19100 family and promotes the use of the GeoUML methodology to put into practice such interoperability. The GeoUML methodology and tools have been enhanced to suit the archaeological domain and to automatically produce several useful documents, configuration files, and code base starting from the conceptual specification. The applicability of the spatio-temporal conceptual model and the usefulness of the produced tools have been tested in three different Italian contexts: Rome, Verona, and Isola della Scala.",An Interoperable Spatio-Temporal Model for Archaeological Data Based on ISO Standard 19100,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
948,http://dx.doi.org/10.1145/3064419,"{'string_id': '10.1145/3064419', 'id_scheme': 'DOI'}","The goal of this article is to propose a modeling method to automatically generate original and new forms of periodic Moorish geometric patterns. The proposed method is based on the symmetry-based approach and the shape grammar formalism. The symmetry-based approach consists of constructing a periodic pattern by using isometric transformations of its template motif. The main contribution of this article is to propose a parametric shape grammar method to generate the template motif. The process used consists first in identifying the shapes vocabulary characterizing the Moorish style. By identifying its common features by the analysis of a dataset of historical Moorish patterns, parametric shape grammar (PSG) rules are then determined. Furthermore, the use of PSGs implies additional mechanisms, such as the consideration of suitable constraints and goals. These mechanisms are imposed by the grid, which is the underlying geometric structure used traditionally by artisans to design sophisticated geometric patterns. The performance of the proposed method is measured by its capability to create original and new motifs of periodic Moorish patterns. This method preserves the authenticity of the style considered, because the generated patterns follow a parametric shape grammar based on a priori domain knowledge and extract one from representative images of Moorish style.",Parametric Shape Grammar Formalism for Moorish Geometric Design Analysis and Generation,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
949,http://dx.doi.org/10.1145/3064527,"{'string_id': '10.1145/3064527', 'id_scheme': 'DOI'}","Research e-infrastructures, digital archives, and data services have become important pillars of scientific enterprise that in recent decades have become ever more collaborative, distributed, and data intensive. The archaeological research community has been an early adopter of digital tools for data acquisition, organization, analysis, and presentation of research results of individual projects. However, the provision of e-infrastructure and services for data sharing, discovery, access, and (re)use have lagged behind. This situation is being addressed by ARIADNE, the Advanced Research Infrastructure for Archaeological Dataset Networking in Europe. This EU-funded network has developed an e-infrastructure that enables data providers to register and provide access to their resources (datasets, collections) through the ARIADNE data portal, facilitating discovery, access, and other services across the integrated resources. This article describes the current landscape of data repositories and services for archaeologists in Europe, and the issues that make interoperability between them difficult to realize. The results of the ARIADNE surveys on users’ expectations and requirements are also presented. The main section of the article describes the architecture of the e-infrastructure, core services (data registration, discovery, and access), and various other extant or experimental services. The ongoing evaluation of the data integration and services is also discussed. Finally, the article summarizes lessons learned and outlines the prospects for the wider engagement of the archaeological research community in the sharing of data through ARIADNE.",ARIADNE,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
950,http://dx.doi.org/10.1145/3064644,"{'string_id': '10.1145/3064644', 'id_scheme': 'DOI'}","Video games and their design are complex in nature, given the variety of aspects and challenges to face and the different areas of expertise involved. Furthermore, serious games have an even tougher challenge, since the knowledge acquisition has the same importance and relevance as entertainment and pleasure for the players. Serious games in cultural heritage require additional effort to introduce immersivity and collaboration among players. This article introduces a framework, named FRACH, to conceive, design, and evaluate immersive and collaborative serious games in cultural heritage. In particular, FRACH provides a design framework with steps to follow during the whole process that is from the early design phase to the evaluation phase of a serious game. We assessed the efficacy of our framework, with a specific case study in cultural heritage, by implementing a section of a serious game named HippocraticaCivitasGame, where players were allowed to visit the thermae of the historical site of San Pietro a Corte and Palazzo Fruscione in the city of Salerno, Italy, and to solve a given puzzle. Results of the game evaluation showed that the game was effective in terms of knowledge acquisition, the participants enjoyed the game, were highly involved in the immersive experience, and, finally, positively rated the idea of using the game for educational learning in the field of cultural heritage.","A Framework to Design, Develop, and Evaluate Immersive and Collaborative Serious Games in Cultural Heritage",Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
951,http://dx.doi.org/10.1145/3075645,"{'string_id': '10.1145/3075645', 'id_scheme': 'DOI'}","Optical character recognition (OCR) engines work poorly on texts published with premodern printing technologies. Engaging the key technological contributors from the IMPACT project, an earlier project attempting to solve the OCR problem for early modern and modern texts, the Early Modern OCR Project (eMOP) of Texas A8M received funding from the Andrew W. Mellon Foundation to improve OCR outputs for early modern texts from the Eighteenth Century Collections Online (ECCO) and Early English Books Online (EEBO) proprietary database products—or some 45 million pages. Added to print problems are the poor quality of the page images in these collections, which would be too time consuming and expensive to reimage. This article describes eMOP's attempts to OCR 307,000 documents digitized from microfilm to make our cultural heritage available for current and future researchers. We describe the reasoning behind our choices as we undertook the project based on other relevant studies; discoveries we made; the data and the system we developed for processing it; the software, algorithms, training procedures, and tools that we developed; and future directions that should be taken for further work in developing OCR engines for cultural heritage materials.",Mass Digitization of Early Modern Texts With Optical Character Recognition,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
952,http://dx.doi.org/10.1145/3084546,"{'string_id': '10.1145/3084546', 'id_scheme': 'DOI'}","Nowadays, ancient coins are under-estimated. In fact, their iconography can provide students and researchers more information than they expect. Existing web solutions allow to retrieve data and to display ancient mints on digital maps, but they do not consider a powerful iconographic approach. The “Digital Iconographic Atlas of Numismatics in Antiquity” (DIANA) fulfill this gap providing a tool for the in-deep analysis of ancient mints specifically considering coin iconography, chronology, and geographical location. In this article, we present the new Cloud-based DIANA architecture that was recently re-engineered extending the Drupal platform. In particular, we focus on a new innovative Graphical User Interface (GUI) and system back-end that allows researchers a smarter diachronic study of the ancient culture according to the mints and coin iconography. The innovative features are highlighted through four case studies regarding personage, animal/mythical creature, flora, and object iconographies.",An Innovative Cloud-Based System for the Diachronic Analysis in Numismatics,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
953,http://dx.doi.org/10.1145/3084547,"{'string_id': '10.1145/3084547', 'id_scheme': 'DOI'}","Global reconstruction of two-dimensional wall paintings (frescoes) from fragments is an important problem for many archaeological sites. The goal is to find the global position and rotation for each fragment so that all fragments jointly “reconstruct” the original surface (i.e., solve the puzzle). Manual fragment placement is difficult and time-consuming, especially when fragments are irregularly shaped and uncolored. Systems have been proposed to first acquire 3D surface scans of the fragments and then use computer algorithms to solve the reconstruction problem. These systems work well for small test cases and for puzzles with distinctive features, but fail for larger reconstructions of real wall paintings with eroded and missing fragments due to the complexity of the reconstruction search space. We address the search problem with an unsupervised genetic algorithm: we evolve a pool of partial reconstructions that grow through recombination and selection over the course of generations. We introduce a novel algorithm for combining partial reconstructions that is robust to noise and outliers, and we provide a new selection procedure that balances fitness and diversity in the population. In experiments with a benchmark dataset, our algorithm is able to achieve larger and more accurate global reconstructions than previous automatic algorithms.",Wall Painting Reconstruction Using a Genetic Algorithm,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
954,http://dx.doi.org/10.1145/3092906,"{'string_id': '10.1145/3092906', 'id_scheme': 'DOI'}","The CENDARI infrastructure is a research-supporting platform designed to provide tools for transnational historical research, focusing on two topics: medieval culture and World War I. It exposes to the end users modern Web-based tools relying on a sophisticated infrastructure to collect, enrich, annotate, and search through large document corpora. Supporting researchers in their daily work is a novel concern for infrastructures. We describe how we gathered requirements through multiple methods to understand historians’ needs and derive an abstract workflow to support them. We then outline the tools that we have built, tying their technical descriptions to the user requirements. The main tools are the note-taking environment and its faceted search capabilities; the data integration platform including the Data API, supporting semantic enrichment through entity recognition; and the environment supporting the software development processes throughout the project to keep both technical partners and researchers in the loop. The outcomes are technical together with new resources developed and gathered, and the research workflow that has been described and documented.",The CENDARI Infrastructure,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
955,http://dx.doi.org/10.1145/3097569,"{'string_id': '10.1145/3097569', 'id_scheme': 'DOI'}","This article describes a study of a digital multimedia system that allows people to enjoy a narrative experience focusing on food and gastronomic culture. With this multimedia application, termed di Piazza in Piazza, people have gotten in contact with several types of digital content about culinary traditions and local Italian products, simply using their hands. This design decision came after the acknowledgment that hands play a prominent role in the relationship between humans and food (think, for example, about the activities of eating and cooking). Studies of di Piazza in Piazza users have basically unveiled two results: (a) it was a really funny experience using hands and (b) narrations focusing on local products and traditions have mostly captivated the interest of visitors, thus confirming the strong cultural relationship among people, culinary traditions, and regional foods.",Handmade Narrations,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
956,http://dx.doi.org/10.1145/3097570,"{'string_id': '10.1145/3097570', 'id_scheme': 'DOI'}","Web archiving initiatives around the world capture ephemeral Web content to preserve our collective digital memory. However, unlocking the potential of Web archives for humanities scholars and social scientists requires a scalable analytics infrastructure to support exploration of captured content. We present Warcbase, an open-source Web archiving platform that aims to fill this need. Our platform takes advantage of modern open-source “big data” infrastructure, namely Hadoop, HBase, and Spark, that has been widely deployed in industry. Warcbase provides two main capabilities: support for temporal browsing and a domain-specific language that allows scholars to interrogate Web archives in several different ways. This work represents a collaboration between computer scientists and historians, where we have engaged in iterative codesign to build tools for scholars with no formal computer science training. To provide guidance, we propose a process model for scholarly interactions with Web archives that begins with a question and proceeds iteratively through four main steps: filter, analyze, aggregate, and visualize. We call this the FAAV cycle for short and illustrate with three prototypical case studies. This article presents the current state of the project and discusses future directions.",Warcbase,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
957,http://dx.doi.org/10.1145/3097571,"{'string_id': '10.1145/3097571', 'id_scheme': 'DOI'}","In this article, we present a semantic-based approach for dealing with the interoperability issue in the conservation-restoration domain. We first evaluate the context and our observations confirm the critical need for a data integration system taking advantage of data semantics. Our solution consists in: (1) building a domain-specific ontology, to rely on a unified understanding of the conservation-restoration data; (2) mapping the shared ontology to each data source, allowing each participating source to manage its own semantic database, consisting of its original data now associated to the semantic level; and (3) integrating all sources’ data, for querying them in the same homogeneous way. The presented achievements have been conducted as part of the PARCOURS project, whose aim is to develop an information system able to provide a unified access to distinct information sources, related to the cultural heritage field in general and the conservation-restoration processes in particular.",Supporting Semantic Interoperability in Conservation-Restoration Domain,Association for Computing Machinery (ACM),2017,,Journal on Computing and Cultural Heritage,10,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
958,http://dx.doi.org/10.1145/3099618,"{'string_id': '10.1145/3099618', 'id_scheme': 'DOI'}","We analyze use of an interactive system for the exploration of highly detailed three-dimensional (3D) models of a collection of protostoric Mediterranean sculptures. In this system, when the object of interest is selected, its detailed 3D model and associated information are presented at high resolution on a large display controlled by a touch-enabled horizontal surface at a suitable distance. The user interface combines an object-aware interactive camera controller with an interactive point-of-interest selector and is implemented within a scalable implementation based on multiresolution structures shared between the rendering and user interaction subsystems. The system was installed in several temporary and permanent exhibitions and was extensively used by tens of thousands of visitors. We provide a data-driven analysis of usage experience based on logs gathered during a 27-month period at four exhibitions in archeological museums for a total of more than 75K exploration sessions. We focus on discerning the main visitor behaviors during 3D exploration by employing tools for deriving interest measures on surfaces and tools for clustering and knowledge discovery from high-dimensional data. The results highlight the main trends in visitor behavior during the interactive sessions. These results provide useful insights for the design of 3D exploration user interfaces in future digital installations.",Data-Driven Analysis of Virtual 3D Exploration of a Large Sculpture Collection in Real-World Museum Exhibitions,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
959,http://dx.doi.org/10.1145/3117801,"{'string_id': '10.1145/3117801', 'id_scheme': 'DOI'}","Virtual environment has the potential to be used as a medium to facilitate cultural learning. However, this requires guidelines on how to design the environment. This article provides a conceptual framework that guides the design of a virtual environment that facilitates cultural learning for casual users. The exploratory sequential mixed-method design approach was used as the basis for the overall research design. Five studies involving experts and end users were performed to identify and evaluate the framework components. The framework consists of four important components: (i) Information Design, (ii) Information Presentation, (iii) Navigation Mechanism, and (iv) Environment Setting, and outlines two types of design elements: basic elements, which are essential for cultural learning to take place, and extended elements, which provide options for enhanced user experience. Results from the framework evaluation suggest that the proposed design elements are useful in facilitating learning and that the experience of using the virtual environment affected the end users’ sense of awareness and appreciation toward heritage value and preservation.",A Conceptual Framework for Designing Virtual Heritage Environment for Cultural Learning,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
960,http://dx.doi.org/10.1145/3127090,"{'string_id': '10.1145/3127090', 'id_scheme': 'DOI'}","The discovery of quasi-crystals has led to a great debate about their unusual structure. The big surprise is that these structures were found in Islamic art several centuries ago. This latest discovery drew the attention of scientists to propose several approaches for the comprehension of these structures by analyzing several quasi-periodic patterns spread around the Islamic world. In this article, we propose a systematic method for generating new quasi-periodic patterns inspired by existing Islamic historical patterns. The method builds Islamic quasi-periodic patterns based on a quasi-periodic tiling and a few intuitive parameters. Given a quasi-periodic tiling, the method divides its tiles (rhombs) into symmetric right triangles and constructs their template motifs. The construction of these template motifs is achieved by a systematic and well-organized process. The content of the tiles is obtained by applying mirror reflections to the constructed template motifs. Finally, the pattern is drawn by putting the content of the constructed tiles in the tiling. To show the effectiveness of this generative method, examples of new quasi-periodic patterns will be presented.",Generating Islamic Quasi-Periodic Patterns,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
961,http://dx.doi.org/10.1145/3127324,"{'string_id': '10.1145/3127324', 'id_scheme': 'DOI'}","This article describes a prototype natural user interface, named the Intangible Musical Instrument, which aims to facilitate access to knowledge of performers that constitutes musical Intangible Cultural Heritage using off-the-shelf motion capturing that is easily accessed by the public at large. This prototype is able to capture, model, and recognize musical gestures (upper body including fingers) as well as to sonify them. The emotional status of the performer affects the sound parameters at the synthesis level. Intangible Musical Instrument is able to support both learning and performing/composing by providing to the user not only intuitive gesture control but also a unique user experience. In addition, the first evaluation of the Intangible Musical Instrument is presented, in which all the functionalities of the system are assessed. Overall, the results with respect to this evaluation were very promising.",A Natural User Interface for Gestural Expression and Emotional Elicitation to Access the Musical Intangible Cultural Heritage,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
962,http://dx.doi.org/10.1145/3131610,"{'string_id': '10.1145/3131610', 'id_scheme': 'DOI'}","In this article, we propose Multi-Entity Bayesian Networks (MEBNs) as the probabilistic ontological framework for the analysis of the Tsamiko and Salsa dances. More specifically, our analysis has the objective of the dancer assessment with respect to both choreography execution accuracy and the synchronization of the dance movements with the musical rhythm. For this task, we make use of the explicit, expert-provided knowledge on dance movements and their relations to the musical beat. Due to the complexity of this knowledge, the MEBNs were used as the probabilistic ontological framework in which the knowledge is formalized. The reason we opt for MEBNs for this task is that they combine Bayesian and formal (first-order) logic into a single model. In this way, the Bayesian probabilistic part of MEBNs was used to capture, using example data and training, the implicit part of the expert knowledge about dances, i.e., this part of the knowledge that cannot be formalized and explicitly defined accurately enough, while the logical maintains the explicit knowledge representation in the same way ontologies do. Moreover, we present in detail the MEBN models we built for Tsamiko and Salsa, using expert-provided explicit knowledge. Last, we conduct experiments that demonstrate the effectiveness of the proposed MEBN-based methodology we employ to achieve our analysis objectives. The results of the experiments demonstrate the superiority of MEBNs to conventional models, such as BNs, in terms of the dancer assessment accuracy.","A Probabilistic, Ontological Framework for Safeguarding the Intangible Cultural Heritage",Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
963,http://dx.doi.org/10.1145/3131778,"{'string_id': '10.1145/3131778', 'id_scheme': 'DOI'}","In this article, we address the problem of 3D inpainting using an exemplar-based method for point clouds. 3D inpainting is a process of filling holes or missing regions in the reconstructed 3D models. Typically, inpainting methods addressed in the literature fill missing regions due to occlusions or inaccurate scanning of 3D models. However, we focus on scenarios involving naturally existing damaged models, which are partly broken or incomplete in the artifacts at cultural heritage sites. We propose an exemplar-based inpainting technique using the region of interest (ROI)-based method to inpaint the missing regions of the damaged model. The ROI of a 3D model is represented as a set of Riemannian manifolds, and metric tensor and Christoffel symbols are used as geometric features to capture the inherent geometry. We then decompose the ROI into basic shape regions, namely, spherical, conical, and cylindrical components, and identify the best-fit match for inpainting. Instead of using a single similar exemplar for inpainting, we select the most relevant best-fit region to fill the missing region from the basic shape regions library obtained from n similar exemplars. We demonstrate the performance of the proposed inpainting method on artifacts at UNESCO World Heritage site Hampi temples, India with varying complexities and sizes for both synthetically generated holes and real missing regions in 3D objects.",Region of Interest-Based 3D Inpainting of Cultural Heritage Artifacts,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
964,http://dx.doi.org/10.1145/3195727,"{'string_id': '10.1145/3195727', 'id_scheme': 'DOI'}","It is of great interest to researchers and scholars in many disciplines (particularly those working on cultural heritage projects) to study parallel passages (i.e., identical or similar pieces of text describing the same thing) in digital text archives. Although there exist a few software tools for this purpose, they are restricted to a specific domain (e.g., the Bible) or a specific language (e.g., Hebrew). In this article, we present in detail how we build a digital infrastructure that can facilitate the search and discovery of parallel passages for any domain in any language. It is at the core of our Samtla (Search And Mining Tools with Linguistic Analysis) system designed in collaboration with historians and linguists. The system has already been used to support research on five large text corpora that span a number of different domains and languages. The key to such a domain-independent and language-independent digital infrastructure is a novel combination of a character-based n-gram language model, space-optimized suffix tree, and generalized edit distance. A comprehensive evaluation through crowdsourcing shows that the effectiveness of our system’s search functionality is on par with the human-level performance.",Finding Parallel Passages in Cultural Heritage Archives,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
965,http://dx.doi.org/10.1145/3216463,"{'string_id': '10.1145/3216463', 'id_scheme': 'DOI'}","Munsell Soil Charts are a very common tool used by archaeologists for the color specification task. Charts are usually employed directly on cultural heritage sites to identify color of soils and collected artifacts. However, charts are designed to be used specifying the color through subjective perception of users, by visual mean, in a time-consuming and error-prone procedure. It is likely that two users may estimate different Munsell notations for the same specimen, as colors are not perceived uniformly by different people. Hence, estimation process should be repeated several times and by more than a single expert user to be considered reliable. In this work, we employ our framework, Automatic Recognition of Color for Archaeology (ARCA), specifically designed to provide a method for objective, deterministic, fast, and automatic Munsell estimation. ARCA is a valuable asset for archaeologists as it provides the definition of a smooth pipeline for an affordable Munsell notation estimation: image acquisition of specimens with general purpose digital cameras in an uncontrolled environment, manual sampling of specimen images in the ARCA desktop application, automatic Munsell color specification, and report generation. We further assess our method with improved color tolerance validations and evaluations, introducing a comparison between ΔE00, ΔE76, ΔL*, Δa*, and Δb* differences. One of the main contributions of this article is the extension of our former dataset ARCA108. We gathered two additional sets of images obtaining a new dataset consisting of pictures of Munsell Soil Charts Editions 2000 and 2009 plus images from a real test case with 16 pottery shards. The new dataset counts 56,160 samples and 328 images, so it has been called ARCA328. Experimental results are reported to investigate which could be the best configuration to be used in the acquisition phase.",Munsell Color Specification using ARCA (Automatic Recognition of Color for Archaeology),Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
966,http://dx.doi.org/10.1145/3230670,"{'string_id': '10.1145/3230670', 'id_scheme': 'DOI'}","Thanks to the digital preservation of cultural heritage materials, multimedia tools (e.g., based on automatic visual processing) considerably ease the work of scholars in the humanities and help them to perform quantitative analysis of their data. In this context, this article assesses three different Convolutional Neural Network (CNN) architectures along with three learning approaches to train them for hieroglyph classification, which is a very challenging task due to the limited availability of segmented ancient Maya glyphs. More precisely, the first approach, the baseline, relies on pretrained networks as feature extractor. The second one investigates a transfer learning method by fine-tuning a pretrained network for our glyph classification task. The third approach considers directly training networks from scratch with our glyph data. The merits of three different network architectures are compared: a generic sequential model (i.e., LeNet), a sketch-specific sequential network (i.e., Sketch-a-Net), and the recent Residual Networks. The sketch-specific model trained from scratch outperforms other models and training strategies. Even for a challenging 150-class classification task, this model achieves 70.3% average accuracy and proves itself promising in case of a small amount of cultural heritage shape data. Furthermore, we visualize the discriminative parts of glyphs with the recent Grad-CAM method, and demonstrate that the discriminative parts learned by the model agree, in general, with the expert annotation of the glyph specificity (diagnostic features). Finally, as a step toward systematic evaluation of these visualizations, we conduct a perceptual crowdsourcing study. Specifically, we analyze the interpretability of the representations from Sketch-a-Net and ResNet-50. Overall, our article takes two important steps toward providing tools to scholars in the digital humanities: increased performance for automation and improved interpretability of algorithms.",How to Tell Ancient Signs Apart? Recognizing and Visualizing Maya Glyphs with CNNs,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
967,http://dx.doi.org/10.1145/3230671,"{'string_id': '10.1145/3230671', 'id_scheme': 'DOI'}","This article presents the findings of a web survey designed to better understand the expectations and use of digital editions of texts. The survey, modelled upon a detailed analysis of 242 projects, recorded 218 complete responses, shedding light on user requirements of digital editions. Specifically, the survey indicates that issues of data reuse, licensing, image availability, and comprehensive documentation are the most requested features of digital editions, although ones which seldom are provided. This analysis feeds into previous studies on good practice in building Digital Humanities resources and puts forward practical recommendations for both creators and funders of digital editions in an effort to promote a stronger consideration of user needs. This survey will be of interest to those who produce digital editions of texts, including developers and engineers, and will also be of interest to those who commission and fund these projects, such as universities, libraries, and archives, whose documentary collections are often showcased in digital editions.",Digital Editions of Text,Association for Computing Machinery (ACM),2019,,Journal on Computing and Cultural Heritage,12,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
968,http://dx.doi.org/10.1145/3230672,"{'string_id': '10.1145/3230672', 'id_scheme': 'DOI'}","Archaeologists spend considerable time orienting and drawing ceramic fragments by hand for documentation, to infer their manufacture, the nature of the discovery site and its chronology, and to develop hypotheses about commercial and cultural exchanges, social organisation, resource exploitation, and taphonomic processes. This study presents a survey of existing solutions to the time-consuming problem of orienting and drawing pottery fragments. Orientation is based on the 3D geometry of pottery models, which can now be acquired in minutes with low-cost 3D scanners. Several methods are presented: they are based on normal vectors, or circle fittings, or profile fittings. All these methods seek to determine the optimal position of the rotation axis. We also present and discuss new approaches and improvements to existing methods. We have developed a suite of functions for the computer-assisted orientation and drawing of archaeological pottery. The profile and contours of the fragment, as well as any possible decoration, can be depicted in various ways: photorealistic rendering or dotted patterns, calculated by ambient occlusion, combined or not with artificial light. The general workflow, evaluated using both synthetic and real-world fragments, is rapid, accurate, and reproducible. It drastically reduces the amount of routine work required to document ceramic artefacts. The information produced, together with the 3D representation of the fragments, can easily be archived and/or exchanged within the archaeological community for further research. The source code (built in the R environment), together with an installation notice and examples, is freely downloadable.",Computer-Assisted Orientation and Drawing of Archaeological Pottery,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
969,http://dx.doi.org/10.1145/3230673,"{'string_id': '10.1145/3230673', 'id_scheme': 'DOI'}","The study of cultural heritage involves many different activities, including visualizing digital data, analyzing information, and sharing results. Current technologies focus on providing better tools for data representation and processing, neglecting the importance of analysis and sharing. In this article, we present a software system, CHER-Ob, which offers powerful tools for evaluating and publishing the results of cultural heritage research. CHER-Ob provides the capability to visualize and add various types of annotations to data in a wide variety of formats. These annotations assist in the analysis phase and are used for sharing the results of a study. A written report can be generated and automatically illustrated using the annotations. In addition, an “animation scheme” is associated with each type of annotation. The schemes make it possible to generate an introductory video overview of an analysis by selecting preferences and annotations. A series of animated sequences of 2D and 3D objects will appear in a user-specified order in a video subtitled by annotations. The system is useful for integrating cultural and digital resources as well as for providing a method to author materials for disseminating cultural heritage findings to the public.",CHER-Ob,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
970,http://dx.doi.org/10.1145/3230674,"{'string_id': '10.1145/3230674', 'id_scheme': 'DOI'}","RGB-D cameras have a great potential to solve several problems arising during the digitization of objects, such as cultural heritage. Three-dimensional (3D) digital preservation is usually performed with the use of high-end 3D scanners, as the 3D points generated by this type of equipment are in average millimeter up to sub-millimeter accurate. The downside of 3D scanners, in addition to the high cost, is the infrastructure requirements. It requires its own source of energy, a large workspace with tripods, special training to calibrate and operate the equipment, and high acquisition time, potentially taking several minutes for capturing a single image. An alternative is the use of low-cost depth cameras that are easy to operate and only require connection to a laptop and a source of energy. There are several recent studies showing the potential of RGB-D sensors. However, they often exhibit errors when applied to a full 360 degrees 3D reconstruction setup, known as the loop closure problem. This kind of error accumulation is intensified by the lower accuracy and large volume of data generated by RGB-D cameras. This article proposes a complete methodology for 3D reconstruction based on RGB-D sensors. To mitigate the loop closure effect, a pairwise alignment method was developed. The proposed approach expands the connectivity graph connections in a pairwise alignment system, by automatically discovering new pairs of meshes with overlapping regions. Then the alignment is more evenly distributed over the aligned pairs, avoiding the loop closure problem of full 3D reconstructions. The experiments were performed on a collection of 30 artworks made by the Baroque artist Antonio Francisco Lisboa, known as Aleijadinho, as part of the Aleijadinho Digital project conducted in partnership with IPHAN (Brazilian National Institute for Cultural and Artistic Heritage) and United Nations Educational, Scientific and Cultural Organization (UNESCO). Experimental results show 3D models that are favorably compared to state-of-the-art methods available in the literature using RGD-D sensors. The main contributions of this work are: a new method for 3D alignment dedicated to attenuate the RGB-D camera loop closure problem; the development and disclosure of a complete, practical solution for 3D reconstruction of artworks; and the construction of 3D digital models of an important and challenging collection of Brazilian cultural heritage, made accessible by a virtual museum.",Exploring RGB-D Cameras for 3D Reconstruction of Cultural Heritage,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
971,http://dx.doi.org/10.1145/3230675,"{'string_id': '10.1145/3230675', 'id_scheme': 'DOI'}","Technologies such as the Internet of Things (IoT) and Social Computing offer promising opportunities to envision and shape augmented experiences in a wide variety of cultural spaces. Such interactive technologies act as a hook to attract visitor's attention, but if they do not provide opportunities to engage people in a more personal and meaningful way, they can be soon discarded. Augmented experiences need to be motivating and connect with values and expectations of visitors and institutions. In a world that is becoming more and more connected and moving towards a participatory culture, the need to participate actively to co-produce knowledge and meaning can be exploited as a powerful intrinsic motivator. In this article, we describe how the Social Display Environment (SDE) promotes participation in cultural spaces whilst keeping the physical connection with the exhibition objects. The SDE makes use of Augmented Reality (AR) technology to augment physical artifacts in an exhibition with overlays of digital information that is generated by the end user (e.g., the visitor). The prototype was exhibited in a cultural center over a weekend to investigate different dynamics in terms of content generation, visitors’ interactions and perceptions with respect to the interplay of digital information and physical artifacts. The prototype was assessed as easy to learn and useful and showed potential to promote a number of meaningful social interactions around the objects involved in the exhibition.",Augmented Experiences in Cultural Spaces through Social Participation,Association for Computing Machinery (ACM),2018,,Journal on Computing and Cultural Heritage,11,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
972,http://dx.doi.org/10.1145/3242901,"{'string_id': '10.1145/3242901', 'id_scheme': 'DOI'}","The assessment of the structural behavior of historic masonry structures like Gothic cathedrals is an important engineering and architectural issue, because of the economic and cultural relevance of such buildings. In this article, we present a complete numerical methodology for point clouds processing, geometrical and parametric 3D modeling, and finite element structural analysis of the spire of the Cathedral of Senlis, France. Our work highlights the particular difficulties linked with digitization and geometrical modeling of highly complex Gothic structures, as well as the need to find compromises between quality and accuracy of extracted data used for geometrical modeling and structural analysis. The methodology enables the semi-automatic transformation of a three-dimensional points cloud, surveyed through terrestrial laser scanner, into a three-dimensional geometrical historic building information modeling (hBIM)-oriented model, and its use to propose a consistent 3D finite element mesh suitable for advanced structural analysis. A full software chain is integrated in the proposed numerical process, so as to use the most important data contained in the real geometry and accurately transposed in the point clouds. After a successful data processing step with 3DReshaper software that proved to be necessary for enhancement of point clouds, a semi-automated geometrical hBIM-oriented modeling step with Rhinoceros5 software and VisualARQ plugin has allowed the construction of a hybrid model by reverse engineering from the point clouds. This 3D model, containing both geometrical and parametric data of the structure, has been exported to the Hyperworks suite for finite element structural analysis under self-weight. Our computations focused on the estimation of the structure deformation and on the distribution of compression and traction stresses in all components of the complex structure. It is found that the spire is safe. Based on reliable and properly detailed results, our study provides significant information for understanding the behavior of the structure and potential damage monitoring.",From Point Cloud Data to Structural Analysis Through a Geometrical hBIM-Oriented Model,Association for Computing Machinery (ACM),2019,,Journal on Computing and Cultural Heritage,12,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
973,http://dx.doi.org/10.1145/3275522,"{'string_id': '10.1145/3275522', 'id_scheme': 'DOI'}","In this article, we propose a new projection-based system to exhibit a realistic virtual cultural object in a museum. We introduce a new design by combining the multi-projection mapping technology with an optical see-through display that overcomes the limitations of the conventional projection-based exhibition by resolving the problems of ambient light and the occlusion by obstacles. We also introduce a mechanically moving projection surface that increases the degree of freedom of the projection content. Both geometric and radiometric calibration methods are used to correctly project a high-quality texture onto the moving 3D projection surface without perspective distortion. To evaluate the strength of the system and a potential for deployment in a general museum context, we conduct a user study that includes both the visitors and museum staff. The result shows that the proposed system can enrich the museum exhibition by creating a virtual cultural object with substantial 3D effect and high-fidelity appearance. We also demonstrate various applications of the system that provide dynamic content of a virtual cultural object by changing its appearance as well as the viewpoint.",A New Projection-based Exhibition System for a Museum,Association for Computing Machinery (ACM),2019,,Journal on Computing and Cultural Heritage,12,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
974,http://dx.doi.org/10.1145/3276772,"{'string_id': '10.1145/3276772', 'id_scheme': 'DOI'}","We consider the problem of localizing visitors in a cultural site from egocentric (first-person) images. Localization information can be useful both to assist the user during his visit (e.g., by suggesting where to go and what to see next) and to provide behavioral information to the manager of the cultural site (e.g., how much time has been spent by visitors at a given location? What has been liked most?). To tackle the problem, we collected a large dataset of egocentric videos using two cameras: a head-mounted HoloLens device and a chest-mounted GoPro. Each frame has been labeled according to the location of the visitor and to what he was looking at. The dataset is freely available in order to encourage research in this domain. The dataset is complemented with baseline experiments performed considering a state-of-the-art method for location-based temporal segmentation of egocentric videos. Experiments show that compelling results can be achieved to extract useful information for both the visitor and the site-manager.",Egocentric Visitors Localization in Cultural Sites,Association for Computing Machinery (ACM),2019,,Journal on Computing and Cultural Heritage,12,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
975,http://dx.doi.org/10.1145/3283253,"{'string_id': '10.1145/3283253', 'id_scheme': 'DOI'}","Cultural heritage institutions are spending considerable effort and resources to provide online access to their collection catalogues and collection management systems, usually through their institutional websites. This improves accessibility and supports research and engagement by diverse user groups, as well as meeting the increasing expectation by audiences that this type of information will be freely and easily available online. However, cultural organisations have not responded to these needs in the same way and have been employing different web tools and features to present their collections online. In this article, we argue that the technological implementation choices and the type of content provided reflect also the philosophy of communication of the institution itself. We used the constructivist approach's learning theory and theory of knowledge and combined these interrelated epistemological and cognitive perspectives to analyse the communication philosophy of a large number of museum online catalogues. The article presents the research carried out initially in 2007 through 2009 and then again in 2017, studying the provision of collections information online by different types of museums across Europe and in the United States. This enabled us to establish categories of presentation types and study the changes over time. The results highlighted the major shift towards participatory practices which have been transforming the cultural heritage world over the past years.",Communicating Museum Collections Information Online,Association for Computing Machinery (ACM),2019,,Journal on Computing and Cultural Heritage,12,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
976,http://dx.doi.org/10.1145/3284388,"{'string_id': '10.1145/3284388', 'id_scheme': 'DOI'}","In recent years, an interdisciplinary effort between archaeologists and computer vision experts has emerged to provide image retrieval tools that facilitate and support cultural heritage preservation. The performance of these tools largely depends on the hieroglyph representation quality. In the literature, the most successful hieroglyph representation for retrieval following the BoVW model includes a thinning hieroglyph process and selects interest points through uniform random sampling. However, thinned hieroglyphs could have noise or redundant information, and a random set of interest points could include non-useful interest points that are different in each iteration. In this article, we propose improving this hieroglyph representation by pruning thinned hieroglyphs and introducing an improved interest-point selection. Our experiments show that our proposal significantly improves the hieroglyph retrieval results of state-of-the-art methods.",Improved Hieroglyph Representation for Image Retrieval,Association for Computing Machinery (ACM),2019,,Journal on Computing and Cultural Heritage,12,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
977,http://dx.doi.org/10.1145/3284425,"{'string_id': '10.1145/3284425', 'id_scheme': 'DOI'}","Acquiring images of archaeological artifacts is an essential step for the study and preservation of cultural heritage. In constrained environments, traditional acquisition techniques may fail or be too invasive. We present an optical device including a camera and a wedge waveguide that is optimized for imaging within confined spaces in archeology. The major idea is to redirect light by total internal reflection to circumvent the lack of room, and to compute the final image from the raw data. We tested various applications on site during an archaeological mission in Medamoud (Egypt). Our device was able to successfully record images of the underground from slim trenches of about 15cm wide, including underwater trenches, and between rocks composing a wall temple. Experts agreed that the acquired images were good enough to get useful information that cannot be obtained as easily with traditional techniques.",Wedge Cameras for Minimally Invasive Archaeology,Association for Computing Machinery (ACM),2019,,Journal on Computing and Cultural Heritage,12,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
978,http://dx.doi.org/10.1145/3287272,"{'string_id': '10.1145/3287272', 'id_scheme': 'DOI'}","Digital heritage interpretation is often untethered from traditional museological techniques and environments. As museums and heritage sites explore the potential of locative technologies and ever more sophisticated content-triggering mechanisms for use outdoors, the kinds of questions that digital heritage researchers are able to explore have become increasingly more complex. Researchers now find themselves in the realm of the immersive, the experiential, and the performative. Working closely with their research participants, they navigate ambiguous terrain, including the often unpredictable affective resonances that are the direct consequences of interaction. This article creates a dialogue between two case studies which, taken together, help to unpack some key methodological and ethical questions emerging from these developments. First, we introduce With New Eyes I See, an itinerant and immersive digital heritage encounter which collapsed boundaries between physical/digital, fact/fiction and past/present. Second, we detail Rock Art on Mobile Phones, a set of dialogic web apps that aimed to explore the potential of mobile devices in delivering heritage interpretation in the rural outdoors. Looking outward from these case studies, we reflect on how traditional evaluation frameworks are being stretched and strained given the kinds of questions that digital heritage researchers are now exploring. Drawing on vignettes from experience-oriented qualitative studies with participants, we articulate specific common evaluative challenges related to the embodied, multimodal, and transmedial nature of the digital heritage experiences under investigation. In doing so, we make the case for reflexivity as a central and more collaborative feature of research design within this field going forward — paying attention to and advocating the reciprocal relationship between researchers and the heritage experiences that we study.",Evaluating Digital Cultural Heritage ‘In the Wild’,Association for Computing Machinery (ACM),2019,,Journal on Computing and Cultural Heritage,12,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
979,http://dx.doi.org/10.1145/3300972,"{'string_id': '10.1145/3300972', 'id_scheme': 'DOI'}","The manual archaeological projectile point morphological classification is an extensive and complex process since it involves a large number of categories. This article presents an algorithm that automatically makes this process, based on the projectile point digital image and using a classification scheme according to global archaeological approaches. The algorithm supports different conditions such as changes in scale and quality of the image. Moreover, it requires only a uniform background and an approximate north--south projectile point orientation. The principal computer methods that compose the algorithm are the curvature scale space map (CSS-map), the gradient contour on the projectile point, and the support vector machines (SVM) algorithm. Finally, the classifier was trained and tested on a dataset of approximately 800 projectile points images, and the results have shown a better performance than other shape descriptors such as Pyramid of Histograms of Orientation Gradients (PHOG), Histogram of Orientation Shape Context (HOOSC) (both used in a bag-of-words context), and geometric moment invariants (Hu moments).",Computer Algorithm for Archaeological Projectile Points Automatic Classification,Association for Computing Machinery (ACM),2019,,Journal on Computing and Cultural Heritage,12,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
980,http://dx.doi.org/10.1145/3303767,"{'string_id': '10.1145/3303767', 'id_scheme': 'DOI'}","Numerous image inpainting algorithms are guided by a basic assumption that the known region in the original image itself can provide sufficient prior information for the guess recovery of the unknown part, which is not often the case in actual art-image inpainting. Sometimes, the art image that needs to be inpainted is so badly damaged that there is little prior information to serve as a good model to infer the appearance of the unknown fragment. Focusing on the lookup strategy for optimal patches, a novel semi-automatic exemplar-based inpainting framework based on a sample dataset is proposed in this article to solve such a problem with three steps: (1) reference images selection from the dataset using deep convolutional network, (2) sample image creation based on reference images with melding algorithm, and (3) exemplar-based inpainting according to the created sample image. Several comparative experiments over Dazu Rock Carvings with the state-of-the-art image completion approaches demonstrate the effectiveness of our contributions. First, the search space for candidate patches is extended from the known region to a sample image. It performs effectively for the inpainting case of little prior information existing in the original image itself. Furthermore, sample image creation is added to reduce the complexity of inpainting via multiple images and avoid the taboo of complete duplication in art restoration. Moreover, Poisson blending is used for post-procedure to improve the visual harmony between the reconstructed fragment and the known region in both color and illumination. Last but not least, our method is successfully applied in the virtual inpainting of Dazu Buddhist face images. The inpainted proposals can be a reference for the final actual artificial inpainting as well as a base for VR show.",Virtual Inpainting for Dazu Rock Carvings Based on a Sample Dataset,Association for Computing Machinery (ACM),2019,,Journal on Computing and Cultural Heritage,12,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
981,http://dx.doi.org/10.1145/3316414,"{'string_id': '10.1145/3316414', 'id_scheme': 'DOI'}","Fourth Industrial Revolution technologies, such as artificial intelligence, big data, the Internet of Things (IoT), and virtual reality, have disrupted legacy methods of operations and have led to progress in many industries worldwide. These technologies also affect the cultural and national heritage. IoT generates large volumes of streaming data; therefore, advanced data analytics using big data analytics and artificial neural networks is an important research topic. In this study, IoT sensor data was collected at the restored Woljeong Bridge, which was originally built in the eighth century, or AD 760, during the Silla Dynasty (57 BC--AD 935) in South Korea. We empirically evaluate a recurrent neural network with recurrent units, including a long short-term memory (LSTM) unit and a gated recurrent unit (GRU). Additionally, we evaluate hybrid deep-learning models (convolution neural networks [CNN]-LSTM and CNN-GRU) to build a prediction model, facilitating the preventive conservation of an invaluable cultural and national heritage site. The experimental results show that the LSTM unit is an effective and robust model. When comparing the hybrid models (i.e., the joint CNN-LSTM and CNN-GRU architectures), we found that the vanilla LSTM and GRU models had superior time-series prediction capabilities.",Cultural Heritage and the Intelligent Internet of Things,Association for Computing Machinery (ACM),2019,,Journal on Computing and Cultural Heritage,12,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
982,http://dx.doi.org/10.1145/3317552,"{'string_id': '10.1145/3317552', 'id_scheme': 'DOI'}","In this article, the design, development, and evaluation of augmented reality (AR)-based mobile application for a tour guide are discussed. The objectives of this article are twofold. First, the research focuses on the development of a complete working set of a mobile tour application furnished with AR. For such an application to be successfully adopted by the general public, user requirements and application usability are investigated, and the application is designed and implemented to fulfill those findings. Second, the developed application is demonstrated by applying it to a UNESCO designated World Heritage site, Hwaseong Fortress in Suwon, South Korea, and evaluated via a survey instrument developed explicitly for mobile application evaluation. A systematically developed survey instrument from the fields of tourism, information systems, and human-computer interaction is tailored to fit into this research and employed for the application evaluation. The application’s operation flow consists of three main functions: navigation to the points of interest, visualization of information with AR technology, and interactive learning activities with AR-based serious games. Efforts are made to provide a more immersive and interactive experience of the historical, cultural, and architectural details of the heritage site utilizing novel AR visualization methods. The evaluation returned positive results with suggestions of possible refinements for future works. The proposed device-aided tour mechanism is anticipated to enhance tourists’ experiences as well as being important guidance in future mobile tourism application development as to how the application should be designed and implemented to be accepted by the general public.",Development of an Augmented Reality Tour Guide for a Cultural Heritage Site,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,12,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
983,http://dx.doi.org/10.1145/3336125,"{'string_id': '10.1145/3336125', 'id_scheme': 'DOI'}","CARE-GIS is a mapping application with Geographic Information Systems (GIS) functionalities that gives a geographic dimension to the important and valuable Corpus Architecturae Religiosae Europeae (CARE) dataset, useful not only for the scientific community but also for a larger public of non-specialists, who can easily access the CARE information from a map. The CARE-GIS application makes it possible to select and display CARE sites’ information (only for Italy, in the current version), by means of query forms, which allows one to cross several fields of the recorded data, to create thematic maps with the selected data. Moreover, the flexible nature of the application, by means of GeoJSON files and the QGIS software, allows one to add more layers, and map historical/archaeological data vs. other types of data, such as geomorphological ones or the analysis of the construction materials, or any other data related to the CARE sites. The displayed CARE data are extracted from the WikiCARE web pages by means of PHP “scraping” functions. A light-weight database is used by the CARE-GIS application, mainly to handle the connection to the WikiCARE website and fasten up the CARE sites’ search functions. The responsive CARE-GIS user interface allows accessing the maps from any kind of digital device, such as computers, tablets, and smartphones, making it a useful web application to get online information, when visiting the CARE mapped historical sites.",Mapping Medieval Christianity. CARE-GIS,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,12,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
984,http://dx.doi.org/10.1145/3340936,"{'string_id': '10.1145/3340936', 'id_scheme': 'DOI'}","This article presents VISE, an interface that enables VIsual Search and Exploration across collections of approximately 836,000 museum objects extracted from the websites of the National Museums Scotland and the Rijksmuseum in the Netherlands. VISE provides an interactive visual summary of information relating to the museum to address online users with no domain knowledge challenges of exploring large museum collection. User satisfaction evaluation was conducted to measure the user satisfaction level for using VISE as the interface for search and exploration of information from large museum collection when compared to non-visual search interface. The evaluation of the visual interface revealed that users are more satisfied and attracted to explore museum objects via VISE than via the system with no visual search interface. Users with little or no domain knowledge find it easier to explore collections and find objects of interest while using VISE in contrast to the system with no visual interface.",VISE,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,12,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
985,http://dx.doi.org/10.1145/3341882,"{'string_id': '10.1145/3341882', 'id_scheme': 'DOI'}","In recent years, digital cultural heritage has attracted much attention in the HCI domain, but there are currently few studies that focus on enhancing the appreciation of intangible cultural heritage content amongst cross-cultural audiences. This article reports on the development of a Digital Gesture Library to support cross-cultural appreciation of traditional Chinese puppetry. We describe fieldwork with professional puppeteers to understand their practices and art form, which informed the development of the Digital Gesture Library, which uses a three-perspective archive of puppetry gestures and a tangible interface to support cross-cultural audiences’ appreciation of puppetry and encourages further exploration of Chinese culture. We present findings on the efficacy of the Digital Gesture Library from qualitative and quantitative user studies and, from this, discuss the opportunities and challenges for developing digital technology for cross-cultural appreciation of intangible heritage.",Supporting the Cross-cultural Appreciation of Traditional Chinese Puppetry Through a Digital Gesture Library,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,12,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
986,http://dx.doi.org/10.1145/3343196,"{'string_id': '10.1145/3343196', 'id_scheme': 'DOI'}","In this article, we present a procedural approach to capture a variety of appearances of American Second Empire houses, which are well known for their mansard roofs and their inspired ornamentation. To develop this procedural approach, we have identified the set of rules and similarities of Second Empire houses. Our procedural approach, therefore, captures the style differences of Second Empire houses with a relatively few number of parameters. Using our interface, we are able to generate virtual houses in a wide variety of styles of American Second Empire architecture. We have also developed a method to break up these virtual models into slices in order to efficiently and economically 3D-print them. Using this method, we have printed miniatures of two landmark buildings in Savannah, GA and Baltimore, MD: The Hamilton-Turner Inn and Enoch Pratt House, respectively. We observe that the virtual models still provide more details because of the limited resolution of the 3D printing process as shown in Figures 1 and 2.",A Procedural Approach to Creating American Second Empire Houses,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
987,http://dx.doi.org/10.1145/3343871,"{'string_id': '10.1145/3343871', 'id_scheme': 'DOI'}","Several studies have highlighted the absence of an integrated comprehensive dataset covering all of the UK’s museums, hence impeding research into the emergence, evolution, and wider impact of the UK’s museums sector. “Mapping Museums” is an interdisciplinary project aiming to develop a comprehensive database of UK museums in existence since 1960, and to use this to undertake an evidence-based analysis of the development of the UK’s museum sector during 1960--2020 and the links to wider cultural, social, and political concerns. A major part of the project has been the iterative, participatory design of a new RDF/S Knowledge Base to store data and metadata relating to the UK’s museums, and a Web Application for the project’s humanities scholars to browse, search, and visualise the data to investigate their research questions. This article presents the challenges we faced in developing the Knowledge Base and Web Application, our methodology and methods, the design and implementation of the system, and the design, outcomes, and implications of a user trial undertaken with a group of experts from the UK’s museums sector.",Creating a Knowledge Base to Research the History of UK Museums through Rapid Application Development,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,12,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
988,http://dx.doi.org/10.1145/3344383,"{'string_id': '10.1145/3344383', 'id_scheme': 'DOI'}","Folk dances often reflect the socio-cultural influences prevailing in different periods and nations; each dance produces a meaning, a story with the help of music, costumes and dance moves. However, dances have no borders; they have been transmitted from generation to generation, along different countries, mainly due to movements of people carrying and disseminating their civilization. Studying the contextual correlation of dances along neighboring countries, unveils the evolution of this unique intangible heritage in time, and helps in understanding potential cultural similarities. In this work we present a method for contextually motion analysis that organizes dance data semantically, to form the first digital dance ethnography. Firstly, we break dance motion sequences into some narrow temporal overlapping feature descriptors, named motion and style words, and then cluster them in a high-dimensional features space to define motifs. The distribution of those motion and style motifs creates motion and style signatures, in the content of a bag-of-motifs representation, that implies for a succinct but descriptive portrayal of motions sequences. Signatures are time-scale and temporal-order invariant, capable of exploiting the contextual correlation between dances, and distinguishing fine-grained difference between semantically similar motions. We then use quartet-based analysis to organize dance data into a categorization tree, while inferred information from dance metadata descriptions are then used to set parent-child relationships. We illustrate a number of different organization trees, and portray the evolution of dances over time. The efficiency of our method is also demonstrated in retrieving contextually similar dances from a database.",Digital Dance Ethnography,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,12,4,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
989,http://dx.doi.org/10.1145/3350427,"{'string_id': '10.1145/3350427', 'id_scheme': 'DOI'}","Museums offer an ideal environment for informal cultural learning on heritage artifacts, where visitors get engaged in learning due to an intrinsic motivation. Sharing the museum space among visitors allows for collective learning experiences and socializing with each other. Museums aim to design and deploy Tangible User Interfaces (TUIs) in order to embrace the physical materialities of artifacts in the visiting experience. TUIs are believed to be more collaborative, attract more visitors, and persuade them to explore further. Cultural learning on heritage artifacts is particularly meaningful from the early age when opinions and attitudes are shaped. Museums accordingly follow a gamification approach (i.e., using game elements in a non-game context) to provide a collaborative and entertaining learning experience to young visitors. In this study, we investigate the implications of merging these two approaches in order to take advantage of the qualities of both TUIs and gamification in an educational museum context. Accordingly, we present TouchTomb and its evaluation in a real-world museum environment. TouchTomb is a situated tangible gamification installation that aims to enhance informal cultural learning for young visitors and to foster engagement and collaboration among them. The basis of the installation is a shared progress bar and three games with different spatial configurations, embedded into a custom fabricated replica of an original ancient Egyptian tomb-chapel wall on a 1:1 scale. Our field study involved 14 school visits with a total number of 190 school pupils (from 10 to 14 years old). We deployed a mixed-method evaluation to investigate how such a tangible gamification approach entertains and educates 15 pupils collectively for a maximum of 15 minutes, including the evaluation procedures. We particularly investigated how the different spatial configurations of the game setups influenced the stages of pupils’ cultural learning, and the levels of engagement and collaboration among them. We conclude the article by discussing the qualities of tangible gamification and its role in facilitating cultural learning. For instance, cultural learning is enhanced by situating heritage artifacts in the experience, and embedding learning in the reward system. Engagement and collaboration among visitors are fostered by creating a sense of ownership and designing a diversity of goals.",Situated Tangible Gamification of Heritage for Supporting Collaborative Learning of Young Museum Visitors,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
990,http://dx.doi.org/10.1145/3350428,"{'string_id': '10.1145/3350428', 'id_scheme': 'DOI'}","With the advent of window glass, its optical properties arise as one important factor in the illumination, and thereby the visual perception, of Roman architecture. Computational simulation allows to reconstruct the daylight illumination of buildings with known geometrical configuration if the scattering properties of window glass can be replicated. We describe a method to generate data-driven models of Roman window glass based on precise gonio-photometric measurements of archaeological finds. The method is employed to model four exemplary glass finds in the context of a simplified architectural scene, demonstrating the potential to support research in building history and archaeology focused on building techniques, lighting concepts, and perception with accurate and reliable photometric data.",Data-Driven Modelling of Daylight Scattering by Roman Window Glass,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
991,http://dx.doi.org/10.1145/3352590,"{'string_id': '10.1145/3352590', 'id_scheme': 'DOI'}","An immersive experience brought about by virtual reality can potentially enhance the appreciation of classical Chinese poetry, which is difficult to describe clearly in everyday language or ordinary media. However, making 3-dimensional illustrations for a 360-degree display in virtual reality is usually a labor-intensive and time-consuming procedure and hard to master for non-professional media creators, such as teachers. Motivated by the homology theory of classical Chinese poetry and painting, we propose an image-based approach of building 2.5-dimensional immersive stories to visualize classical Chinese poetry. Specifically, using Chinese shadow play as a metaphor, we have designed and implemented ShadowPlay2.5D, a sketch-based authoring tool to help novices create 360-degree videos of classical Chinese poetry easily. To ensure coverage of the diverse themes in Chinese poetry and preserve the sense of culture, we build a Chinese ink-painting style image repository of essential poetic elements identified via crowdsourcing. To facilitate construction of 2.5-dimensional scenes, we design features that support puppet-like animation, instancing, and camera organization in a 3-dimensional environment. Through two user studies, we show that ShadowPlay2.5D can help novices make a short 360-degree video in about 10--15 minutes, and the 2.5D stylized illustrations created can bring about a better immersive experience for poetry appreciation.",ShadowPlay2.5D,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
992,http://dx.doi.org/10.1145/3352933,"{'string_id': '10.1145/3352933', 'id_scheme': 'DOI'}","This article attempts to understand how present Virtual Reality (VR) environments can contribute to enhancing the communication of cultural heritage by providing an experience of the past that is acceptable for the younger generation and how museums and cultural institutions should adopt and use such technologies. Aspects of acceptance, experience, and expectation of VR with the underlying values are not well understood but are important for the sustainability of the communication of cultural heritage as a bequest to future generations. We conducted a combined quantitative--qualitative study on the participants who have various prior experience with gaming and VR, and different levels of knowledge on the history presented within the virtual environment. This study investigates how participants accept and are stimulated in terms of personal experience and their expectations and ideas for the future of museums if VR is used for enhancing the learning of cultural heritage. Prior gaming and VR experience were investigated to see whether they do indeed influence the preference for using VR for learning cultural heritage. We demonstrated that particular age groups and background are especially agreeable to virtual reality as environments for learning and experiencing cultural heritage, regardless of their knowledge of the historical context of the virtually reconstructed site. Our findings also revealed important behaviours in our demographics group with regards to user preferred length of time and the believability of the virtual environment and how it influences aspects of their experience such as the exploration of the heritage site, familiarity, and meaning making. The study has implications for the use of VR for enhancing the experience of cultural heritage in museums and cultural institutions.","The Effects of VR Environments on the Acceptance, Experience, and Expectations of Cultural Heritage Learning",Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
993,http://dx.doi.org/10.1145/3354002,"{'string_id': '10.1145/3354002', 'id_scheme': 'DOI'}","For more than a decade, User eXperience (UX) has grown into a core concept of Human--Computer Interaction (HCI) and has been widely disseminated and accepted in the HCI community. At the same time, Cultural Heritage (CH) has been a favored domain for UX research, while the visitor is able to exploit CH material before, during, and after the visit, having different goals and requirements in each phase. Thus, CH organisations need to carefully explore the potential of investing in UX, as visitor satisfaction and intention to return and spread positive word-of-mouth is closely linked to the destination's overall success. A multitude of methods for UX analysis exist, but a clear overview of the current state of the available UX research methods in CH is missing. Recent studies [Othman 2012; Roussou 2018] have successfully developed frameworks to measure UX in cultural spaces, but most of them still lack understanding about how visitors interact with new technologies and simultaneously with the exhibits. In this article, a survey that describes the conceptual frameworks, models, research methodologies, and paradigms of incorporating Cultural User eXperience (CUX) in applications is reported, as well as the overall interaction between UX and CH and the related aspects that are influenced by the CH application domain and the parameters leading to optimization of the CUX. The outcome of this survey lays ground for understanding and defining the concept of UX research in CH, while current challenges and issues for future work are discussed.",Adding Culture to UX,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,1,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
994,http://dx.doi.org/10.1145/3355395,"{'string_id': '10.1145/3355395', 'id_scheme': 'DOI'}","Digital technology in museum practice provides new means of interaction with artifacts and collections. In particular, we need interactive installations in order to encourage and stimulate visitors to learn and understand archaeological musical instruments through engagement and active participation: these instruments (i.e., interactive artifacts per se) are de facto unplayable and inaccessible to visitors, as a consequence of their preservation issues. However, presenting artifacts to the general public is a complex task for their multifaceted nature, and digital technology must not sacrifice accuracy or depth of information for the sake of entertainment. Moreover, deploying digital technology is a multidisciplinary effort that requires an interplay among different fields, from history and archaeology to information engineering and craftsmanship. In this article, we present a methodology to relate such disciplines in order to design a digital multimedia installation that promotes archaeological musical instruments in a museum. In defining the problem, we identify four different aspects to consider: the museum collection, the museum environment, the manufacturing opportunities for the installation, and the user experience. Such aspects are integrated in a design approach that is centered on Design Thinking. The proposed methodology is exemplified in the designing and manufacturing of an installation for a Pan flute from Egypt dated back to 700 A.D., a case in which multisensory interaction is particularly important to convey the lost sound of the instrument. We describe in detail an installation (exhibited at the Museum of Archaeological Science and Art at the University of Padova), which virtually recreates the Pan flute and communicates information related to its history, iconography, acoustics, and musicology. Just after the deployment of the installation, we also carried out an assessment with a group of experts in the fields of information engineering, music, musicology, and archaeology. The good results obtained demonstrate that the installation is a convenient way of interaction, simple to use and aesthetically integrated in the museum context.",Technology-Enhanced Interaction with Cultural Heritage,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
995,http://dx.doi.org/10.1145/3359590,"{'string_id': '10.1145/3359590', 'id_scheme': 'DOI'}","The visualisation of historical information and storytelling in museums is a crucial process for transferring knowledge by directly and simplistically engaging the museum audience. Until recently, technological limitations meant museums were limited to 2D and 3D screen-based information displays. However, advancements in Mixed Reality (MR) devices permit the propagation of a virtual overlay that amalgamates both real-world and virtual environments into a single spectrum. These holographical devices project a 3D space around the user which can be augmented with virtual artefacts, thus potentially changing the traditional museum visitor experience. Few research studies focus on utilising this virtual space to generate objects that do not visually inhibit or distract the operator. Therefore, this article aims to introduce the Ambient Information Visualisation Concept (AIVC) as a new form of storytelling, which can enhance the communication and interactivity between museum visitors and exhibits by measuring and sustaining an optimum spatial environment around the user. Furthermore, this article investigates the perceptual influences of AIVC on the users’ level of engagement in the museum. This article utilises the Microsoft HoloLens, which is one of the most cutting-edge imagining technologies available to date, in order to deploy the AIVC in a historical storytelling scene “The Battle” in the Egyptian department at The Manchester Museum. This research further seeks to measure the user acceptance of the MR prototype by adopting the Technology Acceptance Model (TAM). The operational approaches investigated in this study include personal innovativeness (PI), enjoyment (ENJ), usefulness (USF), ease of use (EOU), and willingness of future use (WFU). The population sampling methodology utilised 47 participants from the museum's daily visitors. Results of this research indicate that the WFU construct is the primary outcome of this study, followed by the usefulness factor. Further findings conclude that the majority of users found this technology highly engaging and easy to use. The combination of the proposed system and AIVC in museum storytelling has extensive applications in museums, galleries, and cultural heritage places to enhance the visitor experience.",Ambient Information Visualisation and Visitors’ Technology Acceptance of Mixed Reality in Museums,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
996,http://dx.doi.org/10.1145/3372421,"{'string_id': '10.1145/3372421', 'id_scheme': 'DOI'}","Research on seismic simulations has focused mainly on methodologies specially tailored to civil engineering. However, we have detected a lack in the area of interactive cultural heritage applications, where speed and plausibility are the main requirements to satisfy. We designed a tool that allows setting up and recreating earthquakes in a simple way. We coupled our earthquake simulator with a structural simulator of physics, specifically tailored to masonry buildings, achieving a high degree of accuracy in the simulations. To validate our model, we performed a series of tests over a set of ancient masonry structures such as walls and churches. We show the feasibility of including earthquake simulations and structural vulnerability, a building property that limits the damage of this under seismic movements, into historical studies for helping professionals understand those events of the past where an earthquake took place.",Earthquake Simulation on Ancient Masonry Buildings,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
997,http://dx.doi.org/10.1145/3373357,"{'string_id': '10.1145/3373357', 'id_scheme': 'DOI'}","In this article, a method is proposed to accurately overlap multiple high-quality images with different shooting positions and intervals by combining corresponding point information between images and 3D shape information. In the proposed method, the correct feature matching of images obtained by rendering the 3D model of the subject is used. In this research, the subjects were the pillars of the Angkor Thom Bayon Temple and the epilithic microorganisms adhering to and eroding their surfaces. Synthetic transformation of a homography utilizing the correct matches is employed to overlap the target images. When overlapping different background images using the conventional method, estimation of the homography matrix becomes difficult owing to incorrect matches; thus, high-quality overlapping images cannot be obtained. The objective of this research was to realize the overlapping image process even under conditions that cause incorrect feature matches, such as differences in background and illumination. In this study, the damaged parts of the pillar surfaces were visualized using the proposed method. Based on the results of an experiment comparing the proposed method with an existing state-of-the-art method, the effectiveness and higher accuracy of the proposed method were verified. The findings of this research contribute to Angkor Thom Bayon Temple preservation, including a preservation project that the present authors are organizing for this culturally and historically valuable United Nations Educational, Scientific and Cultural Organization World Heritage site.",Accurate Overlapping Method of Ultra-Long Interval Time-Lapse Images for World Heritage Site Investigation,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
998,http://dx.doi.org/10.1145/3376120,"{'string_id': '10.1145/3376120', 'id_scheme': 'DOI'}","Being a world-renowned heritage site, the Ming Great Wall, located in Qinghai Province, China, mainly has existed as the form of earthen sites with high historical, artistic, and scientific values. However, exposed under environmental impacts for about 500 years, these sites have been seriously threatened. The study on their damage assessment using reasonable methods is the key premise for further protection work. As there are few studies focused on the damage assessment of earthen sites, especially by using machine learning methods, this study explored to apply the current effective machine learning approaches, namely Support Vector Machine (SVM) and back-propagation (BP) Neural Network, into solving this problem. The authors used two such algorithms by training and testing the existing data of damage assessment results of 18 earthen sites located in Qinghai Province. By comparing experimental results, the prediction effects of SVM are much better than those of BP Neural Network. To test its practicability, Liutun Great Wall was chosen as an engineering case, showing better prediction and reasonable performance of SVM. This research has proved that SVM can be selected as a suitable model to assess the damage levels of earthen sites located in Qinghai Province for their future conservation works.",Damage Assessment of Earthen Sites of the Ming Great Wall in Qinghai Province,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
999,http://dx.doi.org/10.1145/3376925,"{'string_id': '10.1145/3376925', 'id_scheme': 'DOI'}","The increase of multidisciplinary research in the field of architectural history has led to the need to set up new experiences and solutions for the handling and integration of the information extracted from historical documents. These solutions seek to support diverse users of the research community with the aim of solving challenges directly related with the digitalisation, structuring, standardisation, and management of historical information. These challenges include, for example, the creation of a digital support that enables a collaborative growth and management of information, the normalisation of terms and vocabularies to make its analysis efficient, the elaborating of a conceptual model, and the development of a metadata support that allows its more expanded dissemination and reuse. This article describes a case study project in which the documents of archives, of research, and of projects previously carried out by the Late Gothic Network (Red Tardogótica) are the raw material for the proposal of an event-oriented historical database (e-database). This e-database means to record and systematise the information about the artistic transfers related with the architectural production in the transition of the Modern Age, a period also known as the Late Gothic. The e-database's design has considered the possibility of its use for the analysis of social networks (abstract-relational model, a Graph model) and the spatiotemporal analysis of the events (geo-temporal model, GIS). The main section of this article describes the architecture of the database, with a view to addressing the questions of the relations between the datasets and the matter of implementing the thesauri and controlled vocabularies that must be respected for the standardisation, recording, and later analysis of the data. Next, we contribute quantitative and qualitative analyses to evaluate the database's important gaps of information. This proposal initially covers the geographical framework of the western Andalusian territory, but it can be expanded to other areas and adapted to other case studies. Finally, the article summarises the learning achieved in this first phase of the case study project and describes the perspectives to broaden its use in the community of architectural history researchers.",A Data-driven Approach for Architectural History Knowledge. Capturing Buildings’ Construction Events for Historical Research Collaboration,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,2,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
1000,http://dx.doi.org/10.1145/3382771,"{'string_id': '10.1145/3382771', 'id_scheme': 'DOI'}","Cultural Heritage can use ICT and game design to provide compelling experiences of visiting ancient ruins. In fact, archaeological sites benefit from being enriched with additional meanings that help visitors to contextualize what they see. For example, young tourists, when accessing cities of the past, could miss the chance of really appreciating the history and the culture of extinguished civilizations. We propose an interactive story for them through which they can explore an archaeological site. We carefully planned the structure of this game according to four main dimensions: magic circle, narrative, immersion, and agency. Afterward, the prototype was presented as a scenario to five tourist guides, who positively evaluated the game and also gave us some valuable feedback.",An Interactive Narrative to Improve Cultural Heritage Experience in Elementary School Children,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
1001,http://dx.doi.org/10.1145/3382773,"{'string_id': '10.1145/3382773', 'id_scheme': 'DOI'}","Visits to cultural heritage sites are generally social in nature, yet resources to support these sociable experiences are often individualized, catering to the solitary visitor. Digital technologies offer means to disrupt this predicament, encouraging social engagements in cultural contexts. Here we present the results of a user study that systematically investigates the effects of face-to-face group conversation and physical interactions within a digital cultural storytelling experience at the Neolithic site of Çatalhöyük, with the objective of promoting engagement, learning, and perspective taking. Seeking to articulate the benefits and weaknesses of promoting social interactions in digital storytelling settings, we start with a story-based experience that was designed for individual use, we extend it with novel system-driven interaction prompts, and then we evaluate the two versions with 102 participants. Our findings provide statistically significant evidence that conversation is related to longer and more absorbed participation in the experience and greater learning regardless of personality traits. Where social interaction is purposefully integrated into the story, more conversation is generated, and these interactive prompts do not disrupt the story flow even though participants report awareness of them. Finally, certain tendencies among users affect their talkativeness and enjoyment, providing future designers of digital stories with guidance to better address audience needs.",“Let Them Talk!”,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
1002,http://dx.doi.org/10.1145/3383314,"{'string_id': '10.1145/3383314', 'id_scheme': 'DOI'}","This article describes a method for early detection of disaster-related damage to cultural heritage. It is based on data from social media, a timely and large-scale data source that is nevertheless quite noisy. First, we collect images posted on social media that may refer to a cultural heritage site. Then, we automatically categorize these images according to two dimensions: whether they are indeed a photo in which a cultural heritage resource is the main subject, and whether they represent damage. Both categorizations are challenging image classification tasks, given the ambiguity of these visual categories; we tackle both tasks using a convolutional neural network. We test our methodology on a large collection of thousands of images from the web and social media, which exhibit the diversity and noise that is typical of these sources, and contain buildings and other architectural elements, heritage and not-heritage, damaged by disasters as well as intact. Our results show that while the automatic classification is not perfect, it can greatly reduce the manual effort required to find photos of damaged cultural heritage by accurately detecting relevant candidates to be examined by a cultural heritage professional.",Detection of Disaster-Affected Cultural Heritage Sites from Social Media Images Using Deep Learning Techniques,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
1003,http://dx.doi.org/10.1145/3383781,"{'string_id': '10.1145/3383781', 'id_scheme': 'DOI'}","The introduction of digital technologies into documentation methods for cultural heritage and archeology made it possible to develop new tools for the acquisition and management of information collected for multidisciplinary studies. These tools are beginning to assert themselves as supports privileged for the description, analysis and comprehension of the objects of study. SIAT is a first step toward the creation of a set of methods, modern tools of analysis, and research in history and archeology. The objective of this approach is to define a continuum of information between all phases of the process from image acquisition, video, and spatial data to the construction of semantically enriched representations by integrating multi-dimensional aspects, multi-scales, multi-layer, and multi-temporal. This research tries to provide a definition and development of a set of data processing modules that can be used by specialists in the conservation of cultural heritage.",SIAT,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
1004,http://dx.doi.org/10.1145/3383782,"{'string_id': '10.1145/3383782', 'id_scheme': 'DOI'}","This work illustrates the use of three different digitization techniques to study and valorize a 15th-century engraved tablet discovered during a preventive archaeological excavation in the area of a former convent. The tablet is covered with engraved inscriptions on both sides and includes a musical score. Digitization allowed for an advanced analysis of the inscriptions, and to generate a complete and precise 3D model of the artifact that was used to produce an interactive application deployed both on tactile tablets and websites. The interactive application integrates a musical interpretation of the score that gives access to a testimony of intangible heritage. This interdisciplinary work gathered archaeologists, researchers from computer science and physics, and a professional musician.","From the Engraved Tablet to the Digital Tablet, History of a 15th-Century Music Score",Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
1005,http://dx.doi.org/10.1145/3386314,"{'string_id': '10.1145/3386314', 'id_scheme': 'DOI'}","Many historical monuments were originally vividly painted (polychromatic), and researchers have been able to reach consensus on this even for monument classes in which only indirect evidence of painting survives. However, academic caution has led to an understandable reluctance to pass this knowledge to the public through use of reconstructions showing the colour schemes of these monuments. As a result, use of polychromatic reconstruction has been very limited and has made a significant impact on public perception for the classical period, with insignificant progress for many other important monument classes. We argue that large-scale projects to create digital reconstructions are inevitable but that adoption of open workflows is vitally important to achieve the objectives of the London Charter. We demonstrate a practical workflow with two medieval stone crosses.",Open Workflows for Polychromatic Reconstruction of Historical Sculptural Monuments in 3D,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
1006,http://dx.doi.org/10.1145/3386730,"{'string_id': '10.1145/3386730', 'id_scheme': 'DOI'}","Ceramic sherds are the most common finds in archaeology. They are complex to analyze and onerous to process. A large number of indistinct sherds coming from excavations must be preliminarily grouped in some categories. This clusterization helps the next phase, in which archaeologists classify the ceramics. Due to the difficulty of these preliminary, repetitive, and routine phases, a great deal of archaeological material remains unstudied in museum repositories or archaeological sites. An effective method to automate these routine phases is presented in this article. The proposed method performs a shape feature segmentation of the sherds, which is fundamental to undertake any further analysis, such as potsherds classification, reconstruction, or cataloging. A set of specific shape features, useful to understand the find properties, is defined and methods for recognizing them are proposed. The method's performance is tested in the analysis of some real, critical cases.",Automatic Shape Feature Recognition for Ceramic Finds,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
1007,http://dx.doi.org/10.1145/3402440,"{'string_id': '10.1145/3402440', 'id_scheme': 'DOI'}","Historical archives represent an immense wealth, the potential of which is endangered by the lack of effective management and access tools. We believe that this issue can be faced by providing archive catalogs with a semantic layer, containing rich semantic metadata, representing the content of documents in a full-fledged formal machine-readable format. In this article, we present the contribution offered in this direction by the PRiSMHA project, in which the conceptual vocabulary of the semantic layer is represented by computational ontologies. However, acquiring semantic knowledge represents a well-known bottleneck for knowledge-based systems; to solve this problem, PRiSMHA relies on a crowdsourcing collaborative model, i.e., an online community of users who collaborate in building semantic representations of the content of archival documents. In this perspective, this article aims at answering the following research question: Starting from the axioms characterizing concepts in the computational ontology underlying the system, how can we derive a user interface enabling users to formally represent the content of archival documents by exploiting the conceptual vocabulary provided by the ontology? Our solution includes the following steps: (a) a manually defined configuration, acting as a pre-filter, to hide “unsuited” classes, properties, and relations; (b) an algorithm, combining heuristics and reasoning, which extracts from the ontology all and only the “compatible” properties and relations, given an entity (event) type; and (c) a set of strategies to rank, group, and present the entity (event) properties and relations, based on the results of a study with users. This integrated solution enabled us to design an ontology-driven user interface enabling users to characterize entities, and in particular (historical) events, on the basis of the vocabulary provided by the ontology.",Building Semantic Metadata for Historical Archives through an Ontology-driven User Interface,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
1008,http://dx.doi.org/10.1145/3402443,"{'string_id': '10.1145/3402443', 'id_scheme': 'DOI'}","We investigate the overlapping of the concepts of prestige and success in art. To this end, we invited a group of art experts and a group of artists to select a small number of artworks that they deemed of high quality among those the crypto art gallery SuperRare displays. We then matched the selections with indicators of market success for the same artworks. We find that prestigious artworks selected by art experts and artists are also successful in the gallery marketplace, tracing an interesting link between prestige and success in the art context. We also observe a clear divergence between the roles of art expert and art maker (artist).",Art for Space,Association for Computing Machinery (ACM),2020,,Journal on Computing and Cultural Heritage,13,3,"[{'value': '1556-4673', 'type': 'print'}, {'value': '1556-4711', 'type': 'electronic'}]"
1009,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/21631,"{'string_id': '10.11588/dah.2015.1.21631', 'id_scheme': 'DOI'}","I present a number of core concepts from data science that are relevant to digital art history and the use of quantitative methods to study any cultural artifacts or processes in general. These concepts are objects, features, data, feature space, and dimension reduction. These concepts enable computational exploration of both large and small visual cultural data. We can analyze relations between works on a single artist, many artists, all digitized production from a whole historical period, holdings in museum collections, collection metadata, or writings about art. The same concepts allow us to study contemporary vernacular visual media using massive social media content. (In our lab, we analyzed works by van Gogh, Mondrian, and Rothko, 6000 paintings by French Impressionists, 20,000 photographs from MoMA photography collection, one million manga pages from manga books, one million artworks of contemporary non-professional artists, and over 13 million Instagram images from 16 global cities.) While data science techniques do not replace other art historical methods, they allow us to see familiar art historical material in new ways, and also to study contemporary digital visual culture.",Data Science and Digital Art History,,2015,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1010,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/21633,"{'string_id': '10.11588/dah.2015.1.21633', 'id_scheme': 'DOI'}","The past five years have witnessed a growing interest amongst art historians in the potential of digital projects to impact, if not transform, the discipline. A steep rise in conferences and institutes dedicated to digital art history, along with funding opportunities and institutional support, has accelerated the rate at which art historians are now engaging with digital techniques. With this new visibility, art historians have criticized themselves for lagging behind other disciplines such as history and archaeology. This article questions the assumption that art historians have been slow to embrace digital tools and methods through a brief historical examination of projects undertaken by institutions and scholars during the infancy of art history computing: the early 1980s through the early 1990s. Using Johanna Drucker's distinction of the ""digitized"" and ""digital"" iterations of art history, this essay traces the genealogies of both categories, arguing that scholars have been more active in theorizing, practicing and creating digital methods than is often seen to be the case. Ultimately, this essay is an attempt to help define from a historical perspective what ""digital art history"" is and how it has been practiced.",Forgotten Genealogies: Brief Reflections on the History of Digital Art History,,2015,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1011,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/21634,"{'string_id': '10.11588/dah.2015.1.21634', 'id_scheme': 'DOI'}","This paper offers a few reflections on the origins, historiography and condition of the field often referred to as Digital Art History (DAH), with references, among others, to the activities of the Computers and the History of Art group (CHArt, est. 1985) and my personal experience, spanning over 20 years, first as a postgraduate student, then doctoral researcher and eventually Lecturer in DAH. The publications and teaching activities of scholars connected to CHArt are seen as indicative of the evolution of the field internationally. Personal experience, or a reality check, is limited to higher education in the UK. The key argument here concerns the questionable benefit of promoting DAH as a discrete discipline and detaching digital practices from the mainstream history of art and its institutions. When introduced in the late 1990s, the ‘DAH’ served to indicate a dramatic shift in the way art history could be practiced, taught, studied and communicated. The changes were brought about by widening access to computers and information technology. DAH was suggested—“perhaps a little ahead of time—as a new kind of intellectual fusion” (W. Vaughan). It is no longer necessary to argue for the wise use of computers. Digital technology has become part and parcel of teaching, learning and research. It is the History of Art and its more traditional research methods and critical perspectives that are seen at risk of neglect. The theories of crisis, even ‘death’ of Art History have contributed to general anxiety over the discipline’s future. However, a discipline has “the ability and power to control and judge its borders” (R. Nelson). The discipline of Art History is richer and stronger through the fusion of digital scholarship with, not separation, from more traditional methodologies and critical canons. The need to continue with the ‘digital’ distinction is questionable.",Debating Digital Art History,,2015,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1012,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/21638,"{'string_id': '10.11588/dah.2015.1.21638', 'id_scheme': 'DOI'}","Taking into account the call of this journal to examine the epistemological and methodological assumptions in the field of art history on the verge of its digital turn, the aim of this essay is to contribute to the ongoing discussion by questioning the role of the framing device in the context of image appropriation and critical interpretation of visual documents. Focusing on the cognitive and structural potential of the frame, a common feature between analogue and digital art historical practice, we try to provide points of historical perspective through a selection of particular examples (Giorgio Vasari, Gustav Ludwig and Aby Warburg) and bring them closer to the notions of instrumentation and interface.",Reframing Art History,,2015,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1013,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/21639,"{'string_id': '10.11588/dah.2015.1.21639', 'id_scheme': 'DOI'}","With reference to the concept of distant reading in literary history, distant viewing is a valuable analogy for a quantitative approach to art history. In this case study of artistic productivity eight samples are analyzed, extracted from a digital thematic research collection about the iconography of Aphrodite/Venus from the Middle Ages to Modern Times. The result is an empirical finding of regularity never before highlighted in art history. The artistic productivity fits perfectly the distribution known as Lotka’s law of scientific productivity in bibliographic science. Issues of collecting and sampling are discussed and the meaning of this empirical finding is hinted. Suggestions for future research are made.",Distant Viewing in Art History. A Case Study of Artistic Productivity,,2015,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1014,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/21640,"{'string_id': '10.11588/dah.2015.1.21640', 'id_scheme': 'DOI'}","Has human beauty always been perceived in the same manner? We used a set of 120,000 paintings from different periods to analyze human faces between the 13th and the 20th centuries in order to establish whether there has been a single canon of beauty (that would maximize reproduction probabilities) or whether this has changed over time. Our study shows that when measuring averageness, symmetry, and orientation, the representation of human faces has not remained constant and that there are substantial differences between the faces depicted between the 15th and 18th centuries when compared to those of both the 13th and 20th centuries. Especially significant is the decrease in the perceived beauty of faces in 20th-century paintings, as the freedom of artists and the openness of society fostered the representation of different types of human faces other than that of classical styles.",A Quantitative Approach to Beauty. Perceived Attractiveness of Human Faces in World Painting,,2015,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1015,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/23250,"{'string_id': '10.11588/dah.2016.2.23250', 'id_scheme': 'DOI'}","GLAM institutions all over the world are digitizing their collections. As the number of items in such a collection amounts to tens or even hundreds of thousands, providing comprehensible access and presentation becomes increasingly difficult. At the same time, a steadily growing amount of this data is openly available. This gives rise to various projects approaching the hidden treasures in these collections with computational tools. The project presented here, Gugelmann Galaxy, lets the user explore an entire collection of digitized images and their textual metadata in an immersive three-dimensional cloud, whose configuration can be rearranged according to different criteria. The project questions traditional models of categorization and curating and implements alternative approaches prototypically.",Gugelmann Galaxy: An Unexpected Journey through a collection of Schweizer Kleinmeister,,2016,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1016,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/23376,"{'string_id': '10.11588/dah.2016.2.23376', 'id_scheme': 'DOI'}","In the past few years, the number of fine-art collections that are dig- itized and publicly available has been growing rapidly. With the availability of such large collections of digitized artworks comes the need to develop multime- dia systems to archive and retrieve this pool of data. Measuring the visual similar- ity between artistic items is an essential step for such multimedia systems, which can benefit more high-level multimedia tasks. In order to model this similarity between paintings, we should extract the appropriate visual features for paintings and find out the best approach to learn the similarity metric based on these fea- tures. We investigate a comprehensive list of visual features and metric learning approaches to learn an optimized similarity measure between paintings. We de- velop a machine that is able to make aesthetic-related semantic-level judgments, such as predicting a painting’s style, genre, and artist, as well as providing simi- larity measures optimized based on the knowledge available in the domain of art historical interpretation. Our experiments show the value of using this similarity measure for the aforementioned prediction tasks.",Large-scale Classification of Fine-Art Paintings: Learning The Right Metric on The Right Feature,,2016,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1017,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/23489,"{'string_id': '10.11588/dah.2016.2.23489', 'id_scheme': 'DOI'}","In this paper, by extending the methodology of media archaeology to the praxis of Cultural Analytics/Media Visualization I ask how have we compared multitude of diverse images and what can we learn about the narratives that these comparisons allow? I turn to the work of Aby Warburg who attempted to organize close to two thousand images in his Mnemosyne Atlas. In comparing contemporary methods of image data visualization through cultural analytics method of remapping and the turn of the century methodology developed by Warburg under the working title of the “iconology of intervals,” I examine the shifts and continuities that have shaped informational aesthetics as well as data-driven narratives. Furthermore, in drawing parallels between contemporary Cultural Analytics/Media Visualization techniques, and Aby Warburg’s Atlas, I argue that contextual and image color data knowledge should continue to be important for digital art history. More specifically, I take the case study of Warburg’s Panel 45 in order to explore what we can learn through different visualization techniques about the role of color in the representation of violence and the promise of prosperous civil society.",Images as Data: Cultural Analytics and Aby Warburg’s Mnemosyne,,2016,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1018,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/23944,"{'string_id': '10.11588/dah.2016.2.23944', 'id_scheme': 'DOI'}","Art Historians who wish to engage with the potential of digital tools for research and narratives about works of art, architecture and cities often have difficulties identifying appropriate software and receiving basic training. To address this challenge, the Wired! group at Duke University began to offer workshops in 2009, and since 2012 these have been taught on site at Venice International University, which created a laboratory for this purpose. Our goal has been to assist scholars in learning basic skills not only to create maps, models, and displays that narrate research questions, but also to acquire enough knowledge to join in effective and directed partnerships with computer scientists and engineers. In 2015, the Getty Foundation covered the costs of travel and living expenses for the faculty and participants, while the Delmas Foundation has helped since 2012 with the costs of tuition.",Graduate Workshop on Digital Tools for Art Historians: The Visualizing Venice Summer Program “The Biennale and the City” (2015),,2016,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1019,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/24761,"{'string_id': '10.11588/dah.2016.2.24761', 'id_scheme': 'DOI'}","World population and the number of cultural artifacts are growing exponentially or faster, while cultural interaction approaches the fidelity of a global nervous system. Every day hundreds of millions of images are loaded into social networks by users all over the world. As this myriad of new artifacts veils the view into the past, like city lights covering the night sky, it is easy to forget that there is more than one Starry Night, the painting by Van Gogh.Like in ecology, where saving rare species may help us in treating disease, art and architectural history can reveal insights into the past, which may hold keys to our own future. With humanism under threat, facing the challenge of understanding the structure and dynamics of art and culture, both qualitatively and quantitatively, is more crucial now than it ever was. The purpose of this article is to provide perspective in the aim of figuring out the process of art history – not art history as a discipline, but the actual history of all made things, in the spirit of George Kubler and Marcel Duchamp. In other words, this article deals with the grand challenge of developing a systematic science of art and culture, no matter what, and no matter how.",Figuring out Art History,,2016,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1020,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/25337,"{'string_id': '10.11588/dah.2016.2.25337', 'id_scheme': 'DOI'}","The development of a professionalized, highly centralized printmaking industry in northern Europe during the mid-sixteenth century has been argued to be the inevitable result of prints' efficacy at reproducing images, and thus encouraging mass production. However, it is unclear whether such a centralized structure was truly inevitable, and if it persisted through the seventeenth century. This paper uses network analysis to infer these historical print production networks from two large databases of existing prints in order to characterize whether and how centralization of printmaking networks changed over the course of this period, and how these changes may have influenced individual printmakers.","Social Network Centralization Dynamics in Print Production in the Low Countries, 1550-1750",,2016,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1021,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/33527,"{'string_id': '10.11588/dah.2016.2.33527', 'id_scheme': 'DOI'}","The use of the computer in Art History is changing the approach towards our objects of research. Now, we are able to compute more images than a human can see in a lifetime. That, in turn, calls for a new definition of the role of the researcher and the tools being used. The access to large amounts of visual data stands in a tradition of conventional methods of Art History, but also augments them with quantity. This article proposes a theoretical model on which to build an understanding of the meta image with which we interactively derive our conclusions.",Big Image Data within the Big Picture of Art History,,2016,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1022,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/33529,"{'string_id': '10.11588/dah.2016.2.33529', 'id_scheme': 'DOI'}","The following is a description of two novel techniques for the direct visualization of image data. Direct visualizations of image data make use of the images in their original visible format. The first technique, the slice histogram, arranges slices of images as histograms, organized by both visual and non-visual variables. The second technique, the growing entourage plot, organizes high-dimensional clusters of images on a 2D canvas by projection. Both techniques are designed for exploratory analysis of image datasets.",Direct visualization techniques for the analysis of image data: the slice histogram and the growing entourage plot,,2016,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1023,http://journals.ub.uni-heidelberg.de/index.php/dah/article/view/33530,"{'string_id': '10.11588/dah.2016.2.33530', 'id_scheme': 'DOI'}",In this article we present a case study on digital representation of the art historical research and metadata brought together for a scientific collection catalogue by the Prussian Palaces and Gardens Foundation Berlin-Brandenburg. The resulting interface aims at linking the structure and texture of a collection of drawings by Frederick William IV of Prussia (1795–1861) with additional contextual information. The article describes the context of the larger research project and presents the resulting visualization and interaction techniques specifically designed for dynamic exploration along time and subjects.,"Linking structure, texture and context in a visualization of historical drawings by Frederick William IV (1795-1861)",,2016,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1024,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/32473,"{'string_id': '10.11588/dah.2018.3.32473', 'id_scheme': 'DOI'}","Digital 3D reconstruction methods have been widely applied to support research and the presentation of historical objects since the 1980s. Whereas 3D reconstruction has been incorporated into a multitude of research applications, essential methodological foundations for more widespread utilisation of digital reconstructions have yet to be developed. Against this background, the aim of this article is to consider the question as to how the methodology of 3D reconstruction alters research cultures in architectural and art history by exemplifying three problem areas, (1) research functions of 3D reconstructions and their drawback to a current research culture in art history, (2) consequences of cross-disciplinary project-based teamwork as main cooperation format within 3D reconstruction projects, and (3) problems and difficulties caused by imagery as primary media for research and communication.",3D Reconstruction Techniques as a Cultural Shift in Art History?,,2018,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1025,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/32544,"{'string_id': '10.11588/dah.2018.3.32544', 'id_scheme': 'DOI'}","Starting in 2003 the Department of Architecture of the University of Leuven (KU Leuven) has conducted digital reconstructions as subject of several master’s dissertations. Over the years different topics have been the subject of study, ranging from Burgundian residences to lost religious heritage, thus addressing a range of methodological difficulties specific to dealing with historic architecture using modern technologies. As Historic Building Information Modeling (HBIM) has found its way to a broader audience over the last few years, attention must be paid to the methodology and the communication of these reconstructions towards a wider audience. Using these new technologies inevitably changes the perspective of the viewer, shifting from a distant observer to a close inspector (keeping in mind that many of the reconstructed elements were never supposed to be seen up close). Together with these new approaches new means of communication and visualization need to be realized, fitting the possibilities of the reconstructions.",(Re-)Creating the past: 10 years of digital historical reconstructions using BIM,,2018,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1026,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/32703,"{'string_id': '10.11588/dah.2018.3.32703', 'id_scheme': 'DOI'}","Uncertainty is ubiquitous in everyday life and especially domains like engineering and industry, risk management or financial markets are increasingly aware of the far-reaching impact of uncertain data. Well-established research fields like the natural sciences are ever since concerned with the intangible phenomenon of uncertainty, as well as several disciplines like geography, information visualization, or any field concerned with the past. In this context, a range of disciplinary approaches were surveyed with regard to methods and techniques developed and applied to deal with uncertainty. As a result thereof various efforts were made to consider suitable taxonomies, quantification methods and visualization strategies. Emphasize is particularly laid on archaeology, highlighting the three-dimensional digital modeling and reconstruction activities and correlated archaeological discourse of the last two decades. Important for all disciplines however is the highly complex interrelation between uncertainty and visualization for the appropriate and comprehensible representation of gained results. Therefore, a range of visualization strategies is introduced, such as non-photorealistic rendering techniques, visual cues or several means for (re)contextualization, which evolved from disciplinary discourse in recent years. Also, some general visualization problems are mentioned. The final thoughts are meant to be somehow encouraging to consciously and creatively utilize uncertainty for future work in 3D modeling/reconstruction projects.",Ubiquitous uncertainty: From a discomforting awareness towards a versatile potential.,,2018,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1027,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/34297,"{'string_id': '10.11588/dah.2018.3.34297', 'id_scheme': 'DOI'}","Sustained dialogue and collaborative work between art historians and technologists has a great deal to offer both fields of inquiry. In this paper, we propose that effective collaborations in Digital Art History, however, require more than just a humanist and a technologist to succeed. Indeed, we find that there are four different roles that need to be filled: Humanist, Technologist, Data Steward, and Catalyst. Our approach is predicated on a few foundational convictions. First, we believe that art historians and technologists occupy distinct problem spaces. As we will outline, although these realms are distinct they are not of necessity in opposition to one another. Second, we bring to the fore essential questions about the status and function of data that must be addressed by the collaborators: what sort of data are being used? What counts as effective and compelling analysis of this data? Third, we recognize that there are certain structural impediments to collaboration, such as different reward structures and motivations. Finally, we assert that each of the participants must have a deep commitment to their particular engagement with the project, which requires sustained effort and the maintenance of disciplinary respect. We firmly believe that the most effective of these projects will not be based on technological solutionism, but rather will be founded in the most humanistic of tools: empathy and respect.",A Role-Based Model for Successful Collaboration in Digital Art History,,2018,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1028,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/47287,"{'string_id': '10.11588/dah.2018.3.47287', 'id_scheme': 'DOI'}","This essay explores the parallel rising of computer vision technology and digital art history, examining some of the current possibilities and limits of computational techniques applied to the cultural and historical studies of images. A fracture emerges: computer scientists seem to lack in the critical approach typical of the humanities, a shortfall which sometimes condemns their attempts to remain technological curiosities. For their part, humanists lack in technical knowledge that is needed to directly investigate big archives of images, with the result that art historians often must limit their attempts in the computer-aided inquires on texts or metadata databases, a task that does not imply the study of the images themselves. A future dialogue between the two areas is claimed as a necessity to foster this new branch of knowledge.",Computational Imagination and Digital Art History,,2018,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1029,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/49913,"{'string_id': '10.11588/dah.2018.3.49913', 'id_scheme': 'DOI'}","As data storage, computational processing power, and retrieval costs diminish, many traditional technologies of data-compression are becoming obsolete. This unprecedented state of data opulence, where more and more data are expected to be always more easily available at ever decreasing costs, is bringing about significant changes in contemporary computation, and fostering a revival of Artificial Intelligence technologies that were seen until recently as of limited practical use. A similar techno-cultural disruption is already conspicuously affecting architectural design. Informational models in three dimensions are replacing the basic tools of the designer’s trade since the Renaissance – scaled drawings in plans, elevations, and sections. Furthermore, Big Data and computation allow digital designers to compose and engage with the messiness of some natural processes without going through the traditional mediation of abstract and general mathematical theories and patterns. Just like computation is replacing the causal laws of modern science with the brute force of data-driven simulation and optimization, blunt information retrieval is increasingly, albeit often subliminally, replacing causality-driven, teleological historiography, and demoting all modern and traditional tools of story-building and story-telling. This major anthropological upheaval challenges our ancestral dependance on shared master-narratives of our cultures and histories.",Big Data and the End of History,,2018,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1030,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/49914,"{'string_id': '10.11588/dah.2018.3.49914', 'id_scheme': 'DOI'}","CyArk is a California-based nonprofit dedicated to digitally documenting and preserving world heritage. Since 2003, they have used photogrammetry and laserscanning to capture 3D data for over 200 sites; most recently, they have partnered with Google Arts & Culture to create an open-access platform for these sites . Here, two members of the CyArk team, John Ristevski (Chairman and CEO) and Elizabeth Lee (Vice President of Programs and Development) sit down with Justin Underhill to discuss the past and present of digital cultural heritage.",In Conversation with CyArk: Digital Heritage in the 21st Century,,2018,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1031,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/49915,"{'string_id': '10.11588/dah.2018.3.49915', 'id_scheme': 'DOI'}","This article responds to two issues affecting the field of contemporary art history: digital technology and the so-called computational turn in the humanities. It is divided into two parts: the first connects problems with “digital art history,” an offspring of digital humanities, to neoliberal metrics; the second suggests how digital art history’s“distant reading” might nevertheless be deployed critically in the analysis of contemporary art.",Against Digital Art History,,2018,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1032,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/49916,"{'string_id': '10.11588/dah.2018.3.49916', 'id_scheme': 'DOI'}","The History of Art is in the midst of its own big bang. Amit Sood, the head of the Google Cultural Institute and Art Project, announced such in February 2016 at the Innovation Conference TED in Vancouver. Through its “Arts & Culture” program, Google already offers virtual tours of over one thousand museums and cultural institutions throughout the world, and also provides access to more than six million high-resolution digitized works of art (it should be noted that these figures continue to rise steeply).",Big Bang Art History,,2018,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1033,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/52672,"{'string_id': '10.11588/dah.2019.4.52672', 'id_scheme': 'DOI'}","As art museums undergo a digital transformation, institutions rethink physical exhibitions and print publications to create online resources which expand or replicate their traditional functions. Collection websites, online exhibitions, online publications, exhibition websites, and online exhibition catalogues coexist with interactive features which cannot be easily categorized in the previous typologies, the exhibition and the publication. Moreover, often these different types of online resources share characteristics and functions. This article intends to define two of the most relevant online resources typologies in art museums, the online exhibition and the online publication. The aim of it is to discuss and understand the importance of rethinking traditional typologies in the digital age. If typologies are necessary is because they help us to advance previous models. Both the definition and discussion are built upon the perspectives of art museums practitioners and a scholarly audience collected through interviews. The viewpoints of the two collectives help us understand existing conventions, preferences, and needs with regards to online exhibitions and publications in art museums.",Defining online resources typologies in art museums: online exhibitions and publications,,2020,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1034,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/52795,"{'string_id': '10.11588/dah.2019.4.52795', 'id_scheme': 'DOI'}","The technological progress of the past decades has had a transformative effect on both cultural institutions and academic research. It is generally accepted that mass digitization projects led by museums, libraries and archives have allowed institutions to reach new audiences and increase the impact of their collections, while the emergence of digital libraries and other types of digital resources has opened up new opportunities for scholars in terms of accessing diverse types of information. Yet, our knowledge of the impact of these resources on the scholarly workflow beyond the stage of discovery remains limited; this paper argues for the importance of understanding user behavior and needs for building digital resources that have a positive effect on the whole scholarly workflow. By employing an ethnographic approach to the study of art historians’ habits we get a detailed view of the effect that digitization and digital resource design can have on scholars’ work, from the seeking of the information to the construction of the research argument. The complex information behavior of art historians and the challenges they often face when interacting with digital resources make them a great example to demonstrate the impact that these can have on the research process.",The impact of digitization and digital resource design on the scholarly workflow in art history,,2020,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1035,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/66410,"{'string_id': '10.11588/dah.2019.4.66410', 'id_scheme': 'DOI'}","This speculative fiction narrates the experience of a virtual museum environment in the near future. The description extends current capabilities of linked data, visualization, and computational analytics while adding features of virtual and augmented reality. The essay takes a skeptical view of the increased spectacularization of cultural memory experience even as it explores the potential for enriched research, pedagogy, and public knowledge that emerging technological platforms may provide.",The Museum Opens,,2019,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1036,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/67697,"{'string_id': '10.11588/dah.2019.4.67697', 'id_scheme': 'DOI'}","This essay explores the ways in which queer critical theory might be productively applied to digital art history and curatorial practice. Reflecting on a recent critically-acclaimed exhibition at the Williams College Museum of Art (WCMA), the author creates a theoretical framework of digital and aesthetic futurity that accords well with a curatorial exercise in visualizing queer affective networks on social media.",Queer criticalities,,2019,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1037,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/72068,"{'string_id': '10.11588/dah.2019.4.72068', 'id_scheme': 'DOI'}","This paper creates a conceptual frame and explanatory point of reference for the collection of papers presented at the exploratory workshop “Data Science for Digital Art History: Tackling Big Data Challenges, Algorithms, and Systems” organized at the KDD 2018 Conference in Data Mining and Knowledge Discovery held in London in August 2018. The goal of the workshop was to probe the field and to build a constructive interdisciplinary dialogue between two research areas: Data Science and Art History. The workshop’s chairs and the authors of this paper share the conviction that Data Science can enrich art studies while analysis of visual data can have a positive impact on Data Science. Thus, the research initiative tried to critically reflect on the interdisciplinary collaboration between diverse research communities and its epistemological and ontological effects.",Method in Interdisciplinary Research,,2020,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1038,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/72070,"{'string_id': '10.11588/dah.2019.4.72070', 'id_scheme': 'DOI'}","The digital medium allows visitors, curators and art historians to gain new insights into their collections through data analysis and rich, interactive visualizations. Motivated by the rise of large-scale cultural heritage collections that have emerged on the Web, we argue that Formal Concept Analysis can be used to highlight the relationships between objects and their features within digital art collections and provide a means for visitors to explore these collections via interactive, narrated pathways. Our work presents four research projects that span 10 years from 2005 - 2015 – ImageSleuth, The Virtual Museum of the Pacific, A Place for Art and a scalability study of Formal Concept Analysis as applied to a data-set from the Brooklyn Museum. Our approach is based on the idea that much of the meaning that can be interpreted from museum collections lies – at least in part – in the way that objects are related to one another. Our work examines how Formal Concept Analysis can drive explorative, narrative-based visitor experiences and reveal new insights into cultural heritage collections.","Navigating Context, Pathways and Relationships in Museum Collections using Formal Concept Analysis",,2020,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1039,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/72071,"{'string_id': '10.11588/dah.2019.4.72071', 'id_scheme': 'DOI'}","Technology is used to compress time and space but at the cost of ‘nearness’. This means it maintains a distance and disjoint between qualitative and quantitative techniques, and therefore between humanists and technology. The knowledge representations that humanists require to investigate a given subject are not the same as those mandated by technologists and database systems more concerned with scale and the efficiency of data processing and retrieval, rather than context and meaning. This perpetuates a humanist perception of information systems as either, useful but ancillary, or problematic. This paper describes an intervention that seeks to combine the qualitative with the quantitative through collaborative research, expressive structured data, and a human-centered and participatory approach to the ‘knowledge graph’. Its design is based on an understanding of the history of historical textual narrative and the benefit of approaching quantitative issues from the bottom up, or qualitatively, incorporating different levels of generalisation, perspectives (different vantage point on reality), and approaches to connections across time and space. A specialist question based on the designs of the artist, Katsushika Hokusai is used as a basis to illustrate how ‘micro’ research questions contribute, in part, to bigger questions and higher quality quantitative analysis.",The problem of distance in digital art history,,2020,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1040,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/73978,"{'string_id': '10.11588/dah.2019.4.73978', 'id_scheme': 'DOI'}","This summary is a short overview of a roundtable discussion that took place at the Renaissance Society of America on the topic of the structure and organization of a Digital Humanities curriculum. I invited two representatives of European and two of US curricula, which were split up respectively into one for Digital Art History and one for general Digital Humanities: Leif Isaksen (Professor of Digital Humanities, Exeter), Peter Bell (Junior professor for Digital Humanities, with a focus on Digital Art History, Erlangen-Nürnberg), Hannah Jacobs (Digital Humanities Specialist in the Wired! Lab for Digital Art & Visual Culture, Duke University), and Ashley Sanders Garcia (Vice Chair of the Digital Humanities Program, UCLA). Both of the European cases are recent implementations of new curricula, whereas the US-American had established courses. While established studies do exist in Europe, as for example at the University of London, they are still quite rare.","Summary of the Roundtable “Setting up a DH Curriculum or Certificate” at the Annual Meeting of the Renaissance Society of America (Toronto, March 19, 2019)",,2020,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1041,https://journals.ub.uni-heidelberg.de/index.php/dah/article/view/75504,"{'string_id': '10.11588/dah.2020.5.75504', 'id_scheme': 'DOI'}","The multifaceted history of digital art has also entailed an evolution of understanding the complex relationships between the material and immaterial in the digital medium. This text traces the histories of digital objects and systems from the 1960s to the so-called ‘post-digital’ era, which finds its artistic expression in works shaped by digital technologies and networks, yet taking the material form of objects such as paintings, sculptures, or photographs. The term neomateriality is used here to capture an objecthood that reflects back the data of humans and the environment, or reveals the way in which digital processes perceive and shape our world. Digital materialities are considered in relation to network cultures and politics, as well as art institutions.",Digital Art Now,,2020,,International Journal for Digital Art History,,,"[{'value': '2363-5401', 'type': 'electronic'}, {'value': '2363-5398', 'type': 'print'}]"
1042,http://dx.doi.org/10.1353/dph.2012.0000,"{'string_id': '10.1353/dph.2012.0000', 'id_scheme': 'DOI'}","This essay explores the ways in which a single tongue can be said to be, in Brunetto Latini’s phrase, at once “delectable” and “common.” It is argued that such titles are suited to medieval texts that trouble the limits of individual languages and, further, that they pertain in exemplary fashion to a restricted but complex corpus of medieval poems that are written in several languages at once.",The Most Delectable of Languages,Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1043,http://dx.doi.org/10.1353/dph.2012.0002,"{'string_id': '10.1353/dph.2012.0002', 'id_scheme': 'DOI'}","In the digital humanities much research has been done concerning stylometry, the computational study of style. Literary authorship at-tribution, especially, has been a central topic. After a brief introduction, I will discuss the enormous potential of this paradigm for medieval philology, a field that studies so many texts of unknown or disputed origin. At the same time, it will be stressed that stylometry’s application to medieval texts is currently not without problems: many attribution techniques are still controversial and do not account for the specific nature of medieval text production. Throughout this paper, I will tentatively apply two well-established attribution techniques (principal components analysis and Burrows’s Delta) to a number of case studies in Middle Dutch studies. These analyses shall be restricted to rhyme words, since these words are less likely to have been altered by scribes.",Stylometry for Medieval Authorship Studies: An Application to Rhyme Words,Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1044,http://dx.doi.org/10.1353/dph.2012.0004,"{'string_id': '10.1353/dph.2012.0004', 'id_scheme': 'DOI'}","Globalization is a “hot topic.” While language figures prominently in debates, its complexity defies descriptions of globalization adequate for other subjects. Increasingly diverse ethnicity in nation states alters the status of once dominant vernaculars—what Dante called the parlar materno—as factors of cultural and social cohesion. Language and religion, rather than nation, now serve as elective markers of identity. Inevitably, however, to assert global status for a language conveys overtones of linguistic, and thus cultural, hegemony. From this perspective, Carl Schmitt’s dictum that “all significant concepts of the modern theory of the state are secularized theological concepts,” resonates in the sphere of language. For when Schmitt refers to modern concepts of state as theological in origin, his thought speaks directly to the concept of global language. For “global” is only a slightly attenuated variant of “universal,” whose totalizing connotation we find somewhat embarrassing today. And yet, historically, the myth of a lost universal language has played an important role in traditional cultures that value creation accounts. The latter evoke a mythic moment, in illo tempore, when all humans spoke a common language for the simple reason that the origin of the one implicated the emergence of the other. Perhaps no one thought so profoundly about the question of global or universal language in this sense as Dante Alighieri in his philosophical treatise on the illustrious vernacular, the De vulgari eloquentia (c. 1304 CE, DVE). Unlike proponents of global language today who seek to extrapolate a (pre)dominant language from among hundreds of dialects, Dante recognized multilingualism as an historical contingency, a product of the linguistic fall at Babel. At that point, universal language—the speech co-created by God and the first man in Genesis—disappeared, fractured into linguistic shards representing the diversity of human culture. Dante’s anthropology of language thus imitates the division of the first human into an infinite series, each unit different from others, but all containing an originary “DNA,” identified in Genesis 1:26–27 as divine similitude. Likewise, Dante proposes that all human language possess a common “deep structure,” a set of principles for a universal grammar that act as the formal cause of language. Dante calls this capacity for language the forma locutionis, the innate capacity for speech, which defines humans. For Dante, only the vernacular or mother tongue can embody this essence. Jointly created by God and the first man, it has the potential to incarnate the highest aspiration of human speech, the volgare illustre or illustrious vernacular. Although Dante lays out his anthropology of the vernacular in De vulgare eloquentia—arguing that one must look to the most refined vernacular poetry for examples—he defers demonstrating the concept fully until Purgatorio and Paradiso. There he deploys text networks invoking Hebrew, Greek, Latin, and diverse vernacular poets and philosophers to showcase the volgare illustre as a universal vernacular—a forma locutionis in which the meaning, aura, and affect of speaking take precedence over language of origin.",Global Language or Universal Language?: From Babel to the Illustrious Vernacular,Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1045,http://dx.doi.org/10.1353/dph.2012.0006,"{'string_id': '10.1353/dph.2012.0006', 'id_scheme': 'DOI'}","In this paper I study the portrayal of Jean de Meun in Arsenal 3339, an early fifteenth-century manuscript collection in which the Rose precedes Jean’s Testament, Codicille, and Tresor ou Sept articles de la foi. I marshal manuscript evidence to show how the person who masterminded the production of Arsenal 3339 refuted certain of the reproaches leveled against Jean de Meun in the 1401–1403 Debate about the Roman de la Rose. They were, in effect, that he had erred on key theological points and had moreover employed questionable instructional methods. Although the Arsenal mastermind, whom I provisionally identify as its compiler, objects to the way that Jean as Rose author had portrayed himself by means of a negative persona, Jean “Clopinel,” which can be roughly translated as “Jean who limps,” he nevertheless declines to join with Jean Gerson and Christine de Pizan to consign the book to the flames. I make the case that the compiler was an early fifteenth-century reader exceptionally well versed in the Rose, the Debate documents, and Gerson’s sermons, who, by expertly designing the collection, seeks to rehabilitate Jean’s professional reputation in response to criticisms lodged by his detractors in the Debate.",“The Foot on Which He Limps”: Jean Gerson and the Rehabilitation of Jean de Meun in Arsenal 3339,Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1046,http://dx.doi.org/10.1353/dph.2012.0008,"{'string_id': '10.1353/dph.2012.0008', 'id_scheme': 'DOI'}","This article offers a codicological description of the most complete extant manuscript containing the poetry of Ausiàs March (Valencia, 1400–1459). It lays out the material characteristics of the codex, and sets them in contrast with a matrix of relevant features from other witnesses of March’s poetry. As a result, this study uncovers several of the stages in the compilatory process of the manuscript and argues for it having been originally owned by Ferrando de Cardona, Admiral of Naples and Duke of Somma (Naples, 1521-Sant Cugat del Vallès, 1571).","Readers and Compilators of Ausiàs March’s Poetry in Barcelona (BNE, MS 2985)",Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1047,http://dx.doi.org/10.1353/dph.2012.0010,"{'string_id': '10.1353/dph.2012.0010', 'id_scheme': 'DOI'}","President Nicolas Sarkozy’s recent call for a debate on national identity generated controversy in France. I use this controversy as a point of departure from which to re-examine the nineteenth-century myth of nationalism that casts the Armagnacs, supporters of Charles VII, as protonationalists. If the type of nationalism that the Armagnacs represented possesses any didactic force today, it is as a negative exemplum. Based on contemporary chronicle representations of the faction revealing how it understood itself and how it was viewed by its enemies, I argue that the Armagnacs are better seen as members of a feuding faction than as early nationalists to conclude that recent debate over national identity would be well served by acknowledging from the outset the fictive nature of what the History of France relates about the trajectory towards national unity and the perennial disagreement over who is French and who is not.","Feuding, Factionalism and Fictions of National Identity: Reconsidering Charles VII’s Armagnacs",Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1048,http://dx.doi.org/10.1353/dph.2012.0012,"{'string_id': '10.1353/dph.2012.0012', 'id_scheme': 'DOI'}","This article discusses the medieval Icelandic Nítíða saga (“The Saga of Nítíða”) as it is preserved in one of many postmedieval paper manuscripts, and takes into account not only the manuscript’s content and physical aspects, but also its social context, including its scribe, Þórður Jónsson (fl. 1667–93). The article foregrounds the existence of different versions of Nítíða saga by considering the differences between this version of the text and the better-known version from earlier manuscripts. In addition, this article offers an explanation for two scribal errors in one version of the saga through comparison with the other, and shows the importance of further manuscript-based research of this sort in Icelandic literary studies and other fields.",Variance Uncovered and Errors Explained: An Analysis of Nítíða saga in the Seventeenth-Century Icelandic Manuscript js 166 FOL.,Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1049,http://dx.doi.org/10.1353/dph.2012.0013,"{'string_id': '10.1353/dph.2012.0013', 'id_scheme': 'DOI'}","Satan and demonic beings as represented in Latin theological treatises undergo a surprising transformation during the Middle Ages. Initially depicted as impersonal manifestations of a divinely ordained world with labile frontiers between the natural and the supernatural, they become passionate beings, usually portrayed as envious, angry, and vengeful, with a particular animus towards morally upstanding humans. Concomitant with the portrayal of demonic emotion is the development of the first-person conversion narrative. This essay documents the evolution in the portrayal of demonic emotions across two key genres of the Latinate Middle Ages: the first-person life narratives of monks that began to appear in the twelfth century, and collections of miracle narratives from the twelfth to the thirteenth centuries. This progressive “personalization” of supernatural beings offers insight into Milton’s portrayal of Satan in Paradise Lost.",Sensitive Spirits: Changing Depictions of Demonic Emotions in the Twelfth and Thirteenth Centuries,Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1050,http://dx.doi.org/10.1353/dph.2012.0015,"{'string_id': '10.1353/dph.2012.0015', 'id_scheme': 'DOI'}","The words emotion and devotion are used as terms of convenience by modern historians when discussing various aspects of late medieval religion. The phrase emotive devotion has been used to describe an affective form of piety, focused on the sufferings of Christ, that is often regarded as the dominant form in fifteenth-century towns. However, if closer attention is paid to late-medieval usage of the terms devotion and emotion (or rather the “passions”) a different picture emerges. This article uses evidence from Bruges, then one of northern Europe’s most important towns, to argue that the “devotion” most favored, at least among those in authority, was one that was tied to social order. Descriptions of rebellion within the town in 1488 are semantically linked to a familiar scholastic discourse regarding the nature of the body and its “passions.” They are also linked to a vocabulary relating to order—and a “devotion” that in this period was specifically connected with “general” or supplicatory processions. The increasing use of this type of procession in the fifteenth century represents a qualitative change in the way that the citizen body was managed by civic authorities in Bruges and, by implication, in other late medieval towns. ",Devotion and Emotion: Creating the Devout Body in Late Medieval Bruges,Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1051,http://dx.doi.org/10.1353/dph.2012.0017,"{'string_id': '10.1353/dph.2012.0017', 'id_scheme': 'DOI'}","This essay examines how Dominicans focused their late fourteenth-century struggle to redefine spirituality around the cults of St. Thomas and St. Catherine of Siena. The relics of the Angelic Doctor were translated from the Cistercian abbey of Fossanova to Toulouse in 1369 through the efforts of French Dominican Master, Elias Raymundus. However, the translation was then countered by Raymond of Capua through the active promotion of the much more affective spirituality of Catherine of Siena. The contest came to a head when Raymond overthrew Elias Raymundus as Master of the Order at the General Chapter of Pentecost in 1380, just after the death of Catherine of Siena at thirty-three. Catherine’s intensely emotional style of spiritual writing is much more closely connected to her understanding of the influence of Thomas Aquinas than often realised and its promotion by Raymond of Capua marks a shift away from a purely cerebral approach to theology.","Thomas Aquinas and Catherine of Siena: Emotion, Devotion and Mendicant Spiritualities in the Late Fourteenth Century",Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1052,http://dx.doi.org/10.1353/dph.2012.0019,"{'string_id': '10.1353/dph.2012.0019', 'id_scheme': 'DOI'}","Recent work on the history of emotions, particularly by Barbara Rosenwein, has explored the idea of “emotional communities” as a means of uncovering the complex layers of identity and self-definition among individuals in past societies. Here I wish to explore three brief examples from French texts of the twelfth and thirteenth centuries in the light of this enquiry. Two are from well-known literary figures (Chrétien de Troyes and Marie de France) while the other is by Guillaume le Breton, biographer and panegyrist of the French king Philip II Augustus (r. 1180–1223). Each writer uses the image of the garden to illustrate the emotional standpoint of prominent individuals and communities within his or her text. The range of emotions on display—jealousy, grief, anger, fear, love, and even something we might label “national pride”—attests to the power of the garden image as a conveyor of meaning for medieval authors and audiences alike.",Gardens as “Emotional Communities”: Three Medieval French Examples,Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1053,http://dx.doi.org/10.1353/dph.2012.0020,"{'string_id': '10.1353/dph.2012.0020', 'id_scheme': 'DOI'}","This essay explores the early fifteenth-century “Cour amoureuse,” or Love Court, associated with the French royal court, in the context of modern psychological work on aggression and flyting, setting up a dialogue between medieval and modern psychological theories of anger. A professor in Pediatric Clinical Neuroscience specializing in the study of anger and aggression comments on an analysis of the “Cour amoureuse” offered by a scholar of medieval studies. The exchange reveals that a distinction present in medieval theories of anger seems to have a basis in neurological reality.",Anger and the “Cour amoureuse” of Charles VI: Medieval and Modern Perspectives,Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1054,http://dx.doi.org/10.1353/dph.2012.0021,"{'string_id': '10.1353/dph.2012.0021', 'id_scheme': 'DOI'}",A long-term digitization project (www.stgallplan.org) to bring the Carolingian plan for the monastery of St. Gall in Switzerland to life has earned justified praise for its impact. The project calls attention to and increases understanding of Carolingian monastic life at one of the great houses of the time. Whether the library was ever intended to be constructed or whether it was an imaginative conceptualization of an ideal library is immaterial to the light the project has shed on Carolingian spirituality. This article both introduces the project and demonstrates how digitization of manuscripts can increase the data available for studying devotion and the religious emotions that it entailed.,Digital Devotion from Carolingian Reichenau and St. Gall,Johns Hopkins University Press,2012,,Digital Philology: A Journal of Medieval Cultures,1,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1055,http://dx.doi.org/10.1353/dph.2013.0000,"{'string_id': '10.1353/dph.2013.0000', 'id_scheme': 'DOI'}","Sometime after its creation, the manuscript under investigation, a fifteenth-century Florentine antiphonary, received a remarkable new program of illumination, which included several heraldic devices and other images copied from or inspired by important Florentine works of art. Thus it lost its original liturgical role and was used simply as a canvas to demonstrate a more political and social purpose. This paper does not attempt to explain how this came about, but rather to explore the intricacies of this manuscript's new context. Besides dating its later illumination to the years around 1480, largely by identifying and analyzing the heraldic evidence, I argue that the manuscript must be placed within the humanist circle of Lorenzo de' Medici. By investigating the nuanced political atmosphere in Florence and the relations between wealthy, intellectual families of Lorenzo's circle, I furthermore conclude that Filippo Strozzi commissioned the illumination of this manuscript in order to solidify his political relationship with the Medici, to glorify his family, and to prove its devotion to Florence.",The Fisher Antiphonary: A Gilded Window onto the Strozzi and Late Fifteenth-Century Florentine Politics,Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1056,http://dx.doi.org/10.1353/dph.2013.0001,"{'string_id': '10.1353/dph.2013.0001', 'id_scheme': 'DOI'}","Authorship attribution often suffers from charges of vicious circularity, of conflating authorship and style. We argue for a more virtuous circularity for attribution arguments made through the quantitative analyses of stylometry. Our study investigated temporal lexeme usage, which has been suggested as a mark of authorship style, in the corpus traditionally attributed to Chrétien de Troyes. Our analyses revealed different usage patterns for these lexemes after line 6150 of the Lancelot, possibly indicating the signature of another author in the text (traditionally presumed to be that of Godefroy de Lagny). By embracing probabilistic arguments instead of certainties, digital philology can employ stylometry to consider questions of authorship attribution for medieval texts.",Virtuous Circles of Authorship Attribution through Quantitative Analysis: Chrétien de Troyes's Lancelot,Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1057,http://dx.doi.org/10.1353/dph.2013.0002,"{'string_id': '10.1353/dph.2013.0002', 'id_scheme': 'DOI'}","In this article, the author explores the idea that during the thirteenth century there were implemented new vernacular legal languages that were communicated to the ignorant clients of the law or lay people by notarial officers. The article theorizes around the effects of this communication, understood as a process of second language acquisition in which the second language is the formulaic system contained in legal documents.",Political Idiots and Ignorant Clients: Vernacular Legal Language in Thirteenth-Century Iberian Culture,Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1058,http://dx.doi.org/10.1353/dph.2013.0003,"{'string_id': '10.1353/dph.2013.0003', 'id_scheme': 'DOI'}","This essay offers an insight into the way digital editions of medieval texts can be employed to replicate the medieval reading experience. Awareness of the characteristic features of medieval textuality, exemplified through select late medieval texts, can help in developing increasingly flexible editorial models, which are more consistent with medieval reading practices than current editions. Editions, transformed from single textual occurrences into fluid, communal, and unfolding processes, can uncover a complex notion of medieval hypertextuality by linking texts, images, and tunes. They can then even trace the reception of a given text. As readers are empowered to zoom in and out specific textual components, of manuscript witnesses, of families and printed editions, digital editions can present individual witnesses alongside editorial apparatuses and thus bridge the gap between the Old and the New Philology.",Old Light on New Media: Medieval Practices in the Digital Age,Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1059,http://dx.doi.org/10.1353/dph.2013.0004,"{'string_id': '10.1353/dph.2013.0004', 'id_scheme': 'DOI'}","In this paper, the author considers the origins of the rhyme, -aura, which has played an important role in Western poetry. Numerous studies have addressed questions pertaining to the history of aura in general, but none have focused on the matter of aura specifically as a rhyme. Within this narrower focus, the history of the aura-rhyme is shown to center on one critical moment, in which the aura-pun in Ovid's Metamorphoses is transformed into the aura-rhyme in Arnaut Daniel's song ""En cest sonet."" Through a comparative reading of these two texts, conclusions are drawn regarding the relation of puns to rhymes, nouns to names, and the problems of isolating the origin of a rhyme historically.",The Metamorphoses of Aura in Ovid and Arnaut Daniel,Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1060,http://dx.doi.org/10.1353/dph.2013.0005,"{'string_id': '10.1353/dph.2013.0005', 'id_scheme': 'DOI'}","This essay discusses the racialized portrayal of Dangier in Bodleian Library MS Douce 195. In contrast to the notion that the Middle Ages had no recourse to race as phenotypical difference, it argues that the orientalization of depictions in late medieval manuscripts like Douce 195— mostly studied through costume—is also to be found in portrayals of skin pigmentation.",Saracens and Race in Roman de la Rose Iconography: The Case of Dangier in MS Douce 195,Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1061,http://dx.doi.org/10.1353/dph.2013.0008,"{'string_id': '10.1353/dph.2013.0008', 'id_scheme': 'DOI'}","This article argues that, in the Series, Thomas Hoccleve not only appeals to the system of patronage, but attempts to revise it, creating a new template for literary sponsorship and engagement based on collaboration and mutual responsibility. The process of literary production Hoccleve outlines supports a reading of patronage that is not exclusively financially-based; rather, it is flexible, less hierarchical, and demands more from the patron than Hoccleve’s previous work—and indeed the work of most medieval authors—would suggest. In order to understand Hoccleve’s attempts to redefine his relationship to the system of literary sponsorship, however, we must be willing to take the structure of the Series seriously, following the signposts of creative collaboration Hoccleve weaves throughout the texts rather than their misleading, seemingly random nature. In addition to offering an alternate reading of Hoccleve’s work in the Series, this article suggests new ways of thinking about literary patronage in the Middle Ages more generally.",The Rehabilitation of Patronage in Hoccleve’s Series,Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1062,http://dx.doi.org/10.1353/dph.2013.0010,"{'string_id': '10.1353/dph.2013.0010', 'id_scheme': 'DOI'}","This essay applies a theoretical model drawn from twentieth-century organization studies to late-medieval texts written in defence of women, in order to refresh critical appreciation of the dynamics of poet-patron relations at play in this corpus, the interest of whose patronage situations has been underestimated. By reading three sample poems (in both textual and paratextual dimensions) as “circuits of power,” we will be able to unpick the manoeuvrings of the agents contributing to a given book’s production and reception; to re-assess the framework of power relations in patronage as fulfilling a productive rather than a regulatory role; and to question more fully what constitutes authority and identity, in its various and multi-faceted forms in premodern literary practice.",Circuits of Power: A Model for Rereading Poet-Patron Relations in Late-Medieval Defences of Women,Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1063,http://dx.doi.org/10.1353/dph.2013.0012,"{'string_id': '10.1353/dph.2013.0012', 'id_scheme': 'DOI'}","Erich Auerbach held an expansive notion of what philology is and does. Taking literature as his starting point (often under the rubric of a concrete Ansatzpunkt, be this a phrase, an isolated feature of style, or a self-contained logical sequence), Auerbach restlessly sought to establish nothing less than an intellectual—or better yet, spiritual (he often calls it “inner”)—history of the Western European mind as it lunged into contemporary modernity. Literary forms were for Auerbach a gateway to forms of thought, feeling, and expression. Philology was the method best suited to grasping these. The result was a philology of the world (Weltphilologie)—a politically, ideologically, and ethically engaged counter-philology that is as relevant today as it was in his own lifetime.",Erich Auerbach’s Earthly (Counter-)Philology,Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1064,http://dx.doi.org/10.1353/dph.2013.0013,"{'string_id': '10.1353/dph.2013.0013', 'id_scheme': 'DOI'}","This article reviews the terminological framework used to describe manuscripts. The Lachmannian terminology allows scholars to classify manuscripts as versions or variants of a work on a purely textual basis, but lacks a rigid designator to indicate a (part of a) manuscript as a unit of text and material considerations. Conversely, scholars who adopt Dagenais’s solution to renounce the work and concentrate on the material scriptum gain a rigid designator, but threaten to lose the ability to classify manuscripts at all. Proceeding from a case study, the article argues that the twelfth-century view of a work’s ontological status enables medievalists to keep classifying their scripta on both textual and material grounds. It explores the possibility of using Dagenais’s scriptum as the foundation for a Neo-Lachmannian terminological framework that allows scholars to study manuscript variance and materiality without losing the ability to classify them.","Work, Version, Text and Scriptum: High Medieval Manuscript Terminology in the Aftermath of the New Philology",Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1065,http://dx.doi.org/10.1353/dph.2013.0014,"{'string_id': '10.1353/dph.2013.0014', 'id_scheme': 'DOI'}","Patronage regularly figures as the dominant paradigm in discussions of late-medieval literary production and circulation. Common practice dictates that behind every text, a patron-figure looms, typically identified as a member of nobility for whom the text was composed. Many medieval texts promote this view by spotlighting extratextual relations in dedication materials or by folding into the narrative itself the story of its inception. Regardless of the source of these narratives, scholarship places under the patronage umbrella a large spectrum of exchanges said to have fostered the literary enterprise while excluding other partnerships that are seen to destabilize the system. Thus, transactions as distinctive as Guillaume de Machaut’s Confort d’amy, presented as an unsolicited text intended to comfort the imprisoned Charles of Navarre; Jean Froissart’s fictive account of literary collaboration with a prince in the Prison amoureuse; Christine de Pizan’s biography of Charles V, identified in the prologue as a commission ordered by the duke of Burgundy; and Geoffrey Chaucer’s dedication of Troilus and Criseyde to fellow poet John Gower and the lawyer Ralph Strode are presented as forms of patronage with little thought to the implications of the relational variations. At the same time, a number of transactions are sidelined in patronage studies because they seem to transgress the boundaries of expected exchange, even though they reproduce many of the same dynamics; hence, regifting, seizure of books, multi-copy dissemination of a single work, and purchase of a book from a third party regularly fail to figure into analysis. To date, scholars have avoided interrogating these arbitrary boundaries that define patronage studies. Critical avoidance of the heterogeneity of literary support networks is further exacerbated by a generalized failure to interrogate the impact of time, place, or cultural practices on sponsorship, not to mention possible manipulations of this exchange based on political, economic or personal concerns.",Introduction: Rethinking the Boundaries of Patronage,Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1066,http://dx.doi.org/10.1353/dph.2013.0016,"{'string_id': '10.1353/dph.2013.0016', 'id_scheme': 'DOI'}","This article explores the politics of owning, or taking ownership, of Bible historiale manuscripts during the Hundred Years’ War. As theft and capture disrupted predictable patterns of patronage and exchange, new owners’ additions, erasures, and alterations can shed new light on ways in which the French Bible conditioned readers’ experience of the war on both sides. Such marginal acts of appropriation also force us to revise traditional notions of patronage to account for the sometimes violent refashioning of books to match the image and needs of unintended owners.",Stolen Scriptures: The Bible Historiale and the Hundred Years’ War,Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1067,http://dx.doi.org/10.1353/dph.2013.0018,"{'string_id': '10.1353/dph.2013.0018', 'id_scheme': 'DOI'}","Written in the wake of the devastating Christian defeat at Nicopolis, the Epistre lamentable consistently draws on individual examples to target universal reform. Mézières establishes an intimate connection to Duke Philip of Burgundy as a fellow sufferer in Christ, attentuating the distance of hierarchical position. Once the duke is consoled by means of proven analogies—Mézières, Burgundy, and indeed all Christians are bound by wounds that replicate those of the Crucifixion—the author can go further. He becomes the duke’s counselor, or physician, and seeks to address how the Christians were routed at Nicopolis despite the justice of their crusade. Mézières dismantles the standard, self-interested bases of authorpatron address by targeting a larger and higher purpose to which to recruit Philip. He points the way to Christian recovery through his long-cherished project of a perfect knighthood, the Order of the Passion of Jesus Christ.",To Console and Control: Philippe de Mézières’s Epistre Lamentable,Johns Hopkins University Press,2013,,Digital Philology: A Journal of Medieval Cultures,2,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1068,http://dx.doi.org/10.1353/dph.2014.0000,"{'string_id': '10.1353/dph.2014.0000', 'id_scheme': 'DOI'}","This study imagines a Sephardic archive not as a physical site that houses the artifacts, texts, and history of a nation-state or Empire, but one that allows us to access those objects (or exposes their absence) and to bring artifacts from different official archives into dialogue in a different, virtual space, thus creating an additional, but not exclusionary, epistemic home, namely that of Sephardic studies. In it the author explores the potential advantages and practical limitations, as well as existing models of transnational resources—such as the Friedberg Genizah Project and the Institute of Microfilmed Hebrew Manuscripts, as well as del Barco and Vegas Montaner’s project of cataloguing the Hebrew manuscripts in Spanish libraries—that could be considered when thinking of what form a Sephardic archive could take. This study also explores how issues of language and identity fare when translated into the metadata used to make digitized information available.",The Sephardic Past in the Digital Future,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1069,http://dx.doi.org/10.1353/dph.2014.0002,"{'string_id': '10.1353/dph.2014.0002', 'id_scheme': 'DOI'}","This essay explores the significance of what are described as the “tears of Moses” in the fourteenth-century Libro de buen amor (“Book of Good Love”). Using digitized corpora, I show how exegesis of this biblical image focused on Pharaoh’s daughter being moved by the weeping of the infant Moses. Her compassion was equated with the bridal and maternal Church lovingly receiving and nourishing the spirit of the Law of Moses as her groom and infant Christ. The Spanish poem parodies these meanings by portraying the relationship between the narrator and his procuress as that of a crying baby being soothed by his mother. In a subversion of the heart-rending cries of Moses being consoled by the Church, the old bawd enables him to seduce the object of his lust by perversely appealing to her maternal compassion.",Return to Commentary: On the Tears of Moses in the Libro de buen amor,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1070,http://dx.doi.org/10.1353/dph.2014.0003,"{'string_id': '10.1353/dph.2014.0003', 'id_scheme': 'DOI'}","This article describes the project An Electronic Corpus of Fifteenth-Century Castilian Cancionero Manuscripts, paying particular attention to issues and solutions in matters related to textual criticism, such as the collation of witnesses and the generation of phylogenetic stemmata.",An Electronic Corpus of Fifteenth-Century Castilian Cancionero Manuscripts,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1071,http://dx.doi.org/10.1353/dph.2014.0004,"{'string_id': '10.1353/dph.2014.0004', 'id_scheme': 'DOI'}","This article illustrates an important while comparably unattended role played by the making of digital databases in our contemporary philological study of medieval poetry. By presenting a case study devoted to the figure of Mossèn Avinyó, a fifteenth-century Catalan nobleman and poet, the authors of this essay exemplify the need for a rigorous perusal of the material evidence that is being archived in digital projects. With paleographical, codicological, and literary evidence, the authors show with great probability that Avinyó himself compiled two manuscripts currently held by the Hispanic Society of America: the Cancionero de Vindel and the Cançoner llemosí del siglo XV. In doing so, the authors also show that these two manuscripts contain the entirety of Avinyó’s lyric production, along with a sampling of the poetry produced by those who traveled in Avinyó’s cultural circles.","Mossèn Avinyó, the Cancionero de Vindel and the Cançoner llemosí del siglo XV",Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1072,http://dx.doi.org/10.1353/dph.2014.0005,"{'string_id': '10.1353/dph.2014.0005', 'id_scheme': 'DOI'}","This article presents the Cançoners DB, a database devoted to medieval Catalan songbooks transmitting Catalan lyric and verse narrative works ca. 1300–1500. In addition to introducing the aims and scope of the project, this essay reflects on the challenges and prospects of studying and indexing the material characteristics of Catalan songbooks in order to research works in verse, book production, and the compared structure of songbooks in Romance language traditions.",Cançoners DB: A New Research Tool for the Study of Medieval Catalan Songbooks,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1073,http://dx.doi.org/10.1353/dph.2014.0007,"{'string_id': '10.1353/dph.2014.0007', 'id_scheme': 'DOI'}","This article has a twofold purpose. By providing ample illustration of the difficulties encountered in the making of a catalogue of medieval Catalan translations, we first consider different cataloguing criteria in order to discuss the suitability of the database format vis-à-vis that of a PDF document. Secondly, through the former discussion we bring forth three main questions for the study of medieval translation in general, namely: autor-ship, mediation, and transmission. The third question elicits our philological discussion of what constitutes a translation and how we may distinguish different translations or versions of a same work.",Reflections on the Making of a Census of Medieval Catalan Translations,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1074,http://dx.doi.org/10.1353/dph.2014.0009,"{'string_id': '10.1353/dph.2014.0009', 'id_scheme': 'DOI'}","This article explains how the Llull DB, a database devoted to Ramon Llull’s original works, apocryphal corpus, historical reception, and modern scholarship, was conceived and developed. In addition to addressing specific issues involved in the design of a database with a focus on the medieval author of a multilingual corpus that enjoyed a wide reception, this essay also shows how the Llull DB has provided the original template for a series of interconnected databases related to medieval Catalonia, and tackles the challenges posed by the need for an interoperative environment for all of them.",A Brief History of the Llull DB and Its Derivatives,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1075,http://dx.doi.org/10.1353/dph.2014.0010,"{'string_id': '10.1353/dph.2014.0010', 'id_scheme': 'DOI'}","The academic study of medieval manuscripts has always been undertaken as if in dialogue with printed books. This essay takes as its starting point the changes in the nature of printed books that have come about in the last decade with the dramatic increase in data storage and retrieval capabilities, paying particular attention to the impact of book digitization by Google Books and similar programs. After first considering how targeted textual searching has changed the way books are used for research—inverting the traditional hierarchy of text and reader by allowing the latter to use the text with greater speed and control than ever before—I suggest that the change in the function of books can also entail for medievalists a change in the meaning of manuscripts. Because manuscripts provide much more information than that which is encoded in their text, including many elements that defy systematic characterization, their idiosyncratic nature seems to be enhanced in comparison to the increasingly utilitarian nature of searchable books. Taking as an exemplary case the digitization of University of Coimbra MS 720, a late-medieval copy of the Dagger of Faith (Pugio fidei) by the Catalan Dominican Ramon Martí, this essay offers some personal reflections on the ways that the use of texts, information, and manuscripts is changing in the age of digital books.",Cracking the Code: Reflections on Manuscripts in the Age of Digital Books,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1076,http://dx.doi.org/10.1353/dph.2014.0011,"{'string_id': '10.1353/dph.2014.0011', 'id_scheme': 'DOI'}","A recently discovered fragment of the Middle English Prose Brut, from the Special Collections department of Queen’s University, Belfast, illuminates the diverse production methods used in the Brut corpus. As part of this process, the author investigates the origins of the unique textual interpolation contained in the Belfast Brut fragment. Through comparison with the Dartmouth Brut, the author suggests some of the different approaches Brut producers took when tasked with “making history.”",Making Histories: Locating the Belfast Fragment of the Middle English Prose Brut,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1077,http://dx.doi.org/10.1353/dph.2014.0012,"{'string_id': '10.1353/dph.2014.0012', 'id_scheme': 'DOI'}","Art is some Brut manuscripts helped shape readers’ understanding of history. Whereas the Dartmouth Brut’s single painted border fulfills an “authorizing” function common in Middle English Prose Brut manuscripts, the seventy miniatures painted by the Master of Edward IV in Lambeth Palace Library MS 6 actually interpret history. This article transcribes instructions to the artist that survive on nine folios of Lambeth 6 and addresses how the manuscript planner and the artist negotiated differences of both medium (text, image) and language (English, French). The planner instructed that pictures match text; the artist’s interpretations and iconography yielded sometimes unexpected historical nuances.",Deciphering the Brut: Lambeth Palace MS 6 and the Perils of Transmission,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1078,http://dx.doi.org/10.1353/dph.2014.0013,"{'string_id': '10.1353/dph.2014.0013', 'id_scheme': 'DOI'}","This essay examines the annotations of three readers of the Dartmouth Brut. Each engaged the manuscript in different ways: the first had prior knowledge of its contents, the second used the text to teach himself English history, and the third mined the chronicle as a primary source for facts. Together, these readers provide insight into why a medieval chronicle—outdated in content and manuscript form—would continue to attract the attention of early modern readers.",Echoes in the Margins: Reading the Dartmouth Brut in Early Modern England,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1079,http://dx.doi.org/10.1353/dph.2014.0015,"{'string_id': '10.1353/dph.2014.0015', 'id_scheme': 'DOI'}","Through analysis of the patterns and layers of annotation in the Middle English Prose Brut manuscript now known as the Dartmouth Brut, I consider what the manuscript’s development over time reveals about annotation itself as a collaborative activity with a tradition of its own, grounded in widely held habits of reading and writing. Recognition of such factors is essential to assessing annotation (or its absence) as potential evidence for the interests of individual readers. I conclude with discussion of the promise and hazards of manuscript digitization for the study of annotation.",“It is to harde for my lernyng”: Making Sense of Annotations in Brut Manuscripts,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1080,http://dx.doi.org/10.1353/dph.2014.0016,"{'string_id': '10.1353/dph.2014.0016', 'id_scheme': 'DOI'}","In this photo essay, we reflect on our respective encounters with the object now known as Dartmouth College, Rauner Special Collections Library, MS 003183. Deborah Howe describes the process that turned a fragile object into a functional codex. Michelle Warren considers how this process intersects with the practices of medieval studies and digital humanities. Together, we trace the kinds of collaborations that orient material history toward the future.","The Dartmouth Brut: Conservation, Authenticity, Dissemination",Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1081,http://dx.doi.org/10.1353/dph.2014.0017,"{'string_id': '10.1353/dph.2014.0017', 'id_scheme': 'DOI'}","The texts and manuscripts of the Middle English prose Brut resist easy classification, largely because they have resulted from a long and ongoing history of re-writing. The articles in this volume grapple with these dynamics by bringing comparative perspectives to a single manuscript. In this afterword, I seek to make clear some of the pluralities at the heart of encountering the digital, physical, and always contingent Brut. I highlight the complexities of transmission with an example of how one late medieval reader responded not to the text at hand, but rather to his memories or expectations of other histories and other texts.",Encountering the Dartmouth Brut in the Midst of History,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1082,http://dx.doi.org/10.1353/dph.2014.0018,"{'string_id': '10.1353/dph.2014.0018', 'id_scheme': 'DOI'}","The text represented by the Dartmouth Brut, written before 1430, was probably influenced by the political turmoil following the death of Henry V. Its omission of the stories of Constantine and Havelok—but inclusion of the story of Cadwallader—appears to represent a stage in the government’s revision of history. The stories of Constantine and Havelok would have been offensive to some since the first suggests a continuation of heroic kingship among the Welsh and the second an unflattering parallel to Humphrey, Duke of Gloucester, who was attempting to control the government of the child-king Henry VI.",“History Repeats itself”: The Dartmouth Brut and Fifteenth-Century Historiography,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1083,http://dx.doi.org/10.1353/dph.2014.0019,"{'string_id': '10.1353/dph.2014.0019', 'id_scheme': 'DOI'}","This essay situates the Dartmouth Brut within the manuscript culture of mid- to late fifteenth-century England. After a brief sketch of the manuscript’s modern ownership, the author turns to the original owners, the Ewre family of County Durham, and then to a recently discovered Brut with some ownership associations with the Dartmouth Brut. Finally, the author compares the competently written Dartmouth manuscript with one of the physically sloppiest of all Brut manuscripts—Glasgow MS Hunter 443. These manuscripts provide useful foils to each other when examined against the large body of Brut manuscripts, illuminating the different methods available to fifteenth-century scribes.",Contextualizing the Dartmouth Brut: From Professional Manuscripts to “The Worst Little Scribbler in Surrey”,Johns Hopkins University Press,2014,,Digital Philology: A Journal of Medieval Cultures,3,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1084,http://dx.doi.org/10.1353/dph.2015.0001,"{'string_id': '10.1353/dph.2015.0001', 'id_scheme': 'DOI'}","This article studies how modern philologists have reconstructed a now lost ninth-century Arabic chronicle, the so-called Chronicle of al-Razi, through the account found in fifteenth-century Castilian chronicles of the defeat of the last Visigothic king of Iberia, Rodrigo. In addition to exploring the construction of a textual history that is populated by a series of phantasmal Iberian translators, manuscripts, and copyists, the creation of a modern Castilian edition of this lost Arabic chronicle speaks to the ideological basis and desires of modern, national philology.",Spanish Philology and the Arab Past: A Case Study,Johns Hopkins University Press,2015,,Digital Philology: A Journal of Medieval Cultures,4,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1085,http://dx.doi.org/10.1353/dph.2015.0002,"{'string_id': '10.1353/dph.2015.0002', 'id_scheme': 'DOI'}","Resistance to digital technology in manuscript study stems in large part from what detractors perceive as a loss of contact with the material artifact. For them, the digital image represents an illegitimate substitution for “the real thing.” In two movements, this article first suggests why such a term radically misunderstands both the relationship of digital image to manuscript, and also the nature of the manuscript’s performance of the work it represents. A second movement illustrates the performative propensity of manuscript technology—under the impetus of the sudden accumulation of vernacular adaptations of classical works fostered by Charles V in the 1370s—to influence court culture.",Materialities of the Manuscript: Codex and Court Culture in Fourteenth–Century Paris,Johns Hopkins University Press,2015,,Digital Philology: A Journal of Medieval Cultures,4,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1086,http://dx.doi.org/10.1353/dph.2015.0003,"{'string_id': '10.1353/dph.2015.0003', 'id_scheme': 'DOI'}","This article questions the “narrator theory of narration” with particular reference to medieval narratives, often retellings of existing stories, and generally neglected by current theory. Part I analyzes the concept of the internal, potentially unreliable narrator, widely regarded as a narratological necessity. Part II examines the history of the term narrator. Part III studies the theories of narration implied by scribal annotations in some medieval manuscripts. Parts IV, V, and VI discuss three medieval cases—Malory’s Morte Darthur, Chaucer’s Physician’s Tale and the anonymous Pearl—in which application of the narrator theory proves to be misleading.",What is a Narrator?: Narrator Theory and Medieval Narratives,Johns Hopkins University Press,2015,,Digital Philology: A Journal of Medieval Cultures,4,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1087,http://dx.doi.org/10.1353/dph.2015.0004,"{'string_id': '10.1353/dph.2015.0004', 'id_scheme': 'DOI'}","Erich Auerbach’s Mimesis: The Representation of Reality in Western Literature (1946) was profoundly influenced by George Wilhelm Friedrich Hegel’s idealist philosophy. Hegel held that concepts determine the structure of reality: the concept of freedom in his philosophy of history and, for Auerbach, the concept of history in the representation of reality. For both, reality is inextricable from reason, rationality, consciousness, and vice versa; hence, history signifies the rise and progress of human consciousness. Auerbach also shared Hegel’s view that the art of mimesis produces an image of a reality, in contrast to Plato, who thought a work of art is a resemblance opposed to reality. This paper analyzes the influence of Hegel’s philosophy of history on Auerbach’s conceptions of literary history, reality, and truth, as evidenced in Mimesis’s famous first chapter, “Odysseus’ Scar.” Auerbach chose the Hegelian concept of reality (Wirklichkeit) as the subtitle of his book to advance its main thesis, opposing realism to myths and legends, rationality to the flight from reason. He refutes the claim that classical Greek myths, legends, and heroes inaugurated Western culture’s representation of reality and, hence, conception of history. Instead, he finds their origins in the Old Testament, with its formulation of world, universal history, and “concept of the historically becoming,” an important Hegelian concept according to which the temporal becoming and unfolding of the life of human beings is meaningful, intelligible, and should be thought of as evolutionary progress toward a certain goal or end.",Constructing and Representing Reality: Hegel and the Making of Erich Auerbach’s Mimesis,Johns Hopkins University Press,2015,,Digital Philology: A Journal of Medieval Cultures,4,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1088,http://dx.doi.org/10.1353/dph.2015.0010,"{'string_id': '10.1353/dph.2015.0010', 'id_scheme': 'DOI'}","In Old English poetry and prose, declarations of unknowing and inexpressibility appear as quasi-formulaic assertions that no human being exists who could know or express the declarations’ subject—even when the text surrounding the declaration of unknowing contains evidence to the contrary. Consistently, in homilies and poetry alike, declarations of unknowing mark out the supernatural, functioning as traditional poetic markers of wonder and awe. With declarations of unknowing as a case study, this paper proposes a network model for visualizing the relationship between motifs, texts, and traditional associations within the larger expressive economy of oral-traditional Old English poetics.",Beyond the Sun’s Setting: Webs of Unknowing in Old English,Johns Hopkins University Press,2015,,Digital Philology: A Journal of Medieval Cultures,4,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1089,http://dx.doi.org/10.1353/dph.2015.0011,"{'string_id': '10.1353/dph.2015.0011', 'id_scheme': 'DOI'}","In this article I analyze the contents and transmission of a group of related manuscripts containing copies of Ælfrician and anonymous homilies in order to demonstrate the linguistic potential of variant copies of texts. I consider them from a comparative textual and linguistic perspective in a way that privileges the scribe and scribal activity over traditional philology or editorial practice. The manuscripts are Oxford, Bodleian Library MSS Bodley 340 and 342, Cambridge, Corpus Christi College MSS 162, 198, and 303. They date from Ælfric’s lifetime (c. 950–c. 1010) to the mid-twelfth century. I focus on three Ælfrician homilies that appear in sequence uniquely in this cluster alongside four anonymous texts that, similarly, are not found together elsewhere. This research, funded by the Andrew W. Mellon foundation, was conducted as part of a collaborative project that sought to refine digital technologies to aid consequential research in book and language history.",Filling the Silence: Shared Content in Four Related Manuscripts of Ælfric’s Catholic Homilies,Johns Hopkins University Press,2015,,Digital Philology: A Journal of Medieval Cultures,4,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1090,http://dx.doi.org/10.1353/dph.2015.0012,"{'string_id': '10.1353/dph.2015.0012', 'id_scheme': 'DOI'}","Two manuscripts in the Parker Library at Corpus Christi College, Cambridge, are copied in a hand which resembles that of Richard Frampton, a known clerk of the House of Lancaster in the early fourteenth century. Frampton copied literary manuscripts as well as working for the civic bureaucracy at the London Guildhall. His connections with royalty, the nobility and with Guildhall colleagues engaged in similar activities prompted an exploration of his social networks by means of software created at the University of Sheffield. The research was funded by the Andrew W. Mellon Foundation and coordinated by the University of Toronto.",Richard Frampton and Two Manuscripts in the Parker Library,Johns Hopkins University Press,2015,,Digital Philology: A Journal of Medieval Cultures,4,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1091,http://dx.doi.org/10.1353/dph.2015.0013,"{'string_id': '10.1353/dph.2015.0013', 'id_scheme': 'DOI'}","This article engages with the scholarly debate surrounding the scribal hands of the Parker Chronicle, the copy of the Anglo-Saxon Chronicle found in Cambridge, Corpus Christi College, MS 173. In light of new tools and practices within the digital humanities, the author suggests that the number of previously differentiated scribes writing between folios 16v–28r ought to be reduced. Fleck examines Corpus Christi’s digital holdings through the tools afforded by Stanford University’s Parker on the Web Project, and addresses questions of scribal identity treated most recently and prominently by Janet Bately and David Dumville, collaborative editors of the Anglo-Saxon Chronicle. In so doing, he demonstrates the utility of these digital tools for democratizing the practice of paleography.",A Digital Re-examination of the Scribal Hands in the Parker Chronicle,Johns Hopkins University Press,2015,,Digital Philology: A Journal of Medieval Cultures,4,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1092,http://dx.doi.org/10.1353/dph.2016.0002,"{'string_id': '10.1353/dph.2016.0002', 'id_scheme': 'DOI'}","This article explores the overlapping experiences of Guillaume de Machaut’s medieval manuscript readers and modern viewers of the digitized codex to argue that they share a nostalgic desire for a multi-sensorial and intimate encounter with the textual body. This study first considers Guillaume’s disgruntled reader in the Jugement dou roy de Navarre before turning to later readers who reenacted her desires when dealing with Machaut’s manuscripts. Finally, nuancing complaints that digitization distances us from the manuscript artifact, I consider how the digital surrogate reignites a desire for the textual body that defined the late-medieval reading experience.","Textual Bodies, the Digital Surrogate, and Desire: Guillaume de Machaut’s Judgment Cycle and his Protean Corpus",Johns Hopkins University Press,2016,,Digital Philology: A Journal of Medieval Cultures,5,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1093,http://dx.doi.org/10.1353/dph.2016.0004,"{'string_id': '10.1353/dph.2016.0004', 'id_scheme': 'DOI'}","This chapter shakes up critical understanding of narrative voice in text and image of Machaut’s dits. It perceives narratorial activity as a function that, far from being proper to one party (“the narrator”), is inherently mobile, circulating around elements of a poem or miniature, whether human characters, animals or landscape features. Miniatures devised for the jugement poems are a particularly rich resource for analyzing this mobility being exploited by artists to draw attention to issues of narrative framing, communication and point of view – including, crucially, the point of view of the manuscript’s reader/viewer.",Picturing Narrative Voice: Communication and Displacement,Johns Hopkins University Press,2016,,Digital Philology: A Journal of Medieval Cultures,5,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1094,http://dx.doi.org/10.1353/dph.2016.0005,"{'string_id': '10.1353/dph.2016.0005', 'id_scheme': 'DOI'}","Medieval scribes used decorated letters to mark the beginnings of songs, highlighting the incipits of texted voices and the vocal designations of others (“Tenor”). But some motets from fourteenth-century France begin with sung, untexted fanfares. This study examines several instances in the Machaut and Ivrea manuscripts where shortages of space, missing opening letters, and unorthodox layout decisions point to scribal confusion in the face of such beginnings. This evidence raises the possibility that untexted introductory sections may have traveled independently of their host compositions, and suggests new answers to the problems these “introitus” sections raise for modern editors and performers.",[I]ntroitus: Untexted Beginnings and Scribal Confusion in the Machaut and Ivrea Manuscripts,Johns Hopkins University Press,2016,,Digital Philology: A Journal of Medieval Cultures,5,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1095,http://dx.doi.org/10.1353/dph.2016.0006,"{'string_id': '10.1353/dph.2016.0006', 'id_scheme': 'DOI'}","The ordering of polyphonic musical works in the Machaut manuscripts relates directly to issues of layout and readability. Considering the size of the manuscripts, calculated according to surface area, the paper compares musical layout across the six Machaut manuscripts including his musical works (Paris, BnF, fr. 1584, 1585, 1586, 9221, 22545–6, and the Ferrell manuscript). Focusing on BnF fr. 9221, which is the largest and latest of these manuscripts, and is often criticized for its musical readings, the paper examines the idea of authority and considers the value of the manuscript to its original owner, the Duke of Berry.",Why Size Matters: Music Layout and Order in the Machaut Manuscripts,Johns Hopkins University Press,2016,,Digital Philology: A Journal of Medieval Cultures,5,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1096,http://dx.doi.org/10.1353/dph.2016.0007,"{'string_id': '10.1353/dph.2016.0007', 'id_scheme': 'DOI'}","This essay examines a late fourteenth-century compendium that combines an abridged version of Machaut’s Confort d’amy with a disparate sampling of contemporary devotional, didactic, and political vernacular writing. Many of the texts, including Machaut’s poem, appear in the compendium in anonymous and abridged versions that erase details pertaining to a specific author or patron. The systematic process of adaptation within the manuscript favors author anonymity and a more personalized approach to abstracting meaning from texts. This essay argues that the continuity of this reading experience across the compendium’s patchwork of devotional and political contents shows a reception history for Machaut’s poetry that was driven by readership more than authorship.","Devotional and Political Consolation in Paris, BnF, fr. 994",Johns Hopkins University Press,2016,,Digital Philology: A Journal of Medieval Cultures,5,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1097,http://dx.doi.org/10.1353/dph.2016.0008,"{'string_id': '10.1353/dph.2016.0008', 'id_scheme': 'DOI'}","This article examines the group of Machaut’s poems (a composite lai and ten ballades) copied into an anthology compiled by the French humanist Simon de Plumetot. More than half of the 155 items were composed by Eustache Deschamps, and most of the remainder are anonymous, but the collection has a markedly didactic character and many pieces make political allusions. The manuscript thus sets Machaut’s love poems in an unusual context. This essay argues that reading these poems as presented in the anthology enables the modern reader to approximate a medieval reading experience.","‘Anonymous’ Machaut: Guillaume de Machaut in Paris, BnF, NAF 6221",Johns Hopkins University Press,2016,,Digital Philology: A Journal of Medieval Cultures,5,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1098,http://dx.doi.org/10.1353/dph.2016.0009,"{'string_id': '10.1353/dph.2016.0009', 'id_scheme': 'DOI'}","This article investigates the history of cataloging medieval manuscripts in order to determine a paradigm for digital catalogs of the present. Inspired by the work of Suzanne Briet (1894–1989), it argues in favor of a culturally informed “documentation” that considers not only the complexity of medieval manuscripts as information objects, but also anticipates the diverse interests of users who depend on digital images and supporting metadata to access manuscripts as primary sources for research. Presented here is a faceted metadata structure that considers the premodern manuscript as physical, textual, historical, as well as digital information. This approach is built upon various traditions for cataloging medieval manuscripts, from Cassiodorus in the sixth century to the electronic databases and digital catalogs of the present.","Cataloging Medieval Manuscripts, from Beasts to Bytes",Johns Hopkins University Press,2016,,Digital Philology: A Journal of Medieval Cultures,5,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1099,http://dx.doi.org/10.1353/dph.2016.0010,"{'string_id': '10.1353/dph.2016.0010', 'id_scheme': 'DOI'}","Manuscript 5366 of the Bibliothèque de l’Arsenal in Paris is a fifteenth-century miscellany written on paper that reproduces several religious texts in Middle French and two short fragments in Latin. The first group of texts—the Gospel of Nicodemus and La vengence Vaspasien (“The Revenge of Emperor Vespasian”, ff. 1r-86r)—is signed with the words: “Cest liure est a Nicole de Bretaigne qui le trouuera cy li rende et elle poyra bien le vin” (“This book belongs to Nicole de Bretaigne; she will pour a good quantity of wine for the finder who returns it”; Paris, Bibliothèque de l’Arsenal, MS 5366, f. 86r). This note is written in the same color ink as the preceding texts and in the same handwriting. Starting on the verso side of this leaf a slightly different hand, most likely a reader, has added three short fragments in French based on the Gospels. The thematic unity of these later additions suggests that they were the result of the spiritual needs and preferences of an owner of this manuscript, most likely a laywoman. Although the involvement of professional scribes cannot be ruled out entirely, this article will argue that one or several laypeople, most likely women, were responsible for the production of this manuscript.","Nicole de Bretaigne and a Fifteenth-Century Collection of Religious Texts in French: Paris, Bibliothèque de l’Arsenal, MS 5366",Johns Hopkins University Press,2016,,Digital Philology: A Journal of Medieval Cultures,5,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1100,http://dx.doi.org/10.1353/dph.2016.0011,"{'string_id': '10.1353/dph.2016.0011', 'id_scheme': 'DOI'}","This article examines one of the decision-making moments in Petrarch’s editing of his Rerum vulgarium fragmenta (Rvf), when he erases the ballata “Donna mi vene spesso ne la mente” from the partial holograph Vaticano Latino 3195 and inserts the madrigal “Or vedi amor che giovenetta donna” over the erasure, creating a dynamic, palimpsestic relationship between the erased ballad and the Rvf. This shift in the making of the text represents the heart of the artistry of Petrarch’s visual poetics. The study of erasures and palimpsests in the partial holograph and other significant early witnesses helps us understand and trace the history of the work and affects our consideration of modern and contemporary editions of the text. The Petrarchive digital edition of the Rvf (&lt;http://petrarchive.org&gt;) implements new solutions, in the encoding and presentation of the edition, for exposing and highlighting the dynamic, palimpsestic features of Petrarch’s visual poetics.",Digital Representations and the Pivotal Instability of “Donna mi venne spesso ne la mente” in the Study of the Fragmenta,Johns Hopkins University Press,2016,,Digital Philology: A Journal of Medieval Cultures,5,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1101,http://dx.doi.org/10.1353/dph.2016.0012,"{'string_id': '10.1353/dph.2016.0012', 'id_scheme': 'DOI'}","This study examines two Franco-Italian manuscripts of the Huon d’Auvergne romance epic, Padua, Biblioteca del Seminario Vescovile, MS 32 and the Barbieri fragment, Bologna, Biblioteca dell’Archiginnasio, B. 3489. Using codicological evidence, the discussion connects these two material witnesses to literary trends of the fourteenth and fifteenth centuries, the period in which the manuscripts were prepared. This article approaches each witness as a unique iteration of the Huon d’Auvergne epic and argues that the Barbieri witness, even though fragmentary, should not be overlooked in an attempt to understand the literary culture of this epic.",A Contextual Analysis of Two Huon d’Auvergne Manuscripts,Johns Hopkins University Press,2016,,Digital Philology: A Journal of Medieval Cultures,5,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1102,http://dx.doi.org/10.1353/dph.2016.0013,"{'string_id': '10.1353/dph.2016.0013', 'id_scheme': 'DOI'}","This paper analyses Christine de Pizan’s inclusion and decoration of the Livre du chemin de lonc estude (Book of the Path of Long Study) in her masterpiece, the Queen’s Manuscript, London, British Library, Harley 4431. It shows how she directed the choice and placement of the miniatures in order to foreground the theme of her book as a gift of wisdom.","The Book as a Gift of Wisdom: The Chemin de lonc estude in the Queen’s Manuscript, London, British Library, Harley 4431",Johns Hopkins University Press,2016,,Digital Philology: A Journal of Medieval Cultures,5,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1103,http://dx.doi.org/10.1353/dph.2017.0001,"{'string_id': '10.1353/dph.2017.0001', 'id_scheme': 'DOI'}","A glowing tribute to Anne de France, duchess of Bourbon, appears on one of the final folios of Paris, BnF, MS fr. 24392, a fifteenth-century collection notable for its rehabilitation of Jean de Meun. The encomium, composed between 1488 and 1498, praises Anne for serving as regent for her brother, the future King Charles VIII. Citing her many attributes, the poet likens Anne's eloquence to that of Christine de Pizan. How and why he makes this comparison is the subject of this two-part article. I conclude that it is not by accident that the encomium poet added his compliment to Anne to Français 24392.","Remembering Christine de Pizan in Paris, BnF, MS fr. 24392, A Manuscript Owned by Anne de France, Duchess of Bourbon",Johns Hopkins University Press,2017,,Digital Philology: A Journal of Medieval Cultures,6,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1104,http://dx.doi.org/10.1353/dph.2017.0002,"{'string_id': '10.1353/dph.2017.0002', 'id_scheme': 'DOI'}","Despite the encyclopedic nature of Jean de Meun's Roman de la Rose, there are two topics that have been accorded a central place in the quagmire of debates, discussions, and digressions that make up the second Rose. They are the successful and guaranteed regeneration of the human species and the proper use of one's (sexual) tools. At the center of both these topics is the allegorical figure of Lady Nature. The comparative analysis of iconographic depictions of Lady Nature in two fourteenth-century manuscripts, Chicago, University of Chicago Library, 1380 and Paris, Sainte-Geneviève, 1126, and in the fifteenth-century manuscript, Paris, BnF, fr. 23492 reveal new interpretative textual linkages, such as the rapprochement of Lady Nature's confession with Pygmalion's creation of his female statue.",Lady Nature in Word and Image in Jean de Meun's Roman de la Rose,Johns Hopkins University Press,2017,,Digital Philology: A Journal of Medieval Cultures,6,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1105,http://dx.doi.org/10.1353/dph.2017.0003,"{'string_id': '10.1353/dph.2017.0003', 'id_scheme': 'DOI'}","Few manuscripts explore Jean de Meun's philosophical anthropology—particularly its engagement with women and sexuality—as directly as does a mid-fourteenth-century miscellany preserved in the Bibliothèque municipale of Dijon, MS 525. Compiled by a scribe named Mathias du Rivau between 1355 and 1362, the miscellany includes a series of texts that either influenced Jean's Rose, or were influenced by it. Satiric and moral in nature, these texts form a counterpoint to the Rose that riffs on Jean's concept of dialectical opposites—contraires choses—particularly in the closing sections (as suggested by Mathias's marginal annotations). Key to the critical dynamic of the miscellany is a group of four texts—identified as prosa mulierum—immediately following the end of the Rose. Scholars who have studied this manuscript have dismissed these texts as misogynistic or even (anachronistically) ""anti-feminist."" Such readings ignore the dialectical context of Dijon 525. Above all, they ignore the nuanced way that the prosa mulierum engage the disparate panoply of Jean de Meun's perspectives on the subject.",Codex as Critic: One Manuscript's Dialogue with the Romance of the Rose,Johns Hopkins University Press,2017,,Digital Philology: A Journal of Medieval Cultures,6,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1106,http://dx.doi.org/10.1353/dph.2017.0004,"{'string_id': '10.1353/dph.2017.0004', 'id_scheme': 'DOI'}","Bespoke software developed during a transcription project on London, British Library, MS Harley 4431 makes it possible to analyze this very large corpus. Investigations so far have concentrated on variant spellings and the textual ornamentation; these are discussed in detail here. Further, guidance is given for other scholars wishing to use the software to examine the manuscript.","Patterns and Fingerprints in London, British Library, MS Harley 4431",Johns Hopkins University Press,2017,,Digital Philology: A Journal of Medieval Cultures,6,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1107,http://dx.doi.org/10.1353/dph.2017.0005,"{'string_id': '10.1353/dph.2017.0005', 'id_scheme': 'DOI'}","This article gives the first complete list of all the known manuscripts of Christine de Pizan's Livre des fais d'armes et de chevalerie (1410) according to their most salient feature, first identified by A. T. P. Byles: those that identify Christine as author and those that remove the female author figure from the text. The article also provides a fuller description of the manuscripts than has hitherto been published and elaborates on the differences noted by Byles between the two major groups, with a view to aiding the selection of manuscripts for a critical edition of this text. Finally, the article suggests a new possible motive behind the initial erasure of Christine's authorship, which affected a large number of manuscripts.",The Manuscripts of the Livre des fais d'armes et de chevalerie,Johns Hopkins University Press,2017,,Digital Philology: A Journal of Medieval Cultures,6,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1108,http://dx.doi.org/10.1353/dph.2017.0009,"{'string_id': '10.1353/dph.2017.0009', 'id_scheme': 'DOI'}","This essay addresses the reception of the New Philology, and its more recent iteration, material philology, in German medieval studies. Whereas the New Philology elicited a largely skeptical response from practitioners of textual criticism and editorial philology, approaches informed by cultural studies have been more open to a constructive engagement with its tenets. Nevertheless, the question remains open as to whether German medieval studies have really followed the lead given by material philology and placed the manuscript as a physical object at the center of research to the extent that is often claimed.","Transmission and Materiality: Philology, Old and New, in German Medieval Studies",Johns Hopkins University Press,2017,,Digital Philology: A Journal of Medieval Cultures,6,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1109,http://dx.doi.org/10.1353/dph.2017.0010,"{'string_id': '10.1353/dph.2017.0010', 'id_scheme': 'DOI'}","With almost twenty years of expertise in the field of computer methods in the humanities, the Center for Digital Humanities at the University of Trier has recently been partner in two projects in the field of medieval manuscript studies: Virtuelles Skriptorium St. Matthias reconstructed the stock of medieval texts in the abbey of St Matthias in Trier by digitizing and publishing more than 450 manuscripts. eCodicology aimed to develop, test, and optimize new algorithms that automatically detect the macro- and microstructural layout elements on manuscript pages. This article discusses the workflows and technologies of both projects with a focus on the potential and limits of machine-reading technologies.",Automatic Layout Analysis and Storage of Digitized Medieval Books,Johns Hopkins University Press,2017,,Digital Philology: A Journal of Medieval Cultures,6,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1110,http://dx.doi.org/10.1353/dph.2017.0011,"{'string_id': '10.1353/dph.2017.0011', 'id_scheme': 'DOI'}","The world-famous Bibliotheca Palatina holds more than 3,000 medieval manuscripts, which are today divided between the Vatican Library in Rome and the Heidelberg University Library. This article discusses the academic, technical, and organizational challenges and solutions for a digitization project that is now drawing to its close. Its goal has been to digitally reconstruct the Bibliotheca Palatina and to make it available, as one single online edition, to scholars all over the world, with new opportunities for access and research.",Digitization at the Heidelberg University Library: The Digital Bibliotheca Palatina Project,Johns Hopkins University Press,2017,,Digital Philology: A Journal of Medieval Cultures,6,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1111,http://dx.doi.org/10.1353/dph.2017.0012,"{'string_id': '10.1353/dph.2017.0012', 'id_scheme': 'DOI'}","Austria has a unique manuscript landscape in the German-speaking world, characterized both by the high number of surviving medieval manuscripts relative to its population, and by the survival of monastic libraries, which were not affected by the secularization that took place in German lands in 1806. This article surveys the range of scholarly projects in Austria that deal with medieval German texts and their online presentation, with a particular focus on the role of the Austrian Academy of Sciences and its Department of Paleography and Codicology, and the research portal manuscripta.at.",manuscripta.at: A Portal for Medieval Manuscripts Held in Austrian Libraries,Johns Hopkins University Press,2017,,Digital Philology: A Journal of Medieval Cultures,6,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1112,http://dx.doi.org/10.1353/dph.2017.0013,"{'string_id': '10.1353/dph.2017.0013', 'id_scheme': 'DOI'}","For over ten years, e-codices–Virtual Manuscript Library of Switzerland has been publishing medieval and select modern manuscripts from Swiss collections online at www.e-codices.ch. The website links highquality digital reproductions with descriptive metadata, all of which are searchable. The digital library offers free public access in four languages (English, French, German, and Italian). The home institution of e-codices is the Medieval Institute at the University of Fribourg (Switzerland). This article presents e-codices along with information regarding technical specifications, metadata and digitization. It highlights some of the digital manuscript library's special features and interesting projects, and it addresses challenges for the future of digital infrastructures such as e-codices.",e-codices–Virtual Manuscript Library of Switzerland: Great Achievements and Even Greater Challenges to Come,Johns Hopkins University Press,2017,,Digital Philology: A Journal of Medieval Cultures,6,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1113,http://dx.doi.org/10.1353/dph.2017.0014,"{'string_id': '10.1353/dph.2017.0014', 'id_scheme': 'DOI'}","Proceeding from Walter Benjamin's ideas on technical reproduction, this essay examines the phenomenon of medieval manuscript culture being reduced and preserved through the medium of digital editing. This effect is demonstrated with reference to the Bern Parzival Project, which captures Wolfram von Eschenbach's romance (ca. 1200) in its rich manuscript transmission. Various components of the electronic edition, including a synopsis of different textual versions, single manuscript transcriptions, and unrooted phylograms borrowed from microbiology for the purpose of documenting witness relationships are discussed here in the context of theoretical issues situated on the boundaries between the sciences and the humanities.","Copying, Emergence, and Digital Reproduction: Transferring Medieval Manuscript Culture into an Electronic Edition",Johns Hopkins University Press,2017,,Digital Philology: A Journal of Medieval Cultures,6,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1114,http://dx.doi.org/10.1353/dph.2017.0015,"{'string_id': '10.1353/dph.2017.0015', 'id_scheme': 'DOI'}","This essay considers the problem of normalized orthography in critical editions of Middle High German texts. The conventions devised by Lachmann, Benecke, and Grimm in the nineteenth century have always been controversial, but the problem is especially urgent now that digital reproductions afford direct access to manuscripts. Drawing on their experience of editing the Kaiserchronik, the authors advocate a print spelling from which a certain amount of scribal variation is filtered out, but in such a way as to respect and reflect the historic writing system of the medieval manuscript. This, they contend, is one of the main tasks for a responsible philology in the digital age.",Responsible Philology: Editing the Kaiserchronik in the Digital Age,Johns Hopkins University Press,2017,,Digital Philology: A Journal of Medieval Cultures,6,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1115,http://dx.doi.org/10.1353/dph.2018.0001,"{'string_id': '10.1353/dph.2018.0001', 'id_scheme': 'DOI'}","Several recent works have addressed the question of the extent to which recent advances in digital scholarship imply a revolution in scholarly editing practice. The current article addresses claims of a qualitative leap in the scholarly editing and suggests that while digital means present many advantages, digital editing practice is less far removed from print scholarly editing than might otherwise be thought.",Digital versus Analogue Textual Scholarship or The Revolution is Just in the Title,Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1116,http://dx.doi.org/10.1353/dph.2018.0002,"{'string_id': '10.1353/dph.2018.0002', 'id_scheme': 'DOI'}","This essay will examine what we define as digital-medieval manuscript culture, the production, dissemination, and use, scholarly or otherwise, of digital surrogates of medieval manuscripts. As we argue here, digital-medieval manuscript culture is continually in process and changing, as our tools, like those of the Middle Ages, evolve. In order to theorize digital manuscript surrogates—which need not be facsimiles, but rather any digital text or object intended to represent the contents and/or physical presence of a manuscript—as hyper-remediations that foreground medieval technologies in new media, we consider five interrelated aspects of digital-medieval manuscript culture: likeness, tangibility, presence, time, and intention(s).",On Digital-Medieval Manuscript Culture: A Tentative Manifesto,Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1117,http://dx.doi.org/10.1353/dph.2018.0003,"{'string_id': '10.1353/dph.2018.0003', 'id_scheme': 'DOI'}","Using the Estoria Digital as a case study, I examine some practical issues that may be encountered in collaborative digital editing projects of medieval texts: I describe some of the practical issues we encountered when creating the Estoria Digital, and some of the strategies we employed to overcome them. Focussing on transcription, collation, and regularization, I propose a model for their effective completion by more than one person. Finally, I consider if collaborative digital editing might be a misnomer in this context: what do we mean by collaborative and editing in the collaborative digital editing of medieval texts?",The Practicalities of Collaboratively Digitally Editing Medieval Prose: The Estoria de Espanna Digital Project as a Case Study,Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1118,http://dx.doi.org/10.1353/dph.2018.0004,"{'string_id': '10.1353/dph.2018.0004', 'id_scheme': 'DOI'}","Collation as an element in the production of digital critical editions is no longer in its infancy. The current article, based on the experience of editing a lengthy medieval prose text for the Estoria de Espanna Digital, addresses the theoretical and practical implications of using digital tools to collate extensive passages of medieval text.",The Estoria de Espanna Digital: Collating Medieval Prose-Challenges… and More Challenges,Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1119,http://dx.doi.org/10.1353/dph.2018.0005,"{'string_id': '10.1353/dph.2018.0005', 'id_scheme': 'DOI'}","Could invisible translators be the key to seeing invisible medieval authors? Medieval manuscripts and modern editions give us texts that combine signals of original authorship with interfering signals of scribal authorship, dialectical variation, etc. This article suggests a way to use medieval texts in modern translation to isolate authentic signals of authorship. By using translation as a signal-processing filter and applying previously tested stylometric techniques based on character n-grams, the present study detects a different signal for Guillaume d'Angleterre than those found for other texts attributed to Chrétien de Troyes, suggesting different authorship.","Invisible Translator, Visible Author: Medieval Authorship Verification Through Modern Translation",Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1120,http://dx.doi.org/10.1353/dph.2019.0000,"{'string_id': '10.1353/dph.2019.0000', 'id_scheme': 'DOI'}","This essay considers the literary influence of the Ramon Llull's Llibre de l'orde de cavalleria at the end of the Middle Ages, both in its relatively few surviving Catalan manuscripts, and in its much more frequently found middle French translation, as the Livre de l'ordre de chevalerie. After an introductory survey of the known manuscripts, the essay examines in detail the work's diffusion and its translations, as well as its later adaptations. Finally, there is a detailed consideration of tradition, influence, and imitations. In each of these cases, the middle French translation is shown to be particularly important and influential.","The French Translation of the Llibre de l'orde de cavalleria: The Diffusion, Translation, and Adaptation of Ramon Llull's Text",Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1121,http://dx.doi.org/10.1353/dph.2019.0001,"{'string_id': '10.1353/dph.2019.0001', 'id_scheme': 'DOI'}","This essay argues two points concerning the systematic use of the protagonists' dreams in the fifteenth-century romance Paris et Vienne, focusing on the single manuscript composed at the Burgundian Court during the 1450s, the base MS for the new edition of de Crécy and Brown-Grant. First, it argues that these dreams form a coherent, important program in the work as a whole, directly linked to the structure of its narrative. Secondly, it argues that this program functions in part by referring to a similar oneiric program in the thirteenth-century Roman de la Rose, in terms of the (ambiguous) meaning of dreams as fictional discourse.",The Dream Program in Paris et Vienne and Its Relation with the Roman de la Rose,Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1122,http://dx.doi.org/10.1353/dph.2019.0002,"{'string_id': '10.1353/dph.2019.0002', 'id_scheme': 'DOI'}","Alain Chartier's Livre de l'Espérance is widely considered the chef d'oeuvre of his opus, melding the personal, the political, and the spiritual into a catechesis of Christianity. Its debt to Boethius, the Bible, and patristic writings has been analyzed. There may be another source less acknowledged: the Commedia of Dante Alighieri. Is it possible that Chartier was influenced by Dante's work? This essay traces the evidence and possibilities for such a textual encounter. Establishing the influence of Dante on Chartier allows further recognition of the paths by which humanist ideas from Italy entered France during the transitional fifteenth century.",Alain Chartier's Livre de l'Espérance: A Remodeling of Dante's Commedia?,Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1123,http://dx.doi.org/10.1353/dph.2019.0003,"{'string_id': '10.1353/dph.2019.0003', 'id_scheme': 'DOI'}","This article focuses on the iconography of Guillaume de Deguileville's Le pelerinage de vie humaine, first printed in its prose version in Lyon in 1485. The iconography here functions as a vehicle for the global identity of the text, ensuring the permanence of this identity over the process of translation. Henrich Mayer, a German printer and editor in Toulouse, preserved the iconography at the cost of adaptations and significant adjustments in the Castilian Peregrino de la vida humana, which was published in 1490. The editor treated the illustrations as the locus of textual filiation.","Traveling Images: The Illustrations of Guillaume de Deguileville's Livre du Pelerin de vie humaine (Mathis Husz edition, Lyon, 1485) and its Castilian Translation by Vincente de Mazuelo (Heinrich Mayer edition, Toulouse, 1490)",Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1124,http://dx.doi.org/10.1353/dph.2019.0007,"{'string_id': '10.1353/dph.2019.0007', 'id_scheme': 'DOI'}","In fourteenth-century France, Charles V's official programs of translations into French have typically been considered the instrument of a royal political agenda and concepts of translatio as a transfer of power from the clerical milieu to the lay court. However, this essay focuses on the works of one of the king's preeminent translators Nicole Oresme, more from the cultural standpoint of the translator than the king's political one. Oresme's scholastic and cultural background helps understanding how translating was an all-encompassing complex intellectual process for him and that any of the political stands that arose from his process reflected personal philosophical beliefs and worldview.",Nicole Oresme's Cultural Translatio in Question,Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1125,http://dx.doi.org/10.1353/dph.2019.0008,"{'string_id': '10.1353/dph.2019.0008', 'id_scheme': 'DOI'}","The epistles and political speeches of Jean Juvénal des Ursins contain varied ideas on language. This essay attempts to offer significant insights into his linguistic considerations. An analysis of the language and translation principles, in the works of the bishop of Charles VII of France, suggests a subtle but strong political activism at a time when nationalism was emerging across Europe. The systematic use of Latin to reflect on modern language can be considered a militant act in the context of the restoration of King of France's prerogatives. Jean Juvénal's militancy also appears in the allegorical devices he uses, which are based on morphological manipulations.","Jean Juvénal des Ursins, Linguist and Translator? Policies and Memory of Language in Late Medieval France",Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1126,http://dx.doi.org/10.1353/dph.2019.0009,"{'string_id': '10.1353/dph.2019.0009', 'id_scheme': 'DOI'}","Knowledge of Guillaume de Machaut's literary and musical works is attested since 1380 in the Francophile court of Aragon, where his art exerted an influence that endured well into the fifteenth century. Catalan poets such as Andreu Febrer and Pere Torroella were greatly influenced by Machaut's poetry. This can be seen in the way they adapted the lay and ballade forms, and in the French section of chansonnier Vega-Aguiló (Barcelona, Biblioteca de Catalunya, MSS 7–8), an important witness to the poetry that originated with the troubadours and was fashionable in the Crown of Aragon until the end of the fifteenth century. This article approaches the role of Machaut's poetry and music in the construction of a literary canon in the Crown of Aragon.","Guillaume de Machaut at the Court of Aragon, 1380–1430",Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1127,http://dx.doi.org/10.1353/dph.2019.0010,"{'string_id': '10.1353/dph.2019.0010', 'id_scheme': 'DOI'}","The medieval Catalan translation of Livy's Decades, which is extant in London, British Library, MS Harley 4893, translates the fourteenth-century French version of Livy by Pierre Bersuire. The textual study of the Catalan translation and the French version suggests that the French manuscript used to produce the Catalan translation came from the court of the Duke of Berry soon after 1383, when John of Aragon (future King John I) had asked the Duke to send him a copy of the French Decades.",Reconstructing the Text: The Fourteenth-Century Catalan Translation of Livy,Johns Hopkins University Press,2018,,Digital Philology: A Journal of Medieval Cultures,7,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1128,http://dx.doi.org/10.1353/dph.2019.0012,"{'string_id': '10.1353/dph.2019.0012', 'id_scheme': 'DOI'}",A remarkable set of late medieval European epics and histories tell of Charlemagne's youth in al-Andalus and his marriage to a converted Saracen princess. This essay considers the cultural work of these narratives by analyzing them as instances of genealogical self-fashioning and interimperial translation in the medieval Mediterranean.,Charles in al-Andalus,Johns Hopkins University Press,2019,,Digital Philology: A Journal of Medieval Cultures,8,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1129,http://dx.doi.org/10.1353/dph.2019.0013,"{'string_id': '10.1353/dph.2019.0013', 'id_scheme': 'DOI'}","An important trend in scholarship is attending to the metamorphoses of particular folktales across regions, periods, and religions, tracking what such changes reveal about the direction of cultural transmission. Part of this trend is looking beyond circulation in Europe to circulation across south-south boundaries, or even from south to north, the latter of which I have done much to try to forward. In this essay, however, I consider an instance of ostensible Europe-to-Africa textual transfer: the body of folktales that circulated around the medieval world about miracles performed by Saint Mary, the mother of Jesus. The Ethiopian Marian tales, which appear in a compilation text titled Täˀammərä Maryam (Miracles of Mary), are sometimes mistakenly said to have arisen solely in Europe. To explore the issue of influence, I examine a Marian tale innovated in Ethiopia titled ""The Cannibal of Qəmər"" (or ""The Miracle of the Man-Eater"" or ""The Story of Belai""). Reading the cannibal tale diachronically, relationally, allegorically, and intertextually reveals much. The Marian tale template first devised in the Levant persists across multiple boundaries yet was radically adapted in Ethiopia, underscoring why care is required when discussing African narrative adaptation of outside texts.","Mary Saves the Man-Eater: Value in the Medieval Ethiopian Marian Miracle Tale of ""The Cannibal of Qəmər""",Johns Hopkins University Press,2019,,Digital Philology: A Journal of Medieval Cultures,8,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1130,http://dx.doi.org/10.1353/dph.2019.0014,"{'string_id': '10.1353/dph.2019.0014', 'id_scheme': 'DOI'}","In 2015 Pope Francis released Laudato Si', an encyclical call to arms on environmental issues, declaring climate change a problem of urgent proportions. The letter received extraordinary levels of global coverage and was hailed as a necessary and timely intervention by a range of politicians, policymakers, and public intellectuals, many of them avowed secularists. Laudato Si' was able to achieve acclaim and influence because it was immediately available via the Internet—a divinely inspired press release, so to speak. At the same time, Laudato Si' defined itself in terms of the medieval church, taking its name from a poem by Saint Francis and grounding its claims on the philosophy of Saint Thomas Aquinas. In so doing, Laudato Si played with multiple temporal registers, drawing on an unbroken spiritual tradition to issue a manifesto within a broken modernity.","""Nothing in this world is indifferent to us"": Laudato Si' as Global and Planetary Medievalism",Johns Hopkins University Press,2019,,Digital Philology: A Journal of Medieval Cultures,8,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1131,http://dx.doi.org/10.1353/dph.2019.0015,"{'string_id': '10.1353/dph.2019.0015', 'id_scheme': 'DOI'}","Since the death of Thailand's King Bhumibol Adulyadej in 2016, his image has continued to play a defining role in the articulation of Thainess (khwampenthai) in reference to the country's ""authentic"" premodern past and its teleological ""progress."" This essay addresses several paradoxes in this image and its relationship to Thailand's embattled political history. How might the royal photograph be both sacred and secular? How are premodern understandings of kingship, including the taboo on the monarchy's public representation, reconciled with the contemporary hypervisibility of the monarchy? And what implications do references to the premodern past have for debates over defining democracy in Thailand?",The Feudal Photograph of a Democratic Dhammaraja: Secularism and Sacrality in Thai Royal Imagery,Johns Hopkins University Press,2019,,Digital Philology: A Journal of Medieval Cultures,8,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1132,http://dx.doi.org/10.1353/dph.2019.0016,"{'string_id': '10.1353/dph.2019.0016', 'id_scheme': 'DOI'}","This essay synthesizes the perspectives of eleven participants in a colloquium that brought together experts in medieval language and culture, second-language acquisition, and video game production to assess the use of three-dimensional (3D) immersive environments for learning about inaccessible or lost cultures. We suggest that technology offers a unique method for experiencing the past; and when harnessed effectively, immersive environments can increase empathy and learning while appealing to a new generation of students.",Revisioning the Global Middle Ages: Immersive Environments for Teaching Medieval Languages and Culture,Johns Hopkins University Press,2019,,Digital Philology: A Journal of Medieval Cultures,8,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1133,http://dx.doi.org/10.1353/dph.2019.0017,"{'string_id': '10.1353/dph.2019.0017', 'id_scheme': 'DOI'}","The inclusion of the medieval Portuguese chivalric romance ""The Twelve of England"" into Luís Vaz de Camões's Os Lusíadas (1572) links Vasco da Gama's voyage to India and subsequent Portuguese expansionism to specific historical moments of national foundation narrated in the epic. This essay argues that Magriço (Álvaro Gonçalves Coutinho), the protagonist of ""The Twelve of England,"" embodies a larger theme in the epic of national belatedness that is integral to the poem's engagement with the temporalities of empire and cultural inheritance. But the Lusiads's utility as a cultural artifact celebrating the rise of new forms of knowledge and power also provides the conceptual basis for its symbolic appropriation by Portugal's imperial rivals, Spain and England. The essay concludes by examining representations of Camões in twentieth-century Brazilian folhetos (""leaflets""), or literatura de cordel, to show how the Lusiads's anxieties about textual survival and generativeness provide a discursive frame for Camões's later textual incarnations.",Oceanic Epic: The Translations of the Lusiads in the Global Renaissance,Johns Hopkins University Press,2019,,Digital Philology: A Journal of Medieval Cultures,8,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1134,http://dx.doi.org/10.1353/dph.2019.0018,"{'string_id': '10.1353/dph.2019.0018', 'id_scheme': 'DOI'}","This article tracks the global trajectories of a tale of a scholarly slave girl (her name in Arabic is Tawaddud) who wins a knowledge contest over the greatest scholars of her time. This essay investigates what made the tale so gripping to its medieval readers and translators: its uncoupling of knowledge from mastery. I use Deleuze's and DeLanda's discussions of assemblage theory to consider how Arabic versions of the tale link heterogeneous fields and genres of knowledge into a fantasy of feminine, enslaved omnicompetence, and also how the tales' global transmigrations create a network of multifaceted and larger-than-Eurocentric research, informed but not deliminated by colonialism and decolonization.",Tawaddud/Teodor and the Stripping of Medieval Mastery,Johns Hopkins University Press,2019,,Digital Philology: A Journal of Medieval Cultures,8,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1135,http://dx.doi.org/10.1353/dph.2019.0022,"{'string_id': '10.1353/dph.2019.0022', 'id_scheme': 'DOI'}","Old English poems are difficult to date, largely anonymous, and written in a formulaic idiom, making it difficult to establish authorship and intertextual relationships among various poems. This essay employs computational stylometry—specifically, n-gram analysis—and network analysis to address this problem. It proposes methods for screening out chance n-grams and for employing n-grams of various lengths within a single analysis to show that, in Old English poetry, formulaic diction exists at the level of idiolect, sociolect, and language. Specifically, the signed poems of Cynewulf evince idiolectal characteristics; and along with Andreas, Christ III, The Phoenix, and Guthlac A and B, they form a poetic subtradition (sociolect).",Using N-gram Analysis to Map Intertextual Networks in Old English Verse,Johns Hopkins University Press,2019,,Digital Philology: A Journal of Medieval Cultures,8,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1136,http://dx.doi.org/10.1353/dph.2019.0023,"{'string_id': '10.1353/dph.2019.0023', 'id_scheme': 'DOI'}","This methodological article researches all available documentary sources regarding the ownership, making, and circulation of books within the royal House of Aragon (ca. 1300–1460) in order to study the creation of personal libraries linked to the royal house and investigate how individual books were transmitted from one personal library to another. The goal is to better define the intellectual profile of some of the kings, queens, and princes of the House of Aragon, such as James II, Peter III, Martin I, the young Alfonso the Magnanimous, Maria of Castile, and Charles Prince of Viana.",The Books of the Kings of Aragon: From James II to Alfonso IV,Johns Hopkins University Press,2019,,Digital Philology: A Journal of Medieval Cultures,8,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1137,http://dx.doi.org/10.1353/dph.2019.0024,"{'string_id': '10.1353/dph.2019.0024', 'id_scheme': 'DOI'}","As with so many of his colleagues working elsewhere in the Crown of Aragon, the Valencia-based printer and bookseller Joan Rix de Cura (ca. 1450–90) is typical of those figures responsible for the introduction of the printing press in the Iberian peninsula. By a stroke of good fortune, the inventory of Rix de Cura's bookshop holdings, totalling 5,261 volumes, has been preserved. An invaluable source for the circulation of classical and humanistic texts in late-fifteenth-century Valencia, the inventory includes an astonishing number of Latin books (with an indication of the number of copies held at the bookshop), mostly imported from Venice.",Books and Readers in Late-Fifteenth-Century Valencia: The Inventory of Joan Rix de Cura (1490),Johns Hopkins University Press,2019,,Digital Philology: A Journal of Medieval Cultures,8,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1138,http://dx.doi.org/10.1353/dph.2019.0025,"{'string_id': '10.1353/dph.2019.0025', 'id_scheme': 'DOI'}","Stand-off markup is widely considered to be a possible solution for overcoming the limitation of inline XML markup, primarily when dealing with multiple overlapping hierarchies. Considering previous contributions on the subject as well as the implementations of stand-off markup, we propose a new TEI-based model for encoding that still uses the regular TEI elements but in a stand-off manner. Our light notation moves the bulk of markup into a separate &lt;standoff&gt; element, grouping layers of related textual features encoded via existing TEI elements (e.g., &lt;name&gt; or &lt;corr&gt;) into individual &lt;stf&gt; elements; furthermore, our proposed notation provides a schema for referencing the transcription using the xml:id attribute. This approach is illustrated through a variety of examples. Our proof-of-concept transformation package works directly on the stand-off markup without the necessity of reducing it back to inline TEI for parsing, querying, and visualizing.",XML-TEI Stand-off Markup: One Step Beyond,Johns Hopkins University Press,2019,,Digital Philology: A Journal of Medieval Cultures,8,2,"[{'value': '2162-9552', 'type': 'electronic'}]"
1139,http://dx.doi.org/10.1353/dph.2020.0001,"{'string_id': '10.1353/dph.2020.0001', 'id_scheme': 'DOI'}","This essay considers medieval precarity and vulnerability through the example of Thomas Hoccleve, whose Regiment of Princes attests to a broad and varied discourse about the effects of marginalization, contingency, and economic insecurity and exploitation in late medieval English literature. Drawing on Judith Butler and others' work on the ""politics of grief,"" I consider how Thomas Hoccleve uses his experience of personal loss and sorrow as an occasion to reflect on the precarious lives of those around him. In so doing, his work in the Regiment of Princes serves as an early example of how personal sorrow can become an imaginative catalyst for broader ethical reflection and advocacy",The Politics of Precarious Grief in Thomas Hoccleve's Regiment of Princes,Johns Hopkins University Press,2020,,Digital Philology: A Journal of Medieval Cultures,9,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1140,http://dx.doi.org/10.1353/dph.2020.0002,"{'string_id': '10.1353/dph.2020.0002', 'id_scheme': 'DOI'}","Petrus Alfonsi's medieval readers often categorized his Disciplina clericalis as a homiletic exemplum collection or as a mirror for princes. I will argue that beneath these layers of misreading, Petrus organizes a collection that emphasizes the subject position of the socially vulnerable: women, pilgrims, travelers, subordinates, apprentices, slaves, and the disabled. In this reading, the normative values of church and state become accruals. Beneath them, I reveal a Disciplina clericalis that it is strangely resonant in our time. It attends to the problem of premodern precarity, recurrently mapping that condition onto body in the form of vulnerability. Disciplina clericalis is less a handbook for preachers or a mirror for princes than a guide for the marginalized in a dangerous and exploitative world.",Sociality at the Margins in Petrus Alfonsi's Disciplina Clericalis,Johns Hopkins University Press,2020,,Digital Philology: A Journal of Medieval Cultures,9,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1141,http://dx.doi.org/10.1353/dph.2020.0003,"{'string_id': '10.1353/dph.2020.0003', 'id_scheme': 'DOI'}","This article explores able-bodied subjects' affective response to people with disabilities, as manifested in the ""tests"" and trials used to ascertain the authenticity of those disabilities. When children drop hot coals on the esh of a deaf-mute to see if he can scream, or when a cleric tricks blind beggars into spending a coin they do not possess, more than simple cruelty is at play (though these actions are undeniably cruel). Through readings of selected texts from twelfth- through fifteenth-century France – including Chrétien de Troyes's Cligès, the Miracles de saint Louis by Guillaume de Saint-Pathus, the fabliau Les trois aveugles de Compiègne, and a ballade by Eustache Deschamps – I examine what vulnerabilities are being exposed, protected, or celebrated through the administration of these tests. I argue that medieval literary tests of disability reveal privilege to be both an embodied and an intellectual or epistemic state: the prospect of a feigned disability exposes an unsettling (because unresolvable) fragility in the mechanisms of knowing.",Able-Bodied Fragility,Johns Hopkins University Press,2020,,Digital Philology: A Journal of Medieval Cultures,9,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1142,http://dx.doi.org/10.1353/dph.2020.0004,"{'string_id': '10.1353/dph.2020.0004', 'id_scheme': 'DOI'}","In the 13th-century text Ami et Amile, Ami is willingly stricken with leprosy after he replaces Amile in the judicial battle to prove his friend's innocence, having been warned beforehand about the affliction by an angel. This paper explores the notion of vulnerability through the lens of leprosy. Medieval society regarded the leper as being touched by God, a physical manifestation of an inward transgression. Ami, as a leper, incites both fear and longing: the disease excludes him from society due to its contagious and incurable nature. Concurrently, it provides the opportunity for charity by tending to the sick and lowly. In this case, it allows Amile and his children to perform charitable Christian sacrifices to save Ami: Amile murders his own children for the blood cure, while the children die for the benefit of another. Their sacrifices are rewarded as the children are restored to life and Amile is forgiven the infanticide, revealing a logic of vulnerability that applies to the characters as well as the political and social institutions in place. In the end, it is vulnerability that permits ultimate redemption and allows secular personages to achieve spiritual elevation.",Precarious Bodies in the Old French Ami et Amile,Johns Hopkins University Press,2020,,Digital Philology: A Journal of Medieval Cultures,9,1,"[{'value': '2162-9552', 'type': 'electronic'}]"
1143,http://dx.doi.org/10.16995/dm.10,"{'string_id': '10.16995/dm.10', 'id_scheme': 'DOI'}","Work on the documentary edition of Huntington Library manuscript Hm 114 of Piers Plowman (Ht) has demonstrated that it is difficult yet possible to use TEI-inspired markup in an experimental way within the scope of a larger TEI-conformant project. Moreover, such experiment is essential. Although much productive work can be done with specifications limited to the texts of manuscripts—including texts as eccentric as that of Ht—such texts cannot be fully modeled and examined without the application of experimental markup.The attempt to carry out such experimentation within the context of the larger TEI-conformant Piers Plowman Electronic Archive (PPEA) project demonstrates the need to develop community-wide recommendations for best practices in controlled experimentation. Text, scribal paratext, and codex co-exist in every manuscript artifact and present patterns of evidence—as yet only partially provided for in the TEI P4 and emerging P5 recommendations—that are eminently suited to machine analysis.",Experimental markup in a TEI-conformant setting,Open Library of the Humanities,2006,,Digital Medievalist,2,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1144,http://dx.doi.org/10.16995/dm.11,"{'string_id': '10.16995/dm.11', 'id_scheme': 'DOI'}","This article reports on three case studies of research projects looking at the conversion of legacy resources. The first of these, the CURSUS Project (http://www.cursus.uea.ac.uk), has created electronic editions of medieval Benedictine liturgical service books. The second was some personal research into proof-of-concept conversion of printed volumes of the Records of Early English Drama project (http://www.reed.utoronto.ca/). The final case formed part of a pilot project studying the problems and possibilities of converting legacy electronic resources archived by the Oxford Text Archive (http://www.ota.ox.ac.uk/). While each of these projects was converting from a different form of media—manuscript, print and electronic—they benefitted from many of the same techniques and overcame many of the same hurdles.","Liturgy, Drama, and the Archive: Three conversions from legacy formats to TEI XML",Open Library of the Humanities,2006,,Digital Medievalist,2,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1145,http://dx.doi.org/10.16995/dm.15,"{'string_id': '10.16995/dm.15', 'id_scheme': 'DOI'}","This paper considers the application of image-processing and data-mining to the analysis of scribal hands. The work of forensic document analysts on feature-extraction is considered, particularly the algorithms developed for automatic handwriting-recognition by Srihari, and by Bulacu and Schomaker. Automatic clustering is also considered using the AutoClass package. Preliminary results of the author’s own experiments with these approaches are presented, and some of the obstacles are outlined which must be overcome before a practical system can be developed for the automatic identification of medieval scribes.",Palaeography and Image-Processing: Some Solutions and Problems,Open Library of the Humanities,2007,,Digital Medievalist,3,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1146,http://dx.doi.org/10.16995/dm.16,"{'string_id': '10.16995/dm.16', 'id_scheme': 'DOI'}","This article discusses the Digital Image Archive of Medieval Music (DIAMM), and the ground-breaking work they have done in digitizing manuscript sources for medieval music. In many cases this has involved digital reconstruction where the notation is illegible, erased, or hidden under later texts. Through their good example DIAMM have been instrumental in educating photographers in archives and researchers alike in the proper methodology of digital photography with regard to medieval manuscripts. Their standards are especially high in capturing the images and the metadata they store. This enables them to undertake the sometimes miraculous digital restoration. The article covers the methodology of the DIAMM project, the delivery of images, and some of the problems they have encountered.",Digital Image Archive of Medieval Music: The evolution of a digital resource,Open Library of the Humanities,2008,,Digital Medievalist,3,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1147,http://dx.doi.org/10.16995/dm.19,"{'string_id': '10.16995/dm.19', 'id_scheme': 'DOI'}","This paper discusses the value of the electronic medium of publication for one project in particular, Inscriptions of Aphrodisias (2007), and uses this one example to extrapolate and illustrate a use-case paradigm of electronic publication and its advantages for research, pedagogy, and dissemination. The categories used in this study are: accessibility, scale, media, hypertext, updates, and iterative research and transparency. Not all of these categories apply equally to any given publication, and the discussion focuses on those that are the most appropriate to a digital publication of tagged epigraphic texts and images.",The Inscriptions of Aphrodisias as electronic publication: A user's perspective and a proposed paradigm,Open Library of the Humanities,2008,,Digital Medievalist,4,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1148,http://dx.doi.org/10.16995/dm.20,"{'string_id': '10.16995/dm.20', 'id_scheme': 'DOI'}","In many ways the Roman province of Baetica is an ideal subject for exploring new approaches to historic transport geography. This is not due to the completeness of its record (for it is not), but because it provides a remarkable breadth of pertinent data. This paper, loosely based on a seminar hosted by the Digital Classicist at King’s College London, will briefly discuss the results of applying some as-yet relatively uncommon techniques to the archaeology and documentary record of transport in the area. It will then go on to tackle some more general issues in creating maps of movement in the past, concluding that there is still much theoretical work to be done, but that the potential for discovering new patterns in old data is great, and indeed, ever growing. The main concept that will be explored is that of a Node Network, an abstract model of the interactions between spatially separate locations. This paper demonstrates the potential of a standard relational database, coupled with a GIS and Network Analysis software package, to make a spatial argument about the relative importance of key towns within a transport network and expose the constituent elements of that argument in a formal, visual manner.",The application of network analysis to ancient transport geography: A case study of Roman Baetica,Open Library of the Humanities,2008,,Digital Medievalist,4,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1149,http://dx.doi.org/10.16995/dm.25,"{'string_id': '10.16995/dm.25', 'id_scheme': 'DOI'}","This paper presents an overview of data acquisition and processing procedures of an interdisciplinary project of philologists and image processing experts aiming at the decipherment and reconstruction of damaged manuscripts. The digital raw image data was acquired via multi-spectral imaging. As a preparatory step we developed a method of foreground-background separation (binarisation) especially designed for multi-spectral images of degraded documents. On the basis of the binarised images further applications were developed: an automatic character decomposition and primitive extraction dissects the scriptural elements into analysable pieces that are necessary for palaeographic and graphemic analyses, writing tool recognition, text restoration, and optical character recognition. The results of the relevant procedures can be stored and interrogated in a database application. Furthermore, a semi-automatic page layout analysis provides codicological information on latent page contents (script, ruling, decorations).",Image Acquisition & Processing Routines for Damaged Manuscripts,Open Library of the Humanities,2011,,Digital Medievalist,6,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1150,http://dx.doi.org/10.16995/dm.26,"{'string_id': '10.16995/dm.26', 'id_scheme': 'DOI'}","The so-called Web 2.0 technologies bring with them new opportunities and new challenges in the field of scholarship. With social software we have a new set of tools with innovative possibilities and it is up to the community of practitioners in the area of the intersection between scholarship and technology to make effective use of them. This paper is part of the author's continuing research into the use of social software (blogs and wikis amongst others) as tools for education (meaning teaching and learning) and research. The primary interest is in how these new tools might facilitate cooperative learning and cooperative research, and help to build communities both of learning and of practice. Put simply, how can they be used to encourage and facilitate people working together, to be a medium for open collaboration; why is this necessary; and what perceived problems need to be addressed to make this happen?",Research communities and open collaboration: the example of the Digital Classicist wik,Open Library of the Humanities,2011,,Digital Medievalist,6,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1151,http://dx.doi.org/10.16995/dm.3,"{'string_id': '10.16995/dm.3', 'id_scheme': 'DOI'}","This article provides a brief introduction to Brown University's Virtual Humanities Lab (VHL)'s major new project: the electronic Esposizioni. The Esposizioni, like other texts available on Brown University's Decameron Web, is a major vernacular work by Boccaccio, and is the text of his unfinished lecture series on Dante's Commedia. The electronic Esposizioni project is fundamentally concerned with the definition of the relationship between two canonical medieval authors, Dante and Boccaccio, as expressed through the primary (commented) text and the secondary (commentary) text.The first part of the article provides an overview of the historical and literary contexts of Boccaccio's commentary on Dante. A great demand for explication and analysis of Dante's poem sprang up in Italy immediately after Dante's death in 1321, and Boccaccio's Esposizioni forms part of this tradition. However, unlike other Dante commentaries, the Esposizioni was not written with the intention of becoming a published book: it is instead the notes for the public lectures on Dante which Boccaccio gave in Florence in 1373-74. The oral intention and nature of this text is one of the elements upon which the electronic Esposizioni project focuses. One of the major benefits of the electronic medium is that we are able to distance the text from the material format of the bound book, which fixes the oral text as a canonical and immutable object.The second part of the article provides further information about how humanities computing has developed in the Italian Studies Department at Brown. Following a major grant from the National Endowment for the Humanities (NEH), the famous Decameron Web now forms part of the VHL, which has a dual role: to provide information about the civic, literary, and intellectual culture of Early Modern Italy to a broad audience of students and specialists alike; and to develop a variety of electronic tools for collaborative teaching and research. This section also considers the potential audience for the electronic Esposizioni project within the popular field of Dante studies, and compares the project to the already-established Dartmouth Dante Project, which is also concerned with Dante commentary.The final section of the paper presents the electronic edition of the Esposizioni, with a technical description of the commented and commentary texts, a discussion of the structural and semantic encoding principles, and some examples of the encoding in practice. Due to its nature as a hybrid medieval text, of which no autograph manuscript survives, Boccaccio's Esposizioni inevitably contains uncertainties. Rather then try to create a definitive online edition, we are thus constructing a Virtual Editing House which will allow scholars with privileged access the opportunity to comment and add their own annotations to the work. The project thus does not only present the commentary and commented text together for the first time in the digital medium, but will allow the creation of a new and ongoing commentary to Dante's poem.",Towards the electronic Esposizioni: the challenges of the online commentary,Open Library of the Humanities,2005,,Digital Medievalist,1,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1152,http://dx.doi.org/10.16995/dm.35,"{'string_id': '10.16995/dm.35', 'id_scheme': 'DOI'}","This paper summarizes how the Anglo-Norman Dictionary project has developed from an originally paper-based and print dictionary, to a fully digital, freely available online resource, comprising not only the Dictionary itself, but a substantial text-base in the same XML format as the Dictionary. The paper reviews the history of the Dictionary; the principles of structured encoding; the relationship between the citations (attestations) in the Dictionary, as instances of parole, and the broader Anglo-Norman language (the langue); and the detailed encoding of entries.","Bytes, Words, Texts: The Anglo-Norman Dictionary and its Text-Base",Open Library of the Humanities,2012,,Digital Medievalist,7,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1153,http://dx.doi.org/10.16995/dm.36,"{'string_id': '10.16995/dm.36', 'id_scheme': 'DOI'}","This paper examines the work done by the Australian Network for Early European Research (NEER) to build a national digital research community in this field. Funded through the Australian Research Council's Research Networks programme during the period 2005-2010, NEER's overall goal was to enhance the scale and focus of Australian research in medieval and early modern studies. Developing and implementing appropriate digital technologies was one of the main methods used to address this goal. In the end, NEER's digital programme produced three main services: a service for collaboration (Confluence), a service for the publication and storage of research outputs (PioNEER), and a service for identifying and engaging with the objects of this research (Europa Inventa). This paper evaluates the effect of these services on Early European research in Australia. It also considers their future, now that government funding for NEER has ended.",Building a Digital Research Community in Medieval and Early Modern Studies: The Australian Network for Early European Research,Open Library of the Humanities,2012,,Digital Medievalist,7,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1154,http://dx.doi.org/10.16995/dm.37,"{'string_id': '10.16995/dm.37', 'id_scheme': 'DOI'}","The aim of this paper is to present the first steps of the Arquibanc Project that has as a main objective to recover archives that are in private hands to make them available to the scientific community. The fact is that the access to these private documents is not always so easy and, in some cases, the preservation of the records cannot be assured. Therefore, our project focuses on creating two online databases that may allow the consultation and use of the documents for research purposes. The databases have been designed to provide a description of each document to which is associated a digital reproduction of the document easier to read on screen.","The ARQUIBANC Project: Location, Recovery, Arrangement, and Dissemination of Catalan Private Archives and Documents",Open Library of the Humanities,2012,,Digital Medievalist,7,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1155,http://dx.doi.org/10.16995/dm.38,"{'string_id': '10.16995/dm.38', 'id_scheme': 'DOI'}","Digital Mappaemundi (DM) is a resource under development to create open source tools for scholars to edit and annotate image and textual data content as linked data, and for other users to search within this rich content. For the purposes of development, our data have been medieval mappaemundi (""maps of the world"") and transcriptions of their geographical source texts.The second phase of DM's alpha development (2009-10) allows users to work with digital images of maps from medieval manuscripts, mark regions-of-interest within images, and associate textual annotations with those regions and then link one or more sets of digital texts to these regions, or target one or more words within these texts as targets to these regions. Scholars may create markers images with individual points, segmented lines, or custom polygonal shapes. Significantly, a scholar may identify any number of markers on any number of images as the targets for textual annotation and link them to any number of digital texts or locations within these texts. Additionally, a given marker may serve as the target for any number of textual annotations. Scholars may organize their annotations into groups called layers so that different research questions involving a single image may be addressed separately through annotation. Scholars may choose to view a single layer of annotation or view multiple layers of annotation overlaid on one another. A robust search function also allows users to organize the annotated content dynamically. At the time of this publication DM has undergone significant evolution in its phase three beta development, with applications for annotation and linked data beyond the original use case of medieval maps. For current functionality and features of the DM environment, as well as a list of medievalist projects using it, see http://ada.drew.edu/dmproject/.",Developing Digital Mappaemundi: An Agile Mode for Annotating Medieval Maps,Open Library of the Humanities,2012,,Digital Medievalist,7,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1156,http://dx.doi.org/10.16995/dm.39,"{'string_id': '10.16995/dm.39', 'id_scheme': 'DOI'}","New technologies and methodologies in the digital humanities can help alleviate some limitations inherent in the traditional methods of creating and publishing critical editions, especially how typical practices privilege major texts and create an artificial version of a text that obscures its textual history. I argue that those who work with manuscripts should place a greater emphasis on creating digital noncritical editions that will capture traditionally lost transcription work, harness community expertise, and create a vast interdisciplinary textual archive. This article describes some key benefits of a Platonic web-based transcription tool that will encourage large-scale collaborative transcription and editing in order to make manuscripts much more visible, accessible, connectable, correctable, and usable.",New Textual Traditions from Community Transcription,Open Library of the Humanities,2012,,Digital Medievalist,7,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1157,http://dx.doi.org/10.16995/dm.4,"{'string_id': '10.16995/dm.4', 'id_scheme': 'DOI'}","This article shows how the System for Palaeographic Inspections (SPI) software suite developed at the University of Pisa can be used to assist palaeographers in their attempts to classify and identify medieval scripts. Working with a small corpus of Tuscan manuscripts from the tenth- through twelfth-century now owned by the Biblioteca Comunale degli Intronati in Siena, the article shows how the software can be used to characterise the calligraphic ideal for each script in a given manuscript, compare letterforms in different scribes’ work, and define relationships among individual scripts and manuscripts.The article concludes with a discussion of potential improvements for the SPI system.",Digital palaeography: using the digital representation of medieval script to support palaeographic analysis,Open Library of the Humanities,2005,,Digital Medievalist,1,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1158,http://dx.doi.org/10.16995/dm.40,"{'string_id': '10.16995/dm.40', 'id_scheme': 'DOI'}","This article discusses the reasons for implementing TEI P5 in a Danish publishing project. We argue that while the standard performs well as a sustainable storage and interchange format, it is generally too complicated to operate efficiently. We show how to cope with this difficulty by introducing a template that makes the daily work easier.",TEI - Keeping It Simple,Open Library of the Humanities,2012,,Digital Medievalist,7,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1159,http://dx.doi.org/10.16995/dm.41,"{'string_id': '10.16995/dm.41', 'id_scheme': 'DOI'}","The Online Medieval Sources Bibliography (OMSB) is a database of modern editions of medieval primary sources. This paper discusses the computing, financing, and logistical challenges we faced in creating the database, as well as our solutions. The OMSB is aimed at a wide audience, from high school students to professors, so we have had to tailor our data to the needs of many different types of researcher, and to keep in mind all of the different ways someone might search for sources. Working with a very limited budget, we have made use of graduate and undergraduate students to provide both programming and data entry, a solution that has provided excellent research experience (and often much-needed funding) for the students involved.",Developing an Online Database on a Shoestring: Growing Pains at the Online Medieval Sources Bibliography,Open Library of the Humanities,2012,,Digital Medievalist,7,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1160,http://dx.doi.org/10.16995/dm.42,"{'string_id': '10.16995/dm.42', 'id_scheme': 'DOI'}","The Cantus database is a well-established project devoted to the creation and distribution of electronic indices of manuscript and early printed sources of Latin chant for the liturgical Office. As of January 2011, there were over 379,000 records in the database, each of which is an individual chant in one of the 134 manuscripts which have been indexed to date. For over a decade, this research tool has been growing and adapting to the needs of chant scholars, musicologists, hagiographers, art historians and researchers in other fields. In addition to the basic search functions and downloading options, there are now several analytical tools available on the website, including a textual concordance and an interactive dendrogram-creation tool. The latter, an example of data-mining, allows the user to select a series of chants which will form the basis of a comparison among the numerous manuscripts whose contents are recorded in Cantus. Similarities in chant series can be interpreted as affinities among manuscripts, and so, the dendrograms which are created (through the calculations of similarity matrices) can assist researchers in identifying related chant repertories, in studying the origins and dissemination of saints' feasts, in providing evidence for the provenance of manuscript sources and, undoubtedly, for numerous other research applications.",The Cantus Database: Mining for Medieval Chant Traditions,Open Library of the Humanities,2012,,Digital Medievalist,7,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1161,http://dx.doi.org/10.16995/dm.43,"{'string_id': '10.16995/dm.43', 'id_scheme': 'DOI'}","This article demonstrates how the search engine developed for this online edition not only serves the research purposes of users of this digital resource, but is also a valuable tool for refining and improving the edition while also aiding the author’s research on the construction of this text. An example of its utility for the edition project is provided which calls into question previous theories regarding the influence John of Wales may have had on this collection of Latin quotations.",The Janus Intertextuality Search Engine: A Research Tool of (and for) the Electronic Manipulus florum Project,Open Library of the Humanities,2012,,Digital Medievalist,7,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1162,http://dx.doi.org/10.16995/dm.44,"{'string_id': '10.16995/dm.44', 'id_scheme': 'DOI'}","In this paper we introduce the numerical tools that have been developed in the context of the Graphem project, in order to automate or leverage several steps in the study of medieval writing samples. We first describe various kinds of features that have been extracted from the samples, and then present two graphical tools to compare writing samples according to the features that have been extracted.","New Tools for Exploring, Analysing and Categorising Medieval Scripts",Open Library of the Humanities,2012,,Digital Medievalist,7,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1163,http://dx.doi.org/10.16995/dm.45,"{'string_id': '10.16995/dm.45', 'id_scheme': 'DOI'}","Europeana Regia is a project to digitise manuscripts from the Middle Ages and the Renaissance, supported by the European Commission and involving five European libraries. During the project, a qualitative study was conducted to determine and rank the expectations and needs of the current and potential users of medieval manuscripts online. Focus groups were organised in three of the project’s partner libraries. Each focus group was dedicated to one of the user categories primarily targeted in the project: 1) researchers and academics; 2) History, Arts and Applied Arts teachers in high schools; 3) the interested general public.The study has confirmed the considerable interest of researchers and academics in this project, but has also pointed out their demanding standards. Compared to the existing offer on other sites, their requests are less concerned with new functionalities than on how well the tools perform, the speed of access and how exhaustive the information would be. Researchers are accustomed to working on the web and can therefore choose and compare what is on offer in the field of online manuscripts.For high schools teachers, the project is seen as an excellent potential teaching aid, but it would require suggestions for courses, themed presentations, selections (noteworthy pages) and a considerable effort to provide mediation (translated passages, reading in the original language, video conferences by specialists, analyses of pages). It is important to encourage them to browse around, in and through a marked space.Interest in the project is less marked in the interested general public, who would only consult medieval manuscripts and illuminated manuscripts from time to time, often motivated by family or cultural events. The illustrations are the main attraction for this public. Within the interested general public, however, there is one category very interested in the project, already identified among the users of cultural or online libraries: a population of seniors, fascinated by the process of how manuscripts are produced, calligraphy, paleography, etc.",Consultation of manuscripts online: a qualitative study of three potential user categories,Open Library of the Humanities,2013,,Digital Medievalist,8,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1164,http://dx.doi.org/10.16995/dm.48,"{'string_id': '10.16995/dm.48', 'id_scheme': 'DOI'}","In 2006, the École nationale des chartes launched a research project whose main purpose was to edit and publish the almost entirely unedited medieval series of charters of the Abbey of Saint-Denis. Because of the great quantity and dissemination of those charters, the critical edition is a progressive digital work based not on the original documents but on the richest medieval cartulary that has been kept, the Cartulaire blanc (i.e. the “White Cartulary”), which contains 2,600 copies of charters. Olivier Guyotjeannin is the scientific leader of the project, Florence Clavaud is the IT leader. In order to provide quick access to the entire content of the cartulary, the edition integrates images of the manuscript, images and an edition of one of the inventories of the charters (the Inventaire général, i.e. the General Inventory), which was established between 1680 and 1720. This digital corpus now includes about 4,370 images and several XML files (EAD files for the edition of the Inventaire général, TEI files for the ongoing edition of the Cartulaire blanc, METS files to express the relations between the components of the corpus), whose models are explained and discussed. The website has been built by using and adapting open source preexisting tools. It was released in June 2010 (see http://saint-denis.enc.sorbonne.fr), and its content and functionalities are evolving. At this stage of the project, this article includes a prospective assessment of the results obtained.",The digital edition of the medieval charters of the Abbey of Saint-Denis: first results and prospects,Open Library of the Humanities,2013,,Digital Medievalist,8,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1165,http://dx.doi.org/10.16995/dm.49,"{'string_id': '10.16995/dm.49', 'id_scheme': 'DOI'}","In editing the Exeter Book poem's The Descent into Hell, also known as John the Baptist's Prayer, I attempted to digitally reconstruct the damaged folia that contain the only surviving copy of the Old English poem. In the process of carrying out the digital reconstructive work, many technical, aesthetic and analytical issues were raised. This paper examines many of the techniques used to digitally reconstruct damaged medieval folia using Photoshop, and also assesses the concerns raised during and after the digital reconstructive work was completed. This article also includes an evaluation of many of the benefits as well as the pitfalls of digital reconstruction carried out on ff. 119v-121v of the Exeter Book, and offers practical resolutions that might be considered when digital restorative work is performed on any given damaged medieval MS.",The advantages and disadvantages of digital reconstruction and Anglo-Saxon manuscripts,Open Library of the Humanities,2015,,Digital Medievalist,9,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1166,http://dx.doi.org/10.16995/dm.50,"{'string_id': '10.16995/dm.50', 'id_scheme': 'DOI'}","Wikipedia’s popularity is unquestioned, but a perceived lack of accuracy and reliability in articles on historical topics prevents historians from embracing it more fully. This article argues that accuracy may be only one component of overall quality. While Wikipedia may have demonstrable shortcomings, it also has strengths in areas such as completeness and accessibility. These strengths appear when historical narratives in Wikipedia are compared to other sources of historical information readily available to American undergraduates. The article compares Wikipedia’s entry on the Investiture Controversy to current scholarship and textbook treatments of the theme. On a broader view of quality, Wikipedia appears in a more favorable light than it does when we employ a narrow focus on accuracy about specific dates and events.","Accuracy and quality in historical representation: Wikipedia, textbooks and the Investiture Controversy",Open Library of the Humanities,2013,,Digital Medievalist,9,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1167,http://dx.doi.org/10.16995/dm.51,"{'string_id': '10.16995/dm.51', 'id_scheme': 'DOI'}","With relatively few scholars and a large number of texts whose manuscript transmission has yet to be mapped, Icelandic literature would benefit from efficient ways of establishing stemmas, to facilitate the study of literature, linguistics, scribal culture, and so Icelandic history more generally. This is also true for much medieval literature. Meanwhile, in saga-studies as in stemmatology generally, there has been little discussion of the role of sampling in textual criticism, even though most scholars must make heavy use of it. This article tests the viability of creating a stemma using a small sample of text by independently drawing a stemma of Konráðs saga keisarasonar, whose stemma was previously established in Zitzelsberger (1981, 1983, 1987), and testing it against these prior publications. Although the approach has limitations, at worst it produces “known unknowns” which can then be resolved through targeted study; in practice it produces results very similar to those of Zitzelsberger; and in some cases it actually allows us to improve on his work. The article also capitalises on internet publication rigorously to include all underlying data and to experiment with new, more transparent, ways of publishing stemmas; and to use digitised data to provide a new overview of the long manuscript tradition of medieval Icelandic romance sagas. Finally, it describes and filiates two new manuscripts of the saga identified in Winnipeg by Katelin Parsons. It concludes by sketching what the stemma of Konráðs saga can tell us about Icelandic scribal culture during its long post-medieval history.","Making stemmas with small samples, and digital approaches to publishing them: testing the stemma of Konráðs saga keisarasonar",Open Library of the Humanities,2013,,Digital Medievalist,9,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1168,http://dx.doi.org/10.16995/dm.52,"{'string_id': '10.16995/dm.52', 'id_scheme': 'DOI'}","This article illustrates how mathematical and statistical tools designed to handle relational data may be useful to help decipher the most important features and defects of a large historical database and to gain knowledge about a corpus made of several thousand documents. Such a relational model is generally enough to address a wide variety of problems, including most databases containing relational tables. In mathematics, it is referred to as a network or a graph. The article's purpose is to emphasise how a relevant relational model of a historical corpus can serve as a theoretical framework which makes available automatic data mining methods designed for graphs. By such methods, for one thing, consistency checking can be performed so as to extract possible transcription errors or interpretation errors during the transcription automatically. Moreover, when the database is so large that a human being is unable to gain much knowledge by even an exhaustive manual exploration, relational data mining can help elucidate the database's main features. First, the macroscopic structure of the relations between entities can be emphasised with the help of network summaries automatically produced by classification methods. A complementary point of view is obtained via local summaries of the relation structure: a set of network-related indicators can be calculated for each entity, singling out, for instance, highly connected entities. Finally, visualisation methods dedicated to graphs can be used to give the user an intuitive understanding of the database. Additional information can be superimposed on such network visualisations, making it possible to intuitively link the relations between entities using attributes that describe each entity. This overall approach is here illustrated with a large corpus of medieval notarial acts, containing several thousand transactions and involving a comparable number of persons.",Exploration of a Large Database of French Notarial Acts with Social Network Methods,Open Library of the Humanities,2014,,Digital Medievalist,9,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1169,http://dx.doi.org/10.16995/dm.53,"{'string_id': '10.16995/dm.53', 'id_scheme': 'DOI'}","The Sources strand of the Pre-Christian Religions of the North project aims to bring together and document the primary evidence for the early religions of Northern Europe. In order to be useful for researchers these sources must include metadata or semantic links that can connect them to concepts relevant to the study of religions. This paper outlines ways in which some very different source types, including textual sources, place-names, pictures and figurines, can be incorporated into a single resource. The aim is to both facilitate analysis and to separate documentary processes from interpretative ones. This structure has been tested using a web interface for both searching and data entry. Some possibilities for representing the visual and spatial data associated with the proposed resource are also demonstrated.",Semantic linking of the Pre-Christian Religions of the North,Open Library of the Humanities,2014,,Digital Medievalist,9,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1170,http://dx.doi.org/10.16995/dm.55,"{'string_id': '10.16995/dm.55', 'id_scheme': 'DOI'}","This paper presents and discusses two of a number of methods for the computer-aided analysis of cartularies that are currently under development at the Instituto de Historia – CSIC. The first one, which we call the Order/Date Model, is oriented to the integral visualization and analysis of an individual cartulary as a project. The second, which we call the Order/Order Model, is applied to pairs of cartularies that share at least part of their contents, and is aimed at revealing to what extent the most recent one made use of the oldest. Our method is based upon a relational database that stores all the information about the cartularies and a number of statistical graphs that generate a two-dimensional grid (the Order/Date or the Order/Order grids) upon which additional variables can be displayed. Our method draws on traditional codicological and palaeographical methods of analysis, but it represents a significant development, as it allows to visualize in an intuitive way very complex phenomena that are otherwise hard to grasp or difficult to analyze manually.",Two graphical models for the analysis and comparison of cartularies,Open Library of the Humanities,2017,,Digital Medievalist,10,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1171,http://dx.doi.org/10.16995/dm.58,"{'string_id': '10.16995/dm.58', 'id_scheme': 'DOI'}","Digitization of dictionaries originally in book form as well as the creation of online dictionaries has revolutionized the way dictionaries are presented and offers not only the opportunity of presenting textual links between dictionary headwords but the possibility of directly connecting one online dictionary to another. This article is an introduction to one of the new functions of the online Anglo-Norman dictionary, i.e. cross-referencing, the provision of links from the Anglo-Norman dictionary entries to other relevant medieval and modern dictionaries. In addition to establishing the usefulness of cross-referencing for dictionary users and presenting how this has been achieved in the Anglo-Norman dictionary, this article examines some of the potential pitfalls that need to be addressed when implementing live links to other dictionaries.","From ""anhelitus"" to ""hanellissement:"" Cross-referencing in the Anglo-Norman dictionary",Open Library of the Humanities,2015,,Digital Medievalist,10,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1172,http://dx.doi.org/10.16995/dm.59,"{'string_id': '10.16995/dm.59', 'id_scheme': 'DOI'}","This paper presents the first complete statistical study of alliteration in the York Cycle of Mystery Plays. To this end, an algorithm is designed to render the phonetic reading of the words of the play and to measure alliteration in the speeches of individual characters. Next, the alliteration statistics of the characters are studied in the entire Cycle, and in each individual play, in order to gain new insight on the possible significance of that linguistic feature in the Plays. Our results indicate that alliteration may have been used as a tool to focus the attention of the audience on one or two major characters in each individual play. Taken in the context of the entire Cycle, there is also a hint of repeating patterns in the manner that alliteration is used within the play.",Alliteration and character focus in the York Plays,Open Library of the Humanities,2015,,Digital Medievalist,10,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1173,http://dx.doi.org/10.16995/dm.6,"{'string_id': '10.16995/dm.6', 'id_scheme': 'DOI'}","The Illustrated Incunable Short Title Catalog on CD-ROM (IISTC), now in its second edition, provides an unrivaled wealth of information on fifteenth-century printing and, as a computer database, allows for rapid searching that would not be possible with printed reference works. However, the database's search interface suffers from numerous problems, as Paul Needham described in a thorough review essay. This article presents a solution to those problems that can be implemented by the end user, and also shows what kind of useful information can be obtained from the IISTC by doing so. The solution entails exporting all records to a very large text file, analyzing the file with scripts written in Perl, importing the information into a full-featured database application, and conducting queries with the database application's more robust and better documented interface. With the IISTC data directly accessible, the database fields can be manipulated to implement features missing in the original IISTC, including separate fields for each part of the imprint data and a count of recorded copies. Query-generated output demonstrated here include a table of incunables with the highest number of copies recorded in the IISTC; printers of Ulm, the number of their signed editions, and their dates; and the number of signed editions printed each year through the end of the fifteenth century. Sample scripts for recreating the results described here, as well as instructions for implementing them and a discussion of points to consider when doing so, are found in the appendices.",Opening the Illustrated Incunable Short Title Catalog on CD-ROM: an end-user’s approach to an essential database,Open Library of the Humanities,2005,,Digital Medievalist,1,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1174,http://dx.doi.org/10.16995/dm.60,"{'string_id': '10.16995/dm.60', 'id_scheme': 'DOI'}","The project The Variance of Njáls saga examines variation in the sixty-three medieval and post-medieval manuscripts of Njáls saga from a linguistic, philological, and literary perspective. This saga is the most extensive of the Icelandic family sagas and is thought to have been composed around 1280. The following article describes methods used in the project to identify synchronic variation at a linguistic level in the fourteenth century manuscripts of the saga, and aspects of an analysis of the stemmatic relationship between manuscripts. In both fields the development of computer-based approaches has advanced notably in the last few years. However, affordable solutions customised for end-users are still lacking. The project therefore focuses on easy tools that can be applied in a short-term project with limited financial and human resources. The manuscripts are transcribed according to the conventions of MENOTA (Medieval Nordic Text Archive) in TEI-XML; a segmentation allowing for an identification of corresponding contents is added; linguistic features relevant for an examination of variation are tagged; and structures relevant for a comparison are displayed for further analysis with the help of XSLT-style sheets. The article discusses challenges that lie in the peculiarities of medieval writing (non-standardised orthography, abbreviations) and tries to outline practicable solutions. Initial results of comparisons of manuscripts based on this approach show variation not only in the semantic domain (substitution of words), but also in the syntactic domain (change of word order, usage of different syntactical constructions).",Easy tools to get to grips with linguistic variation in the manuscripts of Njáls saga.,Open Library of the Humanities,2015,,Digital Medievalist,10,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1175,http://dx.doi.org/10.16995/dm.61,"{'string_id': '10.16995/dm.61', 'id_scheme': 'DOI'}","This paper addresses the question of objective categories of medieval scripts and their elaboration through both medieval palaeography and image analysis. It introduces a dataset of 9800 images and metadata from the catalogues of dated manuscripts in France, as a ground truth and evaluation protocol, to be used for image feature analysis, taxonomy building, and clustering methods. It further compares the results of the categorization performed by two teams, one in Lyon (LIRIS/INSA, Frank Lebourgeois) and the other in Tel-Aviv (The Blavatnik School of Computer Science at Tel Aviv University, Lior Wolf). It also addresses the questions of taxonomy, interpretation and goals of the interdisciplinary research, such as development of expert systems or exploratory research.",Clustering of medieval scripts through computer image analysis: Towards an evaluation protocol,Open Library of the Humanities,2016,,Digital Medievalist,10,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1176,http://dx.doi.org/10.16995/dm.62,"{'string_id': '10.16995/dm.62', 'id_scheme': 'DOI'}","In the textual tradition of a literary work, our sources (manuscripts, printed books etc.) commonly bear, together with the ""main text"", different kinds of ""paratexts"" commenting on it (including interlinear annotations, glosses, scholia, footnotes, modern scholarly introductions and commentaries, and many others). This article proposes a unified model for a document-based digital critical edition including both the main texts and the paratexts as they appear in different single sources. The problematic aspects of such an ""enlarged"" digital edition are discussed, including the relations between the different paratexts and the main text they refer to within each single textual source, as well as the ""alignment"" of different main texts and paratexts in different sources.",Towards a digital model to edit the different paratextuality levels within a textual tradition,Open Library of the Humanities,2008,,Digital Medievalist,4,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1177,http://dx.doi.org/10.16995/dm.63,"{'string_id': '10.16995/dm.63', 'id_scheme': 'DOI'}","The Virtual Lightbox for Museums and Archives (VLMA) is a tool for collecting and reusing, in a structured fashion, the online contents of museums and archive datasets. It is not restricted to datasets with visual components although VLMA includes a lightbox service that enables comparison and manipulation of visual information. With VLMA, one can browse and search collections, construct personal collections, annotate them, export these collections to XML or Impress (Open Office) presentation format, and share collections with other VLMA users. VLMA was piloted as an e-Learning tool as part of JISC’s e-Learning focus in its first phase (2004-2005) and in its second phase (2005-2006) it has incorporated new partner collections while improving and expanding interfaces and services. This paper concerns its development as a research and teaching tool, especially to teachers using museum collections, and discusses the recent development of VLMA.","VLMA: A tool for creating, annotating and sharing virtual museum collections",Open Library of the Humanities,2008,,Digital Medievalist,4,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1178,http://dx.doi.org/10.16995/dm.66,"{'string_id': '10.16995/dm.66', 'id_scheme': 'DOI'}","Digital humanities scholarship contributes to current conversations on literature in many forms, especially in its recontextualizing of what it means to read. By integrating visual, spatial, and quantitative forms of knowledge alongside the practice of text-based hermeneutics, digital techniques expand the possibilities of interpreting texts, particularly with the emergence of widely available geospatial and data visualization tools. This article outlines and reflects on a methodology for producing geospatial and data visualizations of place names in the Icelandic outlaw sagas, and discusses how the results corroborate existing research and also facilitate critical methods of ‘reading’ these texts spatially. While articulating the saga-specific findings of the visualizations, this article also contextualizes the conceptual work of digital literary mapping as a method that is particularly insightful as we determine the role and validity of digital techniques, especially for interdisciplinary and historically-situated work.",Spatial Reading: Digital Literary Maps of the Icelandic Outlaw Sagas,Open Library of the Humanities,2018,,Digital Medievalist,11,1,"[{'value': '1715-0736', 'type': 'electronic'}]"
1179,http://dx.doi.org/10.16995/dm.67,"{'string_id': '10.16995/dm.67', 'id_scheme': 'DOI'}","We propose an automatic method for attributing manuscript pages to scribes. The system uses digital images as published by libraries. The attribution process involves extracting from each query page approximately letter-size components. This is done by means of binarization (ink-background separation), connected component labelling, and further segmentation, guided by the estimated typical stroke width. Components are extracted in the same way from the pages of known scribal origin. This allows us to assign a scribe to each query component by means of nearest-neighbour classification. Distance (dissimilarity) between components is modelled by simple features capturing the distribution of ink in the bounding box defined by the component, together with Euclidean distance. The set of component-level scribe attributions, which typically includes hundreds of components for a page, is then used to predict the page scribe by means of a voting procedure. The scribe who receives the largest number of votes from the 120 strongest component attributions is proposed as its scribe. The scribe attribution process allows the argument behind an attribution to be visualized for a human reader. The writing components of the query page are exhibited along with the matching components of the known pages. This report is thus open to inspection and analysis using the methods and intuitions of traditional palaeography. The present system was evaluated on a data set covering 46 medieval scribes, writing in Carolingian minuscule, Bastarda, and a few other scripts. The system achieved a mean top-1 accuracy of 98.3% as regards the first scribe proposed for each page, when the labelled data comprised one randomly selected page from each scribe and nine unseen pages for each scribe were to be attributed in the validation procedure. The experiment was repeated 50 times to even out random variation effects.",Automatic Scribe Attribution for Medieval Manuscripts,Open Library of the Humanities,2018,,Digital Medievalist,11,1,"[{'value': '1715-0736', 'type': 'electronic'}]"
1180,http://dx.doi.org/10.16995/dm.68,"{'string_id': '10.16995/dm.68', 'id_scheme': 'DOI'}","The cultural heritage of medieval Sicily faces enormous challenges. Rich and diverse as it is, it is beset by numerous problems that have rendered it fragile and often inaccessible. The situation is such that many sites are unsigned. Others are very difficult to get to. And even others – ones that are more easily located – have erratic hours, essentially locking out the average visitor to Sicily unless s/he is willing to invest the time and have the language skills necessary to persuade residents in the surrounding area to get the access keys.Given these challenges, we are developing The Norman Sicily Project to document the cultural heritage of medieval Sicily during its Norman period (in other words, c. 1061–1194) so that a wide audience can learn about what was once there and what still remains. The site attempts to reconstruct what we know about the society by bringing together images, basic identifying information, geolocation data and, in some cases, videos, using modern web development techniques. It also offers genealogical information and visualization tools that can help visitors understand the data in new ways as well as sustainability data related to the monuments’ physical states. The intention is that the project will offer scholars, students and the general public who are interested in Norman Sicily the opportunity to learn from and collaborate with each other while suggesting a web-based model for other medieval communities.",The Norman Sicily Project: A Digital Portal to Sicily’s Norman Past,Open Library of the Humanities,2019,,Digital Medievalist,12,1,"[{'value': '1715-0736', 'type': 'electronic'}]"
1181,http://dx.doi.org/10.16995/dm.69,"{'string_id': '10.16995/dm.69', 'id_scheme': 'DOI'}","This article discusses how digital projects can be employed to encourage undergraduates to think across disciplinary divides, to integrate field and online research, and to confront methodological issues in a more direct way. One of these projects draws on an open-source, web-publishing platform called Omeka and was designed for an interdisciplinary course on the archaeology and history of medieval London offered at Fordham University’s London Centre. The project aimed to give students first-hand experience with the material culture of a medieval city and consisted of two parts. The first, an Object Report, required each student to research and write a short essay on a single medieval object on display at the Museum of London, highlighting the significance of the object within the context of civic, religious, and domestic life in medieval London. In addition, students uploaded images and found illustrations of their objects in medieval manuscripts. The second part, a Site Report, required a visit to a medieval London location– a church, a monastery, or cemetery, for example– to research its significance in the middle ages. Students also uploaded images of their site, which they photographed themselves, and identified the site’s location on a (preferably medieval) map of London. Another similar project was designed using the Weebly web-editing platform for students taking Western Tradition I at Marymount California University, which does not have access to Omeka. Both the Omeka and Weebly projects allowed students to grapple with larger questions about integrating material objects into pre-modern history, but they were especially valuable for teaching students about the importance of being a responsible researcher since students contributed to a digital humanities project that made their research available to a wide public.",Omeka and Other Digital Platforms for Undergraduate Research Projects on the Middle Ages,Open Library of the Humanities,2018,,Digital Medievalist,11,1,"[{'value': '1715-0736', 'type': 'electronic'}]"
1182,http://dx.doi.org/10.16995/dm.75,"{'string_id': '10.16995/dm.75', 'id_scheme': 'DOI'}","In the present paper we analyze nine manuscripts from the 11th–14th century Menaia (Greek: μηνάιον), Old-Slavic hymnographic texts, using a vector space model. The analysis and classification of the manuscripts in previous studies have been rather subjective. In an attempt to be objective we use contemporary research methods. Vector analysis allows one to separate the Putyatina Menaion and the Menaion Q.п.1.25 from the set of analyzed texts, since both manuscripts share both textological and lexical similarities. Similar findings were reached in existing studies. Manuscript BAN 16.14.13 is shown to be quite similar to the set of analyzed texts. The results are new to the literature.",On the Classification of the Slavic Menaia Manuscripts,Open Library of the Humanities,2018,,Digital Medievalist,11,1,"[{'value': '1715-0736', 'type': 'electronic'}]"
1183,http://dx.doi.org/10.16995/dm.81,"{'string_id': '10.16995/dm.81', 'id_scheme': 'DOI'}","This paper presents the methodology and the results of an analytical study of the three witnesses of Dante’s Commedia copied by Giovanni Boccaccio, focusing on the importance of their digital accessibility. These extraordinary materials allow us to further our knowledge of Boccaccio’s cultural trajectory as a scribe and as an author, and could be useful for the study of the textual tradition of Dante’s Commedia. In the first section of the paper, the manuscripts and their role in previous scholarship are introduced. A thorough analysis of a choice of variants is then offered, applying specific categories for organizing the varia lectio. This taxonomy shows how fundamental it is to combine the methodological tools for studying copies (as usual in medieval philology) and those for studying author’s manuscripts (as usual in modern philology) in dealing with the three manuscripts of Boccaccio’s Commedia: in fact, the comparative analysis of the three manuscripts has much to reveal not only of their genetic relationship but also of Boccaccio’s editorial practices. Furthermore, the analytic categories inform the computational model behind the web application ‘La Commedia di Boccaccio’, &lt;http://boccacciocommedia.unil.ch/&gt; created for accessing and querying the variants. The model, implemented in a relational database, allows for the systematic management of different features of textual variations, distinguishing readings and their relationships, without setting a base text. The paper closes on a view to repurposing the model for handling other textual transmissions, working at the intersection between textual criticism and information technology.",Querying Variants: Boccaccio’s ‘Commedia’ and Data-Models,Open Library of the Humanities,2019,,Digital Medievalist,12,1,"[{'value': '1715-0736', 'type': 'electronic'}]"
1184,http://dx.doi.org/10.16995/dm.83,"{'string_id': '10.16995/dm.83', 'id_scheme': 'DOI'}","The task of automatically separating Middle Dutch words into syllables is a challenging one. A first method was presented by Bouma and Hermans (2012), who combined a rule-based finite-state component with data-driven error correction. Achieving an average word accuracy of 96.5%, their system surely is a satisfactory one, although it leaves room for improvement. Generally speaking, rule-based methods are less attractive for dealing with a medieval language like Middle Dutch, where not only each dialect has its own spelling preferences, but where there is also much idiosyncratic variation among scribes. This paper presents a different method for the task of automatically syllabifying Middle Dutch words, which does not rely on a set of pre-defined linguistic information. Using a Recurrent Neural Network (RNN) with Long-Short-Term Memory cells (LSTM), we obtain a system which outperforms the rule-based method both in robustness and in effort.",Data-Driven Syllabification for Middle Dutch,Open Library of the Humanities,2019,,Digital Medievalist,12,1,"[{'value': '1715-0736', 'type': 'electronic'}]"
1185,http://dx.doi.org/10.16995/dm.86,"{'string_id': '10.16995/dm.86', 'id_scheme': 'DOI'}","The many source texts that late medieval adaptors like Thomas Malory worked with constitute a potential wealth of information concerning the genesis of the linguistic and stylistic features shaping their works. As a result, their texts provide a useful framework for further developing and testing methods of stylometric analysis in the context of adaptation as a collaborative form of authorship. Our interdisciplinary team has undertaken a stylometric analysis of the eight different sections of Thomas Malory’s Morte Darthur, in order to identify differences between these sections and how they correspond to the language of the Old French and Middle English sources that Malory is known to have worked with in the different sections of his work. Our findings provide a basis for addressing unresolved scholarly questions concerning Malory’s Morte, such as the nature of the source used for his “Tale of Sir Gareth” and whether Malory himself was responsible for the differences between the two surviving versions of his “Roman War” episode in Book II of the Morte. They further shed light on ongoing controversies concerning the overall textual unity of the Morte and the process by which Malory created the different sections of his work—an issue that lies at the heart of broader debates concerning where Malory and other late medieval adaptors are to be situated on the continuum between “faithful translator” and “original author.”",A Computational Approach to Source Adaptation in Thomas Malory’s Morte Darthur,Open Library of the Humanities,2019,,Digital Medievalist,12,1,"[{'value': '1715-0736', 'type': 'electronic'}]"
1186,http://dx.doi.org/10.16995/dm.9,"{'string_id': '10.16995/dm.9', 'id_scheme': 'DOI'}","Digital media offer great possibilities for access by the disabled. To ensure access, however, designers must take reasonable measures to ensure that their digital texts conform to relevant accessibility standards. Observing such standards benefits all users and may be a legal requirement. Fortunately, they for the most part are not difficult to implement. This article examines some methods for ensuring accessibility. It also advocates the development of disciplinary standards for accessibility in the design of medieval digital projects. A final section shows the same hypothetical text in accessible and inaccessible codings.",Why Universal Accessibility Should Matter to the Digital Medievalist,Open Library of the Humanities,2005,,Digital Medievalist,1,,"[{'value': '1715-0736', 'type': 'electronic'}]"
1187,http://dx.doi.org/10.16995/dscn.10,"{'string_id': '10.16995/dscn.10', 'id_scheme': 'DOI'}","This chapter extends from research on the genealogies of modernist laboratories (aesthetic, scientific, and corporate) and their relationship to the formation of contemporary digital-humanities and new-media laboratories and collaboratories. The trajectory of this project, broadly conceived, leads from the modular principles of early twentieth-century industrial design and avant-garde aesthetics, to the implementation of modular architecture in the construction of mid-century corporate science labs, to the modularization of markup languages, interoperable digital tools, and collaborative and cross-disciplinary lab environments in the late twentieth and early twenty-first centuries. In doing so, I traverse the conjuncture of multiple modularities and disciplinarities—from architecture, art, and industrial design to markup languages, source code, and digital tools—in the historical contexts of cultural, industrial, and postindustrial modernities.",Chapter 3ModLabs,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1188,http://dx.doi.org/10.16995/dscn.11,"{'string_id': '10.16995/dscn.11', 'id_scheme': 'DOI'}","This essay examines the possibilities for creating digital editions as representations of social and topographical networks of time and space, rather than as standalone e-versions of printed books.",Chapter 14Beyond GIS: On mapping early modern narratives and the chronotope,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1189,http://dx.doi.org/10.16995/dscn.12,"{'string_id': '10.16995/dscn.12', 'id_scheme': 'DOI'}"," In this chapter, we assess how existing digital projects that feature women's manuscripts (c. 1550-1900) can aid research on literature, history, and cultural studies. We argue that the best digital remediations of women's manuscripts contribute, paradoxically, both to their hypermediacy (those elements that remind users they are not faced with a manuscript) and their immediacy (those aspects that hide the remediation and encourage users not to reflect on the medium). We showcase the range of scholarly engagement possible through a variety of sites, including British Literary Manuscripts Online, Perdita Manuscripts, and Jane Austen's Fiction Manuscripts. Analyzing these resources demonstrates how they can best be used by teachers and scholars. By evaluating digital remediations of women's manuscripts, we highlight best practices for manuscript digitization and point to new directions for digital projects and literary study.",Chapter 12Evaluating digital remediations of women's manuscripts,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1190,http://dx.doi.org/10.16995/dscn.13,"{'string_id': '10.16995/dscn.13', 'id_scheme': 'DOI'}","Book history's disciplinary focus on the materiality and circulation of texts and textual objects has much to contribute to the domain of digital humanities. This chapter probes the legacy of D.F. McKenzie's notion of 'broken phials' and 'indeterminate texts' using the example of a flagship digital project The Printers' Web undertaken at Wai-te-ata Press, Victoria University of Wellington, New Zealand. It offers an enhanced metacritical understanding of nineteenth century knowledge networks and points to new directions in textual scholarship in the twenty-first century.",Chapter 2At the crossroads: Book history meets digital humanities,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1191,http://dx.doi.org/10.16995/dscn.14,"{'string_id': '10.16995/dscn.14', 'id_scheme': 'DOI'}","Since the advent of digital scholarly editions, there have been many arguments to the effect that digital versions are able to offer more to humanities scholars than printed ones. Though this opinion is shared by most scholars producing digital editions, a number have also published printed versions alongside or even after launching digital ones.",Scholarly editions in print and on the screen: A theoretical comparison,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1192,http://dx.doi.org/10.16995/dscn.15,"{'string_id': '10.16995/dscn.15', 'id_scheme': 'DOI'}",Digital textuality will be defining the nature and uses of literacy to the same degree as printing has done since Gutenberg's invention. The article explores the implications of one of the fundamental differences between the screen and paper substrates: digital fluidity versus the fixity of paper.,Chapter 1Feeding our reading machines: From the typographic page to the docuverse,Open Library of the Humanities,2017,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1193,http://dx.doi.org/10.16995/dscn.16,"{'string_id': '10.16995/dscn.16', 'id_scheme': 'DOI'}","The genre of the scholarly edition is diverse in its forms, but it generally involves a reproduction of original source material in the context of a scholarly apparatus consisting of annotations which add scholarly value to the text by explaining the editorial decisions made in a particular passage, drawing attention to an interesting place in the text, or linking sections of the facsimile with secondary scholarship. This scholarly apparatus acts as a contextualization of the primary material with which it is concerned. Instead of being embedded within the original material, these additions are placed in margins, footers, and appendices separate from but engaged with the material. In every case of a footnote, marginal note, or other part of a scholarly apparatus, we can say that something provided by the editor is a “body” associated with something else that acts as the “target” of the commentary. This association of “body” and “target” is the essence of Open Annotation, an emerging standard on the World Wide Web for associating Web content in this manner. This sense of annotation—of associating one piece of content with another for some express purpose—is the foundation of the Shared Canvas data model, which we present in this chapter as one approach to representing digital facsimile editions in an open, shareable form.",​Chapter 9Beyond accessibility: Using Shared Canvas in digital facsimile editions,Open Library of the Humanities,2017,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1194,http://dx.doi.org/10.16995/dscn.17,"{'string_id': '10.16995/dscn.17', 'id_scheme': 'DOI'}","The computational turn in the humanities has precipitated the need for sustainable software development projects that are specifically focused on humanities research problems, and the need for graduate and undergraduate training models that address the trans-disciplinary nature of computational humanities research. In this paper, we describe one approach for addressing those two challenges simultaneously: an interdisciplinary research and development team called the Digital Innovation Group (DigInG). DigInG quickly and necessarily became an experiment in trans-disciplinary education at the interface of digital humanities and computer science. Not only does DigInG play an important role in developing a computational infrastructure for d/cHPS research, it also creates an environment for hands-on training for graduate and undergraduate students in computer science, biology, and history and philosophy of science. We discuss the rationale, benefits, and challenges of DigInG since its inception. Our primary objectives are to broaden the discussion about how digital and computational humanities programs are organized, and to suggest that software development and training in the digital humanities need not be conceived as independent activities.",Software development & trans-disciplinary training at the interface of digital humanities and computer science,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1195,http://dx.doi.org/10.16995/dscn.18,"{'string_id': '10.16995/dscn.18', 'id_scheme': 'DOI'}","This paper outlines the circumstances surrounding a libel case that was filed against academic librarian Dale Askey by publisher Herbert Richardson and his company Edwin Mellen Press, the resulting online debate, protest, and advocacy, and the effort by a small team to capture, preserve, and make available preserved websites related to the event. Ruest, a programmer and archivist-librarian, presents the technical aspects of capturing and preserving WARC files. St.Onge, an archivist, reflects on some of the challenges of creating a traditional finding aid to contextualize and provide access to the collected electronic content. Milligan, a historian, discusses some preliminary findings based on analysis of the data set. Finally, the authors reflect on the issues brought to the surface by their engagement with questions of academic freedom, librarianship, and public advocacy and on how smaller groups of like-minded professionals can preserve online material whose afterlife might otherwise prove fleeting.","The great WARC adventure: Using SIPS, AIPS, and DIPS to document SLAPPs",Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1196,http://dx.doi.org/10.16995/dscn.19,"{'string_id': '10.16995/dscn.19', 'id_scheme': 'DOI'}","Reading a work of digital fiction can be a challenging experience for anyone who is approaching it from a print-oriented stance. Paying particular attention to the materiality of the medium in which the work is presented can be a useful tool and this is what N. Katherine Hayles has emphasized upon by coining the phrase Media-Specific Analysis. This approach to texts, as its name states, is based on exploring the materiality of the medium in which the work is instantiated as an active component in the signifying process. The case study chosen for this paper, Chemical landscapes digital tales (2006) by Edward Falco, brings its materiality into the foreground and makes it an essential part of the reading process. In order to devise a working method for the media-specific analysis of this work (and other works of web-fiction) three different but interrelated dimensions of the text—physical organization and design, narrative strategy, and reading process—will be analyzed in this paper.",A Window Toward the Medium: A Media Specific Analysis of Chemical Landscapes Digital Tales,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1197,http://dx.doi.org/10.16995/dscn.21,"{'string_id': '10.16995/dscn.21', 'id_scheme': 'DOI'}"," The remix demonstrates that creativity is alive and well, but also under attack by corporate copyright holders trying to shut down media sharing and remixed media. Artistry now lies in how pre-existing materials are remixed and rendered new; it is through acts of transformation of the original that the remix's value is found. Artists reuse other works to reveal or transmit political messages, and not simply to re-create them. The remix is not killing narrative but instead is fostering a transformation and renaissance of creative practice, outside of the legal boundaries of copyright. The criminalization of new kinds of creative practice is a growing barrier to the remix's creative renaissance. Remix reimagines pre-existing texts, plots and genres as a way of interrogating the future of culture, via digital collaborative authorship and a distinct remix aesthetic of interruption, disturbance, and capture/leakage.","""It’s Creativity, Jim, but not as we know it"": Authorship in the Age of the Remix.",Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1198,http://dx.doi.org/10.16995/dscn.22,"{'string_id': '10.16995/dscn.22', 'id_scheme': 'DOI'}","This paper makes a contribution to the growing literatures on critical media literacy and intellectual property. While new forms of participatory culture, social production, user-generated content and new forms of social engagement have developed through the use of digital technology, those very forms of engagement are shaped by legal definitions and commercial practices that constitute the operating system of the digital economy. It will be shown that a shift toward propertization has occurred in copyright, trademark and the right of publicity. Further, each of these forms of intellectual property law has been strengthened, lengthened and, increasingly, used in relation to the others. The objects of various intellectual property forms (creative works, marks, and publicity rights) are abstracted from their original contexts and treated in an analogous way – as property, by the information and entertainment industries, the courts, and legislators, and even the general public. In such a media ecology it makes sense to see cultural properties as transpropertied, where the multi-faceted character of such intellectual artifacts requires the interaction and protection of formerly distinct forms of IP protection. Using examples from literature, cinema, and comic books the paper will trace the development of transproperty and will offer suggestions for how an understanding of transproperty strengthens our media literacy.",Transproperty: Intellectual property and the ideal property form,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1199,http://dx.doi.org/10.16995/dscn.235,"{'string_id': '10.16995/dscn.235', 'id_scheme': 'DOI'}","In this article, I discuss a project in which I use text mining and corpus linguistics to explore patterns of sentimentality in mid-Victorian novels. The strong and formulaic conventions of sentimentality as a genre enable me to investigate how text analysis, machine classification, and word cloud visualisations can reveal low-level patterns that correspond to the higher-level formations of sentimentality. Specifically, I use the Naïve Bayes classification algorithm and Dunning's Log Likelihood Ratio (through the MONK Project and WordHoard) to identify instances of sentimentality and isolate the words that are most salient in affective writing. I also use the study to think through issues raised when employing computational text analysis and distant reading to inform critical positions.",Patterns of Sentimentality in Victorian Novels,Open Library of the Humanities,2013,[],Digital Studies/Le champ numérique,3,2,"[{'value': '1918-3666', 'type': 'electronic'}]"
1200,http://dx.doi.org/10.16995/dscn.236,"{'string_id': '10.16995/dscn.236', 'id_scheme': 'DOI'}","From Virgil Thomson’s 1934 musical score for Gertrude Stein’s opera Four Saints in Three Acts, to readings of The Making of Americans (1925) using digital tools, artists and scholars have used musical composition and computational tools to help express something otherwise obscured in traditional readings of Stein’s texts. At first glance, it seems there is little in common between these efforts: readers argue that setting a Steinian text to music is a welcome abstraction that brings the text to new heights of expression, while computing them seems to concretise their subtleties and diminish their expressive possibilities. I argue that it is the situated hermeneutical framework in which visualisations, like musical scores, invite us to ""play,"" that guides our scholarly, interpretive performances with Stein’s texts. In particular, the visualisations I introduce here help us reconsider the significance of oral traditions in Stein’s self-proclaimed magnum opus, The Making of Americans.",Distant Listening or Playing Visualisations Pleasantly with the Eyes and Ears,Open Library of the Humanities,2013,[],Digital Studies/Le champ numérique,3,2,"[{'value': '1918-3666', 'type': 'electronic'}]"
1201,http://dx.doi.org/10.16995/dscn.237,"{'string_id': '10.16995/dscn.237', 'id_scheme': 'DOI'}","Focusing specifically on how digital content is duplicated across the web (often with and without attribution), this essay argues that humanities practitioners currently need platforms to help them: (1) facilitate the aggregation of web-based content, (2) responsibly embed and share that content, (3) interpret the content in a flexible and open way, (4) identify and create relationships between the content, and (5) express those relationships through a variety of formats, genres, views, and arrangements. To justify these claims, the authors examine two platforms, ThoughtMesh and Scalar, using methods across digital rhetoric, interaction design, data structuring, and software studies. While the essay is largely intended for audiences interested in digital scholarly communications and platform development, it should also appeal to those invested in linked open data, media studies, and computers and writing.",After the Document Model for Scholarly Communication: Some Considerations for Authoring with Rich Media,Open Library of the Humanities,2013,[],Digital Studies/Le champ numérique,3,2,"[{'value': '1918-3666', 'type': 'electronic'}]"
1202,http://dx.doi.org/10.16995/dscn.238,"{'string_id': '10.16995/dscn.238', 'id_scheme': 'DOI'}","Robert Southey, a member of “the Lake School” of poetry—really, the first “avant garde” in the history of English literature—was as prolific a letter writer as he was of poetry and prose during the Romantic era in England, roughly 1780-1830: “to write to a dear friend,” Southey says, “is to me like escaping from prison.” A massive digital scholarly edition is underway, The Collected Letters of Robert Southey. It is divided into eight Parts: I:1791-1797, II:1798-1803, III:1804-09, IV:1810-15, V:1816-21, VI:1822-27, VII:1828-33, VIII:1833-39. Currently Parts I and II have been completely edited and made available to the public. These letters capture a set of intellectual, amicable, and financial relationships established while Southey lived in and traveled away from his home base of Bristol in “the West Country,” before he actually moved to the Lake District. Because each person and place name in the letters was encoded using TEI P5, we have been able to create a data set that indicates who is mentioned in letters to whom. That data set was fed into a Directed and Undirected Graph to be visualized, which is available to see and manipulate online here (http://dhhub.org/demos/voyeur/).",How to Read a Literary Visualisation: Network Effects in the Lake School of Romantic Poetry,Open Library of the Humanities,2013,[],Digital Studies/Le champ numérique,3,2,"[{'value': '1918-3666', 'type': 'electronic'}]"
1203,http://dx.doi.org/10.16995/dscn.239,"{'string_id': '10.16995/dscn.239', 'id_scheme': 'DOI'}","How can crowdsourcing be used in the humanities? This paper describes a crowdsourcing project at the University of Alberta that developed a tool for participants to sign out audio clips for transcribing and translating from Ukrainian. The audio clips were from fieldwork on Ukrainian folklore and include stories, songs, and beliefs. We describe the design process and provideCrowdsourcing only works if you can motivate sufficient participants and different participant communities call for different strategies. an environmental comparison with other crowdsourcing projects. We discuss the challenge of motivating community members with the language skills to volunteer to help with the project. Crowdsourcing only works if you can motivate sufficient participants, and different participant communities call for different strategies.",Ukrainian Folklore Audio Project,Open Library of the Humanities,2013,[],Digital Studies/Le champ numérique,3,2,"[{'value': '1918-3666', 'type': 'electronic'}]"
1204,http://dx.doi.org/10.16995/dscn.240,"{'string_id': '10.16995/dscn.240', 'id_scheme': 'DOI'}","This essay considers the opportunities afforded by the Simulated Environment for Theatre (SET) for exploring the functions of and relationships between historical theatrical texts and other records of performance. Over the course of SET’s development, the system’s digital environment for visualizing theatrical text and performance has increasingly emphasised affordances that are not available in other digital or material environments. Our most recent focus has been on SET’s potential applications for theatre historians, in particular because of the system’s user-controlled, hybrid two- and three-dimensional interface, which permits the simultaneous visualization of a variety of research materials and performance simulations. Through a research project exploring the theatre-historical significance of Soulpepper Theatre’s 2011 production of Judith Thompson’s White Biting Dog in relation to the Tarragon Theatre’s 1984 premiere of the play, we have identified some current strengths of SET as a tool for theatre-historical research, as well as some technical improvements that would enhance its efficacy, and some design challenges warranting further consideration. With some refinements, SET has the potential to add to the affordances offered by existing digital tools through its ability to visualise theatre-historical methodologies and its positioning of the user as the agent of research, making it an environment for historiographical as well as historical scholarship.",Visualising Theatre Historiography: Judith Thompson's White Biting Dog (1984 and 2011) in the Simulated Environment for Theatre (SET),Open Library of the Humanities,2013,[],Digital Studies/Le champ numérique,3,2,"[{'value': '1918-3666', 'type': 'electronic'}]"
1205,http://dx.doi.org/10.16995/dscn.241,"{'string_id': '10.16995/dscn.241', 'id_scheme': 'DOI'}","When one needs to apply computer-assisted conceptual analyses of philosophical texts beyond their linguistic dimension, statistic macro-textual approaches have given appealing results but remain limited. In this article, we present a computer-assisted conceptual analysis methodology applied to a philosophical text. This methodology attempts to be as close as possible to the criteria of the philosophical approach. The method allows a systematic exploration through the multifaceted contexts of a specific philosophical concept. Our methodology is based on the use of a) a concordancer b) a clustering method and c) an interpretative annotation strategy. This method is applied to the concept of evolution in Bergson's corpus.",CARCAT: Computer-Assisted Reading and Conceptual Analysis of Texts: An experiment applied to the concept of evolution in the work of Henri Bergson,Open Library of the Humanities,2012,[],Digital Studies/Le champ numérique,3,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1206,http://dx.doi.org/10.16995/dscn.242,"{'string_id': '10.16995/dscn.242', 'id_scheme': 'DOI'}","This paper describes a project-based senior undergraduate course in electronic scholarly editing at the University of Saskatchewan. Students used HTML to prepare and publish on the World Wide Web electronic documentary editions of two seventeenth-century books: the anonymous Eighth Liberal Science: or a New-found Art and Order of Drinking [1650], and Edward Whitaker's Directions for Brewing Malt Liquors [1700]. The course offered students the benefits of project-based pedagogy—collaboration, original research, independent decision making, and preparation of a concrete product with real-world usefulness. The paper also describes the editions produced by the class, with particular emphasis on the textual issues that emerged, and the students' technological and editorial resolution of those issues. It concludes with a discussion of the benefits of project-based pedagogy in the undergraduate humanities classroom.",Electronic Scholarly Editing in the University Classroom: an Approach to Project-based Learning,Open Library of the Humanities,2012,[],Digital Studies/Le champ numérique,3,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1207,http://dx.doi.org/10.16995/dscn.243,"{'string_id': '10.16995/dscn.243', 'id_scheme': 'DOI'}","This paper draws on the experience of the University of New Brunswick (UNB) Electronic Text Centre (ETC) digitizing the papers of prominent New Brunswick Loyalist Edward Winslow to address issues relating to the development of laterally developed projects. It defines laterally developed projects as those that reconstitute previously existing project architecture to new ends. It argues that there are many reasons to develop projects laterally. First, it can be financially prudent to develop projects laterally; given the difficulties in securing grant money for projects, lateral development can assist digital humanists in building new and more robust projects on the shoulders of previously competed work. Second, lateral project development can facilitate greater production in the digital humanities, as laterally developed projects need not be undertaken by the researcher or institution that developed the original. Indeed, once the core elements of a project have been published online any other researcher can build upon this previously completed work. However, this presents a bit of a problem, as to fully realize the potential of lateral project development we will have to do away with a proprietary mindset with regard to our digital projects and be willing to share already completed project elements and modules with other researchers","Modular Design, Lateral Project Development, and the Sharing of Work: Lessons from the Edward Winslow Family Papers and the Atlantic Canada Virtual Archives",Open Library of the Humanities,2012,[],Digital Studies/Le champ numérique,3,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1208,http://dx.doi.org/10.16995/dscn.244,"{'string_id': '10.16995/dscn.244', 'id_scheme': 'DOI'}","In his book Humanities Computing, in a chapter fittingly titled ""Discipline,"" Willard McCarty attempts to define the discipline of Humanities Computing by understanding what is happening within the discipline. According to McCarty it is through this understanding that ""we may get to the disciplinary conditions from which specific methods arise as desire or need direct."" This suggests that one way to understand the beginnings of a discipline are to look not at the founding people, but at the desires and needs articulated at the time. This paper will therefore look at how an agenda was set for humanities computing in English Canada around a particular moment of emergence, in this case the emergence of a scholarly society, the Ontario Consortium for Computing in the Humanities, which evolved into the national Consortium for Computers in the Humanities (COCH/COSH).",Before the Beginning: The Formation of Humanities Computing as a Discipline in Canada,Open Library of the Humanities,2013,[],Digital Studies/Le champ numérique,3,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1209,http://dx.doi.org/10.16995/dscn.257,"{'string_id': '10.16995/dscn.257', 'id_scheme': 'DOI'}","This paper describes the preliminary results of combining two complementary technologies: Orlando, a semantically-tagged XML collection of born-digital scholarly resources, and the Mandala Browser, an XML visualization tool. Orlando's current delivery system privileges text as an approach to literary historical scholarship. The Mandala browser represents a radically different way of mediating between the user and the text, translating a text or set of texts into a circular visual form and pushing the user towards a more distant, or at least a more selective, reading of the materials than that associated with conventional print or screen rendering. Through experimental visualizations of Orlando content, we began to address questions concerning the participation of Victorian and Renaissance writers in various genres, the relationship between reproduction and literary production, the connection of censorship to the destruction of literary works, and the relationship between suffrage and liberal or conservative political groups. We argue that, just as a postcolonialist or a new historicist needs to learn about the tenets and processes involved in a postcolonial or new historical critical framework, so too an algorithmic critic should expect to invest some time learning the techniques of a given approach and how to apply them to a particular text or body of texts. These investigations may interest other humanities scholars working with online digital collections, as well as those thinking through the question of how to involve computational processes in complex inquiries using large quantities of texts.",Reading with the Mandala Browser: A Case Study in Algorithmic Criticism via Experimental Visualization,Open Library of the Humanities,2011,[],Digital Studies/Le champ numérique,2,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1210,http://dx.doi.org/10.16995/dscn.258,"{'string_id': '10.16995/dscn.258', 'id_scheme': 'DOI'}","The following article describes a simple technique to identify lexically-similar passages in large collections of text using sequence alignment algorithms. Primarily used in the field of bioinformatics to identify similar segments of DNA in genome research, sequence alignment has also been employed in many other domains, from plagiarism detection to image processing. While we have applied this approach to a wide variety of diverse text collections, we will focus our discussion here on the identification of similar passages in the famous 18th-century Encyclopédie of Denis Diderot and Jean d’Alembert. Reference works, such as encyclopedias and dictionaries, are generally expected to “reuse” or “borrow” passages from many sources and Diderot and d’Alembert’s Encyclopédie was no exception. Drawn from on an immense variety of source material, both French and non-French, many, if not most, of the borrowings that occur in the Encyclopédie are not sufficiently identified (according to our standards of modern citation), or are only partially acknowledged in passing. The systematic identification of recycled passages can thus offer us a clear indication of the sources the philosophes were exploiting as well as the extent to which the intertextual relations that accompanied its composition and subsequent reception can be explored. In the end, we hope this approach to “Encyclopedic intertextuality” using sequence alignment can broaden the discussion concerning the relationship of Enlightenment thought to previous intellectual traditions as well as its reuse in the centuries that followed.",Something Borrowed: Sequence Alignment and the Identification of Similar Passages in Large Text Collections,Open Library of the Humanities,2011,[],Digital Studies/Le champ numérique,2,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1211,http://dx.doi.org/10.16995/dscn.259,"{'string_id': '10.16995/dscn.259', 'id_scheme': 'DOI'}","This paper explores the role of both digital and in-person collaboration tools and considers the need for a balance between these within the context of Digital Humanities (DH) research teams. First, the uses, benefits and trade-offs of various communication channels are outlined. From there, the application of these various channels within DH research teams and their impact on collaboration will be explored by drawing upon interviews with members of DH teams. Best practices and recommendations for DH research teams and funding agencies will conclude the paper.",The Balance between On-line and In-person Interactions: Methods for the Development of Digital Humanities Collaboration,Open Library of the Humanities,2011,[],Digital Studies/Le champ numérique,2,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1212,http://dx.doi.org/10.16995/dscn.260,"{'string_id': '10.16995/dscn.260', 'id_scheme': 'DOI'}","In recent years, Wikipedia has emerged as one of the most prominent sources, of any sort, of information and ideas relating to what one might call early modern studies. This article considers Wikipedia's troubled relationship with conventional academic authority, and also the paradox whereby Wikipedia articles are at the same time very mutable and very persistent. As case studies, it looks in detail at the evolution and dissemination of two Wikipedia articles, on The Tempest and on the minor writer Gervase Markham. Wikipedia, it will be argued, is a project whose conflicted attitudes to knowledge and authority have parallels with the early modern.",Prospero and Plagiarism: Early Modern Studies and the rise of Wikipedia,Open Library of the Humanities,2010,[],Digital Studies/Le champ numérique,2,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1213,http://dx.doi.org/10.16995/dscn.273,"{'string_id': '10.16995/dscn.273', 'id_scheme': 'DOI'}","The study has two main parts: the first one reviews the current scholarship and theories about the materiality of text in print and digital form exemplified in the works of Johanna Drucker, Jerome McGann, Matthew Kirschenbaum, G. Thomas Tanselle and D. F. McKenzie and then presents a hybrid model of materiality that is comprehensive and practical. This model is designed to make the most use of the capacities of the digital medium and it stems from an understanding of the material repercussions of the migration from print to digital. The second part elaborates on the elements of the codex book and overviews the history of the novel genre. This is to prepare a basis for a prototype for digitizing novels, called iNovel. The last part of the study presents a detailed description of the prototype and its different tools. This prototype serves as a realization of the theoretical model described earlier.",Enter the Digital: Emergent Materiality and the Digitization of Literary Texts: The Novel as a Case Study,Open Library of the Humanities,2017,[],Digital Studies/Le champ numérique,7,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1214,http://dx.doi.org/10.16995/dscn.274,"{'string_id': '10.16995/dscn.274', 'id_scheme': 'DOI'}","In this article I briefly discuss the connections between the geopolitical scenario emerging from the creation of the BRICS New Development Bank, and the digitization of languages and cultures carried out in a substantially Anglophone-driven economic and technological context. The appearance of the new BRICS bank, and especially the plan for an “independent Internet” are not only challenging the financial system, but in the long-run could also affect the current digital knowledge monopolies, activating new ways to encode and decode cultural objects, and challenge present digital standards. Digital Humanists, on all levels, are called upon to react to this developing geopolitical scenario, asking themselves questions about political representation and cultural diversity, encoding standards, digital infrastructures and linguistic hegemonies. An old equilibrium based on unequal power relationships is perhaps close to an end, and this is a unique time and opportunity to create a genuinely democratic and international scholarly community.",Digital Humanities and the Geopolitics of Knowledge,Open Library of the Humanities,2017,[],Digital Studies/Le champ numérique,7,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1215,http://dx.doi.org/10.16995/dscn.275,"{'string_id': '10.16995/dscn.275', 'id_scheme': 'DOI'}","Background: The author describes curating sound(s) in a radio context and how this practice prompts creative Digital Humanities research, scholarship, and presentation. The author describes his Radio Nouspace project as a site, focus, and methodology. A particular endeavor is curation by re-creation of vintage radio dramas before live audiences in order to prompt listeners to consider the ability of sounds to convey appreciation, emotion, experience, information, and meaning(s).","Radio Nouspace: Sound, Radio, Digital Humanities",Open Library of the Humanities,2017,[],Digital Studies/Le champ numérique,7,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1216,http://dx.doi.org/10.16995/dscn.276,"{'string_id': '10.16995/dscn.276', 'id_scheme': 'DOI'}","This case study offers strategies for TEI-based projects with limited funding. By focusing on the needs of our volunteers, the Victorian Women Writers Project has developed truly collaborative relationships with the project’s partners. Contributions to the project’s resources have grown out of digital humanities survey courses, literature classes, and independent work. The paper concludes with a brief sketch of our efforts to support continued work by rethinking our social media outreach and our online presence.",Toward Sustainable Growth: Lessons Learned Through the Victorian Women Writers Project,Open Library of the Humanities,2017,[],Digital Studies/Le champ numérique,7,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1217,http://dx.doi.org/10.16995/dscn.277,"{'string_id': '10.16995/dscn.277', 'id_scheme': 'DOI'}","Founders Online, a digital initiative of the National Historical Publications and Records Commission (NHPRC) of the U.S. National Archives, launched in June 2013. Since its debut, the site has attracted over a million visitors interested in learning more about the creation of the United States of America in the words of six of its Founding Fathers. Founders Online contains 177,000 letters or other writings of these men and their contemporaries. Widely used by academics and the general public, the site has demonstrated the value of digital humanities’ emphasis on free access. As a former assistant editor at Documents Compass, a program of the Virginia Foundation of the Humanities, I served as a project manager on the Early Access portion of the project. We worked directly with the staffs of the currently active Founding Fathers documentary editing projects to make preliminary versions of unpublished documents available for early viewing on Founders Online. These Early Access documents will eventually be replaced by fully vetted and annotated versions to be completed later by the documentary editing projects. Relying on a large staff of over thirty people, we transcribed or proofread over 50,000 Early Access documents from 2012 to 2015. My Early Access experience demonstrated the need to give employees constant feedback, to reward them for good work, and to encourage specialization among project staff. My experience also reemphasized the need for unified metadata standards when aggregating different sets of data from multiple projects into a single digital platform.","Founders Online: Early Access: Reflections on Open Access, Crowd Sourcing, and Metadata Standards",Open Library of the Humanities,2017,[],Digital Studies/Le champ numérique,7,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1218,http://dx.doi.org/10.16995/dscn.278,"{'string_id': '10.16995/dscn.278', 'id_scheme': 'DOI'}","Military history has provided significant insight into the factors determining the outcome of armed conflict through time. At the same time, it often fails to adequately assess variables unrelated to historical accounts per se that may contribute to military outcomes. For example, in 1066, English and Norman forces engaged in a decisive battle near Hastings, U.K. Numerous historical accounts have chronicled this event, using a combination of eyewitness and participant testimony, as well as written records, and art forms. Few, however, have paid significant attention to the role of the local landscape in shaping events. In the case of Hastings, the battlefield itself provides an example of the way in which geography can contribute to our understanding of historical events. By applying environmental sources and a regressive cartographic analysis, this study demonstrates that there is, in fact, considerable evidence to suggest how the landscape appeared back to the time of the battle. This finding is significant, insofar as it opens the door to new research on the Battle of Hastings which may shed additional light on the events that occurred there and the factors that influenced the outcome of this crucial conflict in British history. It also reveals the importance of applying new methodological approaches to traditional disciplines such as history, to deepen and expand existing analysis.",How did East Sussex Really Appear in 1066? The Cartographic Evidence,Open Library of the Humanities,2018,[],Digital Studies/Le champ numérique,8,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1219,http://dx.doi.org/10.16995/dscn.282,"{'string_id': '10.16995/dscn.282', 'id_scheme': 'DOI'}","This article pursues the idea that within interdisciplinary teams in which researchers might find themselves participating, there are very different notions of research outcomes, as well as languages in which they are expressed. We explore the notion of the software prototype within the discussion of making and building in digital humanities. The backdrop for our discussion is a collaboration between project team members from computer science and literature that resulted in a tool named TopoText that was built to geocode locations within an unstructured text and to perform some basic Natural Language Processing (NLP) tasks about the context of those locations. In the interest of collaborating more effectively with increasingly larger and more multidisciplinary research communities, we move outward from that specific collaboration to explore one of the ways that such research is characterized in the domain of software engineering—the ISO/IEC 25010:2011 standard. Although not a perfect fit with discourses of value in the humanities, it provides a possible starting point for forging shared vocabularies within the research collaboratory. In particular, we focus on a subset of characteristics outlined by the standard and attempt to translate them into terms generative of further discussion in the digital humanities community.",Prototyping Across the Disciplines,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,8,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1220,http://dx.doi.org/10.16995/dscn.283,"{'string_id': '10.16995/dscn.283', 'id_scheme': 'DOI'}","The goal of our research is to improve the process of disseminating knowledge about built heritage. To this end, we propose to implement upgradeable digital models that include the time dimension. These models are designed to illustrate the evolution of heritage buildings and sites over time and record the historian’s interpretation of documentary sources. The research objective is to develop a 4D modelling protocol to optimize data organization so information is easier to access and modify. This paper explains why BIM platforms do not seem appropriate tools for the work we are doing. We describe our method based on the synergy between a SQLite database and a 3D software (Autodesk Maya) linked by an algorithm written in Python. We conducted a case study involving a heritage place in Montreal, Canada. This site provided an opportunity to test the protocol developed because it is composed of several buildings that have evolved asynchronously.",4D Modelling of Built Heritage: A System Offering an Alternative to Using BIM,Open Library of the Humanities,2018,[],Digital Studies/Le champ numérique,8,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1221,http://dx.doi.org/10.16995/dscn.284,"{'string_id': '10.16995/dscn.284', 'id_scheme': 'DOI'}","This study identifies how the flagship Digital Humanities conference has evolved since 2004 and continues to evolve by analyzing the topical, regional, and authorial trends in its presentations. Additionally, we explore the extent to which Digital Humanists live up to the characterization of being diverse, collaborative, and global using the conference as a proxy. Given the increased popularization of “digital humanities” within the last decade, and especially recent successes in popular press and grant initiatives, this study tempers the sometimes utopic rhetoric that appears alongside mentions of the term.",What’s Under the Big Tent?: A Study of ADHO Conference Abstracts,Open Library of the Humanities,2017,[],Digital Studies/Le champ numérique,7,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1222,http://dx.doi.org/10.16995/dscn.285,"{'string_id': '10.16995/dscn.285', 'id_scheme': 'DOI'}","This paper returns to the relationship of “narrative versus database” (an argument originally made by Lev Manovich in 2001) as one that can be further addressed. A specific issue persists in text analysis research in the digital humanities: the difficulty of representing the figurative meaning of narratives through digital tools. Towards an accommodation, this paper adopts a narratological framework in order to propose alternative models of content management and organization that more closely resemble figurative meaning making in human language. These alternative models therefore better allow for the computational representation of figurative elements that N. Katherine Hayles describes as “the inexplicable, the unspeakable, the ineffable” of narrative literature. This paper argues that the construction of figurative meaning through paradigmatic substitution (as part of an imaginary vocabulary that is drawn from in the process of meaning making) is difficult to account for in the relational database—arguably still the most culturally prominent database model. By focusing on NoSQL (“no” or “not only” Structured Query Language) databases, this paper explores how layers of figurative meaning can be represented together through these flexible and non-relational models. In particular, the ability of non-relational databases to group together multiple values—encouraging their association, comparison, and juxtaposition—can be analyzed as a computational albeit imprecise counterpart to the formation of paradigmatic and figurative meaning. Thus, towards accounting for a word, image, or idea’s layers of meaning as expressed in literature, this paper offers a study of the limitations of digital tools and their critical negotiation with humanities research and reflection.",On the Value of Narratives in a Reflexive Digital Humanities,Open Library of the Humanities,2018,[],Digital Studies/Le champ numérique,8,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1223,http://dx.doi.org/10.16995/dscn.286,"{'string_id': '10.16995/dscn.286', 'id_scheme': 'DOI'}","The Digital Humanities Summer Institute gives students and scholars a chance to broaden their knowledge of the Digital Humanities within a feasible timeframe. The DHSI Colloquium was first founded by Diane Jakacki and Cara Leitch to act as a means of supporting graduates who wanted to be a part of such a gathering. The Colloquium has grown in recent years, to the point where it is now seen as an important part of the field’s conference calendar for emerging and established scholars alike, but it remains a non-threatening space in which students, scholars, and practitioners can share their ideas. This issue is testament to that diversity, as well as the strength of the research being presented at the Colloquium. It includes Scott B. Weingart and Nickoal Eichmann-Kalwara, Mary Borgo, William B. Kurtz, and John Barber. “What’s Under the Big Tent?: A Study of ADHO Conference Abstracts,” which portrays the discipline as one which is dominated by specific groups and practices. Using the Victorian Women Writers Project as a case-study, Mary Borgo treats models for the sustainable growth of TEI-based digital resources. William B. Kurtz details his experiences working on a digital initiative, in this instance, Founders Online: Early Access, and engages with the need for such projects to hold broader public appeal. John Barber’s “Radio Nouspace: Sound, Radio, Digital Humanities,” describes the curation of sound within the context of radio, and how such activity connects to creative digital scholarship. Together, these articles represent the purpose of facilitating a community comprised of divergent interests and perspectives, a community which can often be positively dissonant.",Introduction: Digital Humanities as Dissonant,Open Library of the Humanities,2018,[],Digital Studies/Le champ numérique,8,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1224,http://dx.doi.org/10.16995/dscn.288,"{'string_id': '10.16995/dscn.288', 'id_scheme': 'DOI'}","This paper examines the stylometric similarities between James Joyce and Flann O’Brien, demonstrating which works from the latter’s oeuvre are stylistically the most Joycean. We will outline the results of a series of quantitative enquiries focused specifically on Joyce and O’Brien, before offering a number of literary interpretations. It has long been argued that Brian O’Nolan, operating under the pseudonym of Flann O’Brien, is a disciple of James Joyce. This relationship remains a concern for scholars, and so our purpose here is to contribute some computational evidence to the discussion. We pinpoint those exact moments where O’Brien’s style is quantitatively similar to that of Joyce, using our results to re-engage existing arguments with renewed statistical precision.",Measuring Joycean Influences on Flann O’Brien,Open Library of the Humanities,2018,[],Digital Studies/Le champ numérique,8,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1225,http://dx.doi.org/10.16995/dscn.291,"{'string_id': '10.16995/dscn.291', 'id_scheme': 'DOI'}","Multivariate analysis (MVA) can be applied to the New Testament textual tradition in order to investigate grouping among its witnesses. This article applies certain MVA methods to a number of example data sets. Each method operates on a matrix that tabulates distances between pairs of items in a data set. The simple matching distance, which is the proportion of disagreements, can be used as a metric for calculating distances between New Testament witnesses. Analysis methods called classical multidimensional scaling (CMDS) and divisive clustering (DC) are useful for revealing group structure when it is well defined. However, they are less useful when grouping is not very distinct. A method called partitioning around medoids (PAM) provides another way to divide a data set into groups. Local maxima in a plot of a statistic called the mean silhouette width (MSW) indicate preferred numbers of groups. Statistical analysis of a data set allows upper and lower critical limits to be defined for the distance between a pair of witnesses. Distances between these limits are not significant in the sense that the same range of distances is expected to occur for generated pairs whose states are randomly chosen from the available pool. Distances that are either less than or greater than these critical limits are not likely to happen by chance. A distance less than the lower critical limit indicates an adjacent relationship while one greater than the upper limit implies an opposite relationship. Applying CMDS, DC, and PAM analysis to data for the Gospel of Mark reveals interesting features of the textual landscape. Witnesses tend to form groups that have points of contact with conventional categories such as the “Alexandrian,” “Byzantine,” “Western,” and “Eastern” types identified by prior generations of researchers. Multivariate analysis can also be used for novel purposes such as identifying group representatives, group cores, and readings useful for classification purposes.",How to Discover Textual Groups,Open Library of the Humanities,2018,[],Digital Studies/Le champ numérique,8,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1226,http://dx.doi.org/10.16995/dscn.292,"{'string_id': '10.16995/dscn.292', 'id_scheme': 'DOI'}","This study examines how social media networks are changing the ways party politics and election campaigns are conducted in Nigeria, especially how these technologies are encouraging new ways of wooing voters and engaging ordinary citizens. Specifically, this study focuses on the growing awareness of the use of Twitter for election campaigns and civic engagement between 2012 and 2015. The theoretical framework used in the study adopts approaches from Computer-Mediated Communication (CMC) and Computer-Mediated Discourse Analysis (CMDA- e.g. Herring 2001, 2004). The data set consists of selected microblogging posts extracted from the Twitter handles of two major political parties and their political candidates. Additional Twitter posts on the election-related issues trending during the period provided supplemental data. The data collection procedures comprised media monitoring and use of computer software packages (e.g. SketchEngine, Topsy, AntConc) for online data harvesting over a period of fourteen months. This method was supplemented with manual downloading using google search. The analytical procedure follows the content analysis of significant textual features and discourse patterns in the online political conversation that sheds light on the persuasive use of technologies as discourse strategy. The study thus highlights the sociality of politics and socio-communicative features of Twitter as a campaign tool to mobilise support and woo voters in an emerging democracy.",The Twittersphere as Political Engagement Space: A Study of Social Media Usage in Election Campaigns in Nigeria,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1227,http://dx.doi.org/10.16995/dscn.293,"{'string_id': '10.16995/dscn.293', 'id_scheme': 'DOI'}","Bullipedia, the online gastronomic encyclopedia, is an idea yet to be developed. In this work, we analyze Stack Overflow (SO) and extract some good practices from this popular question-and-answer (Q&amp;A) site to incorporate them into the future Bullipedia. SO is an online forum in which users ask and answer questions related to programming, web development, operating systems, and other technical topics. Expertise is rewarded through a detailed reputation system: questions and answers can receive up and downvotes from other members of the community so that their authors (askers and answerers) gain reputation for posing good questions and providing helpful solutions. Besides this, the asker may mark (accept) one of the answers as the best one at any point. In this paper, we present a study on how this reputation system can be used to predict the likely accepted answer (from a set of candidate answers) for a yet unresolved question. In our approach, we selected a subset of questions with their respective answers, and for each answer we created a question-answer pair (quan). Then we extracted a set of key features from every quan, and applied supervised machine learning techniques to train a classifier that learnt, based on those features, whether or not a quan contained the accepted answer for that question. Finally, we made use of the trained classifier to predict if, given a quan (related to a question with no marked answer), its answer might potentially be the accepted one for the question. Our findings show that the model previously obtained predicted the possible answer correctly for every question with high accuracy (88 percent of the time). A question and its accepted answer constitutes a source of quality knowledge, as it provides the solution to a specific problem. We propose to adopt a similar Q&amp;A forum and reputation system for Bullipedia, and then apply a similar classification model to identify the best answer for unsolved questions.",Community Engagement and Quality Knowledge with Stackoverflow’s Reputation System: Applications to Bullipedia,Open Library of the Humanities,2018,[],Digital Studies/Le champ numérique,8,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1228,http://dx.doi.org/10.16995/dscn.295,"{'string_id': '10.16995/dscn.295', 'id_scheme': 'DOI'}","This essay demonstrates how topic modeling can be fruitfully applied to TEI-encoded plays, which allows scholars to analyze speeches by individual characters. Our analysis centers on Shakespeare’s corpus and characters who reappear in multiple plays. Specifically, we use topic models to show that young Prince Hal (in and 2 Henry IV) does not speak the same language as his later self, Henry V (in his titular play): his linguistic shift mirrors his shift in status. Hal himself announces, “I have turned away my former self”—his change in diction bears out his assertion. Conversely, topic models reveal that Falstaff is Falstaff across multiple plays and genres (notably, and 2 Henry IV and The Merry Wives of Windsor), despite scholarly claims to that the Falstaff of comedy is a watered-down version of the braggart drunk of the history plays. Ultimately, we hope that this algorithmically-informed analysis of Shakespeare’s plays is not taken as a final answer, but, instead, as a prompt. As this research reveals, topic modeling plays with attention to each speaker opens the door for new comparisons, and in turn, expands on previous interpretations of literature.",Is Falstaff Falstaff? Is Prince Hal Henry V?: Topic Modeling Shakespeare’s Plays,Open Library of the Humanities,2018,[],Digital Studies/Le champ numérique,8,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1229,http://dx.doi.org/10.16995/dscn.299,"{'string_id': '10.16995/dscn.299', 'id_scheme': 'DOI'}","From a digital historian’s point of view, Ancien Régime French texts suffer from obsolete grammar, unreliable spelling, and poor optical character recognition, which makes these texts ill-suited to digital analysis. This paper summarizes methodological experiments that have allowed the author to extract useful quantitative data from such unlikely source material. A discussion of the general characteristics of hand-keyed and OCR’ed historical corpora shows that they differ in scale of difficulty rather than in nature. Behavioural traits that make text mining certain eighteenth century corpora particularly challenging, such as error clustering, a relatively high cost of acquisition relative to salience, outlier hiding, and unpredictable patterns of error repetition, are then explained. The paper then outlines a method that circumvents these challenges. This method relies on heuristic formulation of research questions during an initial phase of open-ended data exploration; selective correction of spelling and OCR errors, through application of Levenshtein’s algorithm, that focuses on a small set of keywords derived from the heuristic project design; and careful exploitation of the keywords and the corrected corpus, either as raw data for algorithms, as entry points from which to construct valuable data manually, or as focal points directing the scholar’s attention to a small subset of texts to read. Each step of the method is illustrated by examples drawn from the author’s research on the hand-keyed Encyclopédie and Bibliothèque Bleue and on collections of periodicals obtained through optical character recognition.",How to Extract Good Knowledge from Bad Data: An Experiment with Eighteenth Century French Texts,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1230,http://dx.doi.org/10.16995/dscn.3,"{'string_id': '10.16995/dscn.3', 'id_scheme': 'DOI'}","In this chapter, I will first position my considerations about the sustainability of ebooks for future research in the context of book history; moreover, I will introduce basic concepts of (migration and emulation) as well as challenges to the long-term preservation of digital data in greater generality. After that and taking current research on past (printed) books as an analogy, I shall try to outline which kinds of (digital) material will be indispensable in the future to do research on today's digital book production: published digital materials and unique (digital) artefacts. Finally, I shall present the results of my exemplary studies to find out which precautions for the future availability of digital material have been taken by various (German) institutions involved in the production of digital forms of books (typically publishers) as well as by public institutions, the mandate of which is to collect and secure future access to current books and unique artefacts (typically libraries and archives). Summarizing, I will expose what this could mean for future research on today´s ebooks--and for the requirements concerning ebook preservation measures and policies.",Chapter 4Will our children have the chance to do research on today’s digital books?,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1231,http://dx.doi.org/10.16995/dscn.301,"{'string_id': '10.16995/dscn.301', 'id_scheme': 'DOI'}","The Game of Writing (GWrit) is an online writing environment where students can comment on each other’s writing and where they get rewards for on task activity (gamification). This paper brings together research on GWrit from the following perspectives: gamification, analytic tools, user habits, evaluation and task completion structures. First we introduce the way the system was designed to support experimenting with gamification and show the gamification rule editing environment we are developing. Second, we discuss the ways we are evaluating the interface of GWrit using Cognitive Walkthrough and Heuristic evaluation approaches. Third, we look at user behavior based on Google Analytics and compare this to the behavior expected and desired. Finally, we discuss the role task completion structures play in motivating learning.",Introduction to a Class-based Online Writing Environment: Gwrit (Game of Writing),Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1232,http://dx.doi.org/10.16995/dscn.302,"{'string_id': '10.16995/dscn.302', 'id_scheme': 'DOI'}","This article examines key ethical issues that are continuing to emerge from the task of archiving data scraped from online sources such as social media sites, blogs, and forums, particularly pertaining to online harassment and hostile groups. Given the proliferation of digital social data, an understanding of ethics and data stewardship that evolves alongside the shifting landscape of digital societies is indeed essential.",Applying an Ethics of Care to Internet Research: Gamergate and Digital Humanities,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1233,http://dx.doi.org/10.16995/dscn.307,"{'string_id': '10.16995/dscn.307', 'id_scheme': 'DOI'}","This paper presents the first phase in the development of a new, TEI-based protocol for the encoding of promptbooks. Because the principal function of a promptbook is to record spatiotemporal events whose communicative importance supersedes that of the book in which they are recorded, current standards for digital encoding do not always apply. With the stage managerial artifacts of The. John Gray and the Canadian Stratford Festival Archives as case studies, we provide a rationale for exploring additions to the existing TEI guidelines to account for the unique characteristics of promptbooks.",Tagging Time and Space: TEI and the Canadian Stratford Festival Promptbooks,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1234,http://dx.doi.org/10.16995/dscn.309,"{'string_id': '10.16995/dscn.309', 'id_scheme': 'DOI'}","This article utilizes a combination of digital and traditional literary-critical analysis in order to explore ominous dreams in the Elizabethan dramatist Thomas Kyd’s undoubted plays The Spanish Tragedy (1587), Soliman and Perseda (1588), and Cornelia (1594). The article proceeds to explore the use of dreams in the anonymously printed early modern plays King Leir (1589) and Arden of Faversham (1590) in order to demonstrate that these works are in good accordance with Kyd’s highly individual thought processes and overall dramaturgy. Close reading of these texts is accompanied by an examination of the characteristics of Kyd’s verse style, as well as collocation matching, which provides quantitative and qualitative evidence for Kyd’s idiosyncratic lexicon of words and phrases. This study extends and corrects the work of generations of attribution scholars and suggests that King Leir and Arden of Faversham can be ascribed to Kyd with a high degree of probability.",“Fearful Dreams” in Thomas Kyd’s Restored Canon,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1235,http://dx.doi.org/10.16995/dscn.315,"{'string_id': '10.16995/dscn.315', 'id_scheme': 'DOI'}","The National Library of Finland has digitized and made available the historical newspapers published in Finland between 1771 and 1910 (Bremer-Laamanen 2014; Kettunen et al. 2014). This collection contains approximately 1.95 million pages in Finnish and Swedish. The Finnish part of the collection consists of about 2.40 billion words. The National Library’s Digital Collections are offered via the digi.kansalliskirjasto.fi web service, also known as Digi. An open data package of the whole collection was released in early 2017 (Pääkkönen et al. 2016). Quality of OCRed collections is an important topic in digital humanities, as it affects general usability and searchability of collections. There is no single available method to assess quality of large collections, but different methods can be used to approximate quality. This paper discusses different corpus analysis style methods to approximate overall lexical quality of the Finnish part of the Digi collection. Methods include usage of parallel samples and word error rates, usage of morphological analysers, frequency analysis of words and comparisons to comparable edited lexical data. Our aim in the quality analysis is twofold: firstly to analyse the present state of the lexical data and secondly, to establish a set of assessment methods that build up a compact procedure for quality assessment after e.g. re-OCRing or post-correction of the material.",How to Do Lexical Quality Estimation of a Large OCRed Historical Finnish Newspaper Collection with Scarce Resources,Open Library of the Humanities,2020,[],Digital Studies/Le champ numérique,10,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1236,http://dx.doi.org/10.16995/dscn.316,"{'string_id': '10.16995/dscn.316', 'id_scheme': 'DOI'}","In this paper, we describe how the hypergeometric test can be used to determine whether a given theme of interest occurs in a storyset at a frequency more than would be expected by chance. By a storyset we mean simply a list of stories defined according to a common attribute (e.g., author, movement, period). The test works roughly as follows: Given a background storyset and a sub-storyset of interest, the test determines whether a given theme is over-represented in the sub-storyset, based on comparing the proportions of stories in the sub-storyset and background storyset featuring the theme. A storyset is said to be “enriched” for a theme with respect to a particular background storyset, when the theme is identified as being significantly over-represented by the test. Furthermore, we introduce here a toy dataset consisting of 280 manually themed Star Trek television franchise episodes. As a proof of concept, we use the hypergeometric test to analyze the Star Trek stories for enriched themes. The hypergeometric testing approach to theme enrichment analysis is implemented for the Star Trek thematic dataset in the R package stoRy. A related R Shiny web application can be found at https://github.com/theme-ontology/shiny-apps.",Theme Enrichment Analysis: A Statistical Test for Identifying Significantly Enriched Themes in a List of Stories with an Application to the Star Trek Television Franchise,Open Library of the Humanities,2020,[],Digital Studies/Le champ numérique,10,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1237,http://dx.doi.org/10.16995/dscn.319,"{'string_id': '10.16995/dscn.319', 'id_scheme': 'DOI'}","This article relates to the 2018 CSDH/SCHN conference proceedings. This paper outlines Michael Iantorno’s and Melissa Mony’s experiences with quantitative game analysis by summarizing the first year of development of the prototype ludomusicological database GameSound. To further the discussion, this article also summarizes and analyzes the work of fellow digital humanities scholar Jason Bradshaw, who applied intriguing types of tool-based analysis to BioShock Infinite. To conclude, the paper hypothesizes where this type of research could lead in the future: both for GameSound and for other projects using similar methods and methodologies.","GameSound, Quantitative Games Analysis, and the Digital Humanities",Open Library of the Humanities,2020,[],Digital Studies/Le champ numérique,10,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1238,http://dx.doi.org/10.16995/dscn.325,"{'string_id': '10.16995/dscn.325', 'id_scheme': 'DOI'}","In the summer of 2017, Quinn Dombrowski, an IT staff member in UC Berkeley’s Research IT group, approached Geoffrey Rockwell about the possibility of merging the DiRT Directory with TAPoR, both popular tool discovery portals. Dombrowski could no longer offer the time commitment required to maintain the organizational structure of the volunteer-run tool directory (2018). This decommissioning of DiRT illustrates a set of problems in the digital humanities around tool directories and the tools within as academic contributions. Tool development, in general, is not considered sufficiently scholarly and often suffers from a lack of ongoing support (Ramsay & Rockwell, 2012). When tool discovery portals are no longer maintained due to a lack of ongoing funding, this leads to a loss of digital humanities knowledge and history. While volunteer-based directories require less outright funding, managing and motivating those volunteers to ensure that they remain actively involved in directory upkeep requires a vast amount work to ensure long-term sustainability (Dombrowski, 2018). This paper will explore the difficult history of tool discovery catalogues and portals and the steps being taken to save the DiRT Directory by integrating it into TAPoR. In particular, we will: – Provide a brief history of the attempts to catalogue tools for digital humanists starting with the first software catalogues, such as those circulated through societies, and ending with digital discovery portals, including DiRT Directory and TAPoR. – Discuss the challenges around the maintenance of discovery portals – Consider the design and metadata decisions made in the merging of DiRT Directory with TAPoR.",Absorbing DiRT: Tool Directories in the Digital Age,Open Library of the Humanities,2020,[],Digital Studies/Le champ numérique,10,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1239,http://dx.doi.org/10.16995/dscn.326,"{'string_id': '10.16995/dscn.326', 'id_scheme': 'DOI'}","Social Media and Digital Activism, in the context of the Global South, have continued to redefine identities and transform traditional notions of citizenship, nationhood, and belonging. With the power of digital technologies to bring public and private issues to light, digital cultures have enabled the shaping of contemporary faces of entire nations in the South. The most potent example of how digital activism and new age digital cultures have helped chisel a South Asian queer identity in India and beyond is Section 377, an archaic, colonial law, established in the country by the British Raj, and that until August 2018, criminalized homosexuality as “carnal intercourse” or an “unnatural sexual act.” Although not applicable specifically to homosexuals, it is widely perceived as an anti-queer law in the country; cloaked in colonial textual ambiguity, it directs the colonial desire for sexual control towards colonial societies. This research focuses on an exploration of Indian queer dissident subaltern counterpublics, and how these communities (both members and non-members) congregate on Twitter to combat postcolonial and Victorian structures through new media as a decolonizing and transnational space. Therefore, the hashtags surrounding this event perform and decolonize Indian cyberspaces. The study equally investigates how the new wave of digital activism post reading down of the Section 377 of the Indian Penal Code has fortified identities of the marginalized and the non-marginalized alike and transformed the notion of the “self” and the “other.” Given this premise, this paper employs Twitter Search API to collect and filter tweets that explore the private and public discourses of a digital movement that has proved instrumental in the creation of digital spaces for the marginalized “other.” This body of work contributes to the pool of research that is focused on the understanding of South Asian digital identities and the creation of their digital spaces as a solution for social, legal, and political discrimination.",Digital Queer: Tracing the Digital Discourse around Section 377 of the Indian Penal Code,Open Library of the Humanities,2020,[],Digital Studies/Le champ numérique,10,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1240,http://dx.doi.org/10.16995/dscn.329,"{'string_id': '10.16995/dscn.329', 'id_scheme': 'DOI'}","This essay describes the first phase of the GEMMS project (2014–2019), the purpose of which is to increase access to early modern English manuscript sermons and sermon notes (1530–1715) by creating a freely accessible online bibliographic database of records for these materials. It reviews the need for this resource, the current state of its development, and the challenges and successes to date as well as proposed developments in its next phase.",GEMMS (Gateway to Early Modern Manuscript Sermons 1530–1715): Confronting the Challenges of Sermons Research,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1241,http://dx.doi.org/10.16995/dscn.332,"{'string_id': '10.16995/dscn.332', 'id_scheme': 'DOI'}","Digital technologies are essential to establishing new forms of dominance through drones and surveillance systems; these forms have significant effects on individuality, privacy, democracy, and American foreign policy; and popular culture registers how the uses of drone technologies for aesthetic, educational, and governmental purposes raise questions about the exercise of individual, governmental, and social power. By extending computational methodologies in the digital humanities like macroanalysis and distant reading in the context of drones and surveillance, this article demonstrates how drone technologies alter established notions of war and peace, guilt and innocence, privacy and the common good; in doing so, the paper connects postcolonial studies to the digital humanities.",Drones and Surveillance Cultures in a Global World,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1242,http://dx.doi.org/10.16995/dscn.34,"{'string_id': '10.16995/dscn.34', 'id_scheme': 'DOI'}"," Aequilibrium is a transmedia and location-based experience which engages players in an adventure that blurs the boundaries between digital and physical spaces, imaginary and real people from the local community. The project is the first Italian Alternate Reality Game (ARG) dedicated to Leonardo Da Vinci and is aimed at promoting the territory of Lomellina adopting innovative communication strategies to engage players in a game which is designed to amuse and encourage tourism. The transmedia and the locative experience were designed by a team of researchers from the Politecnico di Torino and Guglielmo Marconi University. Starting from the analysis of the Aequilibrium project, this essay focuses on how alternate reality games can promote participation in cultural experiences which offer cultural activators fostering collaboration and creative grassroots production within communities of players.",Alternate Reality Game and Culural Heritage: The “Aequilibrium” Project,Open Library of the Humanities,2015,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1243,http://dx.doi.org/10.16995/dscn.346,"{'string_id': '10.16995/dscn.346', 'id_scheme': 'DOI'}","This article presents how certain practices in contemporary art enrich the analysis and critical reflections on digital culture, taking art history and museology as two important issues. The former touches on culture’s relational and participatory value in networks; the latter is related to the effects of this value in the visualisation and canonization of works of art. What is the nature of my appropriation of art when I’m invited to choose works, on a museum’s website, to create “my collection,” “my art gallery,” “my personal museum?” Does this type of invitation favour the democratisation of art knowledge? Or, on the contrary, do these initiatives replicate institutional principles that maintain the prescribing museum’s authority? Does participatory cybermuseology present new issues that result in communal action? Or, alternatively, does it consistently reproduce the traditional canonization process of works of art without permitting a critical reinterpretation? To answer these questions, the theoretical analysis presents proposes layouts, online, of works that originate both from the public domain and from collections from large museums, ranging from the artistic practices of Julia Weist to the Google Art Project.",Muséologie numérique: Action participative en réseau et canonisation selon l’intelligence artificielle,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1244,http://dx.doi.org/10.16995/dscn.347,"{'string_id': '10.16995/dscn.347', 'id_scheme': 'DOI'}","This article studies a part of the works of the Wu Ming literary collection. A true cultural, editorial and generational phenomenon in Italy, it is considered emblematic of the network culture and is considered a pioneer of the collision between old and new medias. Thanks to digital technology, we are indeed participating in the multiplication of textualities. Forums, blog, novel, collective forms of writing all are being born in the world of the web, redefining textuality in both its form and its content. This evolution is considered here through a perspective of a more general change in cultural sociability. After historicising these changes. We will commit ourselves to an analysis of its use of digital tools as a means of political and literary expression with the goal of highlighting its intellectual approach. We will show that Wu Ming’s use of the net, expanded to literary work, contributes to the entrenchment of a new culture based on another dialectic between producers and consumers of texts, producing a different view on the works. The web becomes a new channel of exchange, of genre contamination, of meaning transmission, and becomes a free space that establishes a new narrative economy and a redefinition of the scholarly domain.",Création littéraire et Web: un cas italien,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1245,http://dx.doi.org/10.16995/dscn.348,"{'string_id': '10.16995/dscn.348', 'id_scheme': 'DOI'}","Critical editing is a practice whose contours have been well established by the philological tradition. The different critical conventions are well known and have been subject to a myriad of scientific studies. Nevertheless, critical edition has been undergoing changes since the development of new technologies. Digital critical editions have multiplied and today represent a large and varied panel. As a consequence, it’s worth observing, beyond technological innovations, what digital editing brings in terms of methodology and publishing conceptualisation. Obviously, in digital editions as in traditional editions, the final goal is offering readers a text that they will be able to consult and use in their research: the edited text should be a stable one, that can be an authoritative resource. The uniqueness of digital edition’s design, however, requires editors to reflect, right from the start of the publication process, on the information that they are going to provide, as well as on the way in which they will be presenting it. From the very beginning of its development, digital editing entails, then, a true awareness of how it will be received. It permits, also, a significant diversification of information presented in editions. It ultimately distinguishes itself with its open nature: digital editing is open in terms of content evolution over time just as it is in terms of collaborative editing.",Le positionnement éditorial dans l’édition critique numérique,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1246,http://dx.doi.org/10.16995/dscn.349,"{'string_id': '10.16995/dscn.349', 'id_scheme': 'DOI'}","For historians dealing with text, the changeover from paper to digital sources represents a major change. By expanding text archives – turned into hypertexts, un-linear and denaturalised –, the digital increases the navigation capabilities within corpora, reveals new linguistics aspects that can be observed, and transforms our reading practices. Logometry, the method using computer-assisted reading, has been contributing to these evolutions over the last four decades by proposing tools that combine qualitative reading and quantitative approaches to digital corpora, with the goal of formalising the hermeneutic interpretive process. This article will present its principle functions through its application to a corpus of electoral manifestos written under the Fifth Republic (1958–2007).",La logométrie en histoire: une herméneutique numérique. Exploration d’un corpus de professions de foi électorales de député-e-s (1958–2007),Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1247,http://dx.doi.org/10.16995/dscn.35,"{'string_id': '10.16995/dscn.35', 'id_scheme': 'DOI'}","That the 2012 Congress of the Canadian Federation for the Humanities and Social Sciences censored the Bonfire of the Humanities, a student-produced alternate reality game, apparently in order to preserve the reputations of its invited speakers and host universities is an alarming incident worthy of further inquiry and censure by Canada's scholarly and artistic communities. Curiously, the justification provided for censoring the game echoes the ethical arguments advanced by scholars and designers of pervasive games who warn that non-player bystanders are at risk of confusing games with reality. This essay considers the case in relation to Jacques Rancière's ethics of recognition. I argue that pervasive gamers should stop disguising their activities to protect bystanders, and begin instead to recognize non-players as equally capable of negotiating the blurred boundaries of game and world. The emancipation of game spectatorship creates genuinely political encounters, and opens up new channels for scholarly experimentation and dissemination.",Don't make a scene: Game studies for an uncertain world,Open Library of the Humanities,2015,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1248,http://dx.doi.org/10.16995/dscn.350,"{'string_id': '10.16995/dscn.350', 'id_scheme': 'DOI'}","Bretez is a cooperative science project linking the humanities and the engineering sciences (digital humanities) whose prime purpose is museography. Its objective, the patrimonial valorisation through its three-dimensional digital restoration and spatial sound restoration, sets itself apart with its new approach to the restoration of the past by combining the 3D with a prominent acoustic aspect that makes the past available and tangible for a very wide audience. This presentation of my work (research focus: archeology of soundscapes) draws on the model that acts as a research template and focuses more specifically on the rendition of soundscapes, while proposing the following question: how can we interpret the past in order to delight our senses without misrepresenting History?",Projet Bretez: une pincée de son dans l’Histoire,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1249,http://dx.doi.org/10.16995/dscn.351,"{'string_id': '10.16995/dscn.351', 'id_scheme': 'DOI'}","In 1970, Gabrielle Roy published Windflower (La Rivière sans repos), a short novel preceded by three short stories “Nouvelles Esquimaudes.” Conserved in the Gabrielle Roy collection of Library and Archives Canada, the archives contain a myriad of manuscripts, dactylograms and sets of proofs, genuine evidence of word done by this author. In this article, the researchers put forward a meticulous genetic study of the three “Nouvelles Esquimaudes,” which takes the “Voyage en Ungava,” a travel journal written by Gabrielle Roy in 1961 in Fort Chimo, as its starting point. The authors highlight the novelist’s work methods, notably her horizontal and vertical rewriting. The digital publishing of the Windflower archives allows in itself the usage of an enlarged critical apparatus, but it also contains several specific concerns, thus raising several challenges.",Les archives sans repos: enjeux d’une édition électronique,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1250,http://dx.doi.org/10.16995/dscn.352,"{'string_id': '10.16995/dscn.352', 'id_scheme': 'DOI'}","It appears that the virtual – in the sense attributed by cybernetics, namely modelling or simulation –, is entering into certain research practices in the human and social sciences, and notably those of certain historians using GIS (or Geographic Information Systems). In that respect, research involving GIS will be examined in order to evaluate, in particular, what the results are for the history discipline in terms of its methods, its concepts, and its renewal, or nonrenewal, of knowledge. Before presenting its initial findings, we will retouch on questions underlying this study and indicate some essential methodological elements.",Histoire et SIG: vers une heuristique des cartes relationnelles,Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1251,http://dx.doi.org/10.16995/dscn.354,"{'string_id': '10.16995/dscn.354', 'id_scheme': 'DOI'}","Digital technologies have profoundly changed our research practices, our editing and publication practices, and how we exchange information. The means have certainly changed, but these striking changes have also influenced the methods and meaning of writing, the concepts of writer and reader, and the methods of access, the tools and methods of distributing texts, images, sounds, etc. Literature has played a major role in the development of cultural domains; it still holds a prominent place, perhaps even more so today, as our current context is greatly associated to the predominance of the textual form. The explosion of data and the transformation of knowledge, both in terms of the quality of available information and the manner of representing it, have furthermore had a considerable impact on human and social sciences research.","Introduction: « Humanités numériques: identités, pratiques et théories »",Open Library of the Humanities,2019,[],Digital Studies/Le champ numérique,9,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1252,http://dx.doi.org/10.16995/dscn.355,"{'string_id': '10.16995/dscn.355', 'id_scheme': 'DOI'}","In this essay, I investigate the development of algorithms from a digital paradigm in Victorian England, specifically through the work of Ada Lovelace and the influences of the Jacquard loom. I consider Lovelace’s algorithms through the framework of poetics, that is, how meaning is made and materialized through symbolic inscription. Within the discursive contexts of industrial manufacturing and Romanticism, I find that an algorithmic mode of production emerges from the consideration and inscription of memory. Since the ramifications of inscription and memory echo throughout contemporary computing and the Digital Humanities, examining the logics and paradigms that computational inscriptions reproduce are increasingly vital today. Thus I ultimately argue for the poetic analyses of algorithms and computer code.",The Poetics of Computer Code: Tracing Digital Inscription in Ada Lovelace’s England,Open Library of the Humanities,2020,[],Digital Studies/Le champ numérique,10,1,"[{'value': '1918-3666', 'type': 'electronic'}]"
1253,http://dx.doi.org/10.16995/dscn.36,"{'string_id': '10.16995/dscn.36', 'id_scheme': 'DOI'}",This chapter examines the ethical concerns surrounding Alternate Reality Games and related experiences that blur distinctions between reality and fiction.,Towards an ethics of alternate reality games,Open Library of the Humanities,2017,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1254,http://dx.doi.org/10.16995/dscn.37,"{'string_id': '10.16995/dscn.37', 'id_scheme': 'DOI'}","The paper explores digital scholarship at Siberian Federal University and considers the incentives for Siberian scholars to use digital data in their research, build their own databases and digital editions, and develop research questions based on new methods and tools. Although the first stage of digital scholarship (providing digital content) seems to be a well mastered skill for Siberian researchers, the second stage of working with digital data and new research questions in the humanities does not seem to be within their comfort zone. We conclude that random digital humanities initiatives do not guarantee either demand for digital humanities or knowledge and understanding of new research questions inspired by new methodologies. Further studies might be needed to understand if we require lobbying for digital humanities and, if so, what kind of promotion, dissemination and training are needed or would be most effective.",Digital Humanities at Siberian Federal University,Open Library of the Humanities,2015,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1255,http://dx.doi.org/10.16995/dscn.38,"{'string_id': '10.16995/dscn.38', 'id_scheme': 'DOI'}","Contemporary language documentation workflow is a largely digital process. While this has had many benefits for how linguists undertake language documentation projects, it has also lead to a disparity between how the process is conceptualised by academic researchers, and how it is conceptualised by the speakers of endangered languages. In this paper I discuss the nature of this disparity, and illustrate this with my own experience of working with speakers of Tibeto-Burman languages in Nepal. In my own research I have incorporated ongoing discussion regarding digital methods into my working relationships with participants, but other researchers have made digital training a specific feature of their research methodology. I discuss two projects that provide positive models for this kind of digital gap bridging. The first is the Iltyem-iltyem sign website and the second is the Aikuma language documentation phone application. After discussion of these positive developments in digital outreach I discuss some of the challenges that we still face in ensuring that what we do is engaging and relevant for the communities we work with. This discussion is not only relevant for language documentation researchers, but for all who work in the digital humanities, as we need to be more aware of the different needs and levels of digital education of different communities.",Language documentation and division: Bridging the digital divide,Open Library of the Humanities,2015,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1256,http://dx.doi.org/10.16995/dscn.39,"{'string_id': '10.16995/dscn.39', 'id_scheme': 'DOI'}","Starting with objectively laconic questions I would like to focus for a moment on the Humanities and encourage to some reflection. For some time researchers have been increasingly interested in entrepreneurship and the commercialisation of research. This phenomenon is discussed in the popular media, but also, perhaps especially, in the universities. The result is various training courses, workshops and conferences aimed at improving university-business collaboration. These have also produced reports and strategies outlining development opportunities. Young researchers (but not only) are encouraged to conduct innovative and modern research that will answer real market needs. And all this work is said to be very important for the development of the country–especially its competitiveness. While it is impossible to disagree with the desire for national improvement, this emphasis on commercialisation does immediately raise a question about where the Humanities fit in. How can they participate into this approach? In which direction should they develop? Can the development of Digital Humanities and media literacy make a difference in a field that is otherwise in danger of being seen as irrelevant to contemporary trends?",What about the Humanists? Digital Humanities as an answer to contemporary perceptions of the Humanities on the Polish language Internet,Open Library of the Humanities,2015,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1257,http://dx.doi.org/10.16995/dscn.4,"{'string_id': '10.16995/dscn.4', 'id_scheme': 'DOI'}","One of the most potent criticisms of the TEI as a means for describing documents is that it is deterministic: it forces upon a text a set of a priori categories and thus largely determines how the text is understood. My contention is that critiques offered by scholars such as Jerome McGann can be answered in part by modifying one's conception of the text's poietic frame. If the entire process of editing is conceived of as an autopoietic system, the act of marking the text according to the TEI standards becomes but one stage of a process that is repeatedly questioned and open to revision. By conceiving of the system as editorial interaction with successive iterations of the text, the editor is made aware of the inherent textual and linguistic ambiguities of the source document and is forced to read actively and engage these ambiguities in pursuit of an adequate electronic text. These ideas will be demonstrated with examples from my work in digitizing a manuscript of records belonging to William Courten, a 17th-century naturalist and collector.",Chapter 5Digital editing as autopoietic process,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1258,http://dx.doi.org/10.16995/dscn.40,"{'string_id': '10.16995/dscn.40', 'id_scheme': 'DOI'}","Starting with objectively laconic questions I would like to focus for a moment on the Humanities and encourage to some reflection. For some time researchers have been increasingly interested in entrepreneurship and the commercialisation of research. This phenomenon is discussed in the popular media, but also, perhaps especially, in the universities. The result is various training courses, workshops and conferences aimed at improving university-business collaboration. These have also produced reports and strategies outlining development opportunities. Young researchers (but not only) are encouraged to conduct innovative and modern research that will answer real market needs. And all this work is said to be very important for the development of the country–especially its competitiveness. While it is impossible to disagree with the desire for national improvement, this emphasis on commercialisation does immediately raise a question about where the Humanities fit in. How can they participate into this approach? In which direction should they develop? Can the development of Digital Humanities and media literacy make a difference in a field that is otherwise in danger of being seen as irrelevant to contemporary trends?",A co z humanistami? – Cyfrowa humanistyka jako odpowiedź na obecny stan postrzegania humanistyki w polskojęzycznym internecie,Open Library of the Humanities,2015,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1259,http://dx.doi.org/10.16995/dscn.41,"{'string_id': '10.16995/dscn.41', 'id_scheme': 'DOI'}","Digital Humanities is getting a lot of attention! But do they exist as a community, unified and consistent? Is this community’s government balanced and democratic? Up to now, no study has explored such a community by investigating its members, looking at them through the prism of multilingualism and geography. The ""Who are you, Digital Humanists?"" survey, conducted after THATCamp Luxembourg (Terras 2012b) and promoted at DH2012 (Hambourg), provided an incomplete, though already significant, sample of 850 people who accepted to answer a questionnaire. The survey found a highly mixed linguistic and geographic profile, the existence of outsiders who either were not aware of the study or did not pay attention to it, the marginalization of English as a first language, and the dominance of English as a second language. It showed that Digital Humanities are strongly influenced by History and Classical Studies, but have little or no bearing on disciplines concerned with today’s world, on the one hand, and with Web sciences, data mining and text mining, on the other hand. A major event on cultural diversity, the Digital Humanities 2012, was governed by Europe and North America, namely the United Kingdom and its former colonies (Ireland, Canada, the United States and Australia). The Anglophone community strikes again, it appears. In order to measure progress in the diversity of the power of our community, this paper proposes the creation of an indicator, the Digital Humanities Decision Power (DHDP), which measures the gap between the various groups of Digital Humanists and their power in the field and the scientific selection procedures. On the basis of this indicator, a collective debate is necessary to make our community more responsive to linguistic and geographic diversity. We call this the Finnish Sauna strategy.",La stratégie du sauna finlandais: Les frontières des Digital Humanities,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1260,http://dx.doi.org/10.16995/dscn.42,"{'string_id': '10.16995/dscn.42', 'id_scheme': 'DOI'}","This article attempts to analyze the definition of ""electronic literature"" as given by the Electronic Literature Organization and to understand the relationship between electronic literature and digital literature. The thesis defended here is that this change of adjective hides a change in the theoretical status of the object that it attempts to define. The definition of electronic literature once concentrated on the tools used in the production of literary works and the critical analysis concentrating on the objects produced with the aid of new technologies. The move to the adjective ""digital"" marks a change in perception: now, from this perspective, the challenge is no longer to study works produced thanks to computers, but to understand the new status of literature in the age of the digital. To demonstrate this thesis, the article proposes to analyze one recent literary example: the 1984 trilogy by Eric Plamondon.","La littérature numérique, existe-t-elle?",Open Library of the Humanities,2015,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1261,http://dx.doi.org/10.16995/dscn.43,"{'string_id': '10.16995/dscn.43', 'id_scheme': 'DOI'}","This paper aims to take into account the process of annotation as it can be inserted in a literary text driven by digital culture. Focusing on a corpus of digital literary texts, it will study how writers use hyperlinking as a way to build a network of allusions or commentaries inside the boundaries of their own text, or as support of reading of an another text. Three ranges of use are to be exposed. The first acts as a mediated reading of one's own work. Guillaume Vissac's Accident de personne, published as a twitter feed in 2010, has been edited as a (digital) whole by digital editor publie.net. The author used hyperlinks to reinforce patterns of similarity between characters and specific circumstances. The second use of semantic links inserts essayistic discourse and urban fabulation, as it can be found in François Bon's Tiers livre, in the web of its tentacular work and in a discursive ground of the social and literary environment of the writer. The third range of use covers writing-as-reading works, where links enact a set of commentaries or a way of rewriting a prior text. Philippe De Jonckheere's site, Désordre.net, and more specifically his project ""Tentative d'épuisement de Tentative d'épuisement d'un lieu parisien de Georges Perec,"" build on this idea. This project takes Perec's own reading of an urban place which tries to exhaust the real world by description, and adds a layer of hyperlinks giving back a reading of the reality depicted by Perec. These three uses show how literary writing in a digital context inherit from the tradition of marginal annotation and inscribe it in the most basic function of electronic publication, which is linking. In that perspective, links appear as a way to build a multidimensional textuality based on the depth of a virtual network.",Inner Margins of the Literary Digital Text: From Allusion to Rewriting,Open Library of the Humanities,2015,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1262,http://dx.doi.org/10.16995/dscn.44,"{'string_id': '10.16995/dscn.44', 'id_scheme': 'DOI'}","Stylometry has got a lot of interest during these recent years because it solved many authorship problems and disputes that were difficult to handle. Author discrimination consists in checking whether two texts are written by the same author or not. In this investigation, we try to make an author discrimination between the Quran (The holy words and statements of God in the Islamic religion) and the Hadith (statements said by the prophet Muhammad) in a segmental form. In fact, 14 text segments are extracted from the Quran book and 11 text segments are extracted from the Bukhari Hadith. These segments have more or less the same size in terms of words and the medium size is about 2080 words per text segment. The Quran is taken in its entirety, whereas for the Prophet’s statements, we chose only the certified texts of the Bukhari book. That is, four series of experiments are done and commented. The first series of experiments concerns several experiments of authorship attribution using different state of the art features and classifiers, the second series of experiments analyses the different texts by using a new parameter called COST, the third series of experiments consists in an authorship discrimination using the frequency of a particular word (“الذين ” meaning those/who in English) and the fourth series of experiments performs a hierarchical clustering on the 25 text segments, in order to assess the real number of clusters (author styles) and to see if the hypothesis of a unique author is possible. This investigation, which represents the continuation of a previous research work on the same topic [Sayoud 2012-a], has further clarified an old enigma, which was impossible to solve for fourteen centuries: all the results of this investigation show unanimously that the two books should have two different authors.",Segmental Analysis-Based Authorship Discrimination between the Holy Quran and Prophet’s Statements,Open Library of the Humanities,2015,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1263,http://dx.doi.org/10.16995/dscn.47,"{'string_id': '10.16995/dscn.47', 'id_scheme': 'DOI'}","This essay considers the viability of pornography marketed as resistant or revolutionary within hegemonic capitalist and heteronormative contexts. It focuses on three contemporary porn websites: Pornhub, Treasure Island Media, and TROUBLEfilms. In its current context, internet pornography represents a complex web of content owners, producers, distributors, licensees, users, uploaders, downloaders, actors, web hosts, and advertisers, and notions of intellectual property often come into play as porn companies try to make money in an environment in which customers do not want to pay. Using the Salish method of ""storying up"" a phenomenon (Maracle 2007), I narrate the history of obscenity and intellectual property and discuss survival strategies employed by the three exemplary netporn sites. Beginning with copyright's and obscenity's legal foundations in the eighteenth century, the story of porn and intellectual property includes the use of easily-copied porn to create sexual publics in the twentieth century and technological and legal changes to porn's distribution in the twenty-first century. I examine three strategies netporn businesses use to stay afloat in the post-industrial economy. These strategies are the re-intermediation of the industry, management of the pornographer's ""author function,"" and regulation of porn consumption through the sale of identity to viewers. As netporn businesses struggle for control over porn distribution and consumption, they facilitate their own survival by generating new sexual, social and economic norms. These norms mediate between the ""pirate"" culture promised by technology and the culture industry's interest in legitimising and entrenching intellectual property rights.",Pornographers and Pirates: Intellectual Property and Netporn,Open Library of the Humanities,2014,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1264,http://dx.doi.org/10.16995/dscn.48,"{'string_id': '10.16995/dscn.48', 'id_scheme': 'DOI'}","In July 2012 the European Parliament defeated the Anti-Counterfeiting Trade Agreement (ACTA). Supposedly the attempt to impose global norms on intellectual property rights and thereby restrict digital copyright was dead. Or was it? This paper argues that the spirit of ACTA may live on in a host of other trade agreements currently being negotiated. That is, ACTA, or even more restrictive versions of it, could be imposed through the back door at least upon weaker states through bilateral agreements with the United States and the European Union. The result could be a spaghetti bowl of rules on digital copyright with some countries enjoying more digital rights, online freedom, and privacy than others.","The Rise, Fall, and Rise of ACTA?",Open Library of the Humanities,2014,[],Digital Studies/Le champ numérique,0,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1265,http://dx.doi.org/10.16995/dscn.5,"{'string_id': '10.16995/dscn.5', 'id_scheme': 'DOI'}","Though the publication of the Complete works of Dutch novelist Willem Frederik Hermans (1992-1995) is a challenge in itself, nowadays the aim of the mere publishing of 24 printed volumes is far from enough. Operating within an academic field of literary theory and criticism, in which a detailed and conscious use of scholarly edited texts is all but obvious, much more effort should be spent to communicate the merits of textual scholarship. With Hermans, an author known for the ongoing revisions of his texts, academic awareness on the textual process and textual fluidity can be raised, by developing digital editions of some of his works, and by integrating genetic criticism's notion of the creative process. But a range of other dissemination strategies should be developed as well. For a project like the Complete works of Willem Frederik Hermans, that almost exclusively takes place within a national context, with a comparatively small group of interested academics, students and other specialists, well-chosen, tailored initiatives should be deployed to reach new and diverse groups of audiences within the general reading public. Within the contexts of an emerging digital culture and a thoroughly changing literary culture and book market, new challenges are abundant. This article focuses on a range of past and future initiatives, all of which were developed in close cooperation with other partners working within the fields of technology and digital culture, literary education and cultural heritage.",Chapter 6 Beyond the Great Divide: New explorations in editing the Complete Works of Dutch Novelist Willem Frederik Hermans,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1266,http://dx.doi.org/10.16995/dscn.6,"{'string_id': '10.16995/dscn.6', 'id_scheme': 'DOI'}","This article canvasses the multiple meanings of the word ""social"" in the contexts of editions, editing and texts. It may refer to a theory of editing; to varieties of practice; to manners of making; or to characterize the edition itself. Further, not all editions which claim to be ""social,"" in any of these senses, are any such thing. A particularly egregious example is the so-called ""Social edition of the Devonshire manuscript,"" whose claim to be ""social"" is poorly based. However, there is real potential in the making of editions which are more inclusive in their making, which achieve a wider impact and create new understandings in expanding circles of readership, whether or not we choose to label these as ""social.""","Chapter 7Social editions, social editing, social texts",Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1267,http://dx.doi.org/10.16995/dscn.7,"{'string_id': '10.16995/dscn.7', 'id_scheme': 'DOI'}","This chapter examines the various textual traditions that inform the design and affordances of three INKE digital tools, namely NewRadial, the Dynamic Table of Contexts, and Bubblelines. In turning to earlier textual exemplars, such as the sammelbände and foldout engravings, the chapter illustrates certain commonalities between past and present textual media. Similarly, by examining the material evidence of Renaissance readers, including manuscript tables of contents and supplements to print indexes, the chapter considers the long history of customization that lies behind many of our digital applications today. In short, the chapter invites readers to situate a series of INKE tools within a much earlier and seemingly unrelated textual tradition.","Chapter 11Designed for the digital reader: The textual traditions in, of, and behind NewRadial, the dynamic table of contexts and Bubblelines",Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1268,http://dx.doi.org/10.16995/dscn.8,"{'string_id': '10.16995/dscn.8', 'id_scheme': 'DOI'}","In contemplating the design and implementation of new knowledge environments, what can we learn from book history and from the natural world about how environmental systems form, develop, and thrive? This essay uses the theory of ecodynamics to theorize the development of new knowledge environments for academic study, with illustrative examples from the history of the Christian Bible, and concludes by deriving some principles for those of us working to develop new digital scholarly resources.",Chapter 10The textual habitat: The development of new knowledge environments,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1269,http://dx.doi.org/10.16995/dscn.9,"{'string_id': '10.16995/dscn.9', 'id_scheme': 'DOI'}", The history of the first edition of the Oxford English Dictionary can be used as an example of a successful academic-public collaboration. The difficulties and eventual success of the OED are instructive for current academic crowd-sourced projects that are being undertaken with tools developed for the World Wide Web. The success of such projects depends more on the effective management of the project than on the tools it uses.,Chapter 8Appeal to the public: Lessons from the early history of the Oxford English Dictionary,Open Library of the Humanities,2016,[],Digital Studies/Le champ numérique,6,,"[{'value': '1918-3666', 'type': 'electronic'}]"
1270,http://www.zfdg.de/2016_002,"{'string_id': '10.17175/2016_002', 'id_scheme': 'DOI'}","In recent years, Automatic Writer Identification (AWI) has received a lot of attention in the document analysis community. However, most research has been conducted on contemporary benchmark sets. These datasets typically do not contain any noise or artefacts caused by the conversion methodology. This article analyses how current state-of-the-art methods in writer identification perform on historical documents. In contrast to contemporary documents, historical data often contain artefacts such as holes, rips, or water stains which make reliable identification error-prone. Experiments were conducted on two large letter collections with known authenticity and promising results of 82% and 89% TOP-1 accuracy were achieved.",Automatic Writer Identification in Historical Documents: A Case Study,,2016,,Zeitschrift für digitale Geisteswissenschaften,Heft 1.2016,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1271,http://www.zfdg.de/2016_003,"{'string_id': '10.17175/2016_003', 'id_scheme': 'DOI'}","This article describes the development of a service portfolio for a research data centre in the humanities; in particular, it explores which services are necessary to support the long-term preservation and re-use of the results of humanities research. First, the fundamental characteristics of digital research data in the humanities are examined. In addition to file-based research data – such as texts, images, videos, and so on – complex data structures become more prominent and pose new challenges for institutions working with research (data) infrastructures. Next, this paper describes the results of the test ingests using three types of research data – database, digital edition, and interactive visualisation – that were carried out during the design phase of the Humanities Data Centre (HDC), followed by a discussion of their implications for the HDC’s initial modular service portfolio, which could be expanded in the future in coordination with other research data centres. This article reflects the developments in the HDC project as of January 2016. Information about new developments can be found on the website: http://humanities-data-centre.de/.",Angebotsgenese für ein geisteswissenschaftliches Forschungsdatenzentrum,,2016,,Zeitschrift für digitale Geisteswissenschaften,Heft 1.2016,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1272,http://www.zfdg.de/2016_004,"{'string_id': '10.17175/2016_004', 'id_scheme': 'DOI'}","The concept of the metaphor is handled critically in poetics on a theoretical level and by means of literary examples. In the process of developing an annotation schema, the ePoetics project differentiates between the significant components of the explication of the metaphor concept, and, in addition, reveals the discourse and reference structures linked to them. Thus, it becomes clear which recognition possibilities and boundaries appear in the collaborative-discursive markup process before the data evaluation – with the digital algorithmic paradigm as foil – and how hermeneutics and algorithms interact – in the tradition of Stephen Ramsay’s ›algorithmic criticism‹.","Auf der Suche nach dem »goldnen Baum« – Digitale Annotation des Metaphernbegriffs in Poetiken: Erkenntnisprozess, diskursive Praktik und ›tertium comparationis‹",,2016,,Zeitschrift für digitale Geisteswissenschaften,Heft 1.2016,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1273,http://www.zfdg.de/2016_006,"{'string_id': '10.17175/2016_006', 'id_scheme': 'DOI'}","The paper deals with linguistic transformation processes from ancient Greek sources which were translated into classical Arabic from the 9th to 11th century AD. The database Glossarium Graeco-Arabicum concentrates on this topic and utilizes visualizations to develop an understanding of the still growing corpus of about 100,000 word records. Various examples demonstrate possible visualization methods for corpus structures, lexical differentiation, grammatical transformation and translation processes for single lexemes.",Mapping the Words. Experimentelle Visualisierungen von Übersetzungsstrukturen zwischen Altgriechisch und Hocharabisch,,2016,,Zeitschrift für digitale Geisteswissenschaften,Heft 1.2016,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1274,http://www.zfdg.de/2016_009,"{'string_id': '10.17175/2016_009', 'id_scheme': 'DOI'}","One common assumption in the Digital Humanities is that one of its functions is to develop and apply new, computer-supported methods to address research topics in literary studies. Research questions in literary studies, however, are historically variable and highly dependent upon what one understands as the object of analysis: what is a text, and what is the point of investigating it? Such formulations are always the effect of media cultures, however. Using four characteristics as examples, this paper discusses the semantic transformation of the literary studies research object in connection with digitisation, and it advocates for an understanding of the Digital Humanities as the location of a fundamental methodological discussion about the nature of the literary object.",Was bedeutet die Digitalisierung für den Gegenstand der Literaturwissenschaft? ,,2016,,Zeitschrift für digitale Geisteswissenschaften,Heft 1.2016,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1275,http://www.zfdg.de/2016_010,"{'string_id': '10.17175/2016_010', 'id_scheme': 'DOI'}","The main tasks of contemporary digital philology are the digitization of texts, the development of complex digital editions, and the creation of text corpora that can be analyzed by machines. These activities build on methods and findings that have been explored in computer linguistics for decades. There is, however, a need to adapt those procedures to the requirements of literary analysis, such as in their granularity. This paper outlines possible approaches for the digital processing of large volumes of text and for the development of complex annotation processes. These methods change the very basis of literary analysis, especially if the texts are available on the internet. The next step, therefore, is to discuss the new possibilities for the analysis of literary text that arise from this situation. These possibilities go beyond the mere development of procedures for systematic and verifiable literary analysis of large text corpora. One new step, for example, would be the representation of multi-dimensional qualities of literary texts by data linkage in the Semantic Web, which could make literary texts available for other disciplines as well.",Die Vernetzung des Textes: Im Möglichkeitsraum digitaler Literaturanalyse,,2017,,Zeitschrift für digitale Geisteswissenschaften,Heft 2.2017,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1276,http://zfdg.de/2016_011,"{'string_id': '10.17175/2016_011', 'id_scheme': 'DOI'}","In the Digital Humanities, the dominant belief is that the use of generic ›tools‹ could or should serve as a bridge between the different disciplines and their methodologies. Differences in cognitive interests and in hermeneutics between literary studies and history are too easily overlooked, and therefore both disciplines require the use of adapted and specialized digital tools. This paper investigates the hermeneutic suitability of current digital methods for the humanities and demonstrates their applicability by means of three examples: digital handwriting recognition, historical network analysis, and historical semantics informed by computational linguistics.",Neue Erkenntnisse durch digitalisierte Geschichtswissenschaft(en)? Zur hermeneutischen Reichweite aktueller digitaler Methoden in informationszentrierten Fächern,,2016,,Zeitschrift für digitale Geisteswissenschaften,Heft 1.2016,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1277,http://www.zfdg.de/2019_004,"{'string_id': '10.17175/2016_012', 'id_scheme': 'DOI'}","The article presents an annotation system for narratological phenomena such as space, time, focalisation, reported speech, narrator's speech, the relationship between the speech of a character/of the narrator and that of a certain character, evaluative remarks, negation, figurative speech, and ambiguity. Guidelines for annotation are being developed in order to enable different people to achieve consistent results when working with the same text (inter-annotator agreement). In doing so, narratological models will be systematically reassessed and refined. The aim of this project is therefore to annotate approximately 100 short German-language narratives. The corpus will be reusable for a variety of purposes. It permits systematic access to corpus segments annotated in the same way and supports quantifiable contributions in the fields of historical narratology, cultural studies and the history of literary genres.",Narratologische Textauszeichnung in Märe und Novelle. Mit Annotationsbeispielen und exemplarischer Auswertung von Sperber und Häslein durch MTLD und Sozialer Netzwerkanalyse,,2019,,Zeitschrift für digitale Geisteswissenschaften,Heft 4.2019,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1278,http://www.zfdg.de/2017_002,"{'string_id': '10.17175/2017_002', 'id_scheme': 'DOI'}","Who is cleverer, man or machine? Since the 1950s, the answer to this question has been linked to Alan Turing and the Turing test he devised. This paper builds upon this foundation with its comparison of two analyses of a historical collection of correspondence: one created by humans using ›close reading‹ and the application of subject terms, and one generated by machines with the help of topic modeling. In addition to concrete evaluation of the topic modeling process, this paper investigates whether, and, if so, to what extent, it is feasible and justifiable to use methods that can hardly be understood without in-depth knowledge of probability calculations and statistics. ",Turing Test für das Topic Modeling. Von Menschen und Maschinen erstellte inhaltliche Analysen der Korrespondenz von Leo von Thun-Hohenstein im Vergleich,,2017,,Zeitschrift für digitale Geisteswissenschaften,Heft 2.2017,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1279,http://www.zfdg.de/2017_003,"{'string_id': '10.17175/2017_003', 'id_scheme': 'DOI'}","Digital analyses of literary genres are often based on the assumption that genres can be identified through constant and unvarying features. However, genres always reveal themselves as historically-based constructs with features that change and evolve over time. This paper illustrates how such historical developments in genres can be retraced with digital methods by means of a prominent example from Middle High German literature, Minnesang. With the help of frequency analysis and topic modeling, a research question which has been much debated will be discussed: whether there was an historical shift in genre during the development of Minnesang that differentiates the lyric poetry of the later period from ›classical‹ Sang. ",Digitale Gattungsgeschichten. Minnesang zwischen generischer Konstanz und Wende,,2017,,Zeitschrift für digitale Geisteswissenschaften,Heft 2.2017,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1280,http://www.zfdg.de/2017_005,"{'string_id': '10.17175/2017_005', 'id_scheme': 'DOI'}","Within the discipline of history, digital approaches, tools, and research methods are taking on increasing importance as both a research topic in its own right and as a way to support the field of history as a whole. In spite of this broad interest, projects that make use of new digital tools and methods such as topic modeling continue to be reviewed very critically. Using two projects as examples, this article therefore demonstrates how topic modeling can be used and further developed in order to support historical research. Both projects deal with bodies of knowledge from the 19th century, one in the context of the scholarly implementation of spectral analysis and the other within the context of historical education during the period of the German Emperor Wilhelm II. Through these examples, this article shows where topic modeling can still be improved and what role traditional hermeneutics must have as a central component in the application of topic modeling in the field of history, and thus the boundaries and capabilities of the use of topic modeling in historical research.",Einsatz von Topic Modeling in den Geschichtswissenschaften: Wissensbestände des 19. Jahrhunderts,,2017,,Zeitschrift für digitale Geisteswissenschaften,Heft 2.2017,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1281,http://www.zfdg.de/2017_006,"{'string_id': '10.17175/2017_006', 'id_scheme': 'DOI'}","In this article, we present current stylometric studies on Delta. (1) We discuss why the use of cosine similarity improves the rate of success; our experiments on vector normalization lead to a better understanding of how Delta works. (2) Based on a corpus of Middle High German texts, we show that metrical properties can also be used for authorship attribution. The degree to which Delta is influenced by non-normalized medieval spellings is also investigated. (3) Using a corpus of Arabic-Latin translations, we explore how selective feature elimination can be used to separate the translator signal from the genre signal.",»Delta« in der stilometrischen Autorschaftsattribution,,2017,,Zeitschrift für digitale Geisteswissenschaften,Heft 2.2017,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1282,http://www.zfdg.de/2018_001,"{'string_id': '10.17175/2018_001', 'id_scheme': 'DOI'}","The Perthes collection represents the survival of one of the few completely preserved archives of a cartographic publisher. This publishing house significantly contributed to the scientific exploration, mapping, and surveying of the earth in the 19th and early 20th centuries. In addition to the maps as end products, the collection contains unique and closely interconnected archival materials that document the process of mapping and map-making at all stages. The following article presents a new prototype of a virtual map laboratory that was developed in the first project phase, which will allow these materials to be studied within the context of their different connections.",Mehr als nur Karten. Das Virtuelle Kartenlabor (GlobMapLab) als Zugang zur Sammlung Perthes,,2016,,Zeitschrift für digitale Geisteswissenschaften,Heft 3.2018,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1283,http://www.zfdg.de/2018_003,"{'string_id': '10.17175/2018_003', 'id_scheme': 'DOI'}","The article analyses the composition of colour in the mise-en-scène of the zombie-movies REC, 28 Days Later and World War Z via a computer based approach. It first evaluates some of the methodological problems of common colour-quantisation by using clustering-algorithms in the context of film studies, then a new developed process is presented which builds on the theory of colour contrasts by Johannes Itten. A simple statistical analysis of two contrast ratios for each of the movies demonstrates that this approach is capable of providing detailed insights into the composition of colour in movies in general. Furthermore, the analysis of the three movies reveals that the accentuation of colour contrast relations serves different functions: colours can support the narrative structure of the movie, emphasize repeating motifs or establish an autonomous aesthetic space.",Everything is Illuminated. Zur Numerischen Analyse von Farbigkeit in Filmen,,2018,,Zeitschrift für digitale Geisteswissenschaften,Heft 3.2018,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1284,http://www.zfdg.de/2018_004,"{'string_id': '10.17175/2018_004', 'id_scheme': 'DOI'}","For quantitative literary analysis, the Spanish Golden Age seems to be a perfect field of research. Many texts are provided by Virtual Libraries like the most known BVMC (Biblioteca Virtual Miguel de Cervantes) in various editions. Though for textual analysis like stylometry not only the text itself, but also the metadata should be complete and correct. As the contemporary editions of the Golden Age are themselves difficult to judge regarding the textual originality, additional information (usually called metadata) about the origin of text, author, date, place is needed. The following paper discusses the problem of existing textual basis for stylometric analysis using the example of authorship attribution for the apocryphal second volume of the Quijote by Avellaneda (pseudonym). The aim is not to discover the real identity of the author Avellaneda, but to test the method with various parameters (cosine Delta, selected wordlist, culling, rolling Delta) and to discuss the results in the context of other existing authorship attributional methods and in the context of traditional working literary history and studies.",Die Digitalisierung des goldenen Zeitalters – Editionsproblematik und stilometrische Autorschaftsattribution am Beispiel des Quijote,,2018,,Zeitschrift für digitale Geisteswissenschaften,Heft 3.2018,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1285,http://www.zfdg.de/2019_002,"{'string_id': '10.17175/2019_002', 'id_scheme': 'DOI'}","This article issues the changing textual working practices in the humanities and their interconnected medium of the digital monography. First, the requirements of research in the humanities and those of the format of the monography are analyzed. Then, the textual working practices are reconstructed as practices of reading and writing, which mutually determine the monography as primary representation of findings in the humanities. Finally, the current problems and future potentials of the digital monography are discussed in their design capabilities.",Nicht geeignet oder nur unzureichend gestaltet? Digitale Monographien in den Geisteswissenschaften,,2019,,Zeitschrift für digitale Geisteswissenschaften,Heft 4.2019,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1286,http://www.zfdg.de/2019_003,"{'string_id': '10.17175/2019_003', 'id_scheme': 'DOI'}","The term blockchain is well known to many, at least since the high of Bitcoin prices at the end of 2017. Its application potentials are discussed and tested in economy and science. Especially the transparency, traceability and unchangeability of the content stored in a blockchain and the decentralization are of particular interest. Potential areas of application that are also of interest for the humanities are discussed in this paper, especially the scientific publication of articles and research data as well as blockchain-based possibilities for signing published content. Based on the concrete example of a scientific journal, the functionality, potential application and caveats of blockchain will be explained and discussed. ",Blockchain für die Geisteswissenschaften? Möglichkeiten des Einsatzes von Blockchain und verwandten Technologien für wissenschaftliche Publikationen,,2019,,Zeitschrift für digitale Geisteswissenschaften,Heft 4.2019,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1287,http://www.zfdg.de/2019_005,"{'string_id': '10.17175/2019_005', 'id_scheme': 'DOI'}",The focus of this contribution is the presentation and discussion of the conceptual design of an XML database of fairytales (expandable to other popular narrative genres) and the required digital infrastructure. The purpose is a worldwide systematization of archival material through a suitable database and the development of a content annotation standard (ConAS) for the typological-comparative analysis of the international fairytales.,Konzept für den digitalen Ausbau der typologisch-vergleichenden Märchenforschung,,2019,,Zeitschrift für digitale Geisteswissenschaften,Heft 4.2019,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1288,http://www.zfdg.de/2019_008,"{'string_id': '10.17175/2019_008', 'id_scheme': 'DOI'}","Emotions are a crucial part of compelling narratives: literature tells us about people with goals, desires, passions, and intentions. In the past, the affective dimension of literature was mainly studied in the context of literary hermeneutics. However, with the emergence of the research field known as Digital Humanities (DH), some studies of emotions in a literary context have taken a computational turn. Given the fact that DH is still being formed as a field, this direction of research can be rendered relatively new. In this survey, we offer an overview of the existing body of research on sentiment and emotion analysis as applied to literature. The research under review deals with a variety of topics including tracking dramatic changes of a plot development, network analysis of a literary text, and understanding the emotionality of texts, among other topics. ",A Survey on Sentiment and Emotion Analysis for Computational Literary Studies,,2019,,Zeitschrift für digitale Geisteswissenschaften,Heft 4.2019,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1289,http://www.zfdg.de/2020_001,"{'string_id': '10.17175/2020_001', 'id_scheme': 'DOI'}","This article explores how digital methods, such as the analysis of Facebook page like networks, can complement ethnographic fieldwork, especially in cases of difficult access to a field. Drawing from anthropology, geography, and media studies, we present two case studies where ethnographic access is challenging: one organic farm in Pakistan, and an NGO that circulates knowledge on psychoactive drugs. On a pragmatic level, we argue that network visualizations based on social media are insightful for difficult fields, but, like ›satellite images‹, they need ethnographic contextualization. On the conceptual level, we understand these networks as complex knowledge networks, where large-scale structures emerge through small-scale action.",Digital methods in difficult ethnographic fields: studying knowledge flows as complex networks through a Facebook analysis,,2020,,Zeitschrift für digitale Geisteswissenschaften,Heft 5.2020,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1290,http://www.zfdg.de/2020_003,"{'string_id': '10.17175/2020_003', 'id_scheme': 'DOI'}","This study presents and tests a methodology with which the paratexts of a medium-sized text corpus are recorded and analysed. In the first part, the theory is introduced, then the separate methodical steps for the acquisition of the different paratextual and textual signals are explained in detail. Their quantification allows the clustering of the collected data and thus the grouping of the texts under study. A corpus of 103 scientific ethnographies is used as an example to demonstrate the results to which the analysis of paratexts can lead. This shows that ethnographic writing in the 20th century oscillates between a scientific presentation on the one hand, and an autobiographical presentation on the other. The analysis focuses both on the text corpus as a whole and on developments over time, which allows to trace the history of the discipline, that had initially conceived the subject of the researcher passively as a neutral observer. It has more recently been presented as an active personality acting and interacting in a social environment. The final part of the study discusses possibilities for automated data collection and the use of machine learning methods. The potential of paratextual analyses to provide insights into the functional differentiation of conventions of representation is thus outlined.",Analyse von Begleittexten am Beispiel von 103 Ethnografien,,2020,,Zeitschrift für digitale Geisteswissenschaften,Heft 5.2020,,"[{'value': '2510-1358', 'type': 'electronic'}]"
1291,http://zfdg.de/sb001_001,"{'string_id': '10.17175/sb001_001', 'id_scheme': 'DOI'}","Traditionally, ontology engineering is based on the presumption that the meaning of a proposition results from the combination of the meaning of its elements (concepts) and its syntactical structure. The reach of this ›principle of compositionality‹ is, however, a contested topic in semantics. Its opponents defend the primacy of propositional meaning and derive the meaning of concepts from their contribution to propositional meaning. In this situation, this paper argues for an approach to ontology design that does not presuppose a stance in this debate. The proposed ›minimal doxographical ontology‹ is intended as a heuristic tool charting unknown or complex domains. It regards propositional meaning as atomic and relates it to a bearer of propositional content (persons or texts). The strengths of such an approach are first discussed in a simplified example, the analysis of legal stipulations on alcoholic beverages. A more complex use case concerns the doxographical analysis of debates in the history of early modern philosophy. In closing, the paper sketches briefly how this approach may be extended using ontologies as hermeneutic tools in the interpretation of sources from the history of philosophy.",What People Said: The Theoretical Foundations of a Minimal Doxographical Ontology and Its Use in the History of Philosophy,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1292,http://www.zfdg.de/sb001_002,"{'string_id': '10.17175/sb001_002', 'id_scheme': 'DOI'}","This paper outlines the historical development and methodical challenges of digital visual studies. Although it was argued early on that the strength of the computer lay in formal rather than iconographic analysis, in practice the iconographic encoding of images and the collection of metadata dominated for a long time. However, scholars and practitioners of digital visual studies soon demanded a thorough ›Ideologiekritik‹ (›critique of ideology‹) of its methods. Today, the field encompasses a broad spectrum of formal and iconographical, as well as reception-based studies. Furthermore, it can help to establish the critical reflection on methodical implications as a core issue for the digital humanities.",Vom Bild zum Bild – Digital Humanities jenseits des Textes,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1293,http://zfdg.de/sb001_003,"{'string_id': '10.17175/sb001_003', 'id_scheme': 'DOI'}","The title of this paper indirectly refers to the theme of the 2014 Conference on Digital Humanities in German-Speaking Areas (»Digital Humanities – methodischer Brückenschlag oder ›feindliche Übernahme‹?«). I assume from this title that computer science is viewed as the originator of such a deliberate conquest. The first section of this paper explores how computer science, the headquarters of the »enemy« threatening to conquer our disciplines, also experienced and overcame an uncertain period of early development. The history of this discipline could serve as a source of inspiration or, at the very least, courage. The following section explores—as indicated by the title of this paper—how computer science could be viewed as a humanities discipline, including a historical perspective on the foundation of both disciplines or disciplinary fields. The historical dimension, but also the uncertainty, that is reflected in the conference theme serves as my starting point to present ten theses in the following section that explain the necessary factors for success if we wish to succeed with the Digital Humanities.",Die Informatik als Geisteswissenschaft,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1294,http://zfdg.de/sb001_004,"{'string_id': '10.17175/sb001_004', 'id_scheme': 'DOI'}","The Digital Humanities are either a field of research or an academic discipline, possibly both. In any case, they are embedded in an extensive infrastructure in regard to organization, information, and communication and build upon long traditions in various areas of research. Furthermore, as a link between the humanities and computer science, the field seems to be highly attractive, not only to these areas, but also to neighboring disciplines as well as to the research funding agencies. Outside of the academic context, the mass media have discovered the digital humanities in recent years. While this attractiveness has several positive integrative effects, the current DH hype is not without certain risks, ranging from the mere misappropriation of the term to explicitly defensive or negative attitudes, and even ignorance and denial: »DH? But there is no such thing!«",Digital Humanities? Gibt’s doch gar nicht!,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1295,http://zfdg.de/sb001_006,"{'string_id': '10.17175/sb001_006', 'id_scheme': 'DOI'}","This paper discusses the methodological effects of the interdisciplinary cooperation between humanities scholars and computer scientists in the context of the project heureCLÉA. The goal of heureCLÉA is to develop a ›digital heuristic‹ that supports the narratological analysis of literary texts by (1) performing automatically annotation tasks up to a certain level of complexity that previously could only be carried out manually, and by (2) identifying statistically salient text phenomena for the subsequent detailed analysis by the human user.In this project, it was essential to devote special attention to the specific disciplinary approaches. In this paper, we illustrate the ways in which the tensions between (non-deterministic) hermeneutics in the humanities and (deterministic) computer science can be productively employed and thus have an effect beyond the concrete research question.",Informatik und Hermeneutik. Zum Mehrwert interdisziplinärer Textanalyse,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1296,http://zfdg.de/sb001_007,"{'string_id': '10.17175/sb001_007', 'id_scheme': 'DOI'}","Medieval and early modern account books are well suited for computer analysis, since they can be easily converted to tabullar data formats. Scholarly research and the creation of critical editions have simultaneously demonstrated that research interest on this type of textual material is so diverse that they should ideally be digitally edited. However, this paper can only point to one single digital edition that also allows the numeric analysis of account books. While previous research on digital editions has concentrated on philologic and paleographic problems, this paper proposes an edition model that also includes the contents of the accounts themselves. Based on the Resource Description Framework (RDF), this model of references between the different forms of representation can be serialized with the encoding guidelines of the Text Encoding Initiative (TEI). Finally, this paper will demonstrate that extensive collections can also be efficiently be digitally edited in this way.",Warum werden mittelalterliche und frühneuzeitliche Rechnungsbücher eigentlich nicht digital ediert?,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1297,http://zfdg.de/sb001_008,"{'string_id': '10.17175/sb001_008', 'id_scheme': 'DOI'}","For Digital Humanities in medieval studies and early modern studies, the digitization of manuscripts is a central field. Since each manuscript displays its own unique characteristics, the automatic generation of a machine-readable text using Optical Character Recognition (OCR) as applied to digital images leads, in most cases, to error-prone results. However, characteristics of handwriting such as the size of letters and spacing, slope, and so on can be used to identify the scribe or scribes. This paper demonstrates how the analysis of manuscript images can be used to identify the scribe or scribes. An algorithym will support additional paleographic and codicological findings and provide evidence for the verification or falsification of uncertain attributions.",Vom Zeichen zur Schrift: Mit Mustererkennung zur automatisierten Schreiberhanderkennung in mittelalterlichen und frühneuzeitlichen Handschriften ,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1298,http://zfdg.de/sb001_009,"{'string_id': '10.17175/sb001_009', 'id_scheme': 'DOI'}","The debate on medieval textuality is shaped by an obvious reservation about the category of authorship. Therefore, the conditions for the acceptance of stylometric methods in the field of Middle High German literature might not seem too favorable. However, it is exactly this tension between tradition and individuality, which bears great significance for those texts, that offers opportunities for the application of stylometric methods, which can lead to a mutual elucidation of method and discipline. The paper provides examples of the ways in which Middle High German texts are influenced by individual as well as genre-specific style factors. Furthermore, it elaborates on the issues that must be addressed for a proper quantification of these categories.",Historische Stilometrie? Methodische Vorschläge für eine Annäherung textanalytischer Zugänge an die mediävistische Textualitätsdebatte.,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1299,http://zfdg.de/sb001_010,"{'string_id': '10.17175/sb001_010', 'id_scheme': 'DOI'}","The first women's movement of the 19th and early 20th centuries was one of the major social movements which produced an immense body of texts. In this paper, we explore the ways in which research into the history of language use, topic development, and forms of communication in the first women's movement can be supported by Digital Humanities methods. We present an integrated documentation system that combines lexical documentation, a full-text corpus and linguistic investigations (such as on argumentation, word usage, etc.), and we show how this type of research activity can be integrated into a large infrastructure project (CLARIN-D).",Kontroverse Kommunikation im Umkreis der ersten Frauenbewegung. Wie können digitale Ressourcen die sprachliche Untersuchung und die Ergebnisdokumentation verbessern? ,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1300,http://zfdg.de/sb001_013,"{'string_id': '10.17175/sb001_013', 'id_scheme': 'DOI'}","Theory-driven research in the social sciences is confronting the Digital Humanities with new challenges. Here, text analysis can not be built directly on linguistic features, but rather the research questions require access to more complex concepts and their meaning. Our methodological contribution to DH argues that it is possible to develop hermeneutically sensitive methods for corpus analysis when the tool components remain transparent for the target discipline and allow for interactive exploration of the underlying data. We provide an overview of interactive tools for text processing and analysis that can be flexibly adjusted to specific disciplinary theories and research questions in the target disciplines, with an underlying architecture that is generic and therefore broadly applicable. ",Computerlinguistische Werkzeuge zur Erschließung und Exploration großer Textsammlungen aus der Perspektive fachspezifischer Theorie,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1301,http://zfdg.de/sb001_014,"{'string_id': '10.17175/sb001_014', 'id_scheme': 'DOI'}","›Mixed Reality‹ describes the combination of virtual environments and natural user interfaces. Here, the user's field of vision is controlled by natural head movements via a head mounted display. Data gloves allow direct interaction with virtual objects and omnidirectional floors enable unrestricted navigation through virtual worlds with natural walking movements. A quasi-experimental study showed that ›mixed reality‹ had a positive effect, particularly on affective and motivational levels. The results are interpreted in the context of the new possibilities for pedagogy offered by such innovative technologies.",Neue Perspektiven für die Erziehungswissenschaften – Das Potenzial von Mixed Reality für Lehr- und Lernprozesse,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1302,http://zfdg.de/sb001_015,"{'string_id': '10.17175/sb001_015', 'id_scheme': 'DOI'}"," This paper provides an overview of spatially oriented epigraphic research in medieval and early modern churches as part of the Spatial Humanities research project Inschriften im Bezugssystem des Raumes (IBR). With the help of a custom made capturing software, this field uses geometric point cloud data from terrestrial laser scans as a primary source for spatial analysis. This data, in combination with epigraphic text sources and annotations, forms the basis for research on various symbolic and functional relations between objects in church interiors.",Semantisch angereicherte 3D-Messdaten von Kirchenräumen als Quellen für die geschichtswissenschaftliche Forschung,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1303,http://www.zfdg.de/sb001_016,"{'string_id': '10.17175/sb001_016', 'id_scheme': 'DOI'}","How would you measure color? The software Redcolor-Tool enables redness in images to be quantified for the first time. The technology attributes values of redness to any given color. This paper explores the methodological implications of color analysis for stylometric and historical approaches to visual studies. Furthermore, it examines the potential of new categories for the humanities which could emerge from quantitative feature metrics. The stylometry section is devoted to red color frequencies and color sums. A ›theory of coordination‹ based in stylometry is proposed as the methodological key. The historical section analyses red as the symbol of power and authority. Portraits from Titian´s Charles V to Bush´s portrait of Angela Merkel are analysed, taking the color red as the interpretational basis of traditional and contemporary political iconography. ",Rot rechnen,,2016,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1304,http://zfdg.de/sb001_017,"{'string_id': '10.17175/sb001_017', 'id_scheme': 'DOI'}","Especially in the early modern period, musicians were a highly mobile group that traveled throughout Europe and beyond. Accordingly, they significantly stimulated the formation of a common European cultural identity. Two successive, international music research projects are exploring this topic with different emphases. Each project database uses the same database system in a different way and with different objectives. Using case studies, appropriate research data visualizations are discussed, which will be further developed in the ongoing projects.",MUSICI und MusMig. Kontinuitäten und Diskontinuitäten,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1305,http://zfdg.de/sb001_018,"{'string_id': '10.17175/sb001_018', 'id_scheme': 'DOI'}","In this paper, we discuss the possibilities as well as the added value of using a web-based databank for video annotation (Pan.do/ra) in cultural studies research on film. The first part demonstrates a variety of potential uses through the example of a music video, and the second part introduces the software on a more general level. ",»Losing My Religion« – Einsatz der Videoannotationsdatenbank Pan.do/ra in der kunstgeschichtlichen Analyse von Musikvideos,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1306,http://zfdg.de/sb001_019,"{'string_id': '10.17175/sb001_019', 'id_scheme': 'DOI'}","This paper presents the digital edition of an important text in German literary history. The satirical magazine Die Fackel (The Torch) was published by the satirist and language critic Karl Kraus in Vienna from 1899 until 1936. The digital edition of the AAC-Fackel was developed by researchers, scholars, programmers, and designers within the framework of the AAC-Austrian Academy Corpus, operated by the Institute for Corpus Linguistics and Text Technology at the Austrian Academy of Sciences in Vienna, and went online on 1 January 2007. The AAC-Fackel offers free online access to the 37 volumes, 415 issues, and 922 issues, comprising 22,586 pages and more than 6 million word forms of Die Fackel and contains a fully searchable database of the entire journal with indexes and search and navigation tools in a functionally designed interface, in which all pages of the original journal are available as digital texts and as facsimile images. The general principles of this scholarly digital edition, and its concept and design considerations, are based on a corpus-based methodological approach and are decisive for addressing questions of critical editions and philological access to this key text in German literary history.",AAC-Fackel. Das Beispiel einer digitalen Musteredition,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1307,http://zfdg.de/sb001_020,"{'string_id': '10.17175/sb001_020', 'id_scheme': 'DOI'}","Traditional approaches to data integration are typically based on the harmonization of heterogeneous data with respect to the constraints of a globally integrative data structure, such as a global schema or ontology. This paper illustrates the limitations of such a harmonization-based approach in the specific context of the Digital Humanities and shows how a research-oriented and case-based data federation can be facilitated by the DARIAH-DE federation architecture. Based on the collection, schema, and crosswalk registries, collections and the data schemata they utilize can be registered and flexibly associated – resulting in the ability to create broad and comprehensive as well as discipline-specific views of research data.",Heterogene Daten in den Digital Humanities: Eine Architektur zur forschungsorientierten Föderation von Kollektionen,,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1308,http://zfdg.de/sb001_021,"{'string_id': '10.17175/sb001_021', 'id_scheme': 'DOI'}","Open review projects and wikis are changing academic knowledge production, just as academic blogs and social media are influencing scholarly communication. These digital forms of publishing challenge the usual practices of publishing and the concepts of authorship within the humanities, both practically and categorically. Therefore, it is necessary to revise the concept of ›scholarly authorship‹ by differentiating new forms of academic collaborative writing and to reflect on the academic quality of the new media formats within the humanities. This can be shown when analyzing self-curated digital publishing infrastructures and open license models of intellectual property.","Vom Urheber zur Crowd, vom Werk zur Version, vom Schutz zur Öffnung? Kollaboratives Schreiben und Bewerten in den Digital Humanities",,2015,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 1,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1309,http://www.zfdg.de/sb002_002,"{'string_id': '10.17175/sb002_002', 'id_scheme': 'DOI'}","This paper explores the concept of the ›edited text‹ and how it developed in the historical scholarly context. It describes the origins of objections that seek to deny the necessity of the ›edited text‹, and to frame it solely as a form of supplementary aid for the reader, while simultaneously insisting, with equally great emphasis, on the primacy of the ›edited text‹ as the cornerstone of the edition. In addition, this paper will also explain how modern editions interact with the construction of the ›edited text‹ as well as review the current state of debate in the field.",Der ›edierte‹ Text: Grundpfeiler der Edition oder ›Zugeständnis‹ an den Leser?,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 2,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1310,http://www.zfdg.de/sb002_003,"{'string_id': '10.17175/sb002_003', 'id_scheme': 'DOI'}","The article explores some of the continuities and discontinuities that mark the ongoing change in international editorial ventures from a nondigital to a digital framework. Placing collaboration at the centre of the editorial work, it is assumed that reading is the minimal act of collaboration which lies at the basis of the one-language/one-literature model, i.e., the national philology paradigm. Such a paradigm appealed to international cooperation in a number of ways (archives, journals, publishing houses, etc.), but it also came to promote the idea of trans-national abstract goals (e.g., in Lachmann’s ideal of recension without interpretation or in Gaston Paris’s utopian cité des sciences). These goals favoured the progressive establishment of the digital paradigm, which in turn encouraged (or facilitated) a change of direction, from the study of in situ textual specificities to taxonomies aiming at a representation of the general structure of texts. Against the backdrop of the new paradigm, international collaboration has been intensified, but, it is argued, the new medium does not take the place of the old medium, rather interacts with it. In the end, the article touches upon some of the challenges faced by digital initiatives, namely regarding sustainability and political issues.",International/global networking against the backdrop of nondigital and digital editorial ventures,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 2,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1311,http://www.zfdg.de/sb002_004,"{'string_id': '10.17175/sb002_004', 'id_scheme': 'DOI'}","Scholarly editions were one of the first engines for introducing digital methods into the humanities and social sciences. The digital infrastructures developed for creating individual editions could lead to a new type of edition, if the data were prepared in such a way as to make them usable in multiple editions. The reuse of data is playing an increasingly important role, but it also generates new problems. This essay describes the opportunities and risks of overlapping editions using the example of the AMIE infrastructure.",Von der Raupe zum Schmetterling oder Wie fliegen lernen – Editionsphilologie zwischen Infrastruktur und Semantic Web,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 2,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1312,http://www.zfdg.de/sb002_005,"{'string_id': '10.17175/sb002_005', 'id_scheme': 'DOI'}","Next to the primary goal of encoding and the systematic markup of the autobiographical documents in the manuscript collection of the Herzog August Library (HAB), it is of central importance for the project Selbstzeugnisse‹ der Frühen Neuzeit in der HAB (›Autobiographical writings of the early modern age in the Herzog August Library‹) to evaluate the options for producing and compiling texts and research results. In comparison with print-based methods, and using the group of autobiographical texts, the opportunities, advantages, and disadvantages of the scholarly distribution of information and communication should be explored. The Selbstzeugnisse in this project are defined as autobiographical writings, such as diaries, travel journals, or autobiographies, although the term is intentionally left open and is under discussion. The project results will be a finding aid to the Selbstzeugnisse using autoptic analysis of the manuscripts, a digital critical edition of the diary of Duke August the Younger of Braunschweig-Wolfenbuettel, and the presentation of the project results in a research portal.","Selbstzeugnisse digital – Erschließung, Präsentation und Benutzbarkeit",,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 2,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1313,http://www.zfdg.de/sb002_006,"{'string_id': '10.17175/sb002_006', 'id_scheme': 'DOI'}","The first recommendations for the classification and evaluation of digital editions were already published in the process of establishing the WWW as the primary medium of publication for digital content in the late 1990s. However, owing in part to methodological and technical developments, basic questions about the evaluation of digital editions are still being discussed. This article explores the review process as a means of evaluating scholarly digital editions, building on the background of the evaluation criteria that provide the basis for reviewing and the broader context of evaluating digital research results as a whole. In this way, many aspects can be worked out for the evaluation of digital editions: the appropriate representation of the edited object in digital data, for example, through text encoding; the innovative interaction with the digital medium; a meaningful consensus between content and form in the presentation of the edited material, etc. Despite the future changes in scholarly communication, the review as an ›evaluative genre with tradition‹ remains an ideal tool for discussing the quality of digital editions.",Reviewing von digitalen Editionen im Kontext der Evaluation digitaler Forschungsergebnisse,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 2,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1314,http://www.zfdg.de/sb002_007,"{'string_id': '10.17175/sb002_007', 'id_scheme': 'DOI'}","Digital editions in the semantic web? This paper explores the question of the relevance and further development of semantic technologies in digital editions in the arts and humanities. The XML TEI-based editions of texts can be set on top of established publication structures. However, the widespread use of the TEI standard has led not to the interoperability of editions – despite the intentions of its creator—but rather to the increasingly strong specialization of the markup by discipline. This situation has led to the need for solutions for aggregation, re-usability, and the networking of editions as well as for encoding using a machine-readable semantic – one that is capable of generating other forms of connections using Linked Open Data (LOD), authority files, and other metadata.",Der nächste Schritt? Semantic Web und digitale Editionen.,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 2,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1315,http://www.zfdg.de/sb002_008,"{'string_id': '10.17175/sb002_008', 'id_scheme': 'DOI'}","The act of digital editing is permanently in flux, and it is currently located between medial experimentation, the text-critical attachment to tradition, and standardization. This experimental character can be subsumed under keywords such as multiperspectivity, multidisciplinarity, and trans- or multimediality. These keywords serve as the (re)productive power driving the permanent pursuit of more, which was initiated by the shift in format, with its changing research interests and goals. However, the corresponding medial and material metamorphoses produce the dilemma of a documentary level of detail in the digital edition that requires new strategies for use. As these new possibilities take digital editing to its limits, the next question is where its future direction will lead.",(Be)Ständige digitale Metamorphosen?! – »perhaps there will always be something more«,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 2,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1316,http://www.zfdg.de/sb002_009,"{'string_id': '10.17175/sb002_009', 'id_scheme': 'DOI'}","Using an examination of the orthography of a print by the printer Georg Stüchs from 1508, this paper demonstrates how modern digital editions can be used for historio-linguistic research. After an overview of the letters/graphemes and abbreviations used by the print shop, this analysis concentrates on the following aspects: the spelling of diphthongs and of /œ, ø:/ und /ʏ, y:/; writing variations for /ɪ, i:/, /ʊ, u:/, /f/ und /t/; the distribution of the letters &lt;S, ſ, ß, s&gt;; and the capitalization of nouns. By comparing the text [A]bschrieffte eines Sandtbriefes with a contemporary print of the same text published independently, also attributed to Georg Stüchs, this paper explores the extent to which the language of a printer’s workshop can be considered homogeneous.",»Newe landte« in der historischen Sprachwissenschaft: Ein Beitrag zur Druckersprache der Nürnberger Offizin Georg Stüchs auf Grundlage der Digitaledition Paesi novamente retrovati - Newe unbekanthe landte.,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 2,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1317,http://www.zfdg.de/sb002_010,"{'string_id': '10.17175/sb002_010', 'id_scheme': 'DOI'}","This article deals with questions about interactive annotations of Robert Musils writings on MUSIL ONLINE. In this way, problem areas and approaches are described to show the development of structures of commentating.",Vom Livre irréalisé zum Texte hyperréalisé?,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 2,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1318,http://www.zfdg.de/sb003_001,"{'string_id': '10.17175/sb003_001', 'id_scheme': 'DOI'}","In his contribution to the first Symposium on Digitalität in den Geisteswissenschaften Martin Warnke took up and expandend the distinction between »digitality« and »digitalisation« previously introduced by Claus Pias for art history. According to this distinction, a practice in the humanities is digitalized if it is today computer-aided, but already existed previously in non-digitalized form. Digital activites are instead those that have only become possible thanks to new digital methods. In my contribution will argue that the history of science is today facing the challenge of becoming not just digitalized, but digital, if it wished to invesitgate the transformation which took place in science due to digitalities.",Wissenschaftsgeschichte zwischen Digitalität und Digitalisierung,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 3,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1319,http://www.zfdg.de/sb003_002,"{'string_id': '10.17175/sb003_002', 'id_scheme': 'DOI'}","This paper presents the relationship between the disciplines involved in the digital humanities, focusing in particular on the role of computer science. Subsequently, requirements for a successful cooperation between scientists of different disciplinary backgrounds are presented, and finally, potentials arising from the digital context for the humanities are formulated. We understand digital humanities as a challenge and at the same time as an opportunity to bridge the separation of the sciences and humanities, which has been cultivated for too long.",Digital Humanities – Eine Herausforderung an die Informatik und an die Geisteswissenschaften,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 3,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1320,http://zfdg.de/sb003_003,"{'string_id': '10.17175/sb003_003', 'id_scheme': 'DOI'}","This essay questions whether digital literary studies can still be meaningfully regarded as part of literary studies. This heretical question is motivated by a praxeological view of a research project for the network analysis of dramatic texts, in particular by reflecting on the project’s underlying ›epistemic thing‹, which in this case consists of specifically-formatted structural data (and not the actual primary texts themselves). What does this corpus of structural data, which was extracted from 465 plays spanning the period from 1730 to 1930, have to do with the ›epistemic things‹ of literary studies? We explore this question by providing insight into our analyses, which describe the structural evolution of the ›plays‹, try to locate ›small world‹ properties in our corpus, and develop new metrics for plot analysis. The results show not only how digital methods can supplement or enrich literary studies; they also raise questions about how digital the field of literary studies already is, since its research objects are increasingly available in digital forms.",Literaturwissenschaft als Hackathon. Zur Praxeologie der Digital Literary Studies und ihren epistemischen Dingen,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 3,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1321,http://www.zfdg.de/sb003_004,"{'string_id': '10.17175/sb003_004', 'id_scheme': 'DOI'}","This paper is written based on workshop discussions and on my research paper presented at the University of Bayreuth’s DFG funded conference, entitled: »Wie Digitalität die Geisteswissenschaften verändert. Neue Forschungsgegenstände und Methoden« March 23–24, 2017. The paper aims to investigate the state of the art, issues, advantages and drawbacks of the use of digital technology for the research of Islamic Art and Architecture. Rather than being conclusive, this paper aspires to discuss research methodology of the History of Islamic Art and Architecture. This is of importance given the lack of such discussions in the given field. The paper is based on my ongoing PhD research of Aesthetics of Ilkhanid Architectural Revetment in Central Iran c. 1300 at the Otto-Friedrich-Universität Bamberg. It discusses the question of the use of digital technology for the benefit of the research. Its particular focus is an inquiry of use of digital technology for the research of stucco revetments and monumental architectural stucco inscriptions (Figures 1 and 2, 3) from 13-14th centuries in Central Iran. The paper argues that the use of digital technology is of key importance for the advancement of Islamic Art studies and for researchers to overcome the traditional approach of stylistic comparison. The paper also explains why Digitality cannot entirely substitute key primary sources stemming from research of museum collections and objects handling or field research of monuments, on which studies of Islamic Material Culture are based.",Islamic Stuccos made Digital. Digitality and Studies of Islamic Art and Architecture,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 3,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1322,http://www.zfdg.de/sb003_010,"{'string_id': '10.17175/sb003_010', 'id_scheme': 'DOI'}","Periodicals have not previously been a powerful object of research; they have been relevant only for certain disciplines with specific methods. Therefore, their mass digitisation and the application of computer-generated analysis can proceed without attracting conspicuous attention. This article focuses not on how the digital humanities change objects and their corresponding analytical methods, but on how they can contribute to the transformation of the journal into an important research object in the first place. I argue that the humanities cannot even conceptualize the journal in its specific multi-dimensionality and that digitisation now offers the chance to compensate for this deficiency. ",›Kleine Archive‹ in den Digital Humanities – Überlegungen zum Forschungsobjekt ›Zeitschrift‹,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 3,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1323,http://www.zfdg.de/sb003_011,"{'string_id': '10.17175/sb003_011', 'id_scheme': 'DOI'}","In October 2002 the project Virtualization of Sculpture. Reconstruction, Presentation, Installation started in the context of the DFG-funded Special Research Area Media Upheavals at the University of Siegen. It ended in 2009. In this project, art history, media studies, and computer science cooperated in order to ‘virtualize’ the complex sculptural ensembles of Gianlorenzo Bernini at the Villa Borghese in Rome. With the help of the scanned sculptures, reconstructed in a virtual model of the Villa Borghese, research questions in art history as well as regarding the theory of images were to be answered experimentally. Although the term was never used in the project itself, it is an early example for the field that is now called Digital Humanities – in the sense that digital technologies were not only used for the scientific work, but were also developed in the course of the project.",»Virtualisierung von Skulptur«. Ein kurzer Erfahrungsbericht,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 3,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1324,http://www.zfdg.de/sb003_012,"{'string_id': '10.17175/sb003_012', 'id_scheme': 'DOI'}","The following article takes the interventions originating from the Digital Humanities as a starting point to analyse research practices in the humanities in general, and literary studies in particular, and to highlight their epistemic implications. Therefore, it sketches out the basic characteristics of praxeological ›science studies‹. On this basis, the dualism ›distant‹ versus ›close reading‹ often labelled as an analytical method is problematized and reconceptualised as the metaphorical description of various hierarchies of scholarly norms.",Objektumgangsnormen in der Literaturwissenschaft,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 3,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1325,http://www.zfdg.de/sb003_014,"{'string_id': '10.17175/sb003_014', 'id_scheme': 'DOI'}","What does it mean to practice art history with digital images? In the sense of Malraux, it would then be designated the history of the digitalisable. This essay explores how medial innovations influence the perception of the past and characterise this process as a feedback phenomenon. Modifications of the artistic canon are the consequences. These questions of canon are relevant for the discipline in as far as it orients itself to the currently valid definition of art. The utopia of a decanonisation or even a general availability of knowledge is closely linked to the idea of research based on data. In response to the core problem of the relationships expressed in this speech between digital and classical humanities, the concept of mechanical agency was introduced, which describes the interaction between human and computer-based acts. In addition to contemplation moored more strongly in media theory, this essay proposes to reopen foundational debates, such as on the relationship between visual culture and art history. With a view to the history of the discipline and in light of the digital turn, this essay advocates for stronger reflection on the concepts of knowledge, work-image-art and canon, as well as on the corresponding (digital) instruments as units that are not neutral.",Kunstgeschichte der digitalen Bilder,,2018,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 3,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1326,http://www.zfdg.de/sb004_001,"{'string_id': '10.17175/sb004_001', 'id_scheme': 'DOI'}","This paper builds a bridge between XML and graphing technologies to examine references in a historical body of text, and explores the extent to which graph queries can facilitate the analysis of referentiality, considering fuzzy information in particular. Examples are chosen form a history research project which examines music critiques of the 19th century on the Messa da Requiem by Giuseppe Verdi.",Referenzielle Varianz in Texten über Musik,,2019,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 4,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1327,http://www.zfdg.de/sb004_002,"{'string_id': '10.17175/sb004_002', 'id_scheme': 'DOI'}","The unique characteristics of Maya writing pose special challenges for research, from which contradictory and doubtful deciphering hypotheses emerge. The project ›Text Database and Dictionary of Classic Mayan‹ aims at creating a machine-readable corpus of all Maya texts and compiling a dictionary on this basis. An inventory of all hieroglyphs is an indispensable instrument for further decipherment. This paper describes the development of the digital Sign Catalogue with special consideration of the modelling of decipherment hypotheses and their qualitative evaluation. Furthermore, the article emphasises the role of knowledge representation in Digital Humanities projects.",Modellierung von Entzifferungshypothesen in einem digitalen Zeichenkatalog für die Maya-Schrift,,2019,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 4,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1328,http://www.zfdg.de/sb004_003,"{'string_id': '10.17175/sb004_003', 'id_scheme': 'DOI'}","The focus of the imports of TEI documents from the German Text Archive (DTA) into a graph database (Neo4j) evaluated here is the marking of uncertain readings and editorial additions in manuscripts. In this context, we also briefly present the TEI guidelines for dealing with doubts and uncertainties. Since several people usually work in editing or transcription projects, we regard responsibility information at dubious points as central to the interoperability of data and the intersubjective traceability of individual decisions. This applies in particular if additional information on the degree of security of a resolution is possible. Graph technologies offer possibilities for modeling, visualization and analysis of uncertainty and responsibility. With a sufficiently large amount of data, personal labeling profiles of the respective editors can be created, for example.",Modellierung von Zweifel – Vorbild TEI im Graphen,,2019,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 4,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1329,http://www.zfdg.de/sb004_004,"{'string_id': '10.17175/sb004_004', 'id_scheme': 'DOI'}","In this article, we introduce a methodological proposal for modelling vagueness in graphs. In addition to the modelling, we also deal with the automatic generation of implicitly stored knowledge when considering vagueness. We use the ideas of algorithms designed for description logics and apply them on graph data. We also present our software development that implements the proposed methodology. We will demonstrate the use of our applications based on three use cases in humanities and cultural studies.",Academic Meta Tool. Ein Web-Tool zur Modellierung von Vagheit,,2019,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 4,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1330,http://www.zfdg.de/sb004_005,"{'string_id': '10.17175/sb004_005', 'id_scheme': 'DOI'}","The paper approaches the problem of the loss of sources (deperdita) from three different perspectives: In a first section, deperdita are identified as a methodological area of uncertainty for historical research and possible categorisations are proposed. The following case study from the work of the Anton Webern Gesamtausgabe illustrates the consequences of such defects in cultural memory caused by missing sources, before proposals for graph-based modeling of negations and deperdita are presented in the last part. The aim of the discussion is to raise awareness of the challenges involved in dealing with historical material that is no longer available and for a constructive and productive approach to uncertainties.",Quellenverluste (Deperdita) als methodologischer Unsicherheitsbereich für Editorik und Datenmodellierung am Beispiel von Anton Weberns George-Lied op. 4 Nr. 5,,2019,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 4,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1331,http://www.zfdg.de/sb004_006,"{'string_id': '10.17175/sb004_006a', 'id_scheme': 'DOI'}","This article aims to outline the challenge of uncertainty for the construction of computational models in the humanities. Since we can neither ignore nor eliminate uncertainty, we need to model it, and so we need computational models of uncertainty. Such models already exist and are being used in practical applications. There is ongoing fundamental research on uncertainty and its representation in mathematics, philosophy, and computer science. Some of these approaches may be suitable for modeling uncertainty in the humanities—but we are still lacking the »bridge« that could relate the uncertainty encountered in the humanities to such formal modeling frameworks. We argue that DH needs to go beyond project-specific models of uncertainty and consider uncertainty more generally; in particular, we must closely examine various types of uncertainty in the humanities and seek to develop more general frameworks for handling it.",Accepting and Modeling Uncertainty,,2019,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 4,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1332,http://zfdg.de/sb004_007,"{'string_id': '10.17175/sb004_007', 'id_scheme': 'DOI'}","Using a specific research project, this article presents an approach for displaying phenomena of uncertainty in humanities contexts – here, legal history – in a data model. In the proposal for modeling a research context, the core thesis depends on three levels that describe a) the historical phenomena, b) the traditional evidence for this phenomena, and c) the latest historical research. Although ambiguities and uncertainties will be present on all three levels, they will be modeled on the third level only since they can not be separated from the historical research. The following paper presents the research project ›Die Regierung der Universalkirche nach dem Konzil von Trient‹ with regard to uncertainties in data modeling (1.). The preliminary, at times unofficial, data model will then be described (2.); finally, this article will discuss future prospects for the further development of the model as well as challenges within the context of the project, such as in data entry and processing (3.).",Ambiguität und Unsicherheit: Drei Ebenen eines Datenmodells,,2019,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 4,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1333,http://www.zfdg.de/sb004_008,"{'string_id': '10.17175/sb004_008', 'id_scheme': 'DOI'}","This paper looks at how deep integration between text and data is attempted in The Codex project. Standoff properties are used to mediate between the plain text stream and entities modelled in the Neo4j graph database. A dynamic standoff property text editor was constructed to enable real-time changes to text and annotations without invalidating standoff property indexes. An examination of the multidimensional affordances offered by standoff properties is explored, with reference to how annotations and graph entities can combine to construct an ›atlas of history‹ using Codex.",The Codex – an Atlas of Relations,,2019,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 4,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1334,http://www.zfdg.de/sb004_009,"{'string_id': '10.17175/sb004_009', 'id_scheme': 'DOI'}","Despite many articles, reports and attention in the meantime in the mass media, blockchain technology is for most associated with bitcoin, risk, speculation, high volatility and also the dark side of the Internet. The potential of the technology is ignored in these sometimes emotional discussions. In order to make a small contribution to objectification, this following essay is written. It explains the environment in which this technology has been implemented, the ingenious approach behind the technology, the current state of development and how the blockchain technology can learn from other approaches to data storage and processing. For this the graph database is used and both approaches are compared. ",Blockchain – die etwas andere Datenbank,,2019,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 4,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1335,http://www.zfdg.de/sb004_010,"{'string_id': '10.17175/sb004_010', 'id_scheme': 'DOI'}","To facilitate the application of network analysis for the study of multi-sided Aegean seals, a graph database was implemented in Neo4j. Many of the seal’s attributes contain uncertainties, which require special attention if they are incorporated into the graph database. The article examines where the uncertainties originate from and presents how they can be modelled in the graph database. Finally, a practical example is presented, which focuses on depictions of creatures on seals and makes use of the created graph database.",A Graph Database of Aegean Seals with Uncertain Attributes,,2019,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 4,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1336,http://www.zfdg.de/sb004_011,"{'string_id': '10.17175/sb004_011', 'id_scheme': 'DOI'}"," This contribution aims to demonstrate the importance of traceable provenance information within knowledge bases in the Digital Humanities. Besides presenting rather general aspects of how to model statements in an abstract and in an exemplary manner, the concept of fact provenance is introduced and integrated with statement expression models. Using so-called provenance chains, it is shown how provenance information that is captured within an information system can be utilized to handle contradictions and reduce the overall uncertainty of the knowledge base. ",Graphbasierte Modellierung von Faktenprovenienz als Grundlage für die Dokumentation von Zweifel und die Auflösung von Widersprüchen,,2019,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 4,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1337,http://www.zfdg.de/sb004_012,"{'string_id': '10.17175/sb004_012', 'id_scheme': 'DOI'}","To represent research data from the realm of the humanities in a digital knowledge graph presents a double challenge: How is uncertain data to be stored, and what does that imply for retrieval and presentation? Which notions are conveyed through uncertain data, and how do they influence our concept of knowledge? The cultural history information system ZUCCARO created by the Bibliotheca Hertziana is a good example for a complex knowledge graph. Three cases of uncertainty or vagueness are to be distinguished: Precision, plausibility and positively negative assertions. Our contribution studies the three aspects with regard to the historic reality, the state of scientific research and the usability of the content data for scholars.","Genau, wahrscheinlich, eher nicht: Beziehungsprobleme in einem Wissensgraph",,2019,,Zeitschrift für digitale Geisteswissenschaften,Sonderband 4,,"[{'value': '2510-1366', 'type': 'electronic'}]"
1338,http://dx.doi.org/10.17928/jjadh.1.1_1,"{'string_id': '10.17928/jjadh.1.1_1', 'id_scheme': 'DOI'}","Gathering information from social media content is becoming increasingly popular. Twitter, a microblog where posts are limited to 140 characters, is an excellent platform for gathering instant and interactive information. Considerable research has focused on Twitter’s effectiveness for disseminating emergency alerts and confirming the safety of acquaintances. However, there has been less emphasis on the analysis of Twitter posts to obtain information specialized to specific domains. Such analysis could enable simple and rapid identification of information related to state-of-the-art technology. Against this background, this study reports on a preliminary analysis of tweets by Japanese academic researchers. Our content analysis and text analysis reveal that many academic researchers tweet about their individual activities, education, or research. Their tweets contain domain-specific knowledge and have identifiable textual characteristics. This study provides basic findings that can be applied to obtain domain-specific knowledge from Twitter.",Analyzing the Content and Text of Tweets by Japanese Academic Researchers,Japanese Association for Digital Humanities,2015,,Journal of the Japanese Association for Digital Humanities,1,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1339,http://dx.doi.org/10.17928/jjadh.1.1_10,"{'string_id': '10.17928/jjadh.1.1_10', 'id_scheme': 'DOI'}","The UC CEISMIC Canterbury Earthquakes Digital Archive was established in response to the devastating earthquakes that struck Canterbury region in New Zealand from September 2010 onwards, including 4 quakes of magnitude 6 or greater and over 11,000 aftershocks. 185 people died and significant parts of Christchurch city were either destroyed or have needed to be demolished, resulting in financial losses of an estimated NZ$30 billion. The rebuild is expected to take 10 – 15 years, and the UC CEISMIC archive is designed to accommodate this, acting as a distributed national (and eventually international) repository for digital content produced as a result of the earthquakes. This paper outlines the design principles and architecture of the archive, describing the commitment to open access and open source that allowed the project team to bring together a broad-ranging national consortium comprised of leading cultural organizations, who work alongside content providers ranging from individual citizens, government agencies and community groups, to large media companies. Principles common to the digital humanities community were used to bond the broader project team, in an interesting example of scholar-led community engagement. The goal is to provide a model that can be used, either in whole or in part, by future teams in need of similar capability.","Open Principles, Open Data: The Design Principles and Architecture of the UC CEISMIC Canterbury Earthquakes Digital Archive",Japanese Association for Digital Humanities,2015,,Journal of the Japanese Association for Digital Humanities,1,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1340,http://dx.doi.org/10.17928/jjadh.1.1_107,"{'string_id': '10.17928/jjadh.1.1_107', 'id_scheme': 'DOI'}","The Collaborative EuropeaN Digital Archival Research Infrastructure (CENDARI) project has developed a new virtual environment for humanities research, reimagining the analogue landscape of research sources for medieval and modern history and humanities research infrastructure models for the digital age. To achieve this, the project has needed to be sensitive to the ways in which historical research practices in the 21st Century are distinct from those of earlier eras, harnessing the affordances of technology to reveal connections and support or refute hypotheses, enabling transnational approaches, and federating sources beyond the well-known and across the largely national organization paradigms that dominate within traditional knowledge infrastructures (libraries, archives and museums). This paper describes both the user-centered development methodology deployed by the project and the resulting technical architecture adopted to meet these challenging requirements. The resulting system is a robust ‘enquiry environment’ able to integrate a variety of data types and standards with bespoke tools for the curation, annotation, communication and validation of historical insight.",The Taste of “Data Soup” and the Creation of a Pipeline for Transnational Historical Research,Japanese Association for Digital Humanities,2015,,Journal of the Japanese Association for Digital Humanities,1,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1341,http://dx.doi.org/10.17928/jjadh.1.1_37,"{'string_id': '10.17928/jjadh.1.1_37', 'id_scheme': 'DOI'}","This study proposes a framework to access to the modern history of Japanese philosophy using natural language processing (NLP) and visualization. In order to discover new knowledge from massive amounts of information, support of information technologies is required. For supporting knowledge discovery from vast amount of books, we developed an OCR-based automatic book-digitizing framework and the system visualizing documents with relationships among them calculated by using NLP techniques. We applied the framework to Japanese journal Shisō (“Thought”) by the Japanese publisher Iwanami Shoten. We show an example of knowledge structure extracted from Shisō by using our visualizing system.","Revealing the Modern History of Japanese Philosophy Using Digitization, Natural Language Processing, and Visualization",Japanese Association for Digital Humanities,2015,,Journal of the Japanese Association for Digital Humanities,1,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1342,http://dx.doi.org/10.17928/jjadh.1.1_44,"{'string_id': '10.17928/jjadh.1.1_44', 'id_scheme': 'DOI'}","To support the automatic semantic analysis of texts in the humanities, it is not sufficient to analyze words and evaluate word pairs, because it is necessary to process larger units, such as phrases, sentences, and paragraphs. This study proposes the introduction of intratextuality into a digital archive system. In the future, this method will be developed as the basis for semantic analysis of larger units. Classical literary structures that are used frequently in the Old and New Testaments were digitized as a case study. A literary structure data format for a relational database was also implemented. The literary structures of 39 books in the Old Testament and 27 books in the New Testament were digitized. The total number of digitized literary structures was 1,507 and the elements of these structures comprised 7,715 pairs. These data were stored in a Java-based relational database system and a web-based viewer program for rhetorical structures was implemented as a JSP servlet. This web-based program will be combined with an existing digital archive system that can manage intertextuality data. The Java-based relational database system and the JSP servlet will facilitate numerical analyses of the intertextuality and intratextuality of digital archive systems of classical texts, thus making it much easier to conduct scientific analyses of the meanings of texts.",Database System for Archiving the Literary Structure of the Bible,Japanese Association for Digital Humanities,2015,,Journal of the Japanese Association for Digital Humanities,1,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1343,http://dx.doi.org/10.17928/jjadh.1.1_58,"{'string_id': '10.17928/jjadh.1.1_58', 'id_scheme': 'DOI'}","Printed books are finished products once published. Digital editions, on the other hand, may have an eternal life as updateable items, at least as archives. Since the work on digital editions started in the different Nordic countries more than 20 years ago, a consensus or convergence of practice seems to have grown in the planning and development of digital scholarly editions in the Nordic countries (here defined as Denmark, Finland, Norway, and Sweden; for Finnish editions, especially those based on material written in Swedish) (Dahlström and Ore 2013). Whereas it is possible to find some examples of similar developments in other countries or continents, it seems that there are remarkable similarities between the Nordic countries. In this paper I want to discuss what factors have contributed to this situation, and to illustrate them by presenting some fairly recent digital editions from Nordic countries.",A Nordic Tradition for Digital Scholarly Editions?,Japanese Association for Digital Humanities,2015,,Journal of the Japanese Association for Digital Humanities,1,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1344,http://dx.doi.org/10.17928/jjadh.1.1_68,"{'string_id': '10.17928/jjadh.1.1_68', 'id_scheme': 'DOI'}","This paper introduces the idea of data sharing strategy based on a conversion service, not on a sharing application, scheme, or ontology, that are dominant in proposals for language documentation. Although these three methods have been basic tactics for sharing corpora, they have a conceptual flaw in terms of descriptive linguistics. In this paper we report the results of a previous project - the LingDy project, and propose a basic concept for corpus sharing strategy to support personal diachronic data sharing. This paper is a revised version of a handout at JADH2012, so readers should be careful that this content is based on results at the time of 2012.",Corpus Sharing Strategy for Descriptive Linguistics,Japanese Association for Digital Humanities,2015,,Journal of the Japanese Association for Digital Humanities,1,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1345,http://dx.doi.org/10.17928/jjadh.1.1_86,"{'string_id': '10.17928/jjadh.1.1_86', 'id_scheme': 'DOI'}","This paper describes a knowledge based character processing model to resolve some problems of coded character model. Currently, in the field of information processing of digital texts, each character is represented and processed by the “Coded Character Model.” In this model, each character is defined and shared using a coded character set (code) and represented by a code-point (integer) of the code. In other words, when knowledge about characters is defined (standardized) in a specification of a coded character set, then there is no need to store large and detailed knowledge about characters into computers for basic text processing. In terms of flexibility, however, the coded character model has some problems, because it assumes a finite set of characters, with each character of the set having a stable concept shared in the community. However, real character usage is not so static and stable. Especially in Chinese characters, it is not so easy to select a finite set of characters which covers all usages. To resolve these problems, we have proposed the “Chaon” model. This is a new model of character processing based on character ontology. This report briefly describes the Chaon model and the CHISE (Character Information Service Environment) project, and focuses on how to represent Chinese characters and their glyphs in the context of multiple unification rules.",Multiple-policy Character Annotation based on CHISE,Japanese Association for Digital Humanities,2015,,Journal of the Japanese Association for Digital Humanities,1,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1346,http://dx.doi.org/10.17928/jjadh.2.1_1,"{'string_id': '10.17928/jjadh.2.1_1', 'id_scheme': 'DOI'}","Below we develop a method to determine whether the use of grammatical particles in Chinese Buddhist scriptures is characteristic for the period of their translation. The corpus consists of three different Chinese translations of an early Indian Mahāyāna text from two different periods. We use the results of Principal Component Analysis (PCA) to discern if the samples of texts from different periods cluster together. We found that PCA used on common grammatical particles that exhibit great variance between texts, but occur evenly distributed within the texts, does in this case indeed yield distinctive patterns that distinguish the translations from different time periods. This is relevant for historical Chinese linguistics and Buddhist studies. It allows us to identify grammatical particles the use of which differs between translations from different periods. This in turn is an important basis for further research into Buddhist Hybrid Chinese translation idioms and the better attribution and dating of Chinese Buddhist texts.",Stylometric Analysis of Chinese Buddhist texts - Do different Chinese translations of the Gaṇḍavyūha reflect stylistic features that are typical for their age?,Japanese Association for Digital Humanities,2017,,Journal of the Japanese Association for Digital Humanities,2,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1347,http://dx.doi.org/10.17928/jjadh.2.1_31,"{'string_id': '10.17928/jjadh.2.1_31', 'id_scheme': 'DOI'}","This paper proposes a method for describing narrative structure that focuses on the behavior of characters in a story. In this approach, focus, polarity, dynamic, motivation, and result are assigned as attributes of behavior. As a case study, the plots of flash fictions by Shin'ichi Hoshi were analyzed and represented formally using such behavioral attributes. Specifically, this approach to plot description enabled the representation of “punch lines,” which comprise a reversal of story elements (such as agent and recipient). Using software to analyze plot descriptions, certain types of reversal-type punch lines were automatically extracted. These extracted reversal patterns were sorted based on the rules of “similarity” and “distant foreshadowing,” and valid reversal-type punch lines were extracted in 60% of the analyzed stories.",Automatic Extraction of Reversal-Type Punch Lines in Shin'ichi Hoshi's Flash Fictions,Japanese Association for Digital Humanities,2017,,Journal of the Japanese Association for Digital Humanities,2,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1348,http://dx.doi.org/10.17928/jjadh.2.1_48,"{'string_id': '10.17928/jjadh.2.1_48', 'id_scheme': 'DOI'}","This paper aims to demonstrate that variables relating to the independency of the string parts are useful for discriminating between symphonies and string quartets in terms of their orchestrations. More specifically, this paper targets the fast initial movements of thirty-nine symphonies and twenty string quartets by Mozart. Conducted analysis extracted three factors within the twelve observed variables and suggests that one of those factors, namely, the Lower part Pitch-Name and Duration factor (LPND), can account for the differences between Mozart’s symphonies and string quartets composed after 1782.","This paper aims to demonstrate that variables relating to the independency of the string parts are useful for discriminating between symphonies and string quartets in terms of their orchestrations. More specifically, this paper targets the fast initial movements of thirty-nine symphonies and twenty string quartets by Mozart. Conducted analysis extracted three factors within the twelve observed variables and suggests that one of those factors, namely, the Lower part Pitch-Name and Duration factor (LPND), can account for the differences between Mozart’s symphonies and string quartets composed after 1782.",Japanese Association for Digital Humanities,2017,,Journal of the Japanese Association for Digital Humanities,2,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1349,http://dx.doi.org/10.17928/jjadh.2.1_60,"{'string_id': '10.17928/jjadh.2.1_60', 'id_scheme': 'DOI'}","The present paper aims at designing a monitoring framework for a yet new interdisciplinary research and education program in Japan, “Cultural Resources Studies.”, ""Bunkashigengaku"" in Japanese. We analyze the linkage between a university, an academic association, and the practitioners’ institutions closely related with cultural resources through the mining of the principal texts produced by them. Our findings reveal the complicated relations among these stakeholder institutions, and attest to the importance of the revision cycle for the advance of interdisciplinary studies.",Designing Research for Monitoring Humanities-based Interdisciplinary Studies: A Case of Cultural Resources Studies (Bunkashigengaku 文化資源学) in Japan,Japanese Association for Digital Humanities,2017,,Journal of the Japanese Association for Digital Humanities,2,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1350,http://dx.doi.org/10.17928/jjadh.2.1_73,"{'string_id': '10.17928/jjadh.2.1_73', 'id_scheme': 'DOI'}","Wikipedia: The Free Encyclopedia was launched in January 2001, and its articles now represent a major resource for understanding the world. Many of these articles have been negotiated and edited for a decade or more, and the history of that editing can provide insight into the recent history of ideas. This paper describes the development of a tool called WIScker that works with the Wikipedia Application Programming Interface (API) to scrape, or ""wisck,"" the revision history of any Wikipedia article, in order to build a corpus for subsequent text analysis and visualization. As an example, we examine a fourteen-year revision history of the article ""Terrorism,"" first introduced into Wikipedia in October 2001, the month after 9/11, and subsequently expanded to provide a more historically informed, though still politically motivated, entry.",WIScking Ideas,Japanese Association for Digital Humanities,2017,,Journal of the Japanese Association for Digital Humanities,2,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1351,http://dx.doi.org/10.17928/jjadh.2.1_90,"{'string_id': '10.17928/jjadh.2.1_90', 'id_scheme': 'DOI'}","Popular songs can be regarded as a fine representation of modern society andculture. In particular, the lyrics of popular songs are the most importantaspect for understanding the sense of values and linguistic sensitivity in agiven generation and community. The purpose of the present study is toinvestigate the chronological variation of popular Japanese songs usingstylometric techniques. This study draws on the lyrics of 858 songs, whichappeared on the Oricon annual top 20 single hit chart between 1976 and 2015. Thelinguistic features investigated in this study include five different types oflexical indices, namely (a) number of words, (b) parts-of-speech, (c) wordtypes, (d) character types, and (e) vocabulary level. Multiple regressionanalysis was conducted to explore the chronological change in the frequencies oflexical indices. The results showed that the frequencies of word types andcharacter types dramatically changed before and after 1990. Moreover, the usagesof auxiliary verbs as well as lower level vocabulary became more prominent,whereas the frequencies of adjectival nouns and conjunctions decreased. Thefindings suggest that a turning point in cultural trends corresponds with thehistorically significant political and economic events, such as the end of theShowa era and the burst of the bubble economy.",Investigating the Chronological Variation of Popular Song Lyrics ThroughLexical Indices,Japanese Association for Digital Humanities,2017,,Journal of the Japanese Association for Digital Humanities,2,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1352,http://dx.doi.org/10.17928/jjadh.3.1_1,"{'string_id': '10.17928/jjadh.3.1_1', 'id_scheme': 'DOI'}","Historians have traditionally relied on close readings of select primary sources to evaluate linguistic and discursive changes over time, but this approach can be limiting in its scope. Numeric representations of language allow us to statistically quantify and compare the significance of discursive changes and capture linguistic relationships over time. Here, we compare two deep learning methods of quantitatively identifying the chronology of linguistic shifts: RNN classification and RNN language modeling. In particular, we examine deep learning methods of isolating stylistic from topical changes, generating “decade embeddings,” and charting the changing average perplexity in a language model trained on chronologically sorted data. We apply these models to a historical diplomatic corpus, finding that the two world wars proved to be notable moments of linguistic change in American foreign relations. With this example we show applications of text-based deep learning methods for digital humanities usages.",The Long Arc of History: Neural Network Approaches to Diachronic Linguistic Change,Japanese Association for Digital Humanities,2018,,Journal of the Japanese Association for Digital Humanities,3,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1353,http://dx.doi.org/10.17928/jjadh.3.1_114,"{'string_id': '10.17928/jjadh.3.1_114', 'id_scheme': 'DOI'}","This article uses character n-grams methods to assess the authorship of three religious texts written in medieval Japan, comparing them to the works of Monkan (1278-1357), a Shingon monk active during the first half of the 14th century. Such texts belong to a literary genre called shōgyō that presents many challenges that render the use of traditional authorship attribution methods inappropriate. They were composed in Japanese kanbun (classical Chinese read in Japanese word order), and their contents are closer to cumulative work than to original creations by a single author. This article thus draws on previous research on the translators of the Chinese Buddhist canon, which have proven to be far more effective than traditional methods developed for modern languages for the analysis of this type of literature. Concretely, it proposes a workflow proceeding from the preparation of the corpus (with manuscript edition, encoding,…) to a concrete data analysis using the variable length n-gram method. The last part of the article deals with future perspectives that would further refine the results, such as taking into account the various speaking voices found inside the shōgyō and their relationship to the author, as well as stylistic analysis based on grammatical patterns. As a whole, the experiment succeeded in showing global trends in the texts of the Japanese Shingon schools, finding stylistic differences between the works of Kūkai, of monks from the 12th century, and of Monkan. Combined with a rigorous historical enquiry into the redaction context of the texts and their manuscripts, the data analysis also demonstrates that one of them was almost certainly written by Monkan, and another either by him or by one of his close disciples.",Data Mining in the Works of the Shingon Monk Monkan (1278–1357): Using Digital Methods to Assess the Contested Authorship of Three Religious Texts from Medieval Japan,Japanese Association for Digital Humanities,2018,,Journal of the Japanese Association for Digital Humanities,3,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1354,http://dx.doi.org/10.17928/jjadh.3.1_33,"{'string_id': '10.17928/jjadh.3.1_33', 'id_scheme': 'DOI'}","The Song Dynasty was a decisive period of transformation in ancient China, during which relationships between scholars and politicians are thought to have become closer and closer, and this change is considered part of the “Tang–Song transition.” In the Song Dynasty, the Yuanyou 元祐 era (1086–94) was a critical and complex period with regard to its political environment. The major purpose of this paper is to investigate the relationships between scholars and politicians during this period. The connections between figures collected from the CBDB (China Biographical Database) include both literary relations and political relations. Two scholars have a literary relation when both of them write to a common third figure, and a political relation between two politicians is demonstrated through such connections as political support associations, recommendation sponsorship, and oppositional political affiliations. In the present study, two matrices are respectively constructed according to literary and political relations among figures and a Poisson-Gamma factorization model is adopted to obtain the key factors of the matrices. According to calculated results and literary history, the scholars can be clearly classified into three groups. We identified two groups of the politicians with this method, while we found other politicians to have steered a course between them. Furthermore, the figures engaged in common literary pursuits are more likely to share common political goals. As a result, the observation that scholars and politicians are related closely in the Yuanyou era confirms that this period must have featured literati politics.",Investigating the Relationships between Scholars and Politicians in Ancient China: Taking the Yuanyou Era as an Example,Japanese Association for Digital Humanities,2018,,Journal of the Japanese Association for Digital Humanities,3,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1355,http://dx.doi.org/10.17928/jjadh.3.1_49,"{'string_id': '10.17928/jjadh.3.1_49', 'id_scheme': 'DOI'}","Although the literary structures employed in the Bible (chiasmus, concentric structures, and parallelism) are important for its interpretation, the ambiguity of these structures renders them problematic. In this paper, in order to establish an objective framework of evaluation for these structures, a quantitative analysis method is proposed. The target hypothesis of literary structures is the Parallel Literary Structure hypothesis about the hierarchical literary structure of the Bible. This hypothesis proposes that the literary structures in all books of the Bible have a parallel common system. Specifically, the validity of text divisions was evaluated based on divisions rendered in a number of extant Bible translations. Then, corresponding pericope pairs (a pericope is a small story unit in the Bible) that include various “common rare” words and phrases were counted, and the number of valid pairs was compared with the number of randomly constructed structures. From this analysis, statistical significances were extracted and the result strongly supports the hypothesis quantitatively.",Quantitative Evaluation Method for a Literary Structure Hypothesis within the Bible,Japanese Association for Digital Humanities,2018,,Journal of the Japanese Association for Digital Humanities,3,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1356,http://dx.doi.org/10.17928/jjadh.3.1_73,"{'string_id': '10.17928/jjadh.3.1_73', 'id_scheme': 'DOI'}","The Landscapes of Injustice project is a multi-institutional seven-year research project funded by a Partnership Grant from the Social Sciences and Humanities Research Council of Canada. The project mission is to investigate, document, and analyze the process by which, beginning in 1942, tens of thousands of people of Japanese ethnicity were interned, and their property was seized and disposed of by the Canadian government and its agents. Part of this process involves identifying where Japanese Canadians were living and working prior to their internment, what property they owned, and how the dispossession affected them. This requires that we identify Japanese Canadian individuals across a range of different types of official and unofficial records, often based only on name, on a scale that makes an entirely manual process impractical. The project has therefore developed a semi-automated algorithmic approach to determining whether any name in the records is Japanese or not. This article describes the algorithm in detail, along with its application and limitations.",Algorithmic Determination of Japanese Ethnic Identity Based on Name,Japanese Association for Digital Humanities,2018,,Journal of the Japanese Association for Digital Humanities,3,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1357,http://dx.doi.org/10.17928/jjadh.3.1_98,"{'string_id': '10.17928/jjadh.3.1_98', 'id_scheme': 'DOI'}","Migrants all over the world have left multiple traces in different countries, and this cultural heritage is of growing interest to researchers and to the migrant communities themselves. Cultural heritage institutions, however, have dwindling funds and resources to meet the demand for the heritage of immigrant communities to be protected. In this article we propose that the key to bridging this gap is to be found in new possibilities that are opened up if resources are linked to enable digital exploration of archival records and collections. In particular, we focus on the value of building a composite and distributed resource around migrants’ life courses. If this approach is used and dispersed collections held by heritage institutions can be linked, migrant communities can have access to detailed information about their families and researchers to a wealth of data—serial and qualitative—for sophisticated and innovative research. Not only does the scattered data become more usable and manageable, it becomes more visible and coherent; patterns can be discovered that were not apparent before. We use the Dutch-Australian collaborative project “Migrant: Mobilities and Connection” as an example and case study of this life course–centered methodology and propose that this may develop into a migration heritage template for migrants worldwide.","Migrating People, Migrating Data: Digital Approaches to Migrant Heritage",Japanese Association for Digital Humanities,2018,,Journal of the Japanese Association for Digital Humanities,3,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1358,http://dx.doi.org/10.17928/jjadh.4.1_37,"{'string_id': '10.17928/jjadh.4.1_37', 'id_scheme': 'DOI'}","The genre of boys’ love (BL), which generally refers to a body of works that depict fictional relationships between beautiful “boys,” is produced for and consumed mainly by women in Japan. Ludic expressions of sexuality and gender unique to BL have gained popularity on a global scale but have also drawn negative attention. In this article, we employ the concept of asobigokoro (playful spirit/heart) to highlight the importance of regional ideas of play and playfulness in game analysis. We argue that asobigokoro functions as a kind of counter-discourse as it privileges non-Eurocentric ways of knowing, understanding, and “playing” with representations of sexuality. Game analysis through an asobigokoro lens enriches the field of regional gaming by drawing on Japanese sociopolitical contexts to situate a reading of Japanese ludic representations. Asobigokoro stresses the importance of understanding cultural variations of “play” and “playfulness” in order to make sense of “taboo” subjects in culturally nuanced ways. In our textual analysis of Enzai: Falsely Accused, we discover that the simultaneous appropriation and subversion of violent and sexually explicit content, which characterizes the game’s asobigokoro, can be traced to Japanese feminist forms of asobi (play), which are rooted within the Yaoi tradition.",Playing with Pain,Japanese Association for Digital Humanities,2019,,Journal of the Japanese Association for Digital Humanities,4,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1359,http://dx.doi.org/10.17928/jjadh.4.1_54,"{'string_id': '10.17928/jjadh.4.1_54', 'id_scheme': 'DOI'}","Attempts to preserve retro game elements or ‘the classic feeling’ through remixing retro elements with modern ones are not a new method. However, it is an effective preservation method. According to Newman, videogame preservation goes beyond simply preserving hardware. In order to deliver “authentic play (and aesthetic) experiences”, it also necessary to determine what are the most significant, qualitative aspects or properties that make a particular videogame what it is (Newman 2012, 122–23). Part of the success of classic characters like Mario is how often they are reused and reimagined (Suominen 2012, 8). Even if you have never played the original Mario games, you have still likely been exposed to the franchise’s most ‘basic’ elements through their various iterations. However, while the game industry typically aims for continual innovation and reinvention (Newman 2012, 9), the changing market and gaming landscape is allowing for alternative opportunities for remixing retro elements. One notable example includes Mega Man 9, which garnered praise for their gorgeous, realistic graphics. While all three games were made for modern consoles, Mega Man 9 was made in classic retro style, not only to return the series to its roots but also to give players a new story and “the classical feel” (Takeshita 2008). By studying the approaches developers have used, it is possible to study what ‘feeling’ they were attempting to preserve, what elements were considered most essential, how these elements were translated, and what new meanings occur.",Remixing Retro,Japanese Association for Digital Humanities,2019,,Journal of the Japanese Association for Digital Humanities,4,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1360,http://dx.doi.org/10.17928/jjadh.4.1_7,"{'string_id': '10.17928/jjadh.4.1_7', 'id_scheme': 'DOI'}","This article focuses on Nintendo’s influential and much-celebrated 1998 videogame The Legend of Zelda: Ocarina of Time (hereafter OoT). In particular, it explores the often unexpectedly creative and wholly transformative ways in the game is played by ‘speedrunners.’ Seeking to race through the game as quickly as possible, speedrunners play in a distinctive way that combines mastery of performance execution with a deep knowledge of the game’s operation and how its systems may be exploited. Speedrunning performances are creative because they involve astonishingly detailed investigations of the minutiae of game behaviors. They are transformative because they disrupt and even invert much-vaunted aspects of the game as sequences that slow down progress are circumvented. As such, what would otherwise class as crucial moments in the storyline, key character development and even locales, are not only raced through but are actively skipped. Compared with the game’s narrative, the connectivity of its spaces, and the complex but clearly mapped passage of time as set out in Nintendo’s officially-endorsed ‘Strategy Guides’ (see Buchanan et al 1998; Hollinger et al 1998; Loe and Guess 1998), the OoT speedrun constructs an altogether different game with vastly altered priorities. And so, the offer to, ‘Join legendary hero Link as he journeys across Hyrule, and even through time, to thwart the plans of Ganondorf.’ (Nintendo 2017), is recast as a breakneck dash to the closing credits sequence evading and avoiding all but the essential moments of gameplay.","Wrong Warping, Sequence Breaking, and Running through Code",Japanese Association for Digital Humanities,2019,,Journal of the Japanese Association for Digital Humanities,4,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1361,http://dx.doi.org/10.17928/jjadh.4.1_72,"{'string_id': '10.17928/jjadh.4.1_72', 'id_scheme': 'DOI'}","No discussion of play and leisure in Japan would be complete without acknowledging the place of pachinko, a game so popular that it dominates the leisure economy in a way no other medium does, not even videogames. Even in decline, pachinko generates more revenue in Japan than any other leisure pursuit and yet it is rarely discussed in game studies in the West or even in the Japanese literature around game culture. This is largely because of the gambling associated with pachinko and because it is unique to Japan – a national industry with no export potential. Pachinko is at the disreputable edge of gaming—economically, culturally, and mechanically. Above all, pachinko machines stand out as monstrous hybrids that combine the electro-mechanical apparatus of a pinball machine with the digital screen of a slot machine in a curious mixed interface that seems to be an anachronism of a pre-computing era. And that is what this paper is about – the hybrid interface of pachinko.",Pachinko,Japanese Association for Digital Humanities,2019,,Journal of the Japanese Association for Digital Humanities,4,1,"[{'value': '2188-7276', 'type': 'electronic'}]"
1362,https://journals.psu.edu/dls/article/view/59648,"{'string_id': '10.18113/P8dls1159648', 'id_scheme': 'DOI'}","Biopolitics identifies a shift in how government operates, specifically at the level of managing the bodies of citizens. The critical thinkers I discuss in this paper––Michel Foucault, Georgio Agamben, Michael Hardt and Antonio Negri––explore biopolitics and the topic of ""life"": what is life, how is it defined, and whom has the power to control it? In this paper, I argue that the presence of language in electronic literature not only points to the role of language in biopolitical power (or biopower), but also affords electronic literature the ability to act as a site of aesthetic resistance (following Hardt and Negri) operating within and against biopolitical regimes. In particular, I illustrate how 'codework,' a form of electronic literature which integrates natural and programming languages to create its surface-level language, provides a tool from which to think through and work against biopower. For this, I turn to the writings of Mez Breeze, an electronic author working primarily with codework to explore questions of subjectivity, embodiment, and power in electronic literature.",The Biopolitics of Electronic Literature: On the Writings of Mez Breeze,,2016,,Digital Literary Studies,1,1,"[{'value': '2376-4228', 'type': 'electronic'}]"
1363,https://journals.psu.edu/dls/article/view/59703,"{'string_id': '10.18113/P8dls1159703', 'id_scheme': 'DOI'}","This paper offers a unique model for collaborative digital scholarly edition environments that features functions and possibilities associated with game-related play, challenge, achievement, and narrativity. These proposed environments can and should encourage two types of technical education simultaneously: the use of the scholarly edition software, and an increasing mastery of scholarly editing practice. Further, successful models of collaboration found in Massively Multiplayer Online Role-Playing Games can be effectively used to facilitate the operation of collaborative, dynamic editing environments, and achievement ""badges"" along with other ways of acknowledging user contributions are effective ways to validate the gaming ""levels"" and ""achievements"" that various contributors have reached within the editing environment. Functionally incorporating these recognizable game elements into the development of digital scholarly editions generates an apprenticeship space that continually demands a performative understanding of the kinds of work that go into scholarly editing, encourages collaborative edition development, and clearly establishes who exactly has accomplished that work (and to which level).",Gaming the Edition: Modelling Scholarly Editions through Videogame Frameworks,,2016,,Digital Literary Studies,1,1,"[{'value': '2376-4228', 'type': 'electronic'}]"
1364,https://journals.psu.edu/dls/article/view/59715,"{'string_id': '10.18113/P8dls1159715', 'id_scheme': 'DOI'}","<additions>, <label>, <head>, <note>: with over five hundred elements, why can't I find the one I need? By tracing the different TEI (Text Encoding Initiative) elements and attributes available to describe and record marginalia, this paper reveals how the placement and purpose of early modern marginalia require careful consideration by digital editors. Each element that an encoder might consider using for in-text marginalia, and the customization that proscribes that use, carries different semantic weight. This essay considers the options for encoding marginalia using TEI, details practices of existing projects, and then turns to manuscripts written by Archbishop William Sancroft (1617-1693) as test cases. Ultimately, encoding marginalia is a data-modeling question that asks us to rethink the nature and function of our texts and paratexts, questioning where one ends and the other begins.",Encoding the Edge: Manuscript Marginalia and the TEI,,2016,,Digital Literary Studies,1,1,"[{'value': '2376-4228', 'type': 'electronic'}]"
1365,https://journals.psu.edu/dls/article/view/59753,"{'string_id': '10.18113/P8dls1159753', 'id_scheme': 'DOI'}","Digital humanities research has frequently been characterized by a high degree of recursivity, that is to say, by attempts to use digital humanities tools and techniques to reflect back on the structures and history of the field. Digital Humanities Masterplots is intended as a contribution to this ongoing project of self-analysis, but one that shifts attention away from actors, institutions, and research agendas and toward what Matthew Kirschenbaum refers to as the digital humanities as a ""discursive construction."" My aim is not to offer a survey of or an opinion piece on work in an emerging field, not least because a critical mass of such essays already exists. Rather, my primary material will comprise a series of second-order reflections on the field, and, more specifically, how they tend to emplot the rise of the digital humanities in order to render it intelligible within a range of intellectual, institutional, and societal contexts. Inspired by the narratologist Mieke Bal's assertion that ""the shape of the story you tell determines what knowledge you produce,"" my claim is that the shapes of these digital humanities stories derive from certain basic assumptions about the aims of humanistic inquiry, and that by attending to them we can better understand an emerging set of positions regarding the role of the humanities as such.",Digital Humanities Masterplots,,2016,,Digital Literary Studies,1,1,"[{'value': '2376-4228', 'type': 'electronic'}]"
1366,http://dx.doi.org/10.22148/001c.11652,"{'string_id': '10.22148/001c.11652', 'id_scheme': 'DOI'}","Distant readers have used predictive modelling to study the strength of the relationship between characterization and binary notions of gender. This essay builds on that research, shedding light on several historical trends concerning anatomical description and its relationship to gender. Some of the evidence suggests that bodily language has long played a larger role in configuring fictional women than it did for fictional men. Other evidence implies that bodily characteristics were increasingly bifurcated along a gender binary, reflecting how characters are more and more physically sorted along a feminine-masculine axis. Taken altogether, this essay unpacks a suggestive correlation: a growing aspect of characterization was increasingly imbricated in heteronormative discourses. By weighing the discrepancies between the evidence presented in this essay, and that of its predecessors, this essay will ultimately suggest that disaggregating statistical models can unfold patterns of literary change that would otherwise remain suppressed.",Fleshing Out Models of Gender in English-Language Novels (1850 – 2000),"Department of Languages, Literatures, and Cultures",2020,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1367,http://dx.doi.org/10.22148/001c.12049,"{'string_id': '10.22148/001c.12049', 'id_scheme': 'DOI'}","This paper compares social media traces from Goodreads to data from the MLA International Bibliography and the Open Syllabus Project, in order to better understand the preferences of readers of Victorian literature from different but overlapping communities. We find that the majority of works of Victorian literature that are indicated as being read on Goodreads occur about as often as they are taught or written about in the academy, although books aimed at an adult audience are written about more frequently in peer-reviewed venues. Interestingly, those works that are statistical outliers in terms of their greater popularity with a general audience than an academic audience tend to feature women authors, children’s literature, and works with a strong female protagonist. Turning to an analysis of the written reviews on Goodreads of three outliers that were more popular with a general audience--A Tale of Two Cities, Jane Eyre, and The Secret Garden--we find that readers tend to comment on plot (especially in Dickens), feminist themes (in Jane Eyre), and the importance of characters (in all three works). In conclusion, we suggest ways in which postsecondary teachers might draw on these results to inform their syllabi and formulate strategies for teaching Victorian literature. We argue that in terms of outliers, popular taste in Victorian literature among Goodreads users reflects more general reading preferences among this user group, as readers turn to the Victorian era to read children’s literature and books featuring strong female characters.",The Social Lives of Books: Reading Victorian Literature on Goodreads,"Department of Languages, Literatures, and Cultures",2020,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1368,http://dx.doi.org/10.22148/001c.12266,"{'string_id': '10.22148/001c.12266', 'id_scheme': 'DOI'}","We extracted 327,322 faces from an archive of Time magazine containing 3,389 issues dating from 1923 to 2014, classified the gender of each extracted face, and discovered that the proportion of female faces contained within this archive varied in interesting ways over time. The proportion of female faces first peaked in the mid-to-late 1940s. This was followed by a dip lasting from the mid-1950s to the early 1960s. The 1970s saw another peak followed by a dip over the course of the 1980s. Finally, we see a slow and steady rise in the proportion of female faces from the early 1990s onwards. In this paper, we seek to make sense of these variations through an interdisciplinary framework drawing on psychology, visual studies (in particular, photography theory), and history. Through a close reading of our Time archive from the 1940s through the 1990s, we conclude that the visual representation of women in Time magazine correlates with attitudes toward women in both the historical context of the era and the textual content of the magazine.","What’s in a Face? Gender representation of faces in Time, 1940s-1990s","Department of Languages, Literatures, and Cultures",2020,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1369,http://dx.doi.org/10.22148/001c.12556,"{'string_id': '10.22148/001c.12556', 'id_scheme': 'DOI'}","This article discusses ways that dramatic structure can be analyzed through the use of social titles in Shakespeare’s plays. Freytag’s (1863) pyramid of dramatic structure is based on patterns he found in Shakespearean and Greek tragedy; more recently, computational methods are being employed to model narrative structure at scale. However, there has not yet been a study which discusses whether or not specific lexical items can be indicative of dramatic structure. Using Shakespeare’s plays as an example, this essay fills the gap by observing how social titles can be used to explore the viability of narrative structure.",Dramatic Structure and Social Status in Shakespeare’s Plays,"Department of Languages, Literatures, and Cultures",2020,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1370,http://dx.doi.org/10.22148/001c.13680,"{'string_id': '10.22148/001c.13680', 'id_scheme': 'DOI'}","One commonly recognized feature of the Ancient Greek corpus is that later texts frequently imitate and allude to model texts from earlier time periods, but analysis of this phenomenon is mostly done for specific author pairs based on close reading and highly visible instances of imitation. In this work, we use computational techniques to examine the similarity of a wide range of Ancient Greek authors, with a focus on similarity between authors writing many centuries apart. We represent texts and authors based on their usage of high-frequency words to capture author signatures rather than document topics and measure similarity using Jensen-Shannon Divergence. We then analyze author similarity across centuries, finding high similarity between specific authors and across the corpus that is not common to all languages.",Like Two Pis in a Pod: Author Similarity Across Time in the Ancient Greek Corpus,"Department of Languages, Literatures, and Cultures",2020,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1371,http://dx.doi.org/10.22148/001c.14112,"{'string_id': '10.22148/001c.14112', 'id_scheme': 'DOI'}","Hosted out of the University of Virginia, Digital Yoknapatawpha is an international collaboration between scholars of William Faulkner and technologists at the Institute for Advanced Technology in the Humanities. The project team has encoded all the locations, characters, and events in Faulkner’s Yoknapatawpha fictions into a relational database that powers an open-access web-portal. Users can avail of an atlas of “deep-maps,” data visualizations, archival material, and aural and visual resources to explore, teach, and research his works. Using techniques common in ecology and demography, this paper leverages the data to investigate the relationship between race, kinship, and space. It concludes, tentatively, that the social world of Yoknapatawpha is far more rigidly bounded along racial lines than current scholarship suggests. In particular, most interactions between characters from different races happen in a familial context, and are the result of racialized labor exploitation or outright enslavement of African-American families by Anglo-Americans. The lack of interactions outside of this context underscores just how little agency non-white characters have in Faulkner’s fiction.","Familial Places in Jim Crow Spaces: Kinship, Demography, and the Color Line in William Faulkner’s Yoknapatawpha County","Department of Languages, Literatures, and Cultures",2020,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1372,http://dx.doi.org/10.22148/16.002,"{'string_id': '10.22148/16.002', 'id_scheme': 'DOI'}","Applause is a significant cultural marker in recorded performances. In poetry performances, applause can be a means by which an audience can indicate its response to a speaker’s performance or to the audience in general; a means for expressing elation and appreciation or, perhaps, dismay; and a way to engage indialog with a poem itself and affect its mode of meaning making.",Measured Applause: Toward a Cultural Analysis of Audio Collections,"Department of Languages, Literatures, and Cultures",2016,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1373,http://dx.doi.org/10.22148/16.004,"{'string_id': '10.22148/16.004', 'id_scheme': 'DOI'}","I developed the concept of cultural analytics in 2005 to refer to “the analysis ofmassive cultural datasets and flows using computational and visualization tech-niques.” In 2007 we established a research lab (Software Studies Initiative, softwarestudies.com) to start working on practical projects.","The Science of Culture? Social Computing, Digital Humanities and Cultural Analytics","Department of Languages, Literatures, and Cultures",2016,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1374,http://dx.doi.org/10.22148/16.005,"{'string_id': '10.22148/16.005', 'id_scheme': 'DOI'}","The concept of genre is as old as literary theory itself, but centuries of debate haven’t produced much consensus on the topic. Part of the reason is that genre looks like a different thing at different points in the life of a text.",The Life Cycles of Genres,"Department of Languages, Literatures, and Cultures",2016,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1375,http://dx.doi.org/10.22148/16.006,"{'string_id': '10.22148/16.006', 'id_scheme': 'DOI'}","Beginnings are always hard to trace. They tend to belong more to the realm of myth, as Tristram Shandy well knew. At what point did it become necessary, in the sense of unavoidable, to use computation to study culture? Was it a certain polemic, new kinds of data (Google Books, Project Gutenberg), the rise of analytical techniques (natural language processing, machine learning), technologies such as the internet or social media, or simply that powerful social actor called “critical mass”.",There Will Be Numbers,"Department of Languages, Literatures, and Cultures",2016,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1376,http://dx.doi.org/10.22148/16.007,"{'string_id': '10.22148/16.007', 'id_scheme': 'DOI'}","Beginnings are always hard to trace. They tend to belong more to the realm of myth, as Tristram Shandy well knew. At what point did it become necessary, in the sense of unavoidable, to use computation to study culture? Was it a certain polemic, new kinds of data (Google Books, Project Gutenberg), the rise of analytical techniques (natural language processing, machine learning), technologies such as the internet or social media, or simply that powerful social actor called “critical mass”.",Linguistic Markers of Status in Food Culture: Bourdieu’s Distinction in a Menu Corpus,"Department of Languages, Literatures, and Cultures",2016,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1377,http://dx.doi.org/10.22148/16.008,"{'string_id': '10.22148/16.008', 'id_scheme': 'DOI'}","Genre is a word whose time has come — and gone — and might now, perhaps, be coming back again. Debates about particular literary kinds have been com-mon in literary criticism since Aristotle’s Poetics, but they acquired a new intensity and reflexivity in the third quarter of the twentieth century, as structuralists and poststructuralists struggled to redefine the concept of genre itself.",Genre Theory and Historicism,"Department of Languages, Literatures, and Cultures",2016,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1378,http://dx.doi.org/10.22148/16.009,"{'string_id': '10.22148/16.009', 'id_scheme': 'DOI'}","Is “literary fiction” a useful genre label in the post-World War II United States? In some sense, the answer is obviously yes; there are sections marked “literary fiction” on Amazon, in bookstores, and on Goodreads, all of which contain many postwar and contemporary titles. Much of what is taught in contemporary fiction classes also falls under the heading of literary fiction, even if that label isn’t alwaysused explicitly.","Genre, Computation, and the Varieties of Twentieth-Century U.S. Fiction","Department of Languages, Literatures, and Cultures",2016,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1379,http://dx.doi.org/10.22148/16.010,"{'string_id': '10.22148/16.010', 'id_scheme': 'DOI'}","The relationship between character identity and character action is an established topic of literary study. In Morphology of the Folktale, Vladimir Propp argues against the separation of “who acts” from “the question of the actions themselves,” instead advocating an approach that studies characters according to their functions.",Understanding Gender and Character Agency in the 19th Century Novel,"Department of Languages, Literatures, and Cultures",2016,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1380,http://dx.doi.org/10.22148/16.011,"{'string_id': '10.22148/16.011', 'id_scheme': 'DOI'}","The distinction between fiction and non-fiction, between a text that is true and one that is not, is one of the oldest on record. Ever since we have been thinking about the act of narration, we have addressed the related meanings of truth and imagination.",Fictionality,"Department of Languages, Literatures, and Cultures",2016,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1381,http://dx.doi.org/10.22148/16.012,"{'string_id': '10.22148/16.012', 'id_scheme': 'DOI'}","Classification is a vexing problem in folkloristics. Although broad genre classifications such as “ballad”, “folktale”, “legend”, “proverb”, and “riddle” are well established and widely accepted, these formal classifications are coarse and dolittle more than provide a first level sort on materials for collections that can easily include tens, if not hundreds, of thousands of records.",The Tell-Tale Hat: Surfacing the Uncertainty in Folklore Classification,"Department of Languages, Literatures, and Cultures",2017,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1382,http://dx.doi.org/10.22148/16.014,"{'string_id': '10.22148/16.014', 'id_scheme': 'DOI'}","According to Rita Felski, context is overrated. Even in the sophisticated variants of contextualization typical of the New Historicism, she explains, scholars’ obsession with historical context as the ultimate source of textual meaning disregards the capacity of literature to resonate across time and space.","Topic Modeling, Epistemology, and the English and German Novel","Department of Languages, Literatures, and Cultures",2017,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1383,http://dx.doi.org/10.22148/16.015,"{'string_id': '10.22148/16.015', 'id_scheme': 'DOI'}","The trajectory of science fiction since World War II has been defined by its rela-tionship with technoscientific imaginaries. In the Golden Age of the 1930s and 1940s, writers like Isaac Asimov and Robert Heinlein dreamed of the robots and",Towards a Poetics of Strangeness: Experiments in Classifying Language of Technological Novelty,"Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1384,http://dx.doi.org/10.22148/16.017,"{'string_id': '10.22148/16.017', 'id_scheme': 'DOI'}",This paper reports on a collaborative project that develops new applications ofspatial text analysis. We offer a methodology to identify and evaluate correlations between semantic and geographic distance in a printed corpus.,Scotland’s Poetics of Space: An Experiment in Geospatial Semantics,"Department of Languages, Literatures, and Cultures",2017,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1385,http://dx.doi.org/10.22148/16.018,"{'string_id': '10.22148/16.018', 'id_scheme': 'DOI'}","As Renaissance conceptions of otherness have become a locus of critical study,no work has been more central for making the case about dominant ideologiesof race than Shakespeare’s Othello. Ania Loomba has suggested that ”more than any other play of the time, Othello allows us to see that skin colour, religion,and location were often contradictorily yoked together within ideologies of ‘race,’and that all these attributes were animated by notions of sexual and gender difference.”",Linked Reading: Digital Historicism and Early Modern Discourses of Race around Shakespeare’s Othello,"Department of Languages, Literatures, and Cultures",2018,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1386,http://dx.doi.org/10.22148/16.019,"{'string_id': '10.22148/16.019', 'id_scheme': 'DOI'}","This essay explores the changing significance of gender in fiction, asking especially whether its prominence in characterization has varied from the end of the eighteenth century to the beginning of the twenty-first. We have reached twoconclusions, which may seem in tension with each other. The first is that gen-der divisions between characters have become less sharply marked over the last 170 years.",The Transformation of Gender in English-Language Fiction,"Department of Languages, Literatures, and Cultures",2018,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1387,http://dx.doi.org/10.22148/16.020,"{'string_id': '10.22148/16.020', 'id_scheme': 'DOI'}","This special issue of Cultural Analyticstackles the urgent question of how so-cial identities can be addressed through computational methods. In particular, it probes the extent to which large datasets can be used to elucidate the kindsof questions that humanities scholars want to ask about historical and represen-tational processes that structure social relations and positions.",The Identity Issue: An Introduction,"Department of Languages, Literatures, and Cultures",2018,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1388,http://dx.doi.org/10.22148/16.021,"{'string_id': '10.22148/16.021', 'id_scheme': 'DOI'}","Readers are never merely passive recipients of textual messages. One of themost powerful insights of reader-response theory in the 1970s and 1980s is that the meaning of a text never resides entirely within the artifact itself. Commentators from Carlo Ginzberg (“aggressive originality”), to Jauss (“horizon of ex-pectations”), to Fish (“interpretive communities”), and Radway (“Reading is notEating”) have long-since established that readers are creators of meaning.","Crossing Over: Gendered Reading Formations at the Muncie Public Library, 1891-1902","Department of Languages, Literatures, and Cultures",2018,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1389,http://dx.doi.org/10.22148/16.022,"{'string_id': '10.22148/16.022', 'id_scheme': 'DOI'}","Literary readings provoke strong feelings, which feed intense critical de-bates. And while recorded literary readings have long been available for study, few scholars have applied to them the empirical methods that the digitalhumanities and interdisciplinary sound studies now offer.",Beyond Poet Voice: Sampling the (Non-) Performance Styles of 100 American Poets,"Department of Languages, Literatures, and Cultures",2018,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1390,http://dx.doi.org/10.22148/16.024,"{'string_id': '10.22148/16.024', 'id_scheme': 'DOI'}","Among the most pressing problems in modernist literary studies are those re-lated to Britain’s engagement with the wider world under empire and to its ownrapidly evolving urban spaces in the years before the Second World War. In both cases, the literary-geographic imagination—or unconscious—of the periodbetween 1880 and 1940 can help to shed light on how texts by British and British-aligned writers of the era understood these issues and how they evolved over time.","Nation, Ethnicity, and the Geography of British Fiction, 1880-1940","Department of Languages, Literatures, and Cultures",2018,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1391,http://dx.doi.org/10.22148/16.025,"{'string_id': '10.22148/16.025', 'id_scheme': 'DOI'}",Digital libraries today distribute their contents in a way that limits the sort ofwork that can be done with them. Modern libraries are so large-often containingmillions of books or articles-that the technical resources needed to work withthem can be immense.,"Stable random projection: lightweight, general-purpose dimensionality reduction for digitized libraries","Department of Languages, Literatures, and Cultures",2018,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1392,http://dx.doi.org/10.22148/16.027,"{'string_id': '10.22148/16.027', 'id_scheme': 'DOI'}","One may as well begin with two overly-cited quotations about novelty. Thefirst: “On or about December, 1910, human character changed.” And, morepredictably: “Make It New!” That these two statements about change and innovation have been so frequentlycited reveals a contradiction in modernist studies: novelty is commonplace,both for modernist writers and for literary critics.",Measuring Modernist Novelty,"Department of Languages, Literatures, and Cultures",2018,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1393,http://dx.doi.org/10.22148/16.028,"{'string_id': '10.22148/16.028', 'id_scheme': 'DOI'}","On its release in 2004, theOxford Dictionary of National Biographywas called“the greatest book ever” and “a more enthralling read than all the novels everentered for the Booker Prize put together.” The tabloidThe Daily Mail, wherethese giddy pronouncements appeared, is not known for understatement, butmore cautious academic researchers have long held theODNBin similarly highesteem.",Historiography’s Two Voices: Data Infrastructure and History at Scale in the Oxford Dictionary of National Biography (ODNB),"Department of Languages, Literatures, and Cultures",2018,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1394,http://dx.doi.org/10.22148/16.030,"{'string_id': '10.22148/16.030', 'id_scheme': 'DOI'}","Whether they work with pages hand-selected from an archive, or keywords hand-selected from a digital corpus, scholars of all kinds wrestle with the issue of exemplarity.",Critical Search: A Procedure for Guided Reading in Large-Scale Textual Corpora,"Department of Languages, Literatures, and Cultures",2018,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1395,http://dx.doi.org/10.22148/16.031,"{'string_id': '10.22148/16.031', 'id_scheme': 'DOI'}",This article seeks to bridge two scholarly fields often seen as incommensurable:cultural analytics (also known as “computational criticism”) and critical racestudies. It does so by discovering generative points of contact between two sets of methods that are also typically viewed as antithetical: data science and critique.Cultural analytics is an emerging field wherein humanist scholars leverage the in-creasing availability of large digital corpora and the affordances of new computa-tional tools.,"Race, Writing, and Computation: Racial Difference and the US Novel, 1880-2000","Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1396,http://dx.doi.org/10.22148/16.032,"{'string_id': '10.22148/16.032', 'id_scheme': 'DOI'}","Across a variety of cultural fields, researchers have identified a near ubiquitous underrepresentation and decentralization of women. This occurs both at the level of who is able to produce cultural works and who is depicted within them. Women are less likely to be directors of Hollywood films and also less likely to have starring roles.",Social Characters: The Hierarchy of Gender in Contemporary English-Language Fiction,"Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1397,http://dx.doi.org/10.22148/16.033,"{'string_id': '10.22148/16.033', 'id_scheme': 'DOI'}","Literary scholars in the information age, whatever issues we may differ on, have long shared a belief that the amount of “information” in literary language is not a phenomenon that can be empirically investigated. We tell our students, and remind each other, that the meaning in literary texts is inherently subjective, culture-specific, and contextual rather than fixed.",Could Fiction Have an Information History? Statistical Probability and the Rise of the Novel,"Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1398,http://dx.doi.org/10.22148/16.035,"{'string_id': '10.22148/16.035', 'id_scheme': 'DOI'}","Data have become pervasive in research in the humanities and the social sci ences. New areas, objects, and situations for study have developed; and new methods for working with data are shepherded by new epistemologies and (po tential) paradigm shifts. But data didn’t just happen to us. We have happened to data.","Data Cultures, Culture as Data","Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1399,http://dx.doi.org/10.22148/16.036,"{'string_id': '10.22148/16.036', 'id_scheme': 'DOI'}","A growing list of high-profile controversies involving the social impacts of ar tificial intelligence systems (AI), digital data collection and algorithmic analy sis have forced difficult conversations around the ethics of data-intensive digi tal technologies and so-called “big data” research.These incidents are directly relevant to newly coalescing cultures of “data science,” an emergent field which seeks both to interpret and capitalize on the creation, collection, and processing of knowledge through large collections of digital data, often in conjunction with particular techniques like machine learning (ML).",Data Is the New What? Popular Metaphors & Professional Ethics in Emerging Data Culture,"Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1400,http://dx.doi.org/10.22148/16.037,"{'string_id': '10.22148/16.037', 'id_scheme': 'DOI'}","Big Data represents a tremendous opportunity to drill down and tap into these critical insights. In fact, the powerful potential to mine and refine this vital, valuable resource points to a direct comparison to a similarly vital resource in the modern economy: crude oil.",De-Agentializing Data Practices: The Shifting Power of Metaphor in 1990s Discourses on Data Mining,"Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1401,http://dx.doi.org/10.22148/16.038,"{'string_id': '10.22148/16.038', 'id_scheme': 'DOI'}","On February 17th, 2017, Mark Zuckerberg published a 5,800-word Facebook post rescripting the company’s Corporate Social Responsibility strategy and defining its future directions. The manifesto, as some commentators referred to it, declared Facebook’s future vision for “developing the social infrastructure for community” and emphasized the company’s focus on fostering a global community that is supportive, safe, informed, civically-engaged, and inclusive.",Towards A Queer Futurity of Data,"Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1402,http://dx.doi.org/10.22148/16.039,"{'string_id': '10.22148/16.039', 'id_scheme': 'DOI'}","Over the past three decades, scholarship in sociology, criminology, law, public policy, history, science and technology studies (STS), and other allied fields has addressed various areas of concern at the intersections of race and correctional supervision in the United States.","The Carceral Archive: Documentary Records, Narrative Construction, and Predictive Risk Assessment","Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1403,http://dx.doi.org/10.22148/16.040,"{'string_id': '10.22148/16.040', 'id_scheme': 'DOI'}","At Ethics and Archiving the Web, a conference convened in March 2018 at the New Museum in New York City, a group of artists, archivists, activists and re searchers met to critically examine the ethical implications of our ability to col lect and archive content from the web.",Archival Circulation on the Web: The Vine-Tweets Dataset,"Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1404,http://dx.doi.org/10.22148/16.041,"{'string_id': '10.22148/16.041', 'id_scheme': 'DOI'}","In 2016, a software developer named David and I met to discuss creating a quan titative demographic survey of the open source software community to which we were both long-time contributors. David and I did not know each other well, but shared a belief that our open source community (OSC, hereafter) was an un safe place for anyone who did not identify as white, cisgendered, heterosexual and male. That lack of safety was further complicated by any one individual’s distance from privileged modes of contribution.",Data Set Failures and Intersectional Data,"Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1405,http://dx.doi.org/10.22148/16.043,"{'string_id': '10.22148/16.043', 'id_scheme': 'DOI'}","Extensive scholarship in media studies has established how formal elements of moving images—such as camera angles, sound, and framing—reflect, establish, and challenge cultural norms. Prior computational analyses attempting to ana lyze these elements have primarily relied on summarizing relatively low-level fea tures.",Visual Style in Two Network Era Sitcoms,"Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1406,http://dx.doi.org/10.22148/16.045,"{'string_id': '10.22148/16.045', 'id_scheme': 'DOI'}","Most of the current academic literature on open data looks outward at thedata’s reuse by the public. This article describes, rather, the cultural practice ofopen data inside city governments.",Performative Data: Cultures of Government Data Practice,"Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1407,http://dx.doi.org/10.22148/16.046,"{'string_id': '10.22148/16.046', 'id_scheme': 'DOI'}","Literary history is no longer written in books alone. As literary reception thrivesin blogs, Wikipedia entries, Amazon reviews, and Goodreads profiles, the Webhas become a key platform for the exchange of information on literature.",The Canon of Dutch Literature According to Google,"Department of Languages, Literatures, and Cultures",2019,,Journal of Cultural Analytics,,,"[{'value': '2371-4549', 'type': 'electronic'}]"
1408,http://dx.doi.org/10.2218/ijdc.v10i1.289,"{'string_id': '10.2218/ijdc.v10i1.289', 'id_scheme': 'DOI'}","In order to better understand the factors that most influence where researchers deposit their data when they have a choice, we collected survey data from researchers who deposited phylogenetic data in either the TreeBASE or Dryad data repositories. Respondents were asked to rank the relative importance of eight possible factors. We found that factors differed in importance for both TreeBASE and Dryad, and that the rankings differed subtly but significantly between TreeBASE and Dryad users. On average, TreeBASE users ranked the domain specialization of the repository highest, while Dryad users ranked as equal highest their trust in the persistence of the repository and the ease of its data submission process. Interestingly, respondents (particularly Dryad users) were strongly divided as to whether being directed to choose a particular repository by a journal policy or funding agency was among the most or least important factors. Some users reported depositing their data in multiple repositories and archiving their data voluntarily.",What Factors Influence Where Researchers Deposit their Data? A Survey of Researchers Submitting to Data Repositories,Edinburgh University,2015,,International Journal of Digital Curation,10,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1409,http://dx.doi.org/10.2218/ijdc.v10i1.324,"{'string_id': '10.2218/ijdc.v10i1.324', 'id_scheme': 'DOI'}","This paper aims to give state-of-the-art information about digital preservation activities in the Czech Republic during the last decade to an English-speaking audience. We briefly describe major phases of the “digital” projects. These were mainly in libraries, with some references to museums, galleries and archives. We focus on aspects related to the preservation of collected born-digital and digitised content. Even now, digital preservation activities in heritage institutions are often on the periphery of the interest of all stakeholders and the infrastructure supporting digital preservation of data in heritage institutions is not well financed or coordinated. Even though the “long decade”, which lasted from the dramatic events of 2002 until approximately 2014, saw a number of successful projects creating digital data in Czech libraries, the handful of projects which were in part focused on digital preservation were not flexible enough to accommodate user requirements and were failing to meet expectations. There is still much room for further development in the area of long-term preservation of digital data in the Czech Republic. This article is a shortened version of one of the analyses written under the “Strategy of the research, development and innovation for the years 2010–2015” program of the Moravian Library in Brno, Czech Republic. ",The long decade of digital preservation in heritage institutions in the Czech Republic: 2002–2014,Edinburgh University,2015,,International Journal of Digital Curation,10,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1410,http://dx.doi.org/10.2218/ijdc.v10i1.328,"{'string_id': '10.2218/ijdc.v10i1.328', 'id_scheme': 'DOI'}"," This article considers digital curation in doctoral study and the role of the doctoral supervisor and institution in facilitating students’ acquisition of digital curation skills, including some of the potentially problematic expectations of the supervisory relationship with regards to digital curation. Research took the form of an analysis of the current digital curation training landscape, focussing on doctoral study and supervision. This was followed by a survey (n=116) investigating attitudes towards importance, expertise, and responsibilities regarding digital curation. This research confirms that digital curation is considered to be very important within doctoral study but that doctoral supervisors and particularly students consider themselves to be largely unskilled at curation tasks. It provides a detailed picture of curation activity within doctoral study and identifies the areas of most concern. A detailed analysis demonstrates that most of the responsibility for curation is thought to lie with students and that institutions are perceived to have very low responsibility and that individuals tend to over-assign responsibility to themselves. Finally, the research identifies which types of support system for curation are most used and makes suggestions for ways in which students, supervisors, institutions, and others can effectively and efficiently address problematic areas and improve digital curation within doctoral study. ",Digital Curation and Doctoral Research,Edinburgh University,2015,,International Journal of Digital Curation,10,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1411,http://dx.doi.org/10.2218/ijdc.v10i1.343,"{'string_id': '10.2218/ijdc.v10i1.343', 'id_scheme': 'DOI'}","User-defined metadata is useful for curating and helping to provide context for experiment records, but our previous investigations have demonstrated that simply providing the facility to add metadata is not enough to ensure that metadata is added, let alone to ensure that the metadata is of high quality. For metadata to be useful it first has to be present, but enforcing metadata generation is of no benefit if it is low quality, inconsistent, or irrelevant. Researchers need support. One strategy to encourage more effective metadata creation is to design user interfaces that invite users to add metadata by asking them questions. If we ask users specific questions about their experiments and other activities then we could capture more relevant or useful metadata, although there is a risk that asking the wrong questions may lead to loss of valuable metadata terms or the creation of irrelevant material. In this paper we report on a study to investigate how different questions could be used to generate metadata by eliciting information in three different conditions: free recall, changing perspective by thinking about search terms to help someone else, and providing cues by using a set of topic-based questions. We also investigate how responses varied with different information types. The results of the study show that different terms are created under the different conditions, as expected. The use of cues generates the highest numbers of terms and the most diverse range, including elements that are not captured in other conditions. However, important themes generated in other conditions are not produced because the cues to create them are missing. The study also generated a number of unexpected findings, including responses describing information that is not in the original material: personal opinions and experiences, and comments about the information text itself. These unexpected responses have both positive and negative consequences for the generation of metadata and the curation of scientific records. The results of studies using these techniques to capture metadata for chemistry experiments are also discussed. ",User-Defined Metadata: Using Cues and Changing Perspectives,Edinburgh University,2015,,International Journal of Digital Curation,10,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1412,http://dx.doi.org/10.2218/ijdc.v10i1.344,"{'string_id': '10.2218/ijdc.v10i1.344', 'id_scheme': 'DOI'}","The Data Curation Profiles Toolkit (DCPT) emerged out of a Purdue University Libraries’ 2004 initiative to engage in multidisciplinary research. It is a tool developed to assist librarians and other information professionals to conduct data interviews and identify the needs of researchers when managing, sharing, or curating their data. The DCPT has been widely adopted and applied in various contexts but its usability as a tool has not been formally assessed. To address this need, we have conducted a survey of users of the DCPT. The survey included quantitative measures of potential influencing factors of using the DCPT and its perceived usability (its usefulness as a tool and its ease of use). Open-ended questions about users’ experiences with the DCPT were also included to better understand the strengths and weaknesses of the tool, as well as areas that could be improved. Factor analysis of the quantitative results and subsequent regression models revealed several underlying factors that affect the perceived usability of the DCPT. Responses to the open-ended questions revealed several themes of users’ concerns: the amount of time required to use the DCPT, the structure and format of the DCPT, alignment of the DCPT with particular contexts, and the use of the DCPT to engage faculty and the library community. By correlating themes identified from the open-ended questions with the analysis of quantitative data, this paper provides the first empirical assessment of the DCPT that could help further improve the toolkit’s usability based on user needs and expectations. The methodology used in the study could readily be applied to assess and improve the utility of other tools used by data and information professional. ",Assessing Perceived Usability of the Data Curation Profile Toolkit Using the Technology Acceptance Model,Edinburgh University,2015,,International Journal of Digital Curation,10,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1413,http://dx.doi.org/10.2218/ijdc.v10i1.347,"{'string_id': '10.2218/ijdc.v10i1.347', 'id_scheme': 'DOI'}","Understanding the methods and processes implemented by data producers to generate research data is essential for fostering data reuse. Yet, producing the metadata that describes these methods remains a time-intensive activity that data producers do not readily undertake. In particular, researchers in the long tail of science often lack the financial support or tools for metadata generation, thereby limiting future access and reuse of data produced. The present study investigates research journal publications as a potential source for identifying descriptive metadata about methods for research data. Initial results indicate that journal articles provide rich descriptive content that can be sufficiently mapped to existing metadata standards with methods-related elements, resulting in a mapping of the data production process for a study. This research has implications for enhancing the generation of robust metadata to support the curation of research data for new inquiry and innovation. ",Mapping Methods Metadata for Research Data,Edinburgh University,2015,,International Journal of Digital Curation,10,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1414,http://dx.doi.org/10.2218/ijdc.v10i1.360,"{'string_id': '10.2218/ijdc.v10i1.360', 'id_scheme': 'DOI'}","The notion of a “designated community” has always been a rather elusive concept across the digital curation landscape. This paper is an effort to revisit the concept to stir up new discussions in the area. More specifically, this study offers a perspective on designated communities through the lens of the web, powered by developments in the last ten years in social media. The research presents a multi-faceted analysis of communities based on HTML content from online web pages to propose heuristics for defining designated communities based on the technology they adopt, properties of knowledge organisation, and how they link to each other. This impacts the building of quantifiable models of designated communities, estimating curation risks associated to the community and further, refining approaches to preservation strategies that meet the needs of the community.",“Designated Communities”: Through the Lens of the Web,Edinburgh University,2015,,International Journal of Digital Curation,10,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1415,http://dx.doi.org/10.2218/ijdc.v10i1.361,"{'string_id': '10.2218/ijdc.v10i1.361', 'id_scheme': 'DOI'}","In this paper we consider the requirements for preserving the memory of science.  This is becoming more challenging as data volumes and rates continue to increase.  Further, to capture a full picture of the scientific memory we need to move beyond the bit preservation challenge to consider how to capture research in context, represent the meaning of the data, and how to interpret data in relation to other scientific artefacts distributed in multiple information spaces. We review the progress of scientific research into the digital preservation of science over the last decade, emphasising in particular the research and development programme of STFC. We conclude with a number of observations into the future directions of research and also the practical deployment of policy and infrastructure to effectively preserve the scientific memory.",Towards the Preservation of the Scientific Memory,Edinburgh University,2015,,International Journal of Digital Curation,10,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1416,http://dx.doi.org/10.2218/ijdc.v10i2.321,"{'string_id': '10.2218/ijdc.v10i2.321', 'id_scheme': 'DOI'}","A study of 56 professors at five American universities found that a majority had little understanding of principles, well-known in the field of data curation, informing the ongoing administration of digital materials and chose to manage and store work-related data by relying on the use of their own storage devices and cloud accounts. It also found that a majority of them had experienced the loss of at least one work-related digital object that they considered to be important in the course of their professional career. Despite such a rate of loss, a majority of respondents expressed at least a moderate level of confidence that they would be able to make use of their digital objects in 25 years. The data suggest that many faculty members are unaware that their data is at risk. They also indicate a strong correlation between faculty members’ digital object loss and their data management practices. University professors producing digital objects can help themselves by becoming aware that these materials are subject to loss. They can also benefit from awareness and use of better personal data management practices, as well as participation in university-level programmatic digital curation efforts and the availability of more readily accessible, robust infrastructure for the storage of digital materials. ",Intellectual Capital at Risk: Data Management Practices and Data Loss by Faculty Members at Five American Universities,Edinburgh University,2015,,International Journal of Digital Curation,10,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1417,http://dx.doi.org/10.2218/ijdc.v10i2.329,"{'string_id': '10.2218/ijdc.v10i2.329', 'id_scheme': 'DOI'}","There is increasing pressure from funders, publishers, the public, universities and other research organisations for researchers to improve their data management and sharing practices. However, little is known about researchers’ data management and sharing practices and concerns. The research reported in this paper seeks to address this by providing insight into the research data management and sharing practices of academics at ten universities in New South Wales, Australia. Empirical data was taken from a survey to which 760 academics responded, with 634 completing at least one section. Results showed that at the time of the survey there were a wide variety of research data in use, including analogue data, and that the challenges researchers faced in managing their data included finding safe and secure storage, particularly after project completion, but also during projects when data are used (and thus stored) on a wide variety of less-than-optimal temporary devices. Data sharing was not widely practiced and only a relatively small proportion of researchers had a research data management plan. Since the survey was completed much has changed: capacities and communities are being built around data management and sharing and policies, and guidelines are being constructed. Data storage and curation services are now more freely available. It will be interesting to observe how the findings of future studies compare with those reported here.",Research Data Management Practices: A Snapshot in Time,Edinburgh University,2015,,International Journal of Digital Curation,10,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1418,http://dx.doi.org/10.2218/ijdc.v10i2.342,"{'string_id': '10.2218/ijdc.v10i2.342', 'id_scheme': 'DOI'}","As science becomes more data-intensive and collaborative, researchers increasingly use larger and more complex data to answer research questions. The capacity of storage infrastructure, the increased sophistication and deployment of sensors, the ubiquitous availability of computer clusters, the development of new analysis techniques, and larger collaborations allow researchers to address grand societal challenges in a way that is unprecedented. In parallel, research data repositories have been built to host research data in response to the requirements of sponsors that research data be publicly available. Libraries are re-inventing themselves to respond to a growing demand to manage, store, curate and preserve the data produced in the course of publicly funded research. As librarians and data managers are developing the tools and knowledge they need to meet these new expectations, they inevitably encounter conversations around Big Data. This paper explores definitions of Big Data that have coalesced in the last decade around four commonly mentioned characteristics: volume, variety, velocity, and veracity. We highlight the issues associated with each characteristic, particularly their impact on data management and curation. We use the methodological framework of the data life cycle model, assessing two models developed in the context of Big Data projects and find them lacking. We propose a Big Data life cycle model that includes activities focused on Big Data and more closely integrates curation with the research life cycle. These activities include planning, acquiring, preparing, analyzing, preserving, and discovering, with describing the data and assuring quality being an integral part of each activity. We discuss the relationship between institutional data curation repositories and new long-term data resources associated with high performance computing centers, and reproducibility in computational science. We apply this model by mapping the four characteristics of Big Data outlined above to each of the activities in the model. This mapping produces a set of questions that practitioners should be asking in a Big Data project ",Revisiting the Data Lifecycle with Big Data Curation,Edinburgh University,2016,,International Journal of Digital Curation,10,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1419,http://dx.doi.org/10.2218/ijdc.v10i2.346,"{'string_id': '10.2218/ijdc.v10i2.346', 'id_scheme': 'DOI'}"," Digital data are accumulating rapidly, yet issues relating to data production remain unexamined. Data sharing efforts in particular are nascent, disunited and incomplete. We investigate the development of data products tailored for diverse communities with differing knowledge bases. We explore not the technical aspects of how, why, or where data are made available, but rather the socio-scientific aspects influencing what data products are created and made available for use. These products differ from compact data summaries often published in journals. We report on development by a national data center of two data collections describing the changing polar environment. One collection characterizes sea ice products derived from satellite remote sensing data and development unfolds over three decades. The second collection characterizes the Greenland Ice Sheet melt where development of an initial collection of data products over a period of several months was informed by insights gained from earlier experience. In documenting the generation of these two collections, a data product development cycle supported by a data product team is identified as key to mobilizing scientific knowledge. The collections reveal a co-evolution of data products and designated communities where community interest may be triggered by events such as environmental disturbance and new modes of communication. These examples of data product development in practice illustrate knowledge mobilization in the earth sciences; the collections create a bridge between data producers and a growing number of audiences interested in making evidence-based decisions.   ",Scientific Knowledge Mobilization: Co-evolution of Data Products and Designated Communities,Edinburgh University,2016,,International Journal of Digital Curation,10,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1420,http://dx.doi.org/10.2218/ijdc.v10i2.359,"{'string_id': '10.2218/ijdc.v10i2.359', 'id_scheme': 'DOI'}","This paper presents the results of a research data assessment and landscape study in the institutional context of Virginia Tech to determine the data sharing and reuse practices of academic faculty researchers. Through mapping the level of user engagement in “openness of data,” “openness of methodologies and workflows,” and “reuse of existing data,” this study contributes to the current knowledge in data sharing and open access, and supports the strategic development of institutional data stewardship. Asking faculty researchers to self-reflect sharing and reuse from both data producers’ and data users’ perspectives, the study reveals a significant gap between the rather limited sharing activities and the highly perceived reuse or repurpose values regarding data, indicating that potential values of data for future research are lost right after the original work is done. The localized and sporadic data management and documentation practices of researchers also contribute to the obstacles they themselves often encounter when reusing existing data.   ",Research Data Sharing and Reuse Practices of Academic Faculty Researchers: A Study of the Virginia Tech Data Landscape,Edinburgh University,2016,,International Journal of Digital Curation,10,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1421,http://dx.doi.org/10.2218/ijdc.v10i2.375,"{'string_id': '10.2218/ijdc.v10i2.375', 'id_scheme': 'DOI'}","This article reports the results of a study examining the state of data guidance provided to authors by 50 oncology journals. The purpose of the study was the identification of data practices addressed in the journals’ policies. While a number of studies have examined data sharing practices among researchers, little is known about how journals address data sharing. Thus, what was discovered through this study has practical implications for journal publishers, editors, and researchers. The findings indicate that journal publishers should provide more meaningful and comprehensive data guidance to prospective authors. More specifically, journal policies requiring data sharing, should direct researchers to relevant data repositories, and offer better metadata consultation to strengthen existing journal policies. By providing adequate guidance for authors, and helping investigators to meet data sharing mandates, scholarly journal publishers can play a vital role in advancing access to research data. ",State of Data Guidance in Journal Policies: A Case Study in Oncology,Edinburgh University,2016,,International Journal of Digital Curation,10,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1422,http://dx.doi.org/10.2218/ijdc.v11i1.339,"{'string_id': '10.2218/ijdc.v11i1.339', 'id_scheme': 'DOI'}"," This article extends previous work known as Preserving Virtual Worlds II (PVWII), funded through a grant from the Institute of Museum and Library Services. The author draws on interview data collected from video game developers, content analysis of several long-running video game series, as well as the project’s advisory board and researcher reports. This paper exposes two fundamental challenges in creating metrics and specifications for the preservation of virtual worlds; namely, that there is no one type of user or designated video game stakeholder community, and that significant properties of games cannot always be located in code or platform. The PVWII data serve to explain why existing ideas about preservation of video games are inadequate when games are treated as digital cultural heritage. Preservation specialists need to bind nebulous and dynamic digital objects, a process that is necessary while inherently artificial.     ",Enrolling Heterogeneous Partners in Video Game Preservation,Edinburgh University,2016,,International Journal of Digital Curation,11,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1423,http://dx.doi.org/10.2218/ijdc.v11i1.357,"{'string_id': '10.2218/ijdc.v11i1.357', 'id_scheme': 'DOI'}","As scientific data volumes, format types, and sources increase rapidly with the invention and improvement of scientific capabilities, the resulting datasets are becoming more complex to manage as well. One of the significant management challenges is pulling apart the individual contributions of specific people and organizations within large, complex projects. This is important for two aspects: 1) assigning responsibility and accountability for scientific work, and 2) giving professional credit to individuals (e.g. hiring, promotion, and tenure) who work within such large projects. This paper aims to review the extant practice of data attribution and how it may be improved. Through a case study of creating a detailed attribution record for a climate model dataset, the paper evaluates the strengths and weaknesses of the current data attribution method and proposes an alternative attribution framework accordingly. The paper concludes by demonstrating that, analogous to acknowledging the different roles and responsibilities shown in movie credits, the methodology developed in the study could be used in general to identify and map out the relationships among the organizations and individuals who had contributed to a dataset.  As a result, the framework could be applied to create data attribution for other dataset types beyond climate model datasets.     ",Recognizing the Diversity of Contributions: A Case Study for Framing Attribution and Acknowledgement for Scientific Data,Edinburgh University,2016,,International Journal of Digital Curation,11,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1424,http://dx.doi.org/10.2218/ijdc.v11i1.387,"{'string_id': '10.2218/ijdc.v11i1.387', 'id_scheme': 'DOI'}","Designated communities are central to validation of preservation. If a designated community is able to understand and use information found within a digital repository, the assumption is that the information has been properly preserved. As judging the trustworthiness of information requires at least some level of understanding of that information, this paper presents results of a study aimed at developing a tool for measuring designated community members’ perceptions of trustworthiness for preserved information found within a digital repository. The study focuses on genealogists at the Washington State Digital Archives who routinely interact with digitized genealogical records, including digitized marriage, death, and birth records. Results of the study include construction of an original Digitized Archival Document Trustworthiness Scale (DADTS). DADTS is a ready-made tool for digital curators to use to measure the trustworthiness perceptions of their designated community members. Implications of this study include the feasibility of engaging members of a designated community in the construction of a scale for measuring trustworthiness perception, thereby providing deeper insight into the understandability and usability of preserved information by that designated community.   ",The Digitized Archival Document Trustworthiness Scale,Edinburgh University,2016,,International Journal of Digital Curation,11,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1425,http://dx.doi.org/10.2218/ijdc.v11i1.389,"{'string_id': '10.2218/ijdc.v11i1.389', 'id_scheme': 'DOI'}","In order to better understand the current state of data management education in multiple fields of science, this study surveyed scientists, including information scientists, about their data management education practices, including at what levels they are teaching data management, which topics they covering, and what barriers they experience in teaching these topics. We found that a handful of scientists are teaching data management in undergraduate, graduate, and other types of courses, as well as outside of classroom settings. Commonly taught data management topics included quality control, protecting data, and management planning. However, few instructors felt they were covering data management topics thoroughly, and respondents cited barriers such as lack of time, lack of necessary expertise, and lack of information for teaching data management. We offer some potential explanations for the existing state of data management education and suggest areas for further research. ",Data Management Education from the Perspective of Science Educators,Edinburgh University,2016,,International Journal of Digital Curation,11,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1426,http://dx.doi.org/10.2218/ijdc.v11i1.401,"{'string_id': '10.2218/ijdc.v11i1.401', 'id_scheme': 'DOI'}","The development of e-Research infrastructure has enabled data to be shared and accessed more openly. Policy mandates for data sharing have contributed to the increasing availability of research data through data repositories, which create favourable conditions for the re-use of data for purposes not always anticipated by original collectors. Despite the current efforts to promote transparency and reproducibility in science, data re-use cannot be assumed, nor merely considered a ‘thrifting’ activity where scientists shop around in data repositories considering only the ease of access to data. The lack of an integrated view of individual, social and technological influential factors to intentional and actual data re-use behaviour was the key motivator for this study. Interviews with 13 social scientists produced 25 factors that were found to influence their perceptions and experiences, including both their unsuccessful and successful attempts to re-use data. These factors were grouped into six theoretical variables: perceived benefits, perceived risks, perceived effort, social influence, facilitating conditions, and perceived re-usability. These research findings provide an in-depth understanding about the re-use of research data in the context of open science, which can be valuable in terms of theory and practice to help leverage data re-use and make publicly available data more actionable.   ",Factors Influencing Research Data Reuse in the Social Sciences: An Exploratory Study,Edinburgh University,2016,,International Journal of Digital Curation,11,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1427,http://dx.doi.org/10.2218/ijdc.v11i1.425,"{'string_id': '10.2218/ijdc.v11i1.425', 'id_scheme': 'DOI'}","The ability to measure the use and impact of published data sets is key to the success of the open data/open science paradigm. A direct measure of impact would require tracking data (re)use in the wild, which is difficult to achieve. This is therefore commonly replaced by simpler metrics based on data download and citation counts. In this paper we describe a scenario where it is possible to track the trajectory of a dataset after its publication, and show how this enables the design of accurate models for ascribing credit to data originators. A Data Trajectory (DT) is a graph that encodes knowledge of how, by whom, and in which context data has been re-used, possibly after several generations. We provide a theoretical model of DTs that is grounded in the W3C PROV data model for provenance, and we show how DTs can be used to automatically propagate a fraction of the credit associated with transitively derived datasets, back to original data contributors. We also show this model of transitive credit in action by means of a Data Reuse Simulator. In the longer term, our ultimate hope is that credit models based on direct measures of data reuse will provide further incentives to data publication. We conclude by outlining a research agenda to address the hard questions of creating, collecting, and using DTs systematically across a large number of data reuse instances in the wild. ",Data trajectories: tracking reuse of published data for transitive credit attribution,Edinburgh University,2016,,International Journal of Digital Curation,11,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1428,http://dx.doi.org/10.2218/ijdc.v11i1.428,"{'string_id': '10.2218/ijdc.v11i1.428', 'id_scheme': 'DOI'}","Scientists in all fields face challenges in managing and sustaining access to their research data. The larger and longer term the research project, the more likely that scientists are to have resources and dedicated staff to manage their technology and data, leaving those scientists whose work is based on smaller and shorter term projects at a disadvantage. The volume and variety of data to be managed varies by many factors, only two of which are the number of collaborators and length of the project. As part of an NSF project to conceptualize the Institute for Empowering Long Tail Research, we explored opportunities offered by Software as a Service (SaaS). These cloud-based services are popular in business because they reduce costs and labor for technology management, and are gaining ground in scientific environments for similar reasons. We studied three settings where scientists conduct research in small and medium-sized laboratories. Two were NSF Science and Technology Centers (CENS and C-DEBI) and the third was a workshop of natural reserve scientists and managers. These laboratories have highly diverse data and practices, make minimal use of standards for data or metadata, and lack resources for data management or sustaining access to their data, despite recognizing the need. We found that SaaS could address technical needs for basic document creation, analysis, and storage, but did not support the diverse and rapidly changing needs for sophisticated domain-specific tools and services. These are much more challenging knowledge infrastructure requirements that require long-term investments by multiple stakeholders.   ","Data Management in the Long Tail: Science, Software, and Service",Edinburgh University,2016,,International Journal of Digital Curation,11,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1429,http://dx.doi.org/10.2218/ijdc.v11i2.390,"{'string_id': '10.2218/ijdc.v11i2.390', 'id_scheme': 'DOI'}","Software plays a significant role in modern academic research, yet lacks a similarly significant presence in the scholarly record. With increasing interest in promoting reproducible research, curating software as a scholarly resource not only promotes access to these tools, but also provides recognition for the intellectual efforts that go into their development. This work reviews existing standards for identifying, promoting discovery of, and providing credit for software development work. In addition, it shows how these guidelines have been integrated into existing tools and community cultures, and provides recommendations for future software curation efforts.   ","Citations for Software: Providing Identification, Access and Recognition for Research Software",Edinburgh University,2017,,International Journal of Digital Curation,11,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1430,http://dx.doi.org/10.2218/ijdc.v12i1.481,"{'string_id': '10.2218/ijdc.v12i1.481', 'id_scheme': 'DOI'}","The Data Seal of Approval (DSA) is one of the most widely used standards for Trusted Digital Repositories to date. Those who developed this standard have articulated seven main benefits of acquiring DSAs: 1) stakeholder confidence, 2) improvements in communication, 3) improvement in processes, 4) transparency, 5) differentiation from others, 6) awareness raising about digital preservation, and 7) less labor- and time-intensive. Little research has focused on if and how those who have acquired DSAs actually perceive these benefits. Consequently, this study examines the benefits of acquiring DSAs from the point of view of those who have them. In a series of 15 semi-structured interviews with representatives from 16 different organizations, participants described the benefits of having DSAs in their own words. Our findings suggest that participants experience all of the seven benefits that those who developed the standard promised. Additionally, our findings reflect the greater importance of some of those benefits compared to others. For example, participants mentioned the benefits of stakeholder confidence, transparency, improvement in processes and awareness raising about digital preservation more frequently than they discussed less labor- and time-intensive (e.g. it being less labor- and time-intensive to acquire DSAs than becoming certified by other standards), improvements in communication, and differentiation from others. Participants also mentioned two additional benefits of acquiring DSAs that are not explicitly listed on the DSA website that were very important to them: 1) the impact of acquiring the DSA on documentation of their workflows, and 2) assurance that they were following best practice. Implications and future directions for research are discussed. ",The Perceived Value of Acquiring Data Seals of Approval,Edinburgh University,2017,,International Journal of Digital Curation,12,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1431,http://dx.doi.org/10.2218/ijdc.v12i1.495,"{'string_id': '10.2218/ijdc.v12i1.495', 'id_scheme': 'DOI'}","In the era of data science, datasets are shared widely and used for many purposes unforeseen by the original creators of the data.  In   this context, defects in datasets can have far reaching consequences,  spreading from dataset to dataset, and affecting the consumers of  data in ways that are hard to predict or quantify.  Some form of waste   is often the result.   For example,  scientists using defective data to propose hypotheses for experimentation may waste their limited wet lab resources chasing the wrong experimental targets.  Scarce drug trial resources may be used to test drugs that actually have little chance of giving a cure.   Because of the potential real world costs, database owners care about providing high quality data. Automated curation tools can be used to an extent to discover and correct some forms of defect. However, in some areas human curation, performed by highly-trained domain experts, is needed to ensure that the data represents our current interpretation of reality accurately. Human curators are expensive, and there is far more curation work to be done than there are curators available to perform it. Tools and techniques are needed to enable the full value to be obtained from the curation effort currently available.  In this paper,we explore one possible approach to maximising the  value obtained from human curators, by automatically extracting information about data defects and corrections from the work that the curators do. This information is packaged in a source independent form, to allow it to be used by the owners of other databases (for which human curation effort is not available or is insufficient).  This amplifies the efforts of the human curators, allowing their work to be applied to other sources, without requiring any additional effort or  change in their processes or tool sets. We show that this approach can discover significant numbers of defects, which can also be found in other sources. ",Amplifying Data Curation Efforts to Improve the Quality of Life Science Data,Edinburgh University,2017,,International Journal of Digital Curation,12,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1432,http://dx.doi.org/10.2218/ijdc.v12i1.530,"{'string_id': '10.2218/ijdc.v12i1.530', 'id_scheme': 'DOI'}","This paper describes a preliminary study of research transparency, which draws on the findings from four focus group sessions with faculty in chemistry, law, urban and social studies, and civil and environmental engineering. The multi-faceted nature of transparency is highlighted by the broad ways in which the faculty conceptualised the concept (data sharing, ethics, replicability) and the vocabulary they used with common core terms identified (data, methods, full disclosure). The associated concepts of reproducibility and trust are noted. The research lifecycle stages are used as a foundation to identify the action verbs and software tools associated with transparency. A range of transparency drivers and motivations are listed. The role of libraries and data scientists is discussed in the context of the provision of transparency services for researchers. ","Research Transparency: A Preliminary Study of Disciplinary Conceptualisation, Drivers, Tools and Support Services",Edinburgh University,2017,,International Journal of Digital Curation,12,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1433,http://dx.doi.org/10.2218/ijdc.v12i1.532,"{'string_id': '10.2218/ijdc.v12i1.532', 'id_scheme': 'DOI'}","Workflows processing data from research activities and driving in silico experiments are becoming an increasingly important method for conducting scientific research. Workflows have the advantage that not only can they be automated and used to process data repeatedly, but they can also be reused – in part or whole – enabling them to be evolved for use in new experiments. A number of studies have investigated strategies for storing and sharing workflows for the benefit of reuse. These have revealed that simply storing workflows in repositories without additional context does not enable workflows to be successfully reused. These studies have investigated what additional resources are needed to facilitate users of workflows and in particular to add provenance traces and to make workflows and their resources machine-readable. These additions also include adding metadata for curation, annotations for comprehension, and including data sets to provide additional context to the workflow. Ultimately though, these mechanisms still rely on researchers having access to the software to view and run the workflows. We argue that there are situations where researchers may want to understand a workflow that goes beyond what provenance traces provide and without having to run the workflow directly; there are many situations in which it can be difficult or impossible to run the original workflow. To that end, we have investigated the creation of an interactive workflow visualization that captures the flow chart element of the workflow with additional context including annotations, descriptions, parameters, metadata and input, intermediate, and results data that can be added to the record of a workflow experiment to enhance both curation and add value to enable reuse. We have created interactive workflow visualisations for the popular workflow creation tool KNIME, which does not provide users with an in-built function to extract provenance information that can otherwise only be viewed through the tool itself. Making use of the strengths of KNIME for adding documentation and user-defined metadata we can extract and create a visualisation and curation package that encourages and enhances curation@source, facilitating effective communication, collaboration, and reuse of workflows. ","Documentation and Visualisation of Workflows for Effective Communication, Collaboration and Publication @ Source",Edinburgh University,2017,,International Journal of Digital Curation,12,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1434,http://dx.doi.org/10.2218/ijdc.v12i2.429,"{'string_id': '10.2218/ijdc.v12i2.429', 'id_scheme': 'DOI'}","This paper develops and tests a lifecycle model for the preservation of research data by investigating the research practices of scientists.  This research is based on a mixed-method approach.  An initial study was conducted using case study analytical techniques; insights from these case studies were combined with grounded theory in order to develop a novel model of the Digital Research Data Lifecycle.  A broad-based quantitative survey was then constructed to test and extend the components of the model.  The major contribution of these research initiatives are the creation of the Digital Research Data Lifecycle, a data lifecycle that provides a generalized model of the research process to better describe and explain both the antecedents and barriers to preservation.  The antecedents and barriers to preservation are data management, contextual metadata, file formats, and preservation technologies.  The availability of data management support and preservation technologies, the ability to create and manage contextual metadata, and the choices of file formats all significantly effect the preservability of research data. ",Modelling the Research Data Lifecycle,Edinburgh University,2018,,International Journal of Digital Curation,12,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1435,http://dx.doi.org/10.2218/ijdc.v12i2.500,"{'string_id': '10.2218/ijdc.v12i2.500', 'id_scheme': 'DOI'}","Social scientists are producing an ever-expanding volume of data, leading to questions about appraisal and selection of content given finite resources to process data for reuse. We analyze users’ search activity in an established social science data repository to better understand demand for data and more effectively guide collection development. By applying a data-driven approach, we aim to ensure curation resources are applied to make the most valuable data findable, understandable, accessible, and usable. We analyze data from a domain repository for the social sciences that includes over 500,000 annual searches in 2014 and 2015 to better understand trends in user search behavior. Using a newly created search-to-study ratio technique, we identified gaps in the domain data repository’s holdings and leveraged this analysis to inform our collection and curation practices and policies. The evaluative technique we propose in this paper will serve as a baseline for future studies looking at trends in user demand over time at the domain data repository being studied with broader implications for other data repositories. ",A Data-Driven Approach to Appraisal and Selection at a Domain Data Repository,Edinburgh University,2018,,International Journal of Digital Curation,12,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1436,http://dx.doi.org/10.2218/ijdc.v13i1.492,"{'string_id': '10.2218/ijdc.v13i1.492', 'id_scheme': 'DOI'}","In this article, we examine how data producers’ and reusers’ privacy concerns shape their views about data sharing and reuse in the field of education, with an emphasis on video records of practice. We find that data producers and reusers were concerned about the risks that qualitative data, and video records of practice in particular, present to themselves, their colleagues, and the subjects represented in the data. Specifically, they emphasized risks relating to the privacy the subjects – teachers and students who appear in the videos. In response to these risks, data producers have engaged in a number of strategies to minimize risk and/or mitigate potential harm including: (1) education and training; (2) using informed consent to facilitate and/or restrict data sharing; and (3) limiting data capture/production. We discuss the implications that our findings have for digital repositories, and for efforts to facilitate the sharing and reuse of qualitative video data in education. ",Issues of Privacy in Qualitative Video Data Reuse,Edinburgh University,2018,,International Journal of Digital Curation,13,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1437,http://dx.doi.org/10.2218/ijdc.v13i1.502,"{'string_id': '10.2218/ijdc.v13i1.502', 'id_scheme': 'DOI'}","Since its creation nearly a decade ago, the Digital Curation Centre (DCC) Curation Lifecycle Model has become the quintessential framework for understanding digital curation. Organizations and consortia around the world have used the DCC Curation Lifecycle Model as a tool to ensure that all the necessary stages of digital curation are undertaken, to define roles and responsibilities, and to build a framework of standards and technologies for digital curation. Yet, research on the application of the model to large-scale digitization projects as a way of understanding their efforts at digital curation is scant. This paper reports on findings of a qualitative case study analysis of Indiana University Bloomington’s multi-million-dollar Media Digitization and Preservation Initiative (MDPI), employing the DCC Curation Lifecycle Model as a lens for examining the scope and effectiveness of its digital curation efforts. Findings underscore the success of MDPI in performing digital curation by illustrating the ways it implements each of the model’s components. Implications for the application of the DCC Curation Lifecycle Model in understanding digital curation for mass digitization projects are discussed as well as directions for future research. ",Media Digitization and Preservation Initiative: A Case Study,Edinburgh University,2018,,International Journal of Digital Curation,13,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1438,http://dx.doi.org/10.2218/ijdc.v13i1.534,"{'string_id': '10.2218/ijdc.v13i1.534', 'id_scheme': 'DOI'}","This paper describes the findings from a participatory prototype design project, where the authors worked with maternal and child health (MCH) researchers and stakeholders to develop a MCH metadata profile and sustainable curation workflow. This work led to the development of three prototypes: 1) a study catalogue hosted in Dataverse, 2) a metadata and research records repository hosted in REDCap and 3) a metadata harvesting tool/dashboard hosted within the Shiny RStudio environment. We present a brief overview of the methods used to develop the metadata profile, curation workflow and prototypes. Researchers and other stakeholders were participant-collaborators throughout the project. The participatory process involved a number of steps, including but not limited to: initial project design and grant writing; scoping and mapping existing practices, workflows and relevant metadata standards; creating the metadata profile; developing semi-automated and manual techniques to harvest and transform metadata; and end project sustainability/future planning. In this paper, we discuss the design process and project outcomes, limitations and benefits of the approach, and implications for researcher-oriented metadata and data curation initiatives. ",Participatory Prototype Design: Developing a Sustainable Metadata Curation Workflow for Maternal Child Health Research,Edinburgh University,2018,,International Journal of Digital Curation,13,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1439,http://dx.doi.org/10.2218/ijdc.v13i1.554,"{'string_id': '10.2218/ijdc.v13i1.554', 'id_scheme': 'DOI'}","Libraries and archives are increasingly producing subject-based digital collections alongside, but separate from, their main digital collections. These smaller projects are often treated as digital one-offs; they are created, launched, promoted, and then largely forgotten. The authors of this study argue that small-scale digital collections instead be treated as test cases for their institutions’ main digitization programs. Because they are lightweight and have relatively low stakes, these collections get pushed through the system quickly and can illuminate its workings and shortcomings in a snapshot form. The authors treat their own experience in developing the Animal Welfare Act History Digital Collection at the National Agricultural Library as a case study in using a digital collection to test and revise an institution’s digitization program. In so doing, this study suggests how agile projects like the AWAHDC can be core components in digital curation policies and their implementation.  ",Getting to Beta,Edinburgh University,2019,,International Journal of Digital Curation,13,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1440,http://dx.doi.org/10.2218/ijdc.v13i1.630,"{'string_id': '10.2218/ijdc.v13i1.630', 'id_scheme': 'DOI'}","For open science to flourish, data and any related digital outputs should be discoverable and re-usable by a variety of potential consumers. The recent FAIR Data Principles produced by the Future of Research Communication and e-Scholarship (FORCE11) collective provide a compilation of considerations for making data findable, accessible, interoperable, and re-usable. The principles serve as guideposts to ‘good’ data management and stewardship for data and/or metadata. On a conceptual level, the principles codify best practices that managers and stewards would find agreement with, exist in other data quality metrics, and already implement. This paper reports on a secondary purpose of the principles: to inform assessment of data’s FAIR-ness or, put another way, data’s fitness for use. Assessment of FAIR-ness likely requires more stratification across data types and among various consumer communities, as how data are found, accessed, interoperated, and re-used differs depending on types and purposes. This paper’s purpose is to present a method for qualitatively measuring the FAIR Data Principles through operationalizing findability, accessibility, interoperability, and re- usability from a re-user’s perspective. The findings may inform assessments that could also be used to develop situationally-relevant fitness for use frameworks. ",Measuring FAIR Principles to Inform Fitness for Use,Edinburgh University,2018,,International Journal of Digital Curation,13,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1441,http://dx.doi.org/10.2218/ijdc.v13i1.631,"{'string_id': '10.2218/ijdc.v13i1.631', 'id_scheme': 'DOI'}","The complexity of preserving virtual reality environments combines the challenges of preserving singular digital objects, the relationships among those objects, and the processes involved in creating those relationships. A case study involving the preservation of the Virtual Bethel environment is presented. This case is active and ongoing. The paper provides a brief history of the Bethel AME Church of Indianapolis and its importance, then describes the unique preservation challenges of the Virtual Bethel project, and finally provides guidance and preservation recommendations for Virtual Bethel, using the National Digital Stewardship Alliance Levels of Preservation. Discussion of limitations of the guidance and recommendations follow. ","Complexities of Digital Preservation in a Virtual Reality Environment, the Case of Virtual Bethel",Edinburgh University,2018,,International Journal of Digital Curation,13,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1442,http://dx.doi.org/10.2218/ijdc.v13i1.632,"{'string_id': '10.2218/ijdc.v13i1.632', 'id_scheme': 'DOI'}","This research study compared four academic libraries’ approaches to curating the metadata of dataset submissions in their institutional repositories and classified them in one of four categories: no curation, pre-ingest curation, selective curation, and post-ingest curation. The goal is to understand the impact that curation may have on the quality of user-submitted metadata. The findings were 1) the metadata elements varied greatly between institutions, 2) repositories with more options for authors to contribute metadata did not result in more metadata contributed, 3) pre- or post-ingest curation process could have a measurable impact on the metadata but are difficult to separate from other factors, and 4) datasets submitted to a repository with pre- or post-ingest curation more often included documentation. ",Giving datasets context: a comparison study of institutional repositories that apply varying degrees of curation,Edinburgh University,2018,,International Journal of Digital Curation,13,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1443,http://dx.doi.org/10.2218/ijdc.v14i1.556,"{'string_id': '10.2218/ijdc.v14i1.556', 'id_scheme': 'DOI'}","This article presents the findings of the Ibadan/Liverpool Digital Curation Curriculum Review Project, a research project conducted to formally benchmark the teaching of digital curation in the archival education programmes at the University of Liverpool, United Kingdom and the University of Ibadan, Nigeria. It provides background to the history and establishment of both universities and the development of their archives curricula. A matrix was developed using the DigCurV Curriculum Framework to assess whether digital curation skills and knowledge outlined in the framework are being taught, practised and tested in the Master’s programmes. These skills and knowledge were assessed according to the four domains outlined in DigCurV: Knowledge and Intellectual Abilities (KIA), Personal Qualities (PQ), Professional Conduct (PC), and Management and Quality Assurance (MQA), to levels appropriate to practitioners and managers. The exercise identified skill and knowledge areas where teaching materials could be shared between the universities, and areas where new materials are needed. ",Digital Curation Education at the Universities of Ibadan and Liverpool,Edinburgh University,2019,,International Journal of Digital Curation,14,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1444,http://dx.doi.org/10.2218/ijdc.v14i1.586,"{'string_id': '10.2218/ijdc.v14i1.586', 'id_scheme': 'DOI'}","Digital/data curation curricula have been around for a couple of decades. Currently, several ALA-accredited LIS programs offer digital/data curation courses and certificate programs to address the high demand for professionals with the knowledge and skills to handle digital content and research data in an ever-changing information environment.  In this study, we aimed to examine the topical scopes of digital/data curation curricula in the context of the LIS field.  We collected 16 syllabi from the digital/data curation courses, as well as textual descriptions of the 11 programs and their core courses offered in the U.S., Canada, and the U.K. The collected data were analyzed using a probabilistic topic modeling technique, Latent Dirichlet Allocation, to identify both common and unique topics. The results are the identification of 20 topics both at the program- and course-levels. Comparison between the program- and course-level topics uncovered a set of unique topics, and a number of common topics.  Furthermore, we provide interactive visualizations for digital/data curation programs and courses for further analysis of topical distributions. We believe that our combined approach of a topic modeling and visualizations may provide insight for identifying emerging trends and co-occurrences of topics among digital/data curation curricula in the LIS field. ",Identifying Topical Coverages of Curricula using Topic Modeling and Visualization Techniques: A Case of Digital and Data Curation,Edinburgh University,2019,,International Journal of Digital Curation,14,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1445,http://dx.doi.org/10.2218/ijdc.v14i1.588,"{'string_id': '10.2218/ijdc.v14i1.588', 'id_scheme': 'DOI'}","This study identifies challenges and promising directions in the curation of 3D data. 3D visualization shows great promise for a range of scholarly fields through interactive engagement with and analysis of spatially complex artifacts, spaces, and data. While the new affordability of emerging 3D capture technologies presents greater academic possibilities, academic libraries need more effective workflows, policies, standards, and practices to ensure that they can support the creation, discovery, access, preservation, and reproducibility of 3D data sets. This study uses nominal group technique with invited experts across several disciplines and sectors to identify common challenges in the creation and re-use of 3D data for the purpose of developing library strategy for supporting curation of 3D data. This article identifies staffing needs for 3D imaging; alignment with IT resources; the roll of archivists in addressing unique challenges posed by these datasets; the importance of data annotation, metadata, and transparency for research integrity and reproducibility; and features for storage, access, and management to facilitate re-use by researchers and educators. Participants identified three main challenges for supporting 3D data that align with the strengths of libraries: 1) development of crosswalks and aggregation tools for discipline-specific metadata models, data dictionaries for 3D research, and aggregation tools for expanding discovery; 2) development of an open source viewer that supports streaming and annotation on archival formats of 3D models and makes archival master files accessible, while also serving derivative files based on user requirements; and 3) widespread of adoption of better documentation and technical metadata for image capture and modeling processes in order to support replicability of research, reproducibility of models, and transparency of scientific process. ",Challenges and Directions in 3D and VR Data Curation,Edinburgh University,2019,,International Journal of Digital Curation,14,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1446,http://dx.doi.org/10.2218/ijdc.v14i1.594,"{'string_id': '10.2218/ijdc.v14i1.594', 'id_scheme': 'DOI'}","The National Imaging Facility (NIF) provides Australian researchers with state-of-the-art instrumentation—including magnetic resonance imaging (MRI), positron emission tomography (PET), X-ray computed tomography (CT) and multispectral imaging – and expertise for the characterisation of animals, plants and materials. To maximise research outcomes, as well as to facilitate collaboration and sharing, it is essential not only that the data acquired using these instruments be managed, curated and archived in a trusted data repository service, but also that the data itself be of verifiable quality. In 2017, several NIF nodes collaborated on a national project to define the requirements and best practices necessary to achieve this, and to establish exemplar services for both preclinical MRI data and clinical ataxia MRI data. In this paper we describe the project, its key outcomes, challenges and lessons learned, and future developments, including extension to other characterisation facilities and instruments/modalities. ",Putting the Trust into Trusted Data Repositories: A Federated Solution for the Australian National Imaging Facility,Edinburgh University,2019,,International Journal of Digital Curation,14,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1447,http://dx.doi.org/10.2218/ijdc.v14i1.595,"{'string_id': '10.2218/ijdc.v14i1.595', 'id_scheme': 'DOI'}","University libraries have played an important role in constructing an infrastructure of support for Research Data Management at an institutional level. This paper presents a comparative analysis of two international surveys of libraries about their involvement in Research Data Services conducted in 2014 and 2018. The aim was to explore how services had developed over this time period, and to explore the drivers and barriers to change. In particular, there was an interest in how far the FAIR data principles had been adopted. Services in nearly every area were more developed in 2018 than before, but technical services remained less developed than advisory. Progress on institutional policy was also evident. However, priorities did not seem to have shifted significantly. Open ended answers suggested that funder policy, rather than researcher demand, remained the main driver of service development and that resources and skills gaps remained issues. While widely understood as an important reference point and standard, because of their relatively recent publication date, FAIR principles had not been widely adopted explicitly in policy. ",Progress in Research Data Services,Edinburgh University,2019,,International Journal of Digital Curation,14,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1448,http://dx.doi.org/10.2218/ijdc.v14i1.598,"{'string_id': '10.2218/ijdc.v14i1.598', 'id_scheme': 'DOI'}","PARADISEC’s PNG collections represent the great diversity in the regions and languages of PNG. In 2016 and 2017, in recognition of the value of PARADISEC’s collections, ANDS (the Australian National Data Service) provided funding for us to concentrate efforts on enhancing the metadata that describes our Papua New Guinea (PNG) collections, an effort designed to maximise the findability and useability of the language and music recordings preserved in the archive for both source communities and researchers. PARADISEC's subsequent engagement with PNG language experts has led to collaborations with members of speaker communities who are part of the PNG diaspora in Australia. In this paper, we show that making historical recordings more findable, accessible and better described can result in meaningful interactions with and responses to the data in source communities. The effects of empowering speaker communities in their relationships to archives can be far reaching – even inverting, or disrupting the power relationships that have resulted from the colonial histories in which archives are embedded. ",Making Meaning of Historical Papua New Guinea Recordings,Edinburgh University,2019,,International Journal of Digital Curation,14,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1449,http://dx.doi.org/10.2218/ijdc.v14i1.643,"{'string_id': '10.2218/ijdc.v14i1.643', 'id_scheme': 'DOI'}","The Data Curation Continuum was developed as a way of thinking about data repository infrastructure. Since its original development over a decade ago, a number of things have changed in the data infrastructure domain. This paper revisits the thinking behind the original data curation continuum and updates it to respond to changes in research objects, storage models, and the repository landscape in general.   ",Updating the Data Curation Continuum,Edinburgh University,2019,,International Journal of Digital Curation,14,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1450,http://dx.doi.org/10.2218/ijdc.v15i1.601,"{'string_id': '10.2218/ijdc.v15i1.601', 'id_scheme': 'DOI'}","This paper describes the development of a systematic approach to the creation, management and curation of linguistic resources, particularly spoken language corpora. It also presents first steps towards a framework for continuous quality control to be used within external research projects by non-technical users, and discuss various domain and discipline specific problems and individual solutions. The creation of spoken language corpora is not only a time-consuming and costly process, but the created resources often represent intangible cultural heritage, containing recordings of, for example, extinct languages or historical events. Since high quality resources are needed to enable re-use in as many future contexts as possible, researchers need to be provided with the necessary means for quality control. We believe that this includes methods and tools adapted to Humanities researchers as non-technical users, and that these methods and tools need to be developed to support existing tasks and goals of research projects. ",Towards Continuous Quality Control for Spoken Language Corpora,Edinburgh University,2020,,International Journal of Digital Curation,15,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1451,http://dx.doi.org/10.2218/ijdc.v15i1.602,"{'string_id': '10.2218/ijdc.v15i1.602', 'id_scheme': 'DOI'}","The decision to allow users access to restricted and protected data is based on the development of trust in the user by data repositories. In this article, I propose a model of the process of trust development at restricted data repositories, a model which emphasizes the increasing levels of trust dependent on prior interactions between repositories and users. I find that repositories develop trust in their users through the interactions of four dimensions – promissory, experience, competence, and goodwill – that consider distinct types of researcher expertise and the role of a researcher’s reputation in the trust process. However, the processes used by repositories to determine a level of trust corresponding to data access are inconsistent and do not support the sharing of trusted users between repositories to maximize efficient yet secure access to restricted research data. I highlight the role of a researcher’s reputation as an important factor in trust development and trust transference, and discuss the implications of modelling the restricted data access process as a process of trust development. ",Facilitating Access to Restricted Data,Edinburgh University,2020,,International Journal of Digital Curation,15,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1452,http://dx.doi.org/10.2218/ijdc.v15i1.646,"{'string_id': '10.2218/ijdc.v15i1.646', 'id_scheme': 'DOI'}","One of the grand curation challenges is to secure metadata quality in the ever-changing environment of metadata standards and file formats. As the Red Queen tells Alice in Through the Looking-Glass: “Now, here, you see, it takes all the running you can do, to keep in the same place.” That is, there is some “running” needed to keep metadata records in a research data repository fit for long-term use and put in place. One of the main tools of adaptation and keeping pace with the evolution of new standards, formats – and versions of standards in this ever-changing environment are validation schemas. Validation schemas are mainly seen as methods of checking data quality and fitness for use, but are also important for long-term preservation. We might like to think that our present (meta)data standards and formats are made for eternity, but in reality we know that standards evolve, formats change (some even become obsolete with time), and so do our needs for storage, searching and future dissemination for re-use. Eventually, we come to a point where transformation of our archival records and migration to other formats will be necessary. This could also mean that even if the AIPs, the Archival Information Packages stay the same in storage, the DIPs, the Dissemination Information Packages that we want to extract from the archive are subject to change of format. Further, in order for archival information packages to be self-sustainable, as required in the OAIS model, it is important to take interdependencies between individual files in the information packages into account. This should be done already by the time of ingest and validation of the SIPs, the Submission Information Packages, and along the line at different points of necessary transformation/migration (from SIP to AIP, from AIP to DIP etc.), in order to counter obsolescence. This paper investigates possible validation errors and missing elements in metadata records from three general purpose, multidisciplinary research data repositories – Figshare, Harvard’s Dataverse and Zenodo, and explores the potential effects of these errors on future transformation to AIPs and migration to other formats within a digital archive.   ",The Red Queen in the Repository,Edinburgh University,2020,,International Journal of Digital Curation,15,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1453,http://dx.doi.org/10.2218/ijdc.v15i1.671,"{'string_id': '10.2218/ijdc.v15i1.671', 'id_scheme': 'DOI'}","Effective data management and data sharing are crucial components of the research lifecycle, yet evidence suggests that many social science graduate programs are not providing training in these areas. The current exploratory study assesses how U.S. masters and doctoral programs in the social sciences include formal, non-formal, and informal training in data management and sharing. We conducted a survey of 150 graduate programs across six social science disciplines, and used a mix of closed and open-ended questions focused on the extent to which programs provide such training and exposure. Results from our survey suggested a deficit of formal training in both data management and data sharing, limited non-formal training, and cursory informal exposure to these topics. Utilizing the results of our survey, we conducted a syllabus analysis to further explore the formal and non-formal content of graduate programs beyond self-report. Our syllabus analysis drew from an expanded seven social science disciplines for a total of 140 programs. The syllabus analysis supported our prior findings that formal and non-formal inclusion of data management and data sharing training is not common practice. Overall, in both the survey and syllabi study we found a lack of both formal and non-formal training on data management and data sharing. Our findings have implications for data repository staff and data service professionals as they consider their methods for encouraging data sharing and prepare for the needs of data depositors. These results can also inform the development and structuring of graduate education in the social sciences, so that researchers are trained early in data management and sharing skills and are able to benefit from making their data available as early in their careers as possible. ",An Exploratory Analysis of Social Science Graduate Education in Data Management and Data Sharing,Edinburgh University,2020,,International Journal of Digital Curation,15,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1454,http://dx.doi.org/10.2218/ijdc.v15i1.710,"{'string_id': '10.2218/ijdc.v15i1.710', 'id_scheme': 'DOI'}","This paper explores the tension between the tools that data reusers in the field of education prefer to use when working with qualitative video data and the tools that repositories make available to data reusers. Findings from this mixed-methods study show that data reusers utilizing qualitative video data did not use repository-based tools. Rather, they valued common, widely available tools that were collaborative and easy to use.   ",Tool Selection Among Qualitative Data Reusers,Edinburgh University,2020,,International Journal of Digital Curation,15,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1455,http://dx.doi.org/10.2218/ijdc.v1i1.2,"{'string_id': '10.2218/ijdc.v1i1.2', 'id_scheme': 'DOI'}","The creation, management and use of digital materials are of increasing importance for a wide range of activities. Much of the knowledge base and intellectual assets of institutions and individuals are now in digital form. The term digital curation is increasingly being used for the actions needed to add value to and maintain these digital assets over time for current and future generations of users. The paper explores this emerging field of digital curation as an area of inter-disciplinary research and practice, and the trends which are influencing its development. It analyses the genesis of the term and how traditional roles relating to digital assets are in transition. Finally it explores some of the drivers for curation ranging from trends such as exponential growth in digital information, to ""life-caching"", digital preservation, the Grid and new opportunities for publishing, sharing, and re-using data. It concludes that significant effort needs to be put into developing a persistent information infrastructure for digital materials and into developing the digital curation skills of researchers and information professionals. Without this, current investment in digitisation and digital content will only secure short-term rather than lasting benefits.","Digital Curation for Science, Digital Libraries, and Individuals",Edinburgh University,2008,,International Journal of Digital Curation,1,,"[{'value': '1746-8256', 'type': 'electronic'}]"
1456,http://dx.doi.org/10.2218/ijdc.v1i1.3,"{'string_id': '10.2218/ijdc.v1i1.3', 'id_scheme': 'DOI'}","A defining characteristic of contemporary copyright law is the willingness of governments to accept the argument that the impact of digital technologies requires copyright owners to be given ever greater control over the use of their works, regardless of the detriment to the copyright regime's 'public interest' elements. Yet a one-size-fits-all 'all rights reserved' copyright regime clearly fails to meet the requirements of many rightsholders. One response has been the Creative Commons movement which seeks, through licences based on existing copyright laws, to provide a simple mechanism for rightsholders to disseminate their works under less restrictive conditions. The Creative Commons' initial success has led to suggestions that its principles could be equally applied to scientific research outputs, such as publications, licensing of research materials, and datasets. This article argues that the Science Commons approach, if based on the Creative Commons model, and premised at its root on utilitarian copyright law, will both fail to address contemporary policy drivers in research, or to provide researchers with the type of rights that they actually want. It suggests that constructing an appropriate set of rights for the Science Commons, particularly for datasets, will require a willingness to step outside the utilitarian model and look to the Continental copyright tradition, which sets less store in economic rights and gives greater weight to moral rights.","Digital Curation, Copyright, and Academic Research",Edinburgh University,2008,,International Journal of Digital Curation,1,,"[{'value': '1746-8256', 'type': 'electronic'}]"
1457,http://dx.doi.org/10.2218/ijdc.v1i1.4,"{'string_id': '10.2218/ijdc.v1i1.4', 'id_scheme': 'DOI'}","The use of digital technologies within research has led to a proliferation of data, many new forms of research output and new modes of presentation and analysis. Many scientific communities are struggling with the challenge of how to manage the terabytes of data and new forms of output, they are producing. They are also under increasing pressure from funding organizations to publish their raw data, in addition to their traditional publications, in open archives. In this paper I describe an approach that involves the selective encapsulation of raw data, derived products, algorithms, software and textual publications within “scientific publication packages”. Such packages provide an ideal method for: encapsulating expert knowledge; for publishing and sharing scientific process and results; for teaching complex scientific concepts; and for the selective archival, curation and preservation of scientific data and output. They also provide a bridge between technological advances in the Digital Libraries and eScience domains. In particular, I describe the RDF-based architecture that we are adopting to enable scientists to construct, publish and manage “scientific publication packages” - compound digital objects that encapsulate and relate the raw data to its derived products, publications and the associated contextual, provenance and administrative metadata.",Scientific Publication Packages – A Selective Approach to the Communication and Archival of Scientific Output,Edinburgh University,2008,,International Journal of Digital Curation,1,,"[{'value': '1746-8256', 'type': 'electronic'}]"
1458,http://dx.doi.org/10.2218/ijdc.v2i1.10,"{'string_id': '10.2218/ijdc.v2i1.10', 'id_scheme': 'DOI'}","The National Archives is developing a range of practical solutions to the active preservation of electronic records, using an extensible service-oriented architecture and a central technical registry (PRONOM). This paper describes TNA’s methodologies for characterisation, preservation planning, and preservation action, the technologies being adopted to implement them, and the role of PRONOM in supporting these services. It describes how this approach fits with international research programmes, and the types of preservation service which TNA may be able to provide externally in the future.",Developing Practical Approaches to Active Preservation,Edinburgh University,2008,,International Journal of Digital Curation,2,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1459,http://dx.doi.org/10.2218/ijdc.v2i1.11,"{'string_id': '10.2218/ijdc.v2i1.11', 'id_scheme': 'DOI'}","Understanding and communicating the cost and value of digital curation activities has now been recognised by a number of projects and initiatives as a very important factor in ensuring the long-term survival of digital assets. A number of projects have developed costing models for digital preservation but there remains a major problem with information assets (digital or otherwise) in that their value is difficult to express in terms that are readily understood by all the stakeholders, especially those who might fund their preservation. This paper introduces a range of issues concerning information value and business models for sustained funding of digital preservation, with particular reference to the espida Project recently completed at the University of Glasgow. This project has developed a model of information value that builds on the Balanced Scorecard approach to business performance developed by Kaplan and Norton. This model casts information curation as an investment where current and ongoing expenditure is incurred in order to produce future returns, benefitting a range of stakeholders. In this formulation, value is seen as multi-facetted and, from the point of view of the individual or organisation funding the curation, explicitly related to the funder’s strategic goals. It also recognises that benefits may only accrue over the long term and that there is a risk that information that is preserved may fail to deliver any return. Examples discussed in the paper concern the establishment of an institutional repository and the establishment of an e-thesis service for an educational institution. It concludes that a deconstruction of benefits of this kind can be more quickly and fully understood even by stakeholders not necessarily expert in the curation field. This facilitates the production of a well-constructed case that clearly articulates information value and the benefit that accrues from its curation, which in turn allows senior management or other funders to make funding decisions based on understandable information: the basic premise of good practice in management. This is a commonly understood idea and one that the espida methodology helps fulfil.",The world is all grown digital.... How shall a man persuade management what to do in such times?,Edinburgh University,2008,,International Journal of Digital Curation,2,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1460,http://dx.doi.org/10.2218/ijdc.v2i1.12,"{'string_id': '10.2218/ijdc.v2i1.12', 'id_scheme': 'DOI'}","A successful research and innovation system depends on the open exchange of ideas, information and knowledge. But both research methods and the scholarly communications system are undergoing fundamental changes which present new opportunities and challenges in communicating the results of research. Funders are at different stages in responding to these changes, and this in turn presents challenges to researchers and research institutions. This paper reports on the findings of a study undertaken in 2006 into the policies, practices and views of a range of the major funders of research in the UK in relation to the management of the information outputs generated with the benefit of their support. It covers the full range of information outputs, including journal articles and monographs, but also other outputs, including data, that are not generally published in traditional form. The article also presents conclusions as to issues that need to be addressed in the development of a coherent and consistent policy framework for the future.",UK Research Funders’ Policies for the Management of Information Outputs,Edinburgh University,2008,,International Journal of Digital Curation,2,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1461,http://dx.doi.org/10.2218/ijdc.v2i1.13,"{'string_id': '10.2218/ijdc.v2i1.13', 'id_scheme': 'DOI'}","This paper builds on the work presented at the ECDL 2006 in automated genre classification as a step toward automating metadata extraction from digital documents for ingest into digital repositories such as those run by archives, libraries and eprint services (Kim & Ross, 2006b). We have previously proposed dividing features of a document into five types (features for visual layout, language model features, stylometric features, features for semantic structure, and contextual features as an object linked to previously classified objects and other external sources) and have examined visual and language model features. The current paper compares results from testing classifiers based on image and stylometric features in a binary classification to show that certain genres have strong image features which enable effective separation of documents belonging to the genre from a large pool of other documents.",“The Naming of Cats”: Automated Genre Classification,Edinburgh University,2008,,International Journal of Digital Curation,2,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1462,http://dx.doi.org/10.2218/ijdc.v2i1.14,"{'string_id': '10.2218/ijdc.v2i1.14', 'id_scheme': 'DOI'}","The reference model for the Open Archival Information System (OAIS) is well established in the research community as a method of modelling the functions of a digital repository and as a basis in which to frame digital curation and preservation issues. In reference to the 5th anniversary review of the OAIS, it is timely to consider how it may be interpreted by an institutional repository. The paper examines methods of sharing essential functions and requirements of an OAIS between two or more institutions, outlining the practical considerations of outsourcing. It also details the approach taken by the SHERPA DP Project to introduce a disaggregated service model for institutional repositories that wish to implement preservation services.",Modelling OAIS Compliance for Disaggregated Preservation Services,Edinburgh University,2008,,International Journal of Digital Curation,2,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1463,http://dx.doi.org/10.2218/ijdc.v2i1.15,"{'string_id': '10.2218/ijdc.v2i1.15', 'id_scheme': 'DOI'}","Occupational information resources - data about the characteristics of different occupational positions - are widely used in the social sciences, across a range of disciplines and international contexts. They are available in many formats, most often constituting small electronic files that are made freely downloadable from academic web pages. However there are several challenges associated with how occupational information resources are distributed to, and exploited by, social researchers. In this paper we describe features of occupational information resources, and indicate the role digital curation can play in exploiting them. We report upon the strategies used in the GEODE research project (Grid Enabled Occupational Data Environment1). This project attempts to develop long-term standards for the distribution of occupational information resources, by providing a standardized framework-based electronic depository for occupational information resources, and by providing a data indexing service, based on e-Science middleware, which collates occupational information resources and makes them readily accessible to non-specialist social scientists.",Data Curation Standards and Social Science Occupational Information Resources,Edinburgh University,2008,,International Journal of Digital Curation,2,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1464,http://dx.doi.org/10.2218/ijdc.v2i1.16,"{'string_id': '10.2218/ijdc.v2i1.16', 'id_scheme': 'DOI'}","The MIT Libraries, the San Diego Supercomputer Center, and the University of California San Diego Libraries are conducting the PLEDGE Project to determine the set of policies that affect operational digital preservation archives and to develop standardized means of recording and enforcing them using rules engines. This has the potential to allow for automated assessment of “trustworthiness” of digital preservation archives. We are also evaluating the completeness of other efforts to define policies for digital preservation such as the RLG/NARA Trusted Digital Repository checklist and the PREMIS metadata schema. We present our results to date.",Digital Archive Policies and Trusted Digital Repositories,Edinburgh University,2008,,International Journal of Digital Curation,2,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1465,http://dx.doi.org/10.2218/ijdc.v2i2.25,"{'string_id': '10.2218/ijdc.v2i2.25', 'id_scheme': 'DOI'}","To date many institutional repository (IR) software suppliers have pushed the IR as a digital preservation solution. We argue that the digital preservation of objects in IRs may better be achieved through the use of light-weight, add-on services. We present such a service – PRONOM-ROAR – that generates file format profiles for IRs. This demonstrates the potential of using third- party services to provide preservation expertise to IR managers by making use of existing machine interfaces to IRs.",PRONOM-ROAR: Adding Format Profiles to a Repository Registry to Inform Preservation Services,Edinburgh University,2008,,International Journal of Digital Curation,2,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1466,http://dx.doi.org/10.2218/ijdc.v2i2.26,"{'string_id': '10.2218/ijdc.v2i2.26', 'id_scheme': 'DOI'}","Astronomy is similar to other scientific disciplines in that scholarly publication relies on the presentation and interpretation of data. But although astronomy now has archives for its primary research telescopes and associated surveys, the highly processed data that is presented in the peer-reviewed journals and is the basis for final analysis and interpretation is generally not archived and has no permanent repository. We have initiated a project whose goal is to implement an end-to-end prototype system which, through a partnership of a professional society, that society’s scholarly publications/publishers, research libraries, and an information technology substrate provided by the Virtual Observatory, will capture high-level digital data as part of the publication process and establish a distributed network of curated, permanent data repositories. The data in this network will be accessible through the research journals, astronomy data centers, and Virtual Observatory data discovery portals.",Digital Data Preservation for Scholarly Publications in Astronomy,Edinburgh University,2008,,International Journal of Digital Curation,2,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1467,http://dx.doi.org/10.2218/ijdc.v2i2.27,"{'string_id': '10.2218/ijdc.v2i2.27', 'id_scheme': 'DOI'}","Scientific data problems do not stand in isolation. They are part of a larger set of challenges associated with the escalation of scientific information and changes in scholarly communication in the digital environment. Biologists in particular are generating enormous sets of data at a high rate, and new discoveries in the biological sciences will increasingly depend on the integration of data across multiple scales. This work will require new kinds of information expertise in key areas. To build this professional capacity we have developed two complementary educational programs: a Biological Information Specialist (BIS) masters degree and a concentration in Data Curation (DC). We believe that BISs will be central in the development of cyberinfrastructure and information services needed to facilitate interdisciplinary and multi-scale science. Here we present three sample cases from our current research projects to illustrate areas in which we expect information specialists to make important contributions to biological research practice.",Graduate Curriculum for Biological Information Specialists: A Key to Integration of Scale in Biology,Edinburgh University,2008,,International Journal of Digital Curation,2,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1468,http://dx.doi.org/10.2218/ijdc.v2i2.28,"{'string_id': '10.2218/ijdc.v2i2.28', 'id_scheme': 'DOI'}","The National Archives and Records Administration (NARA) and EU SHAMAN projects are working with multiple research institutions on tools and technologies that will supply a comprehensive, systematic, and dynamic means for preserving virtually any type of electronic record, free from dependence on any specific hardware or software. This paper describes the joint development work between the University of Liverpool and the San Diego Supercomputer Center (SDSC) at the University of California, San Diego on the NARA and SHAMAN prototypes. The aim is to provide technologies in support of the required generic data management infrastructure. We describe a Theory of Preservation that quantifies how communication can be accomplished when future technologies are different from those available at present. This includes not only different hardware and software, but also different standards for encoding information. We describe the concept of a “digital ontology” to characterize preservation processes; this is an advance on the current OAIS Reference Model of providing representation information about records. To realize a comprehensive Theory of Preservation, we describe the ongoing integration of distributed shared collection management technologies, digital library browsing, and presentation technologies for the NARA and SHAMAN Persistent Archive Testbeds.",Digital Preservation Theory and Application: Transcontinental Persistent Archives Testbed Activity,Edinburgh University,2008,,International Journal of Digital Curation,2,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1469,http://dx.doi.org/10.2218/ijdc.v3i1.38,"{'string_id': '10.2218/ijdc.v3i1.38', 'id_scheme': 'DOI'}","The National Digital Information Infrastructure and Preservation Program (NDIIPP) was initiated in December 2000 when the U.S. Congress authorized the Library of Congress to work with a broad range of institutions to develop a national strategy for the preservation of at-risk digital content. Guided by a strategy of collaboration and iteration, the Library of Congress began the formation of a national network of partners dedicated to collecting and preserving important born-digital information. Over the last six years, the Library and its partners have been engaged in learning through action that has resulted in an evolving understanding of the most appropriate roles and functions for a national network of diverse stakeholders. The emerging network is complex and inclusive of a variety of stakeholders; content producers, content stewards and service providers from the public and private sectors. Lessons learned indicate that interoperability is a challenge in all aspects of collaborative work.",Evolving a Network of Networks: The Experience of Partnerships in the National Digital Information Infrastructure and Preservation Program,Edinburgh University,2008,,International Journal of Digital Curation,3,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1470,http://dx.doi.org/10.2218/ijdc.v3i1.39,"{'string_id': '10.2218/ijdc.v3i1.39', 'id_scheme': 'DOI'}","This paper first explores some of the reasons why collaboration is becoming increasingly important in supporting scientific data curation, digital preservation initiatives and institutional repository development. It then investigates the concepts of trust and control used in the organisation science literature and attempts to apply them to the work on trustworthy repositories being carried out by various international initiatives.",Toward Distributed Infrastructures for Digital Preservation: The Roles of Collaboration and Trust,Edinburgh University,2008,,International Journal of Digital Curation,3,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1471,http://dx.doi.org/10.2218/ijdc.v3i1.40,"{'string_id': '10.2218/ijdc.v3i1.40', 'id_scheme': 'DOI'}","The purpose of the DareLux (Data Archiving River Environment Luxembourg) Project was the preservation of unique and irreplaceable datasets, for which we chose hydrology data that will be required to be used in future climatic models. The results are: an operational archive built with XML containers, the OAI-PMH protocol and an architecture based upon web services. Major conclusions are: quality control on ingest is important; digital rights management demands attention; and cost aspects of ingest and retrieval cannot be underestimated. We propose a new paradigm for information retrieval of this type of dataset. We recommend research into visualisation tools for the search and retrieval of this type of dataset.",Dataset Preservation for the Long Term: Results of the DareLux Project,Edinburgh University,2008,,International Journal of Digital Curation,3,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1472,http://dx.doi.org/10.2218/ijdc.v3i1.41,"{'string_id': '10.2218/ijdc.v3i1.41', 'id_scheme': 'DOI'}","The explosion in the production of scientific data in recent years is placing strains upon conventional systems supporting integration, analysis, interpretation and dissemination of data and thus constraining the whole scientific process. Support for handling large quantities of diverse information can be provided by e-Science methodologies and the cyber-infrastructure that enables collaborative handling of such data. Regard needs to be taken of the whole process involved in scientific discovery. This includes the consideration of the requirements of the users and consumers further down the information chain and what they might ideally prefer to impose on the generators of those data. As the degree of digital capture in the laboratory increases, it is possible to improve the automatic acquisition of the ‘context of the data’ as well as the data themselves. This process provides an opportunity for the data creators to ensure that many of the problems they often encounter in later stages are avoided. We wish to elevate curation to an operation to be considered by the laboratory scientist as part of good laboratory practice, not a procedure of concern merely to the few specialising in archival processes. Designing curation into experiments is an effective solution to the provision of high-quality metadata that leads to better, more re-usable data and to better science.",Curation of Laboratory Experimental Data as Part of the Overall Data Lifecycle,Edinburgh University,2008,,International Journal of Digital Curation,3,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1473,http://dx.doi.org/10.2218/ijdc.v3i1.42,"{'string_id': '10.2218/ijdc.v3i1.42', 'id_scheme': 'DOI'}","A preservation environment manages communication from the past while communicating with the future. Information generated in the past is sent into the future by the current preservation environment. The proof that the preservation environment preserves authenticity and integrity while performing the communication constitutes a theory of digital preservation. We examine the representation information that is needed about the preservation environment for a theory of digital preservation. The representation information includes descriptions of the preservation management policies, the preservation processes, and the state information that is needed to verify the correct working behavior of the system. We demonstrate rule-based data grids that can verify that prior policies correctly enforced preservation properties, while sending into the future descriptions of the current preservation management policies.",Towards a Theory of Digital Preservation,Edinburgh University,2008,,International Journal of Digital Curation,3,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1474,http://dx.doi.org/10.2218/ijdc.v3i1.43,"{'string_id': '10.2218/ijdc.v3i1.43', 'id_scheme': 'DOI'}","The OAIS concept of Representation Information (RI) is a potential strategy for the curation and preservation of all types of information. We share insights gained from our exploration of issues concerned with the capture of crystallography and engineering RI as well as its structuring, collection and curation. In addition, we discuss the supporting technical, IPR and globally collaborative infrastructures required to make such a strategy successful.",Challenges and Issues Relating to the Use of Representation Information for the Digital Curation of Crystallography and Engineering Data,Edinburgh University,2008,,International Journal of Digital Curation,3,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1475,http://dx.doi.org/10.2218/ijdc.v3i1.44,"{'string_id': '10.2218/ijdc.v3i1.44', 'id_scheme': 'DOI'}","File format obsolescence is a major risk factor threatening the ongoing usefulness of digital information collections. While the preservation community has become increasingly interested in tools for assessing a wide range of risks, the National Library of Australia is developing mechanisms specifically focused on the risks of format obsolescence. The paper reports on the AONS II Project, undertaken in conjunction with the Australian Partnership for Sustainable Repositories (APSR). The project aimed to refine and develop a software tool that would automatically find and report indicators of obsolescence risks, to help repository managers decide if preservation action is needed. The paper discusses the current mismatch between this objective and the available sources of information on file formats, and emphasises the need to take account of both local and global factors in assessing risk. The paper calls for the preservation community to engage with the further development of thinking about file format obsolescence.",Defining File Format Obsolescence: A Risky Journey,Edinburgh University,2008,,International Journal of Digital Curation,3,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1476,http://dx.doi.org/10.2218/ijdc.v3i1.45,"{'string_id': '10.2218/ijdc.v3i1.45', 'id_scheme': 'DOI'}","The Data Documentation Initiative (DDI) is an emerging metadata standard for the social sciences. The DDI is in active use by many data specialists and archivists, but researchers themselves have been slow to recognize the benefits of the standards approach to metadata. This paper outlines how the DDI has evolved since its inception in 1995 and discusses ways to broaden its impact in the social science research community.",Data Documentation Initiative: Toward a Standard for the Social Sciences,Edinburgh University,2008,,International Journal of Digital Curation,3,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1477,http://dx.doi.org/10.2218/ijdc.v3i1.46,"{'string_id': '10.2218/ijdc.v3i1.46', 'id_scheme': 'DOI'}","The success of eScience research depends not only upon effective collaboration between scientists and technologists but also upon the active involvement of data archivists. Archivists rarely receive scientific data until findings are published, by which time important information about their origins, context, and provenance may be lost. Research reported here addresses the life cycle of data from collaborative ecological research with embedded networked sensing technologies. A better understanding of these processes will enable archivists to participate in earlier stages of the life cycle and to improve curation of these types of scientific data. Evidence from our interview study and field research yields a nine-stage life cycle. Among the findings are the cumulative effect of decisions made at each stage of the life cycle; the balance of decision-making between scientific and technology research partners; and the loss of certain types of data that may be essential to later interpretation.",Moving Archival Practices Upstream: An Exploration of the Life Cycle of Ecological Sensing Data in Collaborative Field Research,Edinburgh University,2008,,International Journal of Digital Curation,3,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1478,http://dx.doi.org/10.2218/ijdc.v3i2.55,"{'string_id': '10.2218/ijdc.v3i2.55', 'id_scheme': 'DOI'}","This paper presents the SCOPE (Scientific Compound Object Publishing and Editing) system which is designed to enable scientists to easily author, publish and edit scientific compound objects. Scientific compound objects encapsulate the various datasets and resources generated or utilized during a scientific experiment or discovery process, within a single compound object, for publishing and exchange. The adoption of “named graphs” to represent these compound objects enables provenance information to be captured via the typed relationships between the components. This approach is also endorsed by the OAI-ORE initiative and hence ensures that we generate OAI-ORE-compliant Scientific Compound Objects. The SCOPE system is an extension of the Provenance Explorer tool – which supports access-controlled viewing of scientific provenance trails. Provenance Explorer provided dynamic rendering of RDF graphs of scientific discovery processes, showing the lineage from raw data to publication. Views of different granularity can be inferred automatically using SWRL (Semantic Web Rules Language) rules and an inferencing engine. SCOPE extends the Provenance Explorer tool and GUI by: 1) Adding an embedded web browser that can be used for incorporating objects discoverable via the Web; 2) Representing compound objects as Named Graphs, that can be saved in RDF, TriX, TriG or as an Atom syndication feed; 3) Enabling scientists to attach Creative Commons Licenses to the compound objects to specify how they may be re-used; 4) Enabling compound objects to be published as Fedora Object XML (FOXML) files within a Fedora digital library.",SCOPE: A Scientific Compound Object Publishing and Editing System,Edinburgh University,2008,,International Journal of Digital Curation,3,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1479,http://dx.doi.org/10.2218/ijdc.v3i2.56,"{'string_id': '10.2218/ijdc.v3i2.56', 'id_scheme': 'DOI'}","Computer games, like other digital media, are extremely vulnerable to long-term loss, yet little work has been done to preserve them. As a result we are experiencing large-scale loss of the early years of gaming history. Computer games are an important part of modern popular culture, and yet are afforded little of the respect bestowed upon established media such as books, film, television and music. We must understand the reasons for the current lack of computer game preservation in order to devise strategies for the future. Computer game history is a difficult area to work in, because it is impossible to know what has been lost already, and early records are often incomplete. This paper uses the information that is available to analyse the current status of computer game preservation, specifically in the UK. It makes a quantitative analysis of the preservation status of computer games, and finds that games are already in a vulnerable state. It proposes that work should be done to compile accurate metadata on computer games and to analyse more closely the exact scale of data loss, while suggesting strategies to overcome the barriers that currently exist.",‘Grand Theft Archive’: A Quantitative Analysis of the State of Computer Game Preservation,Edinburgh University,2008,,International Journal of Digital Curation,3,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1480,http://dx.doi.org/10.2218/ijdc.v3i2.57,"{'string_id': '10.2218/ijdc.v3i2.57', 'id_scheme': 'DOI'}","This paper explores data practices in a Long Term Ecological Research (LTER) setting. It describes a number of salient data characteristics that are specific to the LTER program and outlines some central features of the curation approach cultivated within the US LTER network. It goes on to identify recent developments within the international LTER program relating to data issues: increasing heterogeneities due to networking, integration of data from additional disciplines, and new technologies in a changing digital landscape. Information management experience within LTER provides one example of the recurrent balancing inherent to the work of data curation. It highlights (1) taking into account the extended temporal horizon of data care, (2) aligning support for data, science and information infrastructure, and (3) integrating site and network-level responsibilities. LTER contributes to the inquiry into how to manage the continuity of digital data and to our understanding of how to design a sustainable information infrastructure.",Digital Data Practices and the Long Term Ecological Research Program Growing Global,Edinburgh University,2008,,International Journal of Digital Curation,3,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1481,http://dx.doi.org/10.2218/ijdc.v3i2.58,"{'string_id': '10.2218/ijdc.v3i2.58', 'id_scheme': 'DOI'}","Ensuring the long-term usability of engineering informatics (EI) artifacts is a challenge, particularly for products with longer lifecycles than the computing hardware and software used for their design and manufacture. Addressing this challenge requires characterizing the nature of EI, defining metrics for EI sustainability, and developing methods for long-term EI curation. In this paper we highlight various issues related to long-term archival of EI and describe the work towards methods and metrics for sustaining EI. We propose an approach to enhance the Open Archival Information System (OAIS) functional model to incorporate EI sustainability criteria, Digital Object Prototypes (DOPs), and end user access requirements. We discuss the end user’s requirements from the point of view of reference, reuse and rationale – the “3Rs” – to better understand the level of granularity and abstractions required in the definition of engineering digital objects. Finally we present a proposed case study and experiment.",Sustaining Engineering Informatics: Toward Methods and Metrics for Digital Curation,Edinburgh University,2008,,International Journal of Digital Curation,3,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1482,http://dx.doi.org/10.2218/ijdc.v3i2.59,"{'string_id': '10.2218/ijdc.v3i2.59', 'id_scheme': 'DOI'}","We present performance data relating to the use of migration in a system we are creating to provide web access to heterogeneous document collections in legacy formats. Our goal is to enable sustained access to collections such as these when faced with increasing obsolescence of the necessary supporting applications and operating systems. Our system allows searching and browsing of the original files within their original contexts utilizing binary images of the original media. The system uses static and dynamic file migration to enhance collection browsing, and emulation to support both the use of legacy programs to access data and long-term preservation of the migration software. While we provide an overview of the architectural issues in building such a system, the focus of this paper is an in-depth analysis of file migration using data gathered from testing our software on 1,885 CD-ROMs and DVDs. These media are among the thousands of collections of social and scientific data distributed by the United States Government Printing Office (GPO) on legacy media (CD-ROM, DVD, floppy disk) under the Federal Depository Library Program (FDLP) over the past 20 years.",Migration Performance for Legacy Data Access,Edinburgh University,2008,,International Journal of Digital Curation,3,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1483,http://dx.doi.org/10.2218/ijdc.v4i1.72,"{'string_id': '10.2218/ijdc.v4i1.72', 'id_scheme': 'DOI'}"," The Digital Preservation Program of the California Digital Library (CDL) is engaged in a process of reinvention involving significant transformations of its outlook, effort, and infrastructure. This includes a re-articulation of its mission in terms of digital curation, rather than preservation; encouraging a programmatic, rather than a project-oriented approach to curation activities; and a renewed emphasis on services, rather than systems. This last shift was motivated by a desire to deprecate the centrality of the repository as place. Having the repository as the locus for curation activity has resulted in the deployment of a somewhat cumbersome monolithic system that falls short of desired goals for responsiveness to rapidly changing user needs and operational and administrative sustainability. The Program is pursuing a path towards a new curation environment based on the principle of devolving curation function to a set of small, simple, loosely coupled services. In considering this new infrastructure, the Program is relying upon a highly deliberative process starting from first principles drawn from library and archival science. This is followed by a stagewise progression of identifying core preservable values, devising strategies promoting those values, defining abstract services embodying those strategies, and, finally, developing systems that instantiate those services. This paper presents a snapshot of the Program's transformative efforts in its early phase.",Preservation Is Not a Place,Edinburgh University,2009,,International Journal of Digital Curation,4,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1484,http://dx.doi.org/10.2218/ijdc.v4i1.75,"{'string_id': '10.2218/ijdc.v4i1.75', 'id_scheme': 'DOI'}"," With new scientific instruments growing exponentially in their capability to generate research data, new infrastructure needs to be developed and deployed to allow researchers to effectively and securely manage their research data from collection, publication, and eventual dissemination to research communities.  In particular, researchers need to be able to easily acquire data from instruments, store and manage potentially large quantities of data, easily process the data, share research resources and work spaces with colleagues both inside and outside of their institution, search and discover across their accessible collections, and easily publish datasets and related research artefacts.  The ARCHER Project has developed production-ready generic e-Research infrastructure including: a Research Repository; Scientific Dataset Managers (both a web and desktop application); Distributed Integrated Multi-Sensor and Instrument Middleware; and a Collaborative Workspace Environment.  Institutions can selectively deploy these components to greatly assist their researchers in managing their research data.",ARCHER – e-Research Tools for Research Data Management,Edinburgh University,2009,,International Journal of Digital Curation,4,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1485,http://dx.doi.org/10.2218/ijdc.v4i1.76,"{'string_id': '10.2218/ijdc.v4i1.76', 'id_scheme': 'DOI'}"," The proliferation of Web, database and social networking technologies has enabled us to produce, publish and exchange digital assets at an enormous rate. This vast amount of information that is either digitized or born-digital needs to be collected, organized and preserved in a way that ensures that our digital assets and the information they carry remain available for future use. Digital curation has emerged as a new inter-disciplinary practice that seeks to set guidelines for disciplined management of information. In this paper we review two recent models for digital curation introduced by the Digital Curation Centre (DCC) and the Digital Curation Unit (DCU) of the Athena Research Centre. We then propose a fusion of the two models that highlights the need to extend the digital curation lifecycle by adding (a) provisions for the registration of usage experience, (b) a stage for knowledge enhancement and (c) controlled vocabularies used by convention to denote concepts, properties and relations. The objective of the proposed extensions is twofold: (i) to provide a more complete lifecycle model for the digital curation domain; and (ii) to provide a stimulus for a broader discussion on the research agenda.",DCC&U: An Extended Digital Curation Lifecycle Model,Edinburgh University,2009,,International Journal of Digital Curation,4,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1486,http://dx.doi.org/10.2218/ijdc.v4i1.77,"{'string_id': '10.2218/ijdc.v4i1.77', 'id_scheme': 'DOI'}"," There is now widespread recognition that data are a valuable long-term resource and that making them publicly available is a way to realise their potential value - both as part of the scholarly record or for re-use by others. The Research Information Network (RIN) report, To share or not to share: Publication and quality assurance of research data outputs (June 2008), investigates whether or not researchers make their research data available to others and the issues they encounter when doing so. Importantly, it seeks to do this by seeking the perspectives of researchers themselves. This paper reflects on how this relates to the more top-down literature on the subject. The discussion of the significance of the RIN's main findings is correlated to the four themes of the RIN report. Firstly, it discusses some distinctions in the types of data that should be shared and preserved and what needs to done to do so effectively. Secondly, it reflects on the motivations for and constraints on researchers publishing their data, and how funders and publishers can address them. Thirdly, it reviews some issues around how data are discovered, accessed and re-used. Finally, it discusses the scholarly and technical quality of published data.",The Publication of Research Data: Researcher Attitudes and Behaviour,Edinburgh University,2009,,International Journal of Digital Curation,4,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1487,http://dx.doi.org/10.2218/ijdc.v4i1.78,"{'string_id': '10.2218/ijdc.v4i1.78', 'id_scheme': 'DOI'}"," We characterize long-term preservation of digital content as an extended relay in time, in which repeated handoffs of information occur independently at every architectural layer: at the physical layer, where bits are handed off between storage systems; at the logical layer, where digital objects are handed off between repository systems; and at the administrative layer, where collections of objects and relationships are handed off between archives, curators, and institutions.  We examine the support of current preservation technologies for these handoffs, note shortcomings, and argue that some modest improvements would result in a ""relay-supporting"" preservation infrastructure, one that provides a baseline level of preservation by mitigating the risk of fundamental information loss.  Finally, we propose a series of tests to validate a relay-supporting infrastructure, including a second Archive Ingest and Handling Test (AIHT).",Relay-supporting Archives: Requirements and Progress,Edinburgh University,2009,,International Journal of Digital Curation,4,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1488,http://dx.doi.org/10.2218/ijdc.v4i1.79,"{'string_id': '10.2218/ijdc.v4i1.79', 'id_scheme': 'DOI'}"," In the future, a scholar or researcher will want to know that a digital object is trusted - that it is authentic and reliable.  Digital objects can be surrogates, resulting from a digitization process, or they can be objects whose only form is digital.  Much has been accomplished in existing open source digital library platforms to provide capabilities for preserving digital objects including now ubiquitous features such as persistent identifiers, integrity checks, audit trails, and versioning.  However, achieving a level of digital object authenticity will require a multi-dimensional approach involving policies, processes, and continued technological innovation.  This paper proposes steps that the institution can take to insure the availability of authentic digital objects in the future.  In this proposal, authenticity is based on definitions from archival diplomatics and relies on methods from public key cryptography for digitally signing an object with a secure time stamp. Trustworthy processes, re-definition of traditional roles, and the implementation of technologies to support authenticity are all required to meet the needs of digital scholarship.  Implementation and policy issues are discussed with specific attention to transformations required of the archival institution and the professional archivist.",An Institutional Framework for Creating Authentic Digital Objects,Edinburgh University,2009,,International Journal of Digital Curation,4,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1489,http://dx.doi.org/10.2218/ijdc.v4i1.80,"{'string_id': '10.2218/ijdc.v4i1.80', 'id_scheme': 'DOI'}"," Product Lifecycle Management (PLM) has become increasingly important in the engineering community over the last decade or so, due to the globalisation of markets and the rising popularity of products provided as services. It demands the efficient capture, representation, organisation, retrieval and reuse of product data over its entire life. Simultaneously, there is now a much greater reliance on CAD models for communicating designs to manufacturers, builders, maintenance crews and regulators, and for definitively expressing designs. Creating the engineering record digitally, however, presents problems not only for its long-term maintenance and accessibility - due in part to the rapid obsolescence of the hardware, software and file formats involved - but also for recording the evolution of designs, artefacts and products. We examine the curation and preservation requirements in PLM and suggest ways of alleviating the problems of sustaining CAD engineering models through the use of lightweight formats, layered annotation and the collection of Representation Information as defined in the Open Archival Information System (OAIS) Reference Model.  We describe two tools which have been specifically developed to aid in the curation of CAD engineering models in the context of PLM: Lightweight Models with Multilayered Annotation (LiMMA) and a Registry/Repository of Representation Information for Engineering (RRoRIfE).",Strategies for the Curation of CAD Engineering Models,Edinburgh University,2009,,International Journal of Digital Curation,4,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1490,http://dx.doi.org/10.2218/ijdc.v4i1.81,"{'string_id': '10.2218/ijdc.v4i1.81', 'id_scheme': 'DOI'}"," Increasing demand to manage and preserve 3-dimensional models for a variety of physical phenomena (e.g., building and engineering designs, computer games, or scientific visualizations) is creating new challenges for digital archives. Preserving 3D models requires identifying technical formats for the models that can be maintained over time, and the available formats offer different advantages and disadvantages depending on the intended future uses of the models. Additionally, the metadata required to manage 3D models is not yet standardized, and getting intellectual proposal rights for digital models is uncharted territory.  The FACADE Project at MIT is investigating these challenges in the architecture, engineering and construction (AEC) industry and has developed recommendations and systems to support digital archives in dealing with digital 3D models and related data. These results can also be generalized to other domains doing 3D modeling.",Curating Architectural 3D CAD Models,Edinburgh University,2009,,International Journal of Digital Curation,4,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1491,http://dx.doi.org/10.2218/ijdc.v4i1.82,"{'string_id': '10.2218/ijdc.v4i1.82', 'id_scheme': 'DOI'}"," Government's use of the Web in the UK is prolific and a wide range of services are now available though this channel. The government set out to address the problem that links from Hansard (the transcripts of Parliamentary debates) were not maintained over time and that therefore there was need for some long-term storage and stewardship of information, including maintaining access. Further investigation revealed that linking was key, not only in maintaining access to information, but also to the discovery of information. This resulted in a project that affects the entire  government Web estate, with a solution leveraging the basic building blocks of the Internet (DNS) and the Web (HTTP and URIs) in a pragmatic way, to ensure that an infrastructure is in place to provide access to important information both now and in the future. ",UK Government Web Continuity: Persisting Access through Aligning Infrastructures,Edinburgh University,2009,,International Journal of Digital Curation,4,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1492,http://dx.doi.org/10.2218/ijdc.v4i1.83,"{'string_id': '10.2218/ijdc.v4i1.83', 'id_scheme': 'DOI'}"," This paper will describe the genesis and realisation of the Australian National Data Service (ANDS). It will commence by outlining the context within which ANDS was conceived, both in the international research and Australian research support domains. It will then describe the process that brought about the ANDS vision and the principles that informed the realisation of that vision. The paper will then outline each of the four ANDS programs (Developing Frameworks, Providing Utilities, Seeding the Commons, and Building Capabilities) while also discussing particular items of note about the approach ANDS is taking. The paper concludes by briefly examining related work in the UK and US.",Design and Implementation of the Australian National Data Service,Edinburgh University,2009,,International Journal of Digital Curation,4,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1493,http://dx.doi.org/10.2218/ijdc.v4i2.90,"{'string_id': '10.2218/ijdc.v4i2.90', 'id_scheme': 'DOI'}","Scientific researchers today frequently package measurements and associated metadata as digital datasets in anticipation of storage in data repositories. Through the lens of environmental data stewardship, we consider the data repository as an organizational element central to data curation. One aspect of non-commercial repositories, their distance-from-origin of the data, is explored in terms of near and remote categories. Three idealized repository types are distinguished – local, center, and archive - paralleling research, resource, and reference collection categories respectively. Repository type characteristics such as scope, structure, and goals are discussed. Repository similarities in terms of roles, activities and responsibilities are also examined. Data stewardship is related to care of research data and responsible scientific communication supported by an infrastructure that coordinates curation activities; data curation is defined as a set of repeated and repeatable activities focusing on tending data and creating data products within a particular arena. The concept of “sphere-of-context” is introduced as an aid to distinguishing repository types. Conceptualizing a “web-of-repositories” accommodates a variety of repository types and represents an ecologically inclusive approach to data curation.",Data Stewardship: Environmental Data Curation and a Web-of-Repositories,Edinburgh University,2009,,International Journal of Digital Curation,4,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1494,http://dx.doi.org/10.2218/ijdc.v4i2.91,"{'string_id': '10.2218/ijdc.v4i2.91', 'id_scheme': 'DOI'}","This paper presents LORE (Literature Object Re-use and Exchange), a light-weight tool which is designed to allow literature scholars and teachers to author, edit and publish compound information objects encapsulating related digital resources and bibliographic records. LORE enables users to easily create OAI-ORE-compliant compound objects, which build on the IFLA FRBR model, and also enables them to describe and publish them to an RDF repository as Named Graphs. Using the tool, literary scholars can create typed relationships between individual atomic objects using terms from a bibliographic ontology and can attach metadata to the compound object. This paper describes the implementation and user interface of the LORE tool, as developed within the context of an ongoing case study being conducted in collaboration with AustLit: The Australian Literature Resource, which focuses on compound objects for teaching and research within the Australian literature studies community.",A Compound Object Authoring and Publishing Tool for Literary Scholars based on the IFLA-FRBR,Edinburgh University,2009,,International Journal of Digital Curation,4,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1495,http://dx.doi.org/10.2218/ijdc.v4i2.92,"{'string_id': '10.2218/ijdc.v4i2.92', 'id_scheme': 'DOI'}","This paper discusses the importance of a particular approach to building and sustaining digital content preservation infrastructures for cultural memory organizations (CMOs), namely distributed approaches that are cooperatively maintained by CMOs (rather than centralized approaches managed by agencies external to CMOs), and why this approach may fill a gap in capabilities for those CMOs actively digitizing historical and cultural content (rather than scientific data). Initial findings are presented from an early organizational effort (the MetaArchive Cooperative) that seeks to fill this gap for CMOs. The paper situates these claims in the larger context of selected exemplars of DP efforts in both the United States and the United Kingdom that are seeking to develop effective DP models in an attempt to recognize those organizational aspects (such as the governmental frameworks, cultural backgrounds, and other differences in emphasis) that are UK and US-specific.",Comparison of Strategies and Policies for Building Distributed Digital Preservation Infrastructure: Initial Findings from the MetaArchive Cooperative,Edinburgh University,2009,,International Journal of Digital Curation,4,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1496,http://dx.doi.org/10.2218/ijdc.v4i2.93,"{'string_id': '10.2218/ijdc.v4i2.93', 'id_scheme': 'DOI'}"," DCC DIFFUSE Standards Frameworks aims to offer domain specific advice on standards relevant to digital preservation and curation, to help curators identify which standards they should be using and where they can be appropriately implemented, to ensure authoritative digital material. The Project uses the DCC Curation Lifecycle Model and Web 2.0 technology, to visually present standards frameworks for a number of disciplines. The Digital Curation Centre (DCC) is actively working with a different relevant organisations to present searchable frameworks of standards, for a number of domains. These include digital repositories, records management, the geo-information sector, archives and the museum sector. Other domains, such as e-science, will shortly be investigated.",DCC DIFFUSE Standards Frameworks: A Standards Path through the Curation Lifecycle,Edinburgh University,2009,,International Journal of Digital Curation,4,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1497,http://dx.doi.org/10.2218/ijdc.v4i2.94,"{'string_id': '10.2218/ijdc.v4i2.94', 'id_scheme': 'DOI'}","Climate and Weather are of increasing interest to the scientific community and the general public. Data curation and stewardship are essential building blocks in the science community’s quest to better understand how natural climate and weather systems behave and how activities of human civilization are altering the natural system. Rudimentary observations of the atmosphere and ocean have been collected for over one hundred years and proxity measurements of the climate can trace our planet’s climatic history for millions of years. These observations coupled with the rapid advances in technology, such as powerful computers, rapid access to massive amounts of data, and satellite observations, have allowed innovative techniques to be used to understand and predict the planet’s climate and weather.",Data Curation in Climate and Weather: Transforming Our Ability to Improve Predictions through Global Knowledge Sharing,Edinburgh University,2009,,International Journal of Digital Curation,4,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1498,http://dx.doi.org/10.2218/ijdc.v4i2.95,"{'string_id': '10.2218/ijdc.v4i2.95', 'id_scheme': 'DOI'}","The Landesarchiv (State Archive) of Baden-Württemberg has designed and implemented a metadata concept for digital content covering a heterogenous range of digital-born and digitised material. Special attention was given to matters of authenticity and to economic ingest and dissemination methods under the requirements of a public archive. This paper describes the outcome of metadata discussions during the implementation period of the DIMAG repository. It treats integration of the repository’s architecture with the archival classification concept, measures for long-term accessibility, the creation of adapted metadata placement, and provisions for exchange with other applications for ingest and use. The deliberately short list of metadata elements is included in this paper. Some existing standards have been evaluated under a real use environment; this paper also introduces modifications applied to them in the project context.",One for Many: A Metadata Concept for Mixed Digital Content at a State Archive,Edinburgh University,2009,,International Journal of Digital Curation,4,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1499,http://dx.doi.org/10.2218/ijdc.v4i2.96,"{'string_id': '10.2218/ijdc.v4i2.96', 'id_scheme': 'DOI'}","This paper describes a technique for embedding document metadata, and potentially other semantic references inline in word processing documents, which the authors have implemented with the help of a software development team. Several assumptions underly the approach; It must be available across computing platforms and work with both Microsoft Word (because of its user base) and OpenOffice.org (because of its free availability). Further the application needs to be acceptable to and usable by users, so the initial implementation covers only small number of features, which will only be extended after user-testing. Within these constraints the system provides a mechanism for encoding not only simple metadata, but for inferring hierarchical relationships between metadata elements from a ‘flat’ word processing file.The paper includes links to open source code implementing the techniques as part of a broader suite of tools for academic writing. This addresses tools and software, semantic web and data curation, integrating curation into research workflows and will provide a platform for integrating work on ontologies, vocabularies and folksonomies into word processing tools.",Embedding Metadata and Other Semantics in Word Processing Documents,Edinburgh University,2009,,International Journal of Digital Curation,4,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1500,http://dx.doi.org/10.2218/ijdc.v4i2.97,"{'string_id': '10.2218/ijdc.v4i2.97', 'id_scheme': 'DOI'}"," Recent developments in the storage industry have resulted in the creation of an industry standard application programmer’s interface (API) known as XAM, the eXtensible Access Method. The XAM API focuses on the creation and management of reference information (otherwise known as fixed content). Storage vendors supporting the XAM API will provide new benefits to applications that are creating and managing large amounts of fixed content. The benefits described by this paper merit consideration and research by developers creating applications for Digital Curators.",The eXtensible Access Method (XAM) Standard,Edinburgh University,2009,,International Journal of Digital Curation,4,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1501,http://dx.doi.org/10.2218/ijdc.v4i3.114,"{'string_id': '10.2218/ijdc.v4i3.114', 'id_scheme': 'DOI'}","In the highly competitive engineering industry, product innovations are created with the help of a product lifecycle management (PLM) tool chain. In order to support fast-paced product development, a major company goal is the reuse of product designs and product descriptions. Due to the product’s complexity, the design of a product not only consists of geometry data but also of valuable engineering knowledge that is created during the various PLM phases. The need to preserve such intellectual capital leads engineering companies to introduce knowledge management and archiving their machine-readable formal representation. However, archived knowledge is in danger of becoming unusable since it is very likely that knowledge semantics and knowledge representation will evolve over long time periods, for example during the 50 operational years of some products. Knowledge evolution and knowledge representation technology changes are crucial issues since a reuse of the archived product information can only be ensured if its rationale and additional knowledge are interpretable with future software and technologies. Therefore, in order to reuse design data fully, knowledge about the design must also be migrated to be interoperable with future design systems and knowledge representation methods. This paper identifies problems, issues, requirements, challenges and solutions that arise while tackling the long-term preservation of engineering knowledge.",Enabling Product Design Reuse by Long-term Preservation of Engineering Knowledge,Edinburgh University,2009,,International Journal of Digital Curation,4,3,"[{'value': '1746-8256', 'type': 'electronic'}]"
1502,http://dx.doi.org/10.2218/ijdc.v4i3.115,"{'string_id': '10.2218/ijdc.v4i3.115', 'id_scheme': 'DOI'}","Drawing on the final report on a recent series of case studies in the life sciences at the University of Edinburgh, this paper explores the attitudes and perceptions of researchers towards data sharing and contrasts these with the policies of the major research funders. Notwithstanding economic, technical and cultural inhibitors, the general ethos in the Life Sciences is one of support to the principle of data sharing. However, this position is subject to a complex range of qualifications, not least the crucial need for sharing through collaboration. The kind of generic vision for data sharing that is currently promoted by national agencies is judged to be neither productive nor effective.  Only close engagement with research practitioners in the identification of bottom-up strategies that preserve the exercise of informed choice - a fundamental and persistent element of scientific research - will produce change on a national scale.",Multi-scale Data Sharing in the Life Sciences: Some Lessons for Policy Makers,Edinburgh University,2009,,International Journal of Digital Curation,4,3,"[{'value': '1746-8256', 'type': 'electronic'}]"
1503,http://dx.doi.org/10.2218/ijdc.v4i3.116,"{'string_id': '10.2218/ijdc.v4i3.116', 'id_scheme': 'DOI'}","The curation of scientific research data at U.S. universities is a story of enterprising individuals and of incremental progress. A small number of libraries and data centers who see the possibilities of becoming “digital information management centers” are taking entrepreneurial steps to extend beyond their traditional information assets and include managing scientific and scholarly research data. The Georgia Institute of Technology (GT) has had a similar development path toward a data curation program based in its library. This paper will articulate GT’s program development, which the author offers as an experience common in U.S. universities. The main characteristic is a program devoid of top-level mandates and incentives, but rich with independent, “bottom-up” action. The paper will address program antecedents and context, inter-institutional partnerships that advance the library’s curation program, library organizational developments, partnerships with campus research communities, and a proposed model for curation program development. It concludes that despite the clear need for data curation put forth by researchers such as the groups of neuroscientists and bioscientists referenced in this paper, the university experience examined suggests that gathering resources for developing data curation programs at the institutional level is proving to be a quite onerous. However, and in spite of the challenges, some U.S. research universities are beginning to establish perceptible data curation programs.",Data Curation Program Development in U.S. Universities: The Georgia Institute of Technology Example,Edinburgh University,2009,,International Journal of Digital Curation,4,3,"[{'value': '1746-8256', 'type': 'electronic'}]"
1504,http://dx.doi.org/10.2218/ijdc.v4i3.117,"{'string_id': '10.2218/ijdc.v4i3.117', 'id_scheme': 'DOI'}","This paper presents a brief literature review and then introduces the methods, design, and construction of the Data Curation Profile, an instrument that can be used to provide detailed information on particular data forms that might be curated by an academic library. These data forms are presented in the context of the related sub-disciplinary research area, and they provide the flow of the research process from which these data are generated. The profiles also represent the needs for data curation from the perspective of the data producers, using their own language. As such, they support the exploration of data curation across different research domains in real and practical terms. With the sponsorship of the Institute of Museum and Library Services, investigators from Purdue University and the University of Illinois interviewed 19 faculty subjects to identify needs for discovery, access, preservation, and reuse of their research data. For each subject, a profile was constructed that includes information about his or her general research, data forms and stages, value of data, data ingest, intellectual property, organization and description of data, tools, interoperability, impact and prestige, data management, and preservation. Each profile also presents a specific dataset supplied by the subject to serve as a concrete example. The Data Curation Profiles are being published to a public wiki for questions and discussion, and a blank template will be disseminated with guidelines for others to create and share their own profiles. This study was conducted primarily from the viewpoint of librarians interacting with faculty researchers; however, it is expected that these findings will complement a wide variety of data curation research and practice outside of librarianship and the university environment.",Constructing Data Curation Profiles,Edinburgh University,2009,,International Journal of Digital Curation,4,3,"[{'value': '1746-8256', 'type': 'electronic'}]"
1505,http://dx.doi.org/10.2218/ijdc.v4i3.120,"{'string_id': '10.2218/ijdc.v4i3.120', 'id_scheme': 'DOI'}"," Developmental Gene Expression Map (DGEMap) is an EU-funded Design Study, which will accelerate an integrated European approach to gene expression in early human development. As part of this design study, we have had to address the challenges and issues raised by the long-term curation of such a resource. As this project is primarily one of data creators, learning about curation, we have been looking at some of the models and tools that are already available in the digital curation field in order to inform our thinking on how we should proceed with curating DGEMap. This has led us to uncover a wide range of resources for data creators and curators alike. Here we will discuss the future curation of DGEMap as a case study. We believe our experience could be instructive to other projects looking to improve the curation and management of their data.",Using the DCC Lifecycle Model to Curate a Gene Expression Database: A Case Study,Edinburgh University,2009,,International Journal of Digital Curation,4,3,"[{'value': '1746-8256', 'type': 'electronic'}]"
1506,http://dx.doi.org/10.2218/ijdc.v4i3.124,"{'string_id': '10.2218/ijdc.v4i3.124', 'id_scheme': 'DOI'}"," This paper describes how the Australian National Data Services (ANDS) is designing systems to support data sharing and Re-use. The paper commences with an overview of the setting for ANDS, before introducing ANDS itself. The paper then structures its discussion of ANDS services for Re-use in terms of the ANDS Data Sharing Verbs: Create, Store, Describe, Identify, Register, Discover, Access and Exploit. For each of the data verbs, a rationale for its importance is provided together with a description of how it is being implemented by ANDS. The paper concludes by arguing for the data verbs approach as a useful way to design and structure flexible services in a heterogenous environment.",Designing for Discovery and Re-Use: the ‘ANDS Data Sharing Verbs’ Approach to Service Decomposition,Edinburgh University,2009,,International Journal of Digital Curation,4,3,"[{'value': '1746-8256', 'type': 'electronic'}]"
1507,http://dx.doi.org/10.2218/ijdc.v4i3.125,"{'string_id': '10.2218/ijdc.v4i3.125', 'id_scheme': 'DOI'}"," As storage costs drop, storage is becoming the lowest cost in a digital repository – and the biggest risk. We examine current modelling of costs and risks in digital preservation, concentrating on the Total Cost of Risk when using digital storage systems for preserving audiovisual material. We review the vital role of storage and show how planning for long-term preservation of data should consider the risks involved in using digital storage technology. Gaps in information necessary for accurate modelling – and planning – are presented. We call for new functionality to support recovery of files with errors, to eliminate the all-or-nothing approach of current IT systems, which in turn reduces the impact of failures of digital storage technology and mitigates against loss of digital data.",The Significance of Storage in the “Cost of Risk” of Digital Preservation,Edinburgh University,2009,,International Journal of Digital Curation,4,3,"[{'value': '1746-8256', 'type': 'electronic'}]"
1508,http://dx.doi.org/10.2218/ijdc.v4i3.126,"{'string_id': '10.2218/ijdc.v4i3.126', 'id_scheme': 'DOI'}","The CIA World Factbook is a prime example of a curated database – a database that is constructed and maintained with a great deal of human effort in collecting, verifying, and annotating data. Preservation of old versions of the Factbook is important for verification of citations; it is also essential for anyone interested in the history of the data such as demographic change. Although the Factbook has been published, both physically and electronically, only for the past 30 years, we appear in danger of losing this history. This paper investigates the issues involved in capturing the history of an evolving database and its application to the CIA World Factbook. In particular it shows that there is substantial added value to be gained by preserving databases in such a way that questions about the change in data, (longitudinal queries) can be readily answered. Within this paper, we describe techniques for recording change in a curated database and we describe novel techniques for querying the change. Using the example of this archived curated database, we discuss the extent to which the accepted practices and terminology of archiving, curation and digital preservation apply to this important class of digital artefacts.",Curating the CIA World Factbook,Edinburgh University,2009,,International Journal of Digital Curation,4,3,"[{'value': '1746-8256', 'type': 'electronic'}]"
1509,http://dx.doi.org/10.2218/ijdc.v4i3.127,"{'string_id': '10.2218/ijdc.v4i3.127', 'id_scheme': 'DOI'}","ESA-ESRIN, the European Space Agency Centre for Earth Observation (EO), is the largest European EO data provider and operates as the reference European centre for EO payload data exploitation. EO Space Missions provide global coverage of the Earth across both space and time generating on a routine continuous basis huge amounts of data (from a variety of sensors) that need to be acquired, processed, elaborated, appraised and archived by dedicated systems. Long-term Preservation of these data and of the ability to discover, access and process them is a fundamental issue and a major challenge at programmatic, technological and operational levels.Moreover these data are essential for scientists needing broad series of data covering long time periods and from many sources. They are used for many types of investigations including ones of international importance such as the study of the Global Change and the Global Monitoring for Environment and Security (GMES) Program. Therefore it is of primary importance not only to guarantee easy accessibility of historical data but also to ensure users are able to understand and use them; in fact data interpretation can be even more complicated given the fact that scientists may not have (or may not have access to) the right knowledge to interpret these data correctly.To satisfy these requirements, the European Space Agency (ESA), in addition to other internal initiatives, is participating in several EU-funded projects such as CASPAR (Cultural, Artistic, and Scientific knowledge for Preservation, Access and Retrieval), which is building a framework to support the end-to-end preservation lifecycle for digital information, based on the OAIS reference model, with a strong focus on the preservation of the knowledge associated with data.In the CASPAR Project ESA plays the role of both user and infrastructure provider for one of the scientific testbeds, putting into effect dedicated scenarios with the aim of validating the CASPAR solutions in the Earth Science domain. The other testbeds are in the domains of Cultural Heritage and of Contemporary Performing Arts; together they provide a severe test of preservation tools and techniques.In the context of the current ESA overall strategies carried out in collaboration with European EO data owners/providers, entities and institutions which have the objective of guaranteeing long-term preservation of EO data and knowledge, this paper will focus on the ESA participation and contribution to the CASPAR Project, describing in detail the implementation of the ESA scientific testbed.",Long-term Preservation of Earth Observation Data and Knowledge in ESA through CASPAR,Edinburgh University,2009,,International Journal of Digital Curation,4,3,"[{'value': '1746-8256', 'type': 'electronic'}]"
1510,http://dx.doi.org/10.2218/ijdc.v5i1.140,"{'string_id': '10.2218/ijdc.v5i1.140', 'id_scheme': 'DOI'}","Digital preservation aims to keep digital objects accessible over long periods of time, ensuring the authenticity and integrity of these digital objects. In such complex environments, Risk Management is a key factor in assuring the normal behaviour of systems over time. Currently, the digital preservation arena commonly uses Risk Management concepts to assess repositories. In this paper, we intend to go further and propose a perspective where Risk Management can be used not only to assess existing solutions, but also to conceive digital preservation environments. Thus, we propose a Risk Management-based approach to design and assess digital preservation environments, including:• the definition of context and identification of strategic objectives to determine specific requirements and characterize which consequences are acceptable within the identified context;• the identification, analysis and evaluation of threats and vulnerabilities that may affect the normal behaviour of a specific business or the achievement of the goals and conformance to the requirements identified in the context characterization; and, • definition of actions to deal with the risks associated with the identified threats and vulnerabilities.We generalize and survey the main requirements, threats, vulnerabilities and techniques that can be applied in the scope of digital preservation. ",Designing Digital Preservation Solutions: A Risk Management-Based Approach,Edinburgh University,2010,,International Journal of Digital Curation,5,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1511,http://dx.doi.org/10.2218/ijdc.v5i1.141,"{'string_id': '10.2218/ijdc.v5i1.141', 'id_scheme': 'DOI'}","This paper reports on research of scholarly research practices and requirements conducted in the context of the Preparing DARIAH European e-Infrastructures project, with a view to ensuring current and future fitness for purpose of the planned digital infrastructure, services and tools. It summarises the findings of earlier research, primarily from the field of human information behaviour as applied in scholarly work, it presents a conceptual perspective informed by cultural-historical activity theory, it introduces briefly a formal conceptual model for scholarly research activity compliant with CIDOC CRM, it describes the plan of work and methodology of an empirical research project based on open-questionnaire interviews with arts and humanities researchers, and presents illustrative examples of segmentation, tagging and initial conceptual analysis of the empirical evidence. Finally, it presents plans for future work, consisting, firstly, of a comprehensive re-analysis of interview segments within the framework of the scholarly research activity model, and, secondly, of the integration of this analysis with the extended digital curation process model we presented in earlier work.",Understanding the Information Requirements of Arts and Humanities Scholarship,Edinburgh University,2010,,International Journal of Digital Curation,5,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1512,http://dx.doi.org/10.2218/ijdc.v5i1.142,"{'string_id': '10.2218/ijdc.v5i1.142', 'id_scheme': 'DOI'}","Towards Interoperable Preservation Repositories (TIPR) is a project funded by the Institute of Museum and Library Services to create and test a Repository eXchange Package (RXP). The package will make it possible to transfer complex digital objects between dissimilar preservation repositories.  For reasons of redundancy, succession planning and software migration, repositories must be able to exchange copies of archival information packages with each other. Every different repository application, however, describes and structures its archival packages differently. Therefore each system produces dissemination packages that are rarely understandable or usable as submission packages by other repositories. The RXP is an answer to that mismatch. Other solutions for transferring packages between repositories focus either on transfers between repositories of the same type, such as DSpace-to-DSpace transfers, or on processes that rely on central translation services.  Rather than build translators between many dissimilar repository types, the TIPR project has defined a standards-based package of metadata files that can act as an intermediary information package, the RXP, a lingua franca all repositories can read and write.",Towards Interoperable Preservation Repositories: TIPR,Edinburgh University,2010,,International Journal of Digital Curation,5,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1513,http://dx.doi.org/10.2218/ijdc.v5i1.143,"{'string_id': '10.2218/ijdc.v5i1.143', 'id_scheme': 'DOI'}","Quality management is an essential part in creating a trustworthy digital archive. The German network of expertise in Digital long-term preservation (nestor), in cooperation with the German Institute for Standards (DIN), has undertaken a small study to analyse systematically the relevance and usage of quality management standards for long-term preservation and to filter out the specific standardisation need for digital archives. This paper summarises the results of the study. It gives an overview on the differences in understanding the task “quality management” within different organisations and how they carry out appropriate measures, such as documentation, transparency, adequacy, and measureability in order to demonstrate the trustworthiness of their digital archive.",The Use of Quality Management Standards in Trustworthy Digital Archives,Edinburgh University,2010,,International Journal of Digital Curation,5,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1514,http://dx.doi.org/10.2218/ijdc.v5i1.144,"{'string_id': '10.2218/ijdc.v5i1.144', 'id_scheme': 'DOI'}","Interactive fiction and video games are part of our cultural heritage. As original systems cease to work because of hardware and media failures, methods to preserve obsolete video games for future generations have to be developed. The public interest in early video games is high, as exhibitions, regular magazines on the topic and newspaper articles demonstrate. Moreover, games considered to be classic are rereleased for new generations of gaming hardware. However, with the rapid development of new computer systems, the way games look and are played changes constantly. When trying to preserve console video games one faces problems of classified development documentation, legal aspects and extracting the contents from original media like cartridges with special hardware. Furthermore, special controllers and non-digital items are used to extend the gaming experience making it difficult to preserve the look and feel of console video games.This paper discusses strategies for the digital preservation of console video games. After a short overview of console video game systems, there follows an introduction to digital preservation and related work in common strategies for digital preservation and preserving interactive art. Then different preservation strategies are described with a specific focus on emulation. Finally a case study on console video game preservation is shown which uses the Planets preservation planning approach for evaluating preservation strategies in a documented decision-making process. Experiments are carried out to compare different emulators as well as other approaches, first for a single console video game system, then for different console systems of the same era and finally for systems of all eras. Comparison and discussion of results show that, while emulation works very well in principle for early console video games, various problems exist for the general use as a digital preservation alternative. We show what future work has to be done to tackle these problems.",Keeping the Game Alive: Evaluating Strategies for the Preservation of Console Video Games,Edinburgh University,2010,,International Journal of Digital Curation,5,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1515,http://dx.doi.org/10.2218/ijdc.v5i1.145,"{'string_id': '10.2218/ijdc.v5i1.145', 'id_scheme': 'DOI'}","Software preservation has not had detailed consideration as a research topic or in practical application. In this paper, we present a conceptual framework to capture and organise the main notions of software preservation, which are required for a coherent and comprehensive approach.  This framework has three main aspects. Firstly a discussion of what it means to preserve software via a performance model which considers how a software artefact can be rebuilt from preserved components and can then be seen to be representative of the original software product. Secondly the development of a model of software artefacts, describing the basic components of all software, loosely based on the FRBR model for representing digital artefacts and their history within a library context. Finally, the definition and categorisation of the properties of software artefacts which are required to ensure that the software product has been adequately preserved. These are broken down into a number of categories and related to the concepts defined in the OAIS standard. We also discuss our experience of recording these preservation properties for a number of BADC software products, which arose from a series of case studies conducted to evaluate the software preservation framework, and also briefly describe the SPEQS toolkit, a tool to capture software preservation properties within a software development.",A Framework for Software Preservation,Edinburgh University,2010,,International Journal of Digital Curation,5,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1516,http://dx.doi.org/10.2218/ijdc.v5i1.146,"{'string_id': '10.2218/ijdc.v5i1.146', 'id_scheme': 'DOI'}","In this paper, we present the Core Scientific Metadata Model (CSMD), a model for the representation of scientific study metadata developed within the Science & Technology Facilities Council (STFC) to represent the data generated from scientific facilities. The model has been developed to allow management of and access to the data resources of the facilities in a uniform way, although we believe that the model has wider application, especially in areas of “structural science” such as chemistry, materials science and earth sciences. We give some motivations behind the development of the model, and an overview of its major structural elements, centred on the notion of a scientific study formed by a collection of specific investigations. We give some details of the model, with the description of each investigation associated with a particular experiment on a sample generating data, and the associated data holdings are then mapped to the investigation with the appropriate parameters. We then go on to discuss the instantiation of the metadata model within a production quality data management infrastructure, the Information CATalogue (ICAT), which has been developed within STFC for use in large-scale photon and neutron sources. Finally, we give an overview of the relationship between CSMD, and other initiatives, and give some directions for future developments.     ",Using a Core Scientific Metadata Model in Large-Scale Facilities,Edinburgh University,2010,,International Journal of Digital Curation,5,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1517,http://dx.doi.org/10.2218/ijdc.v5i1.147,"{'string_id': '10.2218/ijdc.v5i1.147', 'id_scheme': 'DOI'}","The Chronopolis Digital Preservation Initiative, one of the Library of Congress’ latest efforts to collect and preserve at-risk digital information, has completed its first year of service as a multi-member partnership to meet the archival needs of a wide range of domains.Chronopolis is a digital preservation data grid framework developed by the San Diego Supercomputer Center (SDSC) at UC San Diego, the UC San Diego Libraries (UCSDL), and their partners at the National Center for Atmospheric Research (NCAR) in Colorado and the University of Maryland's Institute for Advanced Computer Studies (UMIACS).Chronopolis addresses a critical problem by providing a comprehensive model for the cyberinfrastructure of collection management, in which preserved intellectual capital is easily accessible, and research results, education material, and new knowledge can be incorporated smoothly over the long term. Integrating digital library, data grid, and persistent archive technologies, Chronopolis has created trusted environments that span academic institutions and research projects, with the goal of long-term digital preservation.A key goal of the Chronopolis project is to provide cross-domain collection sharing for long-term preservation. Using existing high-speed educational and research networks and mass-scale storage infrastructure investments, the partnership is leveraging the data storage capabilities at SDSC, NCAR, and UMIACS to provide a preservation data grid that emphasizes heterogeneous and highly redundant data storage systems.In this paper we will explore the major themes within Chronopolis, including:a) The philosophy and theory behind a nationally federated data grid for preservation. b) The core tools and technologies used in Chronopolis. c) The metadata schema that is being developed within Chronopolis for all of the data elements. d) Lessons learned from the first year of the project.e) Next steps in digital preservation using Chronopolis: how we plan to strengthen and broaden our network with enhanced services and new customers.",Chronopolis Digital Preservation Network,Edinburgh University,2010,,International Journal of Digital Curation,5,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1518,http://dx.doi.org/10.2218/ijdc.v5i1.148,"{'string_id': '10.2218/ijdc.v5i1.148', 'id_scheme': 'DOI'}","For years, discussions of digital preservation have routinely featured comments such as “bit preservation is a solved problem; the real issues are ...”. Indeed, current digital storage technologies are not just astoundingly cheap and capacious, they are astonishingly reliable. Unfortunately, these attributes drive a kind of “Parkinson’s Law” of storage, in which demands continually push beyond the capabilities of systems implementable at an affordable price. This paper is in four parts:Claims, reviewing a typical claim of storage system reliability, showing that it provides no useful information for bit preservation purposes.Theory, proposing “bit half-life” as an initial, if inadequate, measure of bit preservation performance, expressing bit preservation requirements in terms of it, and showing that the requirements being placed on bit preservation systems are so onerous that the experiments required to prove that a solution exists are not feasible.Practice, reviewing recent research into how well actual storage systems preserve bits, showing that they fail to meet the requirements by many orders of magnitude.Policy, suggesting ways of dealing with this unfortunate situation. ",Bit Preservation: A Solved Problem?,Edinburgh University,2010,,International Journal of Digital Curation,5,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1519,http://dx.doi.org/10.2218/ijdc.v5i1.149,"{'string_id': '10.2218/ijdc.v5i1.149', 'id_scheme': 'DOI'}","Metadata extraction is a critical aspect of ingestion of collections into digital archives and libraries. A method for automatically recognizing document types and extracting metadata from digital records has been developed. The method is based on a method for automatically annotating semantic categories such as person’s names, job titles, dates, and postal addresses that may occur in a record. It extends this method by using the semantic annotations to identify the intellectual elements of a document’s form, parsing these elements using context-free grammars that define documentary forms, and interpreting the elements of the form of the document to identify metadata such as the chronological date, author(s), addressee(s), and topic. Context-free grammars were developed for fourteen of the documentary forms occurring in Presidential records. In an experiment, the document type recognizer successfully recognized the documentary form and extracted the metadata of two-thirds of the records in a series of Presidential e-records containing twenty-one document types.",Grammar-Based Recognition of Documentary Forms and Extraction of Metadata,Edinburgh University,2010,,International Journal of Digital Curation,5,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1520,http://dx.doi.org/10.2218/ijdc.v5i1.150,"{'string_id': '10.2218/ijdc.v5i1.150', 'id_scheme': 'DOI'}","Emulation is frequently discussed as a failsafe preservation strategy for born-digital documents that depend on contemporaneous software for access (Rothenberg, 2000). Yet little has been written about the contextual knowledge required to successfully use such software. The approach we advocate is to preserve necessary contextual information through scripts designed to control the legacy environment, and created during the preservation workflow. We describe software designed to minimize dependence on this knowledge by offering automated configuration and execution of emulated environments. We demonstrate that even simple scripts can reduce impediments to casual use of the digital objects being preserved. We describe tools to automate the remote use of preserved objects on local emulation environments.  This can help eliminate both a dependence on physical reference workstations at preservation institutions, and provide users accessing materials over the web with simplified, easy-to-use environments. Our implementation is applied to examples from an existing collection of over 4,000 virtual CD-ROM images containing thousands of custom binary executables.",Assisted Emulation for Legacy Executables,Edinburgh University,2010,,International Journal of Digital Curation,5,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1521,http://dx.doi.org/10.2218/ijdc.v6i1.168,"{'string_id': '10.2218/ijdc.v6i1.168', 'id_scheme': 'DOI'}","For millions of legacy documents, correct rendering depends upon resources such as fonts that are not generally embedded within the document structure. Yet there is a significant risk of information loss due to missing or incorrectly substituted fonts. Large document collections depend on thousands of unique fonts not available on a common desktop workstation, which typically has between 100 and 200 fonts. Silent substitution of fonts, performed by applications such as Microsoft Office, can yield poorly rendered documents. In this paper we use a collection of 230,000 Word documents to assess the difficulty of matching font requirements with a database of fonts. We describe the identifying information contained in common font formats, font requirements stored in Word documents, the API provided by Windows to support font requests by applications, the documented substitution algorithms used by Windows when requested fonts are not available, and the ways in which support software might be used to control font substitution in a preservation environment.",Born Broken: Fonts and Information Loss in Legacy Digital Documents,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1522,http://dx.doi.org/10.2218/ijdc.v6i1.169,"{'string_id': '10.2218/ijdc.v6i1.169', 'id_scheme': 'DOI'}","UK data centres are an important part of efforts to gain maximum value from research data. However, if they are to operate effectively, the services that they provide must be based upon an understanding of researchers' practices and needs. Furthermore, in order to build a case for ongoing funding, data centres must be able to demonstrate their value to researchers work and, increasingly, their contribution to wider political ""impact"" agendas. This paper presents the findings of a survey of users of five UK data centres. It suggests that research data centres are highly valued by their users. Benefits appear to be particularly strong around improving research efficiency, especially access to data. Data centres are less important in terms of stimulating novel research questions. Despite a few interesting cases of observable impact, in the main it remains difficult to understand the wider reach of research which draws upon data centre resources.",Use and Impact of UK Research Data Centres,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1523,http://dx.doi.org/10.2218/ijdc.v6i1.170,"{'string_id': '10.2218/ijdc.v6i1.170', 'id_scheme': 'DOI'}","MESSAGE (Mobile Environmental Sensing System Across Grid Environments) was an ambitious, multi-partner, interdisciplinary e-Science research project, jointly funded by the Engineering and Physical Sciences Research Council (EPSRC) and the UK Department for Transport (DfT) between 2006 and 2009. It aimed to develop and demonstrate the potential of diverse, low cost sensors to provide heterogeneous data for the planning, management and control of the environmental impacts of transport activity at urban, regional and national level. During the last year of the project, the Digital Curation Centre (DCC) interviewed and observed members of the project team in order to identify and analyse key aspects of their data-related activities, recording attitudes towards the data that they create and/or re-use. This paper describes the major issues identified over the course of the case study, which are presented in parallel with the perspectives of the project team in order to demonstrate the multiplicity of views that may be projected onto a single dataset. It concludes with a contextualisation of the case study's themes with those of a number of contemporary reports.",The Milieu and the MESSAGE: Talking to Researchers about Data Curation Issues in a Large and Diverse e-Science Project,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1524,http://dx.doi.org/10.2218/ijdc.v6i1.171,"{'string_id': '10.2218/ijdc.v6i1.171', 'id_scheme': 'DOI'}","As collections become larger in size, more complex in structure and increasingly diverse in composition, new approaches are needed to help curators assess digital files and make decisions about their long-term preservation. We present research on the use of interactive visualization to analyze file characterization information for the purpose of assessing the preservation condition of a vast collection of complex electronic records. The case study collection contains over 1,000,000 files of diverse formats arranged in varied record structures and record groups. The visualization application uses tree maps and a relational database management system (RDBMS) to represent the collection's arrangement and to show available characterization information at different levels of aggregation, classification and abstraction. Through this visualization interface curators can interact dynamically with the collections' characterization information to discover trends, as well as compare and contrast various file characteristics across the collection. Curators may select and weight the variables that they want to analyze. They can pursue analysis workflows that go from a high-level overview of the collection's preservation condition based on file format risks, to obtaining more detailed results about the condition of record groups and individual records. While there are various digital preservation planning tools available, to our knowledge none have been designed specifically to visually present assessment information across vast and complex collections. We present research to address the need for such a tool.",Assessing the Preservation Condition of Large and Heterogeneous Electronic Records Collections with Visualization,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1525,http://dx.doi.org/10.2218/ijdc.v6i1.172,"{'string_id': '10.2218/ijdc.v6i1.172', 'id_scheme': 'DOI'}","There is almost universal agreement that scientific data should be shared for use beyond the purposes for which they were initially collected. Access to data enables system-level science, expands the instruments and products of research to new communities, and advances solutions to complex human problems. While demands for data are not new, the vision of open access to data is increasingly ambitious. The aim is to make data accessible and usable to anyone, anytime, anywhere, and for any purpose. Until recently, scholarly investigations related to data sharing and reuse were sparse. They have become more common as technology and instrumentation have advanced, policies that mandate sharing have been implemented, and research has become more interdisciplinary. Each of these factors has contributed to what is commonly referred to as the ""data deluge"". Most discussions about increases in the scale of sharing and reuse have focused on growing amounts of data. There are other issues related to open access to data that also concern scale which have not been as widely discussed: broader participation in data sharing and reuse, increases in the number and types of intermediaries, and more digital data products. The purpose of this paper is to develop a research agenda for scientific data sharing and reuse that considers these three areas.",Beyond the Data Deluge: A Research Agenda for Large-Scale Data Sharing and Reuse,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1526,http://dx.doi.org/10.2218/ijdc.v6i1.173,"{'string_id': '10.2218/ijdc.v6i1.173', 'id_scheme': 'DOI'}","Data curation includes the goal of facilitating the re-use and combination of datasets, which is often impeded by incompatible data schema. Can we use ontologies to help with data integration? We suggest a semi-automatic process that involves the use of automatic text searching to help identify overlaps in metadata that accompany data schemas, plus human validation of suggested data matches.Problems include different text used to describe the same concept, different forms of data recording and different organizations of data. Ontologies can help by focussing attention on important words, providing synonyms to assist matching, and indicating in what context words are used. Beyond ontologies, data on the statistical behavior of data can be used to decide which data elements appear to be compatible with which other data elements. When curating data which may have hundreds or even thousands of data labels, semi-automatic assistance with data fusion should be of great help.",Use of Ontologies for Data Integration and Curation,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1527,http://dx.doi.org/10.2218/ijdc.v6i1.174,"{'string_id': '10.2218/ijdc.v6i1.174', 'id_scheme': 'DOI'}","Rescuing data from inaccessible or damaged storage media for the purpose of preserving the digital data for the long term is one of the dimensions of digital archaeology. With the current pace of technological development, any system can become obsolete in a matter of years and hence the data stored in a specific storage media might not be accessible anymore due to the unavailability of the system to access the media. In order to preserve digital records residing in such storage media, it is necessary to extract the data stored in those media by some means.One early storage medium for home computers in the 1980s was audio tape. The first home computer systems allowed the use of standard cassette players to record and replay data. Audio cassettes are more durable than old home computers when properly stored. Devices playing this medium (i.e. tape recorders) can be found in working condition or can be repaired, as they are usually made out of standard components. By re-engineering the format of the waveform and the file formats, the data on such media can then be extracted from a digitised audio stream and migrated to a non-obsolete format.In this paper we present a case study on extracting the data stored on an audio tape by an early home computer system, namely the Philips Videopac+ G7400. The original data formats were re-engineered and an application was written to support the migration of the data stored on tapes without using the original system. This eliminates the necessity of keeping an obsolete system alive for enabling access to the data on the storage media meant for this system. Two different methods to interpret the data and eliminate possible errors in the tape were implemented and evaluated on original tapes, which were recorded 20 years ago. Results show that with some error correction methods, parts of the tapes are still readable even without the original system. It also implies that it is easier to build solutions while original systems are still available in a working condition.",Migrating Home Computer Audio Waveforms to Digital Objects: A Case Study on Digital Archaeology,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1528,http://dx.doi.org/10.2218/ijdc.v6i1.175,"{'string_id': '10.2218/ijdc.v6i1.175', 'id_scheme': 'DOI'}","Emulation has been widely discussed as a preservation strategy for digital documents that depend upon proprietary executables, as well as for legacy programs. The fundamental assumption of this strategy is that an artifact (document or program) will be bundled with any required contemporaneous software in an archival information package (AIP) which can be loaded and executed in an emulation environment by patrons wishing to access the preserved artifact, yet little has been written about how to identify the required components for such an AIP. Even where a digital document was distributed with a binary viewer, there may be dependencies on other software libraries. In this paper we discuss a pilot study that performed dependency analysis for digital materials originally distributed on CD-ROM. In particular, we show how to utilize a small number of existing off-the-shelf libraries to build a tool that can analyze executables within ISO (CD-ROM) images, and then examine the results of applying this tool to a body of archived images.",Dependency Analysis of Legacy Digital Materials to Support Emulation Based Preservation,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1529,http://dx.doi.org/10.2218/ijdc.v6i1.176,"{'string_id': '10.2218/ijdc.v6i1.176', 'id_scheme': 'DOI'}","Underpinning every digital library and digital repository there is a policy framework, which makes the digital library viable - without a policy framework a digital library is little more than a container for content. Policy governs how a digital library is instantiated and run. It is therefore a meta-domain which is situated both outside the digital library and any technologies used to deliver it, and within the digital library itself. Policy is also a key aspect of digital library and digital repository interoperability in a common and integrated information space. Policy interoperability - that is the exchange and reuse of policies - is a step beyond policy standardisation. Furthermore, effective and efficient policy frameworks are also one of the Digital Curation Center (DCC), DigitalPreservationEurope (DPE), nestor and Center for Research Libraries (CRL) core criteria for digital repositories. In this article, we share our research on policy interoperability levels and the experimental survey on policy interoperability conducted with real-life digital libraries, as a contribution towards the definition of a Policy Interoperability Framework.",Towards a Holistic Approach to Policy Interoperability in Digital Libraries and Digital Repositories,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1530,http://dx.doi.org/10.2218/ijdc.v6i1.177,"{'string_id': '10.2218/ijdc.v6i1.177', 'id_scheme': 'DOI'}","Large, collaboratively managed datasets have become essential to many scientific and engineering endeavors, and their management has increased the need for ""eScience professionals"" who solve large scale information management problems for researchers and engineers. This paper considers the dimensions of work, worker, and workplace, including the knowledge, skills, and abilities needed for eScience professionals. We used focus groups and interviews to explore the needs of scientific researchers and how these needs may translate into curricular and program development choices. A cohort of five masters students also worked in targeted internship settings and completed internship logs. We organized this evidence into a job analysis that can be used for curriculum and program development at schools of library and information science.",Education for eScience Professionals: Integrating Data Curation and Cyberinfrastructure,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1531,http://dx.doi.org/10.2218/ijdc.v6i1.178,"{'string_id': '10.2218/ijdc.v6i1.178', 'id_scheme': 'DOI'}","Over the past decade, a rich body of research and practice has emerged under the rubrics of electronic records, digital preservation and digital curation. Most of this work has taken place as research activity (often financed by government agencies) within libraries and information/computer science departments. Many projects focus on one format of information, such as research publications or data, potentially de-contextualizing individual records. Meanwhile, most institutional archives and manuscript repositories, which possess a rich theoretical and practical framework for preserving context among mixed analog materials, have failed to extend their capabilities to digital records. As a result, relatively few institutions have implemented systematic methods to capture, preserve and provide access to the complete range of documentation that end users need to understand and interpret past human activity.The Practical E-Records Method attempts to address this problem by providing easy-to-implement software reviews, guidance/policy templates, and program recommendations that blend digital curation research findings with traditional archival processes and workflows. Using the method discussed in this paper, archives and manuscript repositories can use existing resources to incrementally develop digital curation skills, building a collaborative, expanding program in the process. Archival programs that make digital curation a systematic institutional function will systematically gather, preserve, and provide access to genres of documentation that are contextually-rich and highly susceptible to loss, complementing efforts undertaken by librarians, information scientists and external service providers. Over the next year, the suggested techniques will be tested and refined at the University of Illinois Archives and possibly elsewhere.",Making Digital Curation a Systematic Institutional Function,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1532,http://dx.doi.org/10.2218/ijdc.v6i1.179,"{'string_id': '10.2218/ijdc.v6i1.179', 'id_scheme': 'DOI'}","Recent work in the semantics of markup languages may offer a way to achieve more reliable results for format conversion, or at least a way to state the goal more explicitly. In the work discussed, the meaning of markup in a document is taken as the set of things accepted as true because of the markup's presence, or equivalently, as the set of inferences licensed by the markup in the document. It is possible, in principle, to apply a general semantic description of a markup vocabulary to documents encoded using that vocabulary and to generate a set of inferences (typically rather large, but finite) as a result. An ideal format conversion translating a digital object from one vocabulary to another, then, can be characterized as one which neither adds nor drops any licensed inferences; it is possible to check this equivalence explicitly for a given conversion of a digital object, and possible in principle (although probably beyond current capabilities in practice) to prove that a given transformation will, if given valid and semantically correct input, always produce output that is semantically equivalent to its input. This approach is directly applicable to the XML formats frequently used for scientific and other data, but it is also easily generalized from SGML/XML-based markup languages to digital formats in general; at a high level, it is equally applicable to document markup, to database exchanges, and to ad hoc formats for high-volume scientific data.Some obvious complications and technical difficulties arising from this approach are discussed, as are some important implications. In most real-world format conversions, the source and target formats differ at least somewhat in their ontology, either in the level of detail they cover or in the way they carve reality into classes; it is thus desirable not only to define what a perfect format conversion looks like, but to quantify the loss or distortion of information resulting from the conversion.",What Constitutes Successful Format Conversion? Towards a Formalization of 'Intellectual Content',Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1533,http://dx.doi.org/10.2218/ijdc.v6i1.180,"{'string_id': '10.2218/ijdc.v6i1.180', 'id_scheme': 'DOI'}","The Web is increasingly becoming a platform for linked data. This means making connections and adding value to data on the Web. As more data becomes openly available and more people are able to use the data, it becomes more powerful. An example is file format registries and the evaluation of format risks. Here the requirement for information is now greater than the effort that any single institution can put into gathering and collating this information. Recognising that more is better, the creators of PRONOM, JHOVE, GDFR and others are joining to lead a new initiative: the Unified Digital Format Registry. Ahead of this effort, a new RDF-based framework for structuring and facilitating file format data from multiple sources, including PRONOM, has demonstrated it is able to produce more links, and thus provide more answers to digital preservation questions - about format risks, applications, viewers and transformations - than the native data alone. This paper will describe this registry, P2, and its services, show how it can be used, and provide examples where it delivers more answers than the contributing resources. The P2 Registry is a reference platform to allow and encourage publication of preservation data, and also an examplar of what can be achieved if more data is published openly online as simple machine-readable documents. This approach calls for the active participation of the digital preservation community to contribute data by simply publishing it openly on the Web as linked data.",Where the Semantic Web and Web 2.0 Meet Format Risk Management: P2 Registry,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1534,http://dx.doi.org/10.2218/ijdc.v6i1.181,"{'string_id': '10.2218/ijdc.v6i1.181', 'id_scheme': 'DOI'}","Many digital preservation scenarios are based on the migration strategy, which itself is heavily tool-dependent. For popular, well-defined and often open file formats – e.g., digital images, such as PNG, GIF, JPEG – a wide range of tools exist. Migration workflows become more difficult with proprietary formats, as used by the several text processing applications becoming available in the last two decades. If a certain file format can not be rendered with actual software, emulation of the original environment remains a valid option. For instance, with the original Lotus AmiPro or Word Perfect, it is not a problem to save an object of this type in ASCII text or Rich Text Format. In specific environments, it is even possible to send the file to a virtual printer, thereby producing a PDF as a migration output. Such manual migration tasks typically involve human interaction, which may be feasible for a small number of objects, but not for larger batches of files.We propose a novel approach using a software-operated VNC abstraction layer in order to replace humans with machine interaction. Emulators or virtualization tools equipped with a VNC interface are very well suited for this approach. But screen, keyboard and mouse interaction is just part of the setup. Furthermore, digital objects need to be transferred into the original environment in order to be extracted after processing. Nevertheless, the complexity of the new generation of migration services is quickly rising; a preservation workflow is now comprised not only of the migration tool itself, but of a complete software and virtual hardware stack with recorded workflows linked to every supported migration scenario. Thus the requirements of OAIS management must include proper software archiving, emulator selection, system image and recording handling. The concept of view-paths could help either to automatically determine the proper pre-configured virtual environment or to set up system images for certain migration workflows. View-paths may rise in demand, as the generation of PDF output files from Word Perfect input could be cached as pre-fabricated emulator system images. The current groundwork provides several possible optimizations, such as using the automation features of the original environments.",Automation of Flexible Migration Workflows,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1535,http://dx.doi.org/10.2218/ijdc.v6i1.182,"{'string_id': '10.2218/ijdc.v6i1.182', 'id_scheme': 'DOI'}","We report on an exploratory study consisting of brief case studies in selected disciplines, examining what motivates researchers to work (or want to work) in an open manner with regard to their data, results and protocols, and whether advantages are delivered by working in this way. We review the policy background to open science, and literature on the benefits attributed to open data, considering how these relate to curation and to questions of who participates in science. The case studies investigate the perceived benefits to researchers, research institutions and funding bodies of utilising open scientific methods, the disincentives and barriers, and the degree to which there is evidence to support these perceptions. Six case study groups were selected in astronomy, bioinformatics, chemistry, epidemiology, language technology and neuroimaging. The studies identify relevant examples and issues through qualitative analysis of interview transcripts. We provide a typology of degrees of open working across the research lifecycle, and conclude that better support for open working, through guidelines to assist research groups in identifying the value and costs of working more openly, and further research to assess the risks, incentives and shifts in responsibility entailed by opening up the research process are needed.",Open Science in Practice: Researcher Perspectives and Participation,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1536,http://dx.doi.org/10.2218/ijdc.v6i1.183,"{'string_id': '10.2218/ijdc.v6i1.183', 'id_scheme': 'DOI'}","Within information systems, a significant aspect of search and retrieval across information objects, such as datasets, journal articles, or images, relies on the identity construction of the objects. This paper uses identity to refer to the qualities or characteristics of an information object that make it definable and recognizable, and can be used to distinguish it from other objects. Identity, in this context, can be seen as the foundation from which citations, metadata and identifiers are constructed.In recent years the idea of including datasets within the scientific record has been gaining significant momentum, with publishers, granting agencies and libraries engaging with the challenge. However, the task has been fraught with questions of best practice for establishing this infrastructure, especially in regards to how citations, metadata and identifiers should be constructed. These questions suggests a problem with how dataset identities are formed, such that an engagement with the definition of datasets as conceptual objects is warranted.This paper explores some of the ways in which scientific data is an unruly and poorly bounded object, and goes on to propose that in order for datasets to fulfill the roles expected for them, the following identity functions are essential for scholarly publications: (i) the dataset is constructed as a semantically and logically concrete object, (ii) the identity of the dataset is embedded, inherent and/or inseparable, (iii) the identity embodies a framework of authorship, rights and limitations, and (iv) the identity translates into an actionable mechanism for retrieval or reference.",Linking to Scientific Data: Identity Problems of Unruly and Poorly Bounded Digital Objects,Edinburgh University,2011,,International Journal of Digital Curation,6,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1537,http://dx.doi.org/10.2218/ijdc.v6i2.190,"{'string_id': '10.2218/ijdc.v6i2.190', 'id_scheme': 'DOI'}","This study explores how researchers at a major Midwestern university are managing their data, as well as the factors that have shaped their practices and those that motivate or inhibit changes to that practice. A combination of survey (n=363) and interview data (n=15) yielded both qualitative and quantitative results bearing on my central research question: In what types of data management activities do researchers at this institution engage? Corollary to that, I also explored the following questions: What do researchers feel could be improved about their data management practices? Which services might be of interest to them? How do they feel those services could most effectively be implemented?In this paper, I situate researchers’ data management practices within a theory of personal information management. I present a view of data management and preservation needs from researchers’ perspectives across a range of domains. Additionally, I discuss the implications that understanding research data management as personal information management has for introducing services to support and improve data management practice.","“You made it, you take care of it”: Data Management as Personal Information Management",Edinburgh University,2011,,International Journal of Digital Curation,6,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1538,http://dx.doi.org/10.2218/ijdc.v6i2.191,"{'string_id': '10.2218/ijdc.v6i2.191', 'id_scheme': 'DOI'}","In the mid 1990s UK digital preservation activity concentrated on ensuring the survival of digital material – spurred on by the US report Preserving Digital Information (The Task Force on Archiving of Digital Information, 1996) and developed through JISC-funded activities. Technical developments and a maturing understanding of organisational activity and workflow saw the emphasis move to ensuring the access, use and reuse of digital materials throughout their lifecycle. Digital Curation emerged as a new discipline supported through the activities of the UK’s Digital Curation Centre and a number of EU 6th Framework Projects. Digital Curation is now embedded in both practice and research; with the development of tools, and the foundation of a number of support units and academic educators offering training and furthering research.",Digital Curation: The Emergence of a New Discipline,Edinburgh University,2011,,International Journal of Digital Curation,6,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1539,http://dx.doi.org/10.2218/ijdc.v6i2.192,"{'string_id': '10.2218/ijdc.v6i2.192', 'id_scheme': 'DOI'}","Virtual environments, such as Second Life, have assumed an increasingly important role in popular culture, education and research. Unfortunately, we have almost no practical experience in how to preserve these highly dynamic, interactive information resources. This article reports on research by the National Digital Information Infrastructure for Preservation Program (NDIIPP)-funded Preserving Virtual Worlds project, which examines the issues that arise when attempting to archive regions from Second Life. Intellectual property and contractual issues can raise significant impediments to the creation of an archival information package for these environments, as can the technical design of the worlds themselves. We discuss the implication of these impediments for distributed models of preservation, such as NDIIPP.","Saving Second Life: Issues in Archiving a Complex, Multi-User Virtual World",Edinburgh University,2011,,International Journal of Digital Curation,6,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1540,http://dx.doi.org/10.2218/ijdc.v6i2.204,"{'string_id': '10.2218/ijdc.v6i2.204', 'id_scheme': 'DOI'}","The challenge of digital preservation of scientific data lies in the need to preserve not only the dataset itself but also the ability it has to deliver knowledge to a future user community. A true scientific research asset allows future users to reanalyze the data within new contexts. Thus, in order to carry out meaningful preservation we need to ensure that future users are equipped with the necessary information to re-use the data. This paper presents an overview of a preservation analysis methodology which was developed in response to that need on the CASPAR and Digital Curation Centre SCARP projects. We intend to place it in relation to other digital preservation practices, discussing how they can interact to provide archives caring for scientific data sets with the full arsenal of tools and techniques necessary to rise to this challenge.",Curating Scientific Research Data for the Long Term: A Preservation Analysis Method in Context,Edinburgh University,2011,,International Journal of Digital Curation,6,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1541,http://dx.doi.org/10.2218/ijdc.v6i2.205,"{'string_id': '10.2218/ijdc.v6i2.205', 'id_scheme': 'DOI'}","This paper discusses many of the issues associated with formally publishing data in academia, focusing primarily on the structures that need to be put in place for peer review and formal citation of datasets. Data publication is becoming increasingly important to the scientific community, as it will provide a mechanism for those who create data to receive academic credit for their work and will allow the conclusions arising from an analysis to be more readily verifiable, thus promoting transparency in the scientific process. Peer review of data will also provide a mechanism for ensuring the quality of datasets, and we provide suggestions on the types of activities one expects to see in the peer review of data. A simple taxonomy of data publication methodologies is presented and evaluated, and the paper concludes with a discussion of dataset granularity, transience and semantics, along with a recommended human-readable citation syntax.",Citation and Peer Review of Data: Moving Towards Formal Data Publication,Edinburgh University,2011,,International Journal of Digital Curation,6,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1542,http://dx.doi.org/10.2218/ijdc.v6i2.206,"{'string_id': '10.2218/ijdc.v6i2.206', 'id_scheme': 'DOI'}","The subject of digital game preservation is one that has moved up the research agenda in recent years with a number of international projects, such as KEEP and Preserving Virtual Worlds, highlighting and seeking to address the impact of media decay, hardware and software obsolescence through different strategies including code emulation, for instance. Similarly, and reflecting a popular interest in the histories of digital games, exhibitions such as Game On (Barbican, UK) and GameCity (Nottingham, UK) experiment with ways of presenting games to a general audience. This article focuses on the UK’s National Videogame Archive (NVA) which, since its foundation in 2008, has developed approaches that both dovetail with and critique existing strategies to game preservation, exhibition and display.The article begins by noting the NVA’s interest in preserving not only the code or text of the game, but also the experience of using it – that is, the preservation of gameplay as well as games. This approach is born of a conceptualisation of digital games as what Moulthrop (2004) has called “configurative performances” that are made through the interaction of code, systems, rules and, essentially, the actions of players at play. The analysis develops by problematising technical solutions to game preservation by exploring the way seemingly minute differences in code execution greatly impact on this user experience.Given these issues, the article demonstrates how the NVA returns to first principles and questions the taken-for-granted assumption that the playable game is the most effective tool for interpretation. It also encourages a consideration of the uses of non-interactive audiovisual and (para)textual materials in game preservation activity. In particular, the focus falls upon player-produced walkthrough texts, which are presented as archetypical archival documents of gameplay. The article concludes by provocatively positing that these non-playable, non-interactive texts might be more useful to future game scholars than the playable game itself.",(Not) Playing Games: Player-Produced Walkthroughs as Archival Documents of Digital Gameplay,Edinburgh University,2011,,International Journal of Digital Curation,6,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1543,http://dx.doi.org/10.2218/ijdc.v6i2.207,"{'string_id': '10.2218/ijdc.v6i2.207', 'id_scheme': 'DOI'}","Telehealth monitoring data is now being collected across large populations of patients with chronic diseases such as stroke, hypertension, COPD and dementia. These large, complex and heterogeneous datasets, including distributed sensor and mobile datasets, present real opportunities for knowledge discovery and re-use, however they also generate new challenges for curation. This paper uses qualitative research with stakeholders in two nationally-funded telehealth projects to outline the perceptions, practices and preferences of different stakeholders with regard to data curation. Telehealth provides a living laboratory for the very different challenges implicit in designing and managing data infrastructure for embedded and ubiquitous computing. Here, technical and human agents are distributed, and interaction and state change is a central component of design, rather than an inconvenient challenge to it. The authors argue that there are lessons to be learned from other domains where data infrastructure has been radically rethought to address these challenges.","Curating Complex, Dynamic and Distributed Data: Telehealth as a Laboratory for Strategy",Edinburgh University,2011,,International Journal of Digital Curation,6,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1544,http://dx.doi.org/10.2218/ijdc.v7i1.210,"{'string_id': '10.2218/ijdc.v7i1.210', 'id_scheme': 'DOI'}","Network modelling provides a framework for the systematic analysis of needs and options for preservation. A number of general strategies can be identified, characterised and applied to many situations; these strategies may be combined to produce robust preservation solutions tailored to the needs of the community and responsive to their environment. This paper provides an overview of this approach. We describe the components of a Preservation Network Model and go on to show how it may be used to plan preservation actions according to the requirements of the particular situation using illustrative examples from scientific archives.",Managing Risks in the Preservation of Research Data with Preservation Networks,Edinburgh University,2012,,International Journal of Digital Curation,7,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1545,http://dx.doi.org/10.2218/ijdc.v7i1.211,"{'string_id': '10.2218/ijdc.v7i1.211', 'id_scheme': 'DOI'}","The digital preservation community currently utilises a number of tools and automated processes to identify and validate digital objects. The identification of digital objects is a vital first step in their long-term preservation, but the results returned by tools used for this purpose are lacking in transparency, and are not easily tested or verified. This paper suggests that a test corpus of digital objects is one way of providing this verification and validation, ultimately improving trust in the tools, and providing further stimulus to their development. Issues to be considered are outlined, and attention is drawn to particular examples of existing digital corpora which could conceivably provide a useable framework or starting point for our own communities needs. This paper does not seek to answer all questions in this area, but merely attempts to set out areas for consideration in any next step that is taken.",Towards the Development of a Test Corpus of Digital Objects for the Evaluation of File Format Identification Tools and Signatures,Edinburgh University,2012,,International Journal of Digital Curation,7,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1546,http://dx.doi.org/10.2218/ijdc.v7i1.212,"{'string_id': '10.2218/ijdc.v7i1.212', 'id_scheme': 'DOI'}","The majority of information about science, culture, society, economy and the environment is born digital, yet the underlying technology is subject to rapid obsolescence. One solution to this obsolescence, format migration, is widely practiced and supported by many software packages, yet migration has well known risks. For example, newer formats – even where similar in function – do not generally support all of the features of their predecessors, and, where similar features exist, there may be significant differences of interpretation.There appears to be a conflict between the wide use of migration and its known risks. In this paper we explore a simple hypothesis – that, where migration paths exist, the majority of data files can be safely migrated leaving only a few that must be handled more carefully – in the context of several scientific data formats that are or were widely used. Our approach is to gather information about potential migration mismatches and, using custom tools, evaluate a large collection of data files for the incidence of these risks. Our results support our initial hypothesis, though with some caveats. Further, we found that writing a tool to identify “risky” format features is considerably easier than writing a migration tool.",Assessing Migration Risk for Scientific Data Formats,Edinburgh University,2012,,International Journal of Digital Curation,7,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1547,http://dx.doi.org/10.2218/ijdc.v7i1.213,"{'string_id': '10.2218/ijdc.v7i1.213', 'id_scheme': 'DOI'}","From where did this tweet originate? Was this quote from the New York Times modified? Daily, we rely on data from the Web, but often it is difficult or impossible to determine where it came from or how it was produced. This lack of provenance is particularly evident when people and systems deal with Web information or with any environment where information comes from sources of varying quality. Provenance is not captured pervasively in information systems. There are major technical, social, and economic impediments that stand in the way of using provenance effectively. This paper synthesizes requirements for provenance on the Web for a number of dimensions, focusing on three key aspects of provenance: the content of provenance, the management of provenance records, and the uses of provenance information. To illustrate these requirements, we use three synthesized scenarios that encompass provenance problems faced by Web users today.",Requirements for Provenance on the Web,Edinburgh University,2012,,International Journal of Digital Curation,7,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1548,http://dx.doi.org/10.2218/ijdc.v7i1.214,"{'string_id': '10.2218/ijdc.v7i1.214', 'id_scheme': 'DOI'}","Freedom of information legislation came into effect in the UK in 2005. All universities that receive block grants from the Higher Education Funding Councils in England, Wales, Scotland and Northern Ireland are subject to the legislation. Recent cases where universities have received requests for data and other information generated by researchers, working in areas such as climate, have given rise to controversy and widespread concern in the research community. This paper examines some of those concerns, relating to responsibilities for the ownership and holding of information, for data and records management, and for the handling of requests under the legislation. It also considers the implications relating to personal data, and to information that may affect the commercial interests of universities operating in a competitive environment, or the interests of the many other organisations which may be involved in research partnerships with universities; and it outlines concerns about the possible impact on quality assurance, peer review, and scholarly discourse. Finally, the paper emphasises the need for support and training for researchers so that they become more aware of the legislation and its implications, and how to deal with requests when they arise.",Freedom of Information in the UK and its Implications for Research in the Higher Education Sector,Edinburgh University,2012,,International Journal of Digital Curation,7,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1549,http://dx.doi.org/10.2218/ijdc.v7i1.215,"{'string_id': '10.2218/ijdc.v7i1.215', 'id_scheme': 'DOI'}","One morning we came in to work to find that one of our servers had made 1.5 million attempts to contact an external server in the preceding hour. It turned out that the calls were being generated by the Library’s digital preservation system (Rosetta) while attempting to validate XML Schema Definition (XSD) declarations included in the XML files of the Library’s online newspaper application Papers Past, which we were in the process of loading into Rosetta. This paper describes our response to this situation and outlines some of the issues that needed to be canvassed before we were able to arrive at a suitable solution, including the digital preservation status of these XSDs; their impact on validation tools, such as JHOVE; and where these objects should reside if they are considered material to the digital preservation process.","A Short Story about XML Schemas, Digital Preservation and Format Libraries",Edinburgh University,2012,,International Journal of Digital Curation,7,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1550,http://dx.doi.org/10.2218/ijdc.v7i1.216,"{'string_id': '10.2218/ijdc.v7i1.216', 'id_scheme': 'DOI'}","Genomic and environmental sciences represent two poles of scientific data. In the first, highly parallel sequencing facilities generate large quantities of sequence data. In the latter, loosely networked remote and field sensors produce intermittent streams of different data types. Yet both genomic and environmental sciences are said to be moving to data intensive research. This paper explores and contrasts data flow in these two domains in order to better understand how data intensive research is being done. Our case studies are next generation sequencing for genomics and environmental networked sensors.Our objective was to enrich understanding of the ‘intensive’ processes and properties of data intensive research through a ‘sociology’ of data using methods that capture the relational properties of data flows. Our key methodological innovation was the staging of events for practitioners with different kinds of expertise in data intensive research to participate in the collective annotation of visual forms. Through such events we built a substantial digital data archive of our own that we then analysed in terms of three traits of data flow: durability, replicability and metrology.Our findings are that analysing data flow with respect to these three traits provides better insight into how doing data intensive research involves people, infrastructures, practices, things, knowledge and institutions. Collectively, these elements shape the topography of data and condition how it flows. We argue that although much attention is given to phenomena such as the scale, volume and speed of data in data intensive research, these are measures of what we call ‘extensive’ properties rather than intensive ones. Our thesis is that extensive changes, that is to say those that result in non-linear changes in metrics, can be seen to result from intensive changes that bring multiple, disparate flows into confluence.If extensive shifts in the modalities of data flow do indeed come from the alignment of disparate things, as we suggest, then we advocate the staging of workshops and other events with the purpose of developing the ‘missing’ metrics of data flow.",Understanding the ‘Intensive’ in ‘Data Intensive Research’: Data Flows in Next Generation Sequencing and Environmental Networked Sensors,Edinburgh University,2012,,International Journal of Digital Curation,7,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1551,http://dx.doi.org/10.2218/ijdc.v7i1.217,"{'string_id': '10.2218/ijdc.v7i1.217', 'id_scheme': 'DOI'}","The capability to validate and view or play binary file formats, as well as to convert binary file formats to standard or current file formats, is critically important to the preservation of digital data and records. This paper describes the extension of context-free grammars from strings to binary files. Binary files are arrays of data types, such as long and short integers, floating-point numbers and pointers, as well as characters. The concept of an attribute grammar is extended to these context-free array grammars. This attribute grammar has been used to define a number of chunk-based and directory-based binary file formats. A parser generator has been used with some of these grammars to generate syntax checkers (recognizers) for validating binary file formats. Among the potential benefits of an attribute grammar-based approach to specification and parsing of binary file formats is that attribute grammars not only support format validation, but support generation of error messages during validation of format, validation of semantic constraints, attribute value extraction (characterization), generation of viewers or players for file formats, and conversion to current or standard file formats. The significance of these results is that with these extensions to core computer science concepts, traditional parser/compiler technologies can potentially be used as a part of a general, cost effective curation strategy for binary file formats.",Grammar-Based Specification and Parsing of Binary File Formats,Edinburgh University,2012,,International Journal of Digital Curation,7,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1552,http://dx.doi.org/10.2218/ijdc.v7i2.226,"{'string_id': '10.2218/ijdc.v7i2.226', 'id_scheme': 'DOI'}","Over the past 20 years, many thousands of CD-ROM titles were published; many of these have lasting cultural significance, yet present a difficult challenge for libraries due to obsolescence of the supporting software and hardware, and the consequent decline in the technical knowledge required to support them. The current trend appears to be one of abandonment – for example, the Indiana University Libraries no longer maintain machines capable of accessing early CD-ROM titles.In previous work, we proposed an access model based upon networked ‘virtual collections’ of CD-ROMs which can enable consortia of libraries to pool the technical expertise necessary to provide continued access to such materials for a geographically sparse base of patrons, who may have limited technical knowledge.In this paper, we extend this idea to CD-ROMs designed to operate on ‘classic’ Macintosh systems with an extensive case study – the catalog of the Voyager Company publications, which was the first major innovator in interactive CD-ROMs. The work described includes emulator extensions to support obsolete CD formats and to enable networked access to the virtual collection.",Developing Virtual CD-ROM Collections: The Voyager Company Publications,Edinburgh University,2012,,International Journal of Digital Curation,7,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1553,http://dx.doi.org/10.2218/ijdc.v7i2.227,"{'string_id': '10.2218/ijdc.v7i2.227', 'id_scheme': 'DOI'}","In this paper we discuss archival storage container formats from the point of view of digital curation and preservation, an aspect of preservation overlooked by most other studies. Considering established approaches to data management as our jumping off point, we selected seven container format attributes that are core to the long term accessibility of digital materials. We have labeled these core preservation attributes. These attributes are then used as evaluation criteria to compare storage container formats belonging to five common categories: formats for archiving selected content (e.g. tar, WARC), disk image formats that capture data for recovery or installation (partimage, dd raw image), these two types combined with a selected compression algorithm (e.g. tar+gzip), formats that combine packing and compression (e.g. 7-zip), and forensic file formats for data analysis in criminal investigations (e.g. aff – Advanced Forensic File format). We present a general discussion of the storage container format landscape in terms of the attributes we discuss, and make a direct comparison between the three most promising archival formats: tar, WARC, and aff. We conclude by suggesting the next steps to take the research forward and to validate the observations we have made.",Digital Forensics Formats: Seeking a Digital Preservation Storage Container Format for Web Archiving,Edinburgh University,2012,,International Journal of Digital Curation,7,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1554,http://dx.doi.org/10.2218/ijdc.v7i2.228,"{'string_id': '10.2218/ijdc.v7i2.228', 'id_scheme': 'DOI'}","The growth of computing technology during the previous three decades has resulted in a large amount of content being created in digital form. As their creators retire or pass away, an increasing number of personal data collections, in the form of digital media and complete computer systems, are being offered to the academic institutional archive. For the digital curator or archivist, the handling and processing of such digital material represents a considerable challenge, requiring development of new processes and procedures. This paper outlines how digital forensic methods, developed by the law enforcement and legal community, may be applied by academic digital archives. It goes on to describe the strategic and practical decisions that should be made to introduce forensic methods within an existing curatorial infrastructure and how different techniques, such as forensic hashing, timeline analysis and data carving, may be used to collect information of a greater breadth and scope than may be gathered through manual activities.",The Forensic Curator: Digital Forensics as a Solution to Addressing the Curatorial Challenges Posed by Personal Digital Archives,Edinburgh University,2012,,International Journal of Digital Curation,7,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1555,http://dx.doi.org/10.2218/ijdc.v8i1.237,"{'string_id': '10.2218/ijdc.v8i1.237', 'id_scheme': 'DOI'}","The preservation of artistic works with technological components, such as musical works, is recognised as an issue by both the artistic community and the archival community. Preserving such works involves tackling the difficulties associated with digital information in general, but also raises its own specific problems, such as constantly evolving digital instruments embodied within software and idiosyncratic human-computer interactions. Because of these issues, standards in place for archiving digital information are not always suitable for the preservation of these works. The impact on the organisation and the descriptions of such archives need to be conceptualised in order to provide these technological components with readability, authenticity and intelligibility. While previous projects emphasized readability and authenticity, less effort has been dedicated to addressing intelligibility issues. The research into the specification of significant properties and its extension, namely significant knowledge, offers some grounds for reflecting on this question. Furthermore, the relevance of taking into account the creative process involved in the production of technological components offers an opportunity to redefine the status of technological agents in the performative aspect of digital records. Altogether, the research on significant knowledge and creative processes provide us with a conceptual framework that we propose to bring together with digital archives models to form a coherent framework. ",A Digital Archives Framework for the Preservation of Cultural Artifacts with Technological Components,Edinburgh University,2013,,International Journal of Digital Curation,8,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1556,http://dx.doi.org/10.2218/ijdc.v8i1.238,"{'string_id': '10.2218/ijdc.v8i1.238', 'id_scheme': 'DOI'}","This paper examines the development of the Open Access movement in scholarly communication, with particular attention to some of the rhetorical strategies and policy mechanisms used to promote it to scholars and scientists. Despite the majority of journal publishers’ acceptance of author self-archiving practices, and the minimal time commitment required by authors to successfully self-archive their work in disciplinary or institutional repositories, the majority of authors still by and large avoid participation. The paper reviews the strategies and arguments used for increasing author participation in open access, including the role of open access mandates. We recommend a service-oriented approach towards increasing participation in open access, rather than rhetoric that speculates on the benefits that open access will have on text/data mining innovation. In advocating for open access participation, we recommend focusing on its most universal and tangible purpose: increasing public open (gratis) access to the published results of publicly funded research. Researchers require strong institutional support to understand the copyright climate of open access self-archiving, user-friendly interfaces and useful metrics, such as repository usage statistics. We recommend that mandates and well-crafted and responsive author support services at universities will ultimately be required to ensure the growth of open access. We describe the mediated deposit service that was developed to support author self-archiving in Spectrum: Concordia University Research Repository. By comparing the number of deposits of non-thesis materials (e.g. articles and conference presentations) that were accomplished through the staff-mediated deposit service to the number of deposits that were author-initiated, we demonstrate the relative significance of this service to the growth of the repository.",The Critical Role of Institutional Services in Open Access Advocacy,Edinburgh University,2013,,International Journal of Digital Curation,8,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1557,http://dx.doi.org/10.2218/ijdc.v8i1.240,"{'string_id': '10.2218/ijdc.v8i1.240', 'id_scheme': 'DOI'}","User-generated content (UGC) has become a part of personal digital collections on the Web, as such collections often contain personal memories, activities, thoughts and even profiles. With the increase in the creation of personal materials on the Web, the needs for archiving and preserving these materials are increasing, not only for the purpose of developing personal archives but also for the purpose of capturing social memory and tracking human traces in this era. Using both survey and interview methods, this study investigated blogs, one popular type of UGC, and analyzed travel bloggers’ perceptions of the value of blogs and the elements of blogs that are important for preservation. The study respondents found personal and sentimental value (e.g., a way to express themselves, a way to keep personal memories and thoughts, and a way to maintain a record for their family) to be the most important reason for preserving blogs, followed by informational value and cultural/historical value. Sharing also appeared as one of the values that respondents found in their blogs. The respondents reported that self-created blog posts (content) and information related to the blog posts (context) are more important to preserve than some other elements (behavior and appearance). Integrating what bloggers consider as most valuable and what archivists think are worth preserving may be an important step when collecting personal blogs.",Defining What Matters When Preserving Web-Based Personal Digital Collections: Listening to Bloggers,Edinburgh University,2013,,International Journal of Digital Curation,8,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1558,http://dx.doi.org/10.2218/ijdc.v8i1.242,"{'string_id': '10.2218/ijdc.v8i1.242', 'id_scheme': 'DOI'}","With digital curation’s increasingly important role in the fast-paced and data-intensive information environment, there is a need to identify a set of competencies for professionals in this growing field. As part of a curriculum development project funded by the U.S. Institute of Museum and Library Services, a total of 173 job advertisements posted between October 2011 and April 2012 were collected from various sources to take into account varying types of professionals in the field of digital curation across North America. Position title, institution types and location, educational background, experience, knowledge and skills, and duties were examined and analyzed. The results of the analysis show that digital curation jobs are characterized by a complex interplay of various skills and knowledge. The findings of this study present emerging requirements for a qualified workforce in the field of digital curation.",Competencies Required for Digital Curation: An Analysis of Job Advertisements,Edinburgh University,2013,,International Journal of Digital Curation,8,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1559,http://dx.doi.org/10.2218/ijdc.v8i1.245,"{'string_id': '10.2218/ijdc.v8i1.245', 'id_scheme': 'DOI'}","To qualify materials for specific applications comprehensive testing is necessary, and consequently the engineering materials community has developed an extensive collection of documentary testing standards to define test conditions, specimen configurations, and post processing and reporting procedures. Unfortunately, in the absence of corresponding data formats, test results are rarely conserved and their value diminishes as the material pedigree, test conditions, and results become disassociated. In an effort to address this issue, prenormative research has demonstrated the viability of deriving data formats from documentary testing standards and thus the possibility to realize a standards-based data infrastructure for the engineering materials community.",Prenormative Research into Standard Messaging Formats for Engineering Materials Data,Edinburgh University,2013,,International Journal of Digital Curation,8,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1560,http://dx.doi.org/10.2218/ijdc.v8i1.246,"{'string_id': '10.2218/ijdc.v8i1.246', 'id_scheme': 'DOI'}","The fast growth of scientific and non-scientific digital data, as well as the proliferation of new types of digital content, has led – among many other things – to a lot of innovative work on the concept of the identifier. Digital identifiers have become the key to preserving and accessing content, just as physical identifier tags have been the key to accessing paper-based content and other physical entities for millennia. Two main schools of thought have emerged: on the one hand, librarians and public repositories have pushed the concept of the Persistent Identifier (PI) as a way to guarantee long term identification and (sometimes) access; on the other hand, the extraordinary success of the web has led several researchers and web experts to push the concept of the Cool URI as the universal mechanism for identifying and accessing digital content. Both views have their pros and cons, but so far (with only a few exceptions) the two visions have developed in parallel, sometimes with a subtle underlying hostility. In this paper, we present the evolution of the Entity Name System (ENS), an open service-based platform developed as part of the OKKAM EU co-funded project, which can reconcile these two approaches. The new system, called ENS2.0, is currently under development and will enable data creators and curators to combine the technical strengths and opportunities of the (Semantic) Web vision with the organizational, economical and social requirements legitimately raised by the PI community and stakeholders. ",Can Persistent Identifiers Be Cool?,Edinburgh University,2013,,International Journal of Digital Curation,8,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1561,http://dx.doi.org/10.2218/ijdc.v8i1.247,"{'string_id': '10.2218/ijdc.v8i1.247', 'id_scheme': 'DOI'}","‘Big Science’ - that is, science which involves large collaborations with dedicated facilities, and involving large data volumes and multinational investments – is often seen as different when it comes to data management and preservation planning. Big Science handles its data differently from other disciplines and has data management problems that are qualitatively different from other disciplines. In part, these differences arise from the quantities of data involved, but possibly more importantly from the cultural, organisational and technical distinctiveness of these academic cultures. Consequently, the data management systems are typically and rationally bespoke, but this means that the planning for data management and preservation (DMP) must also be bespoke. These differences are such that ‘just read and implement the OAIS specification’ is reasonable Data Management and Preservation (DMP) advice, but this bald prescription can and should be usefully supported by a methodological ‘toolkit’, including overviews, case-studies and costing models to provide guidance on developing best practice in DMP policy and infrastructure for these projects, as well as considering OAIS validation, audit and cost modelling. In this paper, we build on previous work with the LIGO collaboration to consider the role of DMP planning within these big science scenarios, and discuss how to apply current best practice. We discuss the result of the MaRDI-Gross project (Managing Research Data Infrastructures – Big Science), which has been developing a toolkit to provide guidelines on the application of best practice in DMP planning within big science projects. This is targeted primarily at projects’ engineering managers, but intending also to help funders collaborate on DMP plans which satisfy the requirements imposed on them. ",Data Management and Preservation Planning for Big Science,Edinburgh University,2013,,International Journal of Digital Curation,8,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1562,http://dx.doi.org/10.2218/ijdc.v8i1.248,"{'string_id': '10.2218/ijdc.v8i1.248', 'id_scheme': 'DOI'}","The LOCKSS system is a leading technology in the field of Distributed Digital Preservation. Libraries run LOCKSS boxes to collect and preserve content published on the Web in PC servers with local disk storage. They form nodes in a network that continually audits their content and repairs any damage. Libraries wondered whether they could use cloud storage for their LOCKSS boxes instead of local disks. We review the possible configurations, evaluate their technical feasibility, assess their economic feasibility, report on an experiment in which we ran a production LOCKSS box in Amazon’s cloud service, and describe some simulations of future costs of cloud and local storage. We conclude that current cloud storage services are not cost-competitive with local hardware for long term storage, including for LOCKSS boxes.",Distributed Digital Preservation in the Cloud,Edinburgh University,2013,,International Journal of Digital Curation,8,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1563,http://dx.doi.org/10.2218/ijdc.v8i1.249,"{'string_id': '10.2218/ijdc.v8i1.249', 'id_scheme': 'DOI'}","To preserve digital information it is vital that the format of that information can be identified, in-perpetuity. This is the major focus of research within the field of Digital Preservation. The National Archives of the UK called for the Digital Preservation and Digital Curation communities to develop a test corpus of digital objects to help further develop tools to aid this purpose. Following that call, an attempt has been made to develop the suite. This paper initially outlines a methodology to generate a skeleton corpus using simple user-generated digital objects. It then explores the lessons learnt in the generation of a corpus using scripting language techniques from the file format signatures described in The National Archives PRONOM technical registry. It will also discuss the use of the digital signature for this purpose, the benefits of developing a test corpus using this technique. Finally, this paper will outline a methodology for future research before exploring how the community can best make use of the output of this project and how this project needs to be taken forward to completion. ",Generation of a Skeleton Corpus of Digital Objects for the Validation and Evaluation of Format Identification Tools and Signatures,Edinburgh University,2013,,International Journal of Digital Curation,8,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1564,http://dx.doi.org/10.2218/ijdc.v8i1.250,"{'string_id': '10.2218/ijdc.v8i1.250', 'id_scheme': 'DOI'}","The changing world of IT services opens the chance to more tightly integrate digital long-term preservation into systems, both for commercial and end users. The emergence of cloud offerings re-centralizes services, and end users interact with them remotely through standardized (web-)client applications on their various devices. This offers the chance to use partially the same concepts and methods to access obsolete computer environments and allows for more sustainable business processes. In order to provide a large variety of user-friendly remote emulation services, especially in combination with authentic performance and user experience, a distributed system model and architecture is required, suitable to run as a cloud service, allowing for the specialization both of memory institutions and third party service providers. The shift of the usually non-trivial task of the emulation of obsolete software environments from the end user to specialized providers can help to simplify digital preservation and access strategies. Besides offering their users better access to their holdings, libraries and archives may gain new business opportunities to offer services to a third party, such as businesses requiring authentic reproduction of digital objects and processes for legal reasons. This paper discusses cloud concepts as the next logical step for accessing original digital material. Emulation-as-a-Service (EaaS) fills the gap between the successful demonstration of emulation strategies as a long term access strategy and it’s perceived availability and usability. EaaS can build upon the ground of research and prototypical implementations of previous projects, and reuse well established remote access technology. In this article we develop requirements and a system model, suitable for a distributed environment. We will discuss the building blocks of the core services as well as requirements regarding access management. Finally, we will try to present a business model and estimate costs to implement and run such a service. The implementations of EaaS will influence future preservation planning in memory institutions, as it shifts the focus on object access workflows. ",Towards Emulation-as-a-Service: Cloud Services for Versatile Digital Object Access,Edinburgh University,2013,,International Journal of Digital Curation,8,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1565,http://dx.doi.org/10.2218/ijdc.v8i1.251,"{'string_id': '10.2218/ijdc.v8i1.251', 'id_scheme': 'DOI'}","ISO 16363:2012, Space Data and Information Transfer Systems - Audit and Certification of Trustworthy Digital Repositories (ISO TRAC), outlines actions a repository can take to be considered trustworthy, but research examining whether the repository’s designated community of users associates such actions with trustworthiness has been limited. Drawing from this ISO document and the management and information systems literatures, this paper discusses findings from interviews with 66 archaeologists and quantitative social scientists. We found similarities and differences across the disciplines and among the social scientists. Both disciplinary communities associated trust with a repository’s transparency. However, archaeologists mentioned guarantees of preservation and sustainability more frequently than the social scientists, who talked about institutional reputation. Repository processes were also linked to trust, with archaeologists more frequently citing metadata issues and social scientists discussing data selection and cleaning processes. Among the social scientists, novices mentioned the influence of colleagues on their trust in repositories almost twice as much as the experts. We discuss the implications our findings have for identifying trustworthy repositories and how they extend the models presented in the management and information systems literatures.",Trust in Digital Repositories,Edinburgh University,2013,,International Journal of Digital Curation,8,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1566,http://dx.doi.org/10.2218/ijdc.v8i1.252,"{'string_id': '10.2218/ijdc.v8i1.252', 'id_scheme': 'DOI'}","In contemporary scientific research, standard-making and standardization are key processes for the sharing and reuse of data. The goals of this paper are twofold: 1) to stress that collaboration is crucial to standard-making, and 2) to urge recognition of metadata standardization as part of the scientific process. To achieve these goals, a participatory framework for developing and implementing scientific metadata standards is presented. We highlight the need for ongoing, open dialogue within and among research communities at multiple levels. Using the Long Term Ecological Research network adoption of the Ecological Metadata Language as a case example in the natural sciences, we illustrate how a participatory framework addresses the need for active coordination of the evolution of scientific metadata standards. The participatory framework is contrasted with a hierarchical framework to underscore how the development of scientific standards is a dynamic and continuing process. The roles played by ‘best practices’ and ‘working standards’ are identified in relation to the process of standardization.",Towards Standardization: A Participatory Framework for Scientific Standard-Making,Edinburgh University,2013,,International Journal of Digital Curation,8,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1567,http://dx.doi.org/10.2218/ijdc.v8i2.262,"{'string_id': '10.2218/ijdc.v8i2.262', 'id_scheme': 'DOI'}","The task repurposing of heterogeneous, distributed data for originally unintended research objectives is a non-trivial problem because the mappings required may not be precise. A particular case is clinical data collected for patient care being used for medical research. The fact that research repositories will record data differently means that assumptions must be made as how to transform of this data. Records of provenance that document how this process has taken place will enable users of the data warehouse to utilise the data appropriately and ensure that future data added from another source is transformed using comparable assumptions. For a provenance-based approach to be reusable and supportable with software tools, the provenance records must use a well-defined model of the transformation process. In this paper, we propose such a model, including a classification of the individual ‘sub-functions’ that make up the overall transformation. This model enables meaningful provenance data to be generated automatically. A case study is used to illustrate this approach and an initial classification of transformations that alter the information is created.",Informative Provenance for Repurposed Data: A Case Study using Clinical Research Data,Edinburgh University,2013,,International Journal of Digital Curation,8,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1568,http://dx.doi.org/10.2218/ijdc.v8i2.263,"{'string_id': '10.2218/ijdc.v8i2.263', 'id_scheme': 'DOI'}","Academic librarians are increasingly engaging in data curation by providing infrastructure (e.g., institutional repositories) and offering services (e.g., data management plan consultations) to support the management of research data on their campuses. Efforts to develop these resources may benefit from a greater understanding of disciplinary differences in research data management needs. After conducting a survey of data management practices and perspectives at our research university, we categorized faculty members into four research domains—arts and humanities, social sciences, medical sciences, and basic sciences—and analyzed variations in their patterns of survey responses. We found statistically significant differences among the four research domains for nearly every survey item, revealing important disciplinary distinctions in data management actions, attitudes, and interest in support services. Serious consideration of both the similarities and dissimilarities among disciplines will help guide academic librarians and other data curation professionals in developing a range of data-management services that can be tailored to the unique needs of different scholarly researchers.",Disciplinary differences in faculty research data management practices and perspectives,Edinburgh University,2013,,International Journal of Digital Curation,8,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1569,http://dx.doi.org/10.2218/ijdc.v8i2.268,"{'string_id': '10.2218/ijdc.v8i2.268', 'id_scheme': 'DOI'}","This paper describes work undertaken by Data Intensive Cyber Environments Center (DICE) at the University of North Carolina at Chapel Hill and the University of Liverpool on the development of an integrated preservation environment, which has been presented at the National Coordination Office for Networking and Information Technology Research and Development (NITRD), at the National Science Foundation, and at the European Commission. The underlying technology is based on the integrated Rule-Oriented Data System (iRODS), which implements a policy-based approach to distributed data management. By differentiating between different phases of the data life cycle based upon the evolution of data management policies, the infrastructure can be tuned to support data publication, data sharing, data analysis and data preservation. It is possible to build generic data management infrastructure that can evolve to meet the management requirements of each user community, federal agency and academic research project. In order to manage the properties of the data collections, we have developed and integrated scalable digital library services that support the discovery of, and access to, material organized as a collection.The integrated preservation environment prototype implements specific technologies that are capable of managing a wide range of preservation requirements, from parsing of legacy document formats, to enforcement of preservation policies, to validation of trustworthiness assessment criteria. Each capability has been demonstrated and is instantiated in multiple instances, both in the United States as part of the DataNet Federation Consortium (DFC) and through multiple European projects, primarily the FP7 SHAMAN project. ","Evolving Persistent Archives and Digital Library Systems: Integrating iRods, Cheshire3 and Multivalent",Edinburgh University,2013,,International Journal of Digital Curation,8,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1570,http://dx.doi.org/10.2218/ijdc.v9i1.288,"{'string_id': '10.2218/ijdc.v9i1.288', 'id_scheme': 'DOI'}"," The growth of television, and in particular television news, has created a challenge in preserving and providing access to the resulting material. At the same time, technology has opened many opportunities to capture this information and make it more widely available. In some ways, it is a race of technology against the speed of content creation. In this paper, we describe a very successful archival project that records, indexes, archives and makes available the totality of the programming of the U.S. based C-SPAN television network, a nonprofit network that telecasts the entirety of the U.S. congressional proceedings, hearings, presidential speeches and other public policy events. As such, it is an archive of unedited primary source events. The use of evolving technology over 25 years has made this archive possible and it exists free on the Internet for world-wide access. ","Creating an Online Television Archive, 1987–2013",Edinburgh University,2014,,International Journal of Digital Curation,9,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1571,http://dx.doi.org/10.2218/ijdc.v9i1.297,"{'string_id': '10.2218/ijdc.v9i1.297', 'id_scheme': 'DOI'}"," The plethora of new data sources, combined with a growing interest in increased access to previously unpublished data, poses a set of ethical challenges regarding individual privacy. This paper sets out one aspect of those challenges: the need to anonymise data in such a form that protects the privacy of individuals while providing sufficient data utility for data users. This issue is discussed using a case study of Scottish Government’s administrative data, in which disclosure risk is examined and data utility is assessed using a potential ‘real-world’ analysis. ",Examining Disclosure Risk and Data Utility: An Administrative Data Case Study,Edinburgh University,2014,,International Journal of Digital Curation,9,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1572,http://dx.doi.org/10.2218/ijdc.v9i1.298,"{'string_id': '10.2218/ijdc.v9i1.298', 'id_scheme': 'DOI'}"," Citizen Cyberscience Projects (CCPs) that recruit members of the public as volunteers to process and produce large datasets promise a great deal of benefits to scientists and science. However, if this promise is to be realised, and citizen science-produced datasets are to be widely used by scientists, it is essential that these datasets win the trust of the scientific community. This task of securing credibility involves, in part, applying standard scientific procedures to clean up datasets formed by volunteer contributions. However, the management of volunteers’ behaviour in terms of how they contribute also plays a significant role in improving both the quality of individual contributions and the overall robustness of the resultant datasets. This can assist CCPs in securing a reputation for producing trustworthy datasets. Through a case study of Galaxy Zoo, a CCP set up to generate datasets based on volunteer classifications of galaxy morphologies, this paper explores how those involved in running the project manage volunteers. In particular, it focuses on how methods for crediting volunteer contributions motivate volunteers to provide higher quality contributions and to behave in a way that better corresponds to statistical assumptions made when combining volunteer contributions into datasets. These methods have made a significant contribution to the success of the project in securing trust in these datasets, which have been well used by other scientists. Implications for practice are then presented for CCPs, providing a list of considerations to guide choices regarding how to credit volunteer contributions to improve the quality and trustworthiness of citizen science-produced datasets. ",Managing the Public to Manage Data: Citizen Science and Astronomy,Edinburgh University,2014,,International Journal of Digital Curation,9,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1573,http://dx.doi.org/10.2218/ijdc.v9i1.299,"{'string_id': '10.2218/ijdc.v9i1.299', 'id_scheme': 'DOI'}"," In eScience, where vast data collections are processed in scientific workflows, new risks and challenges are emerging. Those challenges are changing the eScience paradigm, mainly regarding digital preservation and scientific workflows. To address specific concerns with data management in these scenarios, the concept of the Data Management Plan was established, serving as a tool for enabling digital preservation in eScience research projects. We claim risk management can be jointly used with a Data Management Plan, so new risks and challenges can be easily tackled. Therefore, we propose an analysis process for eScience projects using a Data Management Plan and ISO 31000 in order to create a Risk Management Plan that can complement the Data Management Plan. The motivation, requirements and validation of this proposal are explored in the MetaGen-FRAME project, focused in Metagenomics. ",Data Management in Metagenomics: A Risk Management Approach,Edinburgh University,2014,,International Journal of Digital Curation,9,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1574,http://dx.doi.org/10.2218/ijdc.v9i1.301,"{'string_id': '10.2218/ijdc.v9i1.301', 'id_scheme': 'DOI'}"," We present a case study of data integration and reuse involving 12 researchers who published datasets in Open Context, an online data publishing platform, as part of collaborative archaeological research on early domesticated animals in Anatolia. Our discussion reports on how different editorial and collaborative review processes improved data documentation and quality, and created ontology annotations needed for comparative analyses by domain specialists. To prepare data for shared analysis, this project adapted editor-supervised review and revision processes familiar to conventional publishing, as well as more novel models of revision adapted from open source software development of public version control. Preparing the datasets for publication and analysis required significant investment of effort and expertise, including archaeological domain knowledge and familiarity with key ontologies. To organize this work effectively, we emphasized these different models of collaboration at various stages of this data publication and analysis project. Collaboration first centered on data editors working with data contributors, then widened to include other researchers who provided additional peer-review feedback, and finally the widest research community, whose collaboration is facilitated by GitHub’s version control system. We demonstrate that the “publish” and “push” models of data dissemination need not be mutually exclusive; on the contrary, they can play complementary roles in sharing high quality data in support of research. This work highlights the value of combining multiple models in different stages of data dissemination. ",Publishing and Pushing: Mixing Models for Communicating Research Data in Archaeology,Edinburgh University,2014,,International Journal of Digital Curation,9,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1575,http://dx.doi.org/10.2218/ijdc.v9i1.302,"{'string_id': '10.2218/ijdc.v9i1.302', 'id_scheme': 'DOI'}"," In this paper we describe eBird, a highly successful citizen science project. With over 150,000 participants worldwide and an accumulation of over 140,000,000 bird observations globally in the last decade, eBird has evolved into a major tool for scientific investigations in diverse fields such as ornithology, computer science, statistics, ecology and climate change. eBird’s impact in scientific research is grounded in careful data curation practices that pay attention to all stages of the data lifecycle, and attend to the needs of stakeholders engaged in that data lifecycle. We describe the important aspects of eBird, paying particular attention to the mechanisms to improve data quality; describe the data products that are available to the global community; investigate some aspects of the downloading community; and demonstrate significant results that derive from the use of openly-available eBird data. ",eBird: Curating Citizen Science Data for Use by Diverse Communities,Edinburgh University,2014,,International Journal of Digital Curation,9,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1576,http://dx.doi.org/10.2218/ijdc.v9i1.303,"{'string_id': '10.2218/ijdc.v9i1.303', 'id_scheme': 'DOI'}"," In the era of research infrastructures and big data, sophisticated data management practices are becoming essential building blocks of successful science. Most practices follow a data-centric approach, which does not take into account the processes that created, analysed and presented the data. This fact limits the possibilities for reliable verification of results. Furthermore, it does not guarantee the reuse of research, which is one of the key aspects of credible data-driven science. For that reason, we propose the introduction of the new concept of Process Management Plans, which focus on the identification, description, sharing and preservation of the entire scientific processes. They enable verification and later reuse of result data and processes of scientific experiments. In this paper we describe the structure and explain the novelty of Process Management Plans by showing in what way they complement existing Data Management Plans. We also highlight key differences, major advantages, as well as references to tools and solutions that can facilitate the introduction of Process Management Plans. ",Process Management Plans,Edinburgh University,2014,,International Journal of Digital Curation,9,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1577,http://dx.doi.org/10.2218/ijdc.v9i1.304,"{'string_id': '10.2218/ijdc.v9i1.304', 'id_scheme': 'DOI'}"," Data sharing is a difficult process for both the data producer and the data reuser. Both parties are faced with more disincentives than incentives. Data producers need to sink time and resources into adding metadata for data to be findable and usable, and there is no promise of receiving credit for this effort. Making data available also leaves data producers vulnerable to being scooped or data misuse. Data reusers also need to sink time and resources into evaluating data and trying to understand them, making collecting their own data a more attractive option. In spite of these difficulties, some data producers are looking for new ways to make data sharing and reuse a more viable option. This paper presents two cases from the surface and climate modeling communities, where researchers who produce data are reaching out to other researchers who would be interested in reusing the data. These cases are evaluated as a strategy to identify ways to overcome the challenges typically experienced by both data producers and data reusers. By working together with reusers, data producers are able to mitigate the disincentives and create incentives for sharing data. By working with data producers, data reusers are able to circumvent the hurdles that make data reuse so challenging. ",Data Producers Courting Data Reusers: Two Cases from Modeling Communities,Edinburgh University,2014,,International Journal of Digital Curation,9,1,"[{'value': '1746-8256', 'type': 'electronic'}]"
1578,http://dx.doi.org/10.2218/ijdc.v9i2.267,"{'string_id': '10.2218/ijdc.v9i2.267', 'id_scheme': 'DOI'}"," In recent years the Portable Document Format (PDF) has become a ubiquitous format in the exchange of documents; in 2005 the PDF/A profile was defined in order to meet long term accessibility needs, and has accordingly come to be regarded as a long-term archiving strategy for PDF files. In the field of archaeology, a growing number of PDF files – containing the detailed results of fieldwork and research – are beginning to be deposited with digital archives such as the Archaeology Data Service (ADS). In the ADS’ experience, the use of PDF/A has had benefits as well as drawbacks: the majority of PDF reports are now in a standard format better suited to longer-term access, however migrating to PDF/A and managing and ensuring reuse of these files is intensive, and fraught with potential pitfalls. Of these, perhaps the most serious has been an unreliability in PDF/A conformance by the wide range of tools and software now available. There are also practical and more theoretical implications for reuse which, as our discipline of archaeology alongside so many others rapidly becomes digitized, presents us with a large corpus of ‘data’ that is human readable, but may not be amenable to machine-based technologies such as NLP. It may be argued that these factors effectively undermine some of the perceived cost benefit of moving from paper to digital, as well as the longer-term sustainability of PDF/A within digital archives. ",The Use of PDF/A in Digital Archives: A Case Study from Archaeology,Edinburgh University,2014,,International Journal of Digital Curation,9,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1579,http://dx.doi.org/10.2218/ijdc.v9i2.272,"{'string_id': '10.2218/ijdc.v9i2.272', 'id_scheme': 'DOI'}","Based on existing appraisal/selection policies in libraries, archives, museum, social science and science data centers, this paper presents a generic appraisal/selection framework for digital curation. In presenting this framework, the author discusses how archival appraisal theories, methods, and criteria adapt to the general digital curation context.",Appraisal and Selection for Digital Curation,Edinburgh University,2014,,International Journal of Digital Curation,9,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1580,http://dx.doi.org/10.2218/ijdc.v9i2.294,"{'string_id': '10.2218/ijdc.v9i2.294', 'id_scheme': 'DOI'}"," Video games are a cultural phenomenon; a medium like no other that has become one of the largest entertainment sectors in the world. While the UK boasts an enviable games development heritage, it risks losing a major part of its cultural output through an inability to preserve the games that are created by the country’s independent games developers. The issues go deeper than bit rot and other problems that affect all digital media; loss of context, copyright and legal issues, and the throwaway culture of the ‘next’ game all hinder the ability of fans and academics to preserve video games and make them accessible in the future. This study looked at the current attitudes towards preservation in the UK’s independent (‘indie’) video games industry by examining current record-keeping practices and analysing the views of games developers. The results show that there is an interest in preserving games, and possibly a desire to do so, but issues of piracy and cost prevent the industry from undertaking preservation work internally, and from allowing others to assume such responsibility. The recommendation made by this paper is not simply for preservation professionals and enthusiasts to collaborate with the industry, but to do so by advocating the commercial benefits that preservation may offer to the industry. ",Video Game Preservation in the UK: A Survey of Records Management Practices,Edinburgh University,2014,,International Journal of Digital Curation,9,2,"[{'value': '1746-8256', 'type': 'electronic'}]"
1581,http://www.jstovr.org/stable/30200526,"{'string_id': '10.2307/30200526', 'id_scheme': 'DOI'}","If there is such a thing as a new computer-assisted literary criticism, its expression lies in a model that is as broad-based as that presented in John Smith's seminal article, ""Computer Criticism,"" and is as encompassing of the discipline of literary studies as it is tied to the evolving nature of the electronic literary text that lies at the heart of its intersection with computing. It is the desire to establish the parameters of such a model for the interaction between literary studies and humanities computing - for a model of the new computer-assisted literary criticism - that gave rise to the papers in this collection and to the several conference panel-presentations and discussions that, in their print form, these papers represent.",A New Computer-Assisted Literary Criticism?,Springer,2002,,Computers and the Humanities,36,3,"[{'value': '00104817', 'type': 'print'}]"
1582,http://www.jstor.org/stable/30200527,"{'string_id': '10.2307/30200527', 'id_scheme': 'DOI'}","The surviving texts of many of Shakespeare's plays include what might be termed a ""performance crux"": a moment that is puzzling to the director and actors, and which calls for some kind of stage business to justify or explain action. Sometimes the director's response is simply to remove the passage. This paper will look at a crux of this kind, and discuss how a modern, multimedia electronic edition can provide tools for the reader or actor to explore the possibilities both of the basic text and the performance that grows from it. In Romeo and Juliet there is an awkward moment when Friar Lawrence flees the tomb and deserts Juliet as he hears people approach. The plot requires that he be present as she awakens and absent as she commits suicide, but Shakespeare is not often as arbitrary as the received text (Quarto 2) makes him: ""Come go good Iuliet, I dare no longer stay. Exit."" By comparing this moment with Quarto 1 (the first published version of the text), Shakespeare's source, later adaptations, and some modern performances, the paper will discuss the mutual illumination of text and performance.",The Text of Performance and the Performance of Text in the Electronic Edition,Springer,2002,,Computers and the Humanities,36,3,"[{'value': '00104817', 'type': 'print'}]"
1583,http://www.jstor.org/stable/30200528,"{'string_id': '10.2307/30200528', 'id_scheme': 'DOI'}","The majority of humanities computing projects within the discipline of literature have been conceived more as digital libraries than monographs which utilise the medium as a site of interpretation. The impetus to conceive electronic research in this way comes from the underlying philosophy of texts and textuality implicit in SGML and its instantiation for the humanities, the TEI, which was conceived as ""a markup system intended for representing already existing literary texts"". This article explores the most common theories used to conceive electronic research in literature, such as hypertext theory, OCHO (Ordered Hierarchy of Content Objects), and Jerome J. McGann's ""noninformational"" forms of textuality. It also argues that as our understanding of electronic texts and textuality deepens, and as advances in technology progresses, other theories, such as Reception Theory and Versioning, may well be adapted to serve as a theoretical basis for conceiving research more akin to an electronic monograph than a digital library.",Computer-Mediated Texts and Textuality: Theory and Practice,Springer,2002,,Computers and the Humanities,36,3,"[{'value': '00104817', 'type': 'print'}]"
1584,http://www.jstor.org/stable/30200529,"{'string_id': '10.2307/30200529', 'id_scheme': 'DOI'}","Parallel to, and to some degree in reaction to French poststructuralist theorization (as championed by Derrida, Foucault, and Lacan, among others) is a French neo-structuralism built directly on the achievements of structuralism using electronic means. This paper examines some exemplary approaches to text analysis in this neo-structuralist vein: SATOR's topoi dictionary, the WinBrill POS tagger and Frangois Rastier's interpretative semantics. I consider how a computerassisted ""Wissenschaft"" accumulation of expertise complements the neo-structuralist approach. Ultimately, electronic critical studies will be defined by their strategic position at the intersection of the two chief technologies shaping our society: the new information processing technology of computers and the representational techniques that have accumulated for centuries in texts. Understanding how these two information management paradigms complement each other is a key issue for the humanities, for computer science, and vital to industry, even beyond the narrow realm of the language industries. The direction of critical studies, a small planet long orbiting in only rarefied academic circles, will be radically altered by the sheer size of the economic stakes implied by a new kind of text, the industrial text, the technological heart of an information society.",Industrial Text and French Neo-Structuralism,Springer,2002,,Computers and the Humanities,36,3,"[{'value': '00104817', 'type': 'print'}]"
1585,http://www.jstor.org/stable/30200530,"{'string_id': '10.2307/30200530', 'id_scheme': 'DOI'}","Deeply held humanist ideas constitute a residual discourse in our late age of print, a discourse that has not been effaced by the rise of technology or by widespread computing practices. Against humanism's Enlightenment idea of the rational individual, mid-twentieth century schools of anti-humanism postulated a ""subject"" constructed by, rather than controlling, its language, culture, and technologies. The contemporary notion of the subject comes from Lacanian psychoanalytic theory, where Lacan draws upon cybernetics and computing as evidence of a symbolic order constructive of subjectivity. This computational symbolic, in turn, owes much to Heidegger's post-war notions of technology. Today, however, work in new media suggests that the dominant discourse on the subject - the discourse underwriting contemporary theory - is being challenged by an emergent discourse of the posthuman. Consequently, theory itself is thrown into question. The question of theory now arises since we must ask whether the posthuman subject of technology is rewriting the anti-humanist subject of theory in new and unanticipated ways. This article thus offers an overview of the shift from humanist, to anti-humanist, to posthumanist assumptions. My goal is to help readers decide whether today's computing environments can still be approached through late twentieth century anti-humanist theories or whether e-texts demand new, media-specific analyses.","The Question concerning Theory: Humanism, Subjectivity, and Computing",Springer,2002,,Computers and the Humanities,36,3,"[{'value': '00104817', 'type': 'print'}]"
1586,http://www.jstor.org/stable/30200531,"{'string_id': '10.2307/30200531', 'id_scheme': 'DOI'}","A range of inter-disciplinary discourses consider the computer-mediated writing space as a unique performance medium with characteristic protocols. Drawing on contemporary performance theory, literary criticism, and communication theory, the author proposes that technologists, academics, and artists are developing idiomatic rhetorics - a lingua franca - to explore the technical and expressive properties of the new ""language machines"" and their hypertextual environments. The role of improvisation - and its cross-disciplinary protocols - provides a further focus in the discussion of computing practice and performance.",Animating the Language Machine: Computers and Performance,Springer,2002,,Computers and the Humanities,36,3,"[{'value': '00104817', 'type': 'print'}]"
1587,http://www.jstor.org/stable/30200532,"{'string_id': '10.2307/30200532', 'id_scheme': 'DOI'}",Computer games have not been adequately theorized within the humanities. In this paper a brief history of computer games is presented as a starting point for developing a topology of games and a theory of computer games as rhetorical artifacts suitable for critical study. The paper addresses the question of why games should be treated seriously and suggests a theoretical approach based on Bakhtin's poetics of the novel where the experience of time and space (the chronotope) provides a framework of questions for discussing computer games.,Gore Galore: Literary Theory and Computer Games,Springer,2002,,Computers and the Humanities,36,3,"[{'value': '00104817', 'type': 'print'}]"
1588,http://www.jstor.org/stable/30200533,"{'string_id': '10.2307/30200533', 'id_scheme': 'DOI'}","Looking specifically at the genre of adaptive narrative, this article explores the future of literature created for and with computer technology, focusing primarily on the trope of mutability as it is played out with new media. Some of the questions asked are: What can the medium of a work of literature, that is its material aspect, tell us about the text? About character? What can it possibly matter if narrative is recounted on papyrus, retold on parchment and rag, and then remediated in pixels? Isn't it the message carried by the medium we are most concerned with, stable or unstable throughout the process of inscription, reinscription, encoding and decoding, translation and remediation? This paper speculates about possibilities rather than attempts to answer these questions, but the structuring and mean-making components considered here stand as examples of some we may want to think about when developing future theories about literature - and all types of writing generated by and for electronic environments.","Mutability, Medium, and Character",Springer,2002,,Computers and the Humanities,36,3,"[{'value': '00104817', 'type': 'print'}]"
1589,http://www.jstor.org/stable/30204526,"{'string_id': '10.2307/30204526', 'id_scheme': 'DOI'}","The database Profil has been set up to offer readers studying modern literary manuscripts a reference tool to identify watermarked papers. In the study of writers' drafts as in artists' sketches, the different kinds of papers used provide valuable information on the genesis of a work of art and watermarks, when they exist, are the best visible hint allowing us to identify paper. A multimedia database, with digitized images more precise than usual traced design, seems to be appropriate to register, visualize, and compare modern watermarked papers. Besides its usefulness for specialists, such a database bearing on modern manuscripts should also be conceived in a didactic perspective, as it is oriented towards literary scholars who are not particularly familiar with the history of modern paper. In this paper we present the database Profil which includes a set of digitized images from a collection of betagraphies made by the reproduction service of the National French Library. Then we explain problems of database normalization when human sciences are involved.",Profil: An Iconographic Database for Modern Watermarked Papers,Springer,2002,,Computers and the Humanities,36,2,"[{'value': '00104817', 'type': 'print'}]"
1590,http://www.jstor.org/stable/30204527,"{'string_id': '10.2307/30204527', 'id_scheme': 'DOI'}","Comprehensive computational lexicons are essential to practical natural language processing (NLP). To compile such computational lexicons by automatically acquiring lexical information, however, we previously require sufficiently large corpora. This study aims at predicting the ideal size of such automatic-lexical-acquisition oriented corpora, focusing on six specific factors: (1) specific versus general purpose prediction, (2) variation among corpora, (3) base forms versus inflected forms, (4) open class items, (5) homographs, and (6) unknown words. Another important and related issue with regard to predictability has something to do with data sparseness. Research using the TOTAL Corpus reveals serious data sparseness in this corpus. This, again, points towards the importance and necessity of reducing data sparseness to a satisfactory level for the automatic lexical acquisition and reliable corpus predictions. The functions of predicting the number of tokens and lemmas in a corpus are based on the piecewise curve-fitting algorithm. Unfortunately, the predicted size of a corpus for automatic lexical acquisition is too astronomical to compile it by using presently existing compiling strategies. Therefore, we suggest a practical and efficient alternative method. We are confident that this study will shed new light on issues such as corpus predictability, compiling strategies and linguistic comprehensiveness.",On the Corpus Size Needed for Compiling a Comprehensive Computational Lexicon by Automatic Lexical Acquisition,Springer,2002,,Computers and the Humanities,36,2,"[{'value': '00104817', 'type': 'print'}]"
1591,http://www.jstor.org/stable/30204528,"{'string_id': '10.2307/30204528', 'id_scheme': 'DOI'}","We describe how to build a large comprehensive, integrated Arabic lexicon by automatic parsing of newspaper text. We have built a parser system to read Arabic newspaper articles, isolate the tokens from them, find the part of speech, and the features for each token. To achieve this goal we designed a set of algorithms, we generated several sets of rules, and we developed a set of techniques, and a set of components to carry out these techniques. As each sentence is processed, new words and features are added to the lexicon, so that it grows continuously as the system runs. To test the system we have used 100 articles (80,444 words) from the Al-Raya newspaper. The system consists of several modules: the tokenizer module to isolate the tokens, the type finder system to find the part of speech of each token, the proper noun phrase parser module to mark the proper nouns and to discover some information about them and the feature finder module to find the features of the words.",Extracting an Arabic Lexicon from Arabic Newspaper Text,Springer,2002,,Computers and the Humanities,36,2,"[{'value': '00104817', 'type': 'print'}]"
1592,http://www.jstor.org/stable/30204529,"{'string_id': '10.2307/30204529', 'id_scheme': 'DOI'}","This paper presents the design, implementation and evaluation of GATE, a General Architecture for Text Engineering. GATE lies at the intersection of human language computation and software engineering, and constitutes an infrastructural system supporting research and development of language processing software.","GATE, a General Architecture for Text Engineering",Springer,2002,,Computers and the Humanities,36,2,"[{'value': '00104817', 'type': 'print'}]"
1593,http://www.jstor.org/stable/30204685,"{'string_id': '10.2307/30204685', 'id_scheme': 'DOI'}","We present statistical models for morphological disambiguation in agglutinative languages, with a specific application to Turkish. Turkish presents an interesting problem for statistical models as the potential tag set size is very large because of the productive derivational morphology. We propose to handle this by breaking up the morhosyntactic tags into inflectional groups, each of which contains the inflectional features for each (intermediate) derived form. Our statistical models score the probability of each morhosyntactic tag by considering statistics over the individual inflectional groups and surface roots in trigram models. Among the four models that we have developed and tested, the simplest model ignoring the local morphotactics within words performs the best. Our best trigram model performs with 93.95% accuracy on our test data getting all the morhosyntactic and semantic features correct. If we are just interested in syntactically relevant features and ignore a very small set of semantic features, then the accuracy increases to 95.07%.",Statistical Morphological Disambiguation for Agglutinative Languages,Springer,2002,,Computers and the Humanities,36,4,"[{'value': '00104817', 'type': 'print'}]"
1594,http://www.jstor.org/stable/30204686,"{'string_id': '10.2307/30204686', 'id_scheme': 'DOI'}","The measure of the lexical richness of literary texts as a tool in the comparative analysis of literary style has been hampered by the problem of the inequality of text lengths within and between literary corpora. This paper proposes an empirical method of description of lexical richness by averaging measures on multiple chunks of text of a standard length within a literary work or corpus. A work's average vocabulary richness, average portion of hapax legomena of the corpus from which it derives, and average repetition of frequently appearing vocabulary may then characterize that work relative to other works partitioned along with it. This method reveals the possibility of significant variance of these measures of vocabulary among works of a single author's corpus and warns against the notion of some absolute authorial stylistic character. We apply this method of vocabulary averaging to the corpora of three playwrights from classical antiquity whose works are chronologically rankable: Euripides, Aristophanes, and Terence. We look for trends in vocabulary richness over time, which we posit functions as an indicator of progressively changing authorial ability or inclination. This method then holds the potential of predicting dates for undateable or tenuously dated works within a corpus of otherwise securely dated texts. From the results derived, a relatively late date for the composition of the redrafted version of Aristophanes' Clouds appears likely; we predict an early composition date for the redraft of Terence's Hecyra (and thus are inclined to think that the playwright did very little redrafting); and finally we find Euripides' Electra and Supplices exhibiting vocabulary characteristics of extremely late composition and we predict dates much later than those assigned based on metrical considerations.",Stylistic Constancy and Change across Literary Corpora: Using Measures of Lexical Richness to Date Works,Springer,2002,,Computers and the Humanities,36,4,"[{'value': '00104817', 'type': 'print'}]"
1595,http://www.jstor.org/stable/30204687,"{'string_id': '10.2307/30204687', 'id_scheme': 'DOI'}","Korean Combinatory Categorial Grammar (KCCG) is an extended combinatory categorial grammar formalism to capture the syntax and interpretation of a ""relative free"" word order, long distance scrambling, and other specific characteristics of Korean. KCCG formalism can uniformly handle word order variations among arguments and adjuncts within a clause, as well as in complex clauses and across clause boundaries, i.e. long distance scrambling. The approach we develop takes advantage of the ability of CCG for type raising and composition along with the ability of variable categories and unordered argument modeling for relatively free word order treatment (Lee et al., 1994; Lee et al., 1997). We apply a probability model and heuristics using Korean characteristics to our KCCG parser. Results of the experiments on various text genre show that the KCCG parser performs at 87.67/87.03% constituent precision/recall.",Korean Combinatory Categorial Grammar and Statistical Parsing,Springer,2002,,Computers and the Humanities,36,4,"[{'value': '00104817', 'type': 'print'}]"
1596,http://www.jstor.org/stable/30204696,"{'string_id': '10.2307/30204696', 'id_scheme': 'DOI'}","Users need more sophisticated tools to handle the growing number of image-based documents available in databases. In this paper, we present a system devoted to the editing and browsing of complex literary hypermedia including original manuscript documents and other handwritten sources. Editing capabilities allow the user to transcribe manuscript images in an interactive way and to encode the resulting textual representation by means of a logical markup language (based on the XML/TEI specification). Both representations (image and structured text) are tightly linked to facilitate the reading and the interpretation of documents. This text/image coupling scheme is an attempt to unify several layers of information in order to provide the user with a global vision of the work. Our system also supplies tools capable of processing and relating information stored both in images and structured texts. Finally, application-specific visualization techniques have been developed in order to provide users with a way to identify relationships between source documents and help them to navigate.",Text-Image Coupling for Editing Literary Sources,Springer,2002,,Computers and the Humanities,36,1,"[{'value': '00104817', 'type': 'print'}]"
1597,http://www.jstor.org/stable/30204788,"{'string_id': '10.2307/30204788', 'id_scheme': 'DOI'}","SENSEVAL was the first open, community-based evaluation exercise for Word Sense Disambiguation programs. It adopted the quantitative approach to evaluation developed in MUC and other ARPA evaluation exercises. It took place in 1998. In this paper we describe the structure, organisation and results of the SENSEVAL exercise for English. We present and defend various design choices for the exercise, describe the data and gold-standard preparation, consider issues of scoring strategies and baselines, and present the results for the 18 participating systems. The exercise identifies the state-of-the-art for fine-grained word sense disambiguation, where training data is available, as 74-78% correct, with a number of algorithms approaching this level of performance. For systems that did not assume the availability of training data, performance was markedly lower and also more variable. Human inter-tagger agreement was high, with the gold standard taggings being around 95% replicable.",Framework and Results for English SENSEVAL,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1598,http://www.jstor.org/stable/30204790,"{'string_id': '10.2307/30204790', 'id_scheme': 'DOI'}","In this paper we present some observations concerning an experiment of (manual/ automatic) semantic tagging of a small Italian corpus performed within the framework of the SENSEVAL/ROMANSEVAL initiative. The main goal of the initiative was to set up a framework for evaluation of Word Sense Disambiguation systems (WSDS) through the comparative analysis of their performance on the same type of data. In this experiment there are two aspects which are of relevance: first, the preparation of the reference annotated corpus, and, second, the evaluation of the systems against it. In both aspects we are mainly interested here in the analysis of the linguistic side which can lead to a better understanding of the problem of semantic annotation of a corpus, be it manual or automatic annotation. In particular, we will investigate, firstly, the reasons for disagreement between human annotators, secondly, some linguistically relevant aspects of the performance of the Italian WSDS and, finally, the lessons learned from the present experiment.",Senseval/Romanseval: The Framework for Italian,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1599,http://www.jstor.org/stable/30204791,"{'string_id': '10.2307/30204791', 'id_scheme': 'DOI'}","We present methods for evaluating human and automatic taggers that extend current practice in three ways. First, we show how to evaluate taggers that assign multiple tags to each test instance, even if they do not assign probabilities. Second, we show how to accommodate a common property of manually constructed ""gold standards"" that are typically used for objective evaluation, namely that there is often more than one correct answer. Third, we show how to measure performance when the set of possible tags is tree-structured in an IS-A hierarchy. To illustrate how our methods can be used to measure inter-annotator agreement, we show how to compute the kappa coefficient over hierarchical tag sets.",Tagger Evaluation Given Hierarchical Tag Sets,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1600,http://www.jstor.org/stable/30204792,"{'string_id': '10.2307/30204792', 'id_scheme': 'DOI'}","SENSEVAL set itself the task of evaluating automatic word sense disambiguation programs (see Kilgarriff and Rosenzweig, this volume, for an overview of the framework and results). In order to do this, it was necessary to provide a 'gold standard' dataset of 'correct' answers. This paper will describe the lexicographic part of the process involved in creating that dataset. The primary objective was for a group of lexicographers to manually examine keywords in a large number of corpus contexts, and assign to each context a sense-tag for the keyword, taken from the Hector dictionary. Corpus contexts also had to be manually part-of-speech (POS) tagged. Various observations made and insights gained by the lexicographers during this process will be presented, including a critique of the resources and the methodology.",Peeling an Onion: The Lexicographer's Experience of Manual Sense-Tagging,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1601,http://www.jstor.org/stable/30204794,"{'string_id': '10.2307/30204794', 'id_scheme': 'DOI'}","This work combines a set of available techniques - which could be further extended to perform noun sense disambiguation. We use several unsupervised techniques (Rigau et al., 1997) that draw knowledge from a variety of sources. In addition, we also apply a supervised technique in order to show that supervised and unsupervised methods can be combined to obtain better results. This paper tries to prove that using an appropriate method to combine those heuristics we can disambiguate words in free running text with reasonable precision.",Combining Supervised and Unsupervised Lexical Knowledge Methods for Word Sense Disambiguation,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1602,http://www.jstor.org/stable/30204795,"{'string_id': '10.2307/30204795', 'id_scheme': 'DOI'}","The selectional preferences of verbal predicates are an important component of a computational lexicon. They have frequently been cited as being useful for WSD, alongside other sources of knowledge. We evaluate automatically acquired selectional preferences on the level playing field provided by SENSEVAL to examine to what extent they help in WSD.",Word Sense Disambiguation Using Automatically Acquired Verbal Preferences,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1603,http://www.jstor.org/stable/30204796,"{'string_id': '10.2307/30204796', 'id_scheme': 'DOI'}","TLC is a supervised training (S) system that uses a Bayesian statistical model and features of a word's context to identify word sense. We describe the classifier's operation and how it can be configured to use only topical context cues, only local cues, or a combination of both. Our results on Senseval's final run are presented along with a comparison to the performance of the best S system and the average for S systems. We discuss ways to improve TLC by enriching its feature set and by substituting other decision procedures for the Bayesian model. Future development of supervised training classifiers will depend on the availability of tagged training data. TLC can assist in the hand-tagging effort by helping human taggers locate infrequent senses of polysemous words.",A Topical/Local Classifier for Word Sense Identification,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1604,http://www.jstor.org/stable/30204798,"{'string_id': '10.2307/30204798', 'id_scheme': 'DOI'}",We describe a simple approach to word sense disambiguation using information filtering and extraction. The method fully exploits and extends the information available in the Hector dictionary. The algorithm proceeds by the application of several filters to prune the candidate set of word senses returning the most frequent if more than one remains. The experimental methodology and its implication are also discussed.,Word Sense Disambiguation by Information Filtering and Extraction,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1605,http://www.jstor.org/stable/30204799,"{'string_id': '10.2307/30204799', 'id_scheme': 'DOI'}","A word sense disambiguation system which is going to be used as part of a NLP system needs to be large scale, able to be optimised towards a specific task and above all accurate. This paper describes the knowledge sources used in a disambiguation system able to achieve all three of these criteria. It is a hybrid system combining sub-symbolic, stochastic and rule-based learning. The paper reports the results achieved in Senseval and analyses them to show the system's strengths and weaknesses relative to other similar systems.",Large Scale WSD Using Learning Applied to SENSEVAL,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1606,http://www.jstor.org/stable/30204800,"{'string_id': '10.2307/30204800', 'id_scheme': 'DOI'}","A Classification Information Model is a pattern classification model. The model decides the proper class of an input instance by integrating individual decisions, each of which is made with each feature in the pattern. Each individual decision is weighted according to the distributional property of the feature deriving the decision. An individual decision and its weight are represented as classification information which is extracted from the training instances. In the word sense disambiguation based on the model, the proper sense of an input instance is determined by the weighted sum of whole individual decisions derived from the features contained in the instance.",Word Sense Disambiguation Using the Classification Information Model: Experimental Results on the SENSEVAL Workshop,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1607,http://www.jstor.org/stable/30204802,"{'string_id': '10.2307/30204802', 'id_scheme': 'DOI'}","The CL Research Senseval system was the highest performing system among the ""Allwords"" systems, with an overall fine-grained score of 61.6 percent for precision and 60.5 percent for recall on 98 percent of the 8,448 texts on the revised submission (up by almost 6 and 9 percent from the first). The results were achieved with an almost complete reliance on syntactic behavior, using (1) a robust and fast ATN-style parser producing parse trees with annotations on nodes, (2) DIMAP dictionary creation and maintenance software (after conversion of the Hector dictionary files) to hold dictionary entries, and (3) a strategy for analyzing the parse trees in concert with the dictionary data. Further considerable improvements are possible in the parser, exploitation of the Hector data (and representation of dictionary entries), and the analysis strategy, still with syntactic and collocational data. The Senseval data (the dictionary entries and the corpora) provide an excellent testbed for understanding the sources of failures and for evaluating changes in the CL Research system.",Senseval: The CL Research Experience,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1608,http://www.jstor.org/stable/30204803,"{'string_id': '10.2307/30204803', 'id_scheme': 'DOI'}","This paper describes the grling-sdm system, which is a supervised probabilistic classifier that participated in the 1998 SENSEVAL competition for word-sense disambiguation. This system uses model search to select decomposable probability models describing the dependencies among the feature variables. These types of models have been found to be advantageous in terms of efficiency and representational power. Performance on the SENSEVAL evaluation data is discussed.","Selecting Decomposable Models for Word-Sense Disambiguation: The ""Grling-Sdm"" System",Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1609,http://www.jstor.org/stable/30204804,"{'string_id': '10.2307/30204804', 'id_scheme': 'DOI'}","Wisdom is a system for performing word sense disambiguation (WSD) using a limited number of linguistic features and a simple supervised learning algorithm. The most likely sense tag for a word is determined by calculating co-occurrence statistics for words appearing within a small window. This paper gives a brief description of the components in the Wisdom system and the algorithm used to predict the correct sense tag. Some results for Wisdom from the Senseval competition are presented, and directions for future work are also explored.",Simple Word Sense Discrimination: Towards Reduced Complexity,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1610,http://www.jstor.org/stable/30204805,"{'string_id': '10.2307/30204805', 'id_scheme': 'DOI'}","We describe a memory-based classification architecture for word sense disambiguation and its application to the SENSEVAL evaluation task. For each ambiguous word, a semantic word expert is automatically trained using a memory-based approach. In each expert, selecting the correct sense of a word in a new context is achieved by finding the closest match to stored examples of this task. Advantages of the approach include (i) fast development time for word experts, (ii) easy and elegant automatic integration of information sources, (iii) use of all available data for training the experts, and (iv) relatively high accuracy with minimal linguistic engineering.",Memory-Based Word Sense Disambiguation,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1611,http://www.jstor.org/stable/30204806,"{'string_id': '10.2307/30204806', 'id_scheme': 'DOI'}","This paper describes a supervised algorithm for word sense disambiguation based on hierarchies of decision lists. This algorithm supports a useful degree of conditional branching while minimizing the training data fragmentation typical of decision trees. Classifications are based on a rich set of collocational, morphological and syntactic contextual features, extracted automatically from training data and weighted sensitive to the nature of the feature and feature class. The algorithm is evaluated comprehensively in the SENSEVAL framework, achieving the top performance of all participating supervised systems on the 36 test words where training data is available.",Hierarchical Decision Lists for Word Sense Disambiguation,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1612,http://www.jstor.org/stable/30204807,"{'string_id': '10.2307/30204807', 'id_scheme': 'DOI'}",This paper describes the evaluation of a WSD method within SENSEVAL. This method is based on Semantic Classification Trees (SCTs) and short context dependencies between nouns and verbs. The training procedure creates a binary tree for each word to be disambiguated. SCTs are easy to implement and yield some promising results. The integration of linguistic knowledge could lead to substantial improvement.,Using Semantic Classification Trees for WSD,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1613,http://www.jstor.org/stable/30204809,"{'string_id': '10.2307/30204809', 'id_scheme': 'DOI'}","The paper describes SENSE, a word sense disambiguation system that makes use of different types of cues to infer the most likely sense of a word given its context. Architecture and functioning of the system are briefly illustrated. Results are given for the ROMANSEVAL Italian test corpus of verbs.",ROMANSEVAL: Results for Italian by SENSE,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1614,http://www.jstor.org/stable/30204811,"{'string_id': '10.2307/30204811', 'id_scheme': 'DOI'}","This paper specifically addresses the question of polysemy with respect to verbs, and whether or not the sense distinctions that are made in on-line lexical resources such as WordNet are appropriate for computational lexicons. The use of sets of related syntactic frames and verb classes are examined as a means of simplifying the task of defining different senses, and the importance of concrete criteria such as different predicate argument structures, semantic class constraints and lexical co-occurrences is emphasized.",Consistent Criteria for Sense Distinctions,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1615,http://www.jstor.org/stable/30204812,"{'string_id': '10.2307/30204812', 'id_scheme': 'DOI'}","This article reports the results of a preliminary analysis of translation equivalents in four languages from different language families, extracted from an on-line parallel corpus of George Orwell's Nineteen Eighty-Four. The goal of the study is to determine the degree to which translation equivalents for different meanings of a polysemous word in English are lexicalized differently across a variety of languages, and to determine whether this information can be used to structure or create a set of sense distinctions useful in natural language processing applications. A coherence index is computed that measures the tendency for different senses of the same English word to be lexicalized differently, and from this data a clustering algorithm is used to create sense hierarchies.",Cross-Lingual Sense Determination: Can It Work?,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1616,http://www.jstor.org/stable/30204813,"{'string_id': '10.2307/30204813', 'id_scheme': 'DOI'}","The paper examines the task of Word Sense Disambiguation (WSD) critically and compares it with Part of Speech (POS) tagging, arguing that the ability of a writer to create new senses distinguishes the tasks and makes it more problematic to test WSD by the mark-up-and-model paradigm, because new senses cannot be marked up against dictionaries. This serves to set WSD apart and puts limits on its effectiveness as an independent NLP task. Moreover, it is argued that current WSD methods based on very small word samples are also potentially misleading because they may or may not scale up. Since all-word WSD methods are now available and are producing figures comparable to the smaller scale tasks, it is argued that we should concentrate on the former and find ways of bootstrapping test materials for such tests in the future.",Is Word Sense Disambiguation Just One More NLP Task?,Springer,2000,,Computers and the Humanities,34,1/2,"[{'value': '00104817', 'type': 'print'}]"
1617,http://www.jstor.org/stable/30204818,"{'string_id': '10.2307/30204818', 'id_scheme': 'DOI'}","The article describes how the British Government sees the electronic campus as a ""quick fix"" for delivering its idea of the learning society and mass higher (and further) education. It suggests that this solution poses a number of major difficulties, especially for the humanities. The issues are located in a global context in which it is argued that so-called ""mega-universities"" will come to dominate course production and distribution. This development will have profound implications for notions of institutional autonomy and the autonomy of individual teachers, and may undermine academic pluralism. The article concludes by arguing that, in the humanities, interventions are needed to secure an appropriate meld between the existing ""face-to-face"" methods of teaching and learning, and the new technology.",Tailor-Made or Off-the-Peg? Virtual Courses in the Humanities,Springer,2000,,Computers and the Humanities,34,3,"[{'value': '00104817', 'type': 'print'}]"
1618,http://www.jstor.org/stable/30204819,"{'string_id': '10.2307/30204819', 'id_scheme': 'DOI'}","This paper discusses selected aspects of the work of the CTI Centre for Textual Studies, a Centre which has its roots in a 1984 initiative and ceased to operate in 1999. The work of the Centre was grounded in humanities computing, a subject area which itself has developed over time. The article compares earlier observations made by Joseph Raben and Susan Hockey about the integration of resources within humanities teaching and learning, to current realities. Its focus is the development of access to distributed resources, beginning with an interface between the early PC and the mainframe and ending with a vision of a humanities portal to distributed resources.",From Concordances to Subject Portals: Supporting the Text-Centred Humanities Community,Springer,2000,,Computers and the Humanities,34,3,"[{'value': '00104817', 'type': 'print'}]"
1619,http://www.jstor.org/stable/30204820,"{'string_id': '10.2307/30204820', 'id_scheme': 'DOI'}","The article offers a case study of the relationship between current developments in Classical Studies and the impact of computing and IT. The first section summarises the main features of the Classical Studies environment, especially the deep seated changes which have been taking place. These changes are then related to specific initiatives in Research, Teaching and Learning. The discussion is framed by a statement of micro-criteria for the evaluation of new developments and by reference to the macro-climate of debate about the nature of cyberspace, especially the dichotomy between conceptions of post-modern diversity and of Enlightenment images of rational structures. It is suggested that these debates mirror those with which the discipline itself engages.",Electrifying the Canon: The Impact of Computing on Classical Studies,Springer,2000,,Computers and the Humanities,34,3,"[{'value': '00104817', 'type': 'print'}]"
1620,http://www.jstor.org/stable/30204821,"{'string_id': '10.2307/30204821', 'id_scheme': 'DOI'}","Web-accessible conferencing software and ""conversational ethics"" drawn from Habermas and Rawls have successfully brought together on-line participants separated by geography and viewpoint, and occasionally resulted in consensus regarding otherwise divisive issues such as abortion. The author describes successes, limitations, and costs of incorporating these technologies and discourse ethics in a religious studies class. Results are striking, but the pedagogical benefits involve technical risks and high labor and time costs. This experience, coupled with recent research, suggests that electronic pedagogies, like other teaching strategies, work for some, but not all students: this argues that we take up electronic teaching as one approach among many.",Wag the Dog? Online Conferencing and Teaching,Springer,2000,,Computers and the Humanities,34,3,"[{'value': '00104817', 'type': 'print'}]"
1621,http://www.jstor.org/stable/30204823,"{'string_id': '10.2307/30204823', 'id_scheme': 'DOI'}","This article presents an overview of developments in computer-aided history teaching in higher education in the UK and the US, which have focused primarily on providing students with access to primary sources in order to enhance their understanding of historical methods and content. From an initial emphasis on research training for postgraduates, which taught quantitative methods, or the use of drill-style question-and-answer programs, advances in hardware capacity and software provision have led to more varied methods of analysis. Computer-assisted learning lends itself particularly to the growing emphasis on students' use of primary sources, as more texts become available in electronic format on CD-ROM or the World Wide Web. Hypermedia can provide a unique learning environment in which students are exposed to different genres of sources such as images, texts and numerical data, encouraging them to discover interconnections and complexities, while learning at their own pace. Students can be expected to develop critical skills by comparing primary sources and forming their own historical interpretations. The various problems and methods of locating and assessing relevant information in cyberspace also foster critical thinking and a spirit of investigation. A computer-assisted course taught in Glasgow showed that students value the ease of access to relevant source materials offered by customised resource packs, which left them with more time to evaluate their contents critically.",Computers and Resource-Based History Teaching: A UK Perspective,Springer,2000,,Computers and the Humanities,34,3,"[{'value': '00104817', 'type': 'print'}]"
1622,http://www.jstor.org/stable/30204829,"{'string_id': '10.2307/30204829', 'id_scheme': 'DOI'}","This article deals with using digitised maps in historical research and their possible contributions to it. The use of cartographic data is especially useful in research dealing with the spatial distribution of various phenomena. The spatial distribution of rural social strata is one such phenomenon that has not yet received much attention. The case study of Pischelsdorf, an Austrian settlement, in the beginning of the nineteenth century shall serve as an example for such research. Pischelsdorf is situated in the Austrian province of Styria, about 150 km from Vienna, and served as a small centre for trade and handicraft for the surrounding villages. This study is based on the land register of 1822 which has been digitised.",Spatial Distribution of Rural Social Strata: Using Digitised Maps in Historical Research,Springer,2000,,Computers and the Humanities,34,4,"[{'value': '00104817', 'type': 'print'}]"
1623,http://www.jstor.org/stable/30204830,"{'string_id': '10.2307/30204830', 'id_scheme': 'DOI'}","William Faulkner's non-chronological story telling style has long been a challenge to critics and a puzzle to beginning literature students. ""A Rose for Emily,"" one of Faulkner's most frequently anthologized stories, exemplifies the complexity of Faulkner's treatment of time. In this paper, we apply a constraint-based problem solving method to an analysis of the chronology of ""A Rose for Emily."" Constraint logic programming is a declarative programming language paradigm that solves problems by enforcing constraints among variables. CLP's ability to sort numeric variables that do not yet have definite values makes it possible to sort the events of ""A Rose for Emily"" with only fragmented and relative time information. In attempting to sort the events of the story, we find an inconsistency in the temporal references scattered throughout the narrative. After removing this inconsistency, we are able to compare our chronology with earlier ones and discuss the thematic relevance of Faulkner's nonlinear plots.","Using Constraint Logic Programming to Analyze the Chronology in ""A Rose for Emily""",Springer,2000,,Computers and the Humanities,34,4,"[{'value': '00104817', 'type': 'print'}]"
1624,http://www.jstor.org/stable/30204831,"{'string_id': '10.2307/30204831', 'id_scheme': 'DOI'}","An architecture for federating heterogeneous dictionary databases is described. It proposes a common description language and query language to provide for the exchange of information between databases with different organizations, on different platforms and in different DBMSs. The common query language has an SQL like structure. The first version of the description language follows the TEI standard tag definitions for dictionaries with the expectation that the description language will be expanded in the future. A practical implementation of the proposals using WWW technology for two multi-lingual dictionaries is described.",An Architecture and Query Language for a Federation of Heterogeneous Dictionary Databases,Springer,2000,,Computers and the Humanities,34,4,"[{'value': '00104817', 'type': 'print'}]"
1625,http://www.jstor.org/stable/30204837,"{'string_id': '10.2307/30204837', 'id_scheme': 'DOI'}","In this paper a number of issues relating to the application of string processing techniques on musical sequences are discussed. A brief survey of some musical string processing algorithms is given and some issues of melodic representation, abstraction, segmentation and categorisation are presented. This paper is not intended to provide solutions to string processing problems but rather to highlight possible stumbling-block areas and raise awareness of primarily music-related particularities that can cause problems in matching applications.","Pattern Processing in Melodic Sequences: Challenges, Caveats and Prospects",Springer,2001,,Computers and the Humanities,35,1,"[{'value': '00104817', 'type': 'print'}]"
1626,http://www.jstor.org/stable/30204838,"{'string_id': '10.2307/30204838', 'id_scheme': 'DOI'}","We consider several perceptual issues in the context of machine recognition of music patterns. It is argued that a successful implementation of a music recognition system must incorporate perceptual information and error criteria. We discuss several measures of rhythm complexity which are used for determining relative weights of pitch and rhythm errors. Then, a new method for determining a localized tonal context is proposed. This method is based on empirically derived key distances. The generated key assignments are then used to construct the perceptual pitch error criterion which is based on note relatedness ratings obtained from experiments with human listeners.",Perceptual Issues in Music Pattern Recognition: Complexity of Rhythm and Key Finding,Springer,2001,,Computers and the Humanities,35,1,"[{'value': '00104817', 'type': 'print'}]"
1627,http://www.jstor.org/stable/30204839,"{'string_id': '10.2307/30204839', 'id_scheme': 'DOI'}","Previous discussions of musical pattern have underlined difficulties in seeking pattern as a sequence of pitches, or of intervals or of other local and atomic features. This paper describes a manner of representing melodies through a hierarchical structure of elaboration, derived from concepts common in music theory (in particular, the concept of reduction found in the work of Schenker and of Lerdahl & Jackendoff). The fundamental structure is a planar directed acyclic graph, each node of which represents a musical note (not necessarily as it is present in the actual melody) and an elaboration which generates that note on the basis of two parents. These graph structures can be converted to trees, aiding processing and comparison, in two ways. Firstly, any graph can be transformed into a set of binary trees in which each node represents an interval between two notes and an elaboration of that interval. Secondly, in the planar graph, the link of a node to one of its parents often provides no useful information and can be disregarded, resulting in a reduction of the graph tending towards a set of trees. From this arises a new approach to the question of melodic segmentation. Examples of melodic fragments represented in this manner demonstrate how the representation makes explicit similarities between fragments which would not be found by an approach using sequences of features.",Representing Melodic Patterns as Networks of Elaborations,Springer,2001,,Computers and the Humanities,35,1,"[{'value': '00104817', 'type': 'print'}]"
1628,http://www.jstor.org/stable/30204840,"{'string_id': '10.2307/30204840', 'id_scheme': 'DOI'}","Musical patterns that recur in approximate, rather than identical, form within a composition (or body of musical work) are considered to be of considerable importance in music analysis. Here we consider the ""evolutionary chain problem"": this is the problem of computing a chain of all ""motif"" recurrences, each of which is a transformation of (""similar"" to) the original motif, but each of which is progressively further from the original. Here we consider several variants of the evolutionary chain problem and we present efficient algorithms for solving them.",Approximate Musical Evolution,Springer,2001,,Computers and the Humanities,35,1,"[{'value': '00104817', 'type': 'print'}]"
1629,http://www.jstor.org/stable/30204841,"{'string_id': '10.2307/30204841', 'id_scheme': 'DOI'}","Classification in music analysis involves the segmentation of a music piece and the categorisation of the segments depending on similarity-based criteria. In this paper we investigate, based on a formal approach, how variations in the representation of the musical segments and in the categorisation algorithm influence the outcome of the classification. More specifically, we vary the choice of features describing each segment, the way these features are represented, and the categorisation algorithm. At the same time, we keep the other parameters, that is the overall model architecture, the music pieces, and the segmentation, fixed. We show that the choice and representation of the features, but not the specific categorisation algorithm, have a strong impact on the obtained analysis. We introduce a distance function to compare the results of algorithmic and human classification, and we show that an appropriate choice of features can yield results that are very similar to a human classification. These results allow an objective evaluation of different approaches to music classification in a uniform setting.",Investigating the Influence of Representations and Algorithms in Music Classification,Springer,2001,,Computers and the Humanities,35,1,"[{'value': '00104817', 'type': 'print'}]"
1630,http://www.jstor.org/stable/30204845,"{'string_id': '10.2307/30204845', 'id_scheme': 'DOI'}","The Gsearch system allows the selection of sentences by syntactic criteria from text corpora, even when these corpora contain no prior syntactic markup. This is achieved by means of a fast chart parser, which takes as input a grammar and a search expression specified by the user. Gsearch features a modular architecture that can be extended straightforwardly to give access to new corpora. The Gsearch architecture also allows interfacing with external linguistic resources (such as taggers and lexical databases). Gsearch can be used with graphical tools for visualizing the results of a query.","Finding Syntactic Structure in Unparsed Corpora: ""The Gsearch Corpus Query System""",Springer,2001,,Computers and the Humanities,35,2,"[{'value': '00104817', 'type': 'print'}]"
1631,http://www.jstor.org/stable/30204846,"{'string_id': '10.2307/30204846', 'id_scheme': 'DOI'}","This article describes the challenges posed by optical music recognition - a topic in computer science that aims to convert scanned pages of music into an on-line format. First, the problem is described; then a generalised framework for software is presented that emphasises key stages that must be solved: staff line identification, musical object location, musical feature classification, and musical semantics. Next, significant research projects in the area are reviewed, showing how each fits the generalised framework. The article concludes by discussing perhaps the most open question in the field: how to compare the accuracy and success of rival systems, highlighting certain steps that help ease the task.",The Challenge of Optical Music Recognition,Springer,2001,,Computers and the Humanities,35,2,"[{'value': '00104817', 'type': 'print'}]"
1632,http://www.jstor.org/stable/30204847,"{'string_id': '10.2307/30204847', 'id_scheme': 'DOI'}","An appropriate standardized data model is necessary to facilitate electronic publication and analysis of archaeological data on the World Wide Web. A hierarchical ""item-based"" model is proposed which can be readily implemented as an Extensible Markup Language (XML) tagging scheme that can represent any kind of archaeological data and deliver it in a cross-platform, standardized fashion to any Web browser. This tagging scheme and the data model it implements permit seamless integration and joint querying of archaeological datasets derived from many different sources.",Archaeological Data Models and Web Publication Using XML,Springer,2001,,Computers and the Humanities,35,2,"[{'value': '00104817', 'type': 'print'}]"
1633,http://www.jstor.org/stable/30204848,"{'string_id': '10.2307/30204848', 'id_scheme': 'DOI'}","This paper presents two grammars for reading numbers of classical and modern Arabic language. The grammars make use of the structured Arabic counting system to present an accurate and compact grammar that can be easily implemented in different platforms. Automating the process of reading numbers from its numerical representation to its sentential form has many applications. Inquiring about your bank balance over the phone, automatically writing the amount of checks (from numerical form to letter form), and reading for the blind people are some of the fields that automated reading of numbers can be of service. The parsing problem of sentential representation of numbers in the Arabic language is also addressed. A grammar to convert from sentential representation to the numerical representation is also presented. Grammars presented can be used to translate from the sentential Arabic numbers to sentential English numbers, and vice versa, by using the common numerical representation as an intermediate code. Such methodology can be used to aid the automatic translation between the two natural languages. All grammars described in this paper have been implemented on a UNIX system. Examples of different number representations and the output of the implementation of the grammars are given as part of the paper.",Sentential Count Rules for Arabic Language,Springer,2001,,Computers and the Humanities,35,2,"[{'value': '00104817', 'type': 'print'}]"
1634,http://www.jstor.org/stable/30204849,"{'string_id': '10.2307/30204849', 'id_scheme': 'DOI'}","This paper studies the problem of detecting multiple changes at unknown times in the mean level of elision in the trimeter sequences of the Orestes, a play written by the Ancient Greek dramatist Euripides (485-406 B.C.). Change-detection statistics proposed by MacNeill (1978) and Jandhayala and MacNeill (1991) are adopted for this purpose. Analysis of the trimeter sequences yields several points of change. A general explanation for their occurrence appears to be that Euripides varies his use of elision according to the emotional content of his text, i.e., he seems to change the form to support the content and, thus, seems to use elision frequency as a dramatic instrument.","Change-Point Analysis: Elision in Euripides' ""Orestes""",Springer,2001,,Computers and the Humanities,35,2,"[{'value': '00104817', 'type': 'print'}]"
1635,http://www.jstor.org/stable/30204850,"{'string_id': '10.2307/30204850', 'id_scheme': 'DOI'}","The most important approaches to computer-assisted authorship attribution are exclusively based on lexical measures that either represent the vocabulary richness of the author or simply comprise frequencies of occurrence of common words. In this paper we present a fully-automated approach to the identification of the authorship of unrestricted text that excludes any lexical measure. Instead we adapt a set of style markers to the analysis of the text performed by an already existing natural language processing tool using three stylometric levels, i.e., token-level, phrase-level, and analysis-level measures. The latter represent the way in which the text has been analyzed. The presented experiments on a Modern Greek newspaper corpus show that the proposed set of style markers is able to distinguish reliably the authors of a randomly-chosen group and performs better than a lexically-based approach. However, the combination of these two approaches provides the most accurate solution (i.e., 87% accuracy). Moreover, we describe experiments on various sizes of the training data as well as tests dealing with the significance of the proposed set of style markers.",Computer-Based Authorship Attribution without Lexical Measures,Springer,2001,,Computers and the Humanities,35,2,"[{'value': '00104817', 'type': 'print'}]"
1636,http://www.jstor.org/stable/30204851,"{'string_id': '10.2307/30204851', 'id_scheme': 'DOI'}","Information access methods must be improved to overcome the information overload that most professionals face nowadays. Text classification tasks, like Text Categorization, help the users to access to the great amount of text they find in the Internet and their organizations. TC is the classification of documents into a predefined set of categories. Most approaches to automatic TC are based on the utilization of a training collection, which is a set of manually classified documents. Other linguistic resources that are emerging, like lexical databases, can also be used for classification tasks. This article describes an approach to TC based on the integration of a training collection (Reuters-21578) and a lexical database (WORDNET 1.6) as knowledge sources. Lexical databases accumulate information on the lexical items of one or several languages. This information must be filtered in order to make an effective use of it in our model of TC. This filtering process is a Word Sense Disambiguation task. WSD is the identification of the sense of words in context. This task is an intermediate process in many natural language processing tasks like machine translation or multilingual information retrieval. We present the utilization of WSD as an aid for TC. Our approach to WSD is also based on the integration of two linguistic resources: a training collection (SEMCOR and Reuters-21578) and a lexical database (WORDNET 1.6). We have developed a series of experiments that show that: TC and WSD based on the integration of linguistic resources are very effective; and, WSD is necessary to effectively integrate linguistic resources in TC.",Integrating Linguistic Resources in TC through WSD,Springer,2001,,Computers and the Humanities,35,2,"[{'value': '00104817', 'type': 'print'}]"
1637,http://www.jstor.org/stable/30204852,"{'string_id': '10.2307/30204852', 'id_scheme': 'DOI'}","This article compares the word frequencies of the few most common words in Spanish as revealed by a modern corpus of over five thousand words with a corpus of Golden-Age Spanish texts of over a million words, and finds that although de is by far the most common word in contemporary Spanish, in the 16th and 17th Centuries it was considerably less frequent, and in many texts was less frequent than y, or que for which shared very similar frequency figures. It is argued that this significant change in the Spanish language comes about in the 20th Century.",Spanish Word Frequency: A Historical Surprise,Springer,2001,,Computers and the Humanities,35,2,"[{'value': '00104817', 'type': 'print'}]"
1638,http://www.jstor.org/stable/30204856,"{'string_id': '10.2307/30204856', 'id_scheme': 'DOI'}","In this paper, some electronically gathered data are presented and analyzed about the presence of the past in newspaper texts. In ten large text corpora of six different languages, all dates in the form of years between 1930 and 1990 were counted. For six of these corpora this was done for all the years between 1200 and 1993. Depicting these frequencies on the timeline, we find an underlying regularly declining curve, deviations at regular places and culturally determined peaks at irregular points. These three phenomena are analyzed. Mathematically spoken, all the underlying curves have the same form. Whether a newspaper gives much or little attention to the past, the distribution of this attention over time turns out to be inversely proportional to the distance between past and present. It is shown that this distribution is largely independent of the total number of years in a corpus, the culture in which it is published, the language and the date of origin of the corpus. The phenomenon is explained as a kind of forgetting: the larger the distance between past and present, the more difficult it is to connect something of the past to an item in the present day. A more detailed analysis of the data shows a breakpoint in the frequency vs. distance from the publication date of the texts. References to events older than approximately 50 years are the result of a forgetting process that is distinctively different from the forgetting speed of more recent events. Pandel's classification of the dimensions of historical consciousness is used to answer the question how these investigations elucidate the historical consciousness of the cultures in which the newspapers are written and read.",Computing Historical Consciousness. A Quantitative Inquiry into the Presence of the Past in Newspaper Texts,Springer,2001,,Computers and the Humanities,35,3,"[{'value': '00104817', 'type': 'print'}]"
1639,http://www.jstor.org/stable/30204857,"{'string_id': '10.2307/30204857', 'id_scheme': 'DOI'}","Intercorrelations among stylistic and emotional variables and construct validity deduced from relationships to other ratings of U.S. presidents suggest that power language (language that is linguistically simple, emotionally evocative, highly imaged, and rich in references to American values) is an important descriptor of inaugural addresses. Attempts to predict the use of power language in inaugural addresses from variables representing the times (year, media, economic factors) and the man (presidential personality) lead to the conclusion that time-based factors are the best predictors of the use of such language (81% prediction of variance in the criterion) while presidential personality adds at most a small amount of prediction to the model. Changes in power language are discussed as the outcome of a tendency to opt for breadth of communication over depth.",The Times and the Man as Predictors of Emotion and Style in the Inaugural Addresses of U.S. Presidents,Springer,2001,,Computers and the Humanities,35,3,"[{'value': '00104817', 'type': 'print'}]"
1640,http://www.jstor.org/stable/30204858,"{'string_id': '10.2307/30204858', 'id_scheme': 'DOI'}","In this paper, we propose a statistical method to automatically extract collocations from Korean POS-tagged corpus. Since a large portion of language is represented by collocation patterns, the collocational knowledge provides a valuable resource for NLP applications. One difficulty of collocation extraction is that Korean has a partially free word order, which also appears in collocations. In this work, we exploit four statistics, 'frequency', 'randomness', 'convergence', and 'correlation' in order to take into account the flexible word order of Korean collocations. We separate meaningful bigrams using an evaluation function based on the four statistics and extend the bigrams to n-gram collocations using a fuzzy relation. Experiments show that this method works well for Korean collocations.",Automatic Extraction of Collocations from Korean Text,Springer,2001,,Computers and the Humanities,35,3,"[{'value': '00104817', 'type': 'print'}]"
1641,http://www.jstor.org/stable/30204859,"{'string_id': '10.2307/30204859', 'id_scheme': 'DOI'}","This paper is concerned with the investigation of the relevance and suitability of the data mining approach to serial documents. Conceptually the paper is divided into three parts. The first part presents the salient features of data mining and its symbiotic relationship to data warehousing. In the second part of the paper, historical serial documents are introduced, and the Ottoman Tax Registers (Defters) are taken as a case study. Their conformance to the data mining approach is established in terms of structure, analysis and results. A high-level conceptual model for the Defters is also presented. The final part concludes with a brief consideration of the implication of data mining for historical research.",Data Mining and Serial Documents,Springer,2001,,Computers and the Humanities,35,3,"[{'value': '00104817', 'type': 'print'}]"
1642,http://www.jstor.org/stable/30204860,"{'string_id': '10.2307/30204860', 'id_scheme': 'DOI'}","This paper describes how traditional and non-traditional methods were used to identify seventeen previously unknown articles that we believe to be by Stephen Crane, published in the New-York Tribune between 1889 and 1892. The articles, printed without byline in what was at the time New York City's most prestigious newspaper, report on activities in a string of summer resort towns on New Jersey's northern shore. Scholars had previously identified fourteen shore reports as Crane's; these possible attributions more than double that corpus. The seventeen articles confirm how remarkably early Stephen Crane set his distinctive writing style and artistic agenda. In addition, the sheer quantity of the articles from the summer of 1892 reveals how vigorously the twenty-year-old Crane sought to establish himself in the role of professional writer. Finally, our discovery of an article about the New Jersey National Guard's summer encampment reveals another way in which Crane immersed himself in nineteenth-century military culture and help to explain how a young man who had never seen a battle could write so convincingly of war in his soon-to-come masterpiece, The Red Badge of Courage. We argue that the joint interdisciplinary approach employed in this paper should be the way in which attributional research is conducted.","Stephen Crane and the ""New-York Tribune"": A Case Study in Traditional and Non-Traditional Authorship Attribution",Springer,2001,,Computers and the Humanities,35,3,"[{'value': '00104817', 'type': 'print'}]"
1643,http://www.jstor.org/stable/30204861,"{'string_id': '10.2307/30204861', 'id_scheme': 'DOI'}","The aim of this paper is to describe a technique for identifying the sources of several types of syntactic ambiguity in Arabic Sentences with a single parse only. Normally, any sentence with two or more structural representations is said to be syntactically ambiguous. However, Arabic sentences with only one structural representation may be ambiguous. Our technique for identifying Syntactic Ambiguity in Single-Parse Arabic Sentences (SASPAS) analyzes each sentence and verifies the conditions that govern the existence of certain types of syntactic ambiguities in Arabic sentences. SASPAS is integrated with the syntactic parser, which is based on Definite Clause Grammar (DCG) formalism. The system accepts Arabic sentences in their original script.",Identifying Syntactic Ambiguities in Single-Parse Arabic Sentence,Springer,2001,,Computers and the Humanities,35,3,"[{'value': '00104817', 'type': 'print'}]"
1644,http://www.jstor.org/stable/30204868,"{'string_id': '10.2307/30204868', 'id_scheme': 'DOI'}","Internet search engines allow access to online information from all over the world. However, there is currently a general assumption that users are fluent in the languages of all documents that they might search for. This has for historical reasons usually been a choice between English and the locally supported language. Given the rapidly growing size of the Internet, it is likely that future users will need to access information in languages in which they are not fluent or have no knowledge of at all. This paper shows how information retrieval and machine translation can be combined in a cross-language information access framework to help overcome the language barrier. We present encouraging preliminary experimental results using English queries to retrieve documents from the standard Japanese language BMIR-J2 retrieval test collection. We outline the scope and purpose of cross-language information access and provide an example application to suggest that technology already exists to provide effective and potentially useful applications.",A Framework for Cross-Language Information Access: Application to English and Japanese,Springer,2001,,Computers and the Humanities,35,4,"[{'value': '00104817', 'type': 'print'}]"
1645,http://www.jstor.org/stable/30204869,"{'string_id': '10.2307/30204869', 'id_scheme': 'DOI'}","Cross-language information retrieval (CLIR), where queries and documents are in different languages, has of late become one of the major topics within the information retrieval community. This paper proposes a Japanese/English CLIR system, where we combine a query translation and retrieval modules. We currently target the retrieval of technical documents, and therefore the performance of our system is highly dependent on the quality of the translation of technical terms. However, the technical term translation is still problematic in that technical terms are often compound words, and thus new terms are progressively created by combining existing base words. In addition, Japanese often represents loanwords based on its special phonogram. Consequently, existing dictionaries find it difficult to achieve sufficient coverage. To counter the first problem, we produce a Japanese/English dictionary for base words, and translate compound words on a word-by-word basis. We also use a probabilistic method to resolve translation ambiguity. For the second problem, we use a transliteration method, which corresponds words unlisted in the base word dictionary to their phonetic equivalents in the target language. We evaluate our system using a test collection for CLIR, and show that both the compound word translation and transliteration methods improve the system performance.",Japanese/English Cross-Language Information Retrieval: Exploration of Query Translation and Transliteration,Springer,2001,,Computers and the Humanities,35,4,"[{'value': '00104817', 'type': 'print'}]"
1646,http://www.jstor.org/stable/30204870,"{'string_id': '10.2307/30204870', 'id_scheme': 'DOI'}","It is important to give useful clues for selecting desired content from a number of retrieval results obtained (usually) from a vague search request. Compared with monolingual retrieval, such a support framework is inevitable and much more significant for filtering given translingual retrieval results. This paper describes an attempt to provide appropriate translation of major keywords in each document in a cross-language information retrieval (CLIR) result, as a browsing support for users. Our idea of determining appropriate translation of major keywords is based on word cooccurrence distribution in the translation target language, considering the actual situation of WWW content where it is difficult to obtain aligned parallel (multilingual) corpora. The proposed method provides higher quality of keyword translation to yield a more effective support in identifying the target documents in the retrieval result. We report the advantage of this browsing support technique through evaluation experiments including comparison with conditions of referring to a translated document summary, and discuss related issues to be examined towards more effective cross-language information extraction.",A Method for Supporting Document Selection in Cross-Language Information Retrieval and Its Evaluation,Springer,2001,,Computers and the Humanities,35,4,"[{'value': '00104817', 'type': 'print'}]"
1647,http://www.jstor.org/stable/30204890,"{'string_id': '10.2307/30204890', 'id_scheme': 'DOI'}","This article examines the usefulness of vocabulary richness for authorship attribution and tests the assumption that appropriate measures of vocabulary richness can capture an author's distinctive style or identity. After briefly discussing perceived and actual vocabulary richness, I show that doubling and combining texts affects some measures in computationally predictable but conceptually surprising ways. I discuss some theoretical and empirical problems with some measures and develop simple methods to test how well vocabulary richness distinguishes texts by different authors. These methods show that vocabulary richness is ineffective for large groups of texts because of the extreme variability within and among them. I conclude that vocabulary richness is of marginal value in stylistic and authorship studies because the basic assumption that it constitutes a wordprint for authors is false.",Another Perspective on Vocabulary Richness,Springer,2003,,Computers and the Humanities,37,2,"[{'value': '00104817', 'type': 'print'}]"
1648,http://www.jstor.org/stable/30204891,"{'string_id': '10.2307/30204891', 'id_scheme': 'DOI'}","Responses in personal interviews about education and career with 415 Swedish men and women (age 34) forms the basis of a speech corpus with 1.8 million words. The vocabulary is described by means of two sets of variables. One is based on the number of tokens and types, word length and sectioning of the running text. The other set divides the corpus into grammatical categories. Both sets of variables are related to a number of background variables such as gender, socioeconomic background, education, and indicators of verbal proficiency at age 13 and 32. This possibility to study the relationship between vocabulary and a broad set of respondent characteristics is a unique feature of this corpus.",Vocabulary in Interviews as Related to Respondent Characteristics,Springer,2003,,Computers and the Humanities,37,2,"[{'value': '00104817', 'type': 'print'}]"
1649,http://www.jstor.org/stable/30204893,"{'string_id': '10.2307/30204893', 'id_scheme': 'DOI'}","GIS methodology was used for the purpose of locating the disputed site of a historically significant battle, which took place in 1854 when miners on an Australian gold field staged an armed uprising against government forces. The route of the first survey of the area (1854) and the earliest known contour map (1856-1857) were overlaid on a modem street grid. Other features such as the vantage points of illustrators and the authors of eyewitness accounts were also incorporated. The resulting composite map was used as the key reference framework for comparing and critically evaluating a large body of primary and secondary written accounts, and for reaching a conclusion concerning the site.",Locating the Eureka Stockade: Use of a Geographical Information System (GIS) in a Historiographical Research Context,Springer,2003,,Computers and the Humanities,37,2,"[{'value': '00104817', 'type': 'print'}]"
1650,http://www.jstor.org/stable/30204894,"{'string_id': '10.2307/30204894', 'id_scheme': 'DOI'}","In their papers, Kalashnikov et al. (1986), Rachev et al. (1989) and Fomenko et al. (1990) introduced the so-called ""volume function"" describing the chronological distribution of information in historical texts. Here we give another approach to constructing similar functions.",Chronological Distribution of Information in Historical Texts,Springer,2003,,Computers and the Humanities,37,2,"[{'value': '00104817', 'type': 'print'}]"
1651,http://www.jstor.org/stable/30204900,"{'string_id': '10.2307/30204900', 'id_scheme': 'DOI'}","Dialectology is the study of dialects, and dialectometry is the measurement of dialect differences, i.e. linguistic differences whose distribution is determined primarily by geography. The earliest works in dialectology showed that language variation is complex both geographically and linguistically and cannot be reduced to simple characterizations. There has thus always been a perceived need for techniques which can deal with large amounts of data in a controlled means, i.e. computational techniques. This special issue of Computers and the Humanities presents a range of recent work on this topic.",Introducing Computational Techniques in Dialectometry,Springer,2003,,Computers and the Humanities,37,3,"[{'value': '00104817', 'type': 'print'}]"
1652,http://www.jstor.org/stable/30204901,"{'string_id': '10.2307/30204901', 'id_scheme': 'DOI'}","Measuring dialect distances can be based on the comparison of words, and the comparison words should be based on the comparison of sounds. In this research we used an adjusted version of an articulation-based system, developed by Almeida and Braun (1986) for finding sound distances, using the IPA system. For comparison of two pronunciations of a word corresponding with two different varieties, we used the Levenshtein algorithm, which finds the easiest way in which one word can be changed into the other by inserting, deleting or substituting sounds. As operations weights of these three operations we used distances as found with the Almeida & Braun system. The dialect distance is now equal to the average of a range of word distances. We applied the technique to 360 Dutch dialects. The transcriptions of 125 words for each dialect are taken from the Reeks Nederlandse Dialectatlassen (Blancquaert and Peé, 1925-1982). We get a division with clear similarities to traditional dialect maps when classifying dialects. Using logarithmic sound distances improves results compared to results based on constant sound distances.",The Use of the Almeida-Braun System in the Measurement of Dutch Dialect Distances,Springer,2003,,Computers and the Humanities,37,3,"[{'value': '00104817', 'type': 'print'}]"
1653,http://www.jstor.org/stable/30204902,"{'string_id': '10.2307/30204902', 'id_scheme': 'DOI'}","The computation of the optimal phonetic alignment and the phonetic similarity between words is an important step in many applications in computational phonology, including dialectometry. After discussing several related algorithms, I present a novel approach to the problem that employs a scoring scheme for computing phonetic similarity between phonetic segments on the basis of multivalued articulatory phonetic features. The scheme incorporates the key concept of feature salience, which is necessary to properly balance the importance of various features. The new algorithm combines several techniques developed for sequence comparison: an extended set of edit operations, local and semiglobal modes of alignment, and the capability of retrieving a set of near-optimal alignments. On a set of 82 cognate pairs, it performs better than comparable algorithms reported in the literature.",Phonetic Alignment and Similarity,Springer,2003,,Computers and the Humanities,37,3,"[{'value': '00104817', 'type': 'print'}]"
1654,http://www.jstor.org/stable/30204903,"{'string_id': '10.2307/30204903', 'id_scheme': 'DOI'}","Gooskens (2003) described an experiment which determined linguistic distances between 15 Norwegian dialects as perceived by Norwegian listeners. The results are compared to Levenshtein distances, calculated on the basis of transcriptions (of the words) of the same recordings as used in the perception experiment. The Levenshtein distance is equal to the sum of the weights of the insertions, deletions and substitutions needed to change one pronunciation into another. The success of the method depends on the reliability of the transcriber. The aim of this paper is to find an acoustic distance measure between dialects which approximates perceptual distance measure. We use and compare different representations of the acoustic signal: Barkfilter spectrograms, cochleagrams and formant tracks. We now apply the Levenshtein algorithm to spectra or formant value bundles instead of transcription segments. From these acoustic representations we got the best results using the formant track representation. However the transcription-based Levenshtein distances correlate still more closely. In the acoustic signal the speaker-dependent influence is kept to some extent, while a transcriber abstracts from voice quality. Using more samples per dialect word (instead of only one as in our research) should improve the accuracy of the measurements.",Norwegian Dialects Examined Perceptually and Acoustically,Springer,2003,,Computers and the Humanities,37,3,"[{'value': '00104817', 'type': 'print'}]"
1655,http://www.jstor.org/stable/30204904,"{'string_id': '10.2307/30204904', 'id_scheme': 'DOI'}","In this text we present ""profile-based linguistic uniformity"", a method designed to compare language varieties on the basis of a wide range of potentially heterogeneous linguistic variables. In many respects a parallel can be drawn with current methods in dialectometry (for an overview, see, Nerbonne and Heeringa, 2001; Heeringa, Nerbonne and Kleiweg, 2002): in both cases dissimilarities between varieties on the basis of individual variables are summarized in global dissimilarities, and a series of language varieties are subsequently clustered or charted using multivariate techniques such as cluster analysis or multidimensional scaling. This global similarity between the methods makes it possible to compare them and to investigate the implications of notable differences. In this text we specifically focus on, and defend one characteristic of our methodology, its profile-based nature.",Profile-Based Linguistic Uniformity as a Generic Method for Comparing Language Varieties,Springer,2003,,Computers and the Humanities,37,3,"[{'value': '00104817', 'type': 'print'}]"
1656,http://www.jstor.org/stable/30204905,"{'string_id': '10.2307/30204905', 'id_scheme': 'DOI'}","The Linguistic Atlas of the Middle and South Atlantic States (LAMSAS) is admirably accessible for reanalysis. The present paper applies a lexical distance measure to assess the lexical relatedness of LAMSAS's sites, a popular focus of investigation in the past (Kurath, 1949; Carver, 1989; McDavid, 1994). Several conclusions are noteworthy: First, and least controversially, we note that LAMSAS is dialectometrically challenging at least due to the range of field workers and questionnaires employed. Second, on the issue of which areas ought to be recognized, we note that our investigations tend to support a three-way North/South/Midlands division rather than a two-way North/South division, i.e. they tend to support Kurath and McDavid rather than Carver, but this tendency is not conclusive. Third, we extend dialectometric technique in suggesting means of dealing with alternate forms and multiple responses.",Lexical Distance in LAMSAS,Springer,2003,,Computers and the Humanities,37,3,"[{'value': '00104817', 'type': 'print'}]"
1657,http://www.jstor.org/stable/30204906,"{'string_id': '10.2307/30204906', 'id_scheme': 'DOI'}","The aim of this study is to show how cluster analysis can shed light on very complex variation in a transitional dialect zone in eastern Finland. In the course of history this area has been on the border between Sweden and Russia and the population has clearly been of two kinds: the Savo people and the Karelians. It is a well-known fact that there is variation among these dialects, but the spread and extent of the variation has not been demonstrated previously. The idiolects of the area were studied in the light of ten phonological and morphological features. The material consisted of recordings of 198 idiolects, totalling around 195 hours and representing 19 parishes. The variation was analysed using hierarchical cluster analysis. While the analysis showed the extent of the variation between idiolects and parishes, it also demonstrated how the effects of the old parishes, borders and settlements are still visible in the dialects. On the parish level, the data formed clear clusters that correspond with the main dialects in the area and its surroundings. On the idiolect level, however, the speakers from the surrounding areas formed fairly homogenous clusters but the idiolects from the Savonlinna area were spread across almost all clusters.",Neighbours or Enemies? Competing Variants Causing Differences in Transitional Dialects,Springer,2003,,Computers and the Humanities,37,3,"[{'value': '00104817', 'type': 'print'}]"
1658,http://www.jstor.org/stable/30204912,"{'string_id': '10.2307/30204912', 'id_scheme': 'DOI'}","As language data and associated technologies proliferate and as the language resources community expands, it is becoming increasingly difficult to locate and reuse existing resources. Are there any lexical resources for such-and-such a language? What tool works with transcripts in this particular format? What is a good format to use for linguistic data of this type? Questions like these dominate many mailing lists, since web search engines are an unreliable way to find language resources. This paper reports on a new digital infrastructure for discovering language resources being developed by the Open Language Archives Community (OLAC). At the core of OLAC is its metadata format, which is designed to facilitate description and discovery of all kinds of language resources, including data, tools, or advice. The paper describes OLAC metadata, its relationship to Dublin Core metadata, and its dissemination using the metadata harvesting protocol of the Open Archives Initiative.",Extending Dublin Core Metadata to Support the Description and Discovery of Language Resources,,2003,,Computers and the Humanities,37,4,"[{'value': '00104817', 'type': 'print'}]"
1659,http://www.jstor.org/stable/30204913,"{'string_id': '10.2307/30204913', 'id_scheme': 'DOI'}","C-rater is an automated scoring engine that has been developed to score responses to content-based short answer questions. It is not simply a string matching program - instead it uses predicate argument structure, pronominal reference, morphological analysis and synonyms to assign full or partial credit to a short answer question. C-rater has been used in two studies: National Assessment for Educational Progress (NAEP) and a statewide assessment in Indiana. In both studies, c-rater agreed with human graders about 84% of the time.",C-rater: Automated Scoring of Short-Answer Questions,Springer,2003,,Computers and the Humanities,37,4,"[{'value': '00104817', 'type': 'print'}]"
1660,http://www.jstor.org/stable/30204914,"{'string_id': '10.2307/30204914', 'id_scheme': 'DOI'}","This paper considers the question of authorship attribution techniques when faced with a pastiche. We ask whether the techniques can distinguish the real thing from the fake, or can the author fool the computer? If the latter, is this because the pastiche is good, or because the technique is faulty? Using a number of mainly vocabulary-based techniques, Gilbert Adair's pastiche of Lewis Carroll, Alice Through the Needle's Eye, is compared with the original 'Alice' books. Standard measures of lexical richness, Yule's K and Orlov's Z both distinguish Adair from Carroll, though Z also distinguishes the two originals. A principal component analysis based on word frequencies finds that the main differences are not due to authorship. A discriminant analysis based on word usage and lexical richness successfully distinguishes the pastiche from the originals. Weighted cusum tests were also unable to distinguish the two authors in a majority of cases. As a cross-validation, we made similar comparisons with control texts: another children's story from the same era, and other work by Carroll and Adair. The implications of these findings are discussed.",Authorship Attribution and Pastiche,Springer,2003,,Computers and the Humanities,37,4,"[{'value': '00104817', 'type': 'print'}]"
1661,http://www.jstor.org/stable/30204915,"{'string_id': '10.2307/30204915', 'id_scheme': 'DOI'}","A common tool for improving the performance quality of natural language processing systems is the use of contextual information for disambiguation. Here I describe the use of a finite state machine (FSM) to disambiguate speech acts in a machine translation system. The FSM has two layers that model, respectively, the global and local structures found in naturally-occurring conversations. The FSM has been modeled on a corpus of task-oriented dialogues in a travel planning situation. In the dialogues, one of the interactants is a travel agent or hotel clerk, and the other a client requesting information or services. A discourse processor based on the FSM was implemented in order to process contextual information in a machine translation system. Evaluation results show that the discourse processor is able to disambiguate and improve the quality of the dialogue translation. Other applications include human-computer interaction and computer-assisted language learning.",Modeling Task-Oriented Dialogue,Springer,2003,,Computers and the Humanities,37,4,"[{'value': '00104817', 'type': 'print'}]"
1662,http://www.jstor.org/stable/30204916,"{'string_id': '10.2307/30204916', 'id_scheme': 'DOI'}","This study describes and evaluates two essay-based discourse analysis systems that identify thesis and conclusion statements from student essays written on six different essay topics. Essays used to train and evaluate the systems were annotated by two human judges, according to a discourse annotation protocol. Using a machine learning approach, a number of discourse-related features were automatically extracted from a set of annotated training data. Using these features, two discourse analysis models were built using C5.0 with boosting: a topic-dependent and a topic-independent model. Both systems outperformed a positional algorithm. While the topic-dependent system showed somewhat higher performance, the topic-independent system showed similar results, indicating that a system can generalize to unseen data - that is, essay responses on topics that the system has not seen in training.",A Machine Learning Approach for Identification of Thesis and Conclusion Statements in Student Essays,Springer,2003,,Computers and the Humanities,37,4,"[{'value': '00104817', 'type': 'print'}]"
1663,http://www.jstor.org/stable/30204917,"{'string_id': '10.2307/30204917', 'id_scheme': 'DOI'}","This paper describes an encoding for representing quantitative metrical analyses in TEI SGML or XML documents, using only characters from the standard keyboard set, and a system for converting this encoding to other forms for display.",Talking about Meter in SGML,Springer,2003,,Computers and the Humanities,37,4,"[{'value': '00104817', 'type': 'print'}]"
1664,http://www.jstor.org/stable/30204922,"{'string_id': '10.2307/30204922', 'id_scheme': 'DOI'}","Many manuscripts of the Greek New Testament are influenced by multiple pathways of textual flow. This makes it difficult to reconstruct a stemma using traditional methods. We describe a novel application of the reduced median algorithm (developed in evolutionary biology) to reconstructing a stemma for selected Greek manuscripts of the Letter of James. This stemma is a network in which contamination is explicitly represented. It is consistent with the ideas that most variants arose early in the history of the Greek New Testament, that early manuscripts were often influenced by both oral and written traditions, and that later copies introduced fewer variants. Since similar problems of contamination occur in many text traditions, we expect methods of this kind to be widely useful.",Representing Multiple Pathways of Textual Flow in the Greek Manuscripts of the Letter of James Using Reduced Median Networks,Springer,2004,,Computers and the Humanities,38,1,"[{'value': '00104817', 'type': 'print'}]"
1665,http://www.jstor.org/stable/30204923,"{'string_id': '10.2307/30204923', 'id_scheme': 'DOI'}","In author attribution studies function words or lexical measures are often used to differentiate the authors' textual fingerprints. These studies can be thought of as quantifying the texts, representing the text with measured variables that stand for specific textual features. The resulting quantifications, while proven useful for statistically differentiating among the texts, bear no resemblance to the understanding a human reader-even an astute one-would develop while reading the texts. In this paper we present an attribution study that, instead, characterizes the texts according to the representational language choices of the authors, similar to a way we believe close human readers come to know a text and distinguish its rhetorical purpose. From our automated quantification of The Federalist papers, it is clear why human readers find it impossible to distinguish the authorship of the disputed papers. Our findings suggest that changes occur in the processes of rhetorical invention when undertaken in collaborative situations. This points to a need to re-evaluate the premise of autonomous authorship that has informed attribution studies of The Federalist case.",Detecting Collaborations in Text: Comparing the Authors' Rhetorical Language Choices in the Federalist Papers,Springer,2004,,Computers and the Humanities,38,1,"[{'value': '00104817', 'type': 'print'}]"
1666,http://www.jstor.org/stable/30204924,"{'string_id': '10.2307/30204924', 'id_scheme': 'DOI'}","Current term recognition algorithms have centred mostly on the notion of term based on the assumption that terms are monoreferential and as such independent of context. The characteristics and behaviour of terms in real texts are however far removed from this ideal because factors such as text type or communicative situation greatly influence the linguistic realisation of a concept. Context, therefore, is important for the correct identification of terms (Dubuc and Lauriston, 1997). Based on this assumption, we have shifted our emphasis from terms towards surrounding linguistic context, namely verbs, as verbs are considered the central elements in the sentence. More specifically, we have set out to examine whether verbs and verbal syntax in particular, could help us towards the task of automatic term recognition. Our findings suggest that term occurrence varies significantly in different argument structures and different syntactic positions. Additionally, deviant grammatical structures have proved rich environments for terms. The analysis was carried out in three different specialised subcorpora in order to explore how the effectiveness of verbal syntax as a potential indicator of term occurrence can be constrained by factors such as subject matter and text type.",An Analysis of Verb Subcategorization Frames in Three Special Language Corpora with a View towards Automatic Term Recognition,Springer,2004,,Computers and the Humanities,38,1,"[{'value': '00104817', 'type': 'print'}]"
1667,http://www.jstor.org/stable/30204925,"{'string_id': '10.2307/30204925', 'id_scheme': 'DOI'}","This study investigates the writing style change of two Turkish authors, Çetin Altan and Yaşar Kemal, in their old and new works using respectively their newspaper columns and novels. The style markers are the frequencies of word lengths in both text and vocabulary, and the rate of usage of most frequent words. For both authors, t-tests and logistic regressions show that the length of the words in new works is significantly longer than that of the old. The principal component analyses graphically illustrate the separation between old and new texts. The works are correctly categorized as old or new with 75 to 100% accuracy and 92% average accuracy using discriminant analysisbased cross validation. The results imply higher time gap may have positive impact in separation and categorization. For Altan a regression analysis demonstrates a decrease in average word length as the age of his column increases. One interesting observation is that for one word each author has similar preference changes over time.",Change of Writing Style with Time,Springer,2004,,Computers and the Humanities,38,1,"[{'value': '00104817', 'type': 'print'}]"
1668,http://www.jstor.org/stable/30204926,"{'string_id': '10.2307/30204926', 'id_scheme': 'DOI'}","University students increasingly use hypertext to write their assignments. To employ hypertext effectively, more information about the hypertext authoring process is needed. There are features of hypertext which are not reflected in traditional theories of composition, especially the possibility to structure information visually. Our study indicates that graphical overview maps which can be edited are a rather attractive feature of hypertext authoring systems. Nevertheless, not all students profit from such features. Students employ different writing styles when they create hypertext documents. The majority of students experiments with hypertext's new features but others are still influenced by the model of the book.",Hypertext Writing Profiles and Visualisation,Springer,2004,,Computers and the Humanities,38,1,"[{'value': '00104817', 'type': 'print'}]"
1669,http://www.jstor.org/stable/30204930,"{'string_id': '10.2307/30204930', 'id_scheme': 'DOI'}","The paper explores the notions of text ownership and its partial inverse, plagiarism, and asks how close or different they are from a procedural point of view that might seek to establish either of these properties. The emphasis is on procedures rather than on the conventional subject division of authorship studies, plagiarism detection etc. We use, as a particular example, our research on the notion of computational detection of text rewriting, in the benign sense of a standard journalist's adaptation of the Press Association newsfeed. The conclusion is that, whatever may be the case in copyright law, procedural detection and establishment of the ownership is a complex and vexed matter. Behind the paper is an unspoken appeal to return to an earlier historical phase, one where texts were normally rewritten and rewritten again and the ownership of text by an individual was a less clear matter than in historically recent times.",On the Ownership of Text,Springer,2004,,Computers and the Humanities,38,2,"[{'value': '00104817', 'type': 'print'}]"
1670,http://www.jstor.org/stable/30204931,"{'string_id': '10.2307/30204931', 'id_scheme': 'DOI'}","The purpose of our research is to consider how the paradigms of EuroWordNet and SIMPLE linguistic projects on the one hand and the OIL methodology on the other hand may affect each other. OIL (Ontology Inference Layer) aims at implementing the ""semantic"" Web idea and is based on the notion of ontology, which is also employed in EuroWordNet and SIMPLE. In both latter projects the meanings of words are partially described by means of the finite sets of relations to other meanings of words, whereas in OIL the user is free to define the arbitrary relations of this kind. The relations considered in EuroWordNet and SIMPLE were defined on the basis of a careful observation of the large linguistic area, and they aim at reflecting the meaning as precisely as possible, therefore it seems useful to merge them with OIL. Moreover, the valuable feature of OIL is its formal language with precisely defined semantics. All things considered, we suggest how certain EuroWordNet and SIMPLE definitions may be expressed in OIL.",Semantic Roles as Slots in OIL Ontologies,Springer,2004,,Computers and the Humanities,38,2,"[{'value': '00104817', 'type': 'print'}]"
1671,http://www.jstor.org/stable/30204932,"{'string_id': '10.2307/30204932', 'id_scheme': 'DOI'}","This essay traces a distinction between computer-mediated writing environments that are tools for correcting student prose and those that are media for communication. This distinction has its roots in the influence of behavioral science on teaching machines and computer-aided writing instruction during the 1960s and 1970s. By looking at the development of the time-shared, interactive, computer-controlled, information television (TICCIT) and early human-computer interaction (HCI) research, this essay demonstrates that hardware and software systems had the potential to work as both tools and media. The influence of this double logic is not only historical but also has implications for post-secondary writing instruction in the age of Microsoft Word, ETS's e-rater, and the ""reading/assessment"" software tools being developed by Knowledge Analysis Technologies (KAT). This essay challenges composition researchers and computational linguists to develop pedagogies and software systems that acknowledge writing environments as situated within the logic of both tools for correction and media for communication.",The Development of Early Computer-Assisted Writing Instruction (1960-1978): The Double Logic of Media and Tools,Springer,2004,,Computers and the Humanities,38,2,"[{'value': '00104817', 'type': 'print'}]"
1672,http://www.jstor.org/stable/30204933,"{'string_id': '10.2307/30204933', 'id_scheme': 'DOI'}","The paper describes our recent developments in automatic extraction of translation equivalents from parallel corpora. We describe three increasingly complex algorithms: a simple baseline iterative method, and two non-iterative more elaborated versions. While the baseline algorithm is mainly described for illustrative purposes, the non-iterative algorithms outline the use of different working hypotheses which may be motivated by different kinds of applications and to some extent by the languages concerned. The first two algorithms rely on cross-lingual POS preservation, while with the third one POS invariance is not an extraction condition. The evaluation of the algorithms was conducted on three different corpora and several pairs of languages.",Extracting Multilingual Lexicons from Parallel Corpora,Springer,2004,,Computers and the Humanities,38,2,"[{'value': '00104817', 'type': 'print'}]"
1673,http://www.jstor.org/stable/30204934,"{'string_id': '10.2307/30204934', 'id_scheme': 'DOI'}","This paper explores theoretical and practical aspects of intertextuality, in relation to the highly interpretative tag within the SGML tagset developed by the Orlando Project for its history of women's writing in the British Isles. Arguing that the concept of intertextuality is both crucial to and poses particular challenges to the creation of an encoding scheme for literary historical text, it outlines the ways in which the project's tags address broader issues of intertextuality. The paper then describes the specific tag in detail, and argues on the basis of provisional results drawn from the Orlando Project's textbase that despite the impossibility of tracking intertextuality exhaustively or devising a tagset that completely disambiguates the concept, this tag provides useful pathways through the textbase and valuable departure points for further inquiry. Finally, the paper argues that the challenges to notions of rigour posed by the concept of intertextuality can help us fruitfully to examine some of the suppositions (gendered and other) that we bring to electronic text markup.",Intertextual Encoding in the Writing of Women's Literary History,Springer,2004,,Computers and the Humanities,38,2,"[{'value': '00104817', 'type': 'print'}]"
1674,http://www.jstor.org/stable/30204935,"{'string_id': '10.2307/30204935', 'id_scheme': 'DOI'}","Idiolects are person-dependent similarities in language use. They imply that texts by one author show more similarities in language use than texts between authors. Sociolects, on the other hand, are group-dependent similarities in language use. They imply that texts by a group of authors, for instance in terms of gender or time period, share more similarities within a group than between groups. Although idiolects and sociolects are commonly used terms in the humanities, they have not been investigated a great deal from corpus and computational linguistic points of view. To test several idiolect and sociolect hypotheses a factorial combination was used of time period (Modernism, Realism), gender of author (male, female) and author (Eliot, Dickens, Woolf, Joyce) totaling 16 corresponding literary texts. In a series of corpus linguistic studies using Boolean and vector models, no conclusive evidence was found for the selected idiolect and sociolect hypotheses. In final analyses testing the semantics within each literary text, this lack of evidence was explained by the low homogeneity within a literary text.",Semantic Variation in Idiolect and Sociolect: Corpus Linguistic Evidence from Literary Texts,Springer,2004,,Computers and the Humanities,38,2,"[{'value': '00104817', 'type': 'print'}]"
1675,http://www.jstor.org/stable/30204939,"{'string_id': '10.2307/30204939', 'id_scheme': 'DOI'}","This paper reports on a method for exploiting a bitext as the primary linguistic information source for the design of a generation environment for specialized bilingual documentation. The paper discusses such issues as Text Encoding Initiative (TEI), proposals for specialized corpus tagging, text segmentation and alignment of translation units and their allocation into translation memories, Document Type Definition (DTD), abstraction from tagged texts, and DTD deployment for bilingual text generation. The parallel corpus used for experimentation has two main features: - It contains bilingual documents from a dedicated domain of legal and administrative publications rich in specialized jargon. - It involves two languages, Spanish and Basque, which are typologically very distinct (both lexically and morpho-syntactically). Starting from an annotated bitext we show how Standard Generalized Markup Language (SGML) elements can be recycled to produce complementary language resources. Several translation memory databases are produced. Furthermore, DTDs for source and target documents are derived and put into correspondence. This paper discusses how these resources are automatically generated and applied to an interactive bilingual authoring system.",Bitext Generation through Rich Markup,Springer,2004,,Computers and the Humanities,38,3,"[{'value': '00104817', 'type': 'print'}]"
1676,http://www.jstor.org/stable/30204940,"{'string_id': '10.2307/30204940', 'id_scheme': 'DOI'}","To reconstruct a stemma or do any other kind of statistical analysis of a text tradition, one needs accurate data on the variants occurring at each location in each witness. These data are usually obtained from computer collation programs. Existing programs either collate every witness against a base text or divide all texts up into segments as long as the longest variant phrase at each point. These methods do not give ideal data for stemma reconstruction. We describe a better collation algorithm (progressive multiple alignment) that collates all witnesses word by word without a base text, adding groups of witnesses one at a time, starting with the most closely related pair.",Article: Collating Texts Using Progressive Multiple Alignment,Springer,2004,,Computers and the Humanities,38,3,"[{'value': '00104817', 'type': 'print'}]"
1677,http://www.jstor.org/stable/30204941,"{'string_id': '10.2307/30204941', 'id_scheme': 'DOI'}","Bach's cantatas are particularly rich in text imagery, and they typically employ chromatic melodies to accentuate the more piquant literary images, especially in recitatives. Heretofore theories about the intentionality of Bach's compositional choices in this regard have necessarily remained conjectural. In the following study, an objective measurement of pitch diversity in the vocal lines of Bach's church cantata recitatives in relation to literary themes was made possible with specially designed computer software allowing pertinent information to be entered efficiently into a relational database. Because the software tracked not only the 90,000 pitches constituting the vocal lines of these movements but also other attributes (e.g., overall length, presence or absence of accompaniment, opening and closing keys, chronological position, among others), interrelationships among the various attributes could be examined. Findings demonstrate clear correlation between pitch diversity and the degree of affective tension implied by particular textual subjects. While the findings do not prove exclusive causation (other factors such as tonal and structural considerations, social occasion, and evolution of style can also play a role), they do link the two elements, especially in light of Bach's method of composition as documented by Robert Marshall. This study is important for its systematic and comprehensive approach, its findings giving definition and clarity to commonly held generalizations about the relationships between melodic chromaticism (of which pitch diversity is an important aspect and indicator) and textual content. Furthermore, the software holds promise for additional studies of Bach's pitch materials and for studies in other stylistic contexts.",Uncovering Text-Music Connections with a Relational Database: Towards an Objective Measurement of Melodic Pitch Diversity in Relation to Literary Themes in Bach's Church Cantata Recitatives,Springer,2004,,Computers and the Humanities,38,3,"[{'value': '00104817', 'type': 'print'}]"
1678,http://www.jstor.org/stable/30204942,"{'string_id': '10.2307/30204942', 'id_scheme': 'DOI'}","Many teachers adopt networked collaborative learning strategies even though these approaches systematically increase the time needed to deal with a given subject. ""But who's making them do it?"". Probably there has to be a return on investment, in terms of time and obviously in terms of educational results, which justifies that commitment. After surveying the particular features of two experimental projects based on networked collaborative learning, the paper will then offer a series of thoughts triggered by observation of the results and the dynamics generated by this specific approach. The purpose of these thoughts is to identify some key factors that make it possible to measure the real added value produced by network collaboration in terms of the acquisition of skills, knowledge, methods and attitudes that go beyond the ""mere"" learning of contents (however fundamental this may be). And it is precisely on the basis of these considerations that teachers usually answer the above question, explaining ""who (or what) made them do it!"".",Networked Collaborative Learning in the Study of Modern History and Literature,Springer,2004,,Computers and the Humanities,38,3,"[{'value': '00104817', 'type': 'print'}]"
1679,http://www.jstor.org/stable/30204943,"{'string_id': '10.2307/30204943', 'id_scheme': 'DOI'}","A few ideas from film theory, most notably Eisenstein's concept of montage, can improve students' understanding of hypertexts and lessen their resistance to open-ended, nonlinear narratives. These structural characteristics, so frustrating to many new readers of hypertext, can also be found in popular and experimental films. In particular, Godfrey Reggio's (1983) documentary Koyaanisqatsi provides a good starting point for merging hypertext and film theory. Koyaanisqatsi not only broke new ground for documentary film; its structure also resembles Landow's model for an axial hypertext. At the same time, techniques pioneered by Landow, Joyce, Guyer, and others involved in creating and critiquing hypertext can be used to examine film. Having students look closely at Koyaanisqatsi's composition allows them to become amateur cinematographers, who now possess software for breaking a film down and examining its composition, montage, transitions, subliminal messages, and motifs - a process that may then be applied to hypertext.","Film as ""Explicador"" for Hypertext",Springer,2004,,Computers and the Humanities,38,3,"[{'value': '00104817', 'type': 'print'}]"
1680,http://www.jstor.org/stable/30204951,"{'string_id': '10.2307/30204951', 'id_scheme': 'DOI'}","This paper discusses some pitfalls in corpus research and suggests solutions on the basis of examples and computer simulations. We first address reliability problems in language transcriptions, agreement between transcribers, and how disagreements can be dealt with. We then show that the frequencies of occurrence obtained from a corpus cannot always be analyzed with the traditional χ² test, as corpus data are often not sequentially independent and unit independent. Next, we stress the relevance of the power of statistical tests, and the sizes of statistically significant effects. Finally, we point out that a t-test based on log odds often provides a better alternative to a χ² analysis based on frequency counts.",Pitfalls in Corpus Research,Springer,2004,,Computers and the Humanities,38,4,"[{'value': '00104817', 'type': 'print'}]"
1681,http://www.jstor.org/stable/30204952,"{'string_id': '10.2307/30204952', 'id_scheme': 'DOI'}","Recent developments in computational terminology call for the design of multiple and complementary tools for the acquisition, the structuring and the exploitation of terminological data. This paper proposes to bridge the gap between term acquisition and thesaurus construction by offering a framework for automatic structuring of multi-word candidate terms with the help of corpus-based links between single-word terms. First, we present a system for corpus-based acquisition of terminological relationships through discursive patterns. This system is built on previous work on automatic extraction of hyponymy links through shallow parsing. Second, we show how hypernym links between single-word terms can be extended to semantic links between multi-word terms through corpus-based extraction of semantic variants. The induced hierarchy is incomplete but provides an automatic generalization of singleword terms relations to multi-word terms that are pervasive in technical thesauri and corpora.",Automatic Acquisition and Expansion of Hypernym Links,Springer,2004,,Computers and the Humanities,38,4,"[{'value': '00104817', 'type': 'print'}]"
1682,http://www.jstor.org/stable/30204953,"{'string_id': '10.2307/30204953', 'id_scheme': 'DOI'}","The World Wide Web (WWW) today is so vast that it has become more and more difficult to find answers to questions using standard search engines. Current search engines can return ranked lists of documents, but they do not deliver direct answers to the user. The goal of Open Domain Question Answering (QA) systems is to take a natural language question, understand the meaning of the question, and present a short answer as a response based on a repository of information. In this paper we present QARAB, a QA system that combines techniques from Information Retrieval and Natural Language Processing. This combination enables domain independence. The system takes natural language questions expressed in the Arabic language and attempts to provide short answers in Arabic. To do so, it attempts to discover what the user wants by analyzing the question and a variety of candidate answers from a linguistic point of view.",Experimenting with a Question Answering System for the Arabic Language,Springer,2004,,Computers and the Humanities,38,4,"[{'value': '00104817', 'type': 'print'}]"
1683,http://www.jstor.org/stable/30204954,"{'string_id': '10.2307/30204954', 'id_scheme': 'DOI'}","Word sense disambiguation automatically determines the appropriate senses of a word in context. We have previously shown that self-organized document maps have properties similar to a large-scale semantic structure that is useful for word sense disambiguation. This work evaluates the impact of different linguistic features on self-organized document maps for word sense disambiguation. The features evaluated are various qualitative features, e.g. part-of-speech and syntactic labels, and quantitative features, e.g. cut-off levels for word frequency. It is shown that linguistic features help make contextual information explicit. If the training corpus is large even contextually weak features, such as base forms, will act in concert to produce sense distinctions in a statistically significant way. However, the most important features are syntactic dependency relations and base forms annotated with part of speech or syntactic labels. We achieve 62.9% ± 0.73% correct results on the fine grained lexical task of the English SENSEVAL-2 data. On the 96.7% of the test cases which need no back-off to the most frequent sense we achieve 65.7% correct results.",Evaluation of Linguistic Features for Word Sense Disambiguation with Self-Organized Document Maps,Springer,2004,,Computers and the Humanities,38,4,"[{'value': '00104817', 'type': 'print'}]"
1684,http://www.jstor.org/stable/30204955,"{'string_id': '10.2307/30204955', 'id_scheme': 'DOI'}",This paper presents an automatic construction of Korean WordNet from preexisting lexical resources. We develop a set of automatic word sense disambiguation techniques to link a Korean word sense collected from a bilingual machine-readable dictionary to a single corresponding English WordNet synset. We show how individual links provided by each word sense disambiguation method can be non-linearly combined to produce a Korean WordNet from existing English WordNet for nouns.,Multiple Heuristics and Their Combination for Automatic WordNet Mapping,Springer,2004,,Computers and the Humanities,38,4,"[{'value': '00104817', 'type': 'print'}]"
1685,http://dx.doi.org/10.31400/dh-hun.2018.1.126,"{'string_id': '10.31400/dh-hun.2018.1.126', 'id_scheme': 'DOI'}","This experimental study is an attempt to explore and test whether, by using current computational linguistics tools for the Hungarian language, it is possible to confirm hypotheses concerning the classification of Dániel Berzsenyi’s (1776-1836) poems drawing on their close reading. The paper also investigates if we can answer questions related to the phoneme theory and practice in Berzsenyi’s poems, which obviously cannot be examined by traditional close reading methods.",The Reading Machine,Eotvos Lorand University (ELTE),2018,"['Dániel Berzseny, R, Magyarlanc, topic modelling, distant reading']",Digitális Bölcsészet,1,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1686,http://dx.doi.org/10.31400/dh-hun.2018.1.129,"{'string_id': '10.31400/dh-hun.2018.1.129', 'id_scheme': 'DOI'}","Born digital content offers a more detailed and complete record of the present than what traditional sources provided of the past. The first section of this paper surveys current efforts and technologies to capture the present digital universe and reflects on the limitations of current web archives. The first attempts to archive the Hungarian web were made in 2017 in the National Széchényi Library, Budapest. The second part explores how this content could be harnessed for historical research, and how it will become the principal source of our recent past. The authors point out how web archives, and the resulting scale of data, will require new strategies and methodologies to deal with born digital sources effectively. They also show that born digital sources will also make it possible to pursue new types of inquiries that yield new results.",Web Archiving and Historical Research,Eotvos Lorand University (ELTE),2018,"['web archives, digital preservation, digital humanities, web historiography']",Digitális Bölcsészet,1,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1687,http://dx.doi.org/10.31400/dh-hun.2018.1.152,"{'string_id': '10.31400/dh-hun.2018.1.152', 'id_scheme': 'DOI'}","The inscriptions, personal greetings of the album amicorums in the 16th–17th centuries can be seen as remarkable resources for several research projects: philological, historical or biographical data can be identified; as well as the flow of knowledge can be detected. These, autograph books are the early examples of networking, and the set of these citations and signatures, included in these books, offers the possibility to interpret them as a network-model. In my study I would like to analyse the inscriptions of the autograph books between 1500 and 1700 as such a network that includes the owners and the inscriptors of the albums as nodes and the inscriptions as edges, contacts. This analysis would like to experiment with or test the method of network science and network analysis on an early modern corpus. In this study I will answer the questions how these interdisciplinary methods of network analysis could be adaptable to a culture-historical corpus and whether this method could help to reveal new or hidden philological or biographical nuances.",Network Analysis of Album Amicorums in the 16th–17th Centuries,Eotvos Lorand University (ELTE),2018,,Digitális Bölcsészet,1,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1688,http://dx.doi.org/10.31400/dh-hun.2018.1.223,"{'string_id': '10.31400/dh-hun.2018.1.223', 'id_scheme': 'DOI'}","The paper discusses the use of digital technologies in museums. It introduces various connotations of the concept of the digital museum. A special focus will be placed on the potentials of enhancing the museum visits in space and time and engaging the visitor as an active participant. Fifteen projects of the MOME TechLab are introduced, which have been created in an educational setting as student projects or as commissioned works by the TechLab Fellows collective. All of them have been designed for and in cooperation with cultural heritage institutions, representing a broad range of topics from literature to music and fine arts. They are clustered according to mechanisms and goals, such as surprise and emotional involvement, playful exploration, interactive data visualization, augmenting objects with additional information, assigning active role to visitors to voice opinion, and offering real and virtual tours beyond the walls of the museum. Most of the projects are demonstrated at http://techlab.mome.hu/en. The paper stresses the necessity of interdisciplinary cooperation and the need to scholarly assess the merits of digital installations in museums.",Digital Museum – in the Light of the Projects of MOME TechLab,Eotvos Lorand University (ELTE),2018,,Digitális Bölcsészet,1,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1689,http://dx.doi.org/10.31400/dh-hun.2018.1.227,"{'string_id': '10.31400/dh-hun.2018.1.227', 'id_scheme': 'DOI'}","The NKFIH-project that seeks to publish the digital edition of Zsigmond Móricz’s (1892–1913) correspondence was launched in 2016 at the Petőfi Literary Museum in Budapest. The project itself has been a huge challenge for the Móricz-research group because they can only partially rely on the earlier paper-based edition. Drawing on the experiences and resolved problems of the first years of the project, this paper focuses on the harmonization and relationship of a museum's programming/IT possibilities, the philological problems and the applied software capacities. As the paper discusses the challenges of an ongoing project, the study does not offer a holistic and comprehensive overview of the entire project, rather a list of problems as encountered along the way and their solutions.",The Digital Critical Edition of the Correspondence of Zsigmond Móricz (1892–1913),Eotvos Lorand University (ELTE),2018,,Digitális Bölcsészet,1,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1690,http://dx.doi.org/10.31400/dh-hun.2018.1.228,"{'string_id': '10.31400/dh-hun.2018.1.228', 'id_scheme': 'DOI'}","This article reviews the latest controversies surrounding digital humanities in the United States and summarizes their main arguments. It discusses a set of questions that these controversies raise, starting with the lack of definition as to what criteria need to be met to qualify a project as a digital humanities project. It discusses whether the digital humanities is a methodology or a new branch of scholarship. It argues for the need to develop an infrastructure to support the pursuit of digital humanities projects. It concludes by identifying future implications of digital humanities research for scholarship in the humanities.",Discussions and visions of the digital humanities,Eotvos Lorand University (ELTE),2018,,Digitális Bölcsészet,1,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1691,http://dx.doi.org/10.31400/dh-hun.2018.1.231,"{'string_id': '10.31400/dh-hun.2018.1.231', 'id_scheme': 'DOI'}","The Budapest Time Machine (Budapest Időgép, https://hungaricana.hu/hu/budapest-idogep/) is a recent project that offers innovative ways to use historical maps as a research tool. The web service contains overlapping maps from different periods with attached, geospacial, archival materials and photographs. These data present the recent history of Budapest and its geographic dimensions. This study demonstrates novel ways in which this resource can be used for research purposes.",Budapest Time Machine,Eotvos Lorand University (ELTE),2018,,Digitális Bölcsészet,1,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1692,http://dx.doi.org/10.31400/dh-hun.2018.1.232,"{'string_id': '10.31400/dh-hun.2018.1.232', 'id_scheme': 'DOI'}","Digitisation methods have significantly evolved in recent years and made it possible for all types of digital humanities projects, among them biographical projects to gain ground. One of the aims of a biographical research is to transform the texts of already digitized encyclopaedias (e.g. the Österreichisches Biographisches Lexikon 1815–1950, ÖBL, which consists of structured and unstructured data) to a machine readable format, and to perform network-, statistical- and computational linguistic analysis on the transformed data. The biographical research of ÖBL started in 2015 in the Institute for Modern and Contemporary Historical Research [Institut für Neuzeit- und Zeitgeschichtsforschung (INZ)] of the Austrian Academy of Sciences within the project Mapping historical networks: Building the new Austrian Prosopographical, Biographical Information System (APIS). The project is aimed at the digital processing of the ÖBL and the creation of a virtual research environment which is implemented in cooperation with the Austrian Center for Digital Humanities (ACDH) and the Institute for Urban and Regional Research [Institut für Stadt- und Regionalforschung (ISR)]. This paper discusses the technical and natural language processing solutions used in the APIS web application to open up biographical data for the researchers by processing and representing them in a structured format.",Digitisation and Network Research,Eotvos Lorand University (ELTE),2018,,Digitális Bölcsészet,1,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1693,http://dx.doi.org/10.31400/dh-hun.2018.1.234,"{'string_id': '10.31400/dh-hun.2018.1.234', 'id_scheme': 'DOI'}","A virtual national library can be distributed among people in arbitrary number of copies. Duplication is the only way of long-term preservation since the last millenials have shown that the durability of text carriers seems to have decreased as the number of copies have grown. Another condition of long-term preservation is that people also use the national cultural heritage to which they have access. This can be facilitated by providing narratives (such as literary history), not solely texts.“Digital philology” follows the classification principles of traditional philology when registering, editing the works and contextualizing them in a narrative. The first milestones of Hungarian digital philology: the repertory of the early modern Hungarian poetry: 1994, the first online critical edition: 1998, the first online Hungarian literary history: 2006. In recent years digital humanities scholars have highlighted hitherto unknown correlations and patterns in literary history thanks to these novel digital editions as well. These correlations are sometimes relevant, sometimes not. My book A vers (The Verse, 1991) foresaw the possibility of a “non-reading” philologist who uses Franco Moretti’s distant reading method.",Digital Humanities in the Virtual National Library,Eotvos Lorand University (ELTE),2018,,Digitális Bölcsészet,1,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1694,http://dx.doi.org/10.31400/dh-hun.2018.1.238,"{'string_id': '10.31400/dh-hun.2018.1.238', 'id_scheme': 'DOI'}","The aim of the #BREW-experiment, started in November 2015, was to test the working logic and mechanism of the social media platform designed for sharing images, Instagram. The researchers opted for an immersive, qualitative approach, focusing on the subculture of third wave coffee enthusiasts (including professionals like roasters, cafés and baristas, along with educated connoisseurs and aesthetically oriented fans). Employing tools developed by digital ethnographers but aiming to explore marketing and visual aesthetics as well, the experiment finetunes and relativizes (at the same time adding a new layer to) Lev Manovich's recently published results that were based on cultural analytic methodology involving big data as a source. The result of our research brings novel approaches both in terms of marketing and digital ethnography, creating an interdisciplinary dialogue with the history of photographic aesthetics and anthropology as well.",#BREW: Influencer Experiment in the Third Wave Coffee Community of Instagram,Eotvos Lorand University (ELTE),2018,,Digitális Bölcsészet,1,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1695,http://dx.doi.org/10.31400/dh-hun.2018.1.242,"{'string_id': '10.31400/dh-hun.2018.1.242', 'id_scheme': 'DOI'}","Chapter 5 of Macroanalysis: Digital Methods and Literary History demonstrates how computational approaches can be used to explore bibliographic and demographic metadata in order to reassess and postulate new claims about literary history. This chapter demonstrates how thoughtprovoking perspectives on literary periods and trends can be derived from a macroanalysis of book-level metadata – including book titles, author ethnicities, publication years, fictional settings, etc. Utilizing a database of metadata related to Irish-American literature, the author provides a reassessment of common narratives of Irish-American literary history and offers an alternative perspective on Charles Fanning’s theory of a “lost generation” of Irish-American writing. The metadata provide context for understanding Fanning’s reading of literary history and suggest that scholarly assumptions about Irish-American literary history might be based too much on a small sample of books by a homogenous group of authors. More broadly, the chapter argues that traditional literary scholars make a mistake in thinking that large-scale analyses are proposed as a replacement to close reading. On the contrary, a macroanalysis of metadata provides the necessary context for close readings and prompts new questions and new perspectives on literary history.",Metadata,Eotvos Lorand University (ELTE),2018,,Digitális Bölcsészet,1,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1696,http://dx.doi.org/10.31400/dh-hun.2019.2.282,"{'string_id': '10.31400/dh-hun.2019.2.282', 'id_scheme': 'DOI'}","Libraries and librarians can support digital humanists not only by having expertise in identifying objectives and working out ways of realization, informed by library and information science (LIS) but because both the digital humanities and LIS have several themes and issues in common. LIS aims to explore and understand the world of information and its human factors, and the latter is in the focus of digital humanists’ research as well. Besides showcasing these common features, examples of services offered to digital humanists (mainly) in Anglo-American academic libraries are presented in the paper.","Thoughts on the Relationship between Digital Humanities, Library and Information Science and Libraries",Eotvos Lorand University (ELTE),2019,,Digitális Bölcsészet,2,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1697,http://dx.doi.org/10.31400/dh-hun.2019.2.336,"{'string_id': '10.31400/dh-hun.2019.2.336', 'id_scheme': 'DOI'}","This paper discusses the potentials of the computer assisted analysis of Hungarian historical texts, which relies on the methods of linguistic, literary studies, information technology and statistics. It reviews the characteristics and applicability of different stylometric methods and demonstrates their use by case studies based on Kelemen Mikes’ (1690-1761) works. It examines the relationships between Mikes’ own works and his translations as well as the thematic separation of his works. The case studies highlight that the effectiveness of the stylometric analyses of Mikes’ writings can be improved by applying the digital Mikes’ dictionary.",The Potentials of Stylometry Analysis of Hungarian Historical Text Corpora,Eotvos Lorand University (ELTE),2019,,Digitális Bölcsészet,2,,"[{'value': '2630-9696', 'type': 'print'}]"
1698,http://dx.doi.org/10.31400/dh-hun.2019.2.387,"{'string_id': '10.31400/dh-hun.2019.2.387', 'id_scheme': 'DOI'}","While at the end of the 20th century information technology research offered an optimistic vision about the future of the cyber world, where the technology first of all, would help our access to the information and knowledge of humanity, in the early 21st century our cyber world has become Searle’s Chinese room: we have started to lose control over the inputs and outputs, the information, the content, or sometimes even our own private sphere. The first part of the study examines the emergence of various forms of cyberbullying through the „Jung Joon-yung scandal” in the K-pop world. This section analyses different forms of online shaming from cyber-stalking, to sharing of non-consensual pornography, and even the cyber-pillory. The second part of the paper interprets some manifestations of virtual or cyber-love with the help of Baudrillard’s concept of simulacrum. It is more than interesting how similarly the false (or elusive) love of the obsessive fans of Korean pop/TV/movie stars is constructed to that of simulacra, ie. robots and other imaginary heroes. The virtual communication not just simply makes it easier to express our different kinds of human emotions by overcoming some of the factors such as self-reflection that make real communication difficult for us. At the same time, virtual communication simplifies what we say (“dislike”, “love” etc.), but it also influences our way of thinking. We become different human beings by doing things in cyberspace that we would not do in real life. The simulacrum problem is interpreted through different Korean TV-drama-series, as the “I’m not a Robot,” the “Are you human too” and the “W”. The paper concludes with the discussion of some ethical problems concerning the digitized human consciousness.",A Design for Living in the Chinese Room I.,Eotvos Lorand University (ELTE),2019,,Digitális Bölcsészet,2,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1699,http://dx.doi.org/10.31400/dh-hun.2019.2.388,"{'string_id': '10.31400/dh-hun.2019.2.388', 'id_scheme': 'DOI'}","Europeana, the European digital platform for cultural heritage, has a heterogeneous collection of metadata records ingested from more than 3200 data providers. The original nature and context of these records were different. In order to create effective services upon them we should know the strength and weakness or in other words the quality of these data. This paper proposes a method and an open source implementation to measure some structural features of these data, such as completeness, multilinguality, uniqueness, record patterns, to reveal quality issues.",Measuring completeness as metadata quality metric in Europeana,Eotvos Lorand University (ELTE),2019,,Digitális Bölcsészet,2,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1700,http://dx.doi.org/10.31400/dh-hun.2019.2.390,"{'string_id': '10.31400/dh-hun.2019.2.390', 'id_scheme': 'DOI'}","Literary scholars have deployed the concept of “spoken language” to describe Kálmán Mikszáth’s fiction since the success of his short story collections entitled A tót atyafiak (Slovak Kinsmen, 1881) and A jó palócok (The Good Palots, 1882). Although this stylistic concept has become a key characteristic feature of Mikszáth’s oeuvre, no attempt has been made to elaborate on its definition. As scholarship assumes a clear-cut and measurable distinction between the written “literary” and “spoken” language, this paper claims that this spoken language has quantifiable linguistic markers. This is demonstrated by the morphological analysis of Kálmán Mikszáth and Mór Jókai’s fictional writings.",Machine-readable Literature: ”Spoken Language” in Mikszáth’s Short Stories,Eotvos Lorand University (ELTE),2019,,Digitális Bölcsészet,2,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1701,http://dx.doi.org/10.31400/dh-hun.2020.3.422,"{'string_id': '10.31400/dh-hun.2020.3.422', 'id_scheme': 'DOI'}","The paper presents a method of automatic analysis of sound devices by using Attila József’s poems as a case study. The first half of the paper discusses the functions of the program hunpoem_analyzer-TEI, which was developed for the automatic annotation of sound devices and addresses the main steps of the annotation process. The second half of the paper presents different frequency lists of lexical features and sound devices extracted from the annotated corpus.",Automatic Analysis of Sound Devices in Attila József’s Poems,Eotvos Lorand University (ELTE),2020,,Digitális Bölcsészet,3,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1702,http://dx.doi.org/10.31400/dh-hun.2020.3.443,"{'string_id': '10.31400/dh-hun.2020.3.443', 'id_scheme': 'DOI'}","In recent decades, the controversy over distant versus close reading has revolved around the question of scaling. Participants in the debate have either advocated distance (or speed), or have insisted on proximity (or slowness). On a meta-critical level, some have even argued for the need for any reading to be able to shift between, and thus combine, different scales. Very little has been said, however, about the limitations of scaling as such, and the irreducibility of reading to the logic of scales. Starting out from a few intricate formulations by some proponents of close and distant reading, this paper attempts to investigate the potentials and limitations of scaling, first by references to “Stanford” (the university as well as its founder), then by looking into Walter Benjamin’s treatment of film, and finally, though most importantly, by re-reading some passages in Poe’s detective story “The Purloined Letter.” These three points of reference (Stanford, Benjamin, Poe) seem analogous in the way they lay mutual emphasis on both serialization and segmentation, fast and slow motion, or distance and proximity. On a closer (or more distant?) look, however, Poe’s text goes even beyond such a scheme of scaling. It testifies to a logic of detection which surpasses mere zooming-in or zooming-out strategies, and points to a notion of reading that is “escalated” not simply because of its extraordinary range in terms of velocity or distance, but more radically because, although it still binds reading to specific scales, it also has an aspect that is utterly heterogeneous to any logic of scaling. The paper attempts to highlight this radically “escalated” (out-of-scale) aspect of reading.",Escalated Reading,Eotvos Lorand University (ELTE),2020,,Digitális Bölcsészet,3,,"[{'value': '2630-9696', 'type': 'electronic'}]"
1703,https://doi.org/10.3366/E1753854809000317,"{'string_id': '10.3366/E1753854809000317', 'id_scheme': 'DOI'}"," In the present investigation, the intelligibility of 17 Scandinavian language varieties and standard Danish was assessed among young Danes from Copenhagen. In addition, distances between standard Danish and each of the 17 varieties were measured at the lexical level and at different phonetic levels. In order to determine how well these linguistic levels can predict intelligibility, we correlated the intelligibility scores with the linguistic distances and we carried out a number of regression analyses. The results show that for this particular set of closely related language varieties phonetic distance is a better predictor of intelligibility than lexical distance. Consonant substitutions, vowel insertions and vowel shortenings contribute significantly to the prediction of intelligibility. ",Phonetic and Lexical Predictors of Intelligibility," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1704,https://doi.org/10.3366/E1753854809000329,"{'string_id': '10.3366/E1753854809000329', 'id_scheme': 'DOI'}"," In the present investigation we aim to determine to which degree various linguistic factors contribute to the intelligibility of Swedish words among Danes. We correlated the results of an experiment on word intelligibility with eleven linguistic factors and carried out logistic regression analyses. In the experiment, the intelligibility of 384 frequent Swedish words was tested among Danish listeners via the Internet. The choice of eleven linguistic factors was motivated by their contribution to intelligibility in earlier studies. The highest correlation was found in the negative correlation between word intelligibility and phonetic distances. Also word length, different syllable numbers, foreign sounds, neighbourhood density, word frequency, orthography, and the absence of the prosodic phenomenon of ‘stød’ in Swedish contribute significantly to intelligibility. Although the results thus show that linguistic factors contribute to the intelligibility of single words, the amount of explained variance was not very large (R2 (Cox and Snell)=.16, R2 (Nagelkerke)=.21) when compared with earlier studies which were based on aggregate intelligibility. Partly, the lower scores result from the logistic regression model used. It was necessary to use logistic regression in our study because the intelligibility scores were coded in a binary variable. Additionally, we attribute the lower correlation to the higher number of idiosyncrasies of single words compared with the aggregate intelligibility and linguistic distance used in earlier studies. Based on observations in the actual data from the intelligibility experiment, we suggest further steps to be taken to improve the predictability of word intelligibility. ",Linguistic Determinants of the Intelligibility of Swedish Words among Danes," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1705,https://doi.org/10.3366/E1753854809000330,"{'string_id': '10.3366/E1753854809000330', 'id_scheme': 'DOI'}"," In this experimental study, we aim to arrive at a global picture of the mutual intelligibility of various Dutch language varieties by carrying out a computer-controlled lexical decision task in which ten target varieties are evaluated – the Belgian and Netherlandic Dutch standard language as well as four regional varieties of both countries. We auditorily presented real as well as pseudo-words in various varieties of Dutch to Netherlandic and Belgian test subjects, who were asked to decide as quickly as possible whether the items were existing Dutch words or not. The experiment's working assumption is that the faster the subjects react, the better the intelligibility of (the language variety of) the word concerned. ",Mutual Intelligibility of Standard and Regional Dutch Language Varieties," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1706,https://doi.org/10.3366/E1753854809000342,"{'string_id': '10.3366/E1753854809000342', 'id_scheme': 'DOI'}"," In this paper we relate linguistic, geographic and social distances to each other in order to get a better understanding of the impact the Dutch-German state border has had on the linguistic characteristics of a sub-area of the Kleverlandish dialect area. This area used to be a perfect dialect continuum. We test three models for explaining today's pattern of linguistic variation in the area. In each model another variable is used as the determinant of linguistic variation: geographic distance (continuum model), the state border (gap model) and social distance (social model). For the social model we use perceptual data for friends, relatives and shopping locations. Testing the three models reveals that nowadays the dialect variation in the research area is closely related to the existence of the state border and to the social structure of the area. The geographic spatial configuration hardly plays a role anymore. ","The Dutch–German Border: Relating Linguistic, Geographic and Social Distances"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1707,https://doi.org/10.3366/E1753854809000354,"{'string_id': '10.3366/E1753854809000354', 'id_scheme': 'DOI'}"," The paper illustrates the results of a correlation study focusing on linguistic variation in an Italian region, Tuscany. By exploiting a multi-level representation scheme of dialectal data, the study analyses attested patterns of phonetic and morpho-lexical variation with the aim of testing the degree of correlation between a) phonetic and morpho-lexical variation, and b) linguistic variation and geographic distance. The correlation analysis was performed by combining two complementary approaches proposed in dialectometric literature, namely by computing both global and place-specific correlation measures and by inspecting their spatial distribution. Achieved results demonstrate that phonetic and morpho-lexical variations in Tuscany seem to follow a different pattern than encountered in previous studies. ",The Space of Tuscan Dialectal Variation: A Correlation Study," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1708,https://doi.org/10.3366/E1753854809000366,"{'string_id': '10.3366/E1753854809000366', 'id_scheme': 'DOI'}"," In this paper we apply various clustering algorithms to the dialect pronunciation data. At the same time we propose several evaluation techniques that should be used in order to deal with the instability of the clustering techniques. The results have shown that three hierarchical clustering algorithms are not suitable for the data we are working with. The rest of the tested algorithms have successfully detected two-way split of the data into the Eastern and Western dialects. At the aggregate level that we used in this research, no further division of sites can be asserted with high confidence. ",Recognising Groups among Dialects," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1709,https://doi.org/10.3366/E1753854809000378,"{'string_id': '10.3366/E1753854809000378', 'id_scheme': 'DOI'}"," Component models such as factor analysis can be used to analyse spatial distributions of a large number of different features – for instance the isogloss data in a dialect atlas, or the distributions of ethnological or archaeological phenomena – with the goal of finding dialects or similar cultural aggregates. However, there are several such methods, and it is not obvious how their differences affect their usability for computational dialectology. We attempt to tackle this question by comparing five such methods using two different dialectological data sets. There are some fundamental differences between these methods, and some of these have implications that affect the dialectological interpretation of the results. ",Comparison of Component Models in Analysing the Distribution of Dialectal Features," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1710,https://doi.org/10.3366/E175385480900038X,"{'string_id': '10.3366/E175385480900038X', 'id_scheme': 'DOI'}","In this study 91 local Swedish dialects were analysed based on vowel pronunciation. Acoustic measurements of vowel quality were made for 18 vowels of 1,014 speakers by means of principal component analysis of vowel spectra. Two principal components were extracted explaining more than of the total variance in the vowel spectra. Plotting vowels in the PC1-PC2 plane showed a solution with strong resemblance to vowels in a formant plane. Per location averages of all speakers were calculated and factor analysis was run with the 91 locations as data cases and the two acoustic component of the 18 words as variables. Nine factors were extracted corresponding to distinct geographic distribution patterns. The factor scores of the analysis revealed co-occurrence of a number of linguistic features.",Factor Analysis of Vowel Pronunciation in Swedish Dialects," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1711,https://doi.org/10.3366/E1753854809000391,"{'string_id': '10.3366/E1753854809000391', 'id_scheme': 'DOI'}"," Levenshtein distance, also known as string edit distance, has been shown to correlate strongly with both perceived distance and intelligibility in various Indo-European languages (Gooskens and Heeringa, 2004; Gooskens, 2006). We apply Levenshtein distance to dialect data from Bai (Allen, 2004), a Sino-Tibetan language, and Hongshuihe (HSH) Zhuang (Castro and Hansen, accepted), a Tai language. In applying Levenshtein distance to languages with contour tone systems, we ask the following questions: 1) How much variation in intelligibility can tone alone explain? and 2) Which representation of tone results in the Levenshtein distance that shows the strongest correlation with intelligibility test results? This research evaluates six representations of tone: onset, contour and offset; onset and contour only; contour and offset only; target approximation (Xu & Wang, 2001), autosegments of H and L, and Chao's (1930) pitch numbers. For both languages, the more fully explicit onset-contour-offset and onset-contour representations showed significantly stronger inverse correlations with intelligibility. This suggests that, for cross-dialectal listeners, the optimal representation of tone in Levenshtein distance should be at a phonetically explicit level and include information on both onset and contour. ",Representing Tone in Levenshtein Distance," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1712,https://doi.org/10.3366/E1753854809000408,"{'string_id': '10.3366/E1753854809000408', 'id_scheme': 'DOI'}"," In this paper the role of concept characteristics in lexical dialectometric research is examined in three consecutive logical steps. First, a regression analysis of data taken from a large lexical database of Limburgish dialects in Belgium and The Netherlands is conducted to illustrate that concept characteristics such as concept salience, concept vagueness and negative affect contribute to the lexical heterogeneity in the dialect data. Next, it is shown that the relationship between concept characteristics and lexical heterogeneity influences the results of conventional lexical dialectometric measurements. Finally, a dialectometric procedure is proposed which downplays this undesired influence, thus making it possible to obtain a clearer picture of the ‘truly’ regional variation. More specifically, a lexical dialectometric method is proposed in which concept characteristics form the basis of a weighting schema that determines to which extent concept specific dissimilarities can contribute to the aggregate dissimilarities between locations. ",The Role of Concept Characteristics in Lexical Dialectometry," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1713,https://doi.org/10.3366/E175385480900041X,"{'string_id': '10.3366/E175385480900041X', 'id_scheme': 'DOI'}"," The present paper investigates to what extent subjects base their judgments of linguistic distances on actual dialect data presented in a listening experiment and to what extent they make use of previous knowledge of the dialects when making their judgments. The point of departure for our investigation were distances between 15 Norwegian dialects as perceived by Norwegian listeners. We correlated these perceptual distances with objective phonetic distances measured on the basis of the transcriptions of the recordings used in the perception experiment. In addition, we correlated the perceptual distances with objective distances based on other datasets. On the basis of the correlation results and multiple regression analyses we conclude that the listeners did not base their judgments solely on information that they heard during the experiments but also on their general knowledge of the dialects. This conclusion is confirmed by the fact that the effect is stronger for the group of listeners who recognised the dialects than for listeners who did not recognise the dialects on the tape. ",What Role does Dialect Knowledge Play in the Perception of Linguistic Distances?," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1714,https://doi.org/10.3366/E1753854809000421,"{'string_id': '10.3366/E1753854809000421', 'id_scheme': 'DOI'}"," This paper describes a new method for quantifying the similarity of the lexical distribution of phonemes in different varieties of a language (in this case English). In addition to introducing the method, it discusses phonological problems which must be addressed if any comparison of this sort is to be attempted, and applies the method to a limited data set of varieties of English. Since the method assesses their structural similarity, it will be useful for analysing the historical development of varieties of English and the relationships (either as a result of common origin or of contact) that hold between them. ",Quantifying Dialect Similarity by Comparison of the Lexical Distribution of Phonemes," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1715,https://doi.org/10.3366/E1753854809000433,"{'string_id': '10.3366/E1753854809000433', 'id_scheme': 'DOI'}"," The research reported in this paper departs from most previous work in dialectometry in several ways. Empirically, it draws on frequency vectors derived from naturalistic corpus data and not on discrete atlas classifications. Linguistically, it is concerned with morphosyntactic (as opposed to lexical or pronunciational) variability. Methodologically, it marries the careful analysis of dialect phenomena in authentic, naturalistic texts to aggregational-dialectometrical techniques. Two research questions guide the investigation: First, on methodological grounds, is corpus-based dialectometry viable at all? Second, to what extent is morphosyntactic variation in non-standard British dialects patterned geographically? By way of validation, findings will be matched against previous work on the dialect geography of Great Britain. ",Corpus-based Dialectometry: Aggregate Morphosyntactic Variability in British English Dialects," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2009,,International Journal of Humanities and Arts Computing,2,1-2,"[{'value': None, 'type': None}]"
1716,https://doi.org/10.3366/ijhac.2009.0007,"{'string_id': '10.3366/ijhac.2009.0007', 'id_scheme': 'DOI'}"," This paper explores a method for creating large-scale urban 3D models using Historical GIS data. The method is capable of automatically generating realistic VR models based on GIS data at a low cost. 3D models of houses are created from polygon data, fences from line data, and pedestrians and trees from point data. The method is applied to the Virtual Kyoto Project in which the landscape of the whole city of Kyoto of the early Edo era (ca 17C) is reconstructed. ",Reconstruction of Kyoto of the Edo era based on arts and historical documents: 3D urban model based on historical GIS data," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2010,,International Journal of Humanities and Arts Computing,3,1-2,"[{'value': None, 'type': None}]"
1717,https://doi.org/10.3366/ijhac.2009.0008,"{'string_id': '10.3366/ijhac.2009.0008', 'id_scheme': 'DOI'}"," In this paper, I propose a new methodology for analysing landscape drawing methods using a GIS. The subject of my analysis is the genre of Japanese screen paintings known as rakuchū rakugai zu, created between the 16th and 18th centuries. Rakuchū rakugai zu provide bird's-eye views of the then-capital city of Kyoto, including buildings, natural features, and human activities. The methodology introduced here uses GIS spatial analysis functions to scan the painting surface onto a survey coordinate grid based on the relative positions of landmarks in the painting. The analytic sequence is as follows: (1) derive coordinate values for landmarks both on the painting and on a survey coordinate grid; (2) generate a link table from these two point-data sets; (3) use the projective transformation and rubber sheeting techniques to project the painting surface onto the survey coordinate grid; and (4) project the areas of the rubber sheet-derived polygons onto the painting. This process gives visual representation to differences between real space and the depicted space. Results show that rakuchū rakugai zu painted in the seventeenth century and later distorted real space more than those painted in the sixteenth century, indicating a decrease in adherence to conventional perspective-based painting. ",Unfolding the landscape drawing method of Rakuchū Rakugai Zu screen paintings in a GIS environment," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2010,,International Journal of Humanities and Arts Computing,3,1-2,"[{'value': None, 'type': None}]"
1718,https://doi.org/10.3366/ijhac.2009.0009,"{'string_id': '10.3366/ijhac.2009.0009', 'id_scheme': 'DOI'}"," There have been growing calls to develop the use of Geographical Information Systems (GIS) across the humanities. For this shift to take place, two things must be demonstrated: first, that it is technically possible to create a useful GIS of textual material, the main medium through which humanities research is conducted; and, secondly that such a database can be used to enhance our understanding of disciplines within the humanities. This paper reports on a pilot project that created a GIS of two textual accounts of tours of the Lake District: Thomas Gray's 1769 excursion and Samuel Taylor Coleridge's self-styled ‘circumcursion’ of 1802. It describes how these accounts were converted into a GIS and explores various methodologies that can then be used to explore the spatialities embedded within their respective tours. The resulting material has been placed on the Internet at http://www.lancs.ac.uk/mappingthelakes. The pilot demonstrates that it is both possible, and conceptually fruitful, to move GIS beyond the quantitative arena in which it currently resides and into more qualitative areas of humanities research. ","Thomas Gray, Samuel Taylor Coleridge and geographical information systems: A literary GIS of two Lake District tours"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2010,,International Journal of Humanities and Arts Computing,3,1-2,"[{'value': None, 'type': None}]"
1719,https://doi.org/10.3366/ijhac.2009.0010,"{'string_id': '10.3366/ijhac.2009.0010', 'id_scheme': 'DOI'}", The use of a Geographic Information System (GIS) in the study of lithic procurement patterns provides crucial information about energy expenditure and territoriality of prehistoric communities. Cost-weight analyses calculate proxy energetic expenditures of agents who transport lithic materials from a quarry to the nearest habitation site. Illustrating energy expenditure values onto maps helps us understand changes in toolstone procurement patterns through time. Comparing energy expenditure values from one time period to another also demonstrates when agents developed the sense of territoriality. This research investigates how the central Mesa Verde Puebloans utilised resources on their landscape from A.D. 600 to 1280. ,Prehistoric resource procurement in the central Mesa Verde region: A study of human mobility and social interactions using GIS," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2010,,International Journal of Humanities and Arts Computing,3,1-2,"[{'value': None, 'type': None}]"
1720,https://doi.org/10.3366/ijhac.2009.0011,"{'string_id': '10.3366/ijhac.2009.0011', 'id_scheme': 'DOI'}"," The paper assesses the role and potential of Cultural Virtual Reality (CVR) for the modelling of archaeological landscapes. As digital visualisations are spreading rapidly as a tool for research and as a medium of communication in landscape research and archaeology, benefits and limitations are discussed. Published case studies are reviewed and the question whether or to which extent computer game engines may assist in the modelling of past landscapes is evaluated. Therefore a virtual model of Laxton castle (England) is introduced, which was created using SEGA gaming software. ",Visualisation in archaeology: An assessment of modelling archaeological landscapes using scientific and gaming software," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2010,,International Journal of Humanities and Arts Computing,3,1-2,"[{'value': None, 'type': None}]"
1721,https://doi.org/10.3366/ijhac.2009.0012,"{'string_id': '10.3366/ijhac.2009.0012', 'id_scheme': 'DOI'}"," A year-long project to investigate the use of digital technologies in prosopographical research resulted in the creation of intermediary objects (called ‘factoids’) within the database to model the multiple relationships possible among elite medieval landholders. These factoids facilitated a modular approach to database development, whereas a Virtual Research Environment (VRE) allowed flexible and speedy data entry. GIS provided a mapping function via Web Map Services. ",Digitising the Middle Ages: The experience of the ‘Lands of the Normans’ project," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2010,,International Journal of Humanities and Arts Computing,3,1-2,"[{'value': None, 'type': None}]"
1722,https://doi.org/10.3366/ijhac.2009.0013,"{'string_id': '10.3366/ijhac.2009.0013', 'id_scheme': 'DOI'}"," One of the early drivers of historical GIS was the development of national historical GISs. These systems usually hold all of a country's census and related statistics from the nineteenth and twentieth centuries. As such they have represent an extremely valuable resource, but at the same time they were and remain extremely expensive and time consuming to build. Was the investment worthwhile? This paper takes one of these systems, the Great Britain Historical GIS, and explores how it was built, what methodologies were developed to exploit the data that it contains, and provides an example to demonstrate how it made possible a unique analysis of railroads in Wales before the First World War. ","National Historical Geographical Information System as a tool for historical research: Population and railways in Wales, 1841–1911"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2010,,International Journal of Humanities and Arts Computing,3,1-2,"[{'value': None, 'type': None}]"
1723,https://doi.org/10.3366/ijhac.2009.0014,"{'string_id': '10.3366/ijhac.2009.0014', 'id_scheme': 'DOI'}"," Visible Past is a cross-platform, scalable research and learning environment. Its primary aim is to help students and scholars experience and communicate with fully immersive, historically accurate models of past geographic realities or to relate information to specific real geographic locations. It also includes location-aware capabilities and in the future will include attention-aware learning scenario. ",Visible Past: A location and attention aware learning and discovery environment for digital humanities," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2010,,International Journal of Humanities and Arts Computing,3,1-2,"[{'value': None, 'type': None}]"
1724,https://doi.org/10.3366/ijhac.2009.0015,"{'string_id': '10.3366/ijhac.2009.0015', 'id_scheme': 'DOI'}"," This papers starts with some mysterious contribution by Michel Foucault (1967) about heterotopias as special epistemological sites. With a recent case-study – an immersive virtual reality art project dealing with some ancient abbey reconstruction and managed by a French engineering school – we analyse the successive attempts to satisfy the system users by extending Foucault's heterotopology, which appears to be useful and creative for the Virtual Reality research communities. ",Exploring informed virtual sites through Michel Foucault's heterotopias," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2010,,International Journal of Humanities and Arts Computing,3,1-2,"[{'value': None, 'type': None}]"
1725,https://doi.org/10.3366/ijhac.2009.0016,"{'string_id': '10.3366/ijhac.2009.0016', 'id_scheme': 'DOI'}"," The UK's Network of Expert Centres: A collaborative approach to the digital arts and humanities. A Network of Expert Centres has been recently been established with a broad mission to facilitate exchange of expertise and outreach across the community of practice engaged in all aspects of digital arts and humanities research and scholarship, including practice-led research; and to promote and facilitate engagement with digital methodologies within the broader research community. Areas of interest and expertise of members include data creation, curation, preservation, management (including rights and legal issues), access and dissemination, and methodologies of data use and re-use. Its membership is open to all such Centres in Great Britain and Ireland. Since the demise of the Arts and Humanities Data Service and the AHRC ICT Methods Network, there has been no centralized supporting infrastructure and co-oordination of activity in this area. While the Network is by no means a substitute for these services, it will provide a much-needed focus and community resource for all aspects of the digital arts and humanities, including digital tools, projects and methods, as well as data management and sustainability. The Network will also provide greater visibility for publicly funded research projects in this area. The Network's mission is to advocate, promote and raise awareness and understanding of the use of ICT in research and scholarship in arts and humanities. It will take an active role in developing the scholarly communication agenda in relation to all aspects of digital publications, and outputs. Publications and outputs developed by members will be disseminated via the community resource, arts-humanities.net, which will support the development and exchange of expertise, knowledge, standards and good practice. The Network will also identify and represent the needs of the research community in this key strategic area, and conduct dialogue with relevant stakeholders including funders, libraries, and those engaged in teaching and research in related disciplines. This article sets out the background to the establishment of the Network, its structure and plans for future development. ",The British lsles Network of Expert Centres: A collaborative approach to the digital arts and humanities," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2010,,International Journal of Humanities and Arts Computing,3,1-2,"[{'value': None, 'type': None}]"
1726,https://doi.org/10.3366/ijhac.2011.0004,"{'string_id': '10.3366/ijhac.2011.0004', 'id_scheme': 'DOI'}"," The present study deploys acoustic theory and digital analysis to investigate the dynamics of the inter-relationship of architecture and music. It assesses the impact of the built environment on music composition and performance. Drawing upon the science that underpins both architecture and music, it is also informed by the qualitative and artistic attributes of both. Reference to a specific case study, St. Patrick's Cathedral in Dublin, describes the design and implementation of a methodology designed to allow for context and era-specific assessment. The research design is interdisciplinary, bridging theory and practice. The methodology is firmly based on the use of digital technologies, which allow for efficient, accurate and replicable procedure. Data capture, analysis and mapping of the architectural site was supplemented by reference to primary archived material. Digital preservation of primary material was an integral part of the project; the resulting record created a more complete digital repository of the cathedral, combining the information which can be read in the structure itself with documents relating to its history. This article makes use of Victor Hugo's concept of the ‘Book of Stone’ to comment on the information gleaned from paper records and digital analysis of the architecture and acoustics of St Patrick's Cathedral, Dublin. ",Drawing on the digital: Analysis and modelling in architecture and music," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,4,1-2,"[{'value': None, 'type': None}]"
1727,https://doi.org/10.3366/ijhac.2011.0005,"{'string_id': '10.3366/ijhac.2011.0005', 'id_scheme': 'DOI'}"," Drawing upon previous theoretical and practical work in historical and qualitative applications of Geographical Information Systems (GIS), this paper, in Giles Deleuze and Félix Guattari's terminology, conceptualizes GIS as ‘an abstract machine’ which plays a ‘piloting role’ which does not ‘function to represent’ something real, but rather ‘constructs a real which is yet to come.’ To illustrate this digital humanities mapping methodology, the essay examines Irish writer Patrick Kavanagh's novel The Green Fool (1938) and epic poem The Great Hunger (1946) and their respective contrasting topophilic and topophobic renderings of landscape, identity and sense of place under the lens M.M. Bakhtin's ‘Historical Poetics’ (chronotope) to illuminate GIS's ability to engage in spatio-discursive visualization and analysis. The conceptualizations and practices discussed in this paper reconsider GIS software/hardware/techniques as a means to engage subjects of concern to literary and cultural studies commensurate with the recent strong interest in the geographical and spatial dimensions of these cognate areas. ",Abstract Machine – Geographical Information Systems (GIS) for literary and cultural studies: ‘Mapping Kavanagh’," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,4,1-2,"[{'value': None, 'type': None}]"
1728,https://doi.org/10.3366/ijhac.2011.0006,"{'string_id': '10.3366/ijhac.2011.0006', 'id_scheme': 'DOI'}"," This article documents the digital humanities aspects of The Holinshed Project at the University of Oxford. It outlines the nature of the project, in particular the need to compare paragraphs of the 1577 and 1587 editions of Holinshed's Chronicles of England, Scotland, and Ireland. In order to accomplish these comparisons, a tool known as the TEI-Comparator was created. This is a bespoke fuzzy text comparison engine with a frontend web interface designed for the project. The TEI-Comparator automatically matches reorganised and fragmented paragraphs in the two editions. It is then used for confirming, removing, creating and annotating the links between the editions. This article describes the steps necessary to use the TEI-Comparator, its comparison algorithm, and the handling of the output it creates with respect to its use for The Holinshed Project. The TEI-Comparator was launched in 2009 as an open source project on Sourceforge and is available for other projects to use. ",The Holinshed Project: Comparing and linking two editions of Holinshed's Chronicle," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,4,1-2,"[{'value': None, 'type': None}]"
1729,https://doi.org/10.3366/ijhac.2011.0007,"{'string_id': '10.3366/ijhac.2011.0007', 'id_scheme': 'DOI'}"," The digitisation of print publications began on a small scale. In 2005 it entered a new phase, mass digitisation, with the beginning of Google's Books programme. This aimed to convert whole collections and libraries into electronic form. A US challenge to Google's digitisation plans in 2005 led to a complex legal dispute, between the company and authors, publishers and others, which remains unresolved. The advantages of the revised ‘Google Books Settlement’ (November 2009) are described, as well as the serious objections to it: these include uncertainties about ownership and use of the digitised texts and their long-term preservation, the quality of digitisation, monopoly of mass digitisation activity, and the fact that Google is not a permanent trusted institution. Alternatives to Google are analysed from the point of view of libraries and others wishing to digitise collections: these include public sponsorship and private partnerships. Future uses of the product of mass digitisation are hard to predict. Among them may be novel ways of analysing large corpora of unstructured text, and substantial interaction with contributor-users. ","Big digitisation: Origins, progress and prospects"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,4,1-2,"[{'value': None, 'type': None}]"
1730,https://doi.org/10.3366/ijhac.2011.0008,"{'string_id': '10.3366/ijhac.2011.0008', 'id_scheme': 'DOI'}"," The paper describes the key concepts and formal grammatical structures underpinning the design of a domain specific language (DSL) developed for dynamically specifying the logical choice-relationships between events and cues within live performance. A brief survey of existing approaches to describing and modelling live performance is given, both those based on generic terms together with approaches specific to particular genres and styles. The development of a generalised model of performer-activity in live performance is then introduced together with logical structures developed from the generic model which inform the language design and allow it to integrate with a wide range of performance traditions and genres. The implementation and application of these ideas in developing the Emerge language is explored through both a formal grammar and a discussion of the choices that had to be made between ease of use for the intended target users and grammatical simplicity. The features of the prototype software and language are briefly described showing how they might enable artists, both individually and collaboratively, to specify decision-making structures in advance of a performance and participate in shaping a performance as it proceeds. ","Words of power: Emerge, a language for the dynamic control of live performance"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,4,1-2,"[{'value': None, 'type': None}]"
1731,https://doi.org/10.3366/ijhac.2011.0009,"{'string_id': '10.3366/ijhac.2011.0009', 'id_scheme': 'DOI'}"," The digitisation of museum collections provides great opportunities for broad communication and access. However, currently the majority of online ‘virtual’ museums present information two-dimensionally. Three-dimensional data capture using laser scanning provides the potential to generate 3D virtual objects that can be used for a much greater interactive experience. This paper presents the results of a JISC-funded project aimed at the generation of a 3D online museum of an internationally important collection of Egyptological artefacts that have not previously been publicly available. The results from the project demonstrate the value of 3D museums, in addition to highlighting some of the future possibilities for interaction with objects and the ways in which such virtual museums can revolutionise access to collections for education and public interest. It also stresses ways in which such collections can benefit scholars in terms of reference collections, object analysis and interpretation. The question of objectivity and authenticity of virtual collections, in comparison with real objects is raised. ",The Eton Myers collection virtual museum," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,4,1-2,"[{'value': None, 'type': None}]"
1732,https://doi.org/10.3366/ijhac.2011.0010,"{'string_id': '10.3366/ijhac.2011.0010', 'id_scheme': 'DOI'}"," Laser scanning of stone sculpture creates accurate digital models that may be accessed and interpreted by a wide range of end users at many levels of education and experience. This article explores that activity by focusing on the process of recording a damaged early medieval inscribed high cross shaft at Toureen Peacaun, Co. Tipperary, Ireland. Revisualising the monument is only one aspect of the scholarly process. Renewed interpretations of cultural heritage objects in digital media provide many answers but also stimulate further challenges for humanities computing, notably, how to address the non-machine readable elements of cultural heritage data. ","Revisioning sculpture: A cultural heritage case study from Toureen Peacaun, County Tipperary, Ireland"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,4,1-2,"[{'value': None, 'type': None}]"
1733,https://doi.org/10.3366/ijhac.2011.0011,"{'string_id': '10.3366/ijhac.2011.0011', 'id_scheme': 'DOI'}"," The salient features of endangered language documentation are diversity and protocol. The Endangered Languages Archive at SOAS is creating a new archive to take advantage of web-based social networking to address these issues. The archive is reconceived as a platform for conducting relationships between information providers (depositors) and information users, using the now-familiar idiom of Facebook. Rather than the archive having to continually broker complex access conditions, depositors and requesters can negotiate directly with each other via the archive to achieve more flexible and creative outcomes. ",Archives 2.0 for endangered languages: From disk space to MySpace," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,4,1-2,"[{'value': None, 'type': None}]"
1734,https://doi.org/10.3366/ijhac.2011.0012,"{'string_id': '10.3366/ijhac.2011.0012', 'id_scheme': 'DOI'}"," Documents of all forms have always been at the centre of research in the humanities and arts. As more literature becomes available electronically, the impact of the digital medium on reading and research also grows. This article provides a review of key research on the limitations of digital reading, spanning a twenty-year period. Most of this literature is from the computer science field of human-computer interaction. From this backdrop, this article reports the main findings of a series of detailed studies that have investigated the current difficulties and limitations of digital document reader software. These studies, all conducted in the last four years, demonstrate that there are major limitations in digital document reader software that impede a number of simple interactions between user and document. It is also demonstrated that the current problems with this software can be overcome through a re-design of the human-computer interface. Ultimately, the current shortcomings of digital document reader software are a major impediment to realising the full potential of digital humanities. However, there is ample scope to make tangible improvements and progress is urgently needed. ",The usability of digital documents – A barrier to digital scholarship," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,4,1-2,"[{'value': None, 'type': None}]"
1735,https://doi.org/10.3366/ijhac.2011.0013,"{'string_id': '10.3366/ijhac.2011.0013', 'id_scheme': 'DOI'}"," The work of scholars is rapidly changing. As new digital resources and tools are developed, and old tools and resources reinvented for the digital world, the practice of scholarship is quickly adapting to the expectations that content should be accessible from anywhere, that it is a raw material to be manipulated, and that an excess of information is the major challenge facing scholars. Despite these expectations, the current generation of tools are inadequate for emerging scholarly practices. JSTOR's Advanced Technology Research (ATR) group has built and collaborated on a number of software projects and platforms that attempt to provide the next generation of tools for scholars. We provide an overview of these projects. ",Providing next-generation tools for scholars: JSTOR's Advanced Technology Research group," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,4,1-2,"[{'value': None, 'type': None}]"
1736,https://doi.org/10.3366/ijhac.2011.0014,"{'string_id': '10.3366/ijhac.2011.0014', 'id_scheme': 'DOI'}"," The Educause Horizon Report states that while web-based tools are rapidly becoming standard in education and in the workplace and technologically mediated communication is the norm, fluency in information, visual, and technological literacy is not formally taught to most students. In the light of this we need new and expanded definitions and paradigms of Information and Media Literacy (IML) that promote mastering critical thinking as well as technical proficiency. This study, forming part of the Irish Integrative Learning Project, attempts to test an assumption that entrants to the humanities are media literate. This assumption, often made by educators in higher education, largely goes unchallenged. This paper reflects on the strengths and weaknesses of a series of media literacy workshops when facilitated in conjunction with the undergraduate History of Art course at University College Cork, Republic of Ireland from 2007–2009. It concludes with an example of how these media literacy workshops, indirectly, had surprisingly positive affects on a dyslexic student by scaffolding his learning and providing him with alternative entry points to learning whereby he was empowered to confidently articulate History of Art's signature disciplinary skills-set at a foundation level: critical visual analysis. ",Too much information: Why facilitate information and media literacy?," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,4,1-2,"[{'value': None, 'type': None}]"
1737,https://doi.org/10.3366/ijhac.2011.0018,"{'string_id': '10.3366/ijhac.2011.0018', 'id_scheme': 'DOI'}"," This article reflects on the author's experience with historical GIS during the research of his 2008 manuscript Mapping Decline: St. Louis and the Fate of the American City. It assesses, in turn, the utility of maps and their role in historical research and writing; the investigatory and explanatory potential of historical GIS; and some of the limits and pitfalls of historical mapping. ","Lost in space, or confessions of an accidental geographer"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,5,1,"[{'value': None, 'type': None}]"
1738,https://doi.org/10.3366/ijhac.2011.0019,"{'string_id': '10.3366/ijhac.2011.0019', 'id_scheme': 'DOI'}"," The recognition that geo-technologies are playing an increasingly important role in historical research led to a meeting in Barcelona in December 2007, attended by fifty leading academics and GIS practitioners from Europe, with additional representation from leading North American experts. The conference benefited from previous meetings in Florence (2000), Ghent (2002), Amsterdam (2004), Belfast (2005), Berlin (2006), and Mainz (2007). The three-day Barcelona conference had three key aims: (1) to survey the use and maturity of historical GIS; (2) to hold an inclusive, facilitated discussion on the feasibility and requirements for a European historical GIS; and (3) to develop a white paper on how to advance historical GIS within Europe. A network of networks was a major proposal to come from the Barcelona meeting, and its outlines and rationale are advanced in this white paper. ",A place in Europe: enhancing European collaboration in historical GIS," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,5,1,"[{'value': None, 'type': None}]"
1739,https://doi.org/10.3366/ijhac.2011.0020,"{'string_id': '10.3366/ijhac.2011.0020', 'id_scheme': 'DOI'}"," In this paper we describe how historical land use information has been derived for the whole of Europe, using the World Atlas of Agriculture, scale 1: 2,500,000. This paper describes the process of converting the analog land-use maps to a digital European historical land-use database, the Historical Land Use Database 1960 (HISLU60). The processing techniques that are applied in this process are commonly used for the processing of satellite imagery. The paper maps were scanned and geo-referenced. Image filtering techniques were used to eliminate the cartographic elements like text and boundaries. Land-use classes were created from the colours in the maps using image classification techniques. As a last step, GIS filtering functions were used to eliminate remaining cartographic elements and small classification errors. The HISLU60 database contains six land-use classes; arable land, grassland, forest, non-agricultural land, inland water and urban. It is a raster dataset with a cell size of 250 meters. The HISLU60 database was validated by performing a statistical comparison with three reference datasets. The overall classification accuracy is around 50 percent. Despite its limitations, the HISLU60 database gives an overview of the pan-European land use around 1960. ",Historical land use databases: a new layer of information for geographical research," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,5,1,"[{'value': None, 'type': None}]"
1740,https://doi.org/10.3366/ijhac.2011.0021,"{'string_id': '10.3366/ijhac.2011.0021', 'id_scheme': 'DOI'}"," The act of classification has the widest implications for scholarship. Whatever the format, it involves the totality of our being. The use of our eyes indicates that decisions about whatever it is that we observe have already been made. Yet the interaction between the mechanical act of seeing and the mind or memory has rarely been registered. An object once seen implies that the researcher's consciousness is engaged. The description of mere shape records that interaction. To establish whether sub-conscious decisions have been made as to the meaning of a shape, it might be placed in an armature. VIPS/ip software, created by both computer scientists and art historians, provides such an armature. The separate roles played by memory, brain, and eye in engaging with the shapes, encountered on the pictograph sites of the Lake of the Woods might then be detected. Subsequent labelling which bears these roles in mind just might isolate the contribution made by memory. The systematic identification and cataloguing of such images by an investigator may also enable us to understand something of the intricate and uncharted past of the Canadian Shield and about ourselves. ",Do not make snap decisions about what you are seeing: how digital analysis of the images from the Canadian Shield highlights the difficulties in classifying shapes," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,5,1,"[{'value': None, 'type': None}]"
1741,https://doi.org/10.3366/ijhac.2011.0022,"{'string_id': '10.3366/ijhac.2011.0022', 'id_scheme': 'DOI'}"," The advent of e-learning has been a welcomed development in African universities, especially in countries where the demand for university education far outstrips capacity. This form of instruction not only has helped in reducing the problem of managing and testing large classes, but it also has helped lecturers in providing valuable assistance to students who would otherwise not have such access. The limitations of the e-learning platform coupled with a distorted student-teacher ratio has raised concerns about quality, especially for traditional humanities disciplines where the emphasis on argumentative rigor and critical thinking are at odds with the science-leaning orientation of e-learning platforms. This concern is especially important because the technology is relatively new and there are problems of access not only in terms of infrastructure but also in terms of the relevant computer literacy skills required of users of the technology. This essay examines the problems associated with the use of e-learning in teaching and examining traditional humanities courses in general but especially the problems encountered in using e-learning in teaching and assessing critical thinking courses at the University of Botswana. I argue that although certain aspects of e-learning are structured, confining, and therefore unsuitable for traditional humanities disciplines, e-learning can still be an appropriate tool for the humanities if used appropriately and creatively. ",E-learning platforms and humanities education: an African case study," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,5,1,"[{'value': None, 'type': None}]"
1742,https://doi.org/10.3366/ijhac.2011.0027,"{'string_id': '10.3366/ijhac.2011.0027', 'id_scheme': 'DOI'}"," The Mapping the Moment: Performance Culture in Nottingham, 1857–67 project (http://www.nottingham.ac.uk/mapmoment) is an interdisciplinary collaboration between performance history and geography, which has as its aim the investigation of the complex connections between the different kinds and sites of entertainment, and between the people and communities involved in making and watching performance, in what was then the rapidly developing East Midlands town of mid-Victorian Nottingham. As questions of space, proximity and temporal relationships are central to the choice-making processes of both venue managers and potential spectators, so we sought to create a resource, based on a map of nineteenth-century Nottingham, that organised information about the town and its performance culture in ways that highlighted such connections and allowed us to explore the relationships between place, performance and audience within the town. Here we describe the creation of the resource and reflect on the development of our spatio-temporal mapping approach, arguing that this offers the potential to address patterns of connectivity and change across both time and space, not as a series of disconnected cross sections but rather as representative of the ‘world of process’ in which the events we study actually take place. ","Mapping the Moment: a spatio-temporal interface for studying performance culture, Nottingham, 1857–1867"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,5,2,"[{'value': None, 'type': None}]"
1743,https://doi.org/10.3366/ijhac.2011.0028,"{'string_id': '10.3366/ijhac.2011.0028', 'id_scheme': 'DOI'}"," Gazetteers play an important but largely unsung role in historical research, used with maps to help place people and events in spatial context. Recent years have seen new interest in digital gazetteers as bridges between the geospatial web and the semantic web, but many existing digital gazetteers and data models do not meet the needs of historians, as they focus on physiographic landforms rather than places of cultural meaning or administrative units. Historical researchers need to know about places whose locations are not knowable with certainty. They need to know about alternative names for places, about how names have evolved over time, and the specific historical contexts in which names were used. While GIS researchers propose temporal gazetteers, which will somehow include the precise dates at which features were created and removed, we propose historical gazetteers in which dates appear mainly in order to help reference particular instances of place names. Longer term, we need cultural gazetteers or toponymic encyclopedias that describe places as well as locate them. ",On historical gazetteers," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,5,2,"[{'value': None, 'type': None}]"
1744,https://doi.org/10.3366/ijhac.2011.0029,"{'string_id': '10.3366/ijhac.2011.0029', 'id_scheme': 'DOI'}"," Whilst the application of online multimedia digital technology within arts and humanities research has burgeoned over the last decade, the practice of openly conducting collaborative and in particular discursive research publicly online remains one of the most unfamiliar and conceptually problematic areas for many academics in the field. Based on user surveys, blog posts, and forum discussions, this article provides both an account and assessment of Web 2.0 technologies in use on a large-scale arts and humanities research project. Examining usage by and impressions of both the project team and the wider community of users, it investigates both the advantages gained and problems faced through the use of a virtual research environment (VRE). It also pays special attention to the use of video and its implications for research practices. ","Old habits, new dynamics: a case study of Web 2.0 in collaborative humanities and arts research"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,5,2,"[{'value': None, 'type': None}]"
1745,https://doi.org/10.3366/ijhac.2011.0030,"{'string_id': '10.3366/ijhac.2011.0030', 'id_scheme': 'DOI'}", One strategic way to understand the impact of digital technologies on a domain of research is to identify how these technologies factor into scholars’ realm of everyday academic practice. I present an evaluative strategy grounded in the field of social informatics referred to as socio-technical interaction networks (STIN) approach. The central tenet of this analytical approach is that it models the context of humanities scholars’ routine engagement with digital technologies as a sociotechnical network-a web of relationships of technological and social elements. Using qualitative data collected from multiple sources I demonstrate the value of STIN as an analytical approach by analysing the assimilation and use of Geographical Information systems (GIS) by researchers studying historical topics. I conclude by identifying some of the core issues emerging from this study and how these issues can be useful to think about policy and strategy aspects related to digital humanities. ,The assimilation and use of GIS by historians: a sociotechnical interaction networks (STIN) analysis," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2011,,International Journal of Humanities and Arts Computing,5,2,"[{'value': None, 'type': None}]"
1746,https://doi.org/10.3366/ijhac.2013.0078,"{'string_id': '10.3366/ijhac.2013.0078', 'id_scheme': 'DOI'}"," This paper reflects on what research infrastructures are, suggesting that there are two dominant themes – one arising from the funding bodies postulating that research infrastructure development offers unprecedented opportunities to drive forward that which is new, which is innovative, and which drives competition and success; the other which suggests that infrastructure is subordinate, a substrate that supports the real work of research, and which becomes a thing that shifts into the background and is invisible. This paper argues that neither of these positions is wholly true or particularly helpful as we move to invest significant sums of money in digital research infrastructures. Instead, I propose that we need to view infrastructure as a material and experiential presence that is embedded in the practices and experience of research, which builds on and enhances that which already exists, that unites scholars with archivists, librarians, and museum curators, and that also finds a place for the amateur. Finally, and most importantly, I wish to argue that we must foreground the ‘research’ in ‘research infrastructures’. concentrating less on the component parts of infrastructure and instead focusing on its relationship to research practices. ",What are Research Infrastructures?," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2013,,International Journal of Humanities and Arts Computing,7,1-2,"[{'value': None, 'type': None}]"
1747,https://doi.org/10.3366/ijhac.2013.0079,"{'string_id': '10.3366/ijhac.2013.0079', 'id_scheme': 'DOI'}"," This paper is concerned with the development of digital humanities infrastructure – tools and resources which make using existing e-content easier to discover, utilise and embed in teaching and research. The past development of digital content in the humanities (in the United Kingdom) is considered with its resource-focused approach, as are current barriers facing digital humanities as a discipline. Existing impacts from e-infrastructure are discussed, based largely on the authors’ own discrete or collaborative projects. This paper argues that we need to consider further how digital resources are actually used, and the ways in which future digital resources might enable new types of research questions to be asked. It considers the potential for such enabling resources to advance digital humanities significantly in the near future. ",E-Infrastructure in the Humanities," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2013,,International Journal of Humanities and Arts Computing,7,1-2,"[{'value': None, 'type': None}]"
1748,https://doi.org/10.3366/ijhac.2013.0080,"{'string_id': '10.3366/ijhac.2013.0080', 'id_scheme': 'DOI'}"," In this article we present our ideas for an integrating activity for archival research on the Holocaust. We analyse how we can improve Holocaust-related collection descriptions for research, which we will make available online, and how EHRI provides travel grants for transnational access to existing infrastructures in Holocaust research. Both approaches help us overcome that Holocaust-related material is geographically dispersed and address the challenges for historical research stemming from the way documentation on the Holocaust has been attempted up to now. We have chosen to implement the EHRI integrated information resource using graph databases. With their emphasis on relationships, graph databases are particularly well suited for historical research in particular and humanities research in general. We analyse the architecture and implementation details of this novel approach and show how graph databases integrate with traditional ways of searching and browsing historical collections. This way, we support more advanced means of access to facts in the documents and enable deep semantically meaningful access to the documents. The innovation of EHRI lies in the combination of digital and non-digital means to integrate existing infrastructures. We believe this might be a model for many related research activities in the humanities. ",Integrating Holocaust Research," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2013,,International Journal of Humanities and Arts Computing,7,1-2,"[{'value': None, 'type': None}]"
1749,https://doi.org/10.3366/ijhac.2013.0081,"{'string_id': '10.3366/ijhac.2013.0081', 'id_scheme': 'DOI'}"," In its widest sense, infrastructure allows us as finite individuals to achieve beyond our individual capacity to know, to do, to see. But even within the narrower context of research infrastructures, broad and diverse definitions exist of how such an infrastructure should deliver these enhancements in knowledge and perspective. The Collaborative EuropeaN Digital Archival Research Infrastructure (CENDARI) project was launched in 2012 to address some of the gaps in provision for digital historical research, building on a long tradition of work in libraries, archives, digital humanities research centres and other research infrastructures. What distinguishes CENDARI, however, is its focus on what the project team has identified as the ‘grand challenges’ for each of its contributing stakeholder groups: collections experts based in libraries and archives, historians of the medieval and modern periods, and e-Scientists. This ethos, combined with its close relationship at European level to DARIAH, the Digital Research Infrastructure for the Arts and Humanities, has instigated a unique and fruitful approach to supporting historical research with digital resources, tools and spaces. ","CENDARI's Grand Challenges: Building, Contextualising and Sustaining a New Knowledge Infrastructure"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2013,,International Journal of Humanities and Arts Computing,7,1-2,"[{'value': None, 'type': None}]"
1750,https://doi.org/10.3366/ijhac.2013.0082,"{'string_id': '10.3366/ijhac.2013.0082', 'id_scheme': 'DOI'}"," Archaeologists regularly deal with large and diverse data sets, which are frequently the only record of excavated and destroyed archaeological sites. They need digital research infrastructures to preserve and provide access to this fragile digital data and to develop tools to manipulate and analyse it. Digital data is also increasing in quantity and size, and is often born digital, but there is a high degree of fragmentation and difficulty of accessing data in an integrated way. There is a small but growing number of national discipline-specific research infrastructures, and there have been a few project based attempts to provide European exemplars. This paper describes ARIADNE, a new research infrastructure for archaeologists, funded by the European Commission. It discusses some of the major issues and challenges and introduces some of the activities that ARIADNE partners will undertake. Given the transnational nature of many archaeological research questions, there is a clear value-added for organising research infrastructures in archaeology at a European level. ",ARIADNE: Advanced Research Infrastructures for Archaeological Dataset Networking in Europe," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2013,,International Journal of Humanities and Arts Computing,7,1-2,"[{'value': None, 'type': None}]"
1751,https://doi.org/10.3366/ijhac.2013.0083,"{'string_id': '10.3366/ijhac.2013.0083', 'id_scheme': 'DOI'}"," CLARIN is a recently-established research infrastructure which aims to build and sustain services based on language resources and tools. CLARIN aims to support and foster the next generation of research in the humanities, which will make use of advanced digital technologies. A distributed infrastructure is necessary in order to overcome the problems of the current fragmented environment, to create an ecosystem in which data and tools can be connected, and in which innovation will be encouraged. Case studies of early CLARIN demonstrators give a flavour of the possibilities of digital transformations in a number of humanities disciplines, and there is huge potential for important future new directions in literary and linguistic computing. For more widespread, thoroughgoing and effective transformations to take place, builders of infrastructure and researchers will need to negotiate and avoid potential pitfalls, and agree to achieve a certain measure of consensus in terms of priorities, categories and concepts. In the context of current debates about the nature of the humanities and their role in society, it will be necessary for digital humanists to be careful to preserve the unique character and importance of research in the humanities, while moving towards research infrastructures which will facilitate digital scholarship. ",The Role of CLARIN in Digital Transformations in the Humanities," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2013,,International Journal of Humanities and Arts Computing,7,1-2,"[{'value': None, 'type': None}]"
1752,https://doi.org/10.3366/ijhac.2013.0084,"{'string_id': '10.3366/ijhac.2013.0084', 'id_scheme': 'DOI'}"," The need for a firm understanding of the working practices of researchers in the humanities emerges as a prerequisite for the development of effective digital research infrastructures. This paper will focus on the rationale behind the design and implementation of two related studies conducted in the context of two European e-Infrastructures projects, DARIAH and EHRI. Within DARIAH the challenge involved conducting, analysing and understanding research practices of arts and humanities researchers, a largely ill-defined community encompassing a wide spectrum of disciplines. Each of them deals with a variety of objects employing an extensive number of methods. In the context of EHRI, the challenge is slightly different, due to the involvement of a better-defined research community. Holocaust researchers share well-identified objects, common ground on methods, and handle similar setbacks. In this paper we discuss the approach adopted for designing and implementing qualitative user-centric studies aimed at capturing activities, methods, and types of information objects employed by researchers grounded in identified research goals and questions. It addresses both generic and specific entities and processes, and supports the understanding of researchers’ working practices in settings as diverse and wide as DARIAH, or as specialized as EHRI. The outcomes of the analysis of working practices are used in determining user requirements for digital infrastructures to serve the respective research communities. ",An Approach to Analyzing Working Practices of Research Communities in the Humanities," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2013,,International Journal of Humanities and Arts Computing,7,1-2,"[{'value': None, 'type': None}]"
1753,https://doi.org/10.3366/ijhac.2013.0085,"{'string_id': '10.3366/ijhac.2013.0085', 'id_scheme': 'DOI'}"," This article offers a critique of the transfer of a technological-scientific paradigm of research infrastructure to the field of the humanities. This critique is informed by our experience of formulating user requirements for the European Holocaust Research Infrastructure (EHRI) project, and especially by a series of interviews we undertook with user-facing archivists working at EHRI partner institutions. We argue that the archival voices we recovered during these interviews articulate a range of concerns that clash with some of the major assumptions which frame current discussions about research infrastructure. In particular, we demonstrate that archival research is currently heavily mediated by archivists. And yet, inter-mediation is a theme that is insufficiently explored in recent theorising about research infrastructure. Contextualising our findings within some recent trends in archival science, we show that an infrastructure such as the EHRI must be build around the complex relationship between scholar, archivist and archive. We conclude by indicating how building infrastructures for humanities research may enable us to fruitfully re-conceptualise and re-energise this relationship by transposing it from the physical world to digital environments. ",The Missing Voice: Archivists and Infrastructures for Humanities Research," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2013,,International Journal of Humanities and Arts Computing,7,1-2,"[{'value': None, 'type': None}]"
1754,https://doi.org/10.3366/ijhac.2013.0086,"{'string_id': '10.3366/ijhac.2013.0086', 'id_scheme': 'DOI'}"," Crowd-sourcing, the process of leveraging public participation in or contribution to a project or activity, is relatively new to academic research, but is becoming increasingly important as the Web transforms collaboration and communication and blurs the boundaries between the academic and non-academic worlds. At the same time, digital research methods are entering the mainstream of humanities research, and there are a number of initiatives addressing the conceptualisation and construction of research infrastructures for the humanities. This paper examines the place of crowd-sourcing activities within such initiatives, presenting a framework for describing and analysing academic humanities crowd-sourcing, and using this framework of ‘primitives’ as a basis for exploring potential relationships between crowd-sourcing and humanities research infrastructures. ",Crowd-sourcing as a Component of Humanities Research Infrastructures," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2013,,International Journal of Humanities and Arts Computing,7,1-2,"[{'value': None, 'type': None}]"
1755,https://doi.org/10.3366/ijhac.2013.0088,"{'string_id': '10.3366/ijhac.2013.0088', 'id_scheme': 'DOI'}"," An interdisciplinary team of researchers were challenged to create a model of a deep map during a three-day charette at the NEH Institute on Spatial Narratives and Deep Maps. Through a reflexive process of ingesting data, probing for fruitful research questions, and considering how a deep map might be used by different audiences, we created a wireframe model of a deep map and explored how it related to spatial narratives. We explored the tension between interfaces for exploratory and structured views of data and sources, and devised a model for the intersections between spatial narratives and deep maps. The process of creating wireframes and prototype screens—and more importantly, the discussions and debates they initiated—helped us understand the complex requirements for deep maps and showed how a deep map can support a humanistic interpretation of the role of space in historical processes. ",Creating Deep Maps and Spatial Narratives through Design," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2013,,International Journal of Humanities and Arts Computing,7,1-2,"[{'value': None, 'type': None}]"
1756,https://doi.org/10.3366/ijhac.2013.0089,"{'string_id': '10.3366/ijhac.2013.0089', 'id_scheme': 'DOI'}"," This methodological paper sets out a definition of, and a vision for, a deep map. A deep map is defined as a platform and a means of expressing historical narratives, and seeks to move beyond the current situation, where time is presented as an attribute of space, and space as an attribute of text. It sets out a conceptual design of a deep map which brings all three together, allowing articulation of a narrative in the history of one denomination of US religion, the Unitarian church in Indianapolis in the nineteenth and twentieth centuries. ",A Religious Centre with a Civic Circumference: Towards the Concept of a Deep Map of American Religion," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2013,,International Journal of Humanities and Arts Computing,7,1-2,"[{'value': None, 'type': None}]"
1757,https://doi.org/10.3366/ijhac.2013.0090,"{'string_id': '10.3366/ijhac.2013.0090', 'id_scheme': 'DOI'}"," The spatial turn within the humanities and need for data richness has led to the re-conceptualisation and exploration of maps as ‘deep maps.’ Building narratives of place is becoming increasingly contingent on data landscapes as opposed to the physical landscapes within which they are situated. To make the assumption that GIS can form the basis for deep maps is to privilege the spatial dimension (and spatial data) over all others. We have sought in our experimentation to take a more open, balanced approach as to how a deep map might be organised as a way of learning/reflecting on what elements a framework should contain. Our subject matter here necessitated attention to the challenges and potentialities of deep mapping ‘things deemed religious.’ We found spatial navigation to be useful for visualizing physical and metaphysical linkages, integrating the geographical portions of our spatial narrative as well as organizing thoughts off the map. ",Envisioning Deep Maps: Exploring the Spatial Navigation Metaphor in Deep Mapping," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2013,,International Journal of Humanities and Arts Computing,7,1-2,"[{'value': None, 'type': None}]"
1758,https://doi.org/10.3366/ijhac.2014.0117,"{'string_id': '10.3366/ijhac.2014.0117', 'id_scheme': 'DOI'}"," A highly structured representation of materials have been largely neglected by historians as a way to explore historical sources in part because the textual nature of the sources often seems to preclude a structured representation. This paper proposes a place for them in the historian's toolkit and explores through a few examples how, as a way to formally express an historical interpretation of a body of material, they provide a mechanism to potentially enrich the exploration and development of an historian's interpretation of that material. Highly structured data exhibits a kind of classical clarity of approach that does not fit well with current postmodern and post-Enlightenment trends in the humanities, and this article touches on these issues, and suggests a few approaches to them. ",Silk Purses and Sow's Ears: Can Structured Data Deal with Historical Sources?," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2014,,International Journal of Humanities and Arts Computing,8,1,"[{'value': None, 'type': None}]"
1759,https://doi.org/10.3366/ijhac.2014.0118,"{'string_id': '10.3366/ijhac.2014.0118', 'id_scheme': 'DOI'}"," It is time to make a critical appraisal of the application of the Geographic Information System (GIS) to historical research. In this article I briefly review the promises and achievements of the use of this technology in history and I also consider some of its possible developments. Based on the experience of a project on the Iberian Peninsula, a special emphasis is put on transnational GIS. In the last section, the new avenues opened by GIS to historical research and the difficulties that its application also involves are considered in the light of the evolution of historiography and of the experience of using databases in history. I argue that although the application of this tool in historical research represented an important innovation, GIS did not bring about a revolution in knowledge production in history. I also raise doubts about the notion of a spatial turn in this field. I finally suggest that the emphasis should be put, not on the technology, but on the historical problems. ",Geographic Information Systems and Historical Research: An Appraisal," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2014,,International Journal of Humanities and Arts Computing,8,1,"[{'value': None, 'type': None}]"
1760,https://doi.org/10.3366/ijhac.2014.0119,"{'string_id': '10.3366/ijhac.2014.0119', 'id_scheme': 'DOI'}"," The Bentham Papers Transcription Initiative2 (Transcribe Bentham for short) is an award-winning crowdsourced manuscript transcription initiative which engages students, researchers, and the general public with the thought and life of the philosopher and reformer, Jeremy Bentham (1748–1832), by making available digital images of his manuscripts for anyone, anywhere in the world, to transcribe. Since its launch in September 2010, over 2.6 million words have been transcribed by volunteers. This paper will examine Transcribe Bentham's contribution to humanities research and the burgeoning field of digital humanities. It will then discuss the potential for the project's volunteers to make significant new discoveries among the vast Bentham Papers collection, and examine several examples of interesting material transcribed by volunteers thus far. We demonstrate here that a crowd-sourced initiative such as Transcribe Bentham can open up activities that were traditionally viewed as academic endeavors to a wider audience interested in history, whilst uncovering new, important historical primary source material. In addition, we see this as a switch in focus for those involved in digital humanities, highlighting the possibilities in using online and social media technologies for user engagement and participation in cultural heritage. ",Crowdsourcing Bentham: Beyond the Traditional Boundaries of Academic History," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2014,,International Journal of Humanities and Arts Computing,8,1,"[{'value': None, 'type': None}]"
1761,https://doi.org/10.3366/ijhac.2014.0120,"{'string_id': '10.3366/ijhac.2014.0120', 'id_scheme': 'DOI'}"," The CARDS-FLY project aims to collect and transcribe a diverse sample of historical personal letters from the 16th to 20th century in a digital format to create a linguistic resource for the historical study of the Portuguese language and society. The letters were written by people from all social layers of society and their historical, social and pragmatic contexts are documented in the digital format. Here we study one particular aspect of this collection, namely the spelling variation. Furthermore, on the basis of this analysis, we improved a statistical spelling normalisation tool that we aim to use to automatically normalise the spelling in the full collection of digitised letters. ",Manuscripts and Machines: The Automatic Replacement of Spelling Variants in a Portuguese Historical Corpus," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2014,,International Journal of Humanities and Arts Computing,8,1,"[{'value': None, 'type': None}]"
1762,https://doi.org/10.3366/ijhac.2014.0121,"{'string_id': '10.3366/ijhac.2014.0121', 'id_scheme': 'DOI'}"," This paper reports from the perspective of a historian who is investigating an early medieval manuscript, aiming at opening it up for further research and exploring its location in space, time, and intellectual context. The manuscript in question and the texts it carries show a complex, interwoven network of intra- and intertextual relations and the paper argues that only a combination, provided by computational means as the methodological key, of two usually distinct research approaches, namely close reading and distant reading, can deliver answers to the research questions imposed. The paper introduces some central methods of an interdisciplinary field, commonly known as digital humanities, in the realm of data representation (data modeling and text encoding) as well as core applications in the realm of data presentation and analysis (digital editing and visualization). As these supportive methods are neither the starting-point for historical research nor an end-in-itself, they are mirrored against scholarly practices of both, of the early Middle Ages and of modern scholarship. ",From the Scholarly Edition to Visualization: Re-using Encoded Data for Historical Research," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2014,,International Journal of Humanities and Arts Computing,8,1,"[{'value': None, 'type': None}]"
1763,https://doi.org/10.3366/ijhac.2014.0122,"{'string_id': '10.3366/ijhac.2014.0122', 'id_scheme': 'DOI'}"," The web is the primary means of communication in developed societies. It contains descriptions of recent events generated through distinct perspectives. Thus, the web is a valuable resource for contemporary historical research. However, its information is extremely ephemeral. Several research studies have shown that only a small amount of information remains available on the web for longer than one year. Web archiving aims to acquire, preserve and provide access to historical information published online. In April 2013, there were at least sixty four web archiving initiatives worldwide. Altogether, these archived collections of web documents form a comprehensive picture of our cultural, commercial, scientific and social history. Web archiving has also an important sociological impact because ordinary citizens are publishing personal information online without preservation concerns. In the future, web archives will probably be the only source of personal memories to many people. We provide some examples of tools that facilitate historical research over web archives highlighting their potential for Humanities. ",The Importance of Web Archives for Humanities," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2014,,International Journal of Humanities and Arts Computing,8,1,"[{'value': None, 'type': None}]"
1764,https://doi.org/10.3366/ijhac.2014.0127,"{'string_id': '10.3366/ijhac.2014.0127', 'id_scheme': 'DOI'}"," This paper uses Land Tax records to attempt to reconstruct the pattern of urbanization in a Pennine fringe township which formed part of the Lancashire cotton complex during the early industrial revolution. It uses logic programming to articulate rules to develop a longitudinal approach which chains together individual Land Tax records for successive years to identify perduring property objects, which are then located geographically using the pooled descriptors drawn from the returns. It investigates not only house repopulation, but also the character of new property development, of sub-division and amalgamation of holdings and the changing control of housing. It allows a remarkably detailed reconstruction of change in the particular locality, revealing events that have gone unnoticed. Pent-up demand associated with proto-industrialization combined with the self-interest of a major absentee landlord to allow a flurry of small scale construction between 1785 and 1805; property then converted to workers’ housing with the onset of industrial urbanism. More generally, it is suggested that a computational approach of this sort allows for a more serious engagement with a source all too often dismissed as unpromising. The paper concludes by drawing out implications of the work for more traditional approaches to interpreting Land Tax returns. ",Reconstructing Urbanization of a Pennine Fringe Township Through Computational Chaining of Land Tax Records: Mottram in Longdendale 1784–1830," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2014,,International Journal of Humanities and Arts Computing,8,2,"[{'value': None, 'type': None}]"
1765,https://doi.org/10.3366/ijhac.2014.0128,"{'string_id': '10.3366/ijhac.2014.0128', 'id_scheme': 'DOI'}"," Prime Minister Robert Peel was forced to resign in 1846 over the Repeal of the Corn Laws. Far from being a relatively unimportant piece of agricultural legislation, the Corn Laws, and their continuance, formed part of the ideology of the Conservative Party of the time. By proposing to Repeal the Corn Laws, Sir Robert was attacking the beliefs on which his party had won victory in the 1841 General Election. The result was a serious split within the Conservative Party over the Corn Laws. The majority of Conservatives voted against their own government, while 114 ‘Peelite’ Conservatives voted with Peel and the government. Why those particular Conservative Members decided to split away from their colleagues has been the subject of a large amount of research, mostly with ‘demand-side’ models which assume that the MP is little more than a mouthpiece for constituency interests. Peel's 1845 motion, a year before Repeal, to increase the yearly grant to the Irish Catholic seminary at Maynooth created very large controversy, and a backbench rebellion in which half of his own party voted against the government. As with Repeal, Maynooth passed only because the Opposition party decided to side with the government. This article uses principal component analysis and a classification tree analysis for the first time to show that while Conservative Members were voting with constituency interests in mind, their previous voting record over Maynooth is an overlooked and important predictor. ",The 1846 Repeal of the Corn Laws: Insights from a Classification Tree Approach," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2014,,International Journal of Humanities and Arts Computing,8,2,"[{'value': None, 'type': None}]"
1766,https://doi.org/10.3366/ijhac.2014.0130,"{'string_id': '10.3366/ijhac.2014.0130', 'id_scheme': 'DOI'}"," Linking historical census data is an important task for the study of the social, economic, and demographic aspects of families and society in the past. Although various (semi-) automatic linking methods have been proposed, state-of-the-art methods have only been targeted at linking records that correspond to individuals. In this paper, we introduce an automatic method aimed at linking both individuals and households across several historical census datasets. The proposed method contains several steps, including data quality analysis and enhancement, household identity detection, as well as individual and household record linking. We have applied this method to a set of six census datasets collected from the district of Rawtenstall in North-East Lancashire in the United Kingdom between 1851 and 1901. Experimental results show that the proposed method can greatly reduce the ambiguity arising from the individual record linkage, and facilitate the accurate matching of households across several decades. ",Automatic Record Linkage of Individuals and Households in Historical Census Data," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2014,,International Journal of Humanities and Arts Computing,8,2,"[{'value': None, 'type': None}]"
1767,https://doi.org/10.3366/ijhac.2014.0131,"{'string_id': '10.3366/ijhac.2014.0131', 'id_scheme': 'DOI'}"," Throughout this paper, I argue for a reapplication of those theories set out by George Bornstein in Material Modernism. More specifically, I suggest that Bornstein's work should be considered in the context of the textual and literary constructs of the digital age. I begin with an account of those elements from Bornstein's argument that I consider to be of most relevance to this particular discourse, giving particular consideration to what he refers to as the ‘bibliographic code.’ I argue that this notion has gathered fresh momentum now that its potential has been enhanced through new forms of computer-based media. What the material modernists of the modernist movement sought to achieve with the material elements of their works, contemporary scholars and critics can seek to replicate and explore with greater clarity and creativity. The bibliographic code has gained new importance, as the degree by which it can be manipulated, I argue, has been extended significantly. ",The New Apparatus of Influence: Material Modernism in the Digital Age," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2014,,International Journal of Humanities and Arts Computing,8,2,"[{'value': None, 'type': None}]"
1768,https://doi.org/10.3366/ijhac.2015.0136,"{'string_id': '10.3366/ijhac.2015.0136', 'id_scheme': 'DOI'}"," Place name mentions in text may have more than one potential referent (e.g. Peru, the country vs. Peru, the city in Indiana). The Edinburgh Language Technology Group (LTG) has developed the Edinburgh Geoparser, a system that can automatically recognise place name mentions in text and disambiguate them with respect to a gazetteer. The recognition step is required to identify location mentions in a given piece of text. The subsequent disambiguation step, generally referred to as georesolution, grounds location mentions to their corresponding gazetteer entries with latitude and longitude values, for example, to visualise them on a map. Geoparsing is not only useful for mapping purposes but also for making document collections more accessible as it can provide additional metadata about the geographical content of documents. Combined with other information mined from text such as person names and date expressions, complex relations between such pieces of information can be identified. The Edinburgh Geoparser can be used with several gazetteers including Unlock and GeoNames to process a variety of input texts. The original version of the Geoparser was a demonstrator configured for modern text. Since then, it has been adapted to georeference historic and ancient text collections as well as modern-day newspaper text.1,2,3,4 Currently, the LTG is involved in three research projects applying the Geoparser to historical text collections of very different types and for a variety of end-user applications. This paper discusses the ways in which we have customised the Geoparser for specific datasets and applications relevant to each project. ",Adapting the Edinburgh Geoparser for Historical Georeferencing," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,1,"[{'value': None, 'type': None}]"
1769,https://doi.org/10.3366/ijhac.2015.0137,"{'string_id': '10.3366/ijhac.2015.0137', 'id_scheme': 'DOI'}"," The partnering of computer assisted qualitative data analysis (CAQDA) and GIS permits the analysis of voluminous historical documents and the incorporation of geographic perspectives to bring out spatial relationships that exist in the texts. The British Parliamentary investigations into sea fishing comprise a large body of evidence on the transformation of the trade, the exponential growth of the fish harvest, and the evolving recognition that over-fishing was underway. The use of CAQDA and GIS described in this article explores some 1,800 pages of testimony given by fisherman, scientific experts, and regulatory officials before the Parliamentary commissions of enquiry in 1863–1866 and 1893–1894. ","Digital Partnership: Combining Text Mining and GIS in a Spatial History of Sea Fishing in the United Kingdom, 1860 to 1900"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,1,"[{'value': None, 'type': None}]"
1770,https://doi.org/10.3366/ijhac.2015.0138,"{'string_id': '10.3366/ijhac.2015.0138', 'id_scheme': 'DOI'}"," This article argues that the study of literary representations of landscapes can be aided and enriched by the application of digital geographic technologies. As an example, the article focuses on the methods and preliminary findings of LITESCAPE.PT—Atlas of Literary Landscapes of Mainland Portugal, an on-going project that aims to study literary representations of mainland Portugal and to explore their connections with social and environmental realities both in the past and in the present. LITESCAPE.PT integrates traditional reading practices and ‘distant reading’ approaches, along with collaborative work, relational databases, and geographic information systems (GIS) in order to classify and analyse excerpts from 350 works of Portuguese literature according to a set of ecological, socioeconomic, temporal and cultural themes. As we argue herein this combination of qualitative and quantitative methods—itself a response to the difficulty of obtaining external funding—can lead to (a) increased productivity, (b) the pursuit of new research goals, and (c) the creation of new knowledge about natural and cultural history. As proof of concept, the article presents two initial outcomes of the LITESCAPE.PT project: a case study documenting the evolving literary geography of Lisbon and a case study exploring the representation of wolves in Portuguese literature. ",Exploring Literary Landscapes: From Texts to Spatiotemporal Analysis through Collaborative Work and GIS," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,1,"[{'value': None, 'type': None}]"
1771,https://doi.org/10.3366/ijhac.2015.0139,"{'string_id': '10.3366/ijhac.2015.0139', 'id_scheme': 'DOI'}"," New data sources, for example in the form of geotagged image libraries and digitised archives of historical text documents, provide us with new opportunities for exploring how place is described. Using a framework derived from work in human geography and information science, we illustrate how there is more to place than names and coordinates. Through a set of case studies we explore different aspects of the seemingly trivial query ‘mountains in the Alps’ addressing a range of issues including ambiguity, the use of vernacular names, ways in which concepts such as mountain are used in different locations and by different groups, approaches to automatically generating macro-maps in space and time and, finally, techniques allowing regions to be characterised and compared based on the terms used to describe them. The use of all these methods in combination allows us to come closer to a meaningful representation of place in the sense of human geography within the context of Geographic Information Science. However, our approaches focus on the naming of places and their material or perceivable properties, and there is still much work to do to properly represent place, and particularly sense of place. Nonetheless, we suggest that such approaches have considerable potential for those working in the digital humanities, and especially those concerned with contributing to a spatial turn therein. ",From Space to Place: Place-Based Explorations of Text," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,1,"[{'value': None, 'type': None}]"
1772,https://doi.org/10.3366/ijhac.2015.0140,"{'string_id': '10.3366/ijhac.2015.0140', 'id_scheme': 'DOI'}"," This paper discusses the development of digital intellectual and technological geographies showing spatial distributions of information and proposes to combine these with network representations of actors and documents relevant for the history knowledge exchange in Early Modern Europe. The amount of technical and fortification drawings that were copied throughout Europe and the New World and the different nature of networks in which they were exchanged raises the question whether they belonged to the Republic of Letters, as some authors claim. We argue that instead of trying to explain knowledge exchange in Early Modern Europe by focusing on The Republic of Letters as one entity consisting of scholars , it might be more useful to reconstruct the spatial distribution of actors and of (non-)textual documents in virtual networks of knowledge. Inspired by the term “deep maps” coined by David Bodenhamer, we will introduce the concept of “deep networks” and explore the requirements for their future development. Hereto, we focus on the representation of historical evidence and of uncertainties in analyses of intellectual and technological letters and drawings and hybrid combinations of these. ",Mapping Knowledge Exchange in Early Modern Europe: Intellectual and Technological Geographies and Network Representations," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,1,"[{'value': None, 'type': None}]"
1773,https://doi.org/10.3366/ijhac.2015.0141,"{'string_id': '10.3366/ijhac.2015.0141', 'id_scheme': 'DOI'}"," In this paper we provide some insights in Homer's Iliad from the perspective of social network analysis. We use the original text and other public available data to create a social network (i.e. a graph) that comprises of all actors in the Iliad together with their interactions. We present some visualizations of these data and discuss concepts like connectivity, connected components and groupings. Furthermore, we calculate some well-established metrics, coming from social network analysis in this social network and discuss the numerical results. These results indicate that the Iliadic network is a small-world network, rather dissasortative and relatively easy to disconnect. ",Homer's Iliad – A Social Network Analytic Approach," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,1,"[{'value': None, 'type': None}]"
1774,https://doi.org/10.3366/ijhac.2015.0145,"{'string_id': '10.3366/ijhac.2015.0145', 'id_scheme': 'DOI'}"," This essay seeks to critically conceptualize the term geocultural space and the emerging field of study with which it is associated by exploring the various ways in which such space is currently being mapped by researchers using digital humanities tools and methods. In drawing together intersecting interests in Geographic Information Systems and spatio-cultural narratives and experiences, this work defines an interdisciplinary field of research that is gathering momentum as geolocative technologies that shape and reshape the ways in which we perceive and experience the world become increasingly prevalent in academic life and in the cultural mainstream. ",Conceptualizing and Mapping Geocultural Space," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,2,"[{'value': None, 'type': None}]"
1775,https://doi.org/10.3366/ijhac.2015.0146,"{'string_id': '10.3366/ijhac.2015.0146', 'id_scheme': 'DOI'}"," As the study of cinema has increasingly turned to the examination of economic ebbs and industrial flows, rather than focussing its attention solely on the critical evaluation of the films themselves, new analytic techniques and tools have been adopted (and adapted) by film scholars. Key amongst these is the use of innovative visualization techniques that can assist in the understanding of the spatial and temporal features of film industry practices. However, like the cinema itself, visualization carries its own spatial and temporal dimension. This article explores some of the benefits and limitations that derive from the use of spatial visualization technologies in the field of cinema studies. In particular, this research presents a new holistic multivariate approach to spatio-temporal visualization for point based historical data. This method has been developed through extending the spatial presence in timeline graphics and through meaningful spatial classification and representation. ",Petal Diagrams: A New Technique for Mapping Historical Change in the Film Industry," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,2,"[{'value': None, 'type': None}]"
1776,https://doi.org/10.3366/ijhac.2015.0147,"{'string_id': '10.3366/ijhac.2015.0147', 'id_scheme': 'DOI'}"," In the field of cinema history, an increased interest in social experience and context has challenged the centrality of the film and the primacy of textual analysis. The ‘Early Cinema in Scotland, 1896–1927’ research project takes a contextual approach, using geo-database tools to facilitate collaboration. This article shows how spatially-enabled methods can also be mobilized to bring issues of representation back into a cinema history project. We argue that, when the films have not survived, their geographical descriptors as recorded by trade-press reviews and catalogues offer new avenues of analysis. The article argues that foregrounding location as a significant element in the film corpus creates a new point of interconnection between film text and context. The juxtapositions and divergences between the spatial patterns of film production and cinema exhibition are connected to pre-cinematic traditions of representation. The spatial distribution also sheds light on the differences between films made for local and international consumption, reflecting on Scotland's position in relation to discourses of modernity. ",Remote Locations: Early Scottish Scenic Films and Geo-databases," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,2,"[{'value': None, 'type': None}]"
1777,https://doi.org/10.3366/ijhac.2015.0148,"{'string_id': '10.3366/ijhac.2015.0148', 'id_scheme': 'DOI'}"," Using ArcGIS, this article maps the narrative locations of French cinema's box office successes and César du meilleur film winners against a self-consciously international version of prestige, the French submission for best foreign language film at the Oscars from 1976 (when the Césars began) to 2012. Mapping domestic consumption and prestige against the for-American-consumption vision of prestige and possible box office appeal will identify the settings that are associated with domestic and international locations of Frenchness. Do films that succeed at the box office connect themselves to France's main population centers—Paris, Lyon, Marseille, Lille—or to less-populated and economically vibrant regions, as with Bienvenue Chez les Ch'tis (2008) in Bergues? To what extent do prestige films seek out marginalized areas in which to set their stories, as in the Paris banlieues of La Haine (1996) or Sète in La Graine et le mullet (2008)? Do the films that France proposes to the Oscar voters address an imagined American preference for one part of France—Paris—over another, or do they turn to other, less globally-integrated locations? Where are the overlaps among these three categories? And where are the empty spaces that neither box office nor prestige address? This article will be a spatial history, drawing on Franco Moretti's ‘distant reading’ approach to groups of films to demonstrate the critical potential for mapping narrative locations as a way to conceive of the multiple nations—in this case France—that cinema imagines for its domestic and international audiences. ","Where is France in French Cinema, 1976–2013?"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,2,"[{'value': None, 'type': None}]"
1778,https://doi.org/10.3366/ijhac.2015.0149,"{'string_id': '10.3366/ijhac.2015.0149', 'id_scheme': 'DOI'}"," This article presents methodological starting points, heuristics and the results of a GIS-based analysis of the history of Slovenian literary culture from the 1780s to 1941. The ethnically Slovenian territory was multilingual and multicultural; it belonged to different state entities with distant capitals, which was reflected in the spatial dynamic of literary culture. The research results have confirmed the hypotheses of the research project ‘The Space of Slovenian Literary Culture,’ which were based on postulates of the spatial turn: the socio-geographical space influenced the development of literature and its media, whereas literature itself, through its discourse, practices and institutions, had a reciprocal influence on the apprehension and structuring of that space, as well as on its connection with the broader region. Slovenian literary discourse was able to manifest itself in public predominantly through the history of spatial factors: (a) the formation, territorial expansion and concentration of the social network of literary actors and media; (b) the persistent references of literary texts to places that were recognized by addressees as Slovenian, thereby grounding a national ideology. Taking all of this into account, and based on meta-theoretical reflection, the project aims to contribute to the development of digital humanities and spatial literary studies. ",Towards a GIS Analysis of Literary Cultures: The Making of the Slovenian Ethnoscape Through Literature," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,2,"[{'value': None, 'type': None}]"
1779,https://doi.org/10.3366/ijhac.2015.0150,"{'string_id': '10.3366/ijhac.2015.0150', 'id_scheme': 'DOI'}"," Situated in the wake of the first and second waves of the Digital Humanities, the Digital Literary Atlas of Ireland, 1922–1949 website provides interactive mapping and timeline features for academics and members of the public who are interested in the intersection of Irish literary culture, history, and environment. The site hosts Google Earth software produced interfaces with the EXHIBIT Timeline functions made available by the Semantic Interoperability of Metadata and Information in unLike Environments (SIMILE) project, developed and hosted by the Massachusetts Institute of Technology's (MIT) Computer Science and Artificial Intelligence Laboratory (CSAIL) and Library. This paper's case study maps the biographical lifepath of the writer Samuel Beckett using digital humanities techniques such as ergodicity, and deformance. The geo-digital-timeline mapping of his biography allows us to visualize the shift in Beckett's literary perspective from a latent Cartesian verisimilitude to more phenomenological and fragmented, existential impressions of time and place. The atlas's visualizations of his Wanderjahre years in various European metropoles chart the intellectual and aesthetic influences shaping the Beckettian literary landscapes of his later and better-known works, such as En Attendant Godot (1953). Beckett's thought, works, and shifts in perception provide insight into how digital cultural mapping practices and third wave digital humanities methodologies and tools can be conceptualized and operationalized. ","Acts of Perception: Samuel Becket, Time, Space and the Digital Literary Atlas of Ireland, 1922–1949"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,2,"[{'value': None, 'type': None}]"
1780,https://doi.org/10.3366/ijhac.2015.0151,"{'string_id': '10.3366/ijhac.2015.0151', 'id_scheme': 'DOI'}"," This paper explores the limits of spatial representation for understanding historic mobilities in a rural Australian setting. For this research, an historical GIS was populated from paper map archives denoting where and when bitumen roads were sealed in the Bega Valley, NSW. Using existing geospatial methods, a temporally sensitive network analysis was conducted, revealing a picture of regional mobility reshaped by modernist infrastructure improvements. Yet a straightforward binary pitting sealed roads as ‘good’ vs unsealed roads as ‘bad’ was challenged in subsequent qualitative interviews with long-time residents. Instead, a range of opinions emerged about the role that differing road surfaces played in everyday and historic mobilities. A fuller picture of the motivations and cultural associations of bitumen vs dirt road driving resulted from deploying humanities research methods of interviewing and discourse analysis. Such an approach revealed preferences and motivations that sometimes challenged inferences gained from mapping results alone. There are limits then in relying solely on historical cartographic data to produce maps and spatial representations and existing humanities techniques may hold, in some instances, greater explanatory power. ",Maps and Mobilities: On the Possibilities and Limits of Spatial Technologies for Humanities Research," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2015,,International Journal of Humanities and Arts Computing,9,2,"[{'value': None, 'type': None}]"
1781,https://doi.org/10.3366/ijhac.2016.0156,"{'string_id': '10.3366/ijhac.2016.0156', 'id_scheme': 'DOI'}"," At the heart of the emergence and development of the Digital Humanities has been the potential to move beyond the out-dated epistemological and metaphysical dichotomies of the later 20th century including quantitative-qualitative, pure-applied, and campus-community. Despite significant steps forward, this potential has been only partially realized as illustrated by DH pioneer Edward L. Ayers’ recent question, ‘Does Digital Scholarship have a future?’ As a way to think through current challenges and opportunities, this paper reflects on the building and initial use of the Canadian Century Research Infrastructure (CCRI). As one of the largest projects in the history of the social sciences and humanities, CCRI enables research on the making of modern Canada by offering complex databases that cover the first half of the twentieth century. Built by scholars from multiple disciplines from coast-to-coast and in collaboration with government agencies and the private sector, CCRI team members came to grips with key DH questions especially those faced by interdisciplinary, multi-institutional, cross-sectoral and internationally-connected initiatives. Thinking through this experience does not generate simple recipes or lessons-learned but does offer promising practices as well as new questions for future scholarly consideration. ","Mindset and Guidelines: Insights to Enhance Collaborative, Campus-wide, Cross-sectoral Digital Humanities Initiatives"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2016,,International Journal of Humanities and Arts Computing,10,1,"[{'value': None, 'type': None}]"
1782,https://doi.org/10.3366/ijhac.2016.0157,"{'string_id': '10.3366/ijhac.2016.0157', 'id_scheme': 'DOI'}"," Scholars have long been interested in networks. Networks of scholarly exchange, trade, kinship, and patronage are some of the many such longstanding subjects of study. Recent and ongoing digital humanities projects are now considering networks with fresh approaches and increasingly complex datasets. At the heart of these digital projects are ‘network ontologies’ — functional data models for distilling the complicated, messy connections between historical people, objects, and places. Although scholars creating network ontologies necessarily focus on different types of content, if these networks are to form a coherent body of scholarship in the future, we must work towards the creation interoperable ontological structures, rather than yet another set of competing standards. Here we examine the methodological considerations behind designing such interoperable ontologies, focusing primarily on the example of Early Modern historical networks. We argue that it would be infeasible to adopt a single ontological standard for all possible digital humanities projects; flexibility is essential to accommodate all subjects and objects of humanistic enquiry, from the micro-level to the longue-durée. However, we believe it possible to establish shared practices to structure these network ontologies on an ongoing basis in order to ensure their long-term interoperability. ",Towards Interoperable Network Ontologies for the Digital Humanities," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2016,,International Journal of Humanities and Arts Computing,10,1,"[{'value': None, 'type': None}]"
1783,https://doi.org/10.3366/ijhac.2016.0158,"{'string_id': '10.3366/ijhac.2016.0158', 'id_scheme': 'DOI'}"," In Fall 2013, the Council on Library and Information Resources (CLIR), funded by the Andrew W. Mellon Foundation, engaged five postdoctoral fellows placed in five different institutions to explore issues related to data curation for medieval studies. In May 2015, these fellows convened a two-day workshop on the sharing and publishing of Linked Open Data (LOD). Funded by a CLIR/Mellon microgrant, the workshop brought together librarians, technologists, and scholars to brainstorm on the challenges posed to medievalists in sharing data on digital platforms.2 The workshop offered a forum in which to discuss the complexity of medieval data and the challenges of sharing and publishing it. It enabled participants to appreciate LOD's potential to express complicated data sets in our area of study and aid the navigation of those data sets, as well as understand how LOD can facilitate scholars to share and publish research outcomes more effectively. In this article, we take the lessons learned from the workshop and apply them to a set of complex data: 13th-century French motets, short pieces of music usually consisting of three lines and incorporating manifold connections and references. Following an outline of LOD, a detailed explanation of the motet and the manner of its composition will set the scene for elucidating the levels of complexity to be found in motet metadata, and hence why the LOD model can aid us in negotiating the data. We will then demonstrate an effective application of LOD by proposing a proof-of-concept system for organizing a select set of motets. ",Medieval Music in Linked Open Data: A Case Study on Linking Medieval Motets," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2016,,International Journal of Humanities and Arts Computing,10,1,"[{'value': None, 'type': None}]"
1784,https://doi.org/10.3366/ijhac.2016.0159,"{'string_id': '10.3366/ijhac.2016.0159', 'id_scheme': 'DOI'}"," This article will explore advances in the field of educational data modeling that have implications for modeling humanistic data. Traditional humanistic inquiry, bolstered by micro-analyses conducted by the scholar, has made way for machine-assisted methods that parse and quantify large amounts of qualitative data to reveal possible trends and focus more analogue approaches. At best, this play between human- and machine-directed approaches can lead to more profound explorations of texts. In this exploration of qualitative-quantitative methodologies that leverage human agency and machine-directed techniques, I suggest a mixed methods approach for dealing with the humanities. Specifically, this discussion will analyze the current methodological tensions related to Educational Data Mining and Learning Analytics to reveal best practices for modeling humanistic data. Principle questions of interest in this essay include: What defines literary “big” data? How can we define DH modeling and where does it depart from traditional data modeling? What role does machine-based modeling have in the context of the scholarly close read? What can we learn from educational data modeling practices that are in the midst of resolving tensions between human-machine patterning? ",Modeling the Humanities: Data Lessons from the World of Education," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2016,,International Journal of Humanities and Arts Computing,10,1,"[{'value': None, 'type': None}]"
1785,https://doi.org/10.3366/ijhac.2016.0160,"{'string_id': '10.3366/ijhac.2016.0160', 'id_scheme': 'DOI'}"," Future historians will describe the rise of the World Wide Web as the turning point of their academic profession. As a matter of fact, thanks to an unprecedented amount of digitization projects and to the preservation of born-digital sources, for the first time they have at their disposal a gigantic collection of traces of our past. However, to understand trends and obtain useful insights from these very large amounts of data, historians will need more and more fine-grained techniques. This will be especially true if their objective will turn to hypothesis-testing studies, in order to build arguments by employing their deep in-domain expertise. For this reason, we focus our paper on a set of computational techniques, namely semi-supervised computational methods, which could potentially provide us with a methodological turning point for this change. As a matter of fact these approaches, due to their potential of affirming themselves as both knowledge and data driven at the same time, could become a solid alternative to some of the today most employed unsupervised techniques. However, historians who intend to employ them as evidences for supporting a claim, have to use computational methods not anymore as black boxes but as a series of well known methodological approaches. For this reason, we believe that if developing computational skills will be important for them, a solid background knowledge on the most important data analysis and results evaluation procedures will become far more capital. ",Semi-supervised Textual Analysis and Historical Research Helping Each Other: Some Thoughts and Observations," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2016,,International Journal of Humanities and Arts Computing,10,1,"[{'value': None, 'type': None}]"
1786,https://doi.org/10.3366/ijhac.2016.0161,"{'string_id': '10.3366/ijhac.2016.0161', 'id_scheme': 'DOI'}"," Contemporary and future historians need to grapple with and confront the challenges posed by web archives. These large collections of material, accessed either through the Internet Archive's Wayback Machine or through other computational methods, represent both a challenge and an opportunity to historians. Through these collections, we have the potential to access the voices of millions of non-elite individuals (recognizing of course the cleavages in both Web access as well as method of access). To put this in perspective, the Old Bailey Online currently describes its monumental holdings of 197,745 trials between 1674 and 1913 as the “largest body of texts detailing the lives of non-elite people ever published.” GeoCities.com, a platform for everyday web publishing in the mid-to-late 1990s and early 2000s, amounted to over thirty-eight million individual webpages. Historians will have access, in some form, to millions of pages: written by everyday people of various classes, genders, ethnicities, and ages. While the Web was not a perfect democracy by any means – it was and is unevenly accessed across each of those categories – this still represents a massive collection of non-elite speech. Yet a figure like thirty-eight million webpages is both a blessing and a curse. We cannot read every website, and must instead rely upon discovery tools to find the information that we need. Yet these tools largely do not exist for web archives, or are in a very early state of development: what will they look like? What information do historians want to access? We cannot simply map over web tools optimized for discovering current information through online searches or metadata analysis. We need to find information that mattered at the time, to diverse and very large communities. Furthermore, web pages cannot be viewed in isolation, outside of the networks that they inhabited. In theory, amongst corpuses of millions of pages, researchers can find whatever they want to confirm. The trick is situating it into a larger social and cultural context: is it representative? Unique? In this paper, “Lost in the Infinite Archive,” I explore what the future of digital methods for historians will be when they need to explore web archives. Historical research of periods beginning in the mid-1990s will need to use web archives, and right now we are not ready. This article draws on first-hand research with the Internet Archive and Archive-It web archiving teams. It draws upon three exhaustive datasets: the large Web ARChive (WARC) files that make up Wide Web Scrapes of the Web; the metadata-intensive WAT files that provide networked contextual information; and the lifted-straight-from-the-web guerilla archives generated by groups like Archive Team. Through these case studies, we can see – hands-on – what richness and potentials lie in these new cultural records, and what approaches we may need to adopt. It helps underscore the need to have humanists involved at this early, crucial stage. ",Lost in the Infinite Archive: The Promise and Pitfalls of Web Archives," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2016,,International Journal of Humanities and Arts Computing,10,1,"[{'value': None, 'type': None}]"
1787,https://doi.org/10.3366/ijhac.2016.0162,"{'string_id': '10.3366/ijhac.2016.0162', 'id_scheme': 'DOI'}"," While intellectual property protections effectively frame digital humanities text mining as a field primarily for the study of the nineteenth century, the Internet offers an intriguing object of study for humanists working in later periods. As a complex data source, the World Wide Web presents its own methodological challenges for digital humanists, but lessons learned from projects studying large nineteenth century corpora offer helpful starting points. Complicating matters further, legal and ethical questions surrounding web scraping, or the practice of large scale data retrieval over the Internet, will require humanists to frame their research to distinguish it from commercial and malicious activities. This essay reviews relevant research in the digital humanities and new media studies in order to show how web scraping might contribute to humanities research questions. In addition to recommendations for addressing the complex concerns surrounding web scraping this essay also provides a basic overview of the process and some recommendations for resources. ",The World Wide Web as Complex Data Set: Expanding the Digital Humanities into the Twentieth Century and Beyond through Internet Research," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2016,,International Journal of Humanities and Arts Computing,10,1,"[{'value': None, 'type': None}]"
1788,https://doi.org/10.3366/ijhac.2016.0163,"{'string_id': '10.3366/ijhac.2016.0163', 'id_scheme': 'DOI'}"," Computational analysis and digital humanities are far from neutral processes and sites unimpeded by the political, social and economic context in which they emerged and are utilized. As an interdisciplinary field, the digital humanities have transformed the relationship of humans to computers broadly conceived. At the same time, the methods, theories, perspectives and the concomitant digital tools developed are being criticized for reproducing the social divisions that exist in society. The effort to recover Black women's subjectivities from the digital minefield is not without its challenges, reflected in our study which searched approximately 800,000 books, newspapers, and articles in the HathiTrust and JSTOR Digital Libraries. The goal was to identify perceptions and lived experiences of Black women that emerged and the resulting knowledge that developed. The project team discovered multiple challenges related to the rescue and recovery of Black women's standpoints or group knowledge. This essay explores how even as computational analysis has embedded biases, it can be utilized to recover the experiences of Black women from within the digitized record. Thus, computational analysis and all that it encompasses not only makes visible Black women's experiences, but also expands the scope of the digital humanities. ",Mechanized Margin to Digitized Center: Black Feminism's Contributions to Combatting Erasure within the Digital Humanities," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2016,,International Journal of Humanities and Arts Computing,10,1,"[{'value': None, 'type': None}]"
1789,https://doi.org/10.3366/ijhac.2016.0164,"{'string_id': '10.3366/ijhac.2016.0164', 'id_scheme': 'DOI'}"," This paper discusses issues concerning the creation of conversion tables for East Asian (Chinese, Japanese, Korean) and European calendars and describes the development of an open source calendar database as part of the history of converting East Asian calendars. East Asian calendars encode both astronomical and political cycles. As a result, date conversion must in practice rely on complex look-up tables and cannot be done merely algorithmically. We provide a detailed overview of the history of such conversion tables and find that the modelling of these tables into the digital follows a trend of increasingly detailed computation of chronological time. The Buddhist Studies Time Authority Database was designed to allow computational conversion of the Chinese, the Japanese and the Korean Calendar, between each other, as well as with the Gregorian, proleptic Gregorian and Julian calendar. It relies on the Julian Day Number (JDN) as common referent for all conversions. ",Modelling East Asian Calendars in an Open Source Authority Database," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2016,,International Journal of Humanities and Arts Computing,10,2,"[{'value': None, 'type': None}]"
1790,https://doi.org/10.3366/ijhac.2016.0165,"{'string_id': '10.3366/ijhac.2016.0165', 'id_scheme': 'DOI'}"," Andean weavings show patterns with intersecting warps in the textile's top view. These cannot be represented as a rectangular grid-like structure (rectilinear weave). A systematic description is essential for enabling weavers to document and secure intellectual property rights, and for preserving their rich cultural heritage. We present a linked data structure for modelling non-rectilinear as well as rectilinear weaving patterns, different from both a grid and a topological representation. We have implemented two editors using the same data structure for two classes of users (non-weaver users and weavers of warp-faced textiles). We describe the data structure and compare the operator sets of the two editors to access it. We give a first evaluation of the editors by showing how they can be used to model the complex structures of real-world textiles. ",Modelling Complex Non-Rectilinear Textile Structures," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2016,,International Journal of Humanities and Arts Computing,10,2,"[{'value': None, 'type': None}]"
1791,https://doi.org/10.3366/ijhac.2016.0169,"{'string_id': '10.3366/ijhac.2016.0169', 'id_scheme': 'DOI'}"," This article is an exploratory study using a smallholding diary and GIS to project the spatio-temporal pattern of a smallholding's travel in late nineteenth century Sweden. Through time-series of smallholding's daily diary notes, we develop an understanding for how smallholders adjusted to and participated in Swedish government policy, which resulted in the process termed ‘agrarian revolution’. Between 1872–1892, Tomtas Smallholding altered its spatio-temporal patterns from seasonal travels to production according to market demands and along new lines of transportation like railways. While the smallholding also used railways to visit distant places, it was primarily their produce that travelled further unto international markets. This also influenced the smallholding demography, from an extended household towards a nuclear family. As market demands shifted from subsistence to dairy products, the smallholding contracted primarily female farm servants due to their specialisation in milking. ",Smallholding Travel in the Agrarian Revolution: Using a Farmer Diary to Map Spatio-temporal Patterns in Late Nineteenth Century Sweden," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2016,,International Journal of Humanities and Arts Computing,10,2,"[{'value': None, 'type': None}]"
1792,https://doi.org/10.3366/ijhac.2016.0170,"{'string_id': '10.3366/ijhac.2016.0170', 'id_scheme': 'DOI'}"," Historians and historical quantitative social scientists, motivated by a renewed interest in quantitative history and by sophisticated tools for digital infrastructure, are developing data repositories for global-scale and collaborative analysis. However, their archives have been slow to grow. This article is directed toward historians who are contemplating such projects. Repository development is very valuable. On the other hand, studies show that repository projects that rely upon voluntary contribution from numerous researchers seldom reach critical mass. Our surveys and our study of the Collaborative for Historical Information and Analysis Data Hoover Project confirm this assessment. We conclude that historical data repositories remain poorly aligned with present day scholarly practices and are unlikely to realize their promise until the social life of data becomes a part of the profession. Because we believe that this is possible we introduce four strategies, each one backed by a successful project, that will help to make data sharing a part of professional practice. These suggestions are: 1) hiring ‘data hoovers’ to solicit and curate data, 2) appealing to close-knit communities and networking their domain-specific archives, 3) rightsizing crowdsourcing tasks, and 4) incorporating peer review. ","Don't Just Build It, They Probably Won't Come: Data Sharing and the Social Life of Data in the Historical Quantitative Social Sciences"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2016,,International Journal of Humanities and Arts Computing,10,2,"[{'value': None, 'type': None}]"
1793,https://doi.org/10.3366/ijhac.2017.0176,"{'string_id': '10.3366/ijhac.2017.0176', 'id_scheme': 'DOI'}"," This project explores the power of lines to tell stories. For architects, lines are elemental devices: they catalyze and structure our ability to describe space. But can they describe time and experience? The following case study explores this question in depth while tracking the historical expansion of suburban life in San Antonio, Texas. The research focuses on five roads, re-imagining them as a series of concentric timelines that stretch from the city's historical center to its suburban periphery. To date the multi-disciplinary team—led by architect Ian Caine with collaboration from a historian and an urban geographer—has utilized three distinct media to represent the chronological growth of San Antonio: 1) a large two-dimensional timeline that the team exhibited in a museum gallery, 2) a video-based spatial-temporal narrative that simulated the experience of driving through the city, and 3) a web-based interactive timeline.1 This article establishes the merits of the first two approaches—which are both complete—before speculating about the potential of the web-based version to recast the timeline as a narrative device capable of illuminating the complex relationship between time, space and experience in the contemporary city. ","Inhabiting the Line: A Digital Chronology of Suburban Expansion for San Antonio, Texas"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2017,,International Journal of Humanities and Arts Computing,11,1,"[{'value': None, 'type': None}]"
1794,https://doi.org/10.3366/ijhac.2017.0177,"{'string_id': '10.3366/ijhac.2017.0177', 'id_scheme': 'DOI'}"," Ethnohistorians and other scholars have long noted how European colonial texts often concealed the presence and participation of indigenous peoples in New World conquests. This scholarship has examined how European sources (both texts and maps) have denied indigenous history, omitted indigenous presence, elided indigenous agency, and ignored indigenous spaces all while exaggerating their own power and importance. These works provide examples of colonial authors performing these erasures, often as a means to dispossess. What they lack, however, is a systematic means of identifying, locating, and measuring these silences in space and time. This article proposes a spatial history methodology which can make visible, as well as measurable and quantifiable the ways in which indigenous people and spaces have been erased by colonial narratives. It presents two methods for doing this. First, narrative analysis and geovisualization are used to deconstruct the imperial histories found in colonial European sources. Second it combines text with maps to tell a new (spatial) narrative of conquest. This new narrative reconstructs indigenous activity through a variety of digital maps, including ‘mood maps’, indigenous activity maps, and maps of indigenous aid. The resulting spatial narrative shows the Spanish conquest of Peru was never inevitable and was dependent on the constant aid of immense numbers of indigenous people. ",Peering Beyond the Imperial Gaze: Using Digital Tools to Construct a Spatial History of Conquest," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2017,,International Journal of Humanities and Arts Computing,11,1,"[{'value': None, 'type': None}]"
1795,https://doi.org/10.3366/ijhac.2017.0178,"{'string_id': '10.3366/ijhac.2017.0178', 'id_scheme': 'DOI'}"," Practicing cultural geography in a web-based digital medium has led to the possibility of resurrecting early-twentieth-century ideas about the primacy of space in organizing information. In this article I discuss the philosophical underpinnings that drove the creation of my digital monograph Enchanting the Desert. Different than a spatial narrative that associates data points with their attendant cartographic representations, I use space—and specifically landscape—to order my information. Spatial proximity bundles and orders items on its own, a priori giving meaning to things existing next to one another in a landscape. Using space as a way to forge topical relevancy appears to be an uncommon approach, but in fact is paramount for a more productive and holistic ecological practice. The digital medium of the interactive web application makes this spatial epistemology possible. ",Practicing Spatial Epistemology with Immanuel Kant's ‘Physical’ Classification," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2017,,International Journal of Humanities and Arts Computing,11,1,"[{'value': None, 'type': None}]"
1796,https://doi.org/10.3366/ijhac.2017.0179,"{'string_id': '10.3366/ijhac.2017.0179', 'id_scheme': 'DOI'}"," This article examines the ways that geohumanities approaches historical research aids in the study of environmental and urban history in one of the twentieth century's fastest growing American urban centers. It explores how San Jose typified the challenges of Silicon Valley's rapid urbanization and desire to chart a new form of industrialisation predicated on the ‘greenness’ of high-tech manufacturing and development. These issues are examined through a variety of mapping and GIS projects that seek to understand areas of cities threatened by natural hazards, to unveil the growth of cities over time, and how polluted areas introduced environmental hazards to social inequality. The article concludes that studies of urban areas cannot be separated from questions about the environment and its role in social justice, urban planning, and politics. ","Green Dreams, Toxic Legacies: Toward a Digital Political Ecology of Silicon Valley"," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2017,,International Journal of Humanities and Arts Computing,11,1,"[{'value': None, 'type': None}]"
1797,https://doi.org/10.3366/ijhac.2017.0180,"{'string_id': '10.3366/ijhac.2017.0180', 'id_scheme': 'DOI'}"," This paper showcases a counter-mapping project with the Tlingit and Tagish peoples of the circumpolar north. Engaging critically with the evolving theory and practice of deep mapping we reveal how collaborative water research designed to provide a framework for indigenous water legislation is expressed visually on a Google Earth platform. This aquatic counter-map, or as we call it, a deep chart, not only empowers the Tlingit and Tagish, but reclaims and revitalises critical cultural values, whilst simultaneously preserving linguistic and cultural memory in a digital form. More broadly, the deep chart is currently being brought into global water ethic debates as a visual ethno-cartographic example of an alternative relationship humans might have with water, paving the way for a new water consciousness. This is in direct contrast to current, dominant and sometimes destructive global water governance and water management models and practices. We showcase a participatory and aqua-centric deep charting paradigm that is evolving through Geographics Information Science (GIS) platforms in collaboration with the CTFN (Tlingit and Tagish) community and government. This research challenges the contemporary cartography of the dominant Anglo-American valuing of water. It also presents an alternative water vision that we argue can shift ‘terra-centric’ thinking towards ‘aqua-centric’ thinking. It further provides a blueprint for informing global water ethics debates. ",A Deep Chart (the Aqua-Face of Deep Mapping)," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2017,,International Journal of Humanities and Arts Computing,11,1,"[{'value': None, 'type': None}]"
1798,https://doi.org/10.3366/ijhac.2017.0181,"{'string_id': '10.3366/ijhac.2017.0181', 'id_scheme': 'DOI'}"," Connection to place is important to people's individual and collective identity, a fundamental dimension of humanity. Scholars can shape place-making, but only by guiding the public to their own sense of place rather than telling them how to do so. Scholars must also be aware of the limitations inherent to all representations of place, and work to foster active engagement through multiple avenues of exploration. This paper explains a collaborative project at the Colorado Springs Pioneers Museum that uses methods definitive of a digital geohumanities to bring past landscapes alive, making the spatial connections that narrate the shaping of the city. We developed a deep mapping project where museum visitors a) interface with web-delivered, time-enabled animated maps on standing gallery tables, b) compare historic photos with adjacent contemporary street views on those maps, and c) engage with other geovisualizations that portray historical and geographical change. Visitors also have the opportunity to use mobile devices to take walking tours throughout the city, and to provide their own content and directions for the project. This hybrid of scholarly-led, multi-media narratives blended with public participation and open navigation can be a template for other deep-mapping projects to create connections to place. ",‘The Story of Us’: Place-Making through Public Interaction with Digital GeoHumanities in Colorado Springs," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2017,,International Journal of Humanities and Arts Computing,11,1,"[{'value': None, 'type': None}]"
1799,https://doi.org/10.3366/ijhac.2017.0182,"{'string_id': '10.3366/ijhac.2017.0182', 'id_scheme': 'DOI'}"," American urban historians have begun to understand that digital mapping provides a potentially powerful tool to describe political power. There are now important projects that map change in the American city along a number of dimensions, including zoning, suburbanization, commercial development, transportation infrastructure, and especially segregation. Most projects use their visual sources to illustrate the material consequences of the policies of powerful agencies and dominant planning ‘regimes.’ As useful as these projects are, they often inadvertently imbue their visualizations with an aura of inevitability, and thereby present political power as a kind of static substance–possess this and you can remake the city to serve your interests. A new project called ‘Imagined San Francisco’ is motivated by a desire to expand upon this approach, treating visual material not only to illustrate outcomes, but also to interrogate historical processes, and using maps, plans, drawings, and photographs not only to show what did happen, but also what might have happened. By enabling users to layer a series of historical urban plans–with a special emphasis on unrealized plans–‘Imagined San Francisco’ presents the city not only as a series of material changes, but also as a contingent process and a battleground for political power. ",‘Imagined San Francisco’: Unpacking Urban Regime Theory through Interactive Cartography," Edinburgh University Press 22 George Square, Edinburgh EH8 9LF UK ",2017,,International Journal of Humanities and Arts Computing,11,1,"[{'value': None, 'type': None}]"
1800,https://doi.org/10.3366/ijhac.2017.0184,"{'string_id': '10.3366/ijhac.2017.0184', 'id_scheme': 'DOI'}"," This paper describes several aspects of a formal digital semantic model that expresses some issues presented by medieval charters. Surprisingly, perhaps, this model does not deal directly with a charter's text and is not mark-up based. Instead, it draws on the authors’ experience with the construction of three highly structured factoid-oriented prosopographical databases that drew heavily on charter sources, and that also did not explicitly contain a digital representation of the charter texts. The paper explains the way in which the structured data model thus derived differs from text-oriented approaches such as TEI/CEI work that has been done so far on charters. It presents a view on why this factoid-based model seems to capture more readily some of the complexity in the apparent meanings of the charters, and suggests that this is because it is also more likely to relate to a richer conception of the broader medieval world in which these charters were created than text-oriented work does. Finally, drawing on recent work on the ChartEx project, it explores how a combined approach, that takes the best of both text-markup and structured data modelling techniques, could evolve in the future. ",Exploring a Model for the Semantics of Medieval Legal Charters,Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1801,https://doi.org/10.3366/ijhac.2017.0185,"{'string_id': '10.3366/ijhac.2017.0185', 'id_scheme': 'DOI'}"," Germany, 1935: the engineer Konrad Zuse (1910–1995), in the living room of his Berlin house, devotes himself to the design and construction of a binary, programmable machine, the Z1, capable of processing data in a fast and efficient way. While building his machines, he also started to devise a conceptual and notational system for writing ‘programs’ to execute applications much more complex than the basic arithmetic calculations. He delved deep into the study of formal logic in order to work out his “computation plan”, the Plankalkül. Although the Plan Calculus didn't exercise much impact on German post-World War hardships, it displays all the traits currently recognized as standard features of modern programming languages. The aim of the present study is to highlight the general purpose and technical specifics of this language, its historical and scientific background, and the philosophical inspiration leading Konrad Zuse to employ the predicate logic in the formalization of the “computation projects” for his machines. ",Konrad Zuse and his Plankalkül: The Hope to Emerge from the Sleep of Sleeping Beauty,Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1802,https://doi.org/10.3366/ijhac.2017.0186,"{'string_id': '10.3366/ijhac.2017.0186', 'id_scheme': 'DOI'}"," The tithe files contain a large amount of agricultural production data, but have not been much used for econometric analysis. The data is the result of averaging and estimating, thus some doubt has been cast on its accuracy. This article shows that for eight counties in the southwest of England, some data from the tithe files is reasonably reliable and was recorded in a consistent manner. The article demonstrates how tithe files and other data can be geocoded with spatial information so that an analysis using Geographical Information Systems (GIS) may be performed. Three tests are carried out, showing that pastoral rents followed a ‘distance decay’ model; that reported arable yields were consistent with exogenous variables; and that arable rents were set with economic variables in mind. ","A Validation of England's Nineteenth Century Tithe Files, Using GIS as a Primary Tool",Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1803,https://doi.org/10.3366/ijhac.2017.0187,"{'string_id': '10.3366/ijhac.2017.0187', 'id_scheme': 'DOI'}"," This paper addresses the viability of two multivariate methods (Principal Components Analysis and Cluster Analysis) in verifying the disputed authorship of a famous Arabic religious book called (Nahjul-Balagha/ Peak of Eloquence). This book occupies an exceptional position in the history of the huge debates held between the two basic Islamic sectors: Sunni'e and Shia. Therefore, it represents a serious challenge to the viability of the multivariate techniques in resolving certain types of historical and sectarian conflicts and controversies. Furthermore, verifying the authorship of this book could be a good opportunity to find out whether there are certain quantitative techniques of attribution that hold for different languages such as English and Arabic. Function words have been targeted in this paper as possible indicators of the author's identity. Accordingly, a set of Arabic function words would be tested using WordSmith Tools (version 5). It turned out that the multivariate techniques are most likely robust for addressing the type of issues raised about Nahjul- Balagha. Besides, it appeared that the statistical patterns of function word usages are quite sensitive to genre in Arabic. ",Authorship Verification in Arabic Using Function Words: A Controversial Case Study of Imam Ali's Book Peak of Eloquence,Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1804,https://doi.org/10.3366/ijhac.2017.0190,"{'string_id': '10.3366/ijhac.2017.0190', 'id_scheme': 'DOI'}"," This contribution reflects on the value of plurality in the ‘network with a thousand entrances’ suggested by McCarty (http://goo.gl/H3HAfs), and others, in association with approaching time-honoured annotative and commentary practices of much-engaged texts. The question is how this approach aligns with tensions, today, surrounding the multiplicity of endeavour associated with modeling practices of annotation by practitioners of the digital humanities. Our work, hence, surveys annotative practice across its reflection in contemporary praxis, from the MIT annotation studio whitepaper (http://goo.gl/8NBdnf) through the work of the Open Annotation Collaboration (http://www.openannotation.org), and manifest in multiple tools facilitating annotation across the web up to and including widespread application in social knowledge creation suites like Wikipedia https://en.wikipedia.org/wiki/Web annotation) ",The Value of Plurality in ‘The Network with a Thousand Entrances’,Edinburgh University Press,2017,,International Journal of Humanities and Arts Computing,11,2,"[{'value': None, 'type': None}]"
1805,https://doi.org/10.3366/ijhac.2017.0191,"{'string_id': '10.3366/ijhac.2017.0191', 'id_scheme': 'DOI'}"," Pope's The Dunciad. A Heroic Poem (1728), Dunciad Variorum (1729) and Dunciad in Four Books (1743) foreground the modelling and critique of different modes of annotation: elements of the commentary are attributed to a variety of fictitious, fictionalised or actually existing annotators, and the shaping of the material is significantly engaged with questions of print technology and mise-en-page. This paper analyses the techniques by which Pope deploys annotation as part of the work's oppositional rhetoric, and argues that, in the hands of his authorised editor Warburton, this approach gave rise to annotation that worked against the long-term interests of the poem and its author. The essay concludes by drawing out some possible implications for the practice of digital annotation. ","Interpretation, Agency, Entropy: Annotating Pope's Dunciads",Edinburgh University Press,2017,,International Journal of Humanities and Arts Computing,11,2,"[{'value': None, 'type': None}]"
1806,https://doi.org/10.3366/ijhac.2017.0192,"{'string_id': '10.3366/ijhac.2017.0192', 'id_scheme': 'DOI'}"," Between 1490 and 1525 various German print shops produced a great number of rather short texts that were the subject of university lectures as fascicles for students. Wide margins and line spacing left room for hand-written notes taken during the lecture course. Many of such annotated copies allow us to assume that the annotations are not textual commentaries in the usual sense. Rather, they are examples of a media practice indicating the tension between orality, handwriting, and printed books, especially since both the oral lecture course and the individual notes represent processes of transmitting and storing knowledge in which the text that is being explained does not even play a central role. We may compare them to modern, frequently collaborative practices that pursue aims going beyond straightforward models of text explication, which may help us develop more precise descriptions as well as a deeper understanding of the hermeneutics of textual annotation. ",Annotation Between Formation And Information,Edinburgh University Press,2017,,International Journal of Humanities and Arts Computing,11,2,"[{'value': None, 'type': None}]"
1807,https://doi.org/10.3366/ijhac.2017.0193,"{'string_id': '10.3366/ijhac.2017.0193', 'id_scheme': 'DOI'}"," While most literary scholars wish to help readers understand literary texts by providing them with explanatory annotations, we want to go a step further and enable them, on the basis of structured information, to arrive at interpretations of their own. We therefore seek to establish a concept of explanatory annotation that is reader-oriented and combines hermeneutics with the opportunities provided by digital methods. In a first step, we are going to present a few examples of existing annotations that apparently do not take into account readerly needs. To us, they represent seven types of common problems in explanatory annotation. We then introduce a possible model of best practice which is based on categories and structured along the lines of the following questions: What kind(s) of annotations do improve text comprehension? Which contexts must be considered when annotating? Is it possible to develop a concept of the reader on the basis of annotations—and can, in turn, annotations address a particular kind of readership, i.e.: in how far can annotations be(come) individualised? ",Explanatory Annotation of Literary Texts and the Reader: Seven Types of Problems,Edinburgh University Press,2017,,International Journal of Humanities and Arts Computing,11,2,"[{'value': None, 'type': None}]"
1808,https://doi.org/10.3366/ijhac.2017.0194,"{'string_id': '10.3366/ijhac.2017.0194', 'id_scheme': 'DOI'}"," Interpretation is widely regarded as the core activity of literary studies. Still, the appropriate balance between the plurality and the limitation of possible interpretations is a non-trivial issue. Whereas it is sensible to accept that literary texts can generally have various meanings, it should not be possible to attribute any kind of meaning to a text. Therefore, while interpreters must be allowed to disagree in their analyses, it must at the same time be possible to review whether a disagreement is actually based on adequate reasons like, for example, textual ambiguity or polyvalence. In this paper, we propose a best practice model as one effective means to review disagreement in accordance with literary studies principles. The model has been developed during the collaborative, computer-assisted annotation of literary texts in a project in which short stories have been analyzed narratologically. The examination of inconsistently annotated text passages revealed four types of reasons for disagreement: misinterpretations, deficient definitions of the categories of analysis, dependencies of the relevant categories on preliminary analyses, and textual ambiguity/polyvalence. We argue that only disagreements based on textual ambiguity are considered legitimate or valuable cases of disagreement, whereas the other three types of disagreement can be resolved in a systematic way. ",The Hermeneutic Profit of Annotation: On Preventing and Fostering Disagreement in Literary Analysis,Edinburgh University Press,2017,,International Journal of Humanities and Arts Computing,11,2,"[{'value': None, 'type': None}]"
1809,https://doi.org/10.3366/ijhac.2017.0195,"{'string_id': '10.3366/ijhac.2017.0195', 'id_scheme': 'DOI'}"," Traditional historico-critical music editions provide scholars and musicians with an edited score based on the interpretation of various sources such as the composer's autograph(s), letters and other materials from copyists or publishers. Their digital counterparts have the potential to offer new and more expanded ways to explore the work. This is based on the possibility to provide large amounts of source materials and to annotate more extensively (since printing costs are irrelevant). Furthermore, audio and video recordings of performances can be integrated. But, similarly important, the user interface makes it much easier to navigate in the complex network of cross-references between various source materials and the editor's annotations. The XML-based Music Encoding Initiative (MEI) format is the standard music representation format for digital music editions. In this article we discuss current MEI-based annotation practices and outline the current challenges for music annotations, including a discussion of anchoring options, the embedding of addressable elements in the local musical context, the annotation of audio, and the categorisation of annotations. This leads to a discussion of open questions such as the ability to secure authorship in open and reusable editions. ",Challenges for Annotation Concepts in Music,Edinburgh University Press,2017,,International Journal of Humanities and Arts Computing,11,2,"[{'value': None, 'type': None}]"
1810,https://doi.org/10.3366/ijhac.2017.0196,"{'string_id': '10.3366/ijhac.2017.0196', 'id_scheme': 'DOI'}"," Annotation and research in the humanities are tightly coupled. Annotations can be seen as expressions of research activity which can be turned into input data for subsequent research. The digital paradigm has profoundly altered the ways in which we humans can handle the information content of our sources and it also affects the practice of annotation. We explore new ways of annotation that were not feasible before the digital times, and we list a few requirements for annotation to act as a reliable source of research information. Rather than conducting an academic discussion on the ontology of annotations, we highlight practical use cases for new kinds of annotations. We illustrate those in a concrete system for linguistic annotations to the Hebrew Bible, SHEBANQ. ",Practical Linguistic Annotation: The Hebrew Bible,Edinburgh University Press,2017,,International Journal of Humanities and Arts Computing,11,2,"[{'value': None, 'type': None}]"
1811,https://doi.org/10.3366/ijhac.2018.0202,"{'string_id': '10.3366/ijhac.2018.0202', 'id_scheme': 'DOI'}"," A central challenge in short-term study abroad programs is making sure that students have meaningful interactions with people abroad, something that scholarship has shown to be tremendously beneficial, both for linguistic gains and for cross-cultural exchange. This article examines the potential benefits of collaborative mapping for encouraging student interaction with local residents, while also considering the value of spatial analysis to the interdisciplinary aims of a course centered on theatre and French Studies. One author is a cultural anthropologist and professor of French, and the other is a professor of theatre. The course explores theatre as a window into French society and culture in three distinct geographic settings (Avignon and the rural Vaucluse, Marseille, and Paris). Space is important to its study of theatrical literature, of theatrical form and site-specific performance, and of the varied meanings of theatre to people living in diverse social environments, including rural settings and neighborhoods in urban centers and peripheries. Written prior to implementing the project for the 2017 version of the course, this article examines a mapping project intended to fully exploit the opportunities of study abroad for student engagement with other perspectives abroad, and for inter-disciplinary dialogue and exchange. ",Collaborative Mapping to Enhance Local Engagement and Interdisciplinary Dialogue in a Short-term Study Abroad Program,Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,1,"[{'value': None, 'type': None}]"
1812,https://doi.org/10.3366/ijhac.2018.0203,"{'string_id': '10.3366/ijhac.2018.0203', 'id_scheme': 'DOI'}"," While the humanities embraced the value of spatial analysis some time ago; political science has yet to marry traditional social science methods with the visual and interactive process of mapping. Some may consider mapping, or spatial analysis, as descriptive without much analytical power. This project argues for the introduction to mapping to a course in the social sciences. My colleague Paul Achter and I and teach a course on The Wire, a critically acclaimed series from David Simon aired on HBO, is a five-part series on the problems facing the City of Baltimore. The Wire is uniquely suited to ‘deep mapping’ because of the role of city spaces in every season. The City of Baltimore is a character as much as any of the human characters in the series. Initiating a class project on creating and developing a spatial map of The Wire locations as a way of understanding Simon's critique of contemporary political and social policy. Politics drive economic disparity and public and private decisions about the use of spaces. A deep map of The Wire could connect episodes spatially to reveal the relationship between location and opportunity, safety, justice, access to jobs, and quality of schools ",Watching and Mapping The Wire : Taking the Spatial Turn in Political Science,Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,1,"[{'value': None, 'type': None}]"
1813,https://doi.org/10.3366/ijhac.2018.0204,"{'string_id': '10.3366/ijhac.2018.0204', 'id_scheme': 'DOI'}"," As destination of choice for many short-term study abroad programs, Berlin offers students of German language, culture and history a number of sites richly layered with significance. The complexities of these sites and the competing narratives that surround them are difficult for students to grasp in a condensed period of time. Using approaches from the spatial humanities, this article offers a case study for enhancing student learning through the creation of digital maps and itineraries in a campus-based course for subsequent use during a three-week program in Berlin. In particular, the concept of deep mapping is discussed as a means of augmenting understanding of the city and its history from a narrative across time to a narrative across the physical space of the city. As itineraries, these course-based projects were replicated on site. In moving from the digital environment to the urban landscape, this article concludes by noting meanings uncovered and narratives formed as we moved through the physical space of the city. ",Emplacing Berlin: Digital Itineraries and Short-term Study Abroad in the City,Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,1,"[{'value': None, 'type': None}]"
1814,https://doi.org/10.3366/ijhac.2018.0205,"{'string_id': '10.3366/ijhac.2018.0205', 'id_scheme': 'DOI'}"," John Milton's 1644 declaration that ‘Books are not absolutely dead things’ makes him a rock star among undergraduate English majors who are covetous of the material, reassuringly physical book. This essay explores that metonymic dichotomy through a project that combined the ‘old’ technology of the hand-press book and the ‘new’ technology of GIS story-telling. Using a visiting special collection of rare books for students at a small college, the project approached hand-press era books in three phases: 1) a bibliographic description and transcription; 2) book forensics, and 3) a ‘deep map’ of a book. With mapping—understood as an expression of spatial thinking—as a guide, students recognized that the singular text, even the dialogic text, is far less remarkable than locating and articulating the links between history, place, literature, and culture. Students engaged with terminology (descriptive bibliography), recognized the temporal lines of the book as an object (provenance), followed the development of a book as a polyglotous intellectual entity, and reviewed the geographic/historical experiences of the author and of the book (biography, publishing). The spatial turn allowed students to construct (and in some cases, deconstruct) the cultural world in which texts, authors and printers collide. ","‘Books are not absolutely dead things’: English Literature, Material Culture and Mapping Text",Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,1,"[{'value': None, 'type': None}]"
1815,https://doi.org/10.3366/ijhac.2018.0206,"{'string_id': '10.3366/ijhac.2018.0206', 'id_scheme': 'DOI'}"," This essay articulates the concept of ‘critical chorography’ through a discussion of the productive intersection of pedagogy in the spatial humanities and map theory as applied to the visualization of early modern maps using contemporary GIS technology. In method, the essay lays out a comparative model for the analysis of maps across early modern and contemporary digital practices by theorizing student-driven practices of mapping first through a deep attention to historical location and documentary evidence, and second, through an inquiry into the digital implications of mapping jurisdictional conflicts through case studies focused on westward expansion in the West Indies. To those ends, the essay examines the drive in contemporary spatial humanities to use the map as a tool of analysis, rather than merely as a platform for visualization and presentation. ",Critical Chorography: A Pedagogical Approach,Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,1,"[{'value': None, 'type': None}]"
1816,https://doi.org/10.3366/ijhac.2018.0207,"{'string_id': '10.3366/ijhac.2018.0207', 'id_scheme': 'DOI'}"," Story Maps and Disability Studies: A Digital Blueprint for Teaching Community Engagement argues that the spatial humanities and disability studies can highlight for students how classroom discussions are connected to public discourse concerning issues of access. This article examines an interdisciplinary project that uses digital cartography to introduce students to concepts in disability studies in order to get them engaged with issues of social justice within their community. The assignment asks students to map the nexus of power relations that they find in their community using digital tools such as ARC GIS and story maps software. This article provides a history of the assignment along with the assignment's framework including the reading list and classroom activities. Story Maps and Disability Studies concludes with a critical reflection on the assignment's educational goals, student projects, and future revisions to the project. ",Story Maps and Disability Studies: A Digital Blueprint for Teaching Community Engagement,Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,1,"[{'value': None, 'type': None}]"
1817,https://doi.org/10.3366/ijhac.2018.0208,"{'string_id': '10.3366/ijhac.2018.0208', 'id_scheme': 'DOI'}"," This essay addresses drawbacks to short-term study abroad courses offered at small liberal-arts colleges, as well as difficulties students encounter in such courses. These difficulties arise due to the extremely full itineraries that the course structure may seek to provide, which can become overly burdensome to students in terms of what they are able to process fully, and what they retain during the course and after the course is over. One avenue of possible amelioration of this difficulty, while not challenging the opportunity to expose students to a wide range of sites and course-topical material, is a reliance on mapping. In terms of pre-travel, the map serves as orientation, a setting of expectations for what will be seen. During the course, while onsite, students are able to use the supplemental materials offered on the interactive map better to understand their surroundings. After the course, revisiting the map, along with their course notes, photos, and journals, provides an essential aide-mémoire. Choosing potential sites for extensive mapping based on those sites students are least familiar with is a way, then, to maximize the map's impact on learning outcomes. ","Mapping as Orientation, Support, and Aide-mémoire in Short-term Travel Courses",Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,1,"[{'value': None, 'type': None}]"
1818,https://doi.org/10.3366/ijhac.2018.0209,"{'string_id': '10.3366/ijhac.2018.0209', 'id_scheme': 'DOI'}"," High impact learning experiences hold forth great promise for enhancing the personal and intellectual development of students. First year experience abroad offers opportunities for students to explore a study abroad topic in depth at an early point in their academic career. Recently, significant numbers of faculty have begun to lead study abroad groups on the Camino de Santiago in Spain. Due to the increasing popularity of walking the Camino overall, overcrowding and other related issues are becoming a problem. I propose that faculty leading student groups on the Camino incorporate digital mapping tools as part of the curriculum to increase awareness among students about the potential impact they may have on local communities. Furthermore, the use of digital mapping tools to either complement or replace the more traditionally assigned journal will help the student meet the desired learning outcomes set forth by the first year experience abroad program—develop, experience and reflect. By understanding spatial relationships, students will ask questions and make connections that they otherwise would not have. ","Pilgrimage, Place, Pedagogy: How Mapping Tools can Enhance First Year Experience Abroad",Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,1,"[{'value': None, 'type': None}]"
1819,https://doi.org/10.3366/ijhac.2018.0217,"{'string_id': '10.3366/ijhac.2018.0217', 'id_scheme': 'DOI'}"," This article presents the technical characteristics of the Belgian STREAM-project (2015–2019). The goal of STREAM is to facilitate and innovate historical research into local and regional processes through the development of a spatiotemporal infrastructure for early modern Brabant and Flanders, two of the most urbanized and developed areas of pre-industrial Europe. To this end, STREAM systematically collects a range of key data from a diversity of historical sources to provide a geographically comprehensive and long-run quantitative and spatial account of early modern society at the local level (parishes, villages, towns) regarding territory, transport, demography, agriculture, industry and trade, related to the development of a tailored historical geographical information system (GIS) based on the well-known Ferraris map (1770–1778). This article discusses the possibilities and pitfalls of the data collection and the construction of a spatial infrastructure for the pre-statistical era. ","STREAM (Spatiotemporal Research Infrastructure for Early Modern Brabant and Flanders): Sources, Data and Methods",Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,2,"[{'value': None, 'type': None}]"
1820,https://doi.org/10.3366/ijhac.2018.0218,"{'string_id': '10.3366/ijhac.2018.0218', 'id_scheme': 'DOI'}"," Primary historical source materials represent a very real challenge to GIS. In contrast to modern datasets where tabulated and structured data usually correspond neatly to fixed boundary units, primary historical source materials occur in predominantly unstructured text format. In these instances, people, events, and places must be unraveled from handwritten scripts and, if possible, subsequently linked and coupled to some form of mapping. In this paper, a database solution to handling primary source material is discussed that enables the original text and its translation to be ingested into the system and for people, familial relations, events, and images to be linked and queried in real time. In particular, the system connects these databases via toponyms to a tightly coupled mapping interface. The system enables thesauri of people's names and places to be created on-the-fly and allows levels of confidence to be assigned by the investigator to these matches. Full database and toponymic search and linking capability is enabled. In addition, a mobile application supports investigation in the field to access details in the databases at the same time as ingesting additional data and information in real-time to the system. Through the use of this dynamic database coupling and union to a map interface it is proposed that the system provides a powerful platform for use with primary unstructured source material and contributes to a deeper mapping and understanding of historical place. ","Deepening Historical GIS: An Integrated Database Solution for Linking People, Place and Events through Unstructured Text",Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,2,"[{'value': None, 'type': None}]"
1821,https://doi.org/10.3366/ijhac.2018.0219,"{'string_id': '10.3366/ijhac.2018.0219', 'id_scheme': 'DOI'}"," This article serves a two-fold purpose: first, to give an overview of the project HGIS de las Indias, and second to ask for its relation with the epistemological currents in spatial humanities, thereby raising some fundamental thoughts and inquietudes about the relationship between projects designed for accommodating particular practical needs and overarching theory. In the first part, thus, we will sketch the initial impulse that made the project come to life; then, we will outline the basic structure, features, and general characteristics of the database and its web application, briefly commenting on how that database design correlates and conflicts with the ontologies of historical sources underlying the reconstruction process. In the second part, we will try to tackle the question of whether projects of such tailoring should have a place within the spatial humanities, which seem to have shifted or solidified their epistemological center in poststructuralist, postmodern, postcolonial, and neo-geographic discourses, advocating for a reflected employment of digital spatial tools focussing on their different roles within the humanities instead of enforcing a growing divide between a theoretical navel-gazing and an atheoretical practice. ",‘The Empire Strikes Back’ ?: HGIS de las Indias and the Postcolonial Death Star,Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,2,"[{'value': None, 'type': None}]"
1822,https://doi.org/10.3366/ijhac.2018.0220,"{'string_id': '10.3366/ijhac.2018.0220', 'id_scheme': 'DOI'}"," This article joins calls for literary scholarship to move beyond the limitations of binary oppositions between ‘close’ and ‘distant’ reading and towards the development of approaches that exploit the macroanalytic potential of digital methods alongside the nuanced analysis that characterises literary scholarship. Drawing on a customised corpus of writing about the English Lake District, we model the application of a multiscalar approach known as geographical text analysis (GTA), which combines aspects of close reading and distant reading, and, in doing so, introduces a new method for literary research. Here, we focus on historical descriptions of the Lake District's soundscape to demonstrate both how perceptions of sound changed over the course of the eighteenth and nineteenth centuries, and how multi-scalar methods can uniquely uncover such historical-literary shifts. Sound, we argue, offers a particularly useful focus since it allows us to draw fruitful parallels between our methods and those applied by the writers we study. In this way, this article advocates for digital humanities scholarship that advances our disciplines in conversation with appropriate historical modes. ",Combining Close and Distant Reading: A Multiscalar Analysis of the English Lake District's Historical Soundscape,Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,2,"[{'value': None, 'type': None}]"
1823,https://doi.org/10.3366/ijhac.2018.0221,"{'string_id': '10.3366/ijhac.2018.0221', 'id_scheme': 'DOI'}"," For three years now, a research group at the University of Santiago de Compostela has been working on the creation of literary maps of Compostela.1 In this application of Geographic Information Systems (GIS) to literature, with the focus on a city of remarkable symbolic relevance, problems emerged, mainly concerning the mappeability of uncertainty and the codification of recurrence and emotions. The point of departure was a very precise group of texts belonging to the subgenre of the ‘estudiantina’ that depict the life of Santiago University students at the end of the 19th and the beginning of the 20th century. In this paper, we intend to present the results, showing the achievements, the limits and the new challenges that emerged. Literary cartographies have proved useful for the study of symbolic production and for tourism. Less attention has been paid to its applicability to education, and yet, the tools used so far for creating literary cartographies can be applied to the study of literary texts produced by students, so that the resulting maps and graphics cast important information for the analysis of how students understand and deal with places, how they relate to the cities they live in and, thereby, reveal the constitution of their mental maps. In this paper, we will compare literary maps of Santiago obtained on the one hand from the database created with the ‘Estudiantina’ and, on the other, from the database of 40 texts written by current students. Taking the maps as a point of departure, we will reflect on the historical changes in the imaginary of the city and the way it is perceived, inhabited and lived by current students. ",The Use of GIS to Draw Literary Maps of Compostela. Applications in the Educational Field,Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,2,"[{'value': None, 'type': None}]"
1824,https://doi.org/10.3366/ijhac.2018.0222,"{'string_id': '10.3366/ijhac.2018.0222', 'id_scheme': 'DOI'}"," The abundance of information contained in nineteenth-century texts means the traditional ‘close reading’ of Victorian culture has limitations. With the burgeoning availability of newspapers in digital format, there is a pressing need to look at how we might effectively and efficiently use these digital resources to help answer research questions and add to key historical and geographical debates. Focusing on the analysis of a large digital corpus, this paper has two key foci: (I) to apply an innovative digital methodology, that combines corpus linguistics and geospatial technologies, to a very large corpus of newspaper texts and; (II) apply said methodology to a case study assessing the presentation of health and disease in a nineteenth-century newspaper. The paper illustrates that by linking existing techniques with new and innovative approaches it is possible to temporally and spatially analyse and map themes of interest in large digital corpora on a scale not possible through more traditional close reading methods. ",Space and Time in 100 Million Words: Health and Disease in a Nineteenth-century Newspaper,Edinburgh University Press,2018,,International Journal of Humanities and Arts Computing,12,2,"[{'value': None, 'type': None}]"
1825,https://doi.org/10.3366/ijhac.2019.0228,"{'string_id': '10.3366/ijhac.2019.0228', 'id_scheme': 'DOI'}"," ‘Qualitative spatial reasoning and representation’ is a range of techniques developed in Artificial Intelligence to meet the need for a computational treatment of qualitative spatial relations. Examples of such relations include ‘next to’, ‘overlapping’, ‘to the left of’, ‘separate from’, ‘including’, and so on. These relations occur within the data found in the spatial humanities, but the computational techniques described here do not appear to have been used in connection with this context. While Geographical Information Systems (GIS) are widely used as a means of visualizing and exploring material in the spatial humanities, GIS technology is acknowledged to be ill-suited to information that is vague, uncertain, ambiguous, imprecise or having other qualities that in a scientific setting could be regarded as imperfections. In the humanities such ‘imperfections’ are of course important, and qualitative spatial relations are one source of data that challenges scientifically based GIS. This article reviews the origin of qualitative spatial reasoning and representation in A. N. Whitehead's mereotopology and argues for exploring how these methods could complement GIS as a computational technique in the humanities. Qualitative representation is applicable to modelling spatial arrangements in many domains, not just geographical space. This is demonstrated through an example of spatial relations in lines of printed text. ",Qualitative Spatial Representation for the Humanities,Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1826,https://doi.org/10.3366/ijhac.2019.0229,"{'string_id': '10.3366/ijhac.2019.0229', 'id_scheme': 'DOI'}"," Techniques for extracting place names (toponyms) from texts and using them to conduct analyses of the geographies within the texts are becoming reasonably well established. These are generally referred to as Geographical Text Analysis (GTA) and allow us to ask questions about the geographies within a corpus. The limitation of this approach is that the geographies that can be uncovered are solely associated with toponyms for which a coordinate-based location can be assigned. While this method is valuable, it is effectively a quantitative representation of the geographies associated with named places. Other representations of geography are ignored. To complement GTA, we need to develop techniques that are capable of representing the more qualitative representations of geography that are found within texts. Drawing on the Corpus of Lake District Writing, this paper presents some initial ideas about how this can be achieved, primarily by using techniques from corpus linguistics. ",Qualitative Geographies in Digital Texts: Representing Historical Spatial Identities in The Lake District,Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1827,https://doi.org/10.3366/ijhac.2019.0230,"{'string_id': '10.3366/ijhac.2019.0230', 'id_scheme': 'DOI'}"," This article responds to the widespread and oft-noted challenges digital humanists face in working with data that is uncertain and characterised by complex narratives. Using an example drawn from the vast archives of post-war interviews with Holocaust survivors, it draws on approaches developed in Qualitative Spatial Representation (QSR) to explore how two key spatial aspects of survivor's narratives – their uncertain wartime trajectories and the slippage in scales as these are retold – can be represented. Spatial information in narratives tends to not provide the exact coordinates necessary to store the information in geospatial databases. Instead, narratives rely much more on often less precise qualitative spatial relations such as ‘near’, ‘next to’, ‘at the corner of’ without precise geometric interpretations. Given that relational databases are ill-equipped to store this kind of relational spatial knowledge from natural language sources, the article argues for a need for digital humanists to return to first principles and reconsider database design. Descriptive triple-based graph representations, which have been devised to accommodate this kind of highly irregular, semi-structured relational knowledge, have the potential to work with, rather than against, the grain of the narrative sources that underlie the work of much of the digital humanities. ","Geographies of the Holocaust: Experiments in GIS, QSR, and Graph Representations",Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1828,https://doi.org/10.3366/ijhac.2019.0231,"{'string_id': '10.3366/ijhac.2019.0231', 'id_scheme': 'DOI'}"," For some time now, the field of Spatial Humanities has acknowledged the need for a system capable of the spatial exploration of historical and archaeological phenomena beyond Geographic Information Systems (GIS). This idea comes from the need to analyse holistically spatial information, including that which is not geographic (i.e. vague, symbolic and imaginary space). In addition, this need becomes more apparent when dealing with traditions that do not conform to the Modern/European/Cartesian conception of space in which GIS is rooted. This article, explores the use of Qualitative Spatial Representation (QSR) and Semantic Triples as possible alternative means to model complex and diverse expressions of spatial information, including social and symbolic conceptions in 16th century Mexican maps. Using as case study the map from the region of Atengo-Misquiahuala (Hidalgo) which combine the Mesoamerican and European traditions, we explore how these approaches might open new venues of research, potentially shedding light to long discussed and problematic Mesoamerican spatial categories. Focusing on a contained and partial example, we examine from a theoretical perspective and as a starting point, the possible future implementation of these approaches for historical and archaeological research. ",Spatial Humanities 3.0: Qualitative Spatial Representation and Semantic Triples as New Means of Exploration of Complex Indigenous Spatial Representations in Sixteenth Century Early Colonial Mexican Maps,Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1829,https://doi.org/10.3366/ijhac.2019.0232,"{'string_id': '10.3366/ijhac.2019.0232', 'id_scheme': 'DOI'}"," The GB1900 project used crowd-sourcing to transcribe all text from the second edition County Series six inch to one mile maps of Great Britain, published between 1888 and 1914, a total of c. 2.55m. geo-located text strings. These locate almost every farm and about half of all street names. The paper describes the final datasets, and how they were created. It then presents a detailed comparison with five other freely-available gazetteers of Britain: Geonames, the US government's NGA gazetteer, the Ordnance Survey's 50k and Open Names datasets, and the English Place Name Survey's DEEP project. Comparisons are presented at national level and, more qualitatively, for an area of eastern England. The results demonstrate both GB1900's greater volume of geo-located entries and its ability to locate places and features identified in other historical sources beyond administrative hierarchies: this is the most detailed historical gazetteer, certainly for Britain and possibly for anywhere. The final online system is described, including its integration of place name histories from DEEP. ",Locating Past Places in Britain: Creating and Evaluating the GB1900 Gazetteer,Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1830,https://doi.org/10.3366/ijhac.2019.0233,"{'string_id': '10.3366/ijhac.2019.0233', 'id_scheme': 'DOI'}"," Historical place databases can be an invaluable tool for capturing the rich meaning of past places. However, this richness presents obstacles to success: the daunting need to simultaneously represent complex information such as temporal change, uncertainty, relationships, and thorough sourcing has been an obstacle to historical GIS in the past. The Qualified Assertion Model developed in this paper can represent a variety of historical complexities using a single, simple, flexible data model based on a) documenting assertions of the past world rather than claiming to know the exact truth, and b) qualifying the scope, provenance, quality, and syntactics of those assertions. This model was successfully implemented in a production-strength historical gazetteer of religious congregations, demonstrating its effectiveness and some challenges. ",A Qualified Assertion Database for the History of Places,Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1831,https://doi.org/10.3366/ijhac.2019.0234,"{'string_id': '10.3366/ijhac.2019.0234', 'id_scheme': 'DOI'}"," For over half-a-century (1803–54), the Australian colony of Van Diemen's Land (Tasmania), played a key part in Britain's globe-spanning unfree diaspora. Today, a rich built and archaeological landscape, augmented by an exhaustive and relatively intact documentary archive, stand as eloquent markers to this convict legacy. As historical archaeologists, we have spent countless hours querying the physical and documentary residues in a bid to understand how the penological, social and economic imperatives of Britain and the colony shaped the management of convict labour. In particular, our task has centred upon the recovery of individual narratives – of both gaoler and gaoled – from such residues, moving away from a traditional focus on the broader outlines of the convict system. This paper illustrates how spatial history methodological processes have been used to relocate individual historic lives back into the convict industrial landscape of the Tasman Peninsula (Tasmania). Focusing on the male-only penal station of Port Arthur (1830–77), we will illustrate how we have reunited the physicality of past spaces and places, with the lives and labours of those who created and navigated them. Simple methodologies have been used to achieve this, designed with onward applicability in mind. A complex series of documents, convict conduct records, have been mined for spatial markers, allowing events and people to be relocated back into space. Through these processes of linkage and visualisation, we have been encouraged to ask further questions about the management of the unfree labour force and how this came to create the landscape we see today. ","Repopulating Landscapes: Using Offence Data to Recreate Landscapes of Incarceration and Labour at the Port Arthur Penal Station, 1830–1877",Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1832,https://doi.org/10.3366/ijhac.2019.0235,"{'string_id': '10.3366/ijhac.2019.0235', 'id_scheme': 'DOI'}"," Art that incorporates brain-computer interface (or BCI) technology sheds fresh light on several aspects of aesthetic theory. Because it is radically interactive and can permit viewers or listeners to modify a work directly by means of their cerebral activity, such art illuminates the role of audience members in shaping that work's meaning; in this way, it literalizes reader-response theory and allows the public to engage even with opaque or alienating pieces. BCI-based art also reframes the significance of the artist's intentions, prompting a reconsideration of the truism that ‘the author is dead’, both by positing a collective form of authorship and by granting a creator access to her own unconscious impulses. Finally, via the notion that it may be possible to transfer unfiltered ideas between brains, BCI-inspired artworks provide a new perspective on art as mediation. Although artists have traditionally been praised for seeming to grant direct access to their emotions, one could argue that artistry happens in the act of concretizing and externalizing one's ideas – that is, in the mediated translation of thought rather than in thought itself. The essay concludes by discussing the implications of this theoretical framework for (among other fields) the digital humanities. ",Brain-computer Interfaces and Art: Toward a Theoretical Framework,Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1833,https://doi.org/10.3366/ijhac.2019.0236,"{'string_id': '10.3366/ijhac.2019.0236', 'id_scheme': 'DOI'}"," The visual image has long been central to how war is seen, contested and legitimised, remembered and forgotten. Archives are pivotal to these ends as is their ownership and access, from state and other official repositories through to the countless photographs scattered and hidden from a collective understanding of what war looks like in individual collections and dusty attics. With the advent and rapid development of social media, however, the amateur and the professional, the illicit and the sanctioned, the personal and the official, and the past and the present, all seem to inhabit the same connected and chaotic space. However, to even begin to render intelligible the complexity, scale and volume of what war looks like in social media archives is a considerable task, given the limitations of any traditional human-based method of collection and analysis. We thus propose the production of a series of ‘snapshots’, using computer-aided extraction and identification techniques to try to offer an experimental way in to conceiving a new imaginary of war. We were particularly interested in testing to see if twentieth century wars, obviously initially captured via pre-digital means, had become more ‘settled’ over time in terms of their remediated presence today through their visual representations and connections on social media, compared with wars fought in digital media ecologies (i.e. those fought and initially represented amidst the volume and pervasiveness of social media images). To this end, we developed a framework for automatically extracting and analysing war images that appear in social media, using both the features of the images themselves, and the text and metadata associated with each image. The framework utilises a workflow comprising four core stages: (1) information retrieval, (2) data pre-processing, (3) feature extraction, and (4) machine learning. Our corpus was drawn from the social media platforms Facebook and Flickr. ",A Framework for Interrogating Social Media Images to Reveal an Emergent Archive of War,Edinburgh University Press,2019,,International Journal of Humanities and Arts Computing,13,1-2,"[{'value': None, 'type': None}]"
1834,https://doi.org/10.3366/ijhac.2020.0243,"{'string_id': '10.3366/ijhac.2020.0243', 'id_scheme': 'DOI'}","At the University of Limoges in the center of France, we started developing an Historical Atlas of the region of Limousin (AHL) in 2014. The Atlas is one part of a project to gather spatial-temporal information and historical sources about the history of the region. It offers an editorial space and cartographic interface where the regional and scientific community can interact, share, and disseminate their historical knowledge and data. As such, this project represents a close interdisciplinary exchange between historians, archeologists, geographers, GIS and data scientists, as well as varied data producers such as public actors (universities, local authorities, archives), private societies (archeology and tourist operators) and associations. This article tells the story of this dialogue and explains the interdisciplinary, multimedia and spatial-temporal data model and public interface that resulted from it. ",Construction and Management of a Geo-Historical Information System for an Interdisciplinary and Contributory Atlas: The Historical Atlas of Limousin,Edinburgh University Press,2020,,International Journal of Humanities and Arts Computing,14,1-2,"[{'value': None, 'type': None}]"
1835,https://doi.org/10.3366/ijhac.2020.0244,"{'string_id': '10.3366/ijhac.2020.0244', 'id_scheme': 'DOI'}"," This research introduces a novel geo-spatial sampling model to overcome a major difficulty in historical economic geography of Bulgarian lands during a crucial period: immediately before and after the de facto independence of the territory from the Ottoman Empire in the second half of the nineteenth century. At its core it seeks to investigate the research question how the Bulgarian independence affected agricultural production in two regions (centered around the cities of Plovdiv and Ruse) of today's Bulgaria, for which there are conflicting yet empirically unsubstantiated claims concerning the economic impact of the political independence. Using our be-spoke geo-sampling strategy we believe, we have sampled regionally representative commensurable agricultural data from the 1840s Ottoman archival documentation, in accord with agricultural censuses conducted by the nascent nation state of Bulgaria in the 1890s. ","Bridging the Gap between Pre-census and Census-era Historical Data: Devising a Geo-sampling Model to Analyse Agricultural Production in the Long Run for Southeast Europe, 1840–1897",Edinburgh University Press,2020,,International Journal of Humanities and Arts Computing,14,1-2,"[{'value': None, 'type': None}]"
1836,https://doi.org/10.3366/ijhac.2020.0245,"{'string_id': '10.3366/ijhac.2020.0245', 'id_scheme': 'DOI'}"," Databases are central to the digital, spatial, and geohumanities. There is surprisingly little scholarly literature, however, on the process of database construction in humanities projects. This article describes a process of interdisciplinary database design that emerged in the course of building the core sections of an historical GIS of Holocaust ghettos. The process foregrounds collaborative design, testing that purposely flushes out paradigmatic differences and ontological problems, and revision to incorporate group decisions and agreed-upon meanings into data structures, field definitions, and instructions for data entry. The result is a deeply integrative form of mixed methodology that incorporates ethical standards along with data entry instructions and team training. ","Integrative, Interdisciplinary Database Design for the Spatial Humanities: the Case of the Holocaust Ghettos Project",Edinburgh University Press,2020,,International Journal of Humanities and Arts Computing,14,1-2,"[{'value': None, 'type': None}]"
1837,https://doi.org/10.3366/ijhac.2020.0246,"{'string_id': '10.3366/ijhac.2020.0246', 'id_scheme': 'DOI'}"," In this paper, we introduce a web GIS platform created expressly for exploring and researching a set of 63,467 historical maps and illustrations extracted from 4,000 titles of Chinese local gazetteers. We layer these images with a published, geo-referenced collection of Land Survey Maps of China (1903–1948), which includes the earliest large-scale maps of major cities and regions in China that are produced with modern cartographic techniques. By bringing together historical illustrations depicting spatial configurations of localities and the earliest modern cartographic maps, researchers of Chinese history can study the different spatial epistemologies represented in both collections. We report our workflow for creating this web GIS platform, starting from identifying and extracting visual materials from local gazetteers, tagging them with keywords and categories to facilitate content search, to georeferencing them based on their source locations. We also experimented with neural networks to train a tagger with positive results. Finally, we display them in the web GIS platform with two modes, Images in Map (IIM) and Maps in Map (MIM), and with content- and location-based filtering. These features together enable researchers easy and quick exploration and comparison of these two large sets of geospatial and visual materials of China. ",Displaying Spatial Epistemologies on Web GIS: Using Visual Materials from the Chinese Local Gazetteers as an Example,Edinburgh University Press,2020,,International Journal of Humanities and Arts Computing,14,1-2,"[{'value': None, 'type': None}]"
1838,https://doi.org/10.3366/ijhac.2020.0247,"{'string_id': '10.3366/ijhac.2020.0247', 'id_scheme': 'DOI'}"," Finding and selecting interesting and relevant information in large online digital text archives can be challenging. We tackle this information access problem from a geographic information science perspective using a case study exploring a semi-structured historical encyclopedia. We propose a three-pronged approach for this, based around (1) automatic retrieval of spatio-temporal and thematic information from digital text documents; (2) transformation of the extracted information to spatialize and visualize spatio-temporal and thematic structures; and (3) integration of the spatialized displays in an interactive web interface driven by a user-centered design and evaluation approach. We implemented an interactive spatialized network display to allow identification of spatio-temporal relationships hidden in the text archive, complemented by an interactive self-organizing map display to visualize thematic relationships in these text documents. We evaluated the utility and usability of the developed interface in a user study with digital humanities scholars. Empirical results show that the developed interface supports target users in the humanities uncovering latent spatio-temporal and thematic relationships and generated new insights through the spatialized text collection. Adopting this approach, we illustrate one avenue to addressing the information access problem in the digital humanities from a GIScience perspective. ",How Can Geographic Information in Text Documents be Visualized to Support Information Exploration in the Humanities?,Edinburgh University Press,2020,,International Journal of Humanities and Arts Computing,14,1-2,"[{'value': None, 'type': None}]"
1839,https://doi.org/10.3366/ijhac.2020.0248,"{'string_id': '10.3366/ijhac.2020.0248', 'id_scheme': 'DOI'}"," This article documents the use of historical GIS with timestamped itineraries to better understand a large multilingual corpus of nineteenth-century travelogues about Spain and their diverse ‘chronotopes’ (meaningful intersections of space and time in a narrative, as defined by Mikhail Bakhtin), which remain unnoticeable when one reads travelogues as traditional literary texts. The authors offer a rationale for using historical maps and GIS with timestamps, discuss the challenges posed by a multilingual historical dataset with partially imprecise or inferred information, and share their approach to overcoming these challenges in data collection, the creation of gazetteer, and timecoding. Despite focusing on travelogues, these tools and approaches are transferable to the visualisation and analysis of other texts in which chronotopes matter. ",Letters from Spain in a Space-time Box: Historical GIS with Timestamped Itineraries for Understanding the Chronotopes of Nineteenth-century Travel Writing,Edinburgh University Press,2020,,International Journal of Humanities and Arts Computing,14,1-2,"[{'value': None, 'type': None}]"
1840,https://doi.org/10.3366/ijhac.2020.0249,"{'string_id': '10.3366/ijhac.2020.0249', 'id_scheme': 'DOI'}"," Larry McMurtry's Pulitzer Prize winning counter-western Lonesome Dove (part of a tetralogy, set between 1840 and 1900) and the works of LA Noir detective fiction writers (from the 1940s to 1997) represent the American west and urban southwest of Los Angeles as a dynamic mosaic of human and environmental borderlandscapes. McMurtry's perspective provides an Anglo-European eye, influenced by Cervantean Iberian literary tropes on the transformation of the West from indigenous and Spanish trails to American rail-road tracks. The LA Noirscapes map phenomenologically illustrates how the location of novel settings cluster in contiguous and convergent places on a street grid palimpsest of Los Angeles between 1949 and 1997. Employing HumGIS methods, this essay considers the marriage of empirical cartography and impressionistic topography; the former concerned with latitude, longitude and space, the latter with plotting literary, historical and cultural perceptions and experiences of place. By engaging the concept of Euclidian space with the phenomenology of place, geographers can contextualize field work, and other methods with literary, cartographical and GIS analysis to uncover the means to craft new avenues to study the dynamic and symbiotic formations of historical landscapes, identities, senses of place and location. ",Historical and Imagined GIS Borderlandscapes of the American West: Larry McMurtry's Lonesome Dove Tetralogy and LA Noirscapes,Edinburgh University Press,2020,,International Journal of Humanities and Arts Computing,14,1-2,"[{'value': None, 'type': None}]"
1841,https://doi.org/10.3366/ijhac.2020.0250,"{'string_id': '10.3366/ijhac.2020.0250', 'id_scheme': 'DOI'}"," This exposition presents little-known connections between geography, through geographic information systems (GISs), mathematics, through matrix algebra, and art, through paintings and images, adding to the geo-humanities, spatial humanities, and humanistic mathematics literature. To this end, findings summarized for spatial statistical analyses of selected Susie Rosmarin paintings (which are reminiscent of visualizations of certain mathematical quantities known as eigenvectors), remotely sensed images that have appeared in art exhibits, and selected famous paintings by historically renowned artists reveal that spatial autocorrelation constitutes a fundamental element of art. These analyses extend the tradition of visualizing fractals as art, and interfacing cartography with art. This paper promotes analytical art, and establishes additional commonalities for GIScience, mathematics, and art. ",A Spatial Analysis of Selected Art: a GIScience-Humanities Interface,Edinburgh University Press,2020,,International Journal of Humanities and Arts Computing,14,1-2,"[{'value': None, 'type': None}]"
1842,https://doi.org/10.3366/ijhac.2020.0251,"{'string_id': '10.3366/ijhac.2020.0251', 'id_scheme': 'DOI'}"," To some, archaeology is best framed as a science, and to others it is envisioned as part of the humanities; but in our view archaeology as a discipline is strongest when it acknowledges the value of and incorporates both scientific and humanistic perspectives. Despite the many successes of scientifically-oriented archaeology, we contend that our discipline cannot achieve its broader aims without the humanities. We illustrate our argument by delineating three research domains – Deep Time Perspectives, Spatial Histories, and Public Engagement – and reviewing recent studies in each that have successfully integrated scientific and humanistic approaches to spatial archaeology. ",Spatial Archaeology: Mapping the Ancient Past with the Humanities and the Sciences,Edinburgh University Press,2020,,International Journal of Humanities and Arts Computing,14,1-2,"[{'value': None, 'type': None}]"
1843,https://doi.org/10.3366/ijhac.2020.0252,"{'string_id': '10.3366/ijhac.2020.0252', 'id_scheme': 'DOI'}"," The importance of spatial precision in geographic information science is not limited to quantitative data. As spatial data can also exist in qualitative form, modifying a discourse quality index from the field of discourse ethics can help understand whether mentioning specific spatial locations changes the quality of spatial narratives. The discourse quality index was modified by incorporating an item into the index that detected the presence and magnitude of a spatial precision construct. The spatial narratives analyzed with this modified index were 151 public comments submitted during a public policy revision process, for a national forest plan revision at the Chugach National Forest in Alaska, USA. Analysis showed when discourse quality values underwent a comparison of means test, the values were significantly changed between comments with no spatial precision versus those considered to have spatial precision. The results suggest, preliminarily, that employing spatial precision in narratives changes discourse quality during deliberative activities. Further, the way in which people use spatial precision to communicate during a policy revision process can impact how spatial narratives are understood and valued. ",The Role of Precision in Spatial Narratives: Using a Modified Discourse Quality Index to Measure the Quality of Deliberative Spatial Data,Edinburgh University Press,2020,,International Journal of Humanities and Arts Computing,14,1-2,"[{'value': None, 'type': None}]"
1844,https://doi.org/10.3366/ijhac.2020.0253,"{'string_id': '10.3366/ijhac.2020.0253', 'id_scheme': 'DOI'}"," Researchers from the spatial humanities can play an active role in the ongoing discussion around representation and uncertainty in geosocial data. Opportunistic geosocial data from crowdsourced and user contributions are typically available in large quantities, however, these data are generally not sampled or collected with intentionality for research purposes. Therefore, digital traces from geolocationally-enabled devices may not provide data that are directly relevant to addressing precise questions or being used in applications of societal interest. It is crucial to first understand characteristics of geosocial data prior to proceeding with interpretation. The spatial humanities could encourage realization of the role data analysts and researchers play in creating narrative in light of large opportunistic data with many facets of uncertainty. An approach is proposed that suggests directions of inquiry into data limitations based on the characteristics of the source and the properties of the data. Finally, this article notes that the digital world influences the activities of individuals in real-life so the generalizability of the behaviour of users may be limited. Integrating other forms of relevant spatial data can help to bound uncertainty and constrain the interpretation of geosocial data. ",Representation in Geosocial Data: Grappling with Uncertainty in Digital Traces of Human Activity,Edinburgh University Press,2020,,International Journal of Humanities and Arts Computing,14,1-2,"[{'value': None, 'type': None}]"
1845,https://doi.org/10.3366/ijhac.2020.0254,"{'string_id': '10.3366/ijhac.2020.0254', 'id_scheme': 'DOI'}"," Most people in developed countries, and many in developing countries, interact with Artificial Intelligence (AI) developed algorithms on an almost daily basis, yet very few are aware of those interactions or of their effect upon people's daily lives.1 Using machine learning, automated reasoning, and other forms of AI, algorithms deployed in information systems take actions according to criteria set up by software developers to maximize profits regardless of the overall effects on the autonomy or welfare of individuals or on society as a whole.2 Those criteria almost always involve location-based and place-based data. In this paper, we consider effects of those computational tools on individual choice and autonomy and on societal structure from the humanistic perspectives of philosophy and jurisprudence. Specifically, we: •review contemporaneous literature and quantitative economic evidence on the effects of pervasive tracking and algorithmic controls on individuals and on society in general, •summarize a range of suggested solutions for lessening the adverse effects, and •describe and expand upon selected legal solutions from the literature. ",Being Human in an Algorithmically Controlled World,Edinburgh University Press,2020,,International Journal of Humanities and Arts Computing,14,1-2,"[{'value': None, 'type': None}]"
1846,http://dx.doi.org/10.3389/fdigh.2015.00003,"{'string_id': '10.3389/fdigh.2015.00003', 'id_scheme': 'DOI'}","Large areas within the old urban core of American cities were destroyed in the post-war era as city leaders sought to counteract metropolitan decentralization by implementing urban renewal programs. For the most part, these places were deemed not worthy of documentation; indeed, areas declared as “blighted” were widely viewed as ugly, cancerous threats to the future health of the city and were enthusiastically demolished. For this reason, they have rarely been the object of study. This paper attempts to address the neglect of such lost landscapes by using GIS to provide a means to envision the urban past. Focusing on the case of the old Auraria district in downtown Denver, the paper pursues three aims. The first aim is to develop and test a new method for reconstructing urban landscapes at the district scale using GIS methods and data derived from historic Sanborn fire insurance maps. The second aim is to conduct preliminary analysis of this historic district-scale GIS, focusing on land use, spatial connectivity, and building massing, in order to evaluate its utility in generating new insight into the form, character, and functioning of the historic city core. The third aim is to assess the degree to which historic urban reconstructions of this kind can become valuable tools in contemporary urban planning efforts.",Envisioning the Urban Past: GIS Reconstruction of a Lost Denver District,Frontiers Media SA,2015,[],Frontiers in Digital Humanities,2,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1847,http://dx.doi.org/10.3389/fdigh.2015.00008,"{'string_id': '10.3389/fdigh.2015.00008', 'id_scheme': 'DOI'}","This article describes the use of latent Dirichlet allocation (LDA), or topic modeling, to explore the discursive makeup of the eighteenth century Encyclopédie of Denis Diderot and Jean le Rond d’Alembert (1751–1772). Expanding upon previous work modeling the Encyclopédie’s ontology, or classification scheme, we examine the abstractions used by its editors to visualize the various “systems” of knowledge that the work proposes, considered here as heuristic tools for navigating the complex information space of the Encyclopédie. Using these earlier experiments with supervised machine-learning models as a point of reference, we introduce the notion of topic modeling as a “discourse analysis tool” for Enlightenment studies. In so doing, we draw upon the tradition of post-structuralist French discourse analysis, one of the first fields to embrace computational approaches to discursive text analysis. Our particular use of LDA is thus aimed primarily at uncovering interdisciplinary “discourses” in the Encyclopédie that run alongside, under, above, and through the original classifications. By mapping these discourses and discursive practices, we can begin to move beyond the organizational (and physical) limitations of the print edition, suggesting several possible avenues of future research. These experiments thus attest once again to the enduring relevance of the Encyclopédie as an exemplary Enlightenment text. Its rich dialogical structure, whether studied using traditional methods of close reading or through the algorithmic processes described in this article, is perhaps only now coming fully to light thanks to recent developments in digital resources and methods.",Discourses and Disciplines in the Enlightenment: Topic Modeling the French Encyclopédie,Frontiers Media SA,2016,[],Frontiers in Digital Humanities,2,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1848,http://dx.doi.org/10.3389/fdigh.2016.00003,"{'string_id': '10.3389/fdigh.2016.00003', 'id_scheme': 'DOI'}","Historical handwritten Hebrew manuscripts are one of the most unique and authentic witnesses of Jewish culture and thought that survived through the centuries. In order to enable a systematic research of the knowledge embedded in the manuscripts, there is a need for a formal conceptual data model with a high level of semantic granularity, an ontology. We propose to build a dynamic web-based framework that will allow scholars to create, enrich, and consult an “ontopedia” (ontology-based encyclopedia) of Hebrew manuscripts. The framework is based on an ontology especially designed and implemented for this domain and goals. We view a manuscript as a “living entity” and propose to design a new ontological data model of the narrative for a manuscript, stages/milestones in its biography (creation, copying, and acquisition). A sequence of events and places constitutes a timeline of history against which manuscripts, people, and their relationships can be placed. A large-scale automated reasoning based on the ontology will also enable us to construct a semantically rich social network of people and manuscripts, and to compare the effect of time and place on the manuscripts’ qualitative characteristics and quantitative distribution.",Toward an Ontopedia for Historical Hebrew Manuscripts,Frontiers Media SA,2016,[],Frontiers in Digital Humanities,3,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1849,http://dx.doi.org/10.3389/fdigh.2016.00006,"{'string_id': '10.3389/fdigh.2016.00006', 'id_scheme': 'DOI'}","With the increased application of network analysis in archeology to form hypotheses, particularly concerning the research on mobility, a need has arisen to validate the network analysis results. This paper presents a case study of a local transport network in the Dutch part of the Roman limes between 70 and 270 AD created using a least-cost approach, and tests the robustness of the local network metric of betweenness centrality and the archeological interpretation thereof. It is demonstrated that while the majority of sites have a robust and thus reliable betweenness centrality, there are still a large number of sites for which the network measurements are very dependent on the precise structure of the network present. Testing robustness of network analysis results thus proves a useful tool for both validating the network modeling results and the archeological interpretations of that network.",Testing the Robustness of Local Network Metrics in Research on Archeological Local Transport Networks,Frontiers Media SA,2016,[],Frontiers in Digital Humanities,3,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1850,http://dx.doi.org/10.3389/fdigh.2016.00007,"{'string_id': '10.3389/fdigh.2016.00007', 'id_scheme': 'DOI'}","We present a tool for re-ranking the results of a specific query by considering the matrix of pairwise similarities among the elements of the set of retrieved results and the query itself. The re-ranking, thus, makes use of the similarities between the various results and does not employ additional sources of information. The tool is based on graphical Bayesian models, which reinforce retrieved items strongly linked to other retrievals, and on repeated clustering to measure the stability of the obtained associations. To this, we add an active relevance-based re-ranking process in order to leverage true matches, which have very low similarity to the query. The utility of the tool is demonstrated within the context of a visual search of documents from the Cairo Genizah. It is also demonstrated in a completely different domain or retrieving, given an input image of a painting, other related paintings.",Active Congruency-Based Reranking,Frontiers Media SA,2016,[],Frontiers in Digital Humanities,3,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1851,http://dx.doi.org/10.3389/fdigh.2016.00008,"{'string_id': '10.3389/fdigh.2016.00008', 'id_scheme': 'DOI'}","Women owned a quarter of all rental units in Montréal, QC, Canada, in 1903, a city where 85% of the population were tenants. In no major city in the world today, do women control an equivalent area of the formal economy. This paper asks did the gender of proprietorship matter? It answers this through a series of tests linking a 30% sample of all immigrant-headed households in the 1901 census with a complete historical GIS of all properties and their owners in the city for 1903. The paper plays special attention to Ashkenazi Jews, Syrians, Chinese, and Italians, as these relatively recent immigrants constituted a major break with the largely British and French ancestry of the majority of the population in this 300-year-old settler colony. It then links the patterns in the sample to an index of all households in the census, to explore how these immigrant families integrated into the larger host communities. The paper shows that landladies and landlords had differing practices with regard to overcrowding and to the enforcement of segregation. The paper makes a sustained argument for rethinking how we should approach the relationship between gender and property.","Gender, Discrimination, and Housing in Turn of the Century Montréal: What Mapping the Census Returns of Immigrants Can Tell Us",Frontiers Media SA,2016,[],Frontiers in Digital Humanities,3,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1852,http://dx.doi.org/10.3389/fdigh.2016.00009,"{'string_id': '10.3389/fdigh.2016.00009', 'id_scheme': 'DOI'}","Digital tools, and in particular GIS, have enormously increased the possibilities for analysis in historical geography. In this article, we shall explain how these tools can be used to study the evolution of population density over a significant period. The territorial units used will be municipalities, as they allow detailed territorial analysis. However, research projects that take municipalities as their points of reference tend to be complex because their territorial boundaries have often undergone numerous changes over the course of modern history. The same has occurred, to a greater or lesser degree, in all of the countries in Europe (Bennett, 1989). The countries that have had the most stable municipal boundaries over the past 150 years include France, Italy, and Spain, though the modifications to their boundaries have also been notable. However, like all relevant challenges, these changes also offer us new opportunities, if we are able to cope with them. In this particular case, the challenge will be to achieve the territorial homogenization of the historical municipal series. In other words, when the municipal limits have changed, it will be necessary to adapt the data from the old municipal territories to the new ones. This exercise will have a number of applications. In this article, we present just one of these: the possibility of detecting areas and periods in which, over the course of history, there has been population growth, decline, or stagnation. This will serve as a relevant indicator, or proxy, for organizing research in other fields. For example, in the case of economic history, it is clear that variations in the density of population provide clues for interpreting the territorial distribution of economic activity. We also understand that it will be possible to apply our research about Spain to other countries and that this will make it possible to evaluate the interest and results that we can expect from the homogenized work. We think that, despite its interest, this type of study has, until now, been very rare on account of the methodological difficulties involved. However, these new digital tools in the field of historical GIS, as spatial aggregation and Moran I techniques, have helped to provide solutions to assume this challenge.",The Use of Digital Tools for Spatial Analysis in Population Geography,Frontiers Media SA,2016,[],Frontiers in Digital Humanities,3,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1853,http://dx.doi.org/10.3389/fdigh.2017.00001,"{'string_id': '10.3389/fdigh.2017.00001', 'id_scheme': 'DOI'}","This article presents one of the first steps of a project that aims at exploring the diffusion patterns of Mediterranean imported goods in Late Iron Age Europe (250–25 BC) and the organization of the commercial interactions of these goods. It brings together two archeologists and a mathematician in the study of a large inventory of 57,735 Italian and Greek imports discovered from England to Serbia. This large amount of new and unpublished data is analyzed through the joint use of network analysis tools and formal statistical methods. The analysis focuses on detecting patterns in the association of imported artifacts that are often found on the same sites. The objectives are to highlight groups of imports that may have circulated together and to emphasize regional selections by local populations. At this stage of the study, two main systems of imports have been highlighted used, respectively, in West and Central Europe. Interesting leads that will need further investigation include the imports status and the role they played in Celtic societies, as acculturated objects or more as objects for acculturation.",Co-presence Analysis and Economic Patterns: Mediterranean Imports in the Celtic World,Frontiers Media SA,2017,[],Frontiers in Digital Humanities,4,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1854,http://dx.doi.org/10.3389/fdigh.2017.00003,"{'string_id': '10.3389/fdigh.2017.00003', 'id_scheme': 'DOI'}","The rapid progress of deep neural network architectures is allowing both to automate the production of artworks and to extend the domain of creative expression. As such, it is opening new ground for professional and amateur artists alike. A major asset of these new computer processes is their capacity to derive, from a training phase, a generative model from which new artifacts can be produced. This attribute allows for a wide range of novel applications. New music or paintings in the style of famous artists can be produced at the click of a button, or combined to form new artworks. New graphical compositions can be “hallucinated” by the deep algorithmic models to produce striking, unexpected, visual forms. By the same token, the dependence on preexisting, protected, artworks lays the ground for potential zones of friction with the rights holders of the source data that helped shape the generative model. This articulation, between the popular creative movement initiated by the deep neural architectures and the preexisting rights of the authors, leads to a confrontation between the present legal framework for the protection of artistic creations and the new modalities offered by these new technological objects. The present work will address the conditions of protection of creations generated by deep neural networks under the main copyright regimes.",Deep Creations: Intellectual Property and the Automata,Frontiers Media SA,2017,[],Frontiers in Digital Humanities,4,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1855,http://dx.doi.org/10.3389/fdigh.2017.00004,"{'string_id': '10.3389/fdigh.2017.00004', 'id_scheme': 'DOI'}","The period between the beginning of the Early Iron Age and the end of the Archaic Period is a time of changes and developments in the Italian Peninsula, which led to the creation of regional ethnic and political groups and to the formation of the first city-states in Western Europe. In the present study, we focus on the evolution of terrestrial route network in the Tyrrhenian region of Latium vetus as it has been hypothesized by scholars from the archeological evidence. Our main goal is to investigate the mechanisms linking decision making processes and the structure of transportation networks. We first attempted to replicate some of its features applying three models previously elaborated for the neighboring region of Southern Etruria. Since it was not possible to attain entirely satisfactory results, we modified the model that performed better in the Etruscan region by including a tunable amount of rich-get-richer bias, which improved considerably its performance. Our results suggest that coordinated decision making with a slightly unbalanced power was responsible for the peculiar characteristics of the route network topology of Latium vetus. Moreover, the mechanism implemented by this model implies that places located at favorable positions can build on their initial advantage and get more and more powerful. This fits very well with the picture elaborated by different scholars on the nature of power balance and dynamics in this region.",Coordinated Decisions and Unbalanced Power. How Latin Cities Shaped Their Terrestrial Transportation Network,Frontiers Media SA,2017,[],Frontiers in Digital Humanities,4,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1856,http://dx.doi.org/10.3389/fdigh.2017.00005,"{'string_id': '10.3389/fdigh.2017.00005', 'id_scheme': 'DOI'}","In the area of Linked Open Data (LOD), meaningful and high-performance interlinking of different datasets has become an ongoing challenge. Necessary tasks are supported by established standards and software, e.g., for the transformation, storage, interlinking, and publication of data. Our use case Swissbib &lt;https://www.swissbib.ch/&gt; is a well-known provider for bibliographic data in Switzerland representing various libraries and library networks. In this article, a case study is presented from the project linked.swissbib.ch which focuses on the preparation and publication of the Swissbib data by means of LOD. Data available in Marc21 XML are extracted from the Swissbib system and transformed into an RDF/XML representation. From approximately 21 million monolithic records, the author information is extracted and interlinked with authority files from the Virtual International Authority File (VIAF) and DBpedia. The links are used to extract additional data from the counterpart corpora. Afterward, data are pushed into an Elasticsearch index to make the data accessible for other components. As a demonstrator, a search portal is developed which presents the additional data and the generated links to users. In addition to that, a REST interface is developed in order to enable also access by other applications. A main obstacle in this project is the amount of data and the necessity of day-to-day (partial) updates. In the current situation, the data in Swissbib and in the external corpora are too large to be processed by established linking tools. The arising memory footprint prevents the correct functioning of these tools. Also triple stores are unhandy by revealing a massive overhead for import and update operations. Hence, we have developed procedures for extracting and shaping the data into a more suitable form, e.g., data are reduced to the necessary properties and blocked. For this purpose, we used sorted N-Triples as an intermediate data format. This method proved to be very promising as our preliminary results show. Our approach could establish 30,773 links to DBpedia and 20,714 links to VIAF and both link sets show high precision values and could be generated in reasonable expenditures of time.",Interlinking Large-scale Library Data with Authority Records,Frontiers Media SA,2017,[],Frontiers in Digital Humanities,4,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1857,http://dx.doi.org/10.3389/fdigh.2017.00007,"{'string_id': '10.3389/fdigh.2017.00007', 'id_scheme': 'DOI'}","Recent approaches have described the evolutionary dynamics of the first Neolithic societies as a cycle of rise and fall. Several authors, using mainly c14 dates as a demographic proxy, identified a general pattern of a boom in population coincident with the arrival of food production economies followed by a rapid decline some centuries afterward in multiple European regions. Concerning Iberia, we also noted that this phenomenon correlates with an initial development of archeological entities (i.e., “cultures”) over large areas (e.g., the Impresso-Cardial in West Mediterranean), followed by a phase of “cultural fragmentation” by the end of Early Neolithic. This results in a picture of higher cultural diversity as an effect of more limited spread of cultural artifacts. In this work, we propose to apply a network approach to the analysis of material culture. In particular, we consider the spatiotemporal patterns of material culture as an emergent effect of local interaction processes. As recent research has pointed out, the spatiotemporal variability of material culture is an emergent phenomenon resulting from individual and group interactions whose structure resembles those of spatially structured complex networks. Our results suggest that the observed global patterns could be explained by the network dynamics, especially by structural (measured as the betweenness centrality) and geographical position of some nodes. The appearance and disappearance of nodes in specific positions correlate with the observed changes in the pattern of material culture distribution throughout the Early Neolithic (c. 7700–6700 cal BP) in East Iberia. In our view, this could be explained by the special role played by those nodes facilitating or limiting the information flow over the entire network. Network growth and posterior fragmentation seem to be the key drivers behind these dynamics.",Iberian Neolithic Networks: The Rise and Fall of the Cardial World,Frontiers Media SA,2017,[],Frontiers in Digital Humanities,4,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1858,http://dx.doi.org/10.3389/fdigh.2017.00008,"{'string_id': '10.3389/fdigh.2017.00008', 'id_scheme': 'DOI'}","When data are poor, we resort to theory modeling. This is a two-step process. We have first to identify the appropriate type of model for the system under consideration and then to tailor it to the specifics of the case. To understand settlement formation, which is the concern of this article, this involves choosing not only input parameter values such as site separations but also input functions that characterizes the ease of travel between sites. Although the generic behavior of the model is understood, the details are not. Different choices will necessarily lead to different outputs (for identical inputs). We can only proceed if choices that are “close” give outcomes that are similar. Where there are local differences, it suggests that there was no compelling reason for one outcome rather than the other. If these differences are important for the historic record, we may interpret this as sensitivity to contingency. We re-examine the rise of Greek city-states as first formulated by Rihll and Wilson in 1979, initially using the same “retail” gravity model. We suggest that, although cities like Athens owe their position to a combination of geography and proximity to other sites, the rise of Thebes is the most contingent, whose success reflects social forces outside the grasp of simple network modeling.",Was Thebes Necessary? Contingency in Spatial Modeling,Frontiers Media SA,2017,[],Frontiers in Digital Humanities,4,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1859,http://dx.doi.org/10.3389/fdigh.2017.00009,"{'string_id': '10.3389/fdigh.2017.00009', 'id_scheme': 'DOI'}","The measurement and tracking of body movement within musical performances can provide valuable sources of data for studying interpersonal interaction and coordination between musicians. The continued development of tools to extract such data from video recordings will offer new opportunities to research musical movement across a diverse range of settings, including field research and other ecological contexts in which the implementation of complex motion capture (MoCap) systems is not feasible or affordable. Such work might also make use of the multitude of video recordings of musical performances that are already available to researchers. This study made use of such existing data, specifically, three video datasets of ensemble performances from different genres, settings, and instrumentation (a pop piano duo, three jazz duos, and a string quartet). Three different computer vision techniques were applied to these video datasets—frame differencing, optical flow, and kernelized correlation filters (KCF)—with the aim of quantifying and tracking movements of the individual performers. All three computer vision techniques exhibited high correlations with MoCap data collected from the same musical performances, with median correlation (Pearson’s r) values of 0.75–0.94. The techniques that track movement in two dimensions (optical flow and KCF) provided more accurate measures of movement than a technique that provides a single estimate of overall movement change by frame for each performer (frame differencing). Measurements of performer’s movements were also more accurate when the computer vision techniques were applied to more narrowly defined regions of interest (head) than when the same techniques were applied to larger regions (entire upper body, above the chest, or waist). Some differences in movement tracking accuracy emerged between the three video datasets, which may have been due to instrument-specific motions that resulted in occlusions of the body part of interest (e.g., a violinist’s right hand occluding the head while tracking head movement). These results indicate that computer vision techniques can be effective in quantifying body movement from videos of musical performances, while also highlighting constraints that must be dealt with when applying such techniques in ensemble coordination research.",Extracting Coarse Body Movements from Video in Music Performance: A Comparison of Automated Computer Vision Techniques with Motion Capture Data,Frontiers Media SA,2017,[],Frontiers in Digital Humanities,4,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1860,http://dx.doi.org/10.3389/fdigh.2017.00017,"{'string_id': '10.3389/fdigh.2017.00017', 'id_scheme': 'DOI'}","A review of the archeological and non-archeological use of visibility networks reveals the use of a limited range of formal techniques, in particular for representing visibility theories. This paper aims to contribute to the study of complex visual relational phenomena in landscape archeology by proposing a range of visibility network patterns and methods. We propose first- and second-order visibility graph representations of total and cumulative viewsheds, and two-mode representations of cumulative viewsheds. We present network patterns that can be used to represent aspects of visibility theories and that can be used in statistical simulation models to compare theorized networks with observed networks. We argue for the need to incorporate observed visibility network density in these simulation models, by illustrating strong differences in visibility network density in three example landscapes. The approach is illustrated through a brief case study of visibility networks of long barrows in Cranborne Chase.",Visibility Network Patterns and Methods for Studying Visual Relational Phenomena in Archeology,Frontiers Media SA,2017,[],Frontiers in Digital Humanities,4,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1861,http://dx.doi.org/10.3389/fdigh.2017.00020,"{'string_id': '10.3389/fdigh.2017.00020', 'id_scheme': 'DOI'}","This article provides insights into aspects of tempo and rhythmic elaboration in Hindustani music, based on a study of a large corpus of recorded performances. Typical tempo developments and stress patterns within a metrical cycle are computed, which we refer to as tempo and rhythm patterns, respectively. Rhythm patterns are obtained by aggregating spectral features over metrical cycles. They reflect percussion patterns that are frequent in the corpus and enable a discussion of the relation between such patterns and the underlying metrical framework, the tāl. Tempo patterns, on the other hand, are computed using reference beat annotations. They document the dynamic development of tempo throughout a metrical cycle and reveal insights into the flexibility of time in Hindustani music for the first time using quantitative methods on a large set of performances. Focusing on aspects of tempo and rhythm, we demonstrate the value of a computational methodology for the analysis of large music corpora by revealing the range of tempi used in performances, intra-cycle tempo dynamics and percussion accents at different positions of the tāl cycle.",Aspects of Tempo and Rhythmic Elaboration in Hindustani Music: A Corpus Study,Frontiers Media SA,2017,[],Frontiers in Digital Humanities,4,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1862,http://dx.doi.org/10.3389/fdigh.2017.00021,"{'string_id': '10.3389/fdigh.2017.00021', 'id_scheme': 'DOI'}","Halal is a notion that applies to both objects and actions, and means permissible according to Islamic law. It may be most often associated with food and the rules of selecting, slaughtering, and cooking animals. In the globalized world, halal can be found in street corners of New York and beauty shops of Manila. In this study, we explore the cultural diversity of the concept, as revealed through social media, and specifically the way it is expressed by different populations around the world, and how it relates to their perception of (i) religious and (ii) governmental authority, and (iii) personal health. Here, we analyze two Instagram datasets, using Halal in Arabic (325,665 posts) and in English (1,004,445 posts), which provide a global view of major Muslim populations around the world. We find a great variety in the use of halal within Arabic, English, and Indonesian-speaking populations, with animal trade emphasized in first (making up 61% of the language’s stream), food in second (80%), and cosmetics and supplements in third (70%). The commercialization of the term halal is a powerful signal of its detraction from its traditional roots. We find a complex social engagement around posts mentioning religious terms, such that when a food-related post is accompanied by a religious term, it on average gets more likes in English and Indonesian, but not in Arabic, indicating a potential shift out of its traditional moral framing.",#Halal Culture on Instagram,Frontiers Media SA,2017,[],Frontiers in Digital Humanities,4,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1863,http://dx.doi.org/10.3389/fdigh.2017.00022,"{'string_id': '10.3389/fdigh.2017.00022', 'id_scheme': 'DOI'}","In this work, we describe a methodology to interpret large persons’ networks extracted from text by classifying cliques using the DBpedia ontology. The approach relies on a combination of NLP, Semantic web technologies, and network analysis. The classification methodology that first starts from single nodes and then generalizes to cliques is effective in terms of performance and is able to deal also with nodes that are not linked to Wikipedia. The gold standard manually developed for evaluation shows that groups of co-occurring entities share in most of the cases a category that can be automatically assigned. This holds for both languages considered in this study. The outcome of this work may be of interest to enhance the readability of large networks and to provide an additional semantic layer on top of cliques. This would greatly help humanities scholars when dealing with large amounts of textual data that need to be interpreted or categorized. Furthermore, it represents an unsupervised approach to automatically extend DBpedia starting from a corpus.",Using Semantic Linking to Understand Persons’ Networks Extracted from Text,Frontiers Media SA,2017,[],Frontiers in Digital Humanities,4,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1864,http://dx.doi.org/10.3389/fdigh.2018.00002,"{'string_id': '10.3389/fdigh.2018.00002', 'id_scheme': 'DOI'}","The field of Spatial Humanities has advanced substantially in the past years. The identification and extraction of toponyms and spatial information mentioned in historical text collections has allowed its use in innovative ways, making possible the application of spatial analysis and the mapping of these places with geographic information systems. For instance, automated place name identification is possible with Named Entity Recognition (NER) systems. Statistical NER methods based on supervised learning, in particular, are highly successful with modern datasets. However, there are still major challenges to address when dealing with historical corpora. These challenges include language changes over time, spelling variations, transliterations, OCR errors, and sources written in multiple languages among others. In this article, considering a task of place name recognition over two collections of historical correspondence, we report an evaluation of five NER systems and an approach that combines these through a voting system. We found that although individual performance of each NER system was corpus dependent, the ensemble combination was able to achieve consistent measures of precision and recall, outperforming the individual NER systems. In addition, the results showed that these NER systems are not strongly dependent on preprocessing and translation to Modern English.",Ensemble Named Entity Recognition (NER): Evaluating NER Tools in the Identification of Place Names in Historical Corpora,Frontiers Media SA,2018,[],Frontiers in Digital Humanities,5,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1865,http://dx.doi.org/10.3389/fdigh.2018.00005,"{'string_id': '10.3389/fdigh.2018.00005', 'id_scheme': 'DOI'}","This paper describes a corpus of about 3,000 English literary texts with about 250 million words extracted from the Gutenberg project that span a range of genres from both fiction and non-fiction written by more than 130 authors (e.g., Darwin, Dickens, Shakespeare). Quantitative narrative analysis (QNA) is used to explore a cleaned subcorpus, the Gutenberg English Poetry Corpus (GEPC), which comprises over 100 poetic texts with around two million words from about 50 authors (e.g., Keats, Joyce, Wordsworth). Some exemplary QNA studies show author similarities based on latent semantic analysis, significant topics for each author or various text-analytic metrics for George Eliot’s poem “How Lisa Loved the King” and James Joyce’s “Chamber Music,” concerning, e.g., lexical diversity or sentiment analysis. The GEPC is particularly suited for research in Digital Humanities, Computational Stylistics, or Neurocognitive Poetics, e.g., as training and test corpus for stimulus development and control in empirical studies.",The Gutenberg English Poetry Corpus: Exemplary Quantitative Narrative Analyses,Frontiers Media SA,2018,[],Frontiers in Digital Humanities,5,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1866,http://dx.doi.org/10.3389/fdigh.2018.00007,"{'string_id': '10.3389/fdigh.2018.00007', 'id_scheme': 'DOI'}","Studies of olfactory perception and about the way humans interact with, and perceive food and beverages require appropriate olfactory devices. Moreover, small size, and portable interfaces are needed within the context of Human Computer Interaction (HCI), to enrich and complete the design of different mediated experiences. In this paper, the authors tested a new portable olfactory device for the orthonasal administration of smells. The main aim was to verify if the experience generated by the odors delivered through such device can affect people's taste perception. Once established that people could perceive odors using the olfactory device, a group of participants was asked to taste two different types of food (Experiment 1) and three types of beverages (Experiment 2) and to evaluate them on a number of perceptual-dimensions (such as pleasantness, freshness, sweetness, saltiness, and bitterness). The participants could taste the food and the beverage without the presence of additional olfactory stimuli, or under conditions where olfactory stimuli (the smell of chocolate or citrus) were also presented using the device. The results showed that the participants' evaluation of food and beverages was significantly modulated by the concurrently presented odors. The experimental results suggest that: (1) the device is effective in controlling the delivery of odors to human participants without the complexity of management that often affect larger odors delivery systems; (2) odors administered by means of such device can have an effects on food and beverage perception, without the need to change their chemical properties.",Development and Testing of a Small-Size Olfactometer for the Perception of Food and Beverages in Humans,Frontiers Media SA,2018,[],Frontiers in Digital Humanities,5,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1867,http://dx.doi.org/10.3389/fdigh.2018.00009,"{'string_id': '10.3389/fdigh.2018.00009', 'id_scheme': 'DOI'}","We conduct the first experiment in the literature in which a novel is translated automatically and then post-edited by professional literary translators. Our case study is Warbreaker, a popular fantasy novel originally written in English, which we translate into Catalan. We translated one chapter of the novel (over 3,700 words, 330 sentences) with two data-driven approaches to Machine Translation (MT): phrase-based statistical MT (PBMT) and neural MT (NMT). Both systems are tailored to novels; they are trained on over 100 million words of fiction. In the post-editing experiment, six professional translators with previous experience in literary translation translate subsets of this chapter under three alternating conditions: from scratch (the norm in the novel translation industry), post-editing PBMT, and post-editing NMT. We record all the keystrokes, the time taken to translate each sentence, as well as the number of pauses and their duration. Based on these measurements, and using mixed-effects models, we study post-editing effort across its three commonly studied dimensions: temporal, technical and cognitive. We observe that both MT approaches result in increases in translation productivity: PBMT by 18%, and NMT by 36%. Post-editing also leads to reductions in the number of keystrokes: by 9% with PBMT, and by 23% with NMT. Finally, regarding cognitive effort, post-editing results in fewer (29 and 42% less with PBMT and NMT, respectively) but longer pauses (14 and 25%).",Post-editing Effort of a Novel With Statistical and Neural Machine Translation,Frontiers Media SA,2018,[],Frontiers in Digital Humanities,5,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1868,http://dx.doi.org/10.3389/fdigh.2018.00010,"{'string_id': '10.3389/fdigh.2018.00010', 'id_scheme': 'DOI'}","Rhetorical figures are valuable linguistic data for literary analysis. In this article, we target the detection of three rhetorical figures that belong to the family of repetitive figures: chiasmus (I go where I please, and I please where I go.), epanaphora also called anaphora (“Poor old European Commission! Poor old European Council.”) and epiphora (“This house is mine. This car is mine. You are mine.”). Detecting repetition of words is easy for a computer but detecting only the ones provoking a rhetorical effect is difficult because of many accidental and irrelevant repetitions. For all figures, we train a log-linear classifier on a corpus of political debates. The corpus is only very partially annotated, but we nevertheless obtain good results, with more than 50% precision for all figures. We then apply our models to totally different genres and perform a comparative analysis, by comparing corpora of fiction, science and quotes. Thanks to the automatic detection of rhetorical figures, we discover that chiasmus is more likely to appear in the scientific context whereas epanaphora and epiphora are more common in fiction.","Rhetorical Figure Detection: Chiasmus, Epanaphora, Epiphora",Frontiers Media SA,2018,[],Frontiers in Digital Humanities,5,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1869,http://dx.doi.org/10.3389/fdigh.2018.00011,"{'string_id': '10.3389/fdigh.2018.00011', 'id_scheme': 'DOI'}","Background: Increasing numbers of variables in surveys and administrative databases are created. Principal component analysis (PCA) is important to summarize data or reduce dimensionality. However, one disadvantage of using PCA is the interpretability of the principal components (PCs), especially in a high-dimensional database. By analyzing the variance distribution according to PCA loadings and approximating PCs with input variables, we aim to demonstrate the importance of variables based on the proportions of total variances contributed or explained by input variables.",Principal Component Approximation and Interpretation in Health Survey and Biobank Data,Frontiers Media SA,2018,[],Frontiers in Digital Humanities,5,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1870,http://dx.doi.org/10.3389/fdigh.2018.00014,"{'string_id': '10.3389/fdigh.2018.00014', 'id_scheme': 'DOI'}","Trust is a crucial guide in interpersonal interactions, helping people to navigate through social decision-making problems and cooperate with others. In human–computer interaction (HCI), trustworthy computer agents foster appropriate trust by supporting a match between their perceived and actual characteristics. As computers are increasingly endowed with capabilities for cooperation and intelligent problem-solving, it is critical to ask under which conditions people discern and distinguish trustworthy from untrustworthy technology. We present an interactive cooperation game framework allowing us to capture human social attributions that indicate trust in continued and interdependent human–agent cooperation. Within this framework, we experimentally examine the impact of two key dimensions of social cognition, warmth and competence, as antecedents of behavioral trust and self-reported trustworthiness attributions of intelligent computers. Our findings suggest that, first, people infer warmth attributions from unselfish vs. selfish behavior and competence attributions from competent vs. incompetent problem-solving. Second, warmth statistically mediates the relation between unselfishness and behavioral trust as well as between unselfishness and perceived trustworthiness. We discuss the possible role of human social cognition for human–computer trust.",A Social Cognition Perspective on Human–Computer Trust: The Effect of Perceived Warmth and Competence on Trust in Decision-Making With Computers,Frontiers Media SA,2018,[],Frontiers in Digital Humanities,5,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1871,http://dx.doi.org/10.3389/fdigh.2018.00015,"{'string_id': '10.3389/fdigh.2018.00015', 'id_scheme': 'DOI'}","This paper analyzes the application of LDA topic modeling to a corpus of poetry. First, it explains how the most coherent LDA-topics have been established by running several tests and automatically evaluating the coherence of the resulting LDA-topics. Results show, on one hand, that when dealing with a corpus of poetry, lemmatization is not advisable because several poetic features are lost in the process; and, on the other hand, that a standard LDA algorithm is better than a specific version of LDA for short texts (LF-LDA). The resulting LDA-topics have then been manually analyzed in order to define the relation between word topics and poems. The analysis shows that there are mainly two kinds of semantic relations: an LDA-topic could represent the subject or theme of the poem, but it could also represent a poetic motif. All these analyses have been undertaken on a large corpus of Golden Age Spanish sonnets. Finally, the paper shows the most relevant themes and motifs in this corpus such as “love,” “religion,” “heroics,” “moral,” or “mockery” on one hand, and “rhyme,” “marine,” “music,” or “painting” on the other hand.",On Poetic Topic Modeling: Extracting Themes and Motifs From a Corpus of Spanish Poetry,Frontiers Media SA,2018,[],Frontiers in Digital Humanities,5,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1872,http://dx.doi.org/10.3389/fdigh.2018.00019,"{'string_id': '10.3389/fdigh.2018.00019', 'id_scheme': 'DOI'}","Middle High German (MHG) epic poetry presents a unique solution to the linguistic changes underpinning the transition from classical Latin poetry, based on syllable length, into later vernacular rhythmic poetry, based on phonological stress. The predominating pattern in MHG verse is the alternation between stressed and unstressed syllables, but syllable length also plays a crucial role. There are a total of eight possible metrical values. Single or half mora syllables can carry any one of three types of stress, resulting in six combinations. The seventh value is a double mora, i.e., a long stressed syllable. The eighth value is an elided syllable. We construct a supervised Conditional Random Field (CRF) model to predict the metrical value of syllables, and subsequently investigate medieval German poets' use of semantic and sonorous emphasis through meter. The features used are: (1) the syllable's position within the line, (2) the syllable's length in characters, (3) the syllable's characters, (4) elision (last two characters of previous syllable and first two characters of focal syllable), (5) syllable weight, and (6) word boundaries. Additional metrical rules are enforced and marginal probabilities are calculated to yield the most likely legal scansion of a line. The model achieves a weighted average F-score of 0.925 on internal cross-validation and 0.909 on held-out testing data. We determine that trochaic alternation with a one syllable anacrusis and words carrying clear stress assignment are the easiest for the model to scan. Lines with multiple double morae of syllables with few characters are the most difficult. We then rank all the epic poetry in the Mittelhochdeutsche Begriffsdatenbank (MHDBDB) by the difficulty of the meter. Finally, we investigate the double mora, which MHG poets used to draw attention to chosen concepts. We conclude that poets generally chose to use the double mora to emphasize highly sonorant words.",A Metrical Analysis of Medieval German Poetry Using Supervised Learning,Frontiers Media SA,2018,[],Frontiers in Digital Humanities,5,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1873,http://dx.doi.org/10.3389/fdigh.2018.00023,"{'string_id': '10.3389/fdigh.2018.00023', 'id_scheme': 'DOI'}","The Design-to-Robotic-Assembly project presented in this paper showcases an integrative approach for stacking architectural elements with varied sizes in multiple directions. Several processes of parametrization, structural analysis, and robotic assembly are algorithmically integrated into a Design-to-Robotic-Production method. This method is informed by the systematic control of density, dimensionality, and directionality of the elements while taking environmental, functional, and structural requirements into consideration. It is tested by building a one-to-one prototype, which is presented and discussed in the paper with respect to the development and implementation of the computational design workflow coupled with robotic kinematic simulation that is enabling the materialization of a multidirectional and multidimensional assembly system.",Design to Robotic Assembly: An Exploration in Stacking,Frontiers Media SA,2018,[],Frontiers in Digital Humanities,5,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1874,http://dx.doi.org/10.3389/fdigh.2018.00028,"{'string_id': '10.3389/fdigh.2018.00028', 'id_scheme': 'DOI'}","The Everyday Sexism Project documents everyday examples of sexism reported by volunteer contributors from all around the world. It collected 100,000 entries in 13+ languages within the first 3 years of its existence. The content of reports in various languages submitted to Everyday Sexism is a valuable source of crowdsourced information with great potential for feminist and gender studies. In this paper, we take a computational approach to analyze the content of reports. We use topic-modeling techniques to extract emerging topics and concepts from the reports, and to map the semantic relations between those topics. The resulting picture closely resembles and adds to that arrived at through qualitative analysis, showing that this form of topic modeling could be useful for sifting through datasets that had not previously been subject to any analysis. More precisely, we come up with a map of topics for two different resolutions of our topic model and discuss the connection between the identified topics. In the low-resolution picture, for instance, we found Public space/Street, Online, Work related/Office, Transport, School, Media harassment, and Domestic abuse. Among these, the strongest connection is between Public space/Street harassment and Domestic abuse and sexism in personal relationships. The strength of the relationships between topics illustrates the fluid and ubiquitous nature of sexism, with no single experience being unrelated to another.",Topic Modeling of Everyday Sexism Project Entries,Frontiers Media SA,2019,[],Frontiers in Digital Humanities,5,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1875,http://dx.doi.org/10.3389/fdigh.2018.00029,"{'string_id': '10.3389/fdigh.2018.00029', 'id_scheme': 'DOI'}","In this paper, we present an analysis of feedback as it occurs in classroom-based and technology supported music instrument learning. Feedback is key to learning in music education and we have developed technology based on ideas from social media and audio annotation which aims to make feedback more effective. The analysis here aims to enhance our understanding of technology-mediated feedback. The result of this analysis is three ontologies describing feedback and feedback systems. First, we developed the teacher's ontology using a qualitative, observational approach to describe the types of feedback that music instrument tutors give to their students. We used this ontology to inform the design of an online music annotation platform for music students. Second, we develop the grounded ontology using a grounded theory approach, based on 2,000 annotations made by students and tutors using the annotation platform. We compare the grounded and teacher's ontologies by examining structural, semantic and expressive features. Through this comparison, we find that the grounded ontology includes elements of the teacher's ontology as well as elements relating to practical and social aspects of the annotation platform, while the teacher's ontology contains more domain knowledge. Third, we formalize the transactional capabilities of the platform into the third ontology, the platform ontology, which we have written in the OWL language, and show how this allows us to develop several practical use cases, including the use of semantic web capabilities in music education contexts.",Technology Enhanced Learning: The Role of Ontologies for Feedback in Music Performance,Frontiers Media SA,2019,[],Frontiers in Digital Humanities,5,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1876,http://dx.doi.org/10.3389/fdigh.2019.00003,"{'string_id': '10.3389/fdigh.2019.00003', 'id_scheme': 'DOI'}","Metaphors are commonly used in interface design within Human-Computer Interaction (HCI). Interface metaphors provide users with a way to interact with the computer that resembles a known activity, giving instantaneous knowledge or intuition about how the interaction works. A widely used one in Digital Musical Instruments (DMIs) is the conductor-orchestra metaphor, where the orchestra is considered as an instrument controlled by the movements of the conductor. We propose a DMI based on the conductor metaphor that allows to control tempo and dynamics and adapts its mapping specifically for each user by observing spontaneous conducting movements (i.e., movements performed on top of fixed music without any instructions). We refer to this as mapping by observation given that, even though the system is trained specifically for each user, this training is not done explicitly and consciously by the user. More specifically, the system adapts its mapping based on the tendency of the user to anticipate or fall behind the beat and observing the Motion Capture descriptors that best correlate to loudness during spontaneous conducting. We evaluate the proposed system in an experiment with twenty four (24) participants where we compare it with a baseline that does not perform this user-specific adaptation. The comparison is done in a context where the user does not receive instructions and, instead, is allowed to discover by playing. We evaluate objective and subjective measures from tasks where participants have to make the orchestra play at different loudness levels or in synchrony with a metronome. Results of the experiment prove that the usability of the system that automatically learns its mapping from spontaneous movements is better both in terms of providing a more intuitive control over loudness and a more precise control over beat timing. Interestingly, the results also show a strong correlation between measures taken from the data used for training and the improvement introduced by the adapting system. This indicates that it is possible to estimate in advance how useful the observation of spontaneous movements is to build user-specific adaptations. This opens interesting directions for creating more intuitive and expressive DMIs, particularly in public installations.",Mapping by Observation: Building a User-Tailored Conducting System From Spontaneous Movements,Frontiers Media SA,2019,[],Frontiers in Digital Humanities,6,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1877,http://dx.doi.org/10.3389/fdigh.2019.00004,"{'string_id': '10.3389/fdigh.2019.00004', 'id_scheme': 'DOI'}","The promise of digitization of historical archives lies in their indexation at the level of contents. Unfortunately, this kind of indexation does not scale, if done manually. In this article we present a method to bootstrap the deployment of a content-based information system for digitized historical archives, relying on historical indexing tools. Commonly prepared to search within homogeneous records when the archive was still current, such indexes were as widespread as they were disconnected, that is to say situated in the very records they were meant to index. We first present a conceptual model to describe and manipulate historical indexing tools. We then introduce a methodological framework for their use in order to guide digitization campaigns and index digitized historical records. Finally, we exemplify the approach with a case study on the indexation system of the X Savi alle Decime in Rialto, a Venetian magistracy in charge for the exaction—and related record keeping—of a tax on real estate in early modern Venice.",Index-Driven Digitization and Indexation of Historical Archives,Frontiers Media SA,2019,[],Frontiers in Digital Humanities,6,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1878,http://dx.doi.org/10.3389/fdigh.2019.00008,"{'string_id': '10.3389/fdigh.2019.00008', 'id_scheme': 'DOI'}","Vast in scale and densely inhabited, Late Neolithic Near Eastern megasites have been variously considered in relation to urbanity. Often viewed as failed experiments on the path to proper urbanism or proto-urban sites, these settlements reveal few signs of hierarchical social stratification despite their large size; as such, they represent a challenge for the understanding of early processes of community formation and social integration. Drawing upon a wide range of data and using socio-material network analysis as a methodological tool, this paper explores the way the late Neolithic site of Çatalhöyük was organized internally and specifically the way individual houses were embedded in the wider social fabric of the site. This study sheds light on the nature of the networks of social engagement and affiliation that emerge in the Holocene within large early agricultural communities and the way such networks were manifested.",Socio-Material Archaeological Networks at Çatalhöyük a Community Detection Approach,Frontiers Media SA,2019,[],Frontiers in Digital Humanities,6,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1879,http://dx.doi.org/10.3389/fdigh.2019.00009,"{'string_id': '10.3389/fdigh.2019.00009', 'id_scheme': 'DOI'}","Immersive technology, such as virtual reality, provides us with novel opportunities to create and explore affective experiences with a transformative potential mediated through awe. The profound emotion of awe, that is experienced in response to witnessing vastness and creates the need for accommodation that can lead to restructuring of one's worldview and an increased feeling of connectedness. An iconic example of the powers of awe is observed in astronauts who develop instant social consciousness and strong pro-environmental values in response to the overwhelming beauty of Earth observed from space. Here on Earth, awe can also be experienced in response to observing vast natural phenomenon or even sometimes in response to some forms of art, presenting vast beauty to its audience. Can virtual reality provide a new powerful tool for reliably inducing such experiences? What are some unique potentials of this emerging medium? This paper describes the evaluation of an immersive installation “AWE”—Awe-inspiring Wellness Environment. The results indicate that the experience of being in “AWE” can elicit some components of awe emotion and induce minor cognitive shifts in participant's worldview similar to the Overview Effect, while this experience also has its own attributes that might be unique to this specific medium. Comparing the results of this exploratory study to other virtual environments designed to elicit Overview Effect provides insights on the relationship between design features and participant's experience. The qualitative results highlight the importance of perceived safety, personal background and familiarity with the environment, and the induction of a small visceral fear reaction as a part of the emotional arc of the virtual journey—as some of the key contributers to the affective experience of the immersive installation. Even though the observed components of awe and a few indications of cognitive shift support the potential of Virtual Reality as a transformative medium, many more iterations of the design and research tools are required before we can achieve and fully explore a profound awe-inspiring transformative experience mediated through immersive technologies.","Understanding AWE: Can a Virtual Journey, Inspired by the Overview Effect, Lead to an Increased Sense of Interconnectedness?",Frontiers Media SA,2019,[],Frontiers in Digital Humanities,6,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1880,http://dx.doi.org/10.3389/fdigh.2019.00010,"{'string_id': '10.3389/fdigh.2019.00010', 'id_scheme': 'DOI'}","The Trypillia megasites of Ukraine are the largest known settlements in 4th millennium BC Europe and possibly the world. With the largest reaching 320 ha in size, megasites pose a serious question about the origins of such massive agglomerations. Most current solutions assume maximum occupation, with all houses occupied at the same time, and target defence against other agglomerations as the cause of their formation. However, recent alternative views of megasites posit smaller long-term occupations or seasonal assembly places, creating a settlement rather than military perspective on origins. Shukurov et al. (2015)'s model of Trypillia arable land-use demonstrates that subsistence stresses begin when site size exceeded 35 ha. Over half of the sites dated to the Trypillia BI stage—the stage before the first megasites—were larger than 35 ha, suggesting that some form of buffering involving exchange of goods for food was in operation. There were two settlement responses to buffering:- clustering of sites with enhanced inter-site exchange networks and the creation of megasites. The trend to increased site clustering can be seen from Phase BI to CI, coeval with the emergence of megasites. We can therefore re-focus the issue of origins on why create megasites in site clusters. In this article, we discuss the two strategies in terms of informal network analysis and suggest reasons why, in some cases, megasites developed in certain site clusters. Finally, we consider the question of whether Trypillia megasites can be considered as “cities.”",The Origins of Trypillia Megasites,Frontiers Media SA,2019,[],Frontiers in Digital Humanities,6,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1881,http://dx.doi.org/10.3389/fdigh.2019.00011,"{'string_id': '10.3389/fdigh.2019.00011', 'id_scheme': 'DOI'}","Empirical studies of ancient cities must break down communities into their component parts, but frequently encounter difficulty with the scarcity of excavated domestic structures (e.g., Kramer, 1982, p. 673). I introduce to the archaeological literature the entropy estimating statistical bootstrap (EESB), a tool developed in information theory and computational social science by DeDeo et al. (2013) which provides a way to assess how representative a small dataset is of a parent population, categorized according to some useful typology. This method can be used to decide when small datasets can add further detail to our quantitative studies of archaeological settlements or when they need to be rejected as too small. I then illustrate its uses within the context of urban demography by examining the distribution of house forms to calculate household characteristics specific to Metapontum, an ancient Greek city. Future applications will include building larger urban datasets that are empirically grounded in the specific evidence for each community, facilitating the work of research programs such as urban scaling.","More Real Than Ideal: Household and Community Diversity at Metapontum, South Italy",Frontiers Media SA,2019,[],Frontiers in Digital Humanities,6,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1882,http://dx.doi.org/10.3389/fdigh.2019.00014,"{'string_id': '10.3389/fdigh.2019.00014', 'id_scheme': 'DOI'}","The conventional history of urban growth defines agrarian-based cities prior to the nineteenth century CE as densely inhabited and commonly bounded by defenses such as walls. By contrast industrial-based cities are viewed as more spread out and without marked boundaries. Since the 1960s a trajectory toward extensive, low-density urbanism with sprawling, scattered suburbs surrounding a denser core has been formally recognized and given various names such as megalopolis in the West and desakota in southern and eastern Asia. These sprawling industrial cities have been regarded as a unique derivative of modern phenomena such as mechanized transport and the commercial property market. However, this set of premises are not valid. The agrarian-based world also contained dispersed, low-density urbanism—on its grandest scale, the vast circa 1,000 sq km urban complex of Greater Angkor and the famous Maya cities of lowland Central America with maximum areas of about 200 sq km. The Maya only used pedestrian and riverine transport so the conventional transport explanation for industrial dispersed urbanism is at best partial. There was another trajectory to extensive, low-density settlement forms for places which were generally &lt;15–20 sq km in extent but could on rare occasions reach areas as large as 40 to 90 sq km. Famous examples are Great Zimbabwe, Chaco Canyon and the European oppida of the late 1st millennium BCE. No-formally agreed term is available to refer to them. I will refer to them by default as “Giants.” The three trajectories to low-density settlement form redefine the history of settlement growth and the meanings of the term “urban.” Worryingly, none of the successive low-density settlements derive from any of the low-density cases of the preceding trajectory. Neither Angkor nor the Classic Maya cities have any connection to the industrial low-density cities. By contrast compact cities, the epitome of the obsolete definition of cities display continuity to succeeding urban forms over several 1,000 years. The implications for modern, giant, low-density cities are ominous.",Trajectories to Low-Density Settlements Past and Present: Paradox and Outcomes,Frontiers Media SA,2019,[],Frontiers in Digital Humanities,6,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1883,http://dx.doi.org/10.3389/fdigh.2019.00015,"{'string_id': '10.3389/fdigh.2019.00015', 'id_scheme': 'DOI'}","The rise of the state in Ancient Italy went hand in hand with an increase in infrastructural power, i.e., settlement centralization and urbanization. The paper discusses theoretical challenges and introduces a modeling approach to a case study, one of the earliest cities in Southern Italy, Pontecagnano, with the aim of understanding the community dynamics at the time of the earliest urbanization (ca. 900–600 BC). The model is a two-mode model that derives from social network analysis, an approach that has been fruitfully adapted to archaeological research. The model is applied to detect trends in burial contexts from the community involved. Burial was, at that time, in the region, a key instrument in the creation of memory and display of status and thus for building and consolidating state power. The analytical network model is able to detect the dynamics in the community over time very well: network Cohesion is expanding and contracting, and points to the existence of tension and a tight control of funerary behavior. The study of Centrality of selected nodes provides a good understanding of the strategies in terms of the circulation of key resources. The latter is particularly significant for studying urbanization because the appropriation of resources was not possible without centralization and the development of infrastructure, as well as an ideology. Based on the study of selected resources, it is suggested that an increase in crop storage has played a particular role in the development of state power and the urbanization process at Pontecagnano. In due course, the paper also addresses methodological challenges of working with fragmented datasets when applying models to study the past.",Modeling the Rise of the City: Early Urban Networks in Southern Italy,Frontiers Media SA,2019,[],Frontiers in Digital Humanities,6,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1884,http://dx.doi.org/10.3389/fdigh.2019.00017,"{'string_id': '10.3389/fdigh.2019.00017', 'id_scheme': 'DOI'}","Comparative studies of cities throughout history are one of the greatest sources of insight into the nature of change in human societies. This paper discusses strategies to anchor these comparisons on well-defined, quantitative and empirical characteristics of cities, derived from theory and observable in the archeological and historical records. We show how quantitative comparisons based on a few simple variables across settlements allow us to analyze how different places and peoples dealt with general problems of any society. These include demographic change, the organization of built spaces, the intensity and size of socioeconomic networks and the processes underlying technological change and economic growth. Because the historical record contains a much more varied and more independent set of experiences than contemporary urbanization, it has a unique power for illuminating present puzzles of human development and testing emergent urban theory.",Quantitative Methods for the Comparative Analysis of Cities in History,Frontiers Media SA,2019,[],Frontiers in Digital Humanities,6,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1885,http://dx.doi.org/10.3389/fdigh.2019.00018,"{'string_id': '10.3389/fdigh.2019.00018', 'id_scheme': 'DOI'}","This article investigates the urban expansion and economic development of ancient Rome through the application of models and theories originally designed for the study of contemporary cities. While the growth of ancient settlements is often difficult to track and analyze, archaeologically observable changes in land use can be read and interpreted as a function of broader economic oscillations over the longue durée. By re-examining the available archaeological and textual evidence pertaining to land use change on Rome's eastern periphery this article demonstrates how the frameworks selected can be successfully appropriated via a narration of Rome's urban transformations from the mid-Republic to the later Imperial period. The ultimate goal is to determine if the patterns of urban expansion identified in modern cities also existed in ancient Rome. The findings provided have the potential to produce rich insights on the dynamics of urban and economic growth across time and geographies, thereby opening the door for new and further studies.","Ancient City, Universal Growth? Exploring Urban Expansion and Economic Development on Rome's Eastern Periphery",Frontiers Media SA,2019,[],Frontiers in Digital Humanities,6,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1886,http://dx.doi.org/10.3389/fdigh.2020.00001,"{'string_id': '10.3389/fdigh.2020.00001', 'id_scheme': 'DOI'}","This paper examines the patterns of Etruscan urbanism by the innovative use of newly available rural data, employing rank size, and indices of centralization. The detailed case study looks at the development of urbanism of pre-Roman Etruria where both robust and delicate urbanism were present alongside one another. To achieve this end, the paper will draw on the complementary features of two recent articles—Redhouse and Stoddart (2011) and Palmisano et al. (2018)—to provide a synthesis that both examines the large places and the supporting rural settlement. The territorial boundaries of the major urban places were predicted by the XTENT model in the first article. The cumulative numbers of rural settlement (and other proxies of population) over time were examined in the second article. This paper will look at the regional variation in landscape organization within the predicted territorial boundaries of the major robust centres and the more delicate transitory centres, as well as the buffer zones in between. At least three phases of boundary development can be examined, equivalent broadly to the Iron Age, Orientalizing/Archaic and Post Archaic periods, seeking to match these with the correspondingly dated rural settlement. The results will be critically examined in terms of broader knowledge of the economic and political development from current fieldwork in Etruria. The ethnographic analysis of Kopytoff (1989) will also be applied to assess the application of the internal African frontier to the central Italian context. In this way, the quantitative will be matched with the qualitative to provide a deeper understanding of urban development in an under-assessed example within the Mediterranean world.",Patterns of Etruscan Urbanism,Frontiers Media SA,2020,[],Frontiers in Digital Humanities,7,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1887,http://dx.doi.org/10.3389/fdigh.2020.00002,"{'string_id': '10.3389/fdigh.2020.00002', 'id_scheme': 'DOI'}","In the past numerous concepts of urbanity have been discussed and a variety of criteria for towns have been developed. They include size, population, legal aspects, way of life, structural and functional approaches. However, since the mentioned criteria cover only a part of the phenomenon and partly use fixed and arbitrary thresholds, they are not sufficient for analysis. We turn to an understanding of urbanity as a process that creates and shapes the scenery of the buildings and people and that is mainly driven by complexity. In this sense, we understand urbanity as a process of adaptation to changing conditions or contexts in a complex settlement system, which is triggered by size, attracted by exemplary solutions and characterized by the emergence of new structures. In this paper we address the issue of relative centrality as proposed by Christaller in the urbanity process as well as centrality within a network sciences approach. Our aim is to interweave different concepts of urbanity, centrality, interaction and connectivity, combining different concepts and research traditions as well as expanding them, resulting in a collection of different terminological frameworks. In the context of adaptation, urbanity is relative in the sense that different places may have gained better or worse adaptation under different conditions. The urbanity process is always shaped by the threat of too much complexity and too little connectivity. Above all, it is a certain surplus of connectivity that characterizes urbanity. This surplus is mapped by the variant of centrality proposed by Christaller. While Christaller's models can be transferred into network sciences frameworks, Christaller does not offer an adequate centrality measure. Therefore, his concept of centrality cannot be transferred correctly without being translated carefully into the network research context. In this article, we argue why this is necessary and explain how it can be done. In this paper the above concept will be applied to the Early Iron Age Princely Seats with a special focus on the Heuneburg. In order to represent similarities and interaction between different nodes a very limited part of the material culture can be used. For this purpose we use fibulae which allow for fairly accurate dating and hence ensure a narrow time slice for the network analysis. Using Fibulae the research will be limited to a certain social segment, which we refer to as “middle class.” This paper is intended to deal with the rather complex issue of urbanity using more simple approaches such as network analysis. In this context, we pursue a tight integration of theory and methodology and we consider certain conceptual issues. This paper has two main results. Firstly, we develop a consistent approach in order to apply social network centrality measures on geographical networks. Secondly, we will analyse which role the above mentioned middle class played in the course of urbanity processes.",Urbanity as a Process and the Role of Relative Network Properties—A Case Study From the Early Iron Age,Frontiers Media SA,2020,[],Frontiers in Digital Humanities,7,,"[{'value': '2297-2668', 'type': 'electronic'}]"
1888,http://dx.doi.org/10.4000/jtei.476,"{'string_id': '10.4000/jtei.476', 'id_scheme': 'DOI'}","The paper presents an XML schema for the representation of genres of computer-mediated communication (CMC) that is compliant with the encoding framework defined by the TEI. It was designed for the annotation of CMC documents in the project Deutsches Referenzkorpus zur internetbasierten Kommunikation (DeRiK), which aims at building a corpus on language use in the most popular CMC genres on the German-speaking Internet. The focus of the schema is on those CMC genres which are written and dialogic―such as forums, bulletin boards, chats, instant messaging, wiki and weblog discussions, microblogging on Twitter, and conversation on “social network” sites.",A TEI Schema for the Representation of Computer-mediated Communication,OpenEdition,2012,,Journal of the Text Encoding Initiative,,Issue 3,"[{'value': '2162-5603', 'type': 'electronic'}]"
1889,http://dx.doi.org/10.4000/jtei.486,"{'string_id': '10.4000/jtei.486', 'id_scheme': 'DOI'}","The present contribution addresses an infrastructural issue of universal relevance, addressed in the specific context of the TEI. We describe a combination of open-source tools and an open-access approach to creating knowledge repositories that have been employed in building a bibliographic reference library for the “TEI for Linguists” special interest group (LingSIG). The authors argue that, for an initiative such as the TEI, it is important to choose open, freely available solutions. If these solutions have the advantage of attracting new users and promoting the initiative itself, so much the better, especially if it is done in a non-committal way: no one using the LingSIG bibliographic repository has to be a member of the LingSIG or a “TEI-er” in general.",Building and Maintaining the TEI LingSIG Bibliography,OpenEdition,2012,,Journal of the Text Encoding Initiative,,Issue 3,"[{'value': '2162-5603', 'type': 'electronic'}]"
1890,http://dx.doi.org/10.4000/jtei.508,"{'string_id': '10.4000/jtei.508', 'id_scheme': 'DOI'}","This paper describes work in progress on I5, a TEI-based document grammar for the corpus holdings of the Institut für Deutsche Sprache (IDS) in Mannheim and the text model used by IDS in its work. The paper begins with background information on the nature and purposes of the corpora collected at IDS and the motivation for the I5 project (section 1). It continues with a description of the origin and history of the IDS text model (section 2), and a description (section 3) of the techniques used to automate, as far as possible, the preparation of the ODD file documenting the IDS text model. It ends with some concluding remarks (section 4). A survey of the additional features of the IDS-XCES realization of the IDS text model is given in an appendix.",A TEI P5 Document Grammar for the IDS Text Model,OpenEdition,2012,,Journal of the Text Encoding Initiative,,Issue 3,"[{'value': '2162-5603', 'type': 'electronic'}]"
1891,http://dx.doi.org/10.4000/jtei.522,"{'string_id': '10.4000/jtei.522', 'id_scheme': 'DOI'}","Although most of the relevant dictionary productions of the recent past have relied on digital data and methods, there is little consensus on formats and standards. The Institute for Corpus Linguistics and Text Technology (ICLTT) of the Austrian Academy of Sciences has been conducting a number of varied lexicographic projects, both digitising print dictionaries and working on the creation of genuinely digital lexicographic data. This data was designed to serve varying purposes: machine-readability was only one. A second goal was interoperability with digital NLP tools. To achieve this end, a uniform encoding system applicable across all the projects was developed. The paper describes the constraints imposed on the content models of the various elements of the TEI dictionary module and provides arguments in favour of TEI P5 as an encoding system not only being used to represent digitised print dictionaries but also for NLP purposes.",Creating Lexical Resources in TEI P5,OpenEdition,2012,,Journal of the Text Encoding Initiative,,Issue 3,"[{'value': '2162-5603', 'type': 'electronic'}]"
1892,http://dx.doi.org/10.4000/jtei.523,"{'string_id': '10.4000/jtei.523', 'id_scheme': 'DOI'}","The TEI has served for many years as a mature annotation format for corpora of different types, including linguistically annotated data. Although it is based on the consensus of a large community, it does not have the legal status of a standard. During the last decade, efforts have been undertaken to develop definitive de jure standards for linguistic data that not only act as a normative basis for the exchange of language corpora but also address recent advancements in technology, such as web-based standards, and the use of large and multiply annotated corpora.",The TEI and Current Standards for Structuring Linguistic Data,OpenEdition,2012,,Journal of the Text Encoding Initiative,,Issue 3,"[{'value': '2162-5603', 'type': 'electronic'}]"
1893,http://dx.doi.org/10.4000/jtei.540,"{'string_id': '10.4000/jtei.540', 'id_scheme': 'DOI'}","Our paper outlines a proposal for the consistent modeling of heterogeneous lexical structures in semasiological dictionaries, based on the element structures described in detail in chapter 9 (Dictionaries) of the TEI Guidelines. The core of our proposal describes a system of relatively autonomous lexical “crystals” that can, within the constraints of the relevant element’s definition, be combined to form complex structures for the description of morphological form, grammatical information, etymology, word-formation, and meaning for a lexical structure.",Consistent Modeling of Heterogeneous Lexical Structures,OpenEdition,2012,,Journal of the Text Encoding Initiative,,Issue 3,"[{'value': '2162-5603', 'type': 'electronic'}]"
1894,http://dx.doi.org/10.4000/jtei.774,"{'string_id': '10.4000/jtei.774', 'id_scheme': 'DOI'}","A variety of initiatives for developing virtual research environments, research infrastructures, and cyberinfrastructures have been funded in recent years. The systems produced vary considerably, but they all face the issue of sustainability, namely how to ensure the continued existence of a resource once the project that created it has finished. This paper addresses the sustainability issues faced by the TextGrid and TEXTvre virtual research environments for textual scholarship, examining the inter-project collaboration and cross-fertilization that took place, and investigating how the projects benefited from this exchange. It also examines how their sustainability is being facilitated by the more general-purpose DARIAH infrastructure, and conversely how their existing collaboration can serve as a model for future collaborations within the DARIAH community.","TextGrid, TEXTvre, and DARIAH: Sustainability of Infrastructures for Textual Scholarship",OpenEdition,2013,,Journal of the Text Encoding Initiative,,Issue 5,"[{'value': '2162-5603', 'type': 'electronic'}]"
1895,http://dx.doi.org/10.4000/jtei.787,"{'string_id': '10.4000/jtei.787', 'id_scheme': 'DOI'}","Project Bamboo, a cyberinfrastructure initiative supported by the Andrew W. Mellon Foundation, takes as its core mission the enhancement of arts and humanities research through the development of shared technology services. Rather than developing new tools for curating or analyzing data, Project Bamboo aims to provide core infrastructure services including identity and access management, collection interoperability, and scholarly data management. The longer-term goal is for many organizations and projects to leverage those services so as to direct their own resources towards innovative tool or collection development. In addition, Bamboo seeks to model a paradigm for tool integration that focuses on tools as discrete services (such as a morphology annotation service and a geoparser service, instead of a web-based environment that does morphological annotation and geoparsing) that can be applied to texts, individually or in combination with other services, to enable complex curatorial and analytical workflows. This paper addresses points of intersection between Project Bamboo and TEI over the course of Bamboo's development, including the role of TEI in Bamboo's ongoing development work. The paper highlights the significant contributions of the TEI community to the early development of the project through active participation in the Bamboo Planning Project. The paper also addresses the influence of TEI on the Bamboo Technology Project's collection interoperability and corpus curation/analysis initiatives, as well as its role in current (as of October 2012) development work.",TEI and Project Bamboo,OpenEdition,2013,,Journal of the Text Encoding Initiative,,Issue 5,"[{'value': '2162-5603', 'type': 'electronic'}]"
1896,http://dx.doi.org/10.4000/jtei.788,"{'string_id': '10.4000/jtei.788', 'id_scheme': 'DOI'}","This project report details the goals and development of the TEI Archiving, Publishing, and Access Service (TAPAS). The project's mission centers on providing repository and publication services to individuals and small projects working with TEI data: a constituency that typically lacks access to institutional resources such as server space, XML expertise, and programming time. In developing this service, TAPAS addresses several important challenges: the creation of a publication ecology that operates gracefully at the level of the individual project and the TAPAS corpus; the problem of the vulnerability of TEI data in cases where projects cease their activity; and the variability and complexity of TEI data. When completed, the TAPAS service will enable its contributors to upload, manage, and publish their TEI data, and it will offer a publication interface through which both individual project collections and the TAPAS collection as a whole can be read, searched, and explored. It will also provide a range of related services such as technical consulting, data curation and conversion work, TEI workshops, tutorials, and community forums. This article discusses the philosophy and rationale behind the project's current development efforts, and examines some of the challenges the project faces. ",TAPAS: Building a TEI Publishing and Repository Service,OpenEdition,2013,,Journal of the Text Encoding Initiative,,Issue 5,"[{'value': '2162-5603', 'type': 'electronic'}]"
1897,http://dx.doi.org/10.4000/jtei.790,"{'string_id': '10.4000/jtei.790', 'id_scheme': 'DOI'}","Islandora is an open-source software framework developed since 2006 by the University of Prince Edward Island's Robertson Library. The Islandora framework is designed to ease the management of security and workflow for digital assets, and to help implementers create custom interfaces for display, search, and discovery. Turnkey options are provided via tools and modules (""solution packs"") designed to support the work of a particular knowledge domain (such as chemistry), a particular content type (such as a digitized newspaper), or a particular task (such as TEI encoding). While it does not yet have native support for TEI, Islandora provides a promising basis on which digital humanities scholars could manage the creation, editing, validation, display, and comparison of TEI-encoded text. UPEI's IslandLives project, with its forthcoming solution pack, provides insight into how an Islandora version 6 installation can support OCR text extraction, automatic structural/semantic encoding of text, and web-based TEI editing and display functions for site administrators. This article introduces the Islandora framework and its suitability for TEI, describes the IslandLives approach in detail, and briefly discusses recent work and future directions for TEI work in Islandora. The authors hope that interested readers may help contribute to the expansion of TEI-related services and features available to be used with Islandora.",Islandora and TEI: Current and Emerging Applications/Approaches,OpenEdition,2013,,Journal of the Text Encoding Initiative,,Issue 5,"[{'value': '2162-5603', 'type': 'electronic'}]"
1898,http://dx.doi.org/10.4000/jtei.811,"{'string_id': '10.4000/jtei.811', 'id_scheme': 'DOI'}","It is twenty-five years since the Text Encoding Initiative was first launched as a research project following an international conference funded by the US National Endowment for the Humanities. This article describes some key stages in its subsequent evolution from research project into research infrastructure. The TEI's changing nature, we suggest, is partly a consequence of its close and highly responsive relation with an active user community, which may also explain both its longevity and its effectiveness as a part of the digital humanities research infrastructure.",The Evolution of the Text Encoding Initiative: From Research Project to Research Infrastructure,OpenEdition,2013,,Journal of the Text Encoding Initiative,,Issue 5,"[{'value': '2162-5603', 'type': 'electronic'}]"
1899,http://dx.doi.org/10.4000/jtei.817,"{'string_id': '10.4000/jtei.817', 'id_scheme': 'DOI'}","A common problem for TEI software development is that projects develop their own custom software stack to address the semantic intricacies present in a deeply-encoded TEI corpus. This article describes the design of version 4 of the PhiloLogic corpus query engine, which is designed to handle heterogeneous TEI encoding through its redesigned abstract data model. We show that such an architecture has substantial benefits for software reuse, allowing for powerful TEI applications to be adapted to new corpora with a minimum of custom programming, and we discuss the more general and theoretical implications of abstraction as a TEI processing technique.",PhiloLogic4: An Abstract TEI Query System,OpenEdition,2013,,Journal of the Text Encoding Initiative,,Issue 5,"[{'value': '2162-5603', 'type': 'electronic'}]"
1900,http://dx.doi.org/10.4000/jtei.949,"{'string_id': '10.4000/jtei.949', 'id_scheme': 'DOI'}","As part of a larger pilot study on the evaluation of digital scholarship, we consider what role, if any, the TEI Consortium and user community might play in evaluating scholarship that utilize the TEI tag set. Our rationale for focusing on the role of the TEI Consortium in the discussion of evaluation is twofold. First, the TEI Guidelines represents an encoding standard for texts that is supported by a large community actively interested in the application and development of these standards. Second, feedback concerning evaluation criteria for digital scholarship has not been explicitly gathered from the TEI community and may provide additional understanding of the value, process, and assessment of text encoding. Determining what to evaluate and how to do so reveals the community’s definitions of scholarship in general. The clarification and articulation of evaluation criteria, therefore, remains a high priority as digital scholarship continues to develop.",Evaluating Digital Scholarship: Suggestions and Strategies for the Text Encoding Initiative,OpenEdition,2014,,Journal of the Text Encoding Initiative,,Issue 7,"[{'value': '2162-5603', 'type': 'electronic'}]"
1901,http://dx.doi.org/10.4000/jtei.968,"{'string_id': '10.4000/jtei.968', 'id_scheme': 'DOI'}","This article provides a brief overview of currently-available digital resources for learning to understand and use the TEI Guidelines. It reflects on and analyzes these resources and their audience through the results of a survey intended to inform future support from the TEI Consortium for novice users. Increasing numbers of students look online for self-directed and task-based tutorials, and increasing numbers of scholars in the humanities recognize the TEI Guidelines as a standard tool for publication and analysis. In this context, the author designed the survey presented in this paper to solicit qualitative feedback from both experienced and aspiring practitioners in the field concerning their skills, needs, and goals, pedagogical as well as technical. The article suggests revising and expanding TEI community resources, proposing possibilities for their new form and functionality.",Learning the TEI in a Digital Environment,OpenEdition,2014,,Journal of the Text Encoding Initiative,,Issue 7,"[{'value': '2162-5603', 'type': 'electronic'}]"
1902,http://dx.doi.org/10.4000/jtei.974,"{'string_id': '10.4000/jtei.974', 'id_scheme': 'DOI'}","This paper attempts to make two contributions to discussions related to TEI: (1) an analysis of how tools used for working with TEI documents encourage certain values and make certain assumptions about the work of textual editing and (2) a report on a methodological framework from outside the humanities that suggests a unique way to study such systems. Borrowing models of design research from the fields of design and human-computer interaction, I argue that prototypes can be used to create new conceptual knowledge, to investigate the values and assumptions of sociotechnical systems, and to communicate alternative visions of those systems. I first analyze an existing tool, the Versioning Machine, as a way of focusing the design of a prototype that reimagines several aspects of that original—specifically, I argue that the Versioning Machine creates an environment that to some extent assumes that TEI documents are created by one editor and intended for one instantiation. The prototype presented experiments with an alternative vision of textual editing as bringing encoded texts and interpretations together in multiple and flexible instantiations. Rather than a technical problem with an optimal solution, I approach this design process as an opportunity to ask how prototypes can give designers access to conceptual issues and allow users to enact alternative values and imagine alternative futures. This research was supported by the Modernist Versions Project, which is funded by a Social Sciences and Humanities Research Council of Canada Partnership Development Grant.",A Design Methodology for Exploring and Communicating System Values and Assumptions,OpenEdition,2014,,Journal of the Text Encoding Initiative,,Issue 7,"[{'value': '2162-5603', 'type': 'electronic'}]"
1903,http://dx.doi.org/10.4000/jtei.979,"{'string_id': '10.4000/jtei.979', 'id_scheme': 'DOI'}","Recent proposals for creating digital scholarly editions (DSEs) through the crowdsourcing of transcriptions and collaborative scholarship, for the establishment of national repositories of digital humanities data, and for the referencing, sharing, and storage of DSEs, have underlined the need for greater data interoperability. The TEI Guidelines have tried to establish standards for encoding transcriptions since 1988. However, because the choice of tags is guided by human interpretation, TEI-XML encoded files are in general not interoperable. One way to fix this problem may be to break down the current all-in-one approach to encoding so that DSEs can be specified instead by a bundle of separate resources that together offer greater interoperability: plain text versions, markup, annotations, and metadata. This would facilitate not only the development of more general software for handling DSEs, but also enable existing programs that already handle these kinds of data to function more efficiently.",Towards an Interoperable Digital Scholarly Edition,OpenEdition,2014,,Journal of the Text Encoding Initiative,,Issue 7,"[{'value': '2162-5603', 'type': 'electronic'}]"
1904,http://doi.org/10.5334/johd.12,"{'string_id': '10.5334/johd.12', 'id_scheme': 'DOI'}","Historical language comparison opens windows onto a human past, long before the availability of written records. Since traditional language comparison within the framework of the comparative method is largely based on manual data comparison, requiring the meticulous sifting through dictionaries, word lists, and grammars, the framework is difficult to apply, especially in times where more and more data have become available in digital form. Unfortunately, it is not possible to simply automate the process of historical language comparison, not only because computational solutions lag behind human judgments in historical linguistics, but also because they lack the flexibility that would allow them to integrate various types of information from various kinds of sources. A more promising approach is to integrate computational and classical approaches within a computer-assisted framework, “neither completely computer-driven nor ignorant of the assistance computers afford” [1, p. 4]. In this paper, we will illustrate what we consider the current state of the art of computer-assisted language comparison by presenting a workflow that starts with raw data and leads up to a stage where sound correspondence patterns across multiple languages have been identified and can be readily presented, inspected, and discussed. We illustrate this workflow with the help of a newly prepared dataset on Hmong-Mien languages. Our illustration is accompanied by Python code and instructions on how to use additional web-based tools we developed so that users can apply our workflow for their own purposes.",Computer-Assisted Language Comparison: State of the Art,Ubiquity Press,2020,"['computer-assisted', 'language comparison', 'historical linguistics', 'Hmong-Mien language family']",Journal of Open Humanities Data,6,1,"[{'value': '2059-481X', 'type': 'electronic'}]"
1905,http://doi.org/10.5334/johd.16,"{'string_id': '10.5334/johd.16', 'id_scheme': 'DOI'}","Dramatic changes in the ethnic composition of countries in the last decades have sparked new interest among social scientists in studying and uncovering the role of ethnic diversity on social, political and economic outcomes. Yet, most ethnic fractionalization indices used by scholars to study these effects treat ethnic heterogeneity as time-invariant, thus concealing its long-term effects. However, failing to take into account historical developments in ethnic composition might seriously hinder our understanding of their effects on social, economic, and political outcomes. This paper introduces a new dataset containing an annual ethnic fractionalization index for 162 countries across all continents in the period of 1945–2013. The Historical Index of Ethnic Fractionalization (HIEF) dataset is a natural extension of previous ethnic fractionalization indices. It offers the opportunity to study the effects of ethnic fractionalization across countries and over time. The article concludes by offering some preliminary descriptive analysis of patterns of change in ethnic fractionalization over time.",Introducing the Historical Index of Ethnic Fractionalization (HIEF) Dataset: Accounting for Longitudinal Changes in Ethnic Diversity,Ubiquity Press,2020,"['ethnic fractionalization', 'ethnic diversity', 'ethnic groups', 'intergroup relations']",Journal of Open Humanities Data,6,1,"[{'value': '2059-481X', 'type': 'electronic'}]"
1906,http://doi.org/10.5334/johd.3,"{'string_id': '10.5334/johd.3', 'id_scheme': 'DOI'}","The HathiTrust Digital Library (HTDL) is a digital library containing about 14 million volumes which comprise billions of pages of content. The HathiTrust Research Center (HTRC) is a collaborative research initiative jointly led by Indiana University and the University of Illinois at Urbana-Champaign. This paper describes the development of a collections data model by the Workset Creation for Scholarly Analysis project, a HTRC research initiative funded by the Andrew W. Mellon Foundation. The resulting HTRC Workset data model is designed to aid humanities scholars by helping them to describe selected portions of the HTDL corpus that serve as the objects of their research. The resulting worksets are persistent, citable, and can be assessed by other scholars for reuse in additional research processes.",The HathiTrust Research Center Workset Ontology: A Descriptive Framework for Non-Consumptive Research Collections,Ubiquity Press,2016,"['research collections', 'formalisms', 'data models', 'digital libraries', 'large-scale corpuses']",Journal of Open Humanities Data,2,,"[{'value': '2059-481X', 'type': 'electronic'}]"
1907,http://dx.doi.org/10.5399/uo/hsda.3.1.1626,"{'string_id': '10.5399/uo/hsda.3.1.1626', 'id_scheme': 'DOI'}","The aim of this paper is to present the activities and research methodologies of CTL (Centro di Elaborazione Informatica di Testi e Immagini nella Tradizione Letteraria), a laboratory of the Scuola Normale Superiore of Pisa, founded and directed by Lina Bolzoni. CTL’s objective is to investigate the complex structure of relationships between the linguistic and the figurative code in literary tradition, paying particular attention to the XVth, XVIth and XVIIth centuries and using information technologies both as an auxiliary research tool and as a medium for scientific dissemination. Here I shall be discussing the Centre’s digitalization projects, and in particular the project concerned with the Orlando Furioso and its remediation into images. This project, active since 2007, is, I believe, particularly representative of the progressive implementation within CTL of both traditional and computer-related methodologies in the planning of digital collections dedicated to words and images.",The digital collections of words and images of the CTL - Scuola Normale Superiore: the case of Orlando Furioso,Oregon State University,2013,,Humanist Studies & the Digital Age,3,1,"[{'value': '2158-3846', 'type': 'electronic'}]"
1908,http://dx.doi.org/10.5399/uo/hsda.3.1.1836,"{'string_id': '10.5399/uo/hsda.3.1.1836', 'id_scheme': 'DOI'}",Griseldaonline è un portale online di letteratura afferente al Dipartimento di Filologia Classica e Italianistica dell'Università di Bologna. L'articolo presenta il portale nella sua genesi e nelle sue possibili evoluzioni.This article presents the Italian literary magazine Griseldaonline that was created in 2002 by a group of young scholars from the University of Bologna. The magazine has encouraged the production of new forms of critical literary writing conceived specifically for the web.,"""Griseldaonline"". Una sfida al labirinto letterario",Oregon State University,2013,,Humanist Studies & the Digital Age,3,1,"[{'value': '2158-3846', 'type': 'electronic'}]"
1909,http://dx.doi.org/10.5399/uo/hsda.3.1.3086,"{'string_id': '10.5399/uo/hsda.3.1.3086', 'id_scheme': 'DOI'}","Working from transcriptions generated through the T-PEN program at St. Louis University, the collaborators of the project ""Petrarch’s Early Manuscripts and Incunabula in the Oregon Petrarch Open Book"" are presently digitizing and encoding in TEI P5 2 key interpretative copies of Petrarch’s Rvf: the late 14th-century manuscript copy from the Queriniana Library in Brescia, D II 21, the Queriniana Library’s copy of the first printed edition (editio princeps) of the Rvf edited by Cristoforo [Berardi?] and published by Vindelin de Speier (Spira) in Venice in 1470. The first part of the article traces a brief history of the Oregon Petrarch Open Book (OPOB), the last section introduces a future project, “Texts, Images and Interpretation in the OPOB”.",Petrarch’s Early Manuscripts and Incunabula in the Oregon Petrarch Open Book,Oregon State University,2013,,Humanist Studies & the Digital Age,3,1,"[{'value': '2158-3846', 'type': 'electronic'}]"
1910,http://dx.doi.org/10.5399/uo/hsda.3.1.3196,"{'string_id': '10.5399/uo/hsda.3.1.3196', 'id_scheme': 'DOI'}","In this presentation, Dr. Stanley Fish provides a lively and insightful critique of the digital humanities and its methods, goals, and claims. He does this using the framework of approaches to interpretation, i.e., how we decide what a given text means. Looking to constitutional law, Dr. Fish outlines three approaches to interpretation and discusses how digital technologies relate to them.","If You Count It, They Will Come",Oregon State University,2013,,Humanist Studies & the Digital Age,3,1,"[{'value': '2158-3846', 'type': 'electronic'}]"
1911,http://dx.doi.org/10.5399/uo/hsda.4.1.3470,"{'string_id': '10.5399/uo/hsda.4.1.3470', 'id_scheme': 'DOI'}","The title does not refer to the application of knowledge through faculty engagement in community-based research, teaching and service – something that is usually understood as community-engaged scholarship. The change of paradigm referred to in the title should be understood within the broader framework of the general transformation of our participatory or convergence culture in the age of social and “spreadable” media (to use a terminology made current by Richard Jenkins). As the Web 2.0 evolves toward the Web 3.0, or the semantic web, community-engaged scholarship remains one of the most crucial components of this larger and more pervasive phenomenon and this also refers to the ""academic community"" at large. What is at stake is not only the transformation of the procedures of scholarly research and learning but also and more importantly the transformation of its goals: traditionally based (at least in the humanities) on the individual researcher and author, knowledge work, or the scholarly mode of production, is increasingly overhauled into a collective, collaborative enterprise, on a much larger scale. Moreover, the general move from discursive to graphic-algorithmic forms of data visualization and interpretation (including textual data), has eventful consequences for critical thinking: as humanists engaged with a potentially radical transformation of the community we belong to, our most daunting task is how to transpose traditional scholarly practices onto the new platform, envisioning new goals and outputs for our traditional tasks.",Change of Paradigm: From Individual to Community-Based Scholarship,Oregon State University,2015,,Humanist Studies & the Digital Age,4,1,"[{'value': '2158-3846', 'type': 'electronic'}]"
1912,http://dx.doi.org/10.5399/uo/hsda.4.1.3584,"{'string_id': '10.5399/uo/hsda.4.1.3584', 'id_scheme': 'DOI'}","This essay introduces the notion of a digital concordance as a reading and research tool to explore intertextual passages online, and illustrates how a digital concordance of highly allusive texts can change how we read and research such texts. I take as example the digital concordance on Intertextual Dante, a project on Digital Dante (digitaldante.columbia.edu) developed in collaboration with the Center for Digital Research and Scholarship at Columbia University, which in the first phase highlights the Ovidian intertextuality in Dante’s Divina Commedia. The essay traces the history of the scholarship on Dante’s Ovid, identifying its strengths and limitations, especially of the concordances in print. Since the intertextual passages in the Commedia require a reading not restricted to the obviously corresponding verses, but attentive to the broader textual contexts, I show how the digital concordance on Intertextual Dante greatly facilitates such a contextualized reading. The visualization of the corresponding passages (side by side, highlighted, within context) and the search options (by category of text reuse or by canto/book/poem) help the study of (Ovidian) intertextuality in the Commedia, as illustrated with examples from the site.",Digital Readers of Allusive Texts: Ovidian Intertextuality in the 'Commedia' and the Digital Concordance on 'Intertextual Dante',Oregon State University,2015,,Humanist Studies & the Digital Age,4,1,"[{'value': '2158-3846', 'type': 'electronic'}]"
1913,http://dx.doi.org/10.5399/uo/hsda.4.1.3682,"{'string_id': '10.5399/uo/hsda.4.1.3682', 'id_scheme': 'DOI'}","As part of the seminar Re-reading Petrarch in the Digital Age –taught at the University of Oregon in Winter 2014– a digital close reading of Francesco Petrarca’s Rerum Vulgarium Fragmenta (Rvf) led to a series of parallel and entwined activities and projects. Deeply integrated with the Oregon Petrarch Open Book Project (OPOB), the course was oriented towards the encoding of Petrarca’s masterpiece based on the implementation of a network of different themes (Poet, Laura, Metaphysics Nature Metamorphoses and Urban Culture). The various occurrences and data obtained from the encoding were collected into an online database that is still operating and updatable. The results of this qualitative data collection were also compared with a quantitative computer-based research of selected keywords extracted from the various themes. From the beginning of the seminar, the class was divided into groups and each of them read the Rvf focusing on one of themes chosen while at the same time paying attention to the others. The different groups considered these themes as interrelated and interdependent. The poet is the sphere in which all the topics emerge in their specific connotations. However, his sphere is related to, dependent on and intersected with the other four spheres. The encoding allowed measuring the different manifestations, levels and meanings of the intersecting relationships. The study of the visual interpretation of the Rvf in the Inc. Queriniano G V 15 paralleled the close reading and encoding activity and helped students to develop a visual rendering of their themes and a pointed and original and creative interpretation of Petrarca’s masterpiece.",Re-Reading Petrarca in the Digital Era,Oregon State University,2015,,Humanist Studies & the Digital Age,4,1,"[{'value': '2158-3846', 'type': 'electronic'}]"
1914,http://dx.doi.org/10.5399/uo/hsda.6.1.2,"{'string_id': '10.5399/uo/hsda.6.1.2', 'id_scheme': 'DOI'}","Collective Intelligence, the book in French, that Pierre Levy wrote before the existence of the worldwide web, when only the Internet existed, it's a philosophical vision of the future, a philosophical vision of what could be a global civilization based on the digital and the general interconnection of all the computers. In the interview, Levy addresses the creation o the WWW by Tim Berners Lee as a form of collective intelligence. He then discusses Berners Lee's proposal for a reform of the WWW to avoid its confiscation by the big platforms, such as Google, Facebook, and Amazon. Then, he introduces his most recent project the IELM as a tool for semantic metadata and a code between the natural languages and the algorithms, and all the apparatus of computations.","Collective Intelligence, the Future of Internet and the IEML: Interview to Pierre Lévy by Art Farley and Massimo Lollini",Oregon State University,2019,,"Semantic Metadata, Humanist Computing, and Digital Humanities",6,1,"[{'value': '2158-3846', 'type': 'electronic'}]"
1915,http://dx.doi.org/10.5399/uo/hsda.6.1.3,"{'string_id': '10.5399/uo/hsda.6.1.3', 'id_scheme': 'DOI'}","At its beginnings Humanities Computing was characterized by a primary interest in methodological issues and their epistemological background. Subsequently, Humanities Computing practice has been prevailingly driven by technological developments and the main concern has shifted from content processing to the representation in digital form of documentary sources. The Digital Humanities turn has brought more to the fore artistic and literary practice in direct digital form, as opposed to a supposedly commonplace application of computational methods to scholarly research. As an example of a way back to the original motivations of applied computation in the humanities, a formal model of the interpretive process is here proposed, whose implementation may be contrived through the application of data processing procedures typical of the so called artificial adaptive systems.",The Origins of Humanities Computing and the Digital Humanities Turn,Oregon State University,2019,,"Semantic Metadata, Humanist Computing, and Digital Humanities",6,1,"[{'value': '2158-3846', 'type': 'electronic'}]"
1916,https://umanisticadigitale.unibo.it/article/view/7196,"{'string_id': '10.6092/issn.2532-8816/7196', 'id_scheme': 'DOI'}","Impact and sustainability are the leitmotiv of current Research Policies in Europe. Science is required to show its positive socio-economic influence on Society, and Humanities Research is not exempted from this requirement. Within this context, Digital Humanities might enhance the external impact of Humanities Research, more specifically the commitment to public engagement, by means of strategic opportunities offered by digital techniques. To pursue this objective, intermediation seems necessary in order ‘to switch Humanities on’ within Society. Actually, virtual objects might be as speechless as real ones, i.e. books in a library without users. The value of a Digital approach lies in its usability, in the additional tools and helps it supplies, and in stimulating users in becoming active agents of their reading. As in a good storytelling the attention to what can provoke reactions is as important as the action narrated itself, digital objects need hints to stimulate and activate users’ curiosity, questions and original solutions. The paper concentrates on three elements: the requirement for external impact of research activities; the need for intermediary agents and a concrete experimentation of intermediation. Digital Humanities are analysed in the context of the Humanities and beyond the academic realm, in their targeting diverse recipients, external to the scientific community. The test conducted so far is addressed to the specific stakeholder community of teachers, called to act as intermediaries between Humanities Research outcomes and the Scholastic Community.",Digital Humanities and Society: an impact requiring ‘intermediation’,Bologna: AlmaDL,2017,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1917,https://umanisticadigitale.unibo.it/article/view/7230,"{'string_id': '10.6092/issn.2532-8816/7230', 'id_scheme': 'DOI'}","Oral archives collected by professional scholars and ordinary people interested in dialects and ethnology are a precious resource for various fields of study (from linguistics to anthropology, from economy to history and politics, etc.) and may contain documents that could be labeled as products of intangible cultural heritage, thus deserving safeguard. Grammo-foni. Le soffitte della voce (Gra.fo), a two-year project jointly conducted by Scuola Normale Superiore and the University of Siena (Regione Toscana PAR FAS 2007-13), discovered, digitized, cataloged and disseminated via a web portal nearly 3000 hours of speech recordings stemming from around 30 oral archives collected by scholars and amateurs in the Tuscan territory. Having preserved such a significant collection of oral documents (e.g. oral biographies, ethno-texts, linguistic questionnaires, oral literature), Gra.fo constitutes a precious repository of Tuscan memory and provides a first-hand documentation of Tuscan language varieties from the early 1960s to the present day [7], [9]. In this article, the Gra.fo project will be described in all its stages, which involve: fostering the level of awareness on the importance of preserving this valuable cultural heritage product; contacting the oral recordings’ owners and co-signing legal agreements for the temporary borrowing of the recordings and accompanying materials; collecting and digitizing the recordings and the accompanying materials; cataloging (with the self-developed software Audiografo) and partially transcribing the oral documents; implementing the downloadable online catalog , an open-ended repository of oral texts which have hitherto been known to a very limited number of potential users. Some problematic issues related to the treatment of oral archives will also be discussed, together with the proposed solutions. These concern the carrier / document relation, the treatment of confidential information, and the cataloging of documents within other documents.",The Gra.fo project: from collection to dissemination,Bologna: AlmaDL,2017,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1918,https://umanisticadigitale.unibo.it/article/view/7250,"{'string_id': '10.6092/issn.2532-8816/7250', 'id_scheme': 'DOI'}","We present DanteSources, a focused Digital Library that aims at building innovative services supporting scholars in creating, evolving and consulting a digital encyclopaedia of Dante Alighieri’s works. DanteSources is endowed with Web services that allow extracting and visualizing data about Dante Alighieri’s primary sources, i.e. other authors’ works which Dante referred to in his own works. The heart of the digital library is a knowledge base built on an ontology that was developed in the RDF/S language, representing Dante’s works and the knowledge about them, extracted from some authoritative commentaries. Finally, on top of this graph, we developed DanteSources, which allows users to extract and display the knowledge stored in the knowledge base in the form of charts and tables. The methodology and the tool we developed are easily reusable to represent the knowledge about primary sources of other authors of the Italian or the international literature.",DanteSources: a Digital Library for Studying Dante Alighieri’s Primary Sources,Bologna: AlmaDL,2017,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1919,https://umanisticadigitale.unibo.it/article/view/7253,"{'string_id': '10.6092/issn.2532-8816/7253', 'id_scheme': 'DOI'}","How can we study, present and teach complex cultural phenomena such as the Russian philological culture of the 1920s? To achieve this goal, we elaborated a knowledge representation that facilitates scientific collaboration, enables distant reading, improves the navigation of scholarly literature, links classical texts to rich international scholarship, and provides a basis of effective visualization. Digital Humanities offer an ideal framework for the intense human-computer collaboration required to carry out such a project. We focus on the network of relations both within and between three key communities of the early Soviet philological milieu – the Formalists, the Marrists and the Bakhtinists – approaching them through the optics of two major philological romans à clef of the period. To this end, we (1) prepared a collection of primary texts; (2) built a repository of secondary literature; (3) using this research literature, enriched primary texts with both general and ad locum annotations; (4) adapted the nano-publications method as a comprehensive approach for representing this scholarly knowledge in the Semantic Web. We make use of the quantitative methods toolkit of the VicoGlossia system, which was developed as part of an international and inter-institutional collaboration.",VICOGLOSSIA: Annotatable and Commentable Library as a Bridge between Reader and Scholar (a proof of concept study: Early Soviet Philological Culture),Bologna: AlmaDL,2018,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1920,https://umanisticadigitale.unibo.it/article/view/7305,"{'string_id': '10.6092/issn.2532-8816/7305', 'id_scheme': 'DOI'}","Conceptual Analysis (CA) is a matter-of-course practice for philosophers and other scholars in the humanities. Exploring one author’s corpus of texts in order to discover the various properties of a concept is a classic example of CA. Recently, a corpus-based computational framework for CA has been emerging in response to the methodological challenges brought about by the massive digitization of texts. In this framework, CA is approached by implementing a computer-assisted text analysis method, within which algorithms are used to support the various cognitive operations involved in CA. In this article, we focus on the retrieval of relevant text segments for analysis. However, this is a complex issue within a computational framework, since the relation between concept and natural language depends on several semantic phenomena, including synonymy, polysemy, and contextual modulation. The main contribution of this article is methodological because it explores the computational approach to CA. We present three algorithmic methods, which identify relevant text segments while taking into account various semantic phenomena. The results show the potential of computer-assisted CA, thereby highlighting the need to overcome the limitations of these first experiments. An additional contribution of this work takes the form of knowledge transfer from Artificial Intelligence to the Humanities.",Conceptual Analysis in a computer-assisted framework: mind in Peirce,Bologna: AlmaDL,2018,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1921,https://umanisticadigitale.unibo.it/article/view/7814,"{'string_id': '10.6092/issn.2532-8816/7814', 'id_scheme': 'DOI'}","This paper introduces two projects in the realm of Digital Humanities, Mapping Visions of Rome and Digital Roman Heritage. The first project was conceived as a digital aid to my traditional research, while the second developed as a digital aid to the collaboration with other people and projects. Together they reflect how I, trained as a Latinist, entered the field of Digital Humanities, in which I have no specific training. The goal of this paper is therefore primarily to explore the opportunities offered by Digital Humanities for humanities researchers like myself who do not regard this field as their native soil but still wish to contribute to it and benefit from digital research infrastructures and technologies. In particular, it will reflect on the way in which Digital Humanities can further interdisciplinary collaboration and exchange, which I consider to be one of its greatest assets.",Mapping Visions of Rome and Digital Roman Heritage Connectivity between Literary and Artistic Heritage in a Digital Age,Bologna: AlmaDL,2018,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1922,https://umanisticadigitale.unibo.it/article/view/8053,"{'string_id': '10.6092/issn.2532-8816/8053', 'id_scheme': 'DOI'}","In this paper we study, analyse and comment on rhetorical figures present in a selected body of poetry of the first half of the 20th century. These figures are at first traced back to famous poets of the past and then compared to classical Latin prose. Linguistic theory is then called upon to show how these rethorical figures can be represented in syntactic structures and classified as noncanonical structures, by positioning discontinuous or displaced linguistic elements in SpecXP projections at various levels of constituency. We then introduce LFG – Lexical Functional Grammar – as the theory that allows us to connect syntactic noncanonical structures with informational structure and psycholinguistic theories for complexity evaluation. We end up with two computational linguistics experiments and then evaluate the results. The first experiment uses the best performing online parsers of Italian to parse poetic structures; the second experiment use Getarun – a system created at the Computational Linguistics Laboratory of Ca' Foscari. As will be shown, the first approach is unable to cope with these structures because only statistical probabilistic information is used. Conversely, as a symbolic rule-based system, the second approach is much superior and facilitates both semantic and pragmatic analyses.",Syntax and Semantics of Italian Poetry in the First Half of the 20th Century,Bologna: AlmaDL,2018,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1923,https://umanisticadigitale.unibo.it/article/view/8161,"{'string_id': '10.6092/issn.2532-8816/8161', 'id_scheme': 'DOI'}","This paper consists of the cataloguing, mapping, and the study of the street poetry of Rome, written on the walls of the Italian capital by a group of street artists who started their activity in 2012: The Poeti der Trullo (The poets of the Trullo). Through the use of digital humanities tools, the article describes the Poeti der Trullo and their district, developing some considerations on the themes and the language that characterise their poems. The aim is to understand the cultural and social roots of this street poetry phenomenon and its effects on contemporary society. This study, which combines tools such as WordSmith, TXM, RStudio, Q-GIS, CartoDB, and Gephi, explains how the Poeti der Trullo interpret and interact with society through an interdisciplinary approach. The activity of the Poeti der Trullo, which blends the tradition of Roman graffiti and Pasquinate with the urban culture of slam poetry, shows a poetic resistance towards the image of contemporary society. Through a simple gesture such as writing a poetic message on a wall, these young performers reveal the strength of a cultural tradition that discloses a new act of resistance which has a pragmatic effect on society. In only six years of existence, these poets have effectively created a movement able to generate effective change in their district.",Rethinking Rome as an Anthology: The Poeti der Trullo’s Street Poetry,Bologna: AlmaDL,2018,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1924,https://umanisticadigitale.unibo.it/article/view/8174,"{'string_id': '10.6092/issn.2532-8816/8174', 'id_scheme': 'DOI'}","This article presents the rationale and the proposal of a preliminary architecture of a formal ontology of the Text Encoding Initiative markup language. The reasons to have a formal and machine-readable semantics for TEI are manifold. In the first place, it would have a number of pragmatic and technical benefits, like better support for semantic interoperability in text encoding practices, easier cross-corpora query processing, seamless integration with Linked Open Data ecosystem. In second place, it would give a formalized account of the quasi-formal notion of the TEI abstract model, fostering the consistency and soundness of the TEI model. Given the complexity of the TEI encoding schema, such an ontology must necessarily have a very complex architecture, and its thorough definition will be a time consuming intellectual activity: in a first stage, we propose to limit its scope to a well-defined subdomain of the TEI, and to build it adopting pre-existing meta-ontology like EARMARK. The final part of the article gives some preliminary details of this design",A Formal Ontology for the Text Encoding Initiative,Bologna: AlmaDL,2018,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1925,https://umanisticadigitale.unibo.it/article/view/8511,"{'string_id': '10.6092/issn.2532-8816/8511', 'id_scheme': 'DOI'}","In this essay, we reflect on distant reading as one of the various takes on reading that currently prevail in literary scholarship as well as the teaching of literature. We focus on three concepts of reading which for various reasons can be considered inter-related: close reading, surface reading and distant reading. We offer a theoretical treatment of distant reading and demonstrate why it is closely related to the concept of machine reading (part of artificial intelligence). Throughout, we focus on the role of the individual reader in all this and argue that Digital Literary Studies have much to gain from paying closer attention to the so-called “natural” reading process of individual humans.",Can Machines Read (Literature)?,Bologna: AlmaDL,2019,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1926,https://umanisticadigitale.unibo.it/article/view/8555,"{'string_id': '10.6092/issn.2532-8816/8555', 'id_scheme': 'DOI'}","In this contribution, two open problems in computational stemmatology are being considered. The first one is contamination, an umbrella term referring to all phenomena of admixture of text variants resulting from scribes considering more than one manuscript or even memory when copying a text. This problem is one of the biggest to date in stemmatology since it implies an entirely different formal approach to the reconstruction of the copy history of a tradition and in turn to the reconstruction of an urtext. (Maas 1937) famously stated that there is no remedy against contamination and (Pasquali and Pieraccioni 1952) coined the terms 'open' vs. 'closed' recensions to distinguish contaminated from uncontaminated. We present a graph theoretical model which formally accommodates traditions with any degree of contamination while maintaining a temporal ordering and give combinatorial numbers and formula on the implication for numbers of possible scenarios.",An open problem in computational stemmatology - a model for contamination,Bologna: AlmaDL,2019,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1927,https://umanisticadigitale.unibo.it/article/view/8575,"{'string_id': '10.6092/issn.2532-8816/8575', 'id_scheme': 'DOI'}","The Digital Humanities (DH), as Rob Kitchin reminds us, have always been interested in the building of infrastructure for research (Kitchin 2014, Loc. 222 of 6164). Imagining how emerging technologies could first be applied to Humanities problems and then scaled up to infrastructure for others to use has been one of the defining features of the field, by which we mean the field has evolved through projects that experimented with the application of new computing technologies to the difficult problems of the Humanities. Such experimentation began with Father Busaís Index Thomisticus (IT) project (Busa 1980; Winter 1999; Busa 1974-1980) which is why many genetic descriptions of the field returns to the Index. The Index Thomisticus (IT) project was not only the first, but also one of the largest Digital Humanities projects of all time, even though the outcome might, by today’s standards be considered “small”. The project lasted 34 years and at its peak (1962) involved a staff of as many as 70 persons all housed in a large ex-textile factory in Gallarate. For that time they were dealing with big data, we might even say really big data, and the infrastructure they had to build was unlike any ever built before. If we want to understand what is involved in scaling up to big infrastructure we should look back to the beginnings of the field and the emergence of big projects like the Index. This paper will therefore look at the Busa’s project as a way to think through big projects by first discussing the historiography of the IT project and DH projects in general. We will ask how can we study projects as bearers of ideas? What resources do we need/have? Then we will look at specific aspects of the project that shed light on DH projects in general. In particular we will look at how the project was communicated, conceived, and the data processing innovations. Finally, we will reflect on what lessons the IT project has for us at a time when big data has become an end in itself. What can we learn from Busa’s attention to data in the face of the temptations of automatically gathered data?",The Index Thomisticus as a Big Data Project,Bologna: AlmaDL,2019,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1928,https://umanisticadigitale.unibo.it/article/view/8585,"{'string_id': '10.6092/issn.2532-8816/8585', 'id_scheme': 'DOI'}","Data visualization is a powerful tool for digital scholarship yet not without its pitfalls. Based on the dissertation “Visualizing Gender Balance” comparing ten computer science conferences, several visualization techniques and tools undergo a critical review. The dataset underlying the visualizations contains data researchers encounter daily: bibliographic information. Analyzing larger sets of authors writing and publishing for conferences in computer science changes our perception of the gender (im)balance in this academic research area. But only a careful curation and visualization can truly reveal what goes on behind the scenes. Still the more complicated, detailed and nuanced the visualization, the harder it becomes for an untrained eye to interpret the patterns.",Visualizing Gender Balance in Conferences,Bologna: AlmaDL,2019,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1929,https://umanisticadigitale.unibo.it/article/view/9063,"{'string_id': '10.6092/issn.2532-8816/9063', 'id_scheme': 'DOI'}","This paper pursues the analysis, modelling and representation of the relationships between the different entities concerned by genetic criticism (documents and publications), in the context of a scholarly digital edition. Our cases study are taken from the project ""Gustave Roud, Œuvres complètes"", whose principal aim is to edit the complete works of the Swiss author Gustave Roud (1897-1976). Roud's writing process is rooted in his diary: a first note, for instance taken during a walk in the countryside, can be typed or copied together with other notes in a dedicated notebook, and then inserted in a journal article, which will be itself modified and assembled with others to form a poetic collection. This mechanism of rhapsodic montage is used at various stages of the writing processes, allowing for multiple assemblages of the same materials, for new texts to be produced from existing ones and for the same diary note to appear in multiple texts. In this article, we present an ontology for modelling the genetic relationships between the documents. The available data-model for genetic criticism are firstly discussed: they concentrate on a single document or on the textual variation between them, but no formal model for their relationships is available. Existing visualizations of those relationships are then taken into account. The ontology that we propose to fill this void is detailed and its compatibility with widespread conceptual models, such as FRBRoo and CIDOC CRM, is discussed. Two case studies are presented to test the ontology. In the article we consider the application of our model to other corpora and we conclude with suggestions for the improvement of this work.","Modeling genetic networks. Gustave Roud's œuvre, from diary to poetry collections",Bologna: AlmaDL,2019,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1930,https://umanisticadigitale.unibo.it/article/view/9162,"{'string_id': '10.6092/issn.2532-8816/9162', 'id_scheme': 'DOI'}","Within the “Museo Virtuale della Musica BellinInRete” project, a corpus of letters, written by the renowned composer Vincenzo Bellini (1801 - 1835) from Catania, will be encoded and made publicly available. This contribution aims at illustrating the part of the project regarding the implementation of the prototype for the metadata and text encoding, indexing and visualisation of Bellini’s correspondence. The encoding scheme has been defined according to the latest guidelines of the Text Encoding Initiative and it has been instantiated on a sample of letters. Contextually, a first environment has been implemented by customizing two open source tools: Edition Visualization Technology and Omega Scholarly platform. The main objective of the digital edition is to engage general public with the cultural heritage held by the Belliniano Civic Museum of Catania. This wide access to Bellini’s correspondence has been conceived preserving the scholarly transcriptions of the letters edited by Seminara within her most recent critical edition (Olschki, 2017). The digital edition of the corpus takes care of handling the correspondence metadata by means of the correspDesc TEI tagset. Finally, Bellini's letters will be accessible via the Web platform as well as integrated into a forthcoming interactive and multimedia tour hosted at the museum.",Bellini’s Correspondence: a Digital Scholarly Edition for a Multimedia Museum,Bologna: AlmaDL,2019,,Umanistica Digitale,,,"[{'value': '2532-8816', 'type': 'electronic'}]"
1931,http://dx.doi.org/10.6853/DADH.201804_1.0002,"{'string_id': '10.6853/DADH.201804_1.0002', 'id_scheme': 'DOI'}","The field and influences of Digital Humanities are expanding significantly in recent years. This paper discusses and reviews many of the influential ideas and representative achievements that were reported in the Series on Digital Humanities and the International Conference on Digital Archives and Digital Humanities. Existing accomplishments have provided not only substantial proofs for the far-reaching promises of Digital Humanities but also further guidance for advancing the field in coming years. The increasing availability of research material and the strengthening power of computing facilities are leading us to a new era of humanistic research. In an open data environment, researchers may access information that are potentially relevant to their research targets from diverse sources with the help of information retrieval and extraction techniques. Digital tools for relating and disambiguating relevant information from abundant raw material can assist researchers to sift through documents that offer insights of varying relevancy. With the solid foundations of the availability of research data and the strength of computing supports, researchers can dig deeply into a research topic. In addition, digital tools may provide explorative search, offering opportunities of discovering new issues that are contextually related to the original research topic. With an appropriate expert-in-the- loop search mechanism, experts may uncover new research topics and may even link relationships between topics that were not previously known, perhaps forming a topic network among research topics. A truly open data environment also allows us to make results of humanistic research verifiable. Readers of a published paper may find and study both the cited papers and the researched material via information curators or data providers so that readers may acquire a deeper understanding of the viewpoints and thoughts of the paper authors. This would be hard to achieve if any part of the data is privately secured.","Toward Linking Open Data, Synchronizing Digital Tools, and Mapping Relevant Topics: A Survey of the Series on Digital Humanities",臺灣數位人文學會 & Ainosco Press,2018,"[' humanistic research ', ' computational linguistics ', ' digital tools ', ' open data ', ' topic networks']",Journal of Digital Archives and Digital Humanities,1,,"[{'value': '2616-5732', 'type': None}]"
1932,http://dx.doi.org/10.6853/DADH.201804_1.0003,"{'string_id': '10.6853/DADH.201804_1.0003', 'id_scheme': 'DOI'}","Computational thinking is a cognitive framework of logical deduction and problem solving. Digital humanities has been rooted on the intersection and fusion of digital technology and humanity study. Based on accumulated experiences in interdisciplinary research, this paper depicts computational thinking and attempts to characterize research implications of data computation on humanities study. The abstract implications are manifested by actual research contexts of several domestic case studies, so as to provide reference and contemplation for the collective growth of digital humanities. Essential characteristics of digital humanities research paradigm inferred by the data computation model include: scale, dimension, intelligence, visualization, contextualization, independent validation, and common grounding. These characterization may help establish transdisciplinary pathways for younger generation scholars and provide further consideration with the research community. Transdisciplinary digital humanities research entails both physical changes of collaborative interaction and chemical changes of creative fusion. It is hoped that more will join the endeavor and begin a great journey of new adventure.",Computational Thinking and Implications for Digital Humanities Research,臺灣數位人文學會 & Ainosco Press,2018,"[' computational thinking ', ' data computation ', ' digital humanities ', ' research implications']",Journal of Digital Archives and Digital Humanities,1,,"[{'value': '2616-5732', 'type': None}]"
1933,http://dx.doi.org/10.6853/DADH.201804_1.0004,"{'string_id': '10.6853/DADH.201804_1.0004', 'id_scheme': 'DOI'}","This paper introduces a semi-automatic text tagging interface to help historians efficiently garner posting records from the Chinese Local Gazetteers (difangzhi 地方志) with the format of ＂who, when, which posting.＂ By turning texts into tabular data forms, this interface aims to lay the basis for analyzing Chinese local gazetteers on a large scale. Although local gazetteers from various locations all follow a general pattern when recording posting data, they still differ in details due to their substantial amount. Therefore, it is unfeasible to ask programmers to extract the posting data using a onesize- fits-all computer program. This tagging interface, on the other hand, provides a simple user interface with built-in patterns to extract the subjects' names, posting titles, dynasties, posting times, basic addresses and entry methods. This allows users to tag most of data in the text quickly, which can then be proofread by them themselves in order to check the tagging result and to correct mistakes. The interface also enables users to adjust the extraction patterns according to each text in order to accurately extract posting data from local gazetteers with distinct patterns.",From Text to Data: Extracting Posting Data from Chinese Local Gazetteers,臺灣數位人文學會 & Ainosco Press,2018,"[' computational thinking ', ' data computation ', ' digital humanities ', ' research implications']",Journal of Digital Archives and Digital Humanities,1,,"[{'value': '2616-5732', 'type': None}]"
1934,http://dx.doi.org/10.6853/DADH.201804_1.0005,"{'string_id': '10.6853/DADH.201804_1.0005', 'id_scheme': 'DOI'}","In total, there are over 1,200 hanging and hand scroll landscape paintings from five dynasties period to North Song dynasty which have been recorded in various painting histories, treatises, and historical records. In this study, we selected more than 120 paintings as experimental samples and another 240 paintings from South Song dynasty as supporting materials. The selected paintings represent over 30 categories of motifs, such as Snowy Scene, Wintry Forests, Dwellings in Autumnal Mountains, the Mountain and Stream, etc. The image capture technology is used on paintings with two easily recognizable motifs 'fish-boat' and 'fisherman,' of which, 22 out of 132 document recorded paintings were selected. The experiment was carried out in several steps: the first step is to establish a descriptive and statistical model to analyze the formative evolution and character identification of fish-boat and fisherman in different time periods; the second step will construct a semantic network with feature words extracted from the motif information, thematic content and scenario settings of the paintings; finally, based on the cross-references of the image analysis and text mining, it attempts to establish a Full Image Information Retrieval Database for landscape paintings and related descriptions. At last, by studying ＂Fisherman-Hermit＂ Theme pattern in painting and its semantics, We test a validity of method of picture-text data model and digital humanity used in Chinese classic painting.",The Digital Humanities Research of the Landscape Painting of the Five Dynasties and Northern Song Dynasty (2): A Study of the ＂Fisherman-Hermit＂ Theme in Painting,臺灣數位人文學會 & Ainosco Press,2018,"['digital humanities ', ' landscape painting ', ' Fisherman image ', ' Northern Song dynasty ', ' Neo-Confucianism']",Journal of Digital Archives and Digital Humanities,1,,"[{'value': '2616-5732', 'type': None}]"
1935,http://dx.doi.org/10.6853/DADH.201804_1.0006,"{'string_id': '10.6853/DADH.201804_1.0006', 'id_scheme': 'DOI'}","In recent years, with the rapid development of information technology, a substantial number of Chinese Buddhist scriptures have been converted to digital format and made available to the public. The Chinese Buddhist Electronic Text Association (CBETA) has made the Chinese Electronic Tripiṭaka Collection widely available for many years and provided a resourceful platform for the Chinese Buddhist texts studies. As of the 2016 version, more than 210 million Chinese characters are freely and publicly available in digital form through the efforts of the CBETA. With the rapid emergence of digital tools, the employment of digital resources for the extraction of implicit information in the texts is a promising methodology. Owing to the privation of high-level computer programming skills and advanced statistical knowledge, the quantitative analysis of digital texts still under-perform. Therefore, there is a barrier for humanities scholars to navigate these digital tools, not to mention the application of such research concept in the field of Chinese Buddhist texts studies. It is inevitable to create a platform that assists humanities scholars in overcoming the mentioned hindrances so that the computational linguistic resources and information technology could aid in their research effort. Hence, the establishment of ＂CBETA Research Platform＂ started. This research platform provides high-quality digital content from the CBETA corpus, combining relevant references based on the latest research findings.",CBETA Research Platform: A Digital Tool for Studying Chinese Buddhist Texts in the New Era,臺灣數位人文學會 & Ainosco Press,2018,"['Digital Buddhist Canon ', ' Chinese Buddhist Electronic Text Association (CBETA) ', ' database ', ' buddhist studies ', ' digital research platform']",Journal of Digital Archives and Digital Humanities,1,,"[{'value': '2616-5732', 'type': None}]"
1936,http://dx.doi.org/10.6853/DADH.201810_2.0001,"{'string_id': '10.6853/DADH.201810_2.0001', 'id_scheme': 'DOI'}","This paper proposes an alternative take to critique in the digital humanities (DH). As it further expands, the DH needs to critically engage with digital technologies by shifting the balance between the thinking and methods of the humanities. Taking the standpoint of a humanist enterprise, the DH ought to examine materiality as the essence of the digital. In so doing it should engage with the subjective experience as a starting point of analysis about how the digital impacts the nature of who we are, and in turn the way in which it affects culture and digital practices.",A Critical Approach to Digital Humanities: Examining the Subjective Sphere,臺灣數位人文學會 & Ainosco Press,2018,"['critical digital humanities ', ' subjective perspective ', ' materiality ', ' humanistic approach']",Journal of Digital Archives and Digital Humanities,2,,"[{'value': '2616-5732', 'type': None}]"
1937,http://dx.doi.org/10.6853/DADH.201810_2.0002,"{'string_id': '10.6853/DADH.201810_2.0002', 'id_scheme': 'DOI'}","Digital Humanities allows researchers to question traditionally-held theories based on ＂close＂ readings by extending research to ＂distant＂ reading. In this paper, the Ubiqu + Ity tool provided by the Visualizing English Print (VEP) Website is used to analyze the spoken text of all major characters within Shakespeare's As You Like It. The Ubiqu+Ity tool provides several language action types (LATs), which are then classified as relational versus selfish, and active versus reflective. These classifications produce a method (called the character path walk) that visualizes the paths the characters ＂walk＂ through the play, leaving their ＂trails＂ behind that can then be visually analyzed. Upon initial analysis, using the play As You Like It as an example, minor characters such as Phoebe and Silvius revealed dramatic arcs in their speeches. Rosalind's roles as Catalyst (early in the play) and Conciliator (of Phoebe and Silvius) are also identified by applying the method. Some areas for further analysis and method development are suggested.",Navigating the Forest Through the Trees: Visualizing Character Paths Through Shakespeare’s As You Like It,臺灣數位人文學會 & Ainosco Press,2018,"['corpus linguistics ', ' textual analysis ', ' data visualization ', ' early modern English drama ']",Journal of Digital Archives and Digital Humanities,2,,"[{'value': '2616-5732', 'type': None}]"
1938,http://dx.doi.org/10.6853/DADH.201810_2.0003,"{'string_id': '10.6853/DADH.201810_2.0003', 'id_scheme': 'DOI'}","The author of Shiji (史記) is Sima Qian, but part of it was rewritten according to the old manuscript of his father Sima Tan. In addition to Sima Qian and Sima Tan, Chu Shaosun is also one of the authors of Shiji. This study used term frequency statistics tool on DocuSky to count 30 chapters of Shiji. Frist, I used virtual words in Shiji as the subject matter doing statistical analysis. And then used the writing of Chu Shaosun as a control group to compare the differences in the word frequency of the three people. And we can use digital methods to prove the similarities and differences between the authors of the Shiji.",Digital Humanities for Authorship Attribution Problem of Shiji: Based on Frequency of Function Words Within the Hereditary Houses Section,臺灣數位人文學會 & Ainosco Press,2018,"['corpus linguistics ', ' textual analysis ', ' data visualization ', ' early modern English drama ']",Journal of Digital Archives and Digital Humanities,2,,"[{'value': '2616-5732', 'type': None}]"
1939,http://dx.doi.org/10.6853/DADH.201810_2.0004,"{'string_id': '10.6853/DADH.201810_2.0004', 'id_scheme': 'DOI'}","The main, and usually the only, purpose of most traditional digital libraries and archiving systems is to provide good contents with a retrieval system that helps one find desirable documents. This is often not sufficient for humanists who want to employ digital tools to explore properties from interesting subsets of the library or system. Humanists usually do not merely rely on the libraries or systems. They keep interesting texts in hard disks accessible for themselves. It can be hard with conventional systems to analyze properties of texts stored in personal computers. To fix this problem, it is desirable to have a platform that allows one to build personal databases that support not only common retrieval functions but also text-analytic ones. In this paper, we propose DocuSky to solve this problem. DocuSky allows a user to upload text contents to build his or her personal databass. It supports fulltext retrieval, post-classification over a search result, as well as analysis on tagged terms. Fulltext retrieval is common for searching desirable documents in a database. For any search result, post-classification groups its metadata and shows the resulting distribution. Analysis on tagged terms, on the other hand, returns a list of tagged terms occurring in that search result. They are the three major functions offered by the well-known Taiwan History Digital Library (THDL) system. In addition to these elementary functions, it also provides a couple of tools to help users analyze the contents in a database. The advance of digital humanities requires closely cooperation of computer engineers and digital humanists. DocuSky encourages tool developers and humanists to re-think about the roles of texts and content-analytics tools. In order to reduce the effort of tool development, we design a set of DocuSky APIs and widgets to ease the access to the content of a personal database.",DocuSky: A Platform for Constructing and Analyzing Personal Text Databases,臺灣數位人文學會 & Ainosco Press,2018,"[' DocuSky ', ' personal text databases ', ' system architecture ', ' text analytic tools ', ' platform for digital humanities']",Journal of Digital Archives and Digital Humanities,2,,"[{'value': '2616-5732', 'type': None}]"
1940,http://dx.doi.org/10.6853/DADH.201810_2.0005,"{'string_id': '10.6853/DADH.201810_2.0005', 'id_scheme': 'DOI'}","The idea of developing Taiwan Biographical Database (TBDB) was inspired by China Biographical Database (CBDB) project, and aims to provide digital tools, such as text mining, social network analysis, and other related tools, for analyzing complex social networks and broaden research visions in the study of Taiwanese history. Considering the participants' knowledge of texts and the ability to acquire copyright, the team chooses the ＂Treatise of Historical Figures＂ of the New Edition of Changhua Local Gazetteer, compiled by four of the team members, as the base text. There are several advantages of this choice. First, the biographies in the Treatise are written according to the same format; second, the biographies are rigorously verified; third, they include people from all periods and all walks of life. Therefore, they should be a good starting point for the TBDB project. Besides learning from CBDB's model, the emphasis in developing TBDB is to build attributes for historical figures that are suited for Taiwan's historical contexts. Currently, we have built initial versions of basic search functions, a map which shows the geographical distribution of historical figures, and social network analysis tools. They are now open to the public in order to optimize and debug. In the future, we will continue to increase both the quality and quantity of the database and also develop new analysis tools.",From CBDB to TBDB: The ＂Treatise of Historical Figures＂ of the New Edition of Changhua Local Gazetteer as a Starting Point,臺灣數位人文學會 & Ainosco Press,2018,"['digital humanities ', ' treatise of historical figures ', ' Taiwan Biographical Database (TBDB) ', ' text mining ', ' social networks analysis (SNA)']",Journal of Digital Archives and Digital Humanities,2,,"[{'value': '2616-5732', 'type': None}]"
1941,http://dx.doi.org/10.6853/DADH.201904_3.0001,"{'string_id': '10.6853/DADH.201904_3.0001', 'id_scheme': 'DOI'}","Ancient Chinese scriptures usually have no punctuation marks, which makes it difficult for modern people to read and understand. Adding modern punctuation to ancient scriptures is the basis for the collation and research of ancient scriptures, however, it is a very tedious process. Therefore, it is of practical significance to realize automatic punctuation of ancient scriptures by means of artificial intelligence (AI). We apply the latest tool of deep learning (DL) in the field of natural language processing (NLP) to train the two models of long short-term memory (LSTM) and convolution neural network (CNN) on a training set of more than 50 million Chinese characters and approximately 10 million punctuations. Then, on the test set of Buddhist texts from six different dynasties, the highest punctuation accuracy of 94.3% was achieved. At present, the system can mark seven kinds of modern punctuations (comma, period, question mark, exclamation mark, dunhao, semicolon, colon) for ancient texts.",A Method and Implementation of Automatic Punctuation,臺灣數位人文學會 & Ainosco Press,2019,"['automatic punctuation ', ' ancient scriptures ', ' long short-term memory (LSTM) ', ' deep learning ', ' natural language processing']",Journal of Digital Archives and Digital Humanities,3,,"[{'value': '2616-5732', 'type': None}]"
1942,http://dx.doi.org/10.6853/DADH.201904_3.0002,"{'string_id': '10.6853/DADH.201904_3.0002', 'id_scheme': 'DOI'}","In this paper, A Yorkshire Tragedy (AYT) is used as a focus for development of an agent-based modeling application, using a Java swarm-type modeling library called Mason. This paper utilizes data generated by stylometric algorithms within the modeling application to visualize interactions and affinities, employing plays, playwrights, and theaters all as agents against a simple backdrop of Early Modern London. During whole-play analysis (WPA), though the title page of A Yorkshire Tragedy attributes authorship to Shakespeare, and first performance to the Globe, this model suggests that it certainly was not written by Shakespeare. Scene analysis (SA) of the play, employing scenes as independent agents, suggests that none of the scenes appear to be written by Shakespeare. In a Stylo© analysis of the plays reported to be first performed at the Globe theater, A Yorkshire Tragedy appears to be stylometrically similar to another anonymous play, The Merry Devil of Edmonton, and plays written by John Ford and John Webster. Further analysis with different stylometric dimensions may provide more clarity in answering this question.",A Yorkshire Tragedy: Using Agent-Based Modeling to Suggest Authorship,臺灣數位人文學會 & Ainosco Press,2019,"['corpus linguistics ', ' textual analysis ', ' stylometry ', ' author attribution ', ' Early Modern English drama ']",Journal of Digital Archives and Digital Humanities,3,,"[{'value': '2616-5732', 'type': None}]"
1943,http://dx.doi.org/10.6853/DADH.201904_3.0003,"{'string_id': '10.6853/DADH.201904_3.0003', 'id_scheme': 'DOI'}","A study was conducted to analyze the collaboration network of popular music production in Taiwan, using artists who took part in the works by the Best Male/ Female Mandarin Singer Award nominees of 18th to 29th Golden Melody as the study population. Overall, the best Mandarin singer music worker community network is quite large (diameter = 7), and the community members are highly connected (average path length = 2.156). However, unlike the small world network model, the whole the network average clustering coefficient value is relatively low (average clustering coefficient = 0.02), showing that the artists have a wide range of collaborators. In addition, a high modularity was observed (# of modules = 22, modularity score = 0.72), suggesting collaboration tend to be more cohesive around certain music styles. We also examined the relationship between an artists' network position and the diversity of the roles she/he played in the creative process such as lyricists, composers, producers, or arrangers. The diversity of an artist's creative role was calculated by Herfindahl-Hirschman Index. A significant correlation was found between a nominee's role diversity and his/her centrality.",Creative Role Analysis of the Best Male/Female Mandarin Singer Nominees in Taiwan Golden Melody Awards (2007-2018),臺灣數位人文學會 & Ainosco Press,2019,"['Golden Melody Awards ', ' best male/female mandarin singers ', ' music workers ', ' social network analysis ', ' centrality']",Journal of Digital Archives and Digital Humanities,3,,"[{'value': '2616-5732', 'type': None}]"
1944,http://dx.doi.org/10.6853/DADH.201904_3.0004,"{'string_id': '10.6853/DADH.201904_3.0004', 'id_scheme': 'DOI'}","The study aims to demonstrate how the International Image Interoperability Framework (IIIF) can be used and enhanced the use and interoperability of Buddhist art images during digital humanities research. First, it sketches out the requirements of the given field with a particular attention to the spatial structure of image and the analytical research of image. Second, it presents the image platform to drive visual-oriented research in digital humanities by drawing on four sets of the application programming interface (API) provided by the IIIF, including Image, Presentation, Content Search, and Authentication. The result are as follows: (1) the Image API provides scholars closer look at every detail of the images, which are the basis of iconography. (2) A structural display of visual data, in terms of their spatial distribution and the relative location among a group of artworks belonging to the same locale, such that the contextual information remains un-fragmented. (3) A model for annotation and categorization is developed to enable study focusing on a specific subject. (4) Inter-exchangeability of research materials among archives and institutes from different countries. (5) An authentication mechanism is set up to manage the access to materials owned by different teams.","The International Image Interoperability Framework (IIIF) Approach to Digital Representation, View and Annotation of Buddhist Caves and Images",臺灣數位人文學會 & Ainosco Press,2019,"['International Image Interoperability Framework (IIIF) ', ' Buddhist art ', ' Buddhist caves and images ', ' digital humanities']",Journal of Digital Archives and Digital Humanities,3,,"[{'value': '2616-5732', 'type': None}]"
1945,http://dx.doi.org/10.6853/DADH.201910_(4).0001,"{'string_id': '10.6853/DADH.201910_(4).0001', 'id_scheme': 'DOI'}","Applying natural language processing techniques to the analysis of digital historical texts has become an important topic in digital human studies. Considering the actual needs of historical event research, this study starts with the needs of individual researchers and designs a set of tools for historical text event analysis, and the effectiveness of the method is further evaluated by applying it to study pirate events in the Ming Dynasty. Since pirates continued to invade China's coastal areas almost throughout the Ming Dynasty, the Maritime Prohibition Policy and coastal defense arrangements in the Ming Dynasty closely related to the growth and decline of pirates' scourge. Based on some research papers and historical document text on the theme of Chinese coastal pirates, we identify and derive event groups through the individual-use text-based historical event analysis tool, and establish a data structure that links historical events, people and place names. By analyzing the type of each pirate event group, we found that it includes war cases and coastal defense affairs, and we can further track their spatiotemporal transition characteristics by name entities in each event group. This study aims to develop an individual-use text-based historical event analysis tool, and integrate geographic information system with digital humanities applications, to facilitate the possibility of digital humanities methods to historical research.",Digital Humanities and Integrated Spatio-Temporal Information Analysis: An Individual-Use Text-Based Historical Event Analysis Tool and Its Application in Studying Ming Pirates Events,臺灣數位人文學會 & Ainosco Press,2019,"['personalized research tool for historical text analysis ', ' event clustering ', ' automatic text markup ', ' natural language processing ', ' geographic information system (GIS)']",Journal of Digital Archives and Digital Humanities,4,,"[{'value': '2616-5732', 'type': None}]"
1946,http://dx.doi.org/10.6853/DADH.201910_(4).0002,"{'string_id': '10.6853/DADH.201910_(4).0002', 'id_scheme': 'DOI'}","Topic modeling is a digital method to discover hidden thematic structure in large collections of unlabeled texts. It is now widely used to analyze massive modern Chinese texts from internet pages, new media and social net, like document classification and clustering, hot event detection and tracking, opinion mining and so on. This paper uses topic modeling in the analysis of the top-three classical works of pre-Qin Confucianism to discuss their ideological inheritance and development. The aim is to cast a new light on ＂close and direct reading＂ of classical Chinese texts through ＂distant and machine reading＂ and encourage some more creative usage of digital methods in the research of Chinese classical works.",An Application of Topic Modeling in the Analysis of Ancient Chinese Classical Works,臺灣數位人文學會 & Ainosco Press,2019,"['machine reading ', ' text analysis ', ' topic modeling ', ' MALLET']",Journal of Digital Archives and Digital Humanities,4,,"[{'value': '2616-5732', 'type': None}]"
1947,http://dx.doi.org/10.6853/DADH.201910_(4).0003,"{'string_id': '10.6853/DADH.201910_(4).0003', 'id_scheme': 'DOI'}","This present work seeks to shed a new light on the language style of the Book of Odes (Shījīng 詩經) from a quantitative perspective. For reading poems written with the so-called incentive process (xīng 興), this work is based on the Mao version (Máoshī 毛詩) as a reference corpus and adopted his sentence segmentation. Almost all the poems written with the incentive process are composed of two couplets: the first, called initiative lines (qǐjù 起句), evokes an affective image; and the second, called envoi lines (yìngjù 應句), expresses one's feeling. Nevertheless, the semantic correlation between two couplets is often ambiguous. In the Mao version, the use of the incentive process is marked with two Chinese characters xīng yě 興也 where one couplet ends and the following begins. Without this mark, it is difficult to recognize this literary form. To interpret the semantic correlation between these lines in macrostructure, traditional commentaries proceed by analogical comparison, causal reasoning, or arrangement of events in sequence. Some recent studies suggest focusing on the microstructure of the incentive process, for example, rhyme schemes, syntactical relations, reiterated formulas, or alternation between function words and content words. The criteria for identifying the incentive process is problematic since his first appearance in the history of Chinese literary theory. However, this process has been considered as the archetypal form in Chinese lyric tradition for a long period. Based on the internal textual data retrieved from the corpus, this work discovers by combinatorial analysis all underlying patterns in all verses and determines representatives patterns as rhythmic patterns according to the frequency of occurrence. Finally, by linking defined rhythmic patterns between lines, this work re-describes stylistic devices of poems written with the incentive process.",A Quantitative Research of the Book of Odes (Shījīng 詩經): The Discovery of the Underlying Rhythm in the Incentive Process,臺灣數位人文學會 & Ainosco Press,2019,"['digital humanities ', ' quantitative analysis ', ' rhythmic analysis ', ' Book of Odes(Shījīng 詩經) ', ' incentive process(xīng 興)']",Journal of Digital Archives and Digital Humanities,4,,"[{'value': '2616-5732', 'type': None}]"
1948,http://dx.doi.org/10.6853/DADH.202004_(5).0001,"{'string_id': '10.6853/DADH.202004_(5).0001', 'id_scheme': 'DOI'}","This paper investigates the issue of ＂paradigm shift＂ in humanities research in the digital era, mainly from the perspective of ＂evidential research＂-meaning textual criticism based on the investigation of both internal and external evidence. All humanities research, whether from the traditional or digital era, must be based on a foundation of ＂data＂ and ＂evidential research.＂ But the forms of data, the challenges one might encounter while using these data, as well the as the methods and tools of ＂evidential research,＂ are constantly changing. Thus, our ＂paradigm＂ for humanities research in the digital era must also necessarily be constantly shifting. Consequently, in this digital era dominated by data and robots, humanities scholars need to study both digital technology and the humanities so that they can combine both humanistic erudition and ＂computational thinking.＂ However, the combination of these two does not guarantee that we can then soar unimpeded through the cosmos of the digital world. After all, this combination constitutes mere ＂literacy,＂ not ＂skill.＂ This new type of literacy may be called ＂digital textual criticism.＂",Digital Textual Criticism: The New Literacy for Humanities Scholars,臺灣數位人文學會 & Ainosco Press,2020,"['digital humanities ', ' digital textual criticism ', ' big data ', ' crowdsourcing ', ' artificial intelligence']",Journal of Digital Archives and Digital Humanities,5,,"[{'value': '2616-5732', 'type': None}]"
1949,http://dx.doi.org/10.6853/DADH.202004_(5).0002,"{'string_id': '10.6853/DADH.202004_(5).0002', 'id_scheme': 'DOI'}","Epigraphy in Chinese Singaporean Temples preserves local documentation of social connections in pre-modern Singapore. However, this valuable historical source has not yet been comprehensively analysed, in part due to the large amount of documents to be catalogued. The present study aims at the development of a frequency analysis of sponsors appearing in epigraphic texts in the pre-modern Chinese Singaporean Temples. This research examines the influence of donors by assessing their frequency, highlights the connections among sponsors, and their active areas, i.e., the links of sponsors to specific temples. The study applies a digital humanities approach, by utilizing Text Encoding Initiative (TEI), Python, DocuSky, and Palladio. TEI is used to mark up the texts of Chinese Epigraphy in Singapore, 1819-1911. The information under various labels can be extracted by Python programming. DocuSky is applied to produce graphic presentations that analyze the frequency of sponsors, assessing terms elicited from TEI files and terms from other sources as well. Palladio and Gephi provide graphic presentations of network among sponsors through the temples. This digital humanities work allows us to map specific aspects of Singapore in history.",Chinese Singaporean Temples: Digital Humanities Approaches to Frequency Lists of Sponsors,臺灣數位人文學會 & Ainosco Press,2020,"['Chinese temple ', ' frequency analysis of sponsors ', ' digital humanities ', ' Text Encoding Initiative (TEI) ']",Journal of Digital Archives and Digital Humanities,5,,"[{'value': '2616-5732', 'type': None}]"
1950,http://dx.doi.org/10.6853/DADH.202004_(5).0003,"{'string_id': '10.6853/DADH.202004_(5).0003', 'id_scheme': 'DOI'}","The National Chengchi University Library began to develop a digital humanities research platform for Mr. Lo Chia-Lun's writings since 2018. This platform provides several basic functions, including a convenient and user-friendly reading interface with the support of an automatic text annotation tool based on Chinese word segmentation and linked data, metadata search, full-text search, single and double words search, post-query classification, and information visualization analysis tool, to support digital humanities research. It aims to assist digital humanists to efficiently read or analyze the contents of Mr. Lo Chia-Lun's Writings through distant reading and close reading. Moreover, the texts of Mr. Lo Chia-Lun's Writings were aligned with time. This characteristic inspired us to develop an advanced function called Observation Tool of Viewpoint Migration and Age Division (OTVMAD) that can assist digital humanists to observe the concept change or migration with time based on dividing the texts into appropriate periods on this digital humanities platform. In addition to optimizing the functions and user interface of this platform, our future work will keep on developing more new functions, such as personal resource archives, character social network analysis, name entity recognition as well as try to apply this digital humanities research platform to assist digital humanities' instruction.",Development and Application of Digital Humanities Research Platform for Mr. Lo Chia-Lun's Writings,臺灣數位人文學會 & Ainosco Press,2020,"['digital humanities ', ' text mining ', ' information visualization ', "" Mr. Lo Chia-Lun's Writings "", ' conceptual history']",Journal of Digital Archives and Digital Humanities,5,,"[{'value': '2616-5732', 'type': None}]"
1951,http://dx.doi.org/10.6853/DADH.202004_(5).0004,"{'string_id': '10.6853/DADH.202004_(5).0004', 'id_scheme': 'DOI'}","This paper deals with the functionality that DocuSky offers to enhance the design and application of metadata when the studies of humanities are being researched within this platform. It concerns not only technical protocols but also the utilization during the procedure of humantic studies. More importantly, this paper would like to demonstrate the approach to compose a metadata and construct a personal database correspondent to scholars' needs basing on their raw materials. As a demonstration, Buddhist inscriptions are employed to elaborate the usage of tools offered by DocuSky. This demonstration further reveals the integration of academic events and the practice of composing metadata. By means of these tools, scholars may be easier to research within diverse perspectives and contexts. DocuSky provides the digital approaches and concepts for the study of humanities as digital humanities. Metadata in this platform is no longer a professional concept attributed to information organization or secondary information for data retrieving, but the intellectual property resulted from the procedure of research as well as the meaning endued by researchers.",Design and Application of Metadata in Digital Humanities Basing on DocuSky: A Case Study of Buddhist Inscriptions,臺灣數位人文學會 & Ainosco Press,2020,"['DocuSky Collaboration Platform ', ' metadata ', ' digital humanities']",Journal of Digital Archives and Digital Humanities,5,,"[{'value': '2616-5732', 'type': None}]"
1952,https://arxiv.org/abs/1010.0803v3,"{'string_id': '1010.0803', 'id_scheme': 'arXiv'}","How are people linked in a highly connected society? Since in many networks a power-law (scale-free) node-degree distribution can be observed, power-law might be seen as a universal characteristics of networks. But this study of communication in the Flickr social online network reveals that power-law node-degree distributions are restricted to only sparsely connected networks. More densely connected networks, by contrast, show an increasing divergence from power-law. This work shows that this observation is consistent with the classic idea from social sciences that similarity is the driving factor behind communication in social networks. The strong relation between communication strength and node similarity could be confirmed by analyzing the Flickr network. It also is shown that node similarity as a network formation model can reproduce the characteristics of different network densities and hence can be used as a model for describing the topological transition from weakly to strongly connected societies. ",Node similarity as a basic principle behind connectivity in complex networks,,2010,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1953,https://arxiv.org/abs/1311.5401v2,"{'string_id': '1311.5401', 'id_scheme': 'arXiv'}","Text data is often seen as ""take-away"" materials with little noise and easy to process information. Main questions are how to get data and transform them into a good document format. But data can be sensitive to noise oftenly called ambiguities. Ambiguities are aware from a long time, mainly because polysemy is obvious in language and context is required to remove uncertainty. I claim in this paper that syntactic context is not suffisant to improve interpretation. In this paper I try to explain that firstly noise can come from natural data themselves, even involving high technology, secondly texts, seen as verified but meaningless, can spoil content of a corpus; it may lead to contradictions and background noise. ",Clustering and Relational Ambiguity: from Text Data to Natural Data,,2013,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1954,https://arxiv.org/abs/1312.4617v2,"{'string_id': '1312.4617', 'id_scheme': 'arXiv'}","Social network has gained remarkable attention in the last decade. Accessing social network sites such as Twitter, Facebook LinkedIn and Google+ through the internet and the web 2.0 technologies has become more affordable. People are becoming more interested in and relying on social network for information, news and opinion of other users on diverse subject matters. The heavy reliance on social network sites causes them to generate massive data characterised by three computational issues namely; size, noise and dynamism. These issues often make social network data very complex to analyse manually, resulting in the pertinent use of computational means of analysing them. Data mining provides a wide range of techniques for detecting useful knowledge from massive datasets like trends, patterns and rules [44]. Data mining techniques are used for information retrieval, statistical modelling and machine learning. These techniques employ data pre-processing, data analysis, and data interpretation processes in the course of data analysis. This survey discusses different data mining techniques used in mining diverse aspects of the social network over decades going from the historical techniques to the up-to-date models, including our novel technique named TRCM. All the techniques covered in this survey are listed in the Table.1 including the tools employed as well as names of their authors. ",A Survey of Data Mining Techniques for Social Media Analysis,,2013,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1955,https://arxiv.org/abs/1312.5817v3,"{'string_id': '1312.5817', 'id_scheme': 'arXiv'}","This study analyzed references and source papers of the Proceedings of 2009-2012 International Conference of Digital Archives and Digital Humanities (DADH), which was held annually in Taiwan. A total of 59 sources and 1,104 references were investigated, based on descriptive analysis and subject analysis of library practices on cataloguing. Preliminary results showed historical materials, events, bureaucracies, and people of Taiwan and China in the Qing Dynasty were the major subjects in the tempo-spatial dimensions. The subject-date figure depicted a long-low head and short-high tail curve, which demonstrated both characteristics of research of humanities and application of technology in digital humanities. The dates of publication of the references spanned over 360 years, which shows a long time span in research materials. A majority of the papers (61.41%) were single-authored, which is in line with the common research practice in the humanities. Books published by general publishers were the major type of references, and this was the same as that of established humanities research. The next step of this study will focus on the comparison of characteristics of both sources and references of international journals with those reported in this article.",Exploring Regional Development of Digital Humanities Research: A Case Study for Taiwan,,2013,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1956,https://arxiv.org/abs/1312.6675v2,"{'string_id': '1312.6675', 'id_scheme': 'arXiv'}","Social media and social networks have already woven themselves into the very fabric of everyday life. This results in a dramatic increase of social data capturing various relations between the users and their associated artifacts, both in online networks and the real world using ubiquitous devices. In this work, we consider social interaction networks from a data mining perspective - also with a special focus on real-world face-to-face contact networks: We combine data mining and social network analysis techniques for examining the networks in order to improve our understanding of the data, the modeled behavior, and its underlying emergent processes. Furthermore, we adapt, extend and apply known predictive data mining algorithms on social interaction networks. Additionally, we present novel methods for descriptive data mining for uncovering and extracting relations and patterns for hypothesis generation and exploration, in order to provide characteristic information about the data and networks. The presented approaches and methods aim at extracting valuable knowledge for enhancing the understanding of the respective data, and for supporting the users of the respective systems. We consider data from several social systems, like the social bookmarking system BibSonomy, the social resource sharing system flickr, and ubiquitous social systems: Specifically, we focus on data from the social conference guidance system Conferator and the social group interaction system MyGroup. This work first gives a short introduction into social interaction networks, before we describe several analysis results in the context of online social networks and real-world face-to-face contact networks. Next, we present predictive data mining methods, i.e., for localization, recommendation and link prediction. After that, we present novel descriptive data mining methods for mining communities and patterns. ",Data Mining on Social Interaction Networks,,2013,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1957,https://arxiv.org/abs/1402.2003v2,"{'string_id': '1402.2003', 'id_scheme': 'arXiv'}","The project presents the strategy adopted by the Rough Cilicia Archaeological Survey team for publishing its primary data and reports via three potentially transformative strategies for digital humanities: Loose coupling of digital data curation and publishing platforms. In loosely coupled systems, components share only a limited set of simple assumptions, which enables systems to evolve dynamically. Collaborative creation of map based narrative content. Connecting print scholarship (book, reports, article) to online resources via two-dimensional barcodes (2D codes) that can be printed on paper and can call up hyperlinks when scanned with a Smartphone. The three strategies are made possible by loosely coupling two autonomous services: Visible Past, dedicated to web collaboration and digital-print publishing and Open Context, which is a geo-historical data archiving and publishing service. The Rough Cilicia Archaeological Survey, Visible Past, and Open Context work together to illustrate a new genre of scholarship, which combine qualitative narratives and quantitative representations of space and social phenomena. The project provides tools for collaborative creation of rich scholarly narratives that are spatially located and for connecting print publications to the digital realm. The project is a case study for utilizing the three new strategies for creating and publishing spatial humanities scholarship more broadly for ancient historians. ","A New Approach to Reporting Archaeological Surveys: Connecting Rough Cilicia, Visible Past and Open Context through loose coupling and 3d codes",,2014,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1958,https://arxiv.org/abs/1405.3539v3,"{'string_id': '1405.3539', 'id_scheme': 'arXiv'}","Using geometric data analysis, our objective is the analysis of narrative, with narrative of emotion being the focus in this work. The following two principles for analysis of emotion inform our work. Firstly, emotion is revealed not as a quality in its own right but rather through interaction. We study the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the 3-way relationship of Emma, Charles and Rodolphe in the novel Madame Bovary. Secondly, emotion, that is expression of states of mind of subjects, is formed and evolves within the narrative that expresses external events and (personal, social, physical) context. In addition to the analysis methodology with key aspects that are innovative, the input data used is crucial. We use, firstly, dialogue, and secondly, broad and general description that incorporates dialogue. In a follow-on study, we apply our unsupervised narrative mapping to data streams with very low emotional expression. We map the narrative of Twitter streams. Thus we demonstrate map analysis of general narratives. ",Pattern Recognition in Narrative: Tracking Emotional Expression in Context,,2014,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1959,https://arxiv.org/abs/1602.08657v2,"{'string_id': '1602.08657', 'id_scheme': 'arXiv'}","The software programs generally used with the TLG (Thesaurus Linguae Graecae) and the CLCLT (CETEDOC Library of Christian Latin Texts) CD-ROMs are not well suited for finding quotations and allusions. QuotationFinder uses more sophisticated criteria as it ranks search results based on how closely they match the source text, listing search results with literal quotations first and loose verbal parallels last. ",QuotationFinder - Searching for Quotations and Allusions in Greek and Latin Texts and Establishing the Degree to Which a Quotation or Allusion Matches Its Source,,2016,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1960,https://arxiv.org/abs/1602.08715v2,"{'string_id': '1602.08715', 'id_scheme': 'arXiv'}","We propose a method for efficiently finding all parallel passages in a large corpus, even if the passages are not quite identical due to rephrasing and orthographic variation. The key ideas are the representation of each word in the corpus by its two most infrequent letters, finding matched pairs of strings of four or five words that differ by at most one word and then identifying clusters of such matched pairs. Using this method, over 4600 parallel pairs of passages were identified in the Babylonian Talmud, a Hebrew-Aramaic corpus of over 1.8 million words, in just over 30 seconds. Empirical comparisons on sample data indicate that the coverage obtained by our method is essentially the same as that obtained using slow exhaustive methods. ",Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus,,2016,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1961,https://arxiv.org/abs/1602.08844v2,"{'string_id': '1602.08844', 'id_scheme': 'arXiv'}","This paper describes the Quantitative Criticism Lab, a collaborative initiative between classicists, quantitative biologists, and computer scientists to apply ideas and methods drawn from the sciences to the study of literature. A core goal of the project is the use of computational biology, natural language processing, and machine learning techniques to investigate authorial style, intertextuality, and related phenomena of literary significance. As a case study in our approach, here we review the use of sequence alignment, a common technique in genomics and computational linguistics, to detect intertextuality in Latin literature. Sequence alignment is distinguished by its ability to find inexact verbal similarities, which makes it ideal for identifying phonetic echoes in large corpora of Latin texts. Although especially suited to Latin, sequence alignment in principle can be extended to many other languages. ",Bioinformatics and Classical Literary Study,,2016,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1962,https://arxiv.org/abs/1603.01207v1,"{'string_id': '1603.01207', 'id_scheme': 'arXiv'}","Despite increasing interest in Syriac studies and growing digital availability of Syriac texts, there is currently no up-to-date infrastructure for discovering, identifying, classifying, and referencing works of Syriac literature. The standard reference work (Baumstark's Geschichte) is over ninety years old, and the perhaps 20,000 Syriac manuscripts extant worldwide can be accessed only through disparate catalogues and databases. The present article proposes a tentative data model for this http URL's New Handbook of Syriac Literature, an open-access digital publication that will serve as both an authority file for Syriac works and a guide to accessing their manuscript representations, editions, and translations. The authors hope that by publishing a draft data model they can receive feedback and incorporate suggestions into the next stage of the project. ",From manuscript catalogues to a handbook of Syriac literature: Modeling an infrastructure for Syriaca.org,,2016,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1963,https://arxiv.org/abs/1603.01597v2,"{'string_id': '1603.01597', 'id_scheme': 'arXiv'}","In this paper we consider two sequence tagging tasks for medieval Latin: part-of-speech tagging and lemmatization. These are both basic, yet foundational preprocessing steps in applications such as text re-use detection. Nevertheless, they are generally complicated by the considerable orthographic variation which is typical of medieval Latin. In Digital Classics, these tasks are traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion. For example, a lexicon is used to generate all the potential lemma-tag pairs for a token, and next, a context-aware PoS-tagger is used to select the most appropriate tag-lemma pair. Apart from the problems with out-of-lexicon items, error percolation is a major downside of such approaches. In this paper we explore the possibility to elegantly solve these tasks using a single, integrated approach. For this, we make use of a layered neural network architecture from the field of deep representation learning. ",Integrated Sequence Tagging for Medieval Latin Using Deep Representation Learning,,2016,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1964,https://arxiv.org/abs/1801.00912v3,"{'string_id': '1801.00912', 'id_scheme': 'arXiv'}","With the rapid evolution of cross-strait situation, ""Mainland China"" as a subject of social science study has evoked the voice of ""Rethinking China Study"" among intelligentsia recently. This essay tried to apply an automatic content analysis tool (CATAR) to the journal ""Mainland China Studies"" (1998-2015) in order to observe the research trends based on the clustering of text from the title and abstract of each paper in the journal. The results showed that the 473 articles published by the journal were clustered into seven salient topics. From the publication number of each topic over time (including ""volume of publications"", ""percentage of publications""), there are two major topics of this journal while other topics varied over time widely. The contribution of this study includes: 1. We could group each ""independent"" study into a meaningful topic, as a small scale experiment verified that this topic clustering is feasible. 2. This essay reveals the salient research topics and their trends for the Taiwan journal ""Mainland China Studies"". 3. Various topical keywords were identified, providing easy access to the past study. 4. The yearly trends of the identified topics could be viewed as signature of future research directions. ",How the Taiwanese Do China Studies: Applications of Text Mining,,2018,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1965,https://arxiv.org/abs/1807.04892v4,"{'string_id': '1807.04892', 'id_scheme': 'arXiv'}","In the past few years, computer vision and pattern recognition systems have been becoming increasingly more powerful, expanding the range of automatic tasks enabled by machine vision. Here we show that computer analysis of building images can perform quantitative analysis of architecture, and quantify similarities between city architectural styles in a quantitative fashion. Images of buildings from 18 cities and three countries were acquired using Google StreetView, and were used to train a machine vision system to automatically identify the location of the imaged building based on the image visual content. Experimental results show that the automatic computer analysis can automatically identify the geographical location of the StreetView image. More importantly, the algorithm was able to group the cities and countries and provide a phylogeny of the similarities between architectural styles as captured by StreetView images. These results demonstrate that computer vision and pattern recognition algorithms can perform the complex cognitive task of analyzing images of buildings, and can be used to measure and quantify visual similarities and differences between different styles of architectures. This experiment provides a new paradigm for studying architecture, based on a quantitative approach that can enhance the traditional manual observation and analysis. The source code used for the analysis is open and publicly available. ",Computer Analysis of Architecture Using Automatic Image Understanding,,2018,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1966,https://arxiv.org/abs/1808.10685v3,"{'string_id': '1808.10685', 'id_scheme': 'arXiv'}","Open-ended survey data constitute an important basis in research as well as for making business decisions. Collecting and manually analysing free-text survey data is generally more costly than collecting and analysing survey data consisting of answers to multiple-choice questions. Yet free-text data allow for new content to be expressed beyond predefined categories and are a very valuable source of new insights into people's opinions. At the same time, surveys always make ontological assumptions about the nature of the entities that are researched, and this has vital ethical consequences. Human interpretations and opinions can only be properly ascertained in their richness using textual data sources; if these sources are analyzed appropriately, the essential linguistic nature of humans and social entities is safeguarded. Natural Language Processing (NLP) offers possibilities for meeting this ethical business challenge by automating the analysis of natural language and thus allowing for insightful investigations of human judgements. We present a computational pipeline for analysing large amounts of responses to open-ended questions in surveys and extract keywords that appropriately represent people's opinions. This pipeline addresses the need to perform such tasks outside the scope of both commercial software and bespoke analysis, exceeds the performance to state-of-the-art systems, and performs this task in a transparent way that allows for scrutinising and exposing potential biases in the analysis. Following the principle of Open Data Science, our code is open-source and generalizable to other datasets. ",Extracting Keywords from Open-Ended Business Survey Questions,,2018,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1967,https://arxiv.org/abs/1912.05082v3,"{'string_id': '1912.05082', 'id_scheme': 'arXiv'}","Scholarship on underresourced languages bring with them a variety of challenges which make access to the full spectrum of source materials and their evaluation difficult. For Coptic in particular, large scale analyses and any kind of quantitative work become difficult due to the fragmentation of manuscripts, the highly fusional nature of an incorporational morphology, and the complications of dealing with influences from Hellenistic era Greek, among other concerns. Many of these challenges, however, can be addressed using Digital Humanities tools and standards. In this paper, we outline some of the latest developments in Coptic Scriptorium, a DH project dedicated to bringing Coptic resources online in uniform, machine readable, and openly available formats. Collaborative web-based tools create online 'virtual departments' in which scholars dispersed sparsely across the globe can collaborate, and natural language processing tools counterbalance the scarcity of trained editors by enabling machine processing of Coptic text to produce searchable, annotated corpora. ",A Collaborative Ecosystem for Digital Coptic Studies,,2019,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1968,https://arxiv.org/abs/2001.01863v7,"{'string_id': '2001.01863', 'id_scheme': 'arXiv'}","The goal of this work is to build a classifier that can identify text complexity within the context of teaching reading to English as a Second Language (ESL) learners. To present language learners with texts that are suitable to their level of English, a set of features that can describe the phonological, morphological, lexical, syntactic, discursive, and psychological complexity of a given text were identified. Using a corpus of 6171 texts, which had already been classified into three different levels of difficulty by ESL experts, different experiments were conducted with five machine learning algorithms. The results showed that the adopted linguistic features provide a good overall classification performance (F-Score = 0.97). A scalability evaluation was conducted to test if such a classifier could be used within real applications, where it can be, for example, plugged into a search engine or a web-scraping module. In this evaluation, the texts in the test set are not only different from those from the training set but also of different types (ESL texts vs. children reading texts). Although the overall performance of the classifier decreased significantly (F-Score = 0.65), the confusion matrix shows that most of the classification errors are between the classes two and three (the middle-level classes) and that the system has a robust performance in categorizing texts of class one and four. This behavior can be explained by the difference in classification criteria between the two corpora. Hence, the observed results confirm the usability of such a classifier within a real-world application. ",Text Complexity Classification Based on Linguistic Information: Application to Intelligent Tutoring of ESL,,2020,,Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1969,https://hal.archives-ouvertes.fr/hal-00919370v3,"{'string_id': 'hal-00919370', 'id_scheme': 'hal'}","This research study tested three different NLP technologies to analyze representative journalistic discourse used in the 2007 and 2012 presidential campaigns in France. The analysis focused on the discourse in relation to the candidate's gender and/ or political party. Our findings suggest that using specific software to examine a journalistic corpus can reveal linguistic patterns and choices made on the basis of political affiliation and/or gender stereotypes. These conclusions are drawn from quantitative and qualitative analysis carried out with three different software programs: SEMY, which semi-automatically provides semantic profiles; ANTCONC, which provides useful Keywords in Context (KWIC) or abstracts of texts, as well as collocations; TERMOSTAT, which reveals discourse specificities, frequencies and the most common morpho-syntactic patterns. Analysis of our data point to convergent asymmetries between female and male candidates in journalistic discourse (however conditionally) for the 2007 and the 2012 French presidential campaigns. We conclude that social gender (i.e., stereotypical expectations of who will be a typical member of a given category) and / or political favoritism may affect the representation of leadership in discourse, which, in turn, may influence the readership, hence the electorate. Thus the study recommends the use of corpus linguistic tools for the semi-automatic investigation of political texts.",ANALYSING JOURNALISTIC DISCOURSE AND FINDING OPINIONS SEMI-AUTOMATICALLY?: A CASE STUDY OF THE 2007 AND 2012 PRESIDENTIAL FRENCH CAMPAIGNS,,2014,"['semi-automatic reading', 'gender stereotypes', 'political favoritism', 'journalistic discourse', 'French presidential campaign', 'lecture automatique', 'stéréotypes de genre', 'biais politique', 'discours journalistique', 'campagne présidentielle française']",Journal of Data Mining & Digital Humanities,2014,,"[{'value': None, 'type': None}]"
1970,https://hal.archives-ouvertes.fr/hal-01024985v4,"{'string_id': 'hal-01024985', 'id_scheme': 'hal'}","The contribution of this article is twofold: the adaptation and application of models of deception from psychology, combined with data-mining techniques, to the text of speeches given by candidates in the 2008 U.S. presidential election; and the observation of both short-term and medium-term differences in the levels of deception. Rather than considering the effect of deception on voters, deception is used as a lens through which to observe the self-perceptions of candidates and campaigns. The method of analysis is fully automated and requires no human coding, and so can be applied to many other domains in a straightforward way. The authors posit explanations for the observed variation in terms of a dynamic tension between the goals of campaigns at each moment in time, for example gaps between their view of the candidate’s persona and the persona expected for the position; and the difficulties of crafting and sustaining a persona, for example, the cognitive cost and the need for apparent continuity with past actions and perceptions. The changes in the resulting balance provide a new channel by which to understand the drivers of political campaigning, a channel that is hard to manipulate because its markers are created subconsciously.",Deception in Speeches of Candidates for Public Office,,2015,"['political discourse', 'corpus analytics', 'U.S. presidential elections', 'deception', 'singular value decomposition']",Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1971,https://hal.archives-ouvertes.fr/hal-01265297v2,"{'string_id': 'hal-01265297', 'id_scheme': 'hal'}","The production of digital critical editions of texts using TEI is now a widely-adopted procedure within digital humanities. The work described in this paper extends this approach to the publication of gnomologia (anthologies of wise sayings) , which formed a widespread literary genre in many cultures of the medieval Mediterranean. These texts are challenging because they were rarely copied straightforwardly ; rather , sayings were selected , reorganised , modified or re-attributed between manuscripts , resulting in a highly interconnected corpus for which a standard approach to digital publication is insufficient. Focusing on Greek and Arabic collections , we address this challenge using semantic web techniques to create an ecosystem of texts , relationships and annotations , and consider a new model – organic , collaborative , interconnected , and open-ended – of what constitutes an edition. This semantic web-based approach allows scholars to add their own materials and annotations to the network of information and to explore the conceptual networks that arise from these interconnected sayings .",Computer - Assisted Processing of Intertextuality in Ancient Languages,,2017,"['TEI', 'gnomologia', 'RDF', 'manuscripts', 'ontology', 'linked data', 'semantic web', 'digital edition', 'anthologies']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1972,https://hal.archives-ouvertes.fr/hal-01276243v3,"{'string_id': 'hal-01276243', 'id_scheme': 'hal'}","The Project The literary tradition in the third and fourth centuries CE: Grammarians, rhetoricians and sophists as sources of Graeco-Roman literature (FFI2014-52808-C2-1-P) aims to trace and classify all types of quotations, both explicit (with or without mention of the author and/or title) and hidden, in a corpus comprising the Greek grammarians, rhetoricians and "" sophists "" of the third and fourth centuries CE. At the same time, we try to detect whether or not these are first-hand quotations, and if our quoting authors (28 in all) are, in turn, secondary sources for the same citations in later authors. We also study the philological (textual) aspects of the quotations in their context, and the problems of limits they sometimes pose. Finally, we are interested in the function of the quotation in the citing work. This is the first time that such a comprehensive study of this corpus is attempted. This paper explains our methodology, and how we store all these data in our electronic card-file. ","Dealing with all types of quotations (and their parallels) in a closed corpus: The methodology of the Project The literary tradition in the third and fourth centuries CE: Grammarians, rhetoricians and sophists as sources of Graeco-Roman literature",,2017,"['Intertextuality', 'Greco-Roman scholars of the Empire', 'Fragmentary literature']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1973,https://hal.archives-ouvertes.fr/hal-01279493v2,"{'string_id': 'hal-01279493', 'id_scheme': 'hal'}","Greek documentary papyri form an important direct source for Ancient Greek. It has been exploited surprisingly little in Greek linguistics due to a lack of good tools for searching linguistic structures. This article presents a new tool and digital platform, “Sematia”, which enables transforming the digital texts available in TEI EpiDoc XML format to a format which can be morphologically and syntactically annotated (treebanked), and where the user can add new metadata concerning the text type, writer and handwriting of each act of writing. An important aspect in this process is to take into account the original surviving writing vs. the standardization of language and supplements made by the editors. This is performed by creating two different layers of the same text. The platform is in its early development phase. Ongoing and future developments, such as tagging linguistic variation phenomena as well as queries performed within Sematia, are discussed at the end of the article.",Preprocessing Greek Papyri for Linguistic Annotation,,2017,"['JavaScript', 'Python', 'MySQL', 'TEI EpiDoc XML', 'Greek', 'papyri', 'linguistic annotation', 'treebank', 'dependency grammar']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1974,https://hal.archives-ouvertes.fr/hal-01280627v4,"{'string_id': 'hal-01280627', 'id_scheme': 'hal'}","If one is convinced that "" quantitative research provides data not interpretation "" [Moretti, 2005, 9], close reading should thus be considered as not only the necessary bridge between big data and interpretation but also the core duty of the Humanities. To test its potential in a neglected field – the Arabic manuscripts of the Letters of Paul of Tarsus – an enhanced, digital edition has been in development as a progression of a Swiss National Fund project. This short paper presents the development of this edition and perspectives regarding a second project. Based on the Edition Visualization Technology tool, the digital edition provides a transcription of the Arabic text, a standardized and vocalized version, as well as French translation with all texts encoded in TEI XML. Thanks to another Swiss National Foundation subsidy, a new research project on the unique New Testament, trilingual (Greek-Latin-Arabic) manuscript, the Marciana Library Gr. Z. 11 (379), 12th century, is currently underway. This project includes new features such as "" Textlink "" , "" Hotspot "" and notes: HumaReC.",Editing New Testament Arabic Manuscripts in a TEI-base: fostering close reading in Digital Humanities,,2017,"['Paul of Tarsus', 'Letters', 'Digital edition', 'Arabic TEI', 'New Testament']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1975,https://hal.archives-ouvertes.fr/hal-01281266v2,"{'string_id': 'hal-01281266', 'id_scheme': 'hal'}","The ancient commentaries provide a large sample of quotations from classical or biblical texts for which Latin gramamrians developed a complex system of insertion of quoted texts. The paper examines how to encode these places using XML Tei, and focuses on difficult cases, such as inaccurate quotations, or quotations of partly or wholly lost texts.","Encoding (inter)textual insertions in Latin ""grammatical commentary""",,2017,"['Ancient commentary', 'allusion', 'fragmentary texts', 'mentioned words', 'Latin grammar', 'XML Tei', 'quotation']",Journal of Data Mining & Digital Humanities,,,"[{'value': None, 'type': None}]"
1976,https://hal.archives-ouvertes.fr/hal-01282568v4,"{'string_id': 'hal-01282568', 'id_scheme': 'hal'}","Most intertextuality in classical poetry is unmarked, that is, it lacks objective signposts to make readers aware of the presence of references to existing texts. Intergeneric relationships can pose a particular problem as scholarship has long privileged intertextual relationships between works of the same genre. This paper treats the influence of Latin love elegy on Lucan’s epic poem, Bellum Civile, by looking at two features of unmarked intertextuality: frequency and distribution. I use the Tesserae project to generate a dataset of potential intertexts between Lucan’s epic and the elegies of Tibullus, Propertius, and Ovid, which are then aggregrated and mapped in Lucan’s text. This study draws two conclusions: 1. measurement of intertextual frequency shows that the elegists contribute fewer intertexts than, for example, another epic poem (Virgil’s Aeneid), though far more than the scholarly record on elegiac influence in Lucan would suggest; and 2. mapping the distribution of intertexts confirms previous scholarship on the influence of elegy on the Bellum Civile by showing concentrations of matches, for example, in Pompey and Cornelia’s meeting before Pharsalus (5.722-815) or during the affair between Caesar and Cleopatra (10.53-106). By looking at both frequency and proportion, we can demonstrate systematically the generic enrichment of Lucan’s Bellum Civile with respect to Latin love elegy.",Measuring and Mapping Intergeneric Allusion in Latin Poetry using Tesserae,,2017,"['allusion', ' Lucan', ' Latin epic', ' Latin love elegy', ' intertextuality', ' generic enrichment', ' Tesserae']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1977,https://hal.archives-ouvertes.fr/hal-01283638v2,"{'string_id': 'hal-01283638', 'id_scheme': 'hal'}","Colophons of Armenian manuscripts are replete with yet untapped riches. Formulae are not the least among them: these recurrent stereotypical patterns conceal many clues as to the schools and networks of production and diffusion of books in Armenian communities. This paper proposes a methodology for exploiting these sources, as elaborated in the framework of a PhD research project about Armenian colophon formulae. Firstly, the reader is briefly introduced to the corpus of Armenian colophons and then, to the purposes of our project. In the third place, we describe our methodology, relying on lemmatization and modelling of patterns into automata. Fourthly and finally, the whole process is illustrated by a basic case study, the occasion of which is taken to outline the kind of results that can be achieved by combining this methodology with a philologico-historical approach to colophons.",Recurrent Pattern Modelling in a Corpus of Armenian Manuscript Colophons,,2017,"['codicology', 'Unitex', 'finite state transducers', 'Armenian colophons', 'automata', 'colophon formula', 'formulaic patterns', 'lemmatization', 'manuscript studies']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1978,https://hal.archives-ouvertes.fr/hal-01287195v4,"{'string_id': 'hal-01287195', 'id_scheme': 'hal'}","This contribution to a special issue on “Computer-aided processing of intertextuality” in ancient texts will illustrate how using digital tools to interact with the Hebrew Bible offers new promising perspectives for visualizing the texts and for performing tasks in education and research. This contribution explores how the corpus of the Hebrew Bible created and maintained by the Eep Talstra Centre for Bible and Computer can support new methods for modern knowledge workers within the field of digital humanities and theology be applied to ancient texts, and how this can be envisioned as a new field of digital intertextuality. The article first describes how the corpus was used to develop the Bible Online Learner as a persuasive technology to enhance language learning with, in, and around a database that acts as the engine driving interactive tasks for learners. Intertextuality in this case is a matter of active exploration and ongoing practice. Furthermore, interactive corpus-technology has an important bearing on the task of textual criticism as a specialized area of research that depends increasingly on the availability of digital resources. Commercial solutions developed by software companies like Logos and Accordance offer a market-based intertextuality defined by the production of advanced digital resources for scholars and students as useful alternatives to often inaccessible and expensive printed versions. It is reasonable to expect that in the future interactive corpus technology will allow scholars to do innovative academic tasks in textual criticism and interpretation. We have already seen the emergence of promising tools for text categorization, analysis of translation shifts, and interpretation. Broadly speaking, interactive tools and tasks within the three areas of language learning, textual criticism, and Biblical studies illustrate a new kind of intertextuality emerging within digital humanities. ",Interactive Tools and Tasks for the Hebrew Bible: From Language Learning to Textual Criticism,,2017,"['Bible Online Learner', 'Joshua 24', ' Hebrew Bible', 'Digital intertextuality', 'Corpus', 'Logos Bible software', 'language learning', 'Hebrew Bible', 'ETCBC', 'textual criticism', 'Joshua']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1979,https://hal.archives-ouvertes.fr/hal-01294158v2,"{'string_id': 'hal-01294158', 'id_scheme': 'hal'}","The project is to develop a database, which is planned to include all available information on the use of the Bible in the patristic works of Migne's Patrologia Graeca. Utilization of the data will be available through a web page equipped with necessary tools for developing data mining techniques and other methods of analysis. The main aim of the project is to revive the catenae, the ancient exegetical tool for biblical interpretation.",Digital Greek Patristic Catena (DGPC). A brief presentation,,2017,"['patristic authors', 'Catena', 'bible references', 'biblical exegesis', 'database', 'Patrologia Graeca']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1980,https://hal.archives-ouvertes.fr/hal-01294591v2,"{'string_id': 'hal-01294591', 'id_scheme': 'hal'}",This paper discusses the word level alignment of lemmatised bitext consisting of the Oratio I of Gregory of Nazianzus in its Greek model and Georgian translation. This study shows how the direct and empirical observations offered by an aligned text enable an accurate analysis of techniques of translation and many philological parameters of the text.,Text Alignment in Ancient Greek and Georgian: A Case-Study on the First Homily of Gregory of Nazianzus,,2017,"['lexical tagging', 'corpus', 'bilingual dictionaries', 'bitext', 'Ancient Greek', 'Ancient Georgian', 'morphological tagging', 'text alignment', 'lemmatisation', ' morphological tagging']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1981,https://hal.archives-ouvertes.fr/hal-01371751v3,"{'string_id': 'hal-01371751', 'id_scheme': 'hal'}","We describe the course of a hackathon dedicated to the development of linguistic tools for Tibetan Buddhist studies. Over a period of five days, a group of seventeen scholars, scientists, and students developed and compared algorithms for intertextual alignment and text classification, along with some basic language tools, including a stemmer and word segmenter.",A Hackathon for Classical Tibetan,,2018,"['Tibetan', 'Buddhist studies', 'hackathon', 'stemming', 'segmentation', 'intertextual alignment', 'text classification']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1982,https://hal.archives-ouvertes.fr/hal-01443713v1,"{'string_id': 'hal-01443713', 'id_scheme': 'hal'}","Cloud Computing and Big Data are the upcoming Information Technology (IT) computing models. These groundbreaking paradigms are leading IT to a new set of rules that aims to change computing resources delivery and exploitation model, thus creating a novel business market that is exponentially growing and attracting more and more investments from both providers and end users that are looking forward to make profits from these innovative models of computing. In the same context, researchers and investigators are wrestling time in order to develop, test and optimize Cloud Computing and Big Data platforms, whereas several studies are ongoing to determine and enhance the essential aspects of these computing models especially compute resources allocation. The processing power scheduling is crucial when it comes to Cloud Computing and Big Data because of the data growth management and delivery design proposed by these new computing models, that requires faster responses from platforms and applications. Hence originates the importance of developing high efficient scheduling algorithms that are compliant with these computing models platforms and infrastructures requirement.",Smarter Round Robin Scheduling Algorithm for Cloud Computing and Big Data,,2017,"['Scheduling Algorithms', 'Cloud Computing Simulation', 'First Come First Served', 'Cloud Computing', 'Big Data', 'Algorithme de planification', 'Simulation Cloud Computing', 'Round Robin', 'Premier Arrivé Premier Servi']",Journal of Data Mining & Digital Humanities,Special Issue on Scientific and Technological Strategic Intelligence (2016),,"[{'value': None, 'type': None}]"
1983,https://hal.archives-ouvertes.fr/hal-01456090v2,"{'string_id': 'hal-01456090', 'id_scheme': 'hal'}","This paper proposes an ontological integration model for credit risk management. It is based on three ontologies; one is global describing credit risk management process and two other locals, the first, describes the credit granting process, and the second presents the concepts necessary for the monitoring of credit system. This paper also presents the technique used for matching between global ontology and local ontologies. ",Applying ontologies to data integration systems for bank credit risk management,,2017,"['credit risk management', ' data integration', ' ontologies alignment']",Journal of Data Mining & Digital Humanities,Special Issue on Scientific and Technological Strategic Intelligence (2016),,"[{'value': None, 'type': None}]"
1984,https://hal.archives-ouvertes.fr/hal-01458216v1,"{'string_id': 'hal-01458216', 'id_scheme': 'hal'}","In this paper we present a system for offline recognition cursive Arabic handwritten text which is analytical without explicit segmentation based on Hidden Markov Models (HMMs). Extraction features preceded by baseline estimation are statistical and geometric to integrate both the peculiarities of the text and the pixel distribution characteristics in the word image. These features are modelled using hidden Markov models. The HMM-based classifiercontains a training module and a recognition module. The training module estimates the parameters of each of the character HMMs uses the Baum-Welchalgorithm. In the recognition phase, feature vectors extracted from an image are passed to a network of word lexicon entries formed of character models. The character sequence providing the maximumlikelihood identifies the recognized entry. If required, the recognition can generate N best output hypotheses rather than just the single best one. To determine the best output hypotheses, the Viterbi algorithm is used.The experiments on images of the benchmark IFN/ENIT database show that the proposed system improves recognition.",Cursive Arabic Handwriting Recognition System Without Explicit Segmentation Based on Hidden Markov Models,,2017,"['HMMs', 'Arabic text', 'handwriting', 'Recognition']",Journal of Data Mining & Digital Humanities,Special Issue on Scientific and Technological Strategic Intelligence (2016),,"[{'value': None, 'type': None}]"
1985,https://hal.archives-ouvertes.fr/hal-01466986v1,"{'string_id': 'hal-01466986', 'id_scheme': 'hal'}","Digital humanities require IT Infrastructure and sophisticated analytical tools, including data visualization, data mining, statistics, text mining and information retrieval. Regarding funding, to build a local data center will necessitate substantial investments. Fortunately, there is another option that will help researchers take advantage of these IT services to access, use and share information easily. Cloud services ideally offer on-demand software and resources over the Internet to read and analyze ancient documents. More interestingly, billing system is completely flexible and based on resource usage and Quality of Service (QoS) level. In spite of its multiple advantages, outsourcing computations to an external provider arises several challenges. Specifically, security is the major factor hindering the widespread acceptance of this new concept. As a case study, we review the use of cloud computing to process digital images safely. Recently, various solutions have been suggested to secure data processing in cloud environement. Though, ensuring privacy and high performance needs more improvements to protect the organization's most sensitive data. To this end, we propose a framework based on segmentation and watermarking techniques to ensure data privacy. In this respect, segementation algorithm is used to to protect client's data against untauhorized access, while watermarking method determines and maintains ownership. Consequentely, this framework will increase the speed of development on ready-to-use digital humanities tools.",A Secured Data Processing Technique for Effective Utilization of Cloud Computing,,2017,"['cloud computing', 'digital humanities', 'security', 'data processing']",Journal of Data Mining & Digital Humanities,Special Issue on Scientific and Technological Strategic Intelligence (2016),,"[{'value': None, 'type': None}]"
1986,https://hal.archives-ouvertes.fr/hal-01528092v2,"{'string_id': 'hal-01528092', 'id_scheme': 'hal'}","The Text Alignment Network (TAN) is a suite of XML encoding formats intended to serve anyone who wishes to encode, exchange, and study multiple versions of texts (e.g., translations, paraphrases), and annotations on those texts (e.g., quotations, word-for-word correspondences). This article focuses on TAN’s innovative intertextual pointers, which, I argue, provide an unprecedented level of readability, interoperability, and semantic context. Because TAN is a new, experimental format, this article provides a brief introduction to the format and concludes with comments on progress and future prospects.",Intertextual Pointers in the Text Alignment Network,,2017,"['alignment', ' TEI', 'XML', ' TAN', ' intertextuality', ' canonical references']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1987,https://hal.archives-ouvertes.fr/hal-01645124v2,"{'string_id': 'hal-01645124', 'id_scheme': 'hal'}","This article explores whether and how network visualization can benefit philological and historical-linguistic study. This is illustrated with a corpus-based investigation of scribes' language use in a lemmatized and morphologically annotated corpus of documentary Latin (Late Latin Charter Treebank, LLCT2). We extract four continuous linguistic variables from LLCT2 and utilize a gradient colour palette in Gephi to visualize the variable values as node attributes in a trimodal network which consists of the documents, writers, and writing locations underlying the same corpus. We call this network the ""LLCT2 network"". The geographical coordinates of the location nodes form an approximate map, which allows for drawing geographical conclusions. The linguistic variables are examined both separately and as a sum variable, and the visualizations presented as static images and as interactive Sigma.js visualizations. The variables represent different domains of language competence of scribes who learnt written Latin practically as a second-language. The results show that the network visualization of linguistic features helps in observing patterns which support linguistic-philological argumentation and which risk passing unnoticed with traditional methods. However, the approach is subject to the same limitations as all visualization techniques: the human eye can only perceive a certain, relatively small amount of information at a time.",Visualizing linguistic variation in a network of Latin documents and scribes,,2018,"['network visualization', ' Latin linguistics', ' Early Middle Ages', ' philology']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1988,https://hal.archives-ouvertes.fr/hal-01671592v1,"{'string_id': 'hal-01671592', 'id_scheme': 'hal'}","This paper presents some computer tools and linguistic resources of the GREgORI project. These developments allow automated processing of texts written in the main languages of the Christian Middel East, such as Greek, Arabic, Syriac, Armenian and Georgian. The main goal is to provide scholars with tools (lemmatized indexes and concordances) making corpus-based linguistic information available. It focuses on the questions of text processing, lemmatization, information retrieval, and bitext alignment.",Processing Tools for Greek and Other Languages of the Christian Middle East,,2017,"['Lemmatization', 'Greek', 'Syriac', 'Arabic', 'Armenian', 'Georgian', 'lexical tagging', 'POS tagging', 'concordances', 'indexes', 'bitext', 'bilingual alignment', 'translation memories', 'mkAlign']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
1989,https://hal.archives-ouvertes.fr/hal-01759191v2,"{'string_id': 'hal-01759191', 'id_scheme': 'hal'}","The EUROPANGE project, involving both medievalists and computer scientists, aims to study the emergence of a corps of administrators in the Angevin controlled territories in the XIII–XV centuries. Our project attempts to analyze the officers' careers, shared relation networks and strategies based on the study of individual biographies. In this paper, we describe methods and tools designed to analyze these prosopographical data. These include OLAP analyzes and network analyzes associated with cartographic and chronological visualization tools.",Prosopographical data analysis. Application to the Angevin officers (XIII–XV centuries),,2018,[],Journal of Data Mining & Digital Humanities,Special Issue on Data Science and Digital Humanities @ EGC 2018,,"[{'value': None, 'type': None}]"
1990,https://hal.archives-ouvertes.fr/hal-01762730v5,"{'string_id': 'hal-01762730', 'id_scheme': 'hal'}","This work applies knowledge engineering’s techniques to medieval illuminations. Inside it, an illumination is considered as a knowledge graph which was used by some elites in the Middle Ages to represent themselves as a social group and exhibit the events in their lives, and their cultural values. That graph is based on combinations of symbolic elements linked each to others with semantic relations. Those combinations were used to encode visual metaphors and influential messages whose interpretations are sometimes tricky for not experts. Our work aims to describe the meaning of those elements through logical modelling using ontologies. To achieve that, we construct logical reasoning rules and simulate them using artificial intelligence mechanisms. The goal is to facilitate the interpretation of illuminations and provide, in a future evolution of current social media, logical formalisation of new encoding and information transmission services.",Causal reasoning and symbolic relationships in Medieval Illuminations,,2019,"['Semantic relation', 'Symbolic relation', 'Ontology', 'Social network', 'Medieval illumination']",Journal of Data Mining & Digital Humanities,Special Issue on Data Science and Digital Humanities @ EGC 2018,,"[{'value': None, 'type': None}]"
1991,https://hal.archives-ouvertes.fr/hal-01913435v3,"{'string_id': 'hal-01913435', 'id_scheme': 'hal'}","The Foucault Fiches de Lecture (FFL) project aims both to explore and to make available online a large set of Michel Foucault’s reading notes (organized citations, references and comments) held at the BnF since 2013. Therefore, the team is digitizing, describing and enriching the reading notes that the philosopher gathered while preparing his books and lectures, thus providing a new corpus that will allow a new approach to his work. In order to release the manuscripts online, and to collectively produce the data, the team is also developing a collaborative platform, based on RDF technologies, and designed to link together archival content and bibliographic data. This project is financed by the ANR (2017-2020) and coordinated by Michel Senellart, professor of philosophy at the ENS Lyon. It benefits from the partnerships of the ENS/PSL and the BnF. In addition, a collaboration with the European READ/Transkribus project has been started so as to produce automatic transcription of the reading notes.",Transcribing Foucault’s handwriting with Transkribus,,2019,"['transkribus', 'reading notes', 'automatic transcription of manuscripts', 'artificial intelligence']",Journal of Data Mining & Digital Humanities,Atelier Digit_Hum,,"[{'value': None, 'type': None}]"
1992,https://hal.archives-ouvertes.fr/hal-01915730v2,"{'string_id': 'hal-01915730', 'id_scheme': 'hal'}","British philosopher and reformer Jeremy Bentham (1748-1832) left over 60,000 folios of unpublished manuscripts. The Bentham Project, at University College London, is creating a TEI version of the manuscripts, via crowdsourced transcription verified by experts. We present here an interface to navigate these largely unedited manuscripts, and the language technologies the corpus was enriched with to facilitate navigation, i.e Entity Linking against the DBpedia knowledge base and keyphrase extraction. The challenges of tagging a historical domain-specific corpus with a contemporary knowledge base are discussed. The concepts extracted were used to create interactive co-occurrence networks, that serve as a map for the corpus and help navigate it, along with a search index. These corpus representations were integrated in a user interface. The interface was evaluated by domain experts with satisfactory results , e.g. they found the distributional semantics methods exploited here applicable in order to assist in retrieving related passages for scholarly editing of the corpus.",Mapping the Bentham Corpus: Concept-based Navigation,,2019,"['Jeremy Bentham', 'manuscripts', 'corpus navigation', 'entity linking', 'keyphrase extraction']",Journal of Data Mining & Digital Humanities,Special Issue: Digital Humanities between knowledge and know-how (Atelier Digit_Hum),,"[{'value': None, 'type': None}]"
1993,https://hal.archives-ouvertes.fr/hal-01981922v3,"{'string_id': 'hal-01981922', 'id_scheme': 'hal'}","This paper addresses the integration of a Named Entity Recognition and Disambiguation (NERD) service within a group of open access (OA) publishing digital platforms and considers its potential impact on both research and scholarly publishing. The software powering this service, called entity-fishing, was initially developed by Inria in the context of the EU FP7 project CENDARI and provides automatic entity recognition and disambiguation using the Wikipedia and Wikidata data sets. The application is distributed with an open-source licence, and it has been deployed as a web service in DARIAH's infrastructure hosted by the French HumaNum. In the paper, we focus on the specific issues related to its integration on five OA platforms specialized in the publication of scholarly monographs in the social sciences and humanities (SSH), as part of the work carried out within the EU H2020 project HIRMEOS (High Integration of Research Monographs in the European Open Science infrastructure). In the first section, we give a brief overview of the current status and evolution of OA publications, considering specifically the challenges that OA monographs are encountering. In the second part, we show how the HIRMEOS project aims to face these challenges by optimizing five OA digital platforms for the publication of monographs from the SSH and ensuring their interoperability. In sections three and four we give a comprehensive description of the entity-fishing service, focusing on its concrete applications in real use cases together with some further possible ideas on how to exploit the annotations generated. We show that entity-fishing annotations can improve both research and publishing process. In the last chapter, we briefly present further possible application scenarios that could be made available through infrastructural projects.",Leveraging Concepts in Open Access Publications,,2019,"['Open Access', 'Named Entity Recognition and Disambiguation (NERD)', 'Entity-Fishing', 'Monographs', 'Digital Publishing Platforms']",Journal of Data Mining & Digital Humanities,2019,,"[{'value': None, 'type': None}]"
1994,https://hal.archives-ouvertes.fr/hal-02109972v2,"{'string_id': 'hal-02109972', 'id_scheme': 'hal'}","The creation of the Artist Libraries Project was sparked by the observation that artist libraries are still not well known, yet many art historians are interested in this archive for the value it adds to understanding the person behind the artist and his or her creative process. The problem is that these libraries are rarely physically preserved. To remedy this dispersion, we built an online database and a website www.lesbibliothequesdartistes.org that house this valuable source in the form of lists of books and their electronic versions. First data on Monet's library have been made available, and several additional artist libraries from the 19 th and 20 th centuries are on the way for 2019. By gathering all these bibliographical data in a central database, it's possible to explore one library and to compare several. This article explains how we built the database and the website and how the implementation of those IT tools has raised questions about the use of this resource as an archive on the one hand, as well as its value for art history on the other.",The Artist Libraries Project in the Labex Les passés dans le présent,,2019,[],Journal of Data Mining & Digital Humanities,Atelier Digit_Hum,,"[{'value': None, 'type': None}]"
1995,https://hal.archives-ouvertes.fr/hal-02154122v2,"{'string_id': 'hal-02154122', 'id_scheme': 'hal'}","Tokenization of modern and old Western European languages seems to be fairly simple, as it stands on the presence mostly of markers such as spaces and punctuation. However, when dealing with old sources like manuscripts written in scripta continua, antiquity epigraphy or Middle Age manuscripts, (1) such markers are mostly absent, (2) spelling variation and rich morphology make dictionary based approaches difficult. Applying convolutional encoding to characters followed by linear categorization to word-boundary or in-word-sequence is shown to be effective at tokenizing such inputs. Additionally, the software is released with a simple interface for tokenizing a corpus or generating a training set.",Evaluating Deep Learning Methods for Word Segmentation of Scripta Continua Texts in Old French and Latin,,2020,"['convolutional network', 'scripta continua', 'tokenization', 'Old French', 'word segmentation']",Journal of Data Mining & Digital Humanities,2020,,"[{'value': None, 'type': None}]"
1996,https://hal.archives-ouvertes.fr/hal-02280013v2,"{'string_id': 'hal-02280013', 'id_scheme': 'hal'}","While online crowdsourced text transcription projects have proliferated in the last decade, there is a need within the broader field to understand differences in project outcomes as they relate to task design, as well as to experiment with different models of online crowdsourced transcription that have not yet been explored. The experiment discussed in this paper involves the evaluation of newly-built tools on the Zooniverse.org crowdsourcing platform, attempting to answer the research question: ""Does the current Zooniverse methodology of multiple independent transcribers and aggregation of results render higher-quality outcomes than allowing volunteers to see previous transcriptions and/or markings by other users? How does each methodology impact the quality and depth of analysis and participation?"" To answer these questions, the Zooniverse team ran an A/B experiment on the project Anti-Slavery Manuscripts at the Boston Public Library. This paper will share results of this study, and also describe the process of designing the experiment and the metrics used to evaluate each transcription method. These include the comparison of aggregate transcription results with ground truth data; evaluation of annotation methods; the time it took for volunteers to complete transcribing each dataset; and the level of engagement with other project elements such as posting on the message board or reading supporting documentation. Particular focus will be given to the (at times) competing goals of data quality, efficiency, volunteer engagement, and user retention, all of which are of high importance for projects that focus on data from galleries, libraries, archives and museums. Ultimately, this paper aims to provide a model for impactful, intentional design and study of online crowdsourcing transcription methods, as well as shed light on the associations between project design, methodology and outcomes.",Individual vs. Collaborative Methods of Crowdsourced Transcription,,2019,[],Journal of Data Mining & Digital Humanities,"Special Issue on Collecting, Preserving, and Disseminating Endangered Cultural Heritage for New Understandings through Multilingual Approaches",,"[{'value': None, 'type': None}]"
1997,https://hal.archives-ouvertes.fr/hal-02324617v2,"{'string_id': 'hal-02324617', 'id_scheme': 'hal'}","In this study, we address the interesting task of classifying historical texts by their assumed period of writ-ing. This task is useful in digital humanity studies where many texts have unidentified publication dates.For years, the typical approach for temporal text classification was supervised using machine-learningalgorithms. These algorithms require careful feature engineering and considerable domain expertise todesign a feature extractor to transform the raw text into a feature vector from which the classifier couldlearn to classify any unseen valid input. Recently, deep learning has produced extremely promising re-sults for various tasks in natural language processing (NLP). The primary advantage of deep learning isthat human engineers did not design the feature layers, but the features were extrapolated from data witha general-purpose learning procedure. We investigated deep learning models for period classification ofhistorical texts. We compared three common models: paragraph vectors, convolutional neural networks (CNN) and recurrent neural networks (RNN), and conventional machine-learning methods. We demon-strate that the CNN and RNN models outperformed the paragraph vector model and the conventionalsupervised machine-learning algorithms. In addition, we constructed word embeddings for each timeperiod and analyzed semantic changes of word meanings over time.",Deep Learning for Period Classification of Historical Hebrew Texts,,2020,"['Machine Learning', 'Deep Learning', 'Diachronic Corpus', 'Period Classification']",Journal of Data Mining & Digital Humanities,2020,,"[{'value': None, 'type': None}]"
1998,https://hal.archives-ouvertes.fr/hal-02513038v2,"{'string_id': 'hal-02513038', 'id_scheme': 'hal'}","The study of watermarks is a key step for archivists and historians as it enables them to reveal the origin of paper. Although highly practical, automatic watermark recognition comes with many difficulties and is still considered an unsolved challenge. Nonetheless, Shen et al. [2019] recently introduced a new approach for this specific task which showed promising results. Building upon this approach, this work proposes a new public web application dedicated to automatic watermark recognition entitled Filigranes pour tous. The application not only hosts a detailed catalog of more than 17k watermarks manually collected from the French National Archives (Minutier central) or extracted from existing online resources (Briquet database), but it also enables non-specialists to identify a watermark from a simple photograph in a few seconds. Moreover, additional watermarks can easily be added by the users making the enrichment of the existing catalog possible through crowdsourcing. Our Web application is available at http://filigranes.inria.fr/.",A Web Application for Watermark Recognition,,2020,"['cross-domain recognition', 'deep learning', 'watermark recognition', 'web application', 'paper analysis']",Journal of Data Mining & Digital Humanities,24,,"[{'value': None, 'type': None}]"
1999,https://hal.archives-ouvertes.fr/hal-02520508v3,"{'string_id': 'hal-02520508', 'id_scheme': 'hal'}","This article proposes use the Transkribus software to report on a ""user experiment"" in a French-speaking context. It is based on the semi-automated transcription project using the diary of the jurist Eugène Wilhelm (1866-1951). This diary presents two main challenges. The first is related to the time covered by the writing process-66 years. This leads to variations in the form of the writing, which becomes increasingly ""unreadable"" with time. The second challenge is related to the concomitant use of two alphabets: Roman for everyday text and Greek for private issues. After presenting the project and the specificities related to the use of the tool, the experiment presented in this contribution is structured around two aspects. Firstly, I will summarise the main obstacles encountered and the solutions provided to overcome them. Secondly, I will come back to the collaborative transcription experiment carried out with students in the classroom, presenting the difficulties observed and the solutions found to overcome them. In conclusion, I will propose an assessment of the use of this Human Text Recognition software in a French-speaking context and in a teaching situation.",Optical Recognition Assisted Transcription with Transkribus: The Experiment concerning Eugène Wilhelm's Personal Diary (1885-1951),,2020,"['TEI', 'User Experience', 'Human Text Recognition', 'Learning process', 'OCR']",Journal of Data Mining & Digital Humanities,Atelier Digit_Hum,,"[{'value': None, 'type': None}]"
2000,https://hal.archives-ouvertes.fr/halshs-01532877v2,"{'string_id': 'halshs-01532877', 'id_scheme': 'hal'}","The ‘Version Variation Visualization’ project has developed online tools to support comparative, algorithm-assisted investigations of a corpus of multiple versions of a text, e.g. variants, translations, adaptations (Cheesman, 2015, 2016; Cheesman et al., 2012, 2012-13, 2016; Thiel, 2014; links: www.tinyurl.com/vvvex). A segmenting and aligning tool allows users to 1) define arbitrary segment types, 2) define arbitrary text chunks as segments, and 3) align segments between a ‘base text’ (a version of the ‘original’ or translated text), and versions of it. The alignment tool can automatically align recurrent defined segment types in sequence. Several visual interfaces in the prototype installation enable exploratory access to parallel versions, to comparative visual representations of versions’ alignment with the base text, and to the base text visually annotated by an algorithmic analysis of variation among versions of segments. Data can be filtered, viewed and exported in diverse ways. Many more modes of access and analysis can be envisaged. The tool is language neutral. Experiments so far mostly use modern texts: German Shakespeare translations. Roos is working on a collection of approx. 100 distinct English-language translations of a Hebrew text with ancient Hebrew and Aramaic passages: the Haggadah (Roos, 2015)",Version Variation Visualization (VVV): Case Studies on the Hebrew Haggadah in English,,2017,"['digital humanities', 'Social science', 'translation', ' corpus', ' Judaism', ' haggadah', ' VVV', ' visualization', ' algorithm', ' retranslation', ' 4 sons', 'humanities']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
2001,https://hal.archives-ouvertes.fr/halshs-01543050v2,"{'string_id': 'halshs-01543050', 'id_scheme': 'hal'}","This paper discusses markup strategies for the identification and description of text reuses in a corpus of patristic texts related to the BIBLINDEX Project, an online index of biblical references in Early Christian Literature. In addition to the development of a database that can be queried by canonical biblical or patristic references, a sample corpus of patristic texts has been encoded following the guidelines of the TEI (Text Encoding Initiative), in order to provide direct access to quoted and quoting text passages to the users of the https://www.biblindex.info platform.",TEI-encoding of text reuses in the BIBLINDEX Project,,2017,"['patristics', ' Bernard of Clairvaux', ' Sources Chrétiennes', ' text reuses', 'BIBLINDEX', ' quotations', ' Biblia Patristica', 'TEI', ' Text Encoding Initiative', ' text markup', ' Bible', ' Greek', ' Latin', ' Septuagint', ' Vulgata']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
2002,https://hal.archives-ouvertes.fr/halshs-01557447v1,"{'string_id': 'halshs-01557447', 'id_scheme': 'hal'}","A new method for grouping manuscripts in clusters is presented with the calculation of distances between readings, then between witnesses. A classification algorithm ("" Hierarchical Ascendant Clustering ""), achieved through computer-aided processing, enables the construction of trees illustrating the textual taxonomy obtained. This method is applied to the Old Latin witnesses of the Gospel of John, and, in order to provide a study of a reasonable size, to a chapter as a whole (chapter 14). The result basically confirms the text-types identified by Bonatius Fischer, founder of the Vetus Latina Institute, while it invalidates the classification adopted by the current edition of the Vetus Latina of the Gospel of John.",A Classification of Manuscripts Based on A New Quantitative Method. The Old Latin Witnesses of John's Gospel as Text Case,,2017,"['Manuscripts', 'Gospel of John', 'latin witnesses']",Journal of Data Mining & Digital Humanities,Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages,,"[{'value': None, 'type': None}]"
2003,http://computerphilologie.digital-humanities.de/jg09/lukasosthof.pdf,"{'string_id': None, 'id_scheme': None}","Spatiality – the topographical distribution of text on the manuscript page – represents one of the central aspects of the epistolographical ›material text‹. In contrast to a diplomatic-mimetic text reproduction, the C.F.Meyer edition distinguishes between significant (structural) vs. meaningless (contingent) spatiality. The contribution reflects the theoretical foundations and implications of this concept in the context of the current materiality debate in textual scholarship and presents a solution for the implementation of the philological model in TEI.",Physische vs. gedeutete Räumlichkeit. Zur Auszeichnung spatialer Informationen in der historisch-kritischen Ausgabe C.F. Meyers Briefwechsel,,2016,,Jahrbuch für Computerphilologie,30,,"[{'value': None, 'type': None}]"
2004,http://projects.chass.utoronto.ca/chwp/Casta02/Wooldridge_casta02.htm,"{'string_id': None, 'id_scheme': None}","The largest electronic text database in existence, the World Wide Web, makes possible, for the first time and without any outlay of costly set-up or the expensive development of project-specific tools, the observation of current usage in a number of different languages. The Web asdatabase has also the enormous advantage over other language corpora of behaving like natural language: it is dynamic, unceasingly renewing itself and thus offering snapshots of the present state of the language, with its proportion of new, established and aging usage. The paper discusses the observation, since 1998, of French, including derivation, polysemy, homophony, syntagmatic variation and the genesis of new concepts and their linguistic naming. The language corpora observed are those of micro-systems — particular words, word families and lexical constructions — and not the macro-system of the language as a whole. The tools used in the observation are keywords and search engines, in particular Google.",The World Wide Web as a Linguistic Corpus,,2005,"['World Wide Web, corpus linguistics, WWW as database, polysemy, homophony, syntagmatic variation, neology']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2005,http://projects.chass.utoronto.ca/chwp/CHC2003/Roberts_Smith2.htm,"{'string_id': None, 'id_scheme': None}","In The Arte of English Poesie, putative author George Puttenham applies the word “tune”, which he says refers to musical structure, to poetic metre, saying “our maker by his measures and concordes of sundry proportions doth counterfait the harmonicall tunes [my italics] of the vocall and instrumentall Musickes”. He uses “tune” to refer, literally, to the number and duration of syllables, stress, spoken pitch, segmental features, or the combinations of some or all of these elements which he says create musical rhythms in spoken verse. Although his usage is not supported in citations dating from 1500-1700 in the OED, the working copy of the Lexicons of Early Modern English database provides new evidence supporting Puttenham's usage. Eleven contemporary hard-word dictionaries, bilingual dictionaries and rhetoric handbooks by Sherry (1550), Cooper (1584), Thomas (1587), Coote (1596), Minsheu (1599), Cawdrey (1604), Florio (1598, 1611), Cotgrave (1611), Cockeram (1623), and Blount (1656) use “tune” to signify musical rhythm, spoken pitch, and prosody. They also use “tune” as a synonym for “accent”, which signifies syllable duration and stress. In all eleven lexical works, “tune” is an easy word used to explain other, more difficult words, including “accent”. The lexicographers' expectation that readers will be familiar with their usage of the word “tune” suggests that people in the English Renaissance generally heard the musical elements of “tune” in spoken language. Poets who use “tune” to describe metre or rhythm, such as Ben Jonson and Shakespeare, may have exploited its musical elements (syllable duration and spoken pitch) in their poetry.",Puttenham rehabilitated: the significance of ‘tune’ in The Arte of English Poesie,,2003,"['poetry, music, Renaissance, tune, Puttenham, The Arte of English Poesie, dictionaries, Lexicons of Early Modern English, LEME, metre, rhythm, Early Modern English']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2006,,"{'string_id': None, 'id_scheme': None}"," This essay argues that the urban collective novel serves as an important modernist precursor to network narratives. The collective novel is a literary form, particularly popular during the 1930s, that explores a wide context through a decentered narrative. Previous discussions of these novels have focused on them as exemplars of modernist form in proletarian literature. However, this essay shows another origin for the form in concerns about the metropolis and mass culture that complicates our understanding. Drawing on examples from novels by John Dos Passos, Daniel Fuchs, Albert Halper, Josephine Herbst, William S. Rollins, Jr., and Josephine Herbst it shows how these texts offered not only radically ambivalent assessments of networked existence but often a pessimistic view of the possibilities of political community, extending at times to specific critiques of communist politics. In its conclusion, the essay draws links between these novels and the cinematic network narratives that became popular in the first decade of the 21st century. ",Missed Connections: The Collective Novel and the Metropolis,,2011,,Digital Humanities Quarterly,005,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2007,,"{'string_id': None, 'id_scheme': None}"," This article argues that Sherwood Anderson’s Winesburg, Ohio, while it cannot be considered a text straightforwardly concerned with technology, offers a modernist version of the story cycle that anticipates the delocalized and highly structured interconnections facilitated by the network. Unlike today’s seamlessly embedded networks, however, the prototypical form depicted in Winesburg, Ohio functions as a kluge, an ill-assorted collection of poorly-matching parts, forming a distressing whole . Anderson’s kluge augurs network technology and therefore suggests that the form loomed large in modernists’ mind. However, Winesburg, Ohio illustrates the network’s propensity to foster users’ inner alienation while enabling their unprecedented connection and thereby warns against the antagonistic quality of the network’s rhizomatic structure. ","Winesburg, Ohio: A Modernist Kluge",,2011,,Digital Humanities Quarterly,005,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2008,,"{'string_id': None, 'id_scheme': None}"," Wesley Beal examines John Dos Passos’s U.S.A. trilogy (1930-36) to read its complex form — what the author once referred to as a four-way conveyor system — as an intricate networking scheme, and considers it as an archive of early network thinking. Contrary to traditional readings of U.S.A. that discuss its formal properties in terms of dispersal and the supposedly ruinous disconnections of modern life, the paper argues that the interplay of these formal fragments results in a semiotic web that drives Dos Passos’s narrative strategy toward a totalizing vision of the nation and, indeed, history. Beal’s paper deals with the progressive widening of network figuration in American modernism to think the very politics of national space, and in focusing on the machinic dynamics of Dos Passos’s form, the article establishes network discourses as the organizing principle of several pre-digitization modes of production, including Fordism and the modern corporation. ",Network Narration in John Dos Passos’s U.S.A. Trilogy,,2011,,Digital Humanities Quarterly,005,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2009,,"{'string_id': None, 'id_scheme': None}"," The Globe is All One: Wars I Have Seen as Proto-Network Narrative charts Gertrude Stein's characterization of the human mind as an data-processor by tracing the conceptual correspondences between her writings and early information theory, including the work of Norbert Weiner, Claude Shannon, and Vannevar Bush. The article argues that Stein first sees language as data that human beings are compelled to parse in even the most contextless and semantically noisy frameworks, which shapes the purpose and form of Stein's notoriously difficult prose poem, Tender Buttons; the ease with which meaning can be exchanged emphasizes the importance of the ways in which a given meaning is selected from a set of possible meanings rather than interpreted or revealed. While this may simply reiterate the mode of Tender Buttons in different terms, the paper's crucial intervention is in its positioning of the chance selection of significance as the mode of signification that animates her wartime memoir Wars I Have Seen. Like words in the poem, political identity in occupied France is unstable and waits to be parsed in the act of encounters with others and with history. In her narrative's foregrounding of and experimental play with the conventions of memoir, Stein discovers the shifting nature of her main character (herself) in a series of ruminations and chance meetings; an early version of the network narrative emerges in the nodal structure of the story that undermines chronological and nationalist frames of signification. ", The Globe is All One: Wars I Have Seen as Proto-Network Narrative,,2011,,Digital Humanities Quarterly,005,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2010,,"{'string_id': None, 'id_scheme': None}"," Claims of the supposed disappearance of materiality in digital culture often entail a nostalgic reimagining of the supposedly embodied, personal or creative aspects of earlier writing technologies, including handwriting. Although handwriting was never a fully embodied writing technology, critics of transparent computer graphics often characterize it as such. This revisionist nostalgia for handwriting is evident not only in critical literature but also in contemporary graphical media such as video games. Two recent Nintendo DS games, Scribblenauts (2009) and The World Ends with You (2007), represent two alternative modalities of such nostalgia for handwriting. Scribblenauts claims to fully restore the creative properties of handwriting, but inevitably fails to do so. By contrast, TWEWY claims to offer not handwriting itself but a digital- and DS-specific equivalent. Therefore, it opens up possibilities for critical reflection on the past meaning of handwriting and on the future of the values that handwriting has come to symbolize. ",Forward to the Past: Nostalgia for Handwriting in Scribblenauts and The World Ends with You ,,2011,,Digital Humanities Quarterly,005,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2011,,"{'string_id': None, 'id_scheme': None}"," What is the status of digital literature in contemporary culture? Many scholars and practitioners assume that digital literature constitutes a contemporary avant-garde, which does its work of experimentation outside or in opposition to the mainstream. The notion of the avant-garde might seem thoroughly out of date in a consideration of the digital future. Important theorists (e.g. Huyssen, Drucker) have argued that the avant-garde is no longer viable even for traditional media and art practices. On the other hand, the avant-gardes of twentieth-century modernism made claims about the function of art that remain surprisingly influential today – within the art community and within popular culture. As Peter Bürger and others have discussed, an important division grew up in modernism on the question of whether art should strive for formal innovation or for sociopolitical change. Avant-gardes of the twentieth century took up positions along a spectrum from pure formalism (e.g. the Abstract Expressionists) to overt political action (e.g. the Situationists). While the digital literature community is in general committed to formal innovation, some are critical of this commitment, in part on the political grounds that (technological) innovation has become a byword for the digital culture industry. Although the modernist problem is still apparent in some digital art and digital literature today, writers such as Jason Nelson seem to be moving beyond that dialectic. ",Digital Literature and the Modernist Problem,,2011,,Digital Humanities Quarterly,005,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2012,,"{'string_id': None, 'id_scheme': None}"," The original usage of avatar referred to the incarnation or human appearance of a deity, particularly Vishnu, in Hindu mythology. The term was adapted to cyberspace to name one’s online persona. This usage has come to include every aspect of one’s online representation, from the icon on a blog, or an email signature to the figure one plays in Second Life. Avatar, then, is a practical point of entry for theorizing the emergence of the new identity experience of electracy, that is supplementing and displacing selfhood, the identity formation of literacy. Playing one’s avatar is to electracy what writing an essay is to literacy. The point addressed in this essay is that an avatar is not merely the appearance of one’s representation, since through interactivity and even telepresence, I am t/here with my image. What is it to be/have an image? The answer begins with noting the literal meaning of the avatar in Sanskrit: Descent. Vishnu has descended (taken on embodiment) nine times, to correct a disordered world condition. This essay initiates a review of the cultural archive to see what is known already about our question (representations of descent). It is perhaps obvious, considering the prominence of Christianity in our heritage, that the West accumulated a huge amount of information about becoming body. Two examples are referenced in this introductory piece: Krishna and Orpheus. ",Avatar Emergency,,2011,,Digital Humanities Quarterly,005,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2013,,"{'string_id': None, 'id_scheme': None}"," Existing scholarship on interactive fiction (IF, also known as the text adventure) tends to treat it as a video game genre and/or as a category of electronic literature. In this essay I argue that IF can be understood as participating in traditions of visual prose and ekphrastic textuality, insofar as IF consists of room and object descriptions which direct the player to visualize the things they describe. Unlike traditional ekphrastic literature, however, IF also asks the player to take practical actions in response to the images he or she visualizes. During the commercial era of IF, ekphrasis was the most effective means available of providing players with immersive visual experiences. However, graphical video games have now surpassed IF in this area. Therefore, in order to justify the continued existence of IF, contemporary IF authors have been forced to conceive of the visuality of IF otherwise than in terms of the logic of transparency. One strategy for doing this, exemplified by Nick Montfort's game, Ad Verbum, is to abandon visuality almost entirely and emphasize IF's linguistic and textual qualities. An alternative strategy, exemplified by Emily Short's game City of Secrets, is to assert that IF is visual in a non-transparent way, because IF offers visual experiences which are user-generated rather than pre-rendered. ",Because It's Not There: Ekphrasis and the Threat of Graphics in Interactive Fiction,,2011,,Digital Humanities Quarterly,005,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2014,,"{'string_id': None, 'id_scheme': None}"," Despite a general interest in exploring the possibilities of multimedia and web-based research, the humanities profession has been slow to accept digital scholarship as a valid form of intellectual endeavor. Questions about labor, peer-review, and co-authorship often arise in academic departments’ attempts to evaluate digital research in the tenure and promotion process. In this essay, we argue that these tensions stem from a general misunderstanding of the kinds of work that goes into producing scholarship in multimedia form. Multimedia work, we suggest, places scholars in an extended network that combines minds, bodies, machines, and institutional practices, and lays bare the fiction that scholars are disembodied intellectuals who labor only with the mind. We argue that while traditional ideas of what counts as scholarship continue to privilege content over form, intellectual labor over physical labor, and print over digital media, new media’s functional (and in some cases even biological) difference from old media contributes to a double erasure, for scholars working in multimedia, of both their intellectual contributions and their material labor. ",New Media in the Academy: Labor and the Production of Knowledge in Scholarly Multimedia,,2011,,Digital Humanities Quarterly,005,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2015,,"{'string_id': None, 'id_scheme': None}"," Locative technologies hold out the promise to transform literary space in all of its dimensions, including its represented spaces, reading interfaces, and the very spaces within which literature is produced and consumed. Yet, despite the growing use of location-based technologies, authors and readers alike have been slow to take to site-specific narrative due to limitations inherent in both the current design of locative media systems and our received notions of what constitutes the narrative experience. This paper argues that new mobile reading platforms in general are altering conceptions of literary space in highly conflicted ways, by radically expanding the sites where narratives can be accessed and experienced even as they reinforce a residual notion of literary reading as a sedentary and decontextualized experience. Locative media likewise hold out the promise of increased mobility and contextual awareness, but confront several cultural and technological factors preventing such an enhanced emplacement of narrative, factors that current performance-oriented approaches cannot fully address. At the level of cognitive engagement, the conditioned expectation of being transported to a remote fictional world interferes with readers' appreciation of the locative narrative's close ties to the real world, as well as the contextual effects it elicits by means of transportation through the actual world. At the technical level, the discontinuous algorithms of place that inform the architecture of most locative media systems hinder the perception of narrative patterning and flow across more extensive spaces. Locative media thus operationalize the spatial tension between conventionally sedentary modes of literary engagement and new modalities of mobility, a tension that is constitutive of our present mediality. The study concludes with a discussion of StoryTrek, a next-generation locative hypernarrative system designed to enable more complex, dynamic and fluid modes of embodied narrative spatiality. By encouraging the user to actively form complex narrative links between real and fictional spaces, StoryTrek enables utopian forms of spatial play that neutralize both the spatial limitations of current locative media design and the sedentary reading practices that continue to structure the experience of digital literature. ",The New Place of Reading: Locative Media and the Future of Narrative,,2011,,Digital Humanities Quarterly,005,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2016,http://projects.chass.utoronto.ca/chwp/reed/,"{'string_id': None, 'id_scheme': None}","In the following cluster of papers, each author discusses her experience using technology in the ""classroom"". All three papers are primarily concerned with pedagogy – how classes changed when computers became an integral part of the course. All raise issues related to how computers change classroom dynamics and, most importantly, how the use of computers changes the structures of authority which exist in the traditional classroom.",Pedagogy in the Electronic English Classroom: A Cluster,,2000,"['Technology, classroom, pedagogy, classroom dynamics, authority']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2017,,"{'string_id': None, 'id_scheme': None}"," Poetic writing for programmable and network media seems to have been captivated by the affordances of new media and questions of whether or not and if so, how certain novel, media-constituted properties and methods of literary objects require us to reassess and reconfigure the literary itself. What if we shift our attention decidedly to practices, processes, procedures — towards ways of writing and ways of reading rather than dwelling on either textual artifacts themselves (even time-based literary objects) or the concepts underpinning objects-as-artifact? What else can we do, given that we must now write on, for, and with the net which is itself no object but a seething mass of manifold processes? Part one of the essay presents a brief analysis of recent experiments in writing to be found with Google, making some claim that such writing may be exemplary, that its aesthetic and conceptual engagements are distinct, and that there is something at stake here for the literary or rather for certain practices of literary art. After very brief discussion in part two of some broader implications of writing with the Google corpus and its tools, part three addresses more examples of writing to be found, and introduces a collaboration with Daniel Howe, The Readers Project, many processes of which engage with writing to be found in Google and making use of its tools. ",Writing to be Found and Writing Readers,,2011,,Digital Humanities Quarterly,005,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2018,,"{'string_id': None, 'id_scheme': None}"," This paper focuses on the use of the trope x is like a node in a network in works of critical theory, and shows it to be an indicator of a larger discourse, nodalism, that has its origins in 19th century neuroscience and Associationist models of mind. This discourse provides a relationship between structure and function that is used in attempts to model thought in technological devices, as in the work of Vannevar Bush and JCR Licklider. These technological instantiations of nodalism in turn provide the material basis for the expansion of the discourse into other domains, such as politics and economics. The paper concludes by considering the potential positive and negative consequence of nodalism, and the role of the digitial humanities in the reproduction of this discourse. ",Nodalism,,2011,,Digital Humanities Quarterly,005,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2019,,"{'string_id': None, 'id_scheme': None}"," The research experiment described in this article, Readies Online, started as a database to make accessible a rare manuscript of important modernist poets and writers including Gertrude Stein, William Carlos Williams, Ezra Pound, F. W. Marinetti, Kay Boyle, Nancy Cunard, Eugene Jolas, and many others. Each of these contributors had sent works prepared for Bob Brown's machine, and he called the prepared texts readies. In the midst of building the collection of texts, the researcher realized that moving the texts through an electronic version of Brown's machine, or through the interface constructed on the website to simulate Brown's machine, changed how one read — even changed the essence of what one read. Speed, pace, direction, and visual cues took on new importance already apparent in reading printed texts, but not stressed. Punctuation now represented an illegible and non-representational, visual cue rather than a direct link to the phono-centric pauses and stops that are more commonly represented by punctuation. The futures of reading, and the use of new devices like e-readers, will have consequences for the definition and practice of what we call reading. ",Readies Online,,2011,,Digital Humanities Quarterly,005,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2020,,"{'string_id': None, 'id_scheme': None}"," Research since the early years of the 21st century consistently shows that through the years more of our time gets spent using media, that being concurrently exposed to media has become a foundational feature of everyday life, and that consuming media for most people increasingly takes place alongside producing media. Contemporary media devices, what people do with them, and how all of this fits into the organization of our everyday life disrupt and unsettle well-established views of the role media play in society. Instead of continuing to wrestle with a distinction between media and society, this contribution proposes we begin our thinking with a view of life not lived with media, but in media. The media life perspective starts from the realization that the whole of the world and our lived experience in it are framed by, mitigated through, and made immediate by (immersive, integrated, ubiquitous and pervasive) media. ",A Life Lived in Media,,2012,,Digital Humanities Quarterly,006,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2021,,"{'string_id': None, 'id_scheme': None}"," The two annotated bibliographies present in this publication document and feature pertinent discussions toward the activity of modeling the social edition, first exploring reading devices, tools and social media issues and, second, social networking tools for professional readers in the Humanities. In this work, which is published conjointly with the LLC piece Toward Modeling the Social Edition: An Approach to Understanding the Electronic Scholarly Edition in the Context of New and Emerging Social Media, we consider a typology of electronic scholarly editions adjacent to activities common to humanities scholars who engage texts as expert readers, noting therein that many methods of engagement both reflect the interrelated nature of long-standing professional reading strategies and are social in nature; extending this framework, the next steps in the scholarly edition’s development in its incorporation of social media functionality reflect the importance of traditional humanistic activities and workflows, and include collaboration, incorporating contributions by its readers and re-visioning the role of the editor away from that of ultimate authority and more toward that of facilitator of reader involvement. ",Pertinent Discussions Toward Modeling the Social Edition: Annotated Bibliographies,,2012,"['scholarly editing', 'editorial theory', 'social networking', 'social media', 'networks', 'community']",Digital Humanities Quarterly,006,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2022,,"{'string_id': None, 'id_scheme': None}"," Over the last couple of years, it has become increasingly clear that the digital humanities is associated with a visionary and forward-looking sentiment, and that the field has come to constitute a site for far-reaching discussions about the future of the field itself as well as the humanities at large. Based on a rich set of materials closely associated with the formation of the digital humanities, this article explores the visions and expectations associated with the digital humanities and how the digital humanities often becomes a laboratory and means for thinking about the state and future of the humanities. It is argued that this forward-looking sentiment comes both from inside and outside the field, and is arguably an important reason for the attraction and importance of the field. Furthermore, the author outlines a visionary scope for the digital humanities and offers a personal visionary statement as the endpoint to the article series. ",Envisioning the Digital Humanities,,2012,,Digital Humanities Quarterly,006,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2023,,"{'string_id': None, 'id_scheme': None}"," This article identifies some of the popular and historical contradictions inherent to the very notion of close reading digital literature, and puts forth an updated conception of what the author argues continues to be a vital practice of literary study. More specifically, it establishes continuities between a pre-digital historical conception of close reading and the sort of materially-conscious hermeneutics that digital textuality requires. The author applies the updated conception of close reading digital literature to Steve Tomasula's TOC , a self-described new media novel. ","The Materialities of Close Reading: 1942, 1959, 2009",,2012,,Digital Humanities Quarterly,006,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2024,,"{'string_id': None, 'id_scheme': None}"," Despite DH’s long history, it is still perceived as a relatively emergent academic discipline which has several implications for its ongoing development and acceptance. In order to understand its role in supporting the field’s development and acceptance, SSHRC commissioned a survey of the larger Humanities and Social Science’s community to understand the issues related to DH’s development and acceptance and the types of activities that should be funded. The survey results suggest there is reason for optimism regarding the growing acceptance of digital methods, resources and tools and electronic dissemination as instructors, researchers, and students are using and publishing in digital outlets and creating and employing digital recourses, methods and tools andventuring into new research fields. This trend is likely to continue as students and younger scholars continue to embrace the digital in all aspects of their personal and professional lives. However, this optimism should be tempered to some extent as students and junior faculty are still less likely than associate professors to present and publish their digital-oriented research for a variety of reasons. The field’s more senior faculty can mentor their junior colleagues and students to this end and shape salary, tenure and promotion policies to recognize and reward these efforts. Finally, issues remain around the amount of funding required for the initial development and ongoing sustainability and relevance of digital resources and may become more critical over time. Granting agencies will need to evaluate their funding role in this regard. ",Developing Academic Capacity in Digital Humanities: Thoughts from the Canadian Community,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2025,,"{'string_id': None, 'id_scheme': None}"," Where will all the books go? A version of that question has occupied commentary about Amazon’s Kindle since news of its release first made its way to public attention in September 2007. Posed out of curiosity, excitement, as well as trepidation, the question includes anxiety about the re-invention of reading embodied by new features and the intricacies of downloading policy. In this essay, I focus on disappearance for the ways in which it indexes concerns about the Kindle’s material impact on reading. Tracking these concerns, I turn to three major moments in the Kindle’s biography in order to emphasize Amazon’s investment in mimicking the transcendence associated with reading and the ways in which this investment is met, and at times exceeded, by readerly desires to possess that transcendence. That the very name Kindle should occasion parallels between e-reading and book burning, that Jeff Bezos should claim unobtrusiveness as the device’s exemplary feature, and that Amazon’s unannounced deletions of Orwell’s 1984 from thousands of Kindle libraries should meet with such ire, makes visible the two ideologies of reading — on the one hand, the invaluable (because ephemeral) force of reading and on the other, reading as a relation of ownership and agency — that stand in both an uneasy and profitable tension for Amazon and its reader-consumers. ","Kindling, Disappearing, Reading",,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2026,,"{'string_id': None, 'id_scheme': None}"," This article describes the authors’ experience teaching college students how to use Google Earth to create atlases of historical Silk Road journeys. It argues that a digital humanities classroom, with clearly defined assignments and extensive evaluation, is an exemplary setting for establishing and verifying genre conventions and review standards. Approaches that are developed for the classroom can be modified for professional settings. The authors introduce criteria for evaluating digital historical atlases as works of humanistic scholarship, and suggest that digital humanists do the same for other disciplines and genres as well. ","Traveling the Silk Road on a Virtual Globe: Pedagogy, Technology and Evaluation for Spatial History",,2013,,Digital Humanities Quarterly,007,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2027,http://projects.chass.utoronto.ca/chwp/Casta02/Petter_abstract.htm,"{'string_id': None, 'id_scheme': None}",Our paper will cover both academic and technical development of the Graves Diary Project (1935-39)url. This project and this paper have been made possible by close collaboration between the editors and the programming staff at the University of Victoria Humanities Computing Centre.,Pedagogy in the Electronic English Classroom: A Cluster,,2005,"['Electronic text, TEI corpus, XML database, eXist, XPath, JavaScript, Laura Riding, expatriate English writers, Spanish Civil War']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2028,,"{'string_id': None, 'id_scheme': None}"," Comics, comic books, and graphic novels are increasingly the target of seriously scholarly attention in the humanities. Moreover, comic books are exceptionally complex documents, with intricate relationships between pictorial and textual elements and a wide variety of content types within a single comic book publication. The complexity of these documents, their combination of textual and pictorial elements, and the collaborative nature of their production shares much in common with other complex documents studied by humanists — illuminated manuscripts, artists’ books, illustrated poems like those of William Blake, letterpress productions like those of the Kelmscott Press, illustrated children’s books, and even Web pages and other born-digital media. Comic Book Markup Language, or CBML, is a TEI-based XML vocabulary for encoding and analyzing comic books, comics, graphic novels, and related documents. This article discusses the goals and motivations for developing CBML, reviews the various content types found in comic book publications, provides an overview and examples of the key features of the CBML XML vocabulary, explores some of the problems and challenges in the encoding and digital representation of comic books, and outlines plans for future work. The structural, textual, visual, and bibliographic complexity of comic books make them an excellent subject for the general study of complex documents, especially documents combining pictorial and textual elements. ",Comic Book Markup Language: An Introduction and Rationale,,2012,,Digital Humanities Quarterly,006,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2029,,"{'string_id': None, 'id_scheme': None}"," The contemporary digital media ecology is one of convergence and hybridity. As virtual and technical interfaces intersect in increasingly complex formulations, the ability to identify organic vs. technical forms has become problematic. Virtual environments predominate within everyday cultural practice arguably limiting real or unmediated human experience. The advent of social media artifacts and networks in particular — those that create fusions of personal experience and communal activity and that support and broadcast user-generated content as a foundation for media productions of real-life — have made organic bodies and personal experience difficult to discern. Extending Mark B.N. Hansen’s model for identifying embodied experience within contemporary mixed reality culture, I argue that embodied expression is more, not less, present in the contemporary media age. Organic expressions, those that emanate from primal, tactile, and motile forces and that operate prior to formal mediatization, are at the core of many social media artifacts circulated within the networks of contemporary culture and operating outside the aesthetics of traditional semiotic representation. Recovering the organic body and foregrounding its presence in such media asserts the functional non-aesthetic principles at work in many social media forms, particularly in those dependant on documenting the minutiae of real-life under-represented in mainstream and traditional media. As personal and public spaces collide, situating the me or the embodied subject within production is problematic. I identify such embodiment within contemporary social media, particularly on YouTube, to illustrate that the human body does not operate from a position of erasure within social media networks and artifacts, and its expressive value is therefore central in much current user-generated me-dia. ",Beyond Representation: Embodied Expression and Social Me-dia,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2030,,"{'string_id': None, 'id_scheme': None}"," At the core of the work done within the digital humanities is a difficult interdisciplinary relationship between the at times divergent cognate fields of computer science and the humanities. This paper will explore some of the characteristics of the digital humanities and examine some of its hard interdisciplinarity relationships. It is the contention of the author that one of the central epistemological challenges within the field is to empower students to successfully manage the thorny interdisciplinary relationship intrinsic to technology and the humanities. Without understanding and managing this relationship, there is a danger that student projects lapse into exceedingly reductive pragmatism or overly theorised clumsiness. The author will suggest a model where this hard-interdisciplinary relationship may be taught and assessed through the critical use and analysis of digital objects within the framework of a Virtual Research Environments (VREs). ",The Sound of Many Hands Clapping: Teaching the Digital Humanities through Virtual Research Environment (VREs),,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2031,,"{'string_id': None, 'id_scheme': None}"," While much valuable scholarship on the digital focuses on particular artifacts or historical processes or subcultures, this essay offers a preliminary treatment of the digital in general, proposing that the digital has its own ontology, a way of being, and that this ontology is manifest in the technologies and human relations that define and surround the digital. In particular, the digital places a central emphasis on abstraction, and digital artifacts and culture demonstrate this ontology of abstraction even while remaining concrete. The kinds of social structures grouped under the label Web 2.0 exemplify the materialized abstraction of the digital, and this essay points out the formal and technical features of the digital that carry the abstract nature of the binary code into the human relations and behaviors of Web 2.0. ",Web 2.0 and the Ontology of the Digital,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2032,,"{'string_id': None, 'id_scheme': None}"," Digital technology often gives us the chance to re-conceive common scholarly practices with the humanities, and one of these is the practice of annotation. Whereas many in the digital humanities look at annotation through the lens of social media, in this paper we consider annotation’s already established function in scholarship: to support the development of an interpretation of a body of material. It begins by applying a “software application” perspective to annotation and it notes that personal annotation sits at the nexus between the publishing application of the material being annotated, and an interpretation development application that aims to support the reader’s thinking. Once this application orientation is taken up, it becomes evident that it is useful to re-conceptualise aspects of annotation beyond the annotation-of-media focus which the World Wide Web has encouraged in all of us. The paper does this by considering annotation in an application that is not media oriented in nature, Northwestern University’s WordHoard, and it explores some of the significance of annotation where the application’s data model – with its inherent semantic significance – is available to be annotated. There is a growing interest in thinking of the WWW as a delivery mechanism for software applications rather than merely for documents, and thus many of the issues that this paper raises could apply to the work of web-oriented developers too. ","Towards a Richer Sense of Digital Annotation: Moving Beyond a ""Media"" Orientation of the Annotation of Digital Objects",,2012,"['annotation', 'software tools']",Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2033,,"{'string_id': None, 'id_scheme': None}"," From Hans Holbein’s The Ambassadors to Robert Lazzarini’s skulls, anamorphic artworks explore the tension between mathematical models of vision and an embodied experience of space. After reviewing the ways in which anamorphosis has been deployed as a philosophical tool for investigating digital media in terms of human phenomenology, specifically through the criticism of Espen Aarseth and Mark Hansen, this paper analyzes how contemporary videogames like Sony’s Echochrome series, levelHead by Julian Oliver, and Mark ten Bosch’s forthcoming Miegakure technically, aesthetically, and conceptually explore anamorphic techniques. While The Ambassadors is famous for its anamorphically skewed skull, a classic memento mori, we propose that the anamorphic effects of videogames can be more accurately described as a memento mortem mortis: not reminders of human mortality, but of a nonhuman the death of death. By foregrounding the impossibility of ever fully resolving the human experience of computational space, the memento mortem mortis in these anamorphic games gestures toward experiential domains altogether indifferent to human phenomenology to create allegories of the beyond. ",Stretched Skulls: Anamorphic Games and the memento mortem mortis ,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2034,,"{'string_id': None, 'id_scheme': None}"," A Day in the Life of the Digital Humanities (Day of DH) is a community documentation project that brings together digital humanists from around the world to document what they do on one day, typically March 18. The goal of the project, which has been run three times since 2009, is to bring together participants to reflect on the question, Just what do computing humanists really do? To do this, participants document their day through photographs and commentary using one of the Day of DH blogs set up for them. The collection of these journals (with links, tags, and comments) is, after editing, made available online. This paper discusses the design of this social project, from the ethical issues raised to the final web of journals and shares some of the lessons we have learned. One of the major challenges of social media is getting participation. We made participating easy by personally inviting a seed group, choosing an accessible technology, maintaining a light but constant level of communication prior to the event, and asking only for a single day of commitment. In addition, we tried to make participation at least rewarding in formal academic terms by structuring the Day of DH as a collaborative publication. In terms of improvements, we have over the iterations changed the handling ethics clearances for images and connected to other social media like Twitter. ",The Design of an International Social Media Event: A Day in the Life of the Digital Humanities,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2035,,"{'string_id': None, 'id_scheme': None}"," The past decade has seen a remarkable proliferation of new works of constrained and appropriated writing that prominently incorporate, and in turn investigate, metadata schemes. I argue that these works ought to be of considerable interest not only to critics of contemporary avant-garde writing — but also to media theorists, librarians and textual scholars. By emphasizing classification protocols, conceptual writing makes an implicit case for the interrelationship of these fields. Each of the four main books under discussion here — Tan Lin’s Seven Controlled Vocabularies, Craig Dworkin’s Perverse Library, M. Nourbese Philip’s Zong! and Simon Morris’ Getting Inside Jack Kerouac’s Head — draws upon pre-existing textual archives. In doing so, these books suggest that processes of data storage, classification and transmission are key to how poetry is created, recognized and disseminated. Conceptual writing’s attention to information classification protocols offers not only a critique of contemporary models of authorship, but also of contemporary frameworks of personal agency and intellectual property. ",From the Personal to the Proprietary: Conceptual Writing's Critique of Metadata,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2036,,"{'string_id': None, 'id_scheme': None}"," This paper contributes to the literature examining the burgeoning field of academic crowdsourcing, by analysing the results of the crowdsourced manuscript transcription project, Transcribe Bentham. First, it describes how the project team sought to recruit volunteer transcribers to take part, and discusses which strategies were successes (and which were not). We then examine Transcribe Bentham's results during its six-month testing period (8 September 2010 to 8 March 2011), which include a detailed quantitative and qualitative analysis of website statistics, work completed by the amateur transcribers, as well as the demographics of the volunteer base and their motivations for taking part. The paper concludes by discussing the success of our community building with reference to this analysis. We find that Transcribe Bentham's volunteer transcribers have produced a remarkable amount of work – and continue to do so, carrying out the equivalent labour of a full-time transcriber – despite the nature and complexity of the task at hand. ",Building A Volunteer Community: Results and Findings from Transcribe Bentham ,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2037,,"{'string_id': None, 'id_scheme': None}"," Malware and criminal operations performed by botnets on the Internet not only pose a new threat, but also point to our increasing reliance upon a new form of machinic agency, which I call the webbot assemblage. Whereas news media coverage of its operations considers only their human aspects, mostly in relation to crime and cyberterrorism, Daniel Suarez's recent novel Daemon provides a suggestive glimpse into how, in a webbot assemblage, new forms of human and machinic agency are complexly intricated. The significance of this assemblage becomes further evident when it is considered in relation to how the Internet is increasingly perceived: no longer as a neutral medium but as an ecosystem defined by netwar, software arms races, and the possible evolution of low forms of artificial life. ",Webbots and Machinic Agency,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2038,http://projects.chass.utoronto.ca/chwp/patterson/,"{'string_id': None, 'id_scheme': None}","Since the correspondences of any one writer over a lifetime document hundreds of associative links comprising perhaps thousands of interpersonal transactions, and because each letter in itself may record many other interpersonal transactions performed or desired, the mapping of such associative links requires a reliable method of storing and retrieving data generated from the letter texts for first constructing individual social network profiles and then later for comparing them. Database programs have the capacity to store, order, reconfigure, and retrieve vast amounts of data. Hypertext theory and technology suggest new approaches to reading as well as representing epistolary texts. Together these technologies can assist in representing the social associations and affiliations of epistolary discourse. The hypertextual ""Anna Jameson and her Friends Database"", designed to reflect my approach to epistolary communication as well as to generate data for the mapping of Jameson's social network profile, is both what I hope will be a useful resource in itself and the first stage of my inquiry into the networking of early to mid-nineteenth-century women writers.","The ""Anna Jameson and her Friends Database"": Mapping Anna Jameson's Associative Links with the Victorian Intellectual Community",,2000,"['Written correspondence, associative links, social networks, hypertext, epistolary discourse, nineteenth century, women writers']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2039,,"{'string_id': None, 'id_scheme': None}"," The concept of a great scientific domain broadens what is normally considered to be within the purview of science while identifying four such domains – the physical, life, social and computing sciences – and suggesting that the humanities naturally fit within the sciences as part of an expanded social domain. The relational architecture that has been developed to aid in understanding disciplinary combinations across great scientific domains then guides an exploration of the structure and content of the digital humanities in terms of a space of relationships between computing and the humanities. ",Towards a Conceptual Framework for the Digital Humanities,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2040,,"{'string_id': None, 'id_scheme': None}"," This article traces the history of Storyspace, the world’s first program for creating, editing and reading hypertext fiction. Storyspace is crucial to the history of hypertext as well as the history of interactive fiction. It argues that Storyspace was built around a topographic metaphor and that it attempts to model human associative memory. The article is based on interviews with key hypertext pioneers as well as documents created at the time. ",Machine Enhanced (Re)minding: the Development of Storyspace,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2041,,"{'string_id': None, 'id_scheme': None}"," Almost since the inception of the industry, the player community has been instrumental in preserving video games and other variable media art. Drawing on a combination of primary and secondary sources of information, including the Preserving Virtual Worlds project (an academic investigation into viable models of preservation for videogames and 3D virtual worlds based on a series of archiving case studies) and the results of a game documentation survey conducted by Donahue, we examine how players are taking responsibility for collecting, managing, curating, and creating long-term access to computer games. Because our interest lies with the contact zone between players and information professionals, we also describe and analyze how we and other scholar-archivists are collaborating with or relying on the user community to preserve virtual worlds, with an eye to how these relationships might eventually be codified within a larger preservation framework.Early versions of portions of this paper were delivered by Kari Kraus, Rachel Donahue, and Megan Winget, Game Change: The Role of Amateur and Professional Cultures in Preserving Virtual Worlds Conference paper. Digital Humanities Conference, College Park, MD (June 22-29 2009); and Rachel Donahue, Do You Want to Save Your Progress? Preservation Strategies of the Game Industry, and What Their Users Could Teach Them, Society of American Archivists Annual Meeting, August 2009. ", Do You Want to Save Your Progress?: The Role of Professional and Player Communities in Preserving Virtual Worlds,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2042,,"{'string_id': None, 'id_scheme': None}"," This extended interview with Ray Siemens was carried out on June 21st at Digital Humanities 2011, Stanford University. It explores Siemens' early training and involvement in the field that is now known as digital humanities. He recalls that his first experience with computing was as a video gamer and programmer in high school. He had the opportunity to consolidate this early experience in the mid-1980s, when he attended the University of Waterloo as an undergraduate in the department of English where he undertook, inter alia, formal training in computing. He communicates stronglythe vibrancy of the field that was already apparent during his graduate years (up to c. 1991) and identifies some of the people in places such as the University of Alberta, University of Toronto, Oxford, and the University of British Columbia who had a formative influence on him. He gives a clear sense of some of the factors that attracted him to computing, for example, the alternatives to close reading that he was able to bring to bear on his literary research from an early stage. So too he reflects on computing developments whose applications were not immediately foreseeable, for example, when in 1986 he edited IBM's TCP/IP manual he could not have foreseen that by 1989 TCP/IP would be firmly established as the communication protocol of the internet. He closes by reflecting on the prescience of the advice that his father, also an academic, gave him regarding the use of computing in his research and on his early encounters with the conference scene. "," Video-gaming, Paradise Lost and TCP/IP: an Oral History Conversation between Ray Siemens and Anne Welsh ",,2012,,Digital Humanities Quarterly,006,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2043,,"{'string_id': None, 'id_scheme': None}"," John Unsworth recounts that he first became involved with computing in the Humanities c. 1989 as a new faculty member at North Carolina State University where he was hired to teach post-World War II American literature. He and his colleagues wanted to set up a new journal as only one other journal existed in that area. They were introduced to the recently released LISTSERV software and the first issue of the journal was circulated on email lists and bulletin boards. It was called Postmodern Culture and twenty-two years later is still published by Johns Hopkins University Press. It was the first peer-reviewed electronic journal in the Humanities; nevertheless, not all senior colleagues were in favour of it and, as a junior faculty member, his participation in it. He recounts that was not able to avail of formal training in computing but he did have technical knowledge of computing, mostly picked up while procrastinating on this PhD. By the early 1990s he was reading Humanist and attending conferences that focused on electronic journals where he encountered a range of academic and non-academic projects. In 1993 he moved to the University of Virginia where he directed the Institute for Advanced Technology in the Humanities (IATH). He reflects on the wide range of people and projects that he worked with and that it was around this time that he became involved with the community now known as digital humanities. He reflects in detail on the first digital humanities conference he attended in Paris in 1994 and concludes by discussing some of the changes that the advent of the Web has heralded. ", Postmodern Culture and More: an Oral History Conversation between John Unsworth and Anne Welsh ,,2012,,Digital Humanities Quarterly,006,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2044,,"{'string_id': None, 'id_scheme': None}"," Harold Short recounts that his interest in Computing and the Humanities goes back to when he was an undergraduate in English and French at a university in the former Rhodesia (now Zimbabwe). There, whilst undertaking summer work in the library, he saw first-hand the potential of digital methods. After arriving in London in 1972 he took an Open University degree in mathematics, computing and systems. Among his early influences he identifies the reading he did on matters related to cognitive science whilst undertaking a postgraduate certificated in education. In the UK he worked at the BBC as programmer, systems analyst and then systems manager. In 1988 he moved to King's College London to take up the post of Assistant Director in Computing Services for Humanities and Information Management. One of his first tasks was to work with the Humanities Faculty to develop an undergraduate programme in humanities and computing. The first digital humanities conference he attended was the first joint international conference of ALLC and ACH, held at the University Toronto in 1989, which c. 450 people attended. He reflects on aspects of the institutional shape of the field towards the end of the 1980s, including the key Centres that existed then, the first meeting of the Association for Literary and Linguistic Computing (ALLC) and those who were active in it such as Roy Wisbey, Susan Hockey and the late Antonio Zampolli. He gives a detailed discussion of the development of what is now the Department of Digital Humanities in King's College London, both in terms of the administrative and institutional issues involved, as well as the intellectual. He also reflects on some of the most successful collaborations that the Department has been involved in, for example, the AHRC funded Henry III Fine Rolls project, and the conditions and working practices that characterised them. He closes by discussing his impressions about the movement of scholars into and out of the discipline and of the institutional issues that have had an impact on digital humanities centres. ", Collaboration Must Be Fundamental or It's Not Going to Work: an Oral History Conversation between Harold Short and Julianne Nyhan ,,2012,,Digital Humanities Quarterly,006,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2045,,"{'string_id': None, 'id_scheme': None}"," This interview was carried out with Willard McCarty on Tuesday 27th March, 2012 in University College London. He recounts that his earliest encounter with computing was in the Lawrence Radiation Laboratory in Berkley where he worked with semi-automated scanning equipment for the Alvarez high-energy physics projects. After his dreams of becoming a physicist were thwarted he transferred to Reed College. There he did not have the opportunity to take formal training in computing; for the most part, Computer Science departments did not exist then. So, he learned to programme on the job with help from a talented physicist turned computer programmer named Bill Gates (no association with Microsoft). His first encounter with what we now call digital humanities was at the University of Toronto where he worked on the Records of Early English Drama project whilst undertaking a PhD on 17th century non-dramatic poetry. In 1984/5, as he was finishing his PhD, he accepted an academic support role at the Centre for Computing in the Humanities at Toronto, where he remained until 1996 when he accepted an academic post in King's College London. In Toronto he was keenly aware of the staff-faculty divide and the marginalised position of those who used computers in Humanities research. Nevertheless, the opportunities that the role brought to meet with a range of scholars interested in computing had a lasting influence on him. So too, with funding from the Social Sciences and Humanities Research Council of Canada he was able to undertake a research project on Ovid's Metamorphosis. He closes the interview by reflecting on his early involvement with the conference scene and people who have influenced him, from academics to his calligraphy teacher Lloyd Reynolds. "," Questioning, Asking and Enduring Curiosity: an Oral History Conversation between Julianne Nyhan and Willard McCarty ",,2012,,Digital Humanities Quarterly,006,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2046,,"{'string_id': None, 'id_scheme': None}"," This extended interview with Geoffrey Rockwell was carried out via Skype on the 28th April 2012. He narrates that he had been aware of computing developments when growing up in Italy but it was in college in the late 1970s that he took formal training in computing. He bought his first computer, an Apple II clone, after graduation when he was working as a teacher in the Middle East. Throughout the interview he reflects on the various computers he has used and how the mouse that he used with an early Macintosh instinctively appealed to him. By the mid-1980s he was attending graduate school in the University of Toronto and was accepted on to the Apple Research Partnership Programme, which enabled him to be embedded in the central University of Toronto Computing Services; he went on to hold a full time position there. Also taking a PhD in Philosophy, he spent many lunch times talking with John Bradley. This resulted in the building of text analysis tools and their application to Hume's Dialogues Concerning Natural Religion, as well as some of the earliest, if not the earliest, conference paper on visualisation in the digital humanities community. He reflects on the wide range of influences that shaped and inspired his early work in the field, for example, the Research Computing Group at the University of Toronto and their work in visual programming environments. In 1994 he applied, and was hired at McMaster University to what he believes was the first job openly advertised as a humanities computing position in Canada. After exploring the opposition to computing that he encountered he reflects that the image of the underdog has perhaps become a foundational myth of digital humanities and questions whether it is still a useful one. ", Trading Stories: an Oral History Conversation between Geoffrey Rockwell and Julianne Nyhan ,,2012,,Digital Humanities Quarterly,006,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2047,,"{'string_id': None, 'id_scheme': None}"," Despite significant investments in the development of digital humanities tools, the use of these tools has remained a fringe element in humanities scholarship. Through an open-ended survey and virtual panel discussion, our study outlines the experience of historians using various digital tools. The results of the study reveal the variety of users interested in digital tools as well as their enthusiasm, reactions, and frustrations, including the expectations and confusion that has created barriers to tool use and to the wider adoption of new research methodologies. We suggest that an emphasis on cultivating a broader audience must be a concern not only for tool builders but also for funders to account adequately for the time and expense of quality interfaces and documentation. ",Building Better Digital Humanities Tools: Toward broader audiences and user-centered designs,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2048,,"{'string_id': None, 'id_scheme': None}"," This paper will briefly survey the historical development of linking systems in the Christian Bible, from their theological foundations to their formation in the architecture of the printed book. It will then examine the apogee of intra-Biblical linking systems in the Thompson Chain Reference Bible, particularly its chain-referencing system for thematic linking between texts. Finally, it will use this mature print technology to consider the state of the hyperlink in current Web-interfaces. It will show that while in many ways modern attempts at a dynamic hyperlink surpass this elaborate linking system in functionality, in a few key functions this old print technology out-performs what is commonly and readily available in current Web-browsers. In pursing this comparative analysis we aim to demonstrate the importance of understanding the organization and navigational structure of the codex in designing digital reading environments that will meet and surpass the affordances of print. ",Old Ways for Linking Texts in the Digital Reading Environment: The Case of the Thompson Chain Reference Bible,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2049,http://projects.chass.utoronto.ca/chwp/Casta02/Mueller_casta02.htm,"{'string_id': None, 'id_scheme': None}","This article describes the origin, principles, practices, and potential of The Nameless Shakespeare electronic edition.",The Nameless Shakespeare,,2005,"['Electronic edition, text-analysis, tagging, data retrieval, interface']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2050,,"{'string_id': None, 'id_scheme': None}"," As museums increasingly place archival materials on display, a body of scholarship has emerged to provide practical advice for staff about exhibiting handwritten documents. However, there has as yet been little scholarship that problematizes the exhibition of manuscripts and the responses they elicit from their audiences. This essay, then, investigates the cultural perception of handwriting as an inherently unique and authentic embodiment of its writer, the assumption of which lies behind its display. Through a series of close readings of responses to the sight of the autograph, I examine the ways in which handwriting’s association with the human body has been historically shaped and interpreted; its current function as a locus for concerns about the loss or degradation of corporeal identity in an increasingly technologized world; and how multimedia museum exhibitions of handwritten documents — as digitally manipulable surrogates of original artifacts — expose, complicate, and break down the oppositions in this cultural discourse. Ultimately, I argue, digital interactives are part of a new exhibitionary paradigm, which not only offers new ways of considering an artifact’s essential meaning, but also refines and redefines our understanding of human effort, intentionality, and embodiment in a digital age. ",In One's Own Hand: Seeing Manuscripts in a Digital Age,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2051,,"{'string_id': None, 'id_scheme': None}"," This critical essay was written for the Prairie Art Gallery catalogue presenting Kate Armstrong's and Michael Tippett's Grafik Dynamo! Its argument, implied in the catalogue version, can be stated explicitly in the present scholarly format, namely that narrative, associated with the development of the modern novel in print, is distinctly unsuited to literary arts produced in and for the electronic medium. Narrative in the Dynamo! is not entirely absent, but its dominance is put into question. The same holds for the place of argumentation in critical writing. The Dynamo! develops episodically, haunted by the comics, and by the popular and literary narratives it samples; the essay develops similarly, in blocks of partly autobiographical, partly analytical text. Propositions emerge not sequentially or through feats of interpretation, but at the moment when a block of text encounters a cited image from the Dynamo! Another collocation having implications for criticism, is the reading of Armstrong/Tippett's work in the context of a particular strain of contemporary fiction in print, which itself demonstrates that narrative was only ever a mode, one among many and not necessarily the dominant mode, in print literature itself. References are made to Pynchon's Gravity's Rainbow, to Pynchon's (and Armstrong/Tippett's) modernist antecedent, Henry Adams, to non-linear, non-sequential narratives by Robert Coover, Don DeLillo, and William Gaddis that are as open to innovation formally as they are expansive in subject matter. In this context, Graphic Sublime also introduces a major, as yet unpublished novel from the early 1970s by Phillip Wohlstetter, Valparaiso. ",Graphic Sublime: On the Art and Designwriting of Kate Armstrong and Michael Tippett,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2052,,"{'string_id': None, 'id_scheme': None}"," Digital humanities and medieval studies share a long history, beginning with one of the first large-scale digital humanities projects, which was carried by Father Roberto Busa using IBM’s Literary Data Processing Center. Why then, do many scholars of historically-minded fields consider digital humanities to be a “helping discipline” instead of a full-fledged area of study in itself? Beginning with the above question, this paper explores the ways in which scholars need not use the digital humanities to update historical disciplines or vice versa. By examining the pre- and post-print histories of the book, and interrogating the ways in which reading technologies and interfaces link the past and future of the book together, the past and present histories of reading coalesce and offer scholars novel ways of approaching many different disciplines that engage with the digital humanities. ",The Boundless Book: A Conversation between the Pre-modern and Posthuman,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2053,,"{'string_id': None, 'id_scheme': None}"," This essay takes as axiomatic that the subject of new media – which in other contexts we call the user, the reader, the writer (or in institutional contexts, the researcher, the teacher, the student…) – is a subject of language. This subject’s engagements with media and, by way of media, with other subjects, are determined by relations founded on language which French psychoanalyst Jacques Lacan terms the social bond of discourse. I propose that modes of critical engagement and teaching in the contemporary digital field, particularly as the field shifts towards a more unified disciplinarity and a more secure institutional footing, can be described in relation to the graphs of Lacan’s four discourses – of the University, Master, Hysteric, and Analyst. I conclude that deliberate reflection on structures of our research and pedagogy, mapped by the graphs, may lead us beyond the confidence games of the master and the University – on and by which our inquiries are founded and oriented, but also narrowed – to the side of the hysteric and the analyst, whose collaborations are more productive of new forms of knowledge. ",The Underside of the Digital Field,,2012,,Digital Humanities Quarterly,006,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2054,,"{'string_id': None, 'id_scheme': None}"," The Humanities now confront a new era in cultural representations — the digital age. As a consequence, our approach to culture may be modified because technology allows us to now visualize our thoughts and theories using digital and computing techniques. This research focuses on merging humanities research with computational sciences to explore the processes involved in culture dynamics. We present an interdisciplinary approach that combines literary studies, economics, and agent-based modeling (ABM) and give details of how literature maybe used as a data set that can be translated into a dynamic Java-based simulation of human interactions constructed around Game Theory. Our model of Cross-Cultural Cooperation is designed to study culture at various levels of granularity simultaneously in order to show how micro-behaviors might lead to macro outcomes such as cultural group formation. We present one experiment based on the literature of discovery and conquest in the U.S. Southwest translated into the language of ABM. Additionally, we explore the role of space, time, and population-size in this process and offer a discussion of possible future directions for this type of research. The creation of our simulator of cultural exchange between individuals of differing cultures allows researchers to experiment with ideas about first and ongoing contact and speculate with “What if?” scenarios. ",An Agent-based Model for the Humanities,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2055,,"{'string_id': None, 'id_scheme': None}"," This article outlines a critical framework for a theory of performative materiality and its potential application to interface design from a humanistic perspective. Discussions of the materiality of digital media have become richer and more complex in the last decade, calling the literal, physical, and networked qualities of digital artifacts and systems to attention. This article extends those discussions by reconnecting them to a longer history of investigations of materiality and the specificity of media in critical theory and aesthetics. In addition, it introduces the concept of performative materiality, the enacted and event-based character of digital activity supported by those literal, physical conditions, and introduces the theoretical concerns that attach to that rubric. Performative materiality is based on the conviction that a system should be understood by what it does, not only how it is structured. As digital humanities matures, it can benefit from a re-engagement with the mainstream principles of critical theory on which a model of performative materiality is based. The article takes these ideas into a more focused look at how we might move towards integrating this model and critical principles into a model of humanistic interface design. ",Performative Materiality and Theoretical Approaches to Interface,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2056,,"{'string_id': None, 'id_scheme': None}"," In this article, Cordell demonstrates the transformative possibilities of large-scale digital archives for literary history and bibliography. Focusing on the recently-uncovered reprinting history of Nathaniel Hawthorne’s short story, The Celestial Railroad, in the nineteenth-century press, the article demonstrates the central, shaping influence of religious readers and editors on Hawthorne’s early career. Cordell shows how the best traditional bibliography of Hawthorne’s works could be dramatically altered after only a few weeks’ work in digital archives of newspapers, magazines, and books, and using relatively simple search tools. Such tools not only expanded the number of known witnesses of the text, but also uncovered numerous paratexts: introductions to the story, articles reviewing or referring to the story, sermons derived from the story, etc. This social text of The Celestial Railroad, Cordell argues, lay buried amidst millions of pages that accumulated in the nineteenth century and required modern tools to be uncovered. The article also discusses how digital interpretive tools can help make better sense of such enlarged bibliographies. By comparing multiple printings of The Celestial Railroad using the Juxta Collation tool from NINES, Cordell argues that textual fluidity can tell modern readers much about how texts were understood by their original publishers and readers. The many changes to and discussions of The Celestial Railroad, for instance, indicate that the tale was popular for its perceived anti-denominational message, but nonetheless deployed as a weapon in denominational debates. ", Taken Possession of: The Reprinting and Reauthorship of Hawthorne's Celestial Railroad in the Antebellum Religious Press,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2057,,"{'string_id': None, 'id_scheme': None}"," Artistic representations of African-descendant cultures in Latin America, the Hispanic Caribbean, and the Hispanic World have emerged as the outcome of multiple exchanges, inventions, and cultural coexistences. Such representations take part of a network where cultural, ethnic, social, artistic, literary, and racial information circulates, giving shape to centers with multiple connections at different scales. In accordance with this, the main aim of this article is to demonstrate how Cuba’s predominance for the representation of such cultural pattern is not only based on the significance of particular artistic figures, but also on the connectivity that the island as a cultural node has with respect to the ‘global’ network of African and African-descendant representations. In order to achieve this aim, we carry out two main tasks: a) network analysis looking at related concepts such as centrality, connectivity, betweenness, modularity, etc., through a methodology that takes into account topic-map analysis and the use of Page Ranking Algorithm [the algorithm used by good part of search engines such as Google] as the basic formula to filter and organize information; b) local analysis of two nodes that take part of Cuba’s cluster in order to compare the results of the analysis of networks with a more socio-literary analysis that focuses on detailed reading of some of their messages (works, paratexts, prologues, interviews, etc): Nicolás Guillén and Alejo Carpentier. As matter of conclusions, we evaluate not only our findings with regards to Cuba but also the methodology as a contribution to the field of the Digital humanities and its practices of analysis. ",Modeling Afro-Latin American Artistic Representations in Topic Maps: Cuba’s Prominence in Latin American Discourse,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2058,,"{'string_id': None, 'id_scheme': None}"," Computational literary analytics that include frequency trends and collocation, topic modeling, and network analysis have relied on rapid and large-scale analysis of the word or strings of words. This essay shows that there are many other features of literary texts by which humanists make meaning other than the word, such as prosody and sound, and how computational methods allow us to do what has historically been a more difficult method of analysis — trying to understand how literary texts make meaning with these features. This paper will discuss a case study that uses theories of knowledge representation and research on phonetic and prosodic symbolism to develop analytics and visualizations that help readers discover aural and prosodic patterns in literary texts. To this end, this paper has two parts: (I) We describe the theories of knowledge representation and research into phonetic and prosodic symbolism that underpin the logics and ontologies of aurality incorporated in our project. This basic theory of aurality is reflected in our use of OpenMary, a text-to-speech application tool for extracting aural features; in the flow we coordinated to pre-process texts in SEASR’s Meandre, a data flow environment; in the instance-based predictive modeling procedure that we developed for the project; and in ProseVis, the reader interface that we created to allow readers to discover aural features across literary texts. And (II), we discuss readings of several works by Gertrude Stein (the portraits Matisse and Picasso and the prose poem Tender Buttons) that were facilitated by this work. ",Sounding for Meaning: Using Theories of Knowledge Representation to Analyze Aural Patterns in Texts,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2059,,"{'string_id': None, 'id_scheme': None}"," Embedded in the rich textual record of international copyright law, we often encounter a quaint, and perhaps naïve definition of the literary around which the law has crystallized and which has the potential to influence the work of all digital humanists, whether they think of themselves as literary scholars or not. The first part of this article explores how a relatively narrow definition of the literary as a category of high or belletristic cultural production has informed the contours of U.S. copyright law, in particular. Section 101, Title 17 of the United States Code expressly defines literary works as any works, other than audiovisual works, expressed in words, numbers, or other verbal or numerical symbols or indicia, regardless of the nature of the material objects, such as books, periodicals, manuscripts, phonorecords, film, tapes, disks, or cards, in which they are embodied . US courts have, however, often employed a narrower, more commonplace understanding of the literary as an aesthetic category when sorting artifacts or content into other legally more significant categories such as idea, expression, criticism, parody, and satire. The second section of the article considers some of the potential implications and consequences of the current regulatory structure for the work of digital humanists. Judges engaging in a fair use analysis more often than not expect scholarship to come packaged in print monographs written in academic language aimed at an audience of disciplinary specialists. When they encounter scholarly artifacts that depart from those formal expectations and draw from pre-existing work, judges are less likely to find the use of pre-existing work is fair and therefore non-infringing. Finally, the article examines whether the literary as a category should be abandoned altogether, or whether digital humanists might productively redefine the literary as part of a strategy for re-imagining the institutional and legal regulations that govern academic work. ","Digital Humanities, Copyright Law, and the Literary",,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2060,http://projects.chass.utoronto.ca/chwp/Casta02/McGillivray_casta02.htm,"{'string_id': None, 'id_scheme': None}",Manuscript transcription in the Cotton Nero A.x. Project is at a graphetic level and captures each distinguishable glyph used by the scribe. When the transcription is organized as a series of XML entities within a codicological DTD a search-and-count algorithm can be appied to the database of graphetic information. Initial statistical analysis of the data reveals dramatic changes in the scribe's writing system at two points in the manuscript that are roughly coincident with quire boundaries (and also textual boundaries). Hypotheses that will guide further investigation of this phenomenon include the possibility that substantial gaps of time separated the scribe's work in copying the four Middle English poems that make up the manuscript.,Statistical Analysis of Digital Paleographic Data: What Can It Tell Us?,,2005,"['Statistical analysis, paleography, textual analysis, medieval literature, manuscript, codicology, Pearl-poet, Gawain-poet']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2061,,"{'string_id': None, 'id_scheme': None}"," Junot Díaz’s writing actively questions the boundaries between genre and literary fiction, aesthetics and politics, and English and Spanish, using a framework of multiple linguistic, formal and cultural registers to establish an authorial presence that defies critical categorization. Díaz arrived explosively on the U.S. literary scene with his second book, the novel The Brief Wondrous Life of Oscar Wao. The multi-generational story of a Dominican American family overshadowed by a brutal dictatorship and the challenges of forging a new life in the United States earned many accolades, including the Pulitzer Prize for Fiction and the National Book Critics Circle Award. I read Díaz’s transgressive blending of genre and linguistic registers as a reverse colonization that calls into question the demarcations of American ethnicity as well as the racial politics of nerds. My argument uses Díaz as both an object of study and a paradigm for the potential of a hybrid digital humanities methodology. The complex cultural translation that Díaz asks his readers to perform creates a middle ground where Caribbean history, language politics and the class and ethnic tensions of immigration collide with the nerdy core of the mainstream American imagination. ",Revenge of the Nerd: Junot Díaz and the Networks of American Literary Imagination,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2062,,"{'string_id': None, 'id_scheme': None}"," cut to fit the toolspun course includes a new gloss by the authors on the original JavaScript code. The code was originally published with some comments to assist those who might want to modify or re-use it; this version expands on those comments to explain more about the process of developing the generator and to reflect on the nature of comments and the glossing of code. This file, including comments both practical and reflective, is offered as one model for the criticism of literary works written in code. ",cut to fit the toolspun course,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2063,,"{'string_id': None, 'id_scheme': None}"," Print-on-demand (POD) versions of out-of-copyright literary editions have recently flooded the digital marketplace, dragging the editorial work of the eighteenth and nineteenth centuries back into circulation. Often assembled by software as facsimile publications or OCR ""plain text"" editions, then printed and delivered before the text is seen by human eyes, these POD books are altering how the material weight of the past bears on the present. Through a case study of Milton's Areopagitica, this essay explores how POD's zombie-like revitalization of earlier texts challenges us to broaden our understanding of the nature of digital textuality, especially as it pertains to the work of electronic editing. ",A Deep History of Electronic Textuality: The Case of English Reprints Jhon Milton Areopagitica,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2064,,"{'string_id': None, 'id_scheme': None}"," In 1995 in the midst of the first widespread wave of digitization, the Modern Language Association issued a Statement on the Significance of Primary Records in order to assert the importance of retaining books and other physical artifacts even after they have been microfilmed or scanned for general consumption. A primary record, the MLA told us then, can appropriately be defined as a physical object produced or used at the particular past time that one is concerned with in a given instance (27). Today, the conceit of a primary record can no longer be assumed to be coterminous with that of a physical object. Electronic texts, files, feeds, and transmissions of all sorts are also now, indisputably, primary records. In the specific domain of the literary, a writer working today will not and cannot be studied in the future in the same way as writers of the past, because the basic material evidence of their authorial activity — manuscripts and drafts, working notes, correspondence, journals — is, like all textual production, increasingly migrating to the electronic realm. This essay therefore seeks to locate and triangulate the emergence of a .txtual condition — I am of course remediating Jerome McGann’s influential notion of a “textual condition” — amid our contemporary constructions of the literary, along with the changing nature of literary archives, and lastly activities in the digital humanities as that enterprise is now construed. In particular, I will use the example of the Maryland Institute for Technology in the Humanities (MITH) at the University of Maryland as a means of illustrating the kinds of resources and expertise a working digital humanities center can bring to the table when confronted with the range of materials that archives and manuscript repositories will increasingly be receiving. ","The .txtual Condition: Digital Humanities, Born-Digital Archives, and the Future Literary",,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2065,,"{'string_id': None, 'id_scheme': None}"," Digital humanities discussions of distant reading, machine reading or not-reading have often turned on a depiction of the field of literary production in which individual texts and authors recede in importance as units of analysis. At the same time, the question of what is specific to the literary in discussions of electronic textuality, or the digital literary, has been under-analyzed. This article contributes to theorizing the digital literary by way of an analysis (or close reading) of the role of machine reading in a postcolonial science fiction novel by Amitav Ghosh, The Calcutta Chromosome. This novel participates in the imagination of electronic textuality and digital forensics at a moment when the imagined possibilities of the digital archive were of intense interest to both cultural critics and literary writers. The figure of the writer of vernacular literature in the novel, I argue, brings together the text's interest in both electronic textuality and the subaltern archive, thus establishing the stakes of the digital precisely on a revamped role for the literary in the context of globalization. As such, Ghosh's novel provides a useful opportunity for re-considering proposals for distant reading in relation to world literary studies and postcolonial criticism. ",The End of Literature: Machine Reading and Amitav Ghosh’s The Calcutta Chromosome ,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2066,,"{'string_id': None, 'id_scheme': None}"," Criminal Code: Procedural Logic and Rhetorical Excess in Videogames explores the code of two videogames, suggesting that reading game code is a fruitful way to enrich our understanding of videogames and the culture they represent. In particular, I show how the code of the open source version of SimCity and the controversial first person shooter JFK: Reloaded reveals elements of the games unavailable to the player and unaccounted for by other critical readings of those games. ",Criminal Code: Procedural Logic and Rhetorical Excess in Videogames,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2067,,"{'string_id': None, 'id_scheme': None}"," What does the category of the literary give to digital humanities? Nothing and everything. This essay considers the ""idiocy"" of the literary: its unaccountable singularity, which guarantees that we continue to return to it as a source, inspiration, and challenge. As a consequence, digital humanities is inspired and irritated by the literary. My essay shows this in three ways. First, through a speculative exploration of the relation between digital humanities and the category of ""the literary."" Second, through a quick survey of the use of literature in digital humanities project. Thirdly, through a specific examination of TEI and character rendering as digital humanities concerns that necessarily engage with the literary. Once again, the literary remains singular and not abstract, literal in a way that challenges and provokes us towards new digital humanities work. ",The Idiocy of the Digital Literary (and what does it have to do with digital humanities)?,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2068,,"{'string_id': None, 'id_scheme': None}"," This essay is a meditation on the possibility of a feminist assessment of digital literary archives and the interdisciplinary tools needed to do such work. Using the Women Writers Project and The Orlando Project as exemplary instances of digital literary scholarship, I discuss possible sites of feminist intervention (content, technological politics, labor structures,etc) and the kinds of theoretical paradigms one might use in such work. I also argue that such assessments are essential to recognizing the ways in which feminist digital literary studies have impacted the field of digital humanities. Drawing on recent work in technology studies and feminist theory, the essay problematizes simplistic celebratory claims and troubles the idea that simply saving women’s work in digital form is enough. I conclude with a set of reflections on the impact of shifting ideas about the value of feminist work and theory in both public and scholarly contexts. This includes a proposal that more established scholars proactively highlight the feminist interventions that they make and that all digital literary scholars consider increasing access to not only the work of women, but to the technologies that are integral to that access as well. ",Whence Feminism? Assessing Feminist Interventions in Digital Literary Archives,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2069,,"{'string_id': None, 'id_scheme': None}"," The Transborder Immigrant Tool is a provocative mobile phone app by the Electronic Disturbance Theater (EDT) that provides sustenance to border crossers by leading them to water and guiding them with poetry. Although the tool can be applied to any border, the chief border it has been tied to and tested on is the US-Mexico border. The EDT present the project as an artistic disruption of the tired national political theater staged at that border. The piece refocuses attention on the basic human needs of those caught in the middle of the stale and stalemated divide. For the EDT, every part of the piece participates in this disruption not merely the finished app or the poetry but the code as well. In this paper, I ask, what would it mean for the code to poetic disruption? One set of poetry for the project created by Amy Sara Carroll offers instructions for desert survival. By presenting instructions as poems, she offers one entre into reading the source code of the app as poetry. Using the methods of Critical Code Studies, I read the code of TBT in light of and as part of the poetic intervention of this complex performance. ",Code as Ritualized Poetry: The Tactics of the Transborder Immigrant Tool,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2070,,"{'string_id': None, 'id_scheme': None}"," Curating is often seen as a mediation between artist, work, and audience, with the curator firmly at the center. The central role of the curator has been further emphasized with the rise of the curator as artist and the institutional and physical limitations of museum spaces. We argue that there are important alternative spaces of meaning to be developed between object and audience. Digital curating, in particular, allows for greater audience participation, both by expanding the potential audience and by allowing visitors to navigate through the virtual galleries under their own direction. We contend that by facilitating the site visitors’ creation of their own visual arguments, a new level of audience participation in visual analysis — indeed, in a fundamental intellectual and intuitive aspect of curating — is made possible. We explore how digital resources can be modified to allow for that contextual sense of making a visual argument through arrangement. An installation at the University of Houston debuted a new tool for digital content management and allows us to make some preliminary observations about the process of visual analysis, its role in curatorial practice, and explore future directions for work. ","Curating Digital Spaces, Making Visual Arguments: A Case Study in New Media Presentations of Ancient Objects",,2013,,Digital Humanities Quarterly,007,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2071,http://projects.chass.utoronto.ca/chwp/CHC2003/McCarty_b2.htm,"{'string_id': None, 'id_scheme': None}","Personification in Ovid's Metamorphoses (understood grammatically, as phenomena created by discernible operations in language) provides a typical problem for the application of computational modelling techniques. An earlier systematic attempt to encode these personifications was described in A.24 as “modelling”. In this paper the result, rather more a phenomenological description than a model, properly so-called, is taken as the basis for construction of a manipulable model. Instances of the abstract entity “fortuna” (fortune, luck, chance) are used to build a relational database of examples. For each example the causative factors are specified; wherever possible these factors are weighted globally to signify their degree of influence on the result. This database is linked to spreadsheet software to generate a chart of these instances in bar-graph format from a formula that computes the total effect of the causative factors. A mathematical function is provisionally introduced to provide more realistic results. Its limitations and directions for further research are discussed.","Depth, Markup and Modelling",,2003,"['modelling, personification, Ovid, Metamorphoses, databases, literary criticism']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2072,,"{'string_id': None, 'id_scheme': None}"," In this article, the significance of the rhetorical and modern definitions of ekphrasis will be discussed through the lens of digital literature and art. It attempts to reinscribe the body in ekphrastic practice by adding touch to the abstracted visualism of the eye, and emphasize defining features of the ancient usage: orality, immediacy and tactility. What I call the digital ekphrasis with its emphasis on enargeia, its strong connections with the ancient definition, and on the bodily interaction with the work of art, conveys an aesthetic of tactility; digitalis=finger. By tracing and elucidating a historical trajectory that takes the concept of ekphrasis in the ancient culture as a starting point, the intention is not to reject the theories of the late 1900s, but through a reinterpretation of ekphrasis put forward an example of how digital perspectives on classic concepts could challenge or revise more or less taken-for-granted assumptions in the humanities. In this context ‘the digital’ is not only a phenomenon that could be tied to certain digital objects or used as a digital tool, but as an approach to history, with strong critical potential. The aim is to show that one of the most important features of our digital culture is that it offers new perspectives – not only on current technology – but also on literary, cultural and aesthetic historical practices. ", A Visual Sense is Born in the Fingertips: Towards a Digital Ekphrasis,,2013,,Digital Humanities Quarterly,007,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2073,,"{'string_id': None, 'id_scheme': None}"," In their seminal report, Our Cultural Commonwealth (2006), the American Council of Learned Societies underscored the need for scholars engaged in digital humanities work to leverage their access to data both to expand their audience to the general public and to generate new research questions. Now is the Future Now? argues that the progress made in digital humanities toward these goals has depended and will depend not only on digital data, but also on their appropriate curation. The article defines digital humanities, data, so-called Big Data, and digital curation. Next it examines digital curation initiatives in the sciences and in the humanities that occurred before the release of Our Cultural Commonwealth. It then considers and evaluates the digital curation work undertaken in the sciences and in the humanities after the report’s publication. In theory and in practice digital curation has benefited substantially from practices developed and tested first in the natural sciences and subsequently adapted for and extended in the humanities. Finally, the piece explores the future work necessary to facilitate symbiosis between digital curation and digital humanities. Collaboration and cooperation, transcending geographical, disciplinary, and institutional boundaries, data sharing, policies and planning, education and training, sustainability — all remain pressing issues in 2013. ",Now is the Future Now? The Urgency of Digital Curation in the Digital Humanities,,2013,,Digital Humanities Quarterly,007,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2074,,"{'string_id': None, 'id_scheme': None}"," Randomly generated content poses problems for theories of digital art: such content is resistant to structural theories, which can only provide templates, and one cannot assume a shared text for close analysis. Instead of reaching fixed endings, such works also tend to be of indefinite length or at least suggest indefinite possible combinations. I argue that the impact of such works can instead be found in how one attempts to work through their underlying grammar, based on limits in the algorithms that generate the content — not those limits themselves, but how their outlines come to be known. Repetitively iterating through these works simultaneously upholds the chance nature of the epiphenomenal occurrences while also illustrating the sameness of the underlying algorithm over time, creating a future-oriented interpretive arc. I examine two works that play off of this technique in different ways: Nick Montfort’s Taroko Gorge, a poetry generator which uses random generation to distill the essence of its object’s possibility, and the action role-playing game Torchlight, which attempts to elevate chance beyond a mere gameplay mechanic and toward an ethic. ",Playing with Chance: On Random Generation in Playable Media and Electronic Literature,,2013,,Digital Humanities Quarterly,007,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2075,,"{'string_id': None, 'id_scheme': None}"," The Simulated Environment for Theatre (SET) offers an alternative to static, two-dimensional visualizations of theatrical text by modeling the process of moving from text to performance in the Western, text-based tradition of live theatre production. Its interface links a three-dimensional ‘Stage View’ populated by character avatars to a ‘Text View’ enhanced with text visualization tools by means of a central ‘Line of Action’ analogous to a timeline. All three parts of the interface can be edited; edits in any individual section appear in the others; and all three can be played back simultaneously. While emphasizing that the traditional medium of theatrical performance is the actor’s body moving in time and space, and providing a digital analogue for that medium, the system also frees users from the limitations of the single, time-bound spectator’s perspective. Potential applications include theatre analysis, theatre pedagogy, and preparation for theatrical production. ",Visualizing Theatrical Text: From Watching the Script to the Simulated Environment for Theatre (SET),,2013,,Digital Humanities Quarterly,007,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2076,,"{'string_id': None, 'id_scheme': None}"," Are we currently confusing being connected with communicating - and does the sort of communication people are typically engaging in on the Internet, in social media and when they use their mobile phones merely lead to superficial rather than meaningful dialogue? If this is the case, it ought to concern Digital Humanities (DH) scholars, many of whom continue to be more interested in how we connect than in the substance and dialogue of that very connectedness. I would like to argue for a better balance between the how and the what of DH - for a qualitative turn of sorts away from an interest in gaining and making accessible more information only, to an interest in also making sense of and understanding that information. For such a turn, computer scientists need input from the humanities whose specialty has always been to turn information into knowledge by means of critical interpretation and contextualization. ",Digital Humanities: On Finding the Proper Balance between Qualitative and Quantitative Ways of Doing Research in the Humanities,,2013,,Digital Humanities Quarterly,007,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2077,,"{'string_id': None, 'id_scheme': None}"," Digital humanities research that requires the digitization of medium-scale, project-specific texts confronts a significant methodological and practical question: is labour-intensive cleaning of the Optical Character Recognition (OCR) output necessary to produce robust results through text mining analysis? This paper traces the steps taken in a collaborative research project that aimed to analyze newspaper coverage of a high-profile murder trial, which occurred in New York City in 1873. A corpus of approximately one-half million words was produced by converting original print sources and image files into digital texts, which produced a substantial rate of OCR-generated errors. We then corrected the scans and added document-level genre metadata. This allowed us to evaluate the impact of our quality upgrade procedures when we tested for possible differences in word usage across two key phases in the trial's coverage using log likelihood ratio . The same tests were run on each dataset – the original OCR scans, a subset of OCR scans selected through the addition of genre metadata, and the metadata-enhanced scans corrected to 98% accuracy. Our results revealed that error correction is desirable but not essential. However, metadata to distinguish between different genres of trial coverage, obtained during the correction process, had a substantial impact. This was true both when investigating all words and when testing for a subset of judgment words we created to explore the murder’s emotive elements and its moral implications. Deeper analysis of this case, and others like it, will require more sophisticated text mining techniques to disambiguate word sense and context, which may be more sensitive to OCR-induced errors. ",Mining for the Meanings of a Murder: The Impact of OCR Quality on the Use of Digitized Historical Newspapers,,2014,,Digital Humanities Quarterly,008,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2078,,"{'string_id': None, 'id_scheme': None}"," Internet technologies are gradually reshaping the function of traditional scholarly publications. There is a growing tendency in some disciplines to publish a digital paper together with supplements such as images, videos, 3D-models and underlying data. More information requires a better usability in terms of overview and finding information, which has led to new features in journals and thus to modifying and extending this genre. These so-called enhanced publications or rich internet publications have various physical forms: PDF documents with embedded interactive models, HTML files enriched with hyperlinks to contextual information and with facilities to highlight information in the text, or aggregations of documents and other resources linked together through metadata which make them findable for semantic search engines. However, many of the originally print-based journals, particularly in the humanities and social sciences, are still rather conservative in format and offer hardly any opportunities for enhanced publishing, which leaves room for some form of self-publishing, perhaps as addition to a regular journal article. In this paper we introduce the Xpos’re tools for authoring and displaying an interactive multimedia scholarly publication, which may be created as a digital companion to a regular journal paper and published, for example, on the author or institution’s website in order to share related research products and to achieve greater visibility. We also report about practical experiences with this software in a few research projects. The Xpos’re software (see: http://xposre.nl/software/) comprises a Flash-based document reader and a set of extensions (plug-ins) that extend the basic functionality of this text browser. The extensions are used to display specific types of multimedia that provide additional functionality, such as viewing images, videos and interactive maps. The input text is XML based, which not only guarantees a durable and flexible encoding of content, but also allows automatic rendering in different formats according to the preferences of users and the limitations of their hardware. In addition, the XML source can be easily transformed to an RDF resource map to meet the requirements of the semantic web. The document reader generates output in HTML, in two flavors, namely a slide based version that uses JavaScript, and a plain HTML text (single page), which is most suitable for printing and which can be used to create e-books. This plain HTML text can also be displayed in a format similar to that of scientific journals through the Xpos’re HTML Reader, which has more features than the automatically generated slides and which can be highly customized by the user. ",Xpos’re: A Tool for Rich Internet Publications,,2014,"['scholarly publishing', 'rich internet publication', 'multimedia', 'authoring', 'Flash', 'HTML', 'RDF']",Digital Humanities Quarterly,008,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2079,,"{'string_id': None, 'id_scheme': None}"," This study analyzes popular songs composed by Japanese female singer-songwriters. Popular songs are a good representation of modern culture and society. Songs by female singer-songwriters account for a large portion of the current Japanese hit charts and particularly play an important role in understanding the Japanese language and communication style. In this study, we applied new methods of computational stylistics to the lyrics of the songs. The results clearly show differences in the characteristics of 10 female singer-songwriters, and we found that the visualization of the lyrics is a typical characteristic of current singer-songwriters. Our findings provide an important case study for computational stylistics and can also be useful for understanding Japanese cultural trends. ",Computational Stylistic Analysis of Popular Songs of Japanese Female Singer-songwriters,,2014,"[' computational stylistics ', 'female singer-songwriters', 'Japanese studies', 'lyrics', 'text analysis', 'statistical methods', None]",Digital Humanities Quarterly,008,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2080,,"{'string_id': None, 'id_scheme': None}"," This article articulates a view of the digital humanities that hopes to advance the discipline across broad scholarly and administrative contexts. It will succeed in its aims if it is both comprehensible to newcomers and stimulating for experienced practitioners: a bridging effort, but one undertaken with serious intent. It proceeds by isolating a key debate for examination, describing two concepts that go a significant distance to solving issues raised by that debate (but not far enough), and exploring the theoretical writings of a selection of high profile digital humanists. The goal (a non-trivial undertaking) is to illustrate the utility of postfoundationalism as a conceptual tool, its interdependence with postindustrial culture, and the light it sheds on our understanding of what DH is. If successful the article, rather than making an essentialist claim that Digital Humanities is defined by postfoundational method, will constitute a contribution to the developing digital humanities agenda. ","Digital Humanities, Postfoundationalism, Postindustrial Culture",,2014,"['defining DH', 'postfoundationalism', 'postindustrialism', 'Theory', 'software studies', 'cultural studies']",Digital Humanities Quarterly,008,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2081,,"{'string_id': None, 'id_scheme': None}"," Well-designed digital tools facilitate the creation of new knowledge in the humanities. Good design is user-centered, focused, and needs-driven, all of which depend on a rich understanding of the target audience or end user. Unsworth’s scholarly primitives and the work of Palmer, Teffeau and Pirmann on scholarly information practices provide a framework for understanding how humanities scholars do their work. We propose applying this framework to the design of a spoken word archive, with the aim of designing a digital tool that is optimized for the documented practices of scholars. We propose that listening and annotation are key activities of humanities scholars performing literary criticism of audio recordings. Taking the SpokenWeb poetry project as an example, we discuss how designing a web-based tool with these key activities in mind could facilitate close and critical engagement with recordings of spoken poetry. We present a methodology for designing a web-based sound archive for literary criticism and we propose features and functionalities that facilitate this criticism. ",A Design Methodology for Web-based Sound Archives,,2014,,Digital Humanities Quarterly,008,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2082,http://projects.chass.utoronto.ca/chwp/CHC2003/McCarty2.htm,"{'string_id': None, 'id_scheme': None}","Modelling, a polysemous idea and all but universal practice in the sciences, is essential to computing and therefore helpful in understanding the implications and consequences of humanities computing for the disciplines in its purview. As defined by their principal application in the physical sciences, models are crude, manipulable simplifications whose heuristic power comes both from the insight they give into otherwise inaccessible aspects of the world and from what they fail to capture. They are thus tools for “knowing true things by what their mockeries be”.",‘Knowing true things by what their mockeries be’: Modelling in the Humanities,,2003,"['modelling, knowledge representation, humanities computing, simulation, philosophy of science, Ovid, , personification']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2083,,"{'string_id': None, 'id_scheme': None}"," Scholars and practitioners of the digital humanities generally recognize the importance of solid project management and oversight. But coursework and publications related to DH project management tend to focus heavily on the difficulties of planning and launching a new project rather than the challenges of maintaining an established one. Meanwhile, online advice for would-be managers is couched in the language of tips and tricks or steps for beginners. Together these phenomena downplay the professional skills needed to successfully manage a project while suggesting that project management is necessary only in the beginning stages of an endeavor. They may even give the impression that scholarship in the digital humanities is inherently ephemeral. Through a case study of project management practices at the William Blake Archive, which began publishing electronic scholarly editions in 1996, this essay details the challenges and rewards of managing an established digital humanities project. Managers of mature projects may be called upon to oversee expansions in scope and mission, research and recommend new features and tools, grow or shrink the number of project staff, seek out alternate sources of support when early grants run out, maintain continuity as collaborators join and leave the project, and develop new workflows and procedures to reflect these and other changes. ",Managing an Established Digital Humanities Project: Principles and Practices from the Twentieth Year of the William Blake Archive,,2014,,Digital Humanities Quarterly,008,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2084,,"{'string_id': None, 'id_scheme': None}"," The aim of PO.EX: A Digital Archive of Portuguese Experimental Literature (http://po-ex.net/) is to represent the intermedia and performative textuality of a large corpus of experimental works and practices in an electronic database, including some early instances of digital literature. This article describes the multimodal editing of experimental works in terms of a hypertext rationale, and then demonstrates the performative nature of the remediation, emulation, and recreation involved in digital transcoding and archiving. Preservation, classification, and networked distribution of artifacts are discussed as representational problems within the current algorithmic and database aesthetics in knowledge production. ",Multimodal Editing and Archival Performance: A Diagrammatic Essay on Transcoding Experimental Literature,,2014,"['Portuguese experimental literature', 'PO.EX', 'digital editing', 'multimodal archive', 'electronic literature', 'database aesthetics']",Digital Humanities Quarterly,008,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2085,,"{'string_id': None, 'id_scheme': None}"," With the digitization of the parliamentary proceedings (Handelingen der Staten Generaal), the structuring of this body of data, and the development of an advanced search engine, we can apply new methods of historical research. This contributes to a further promotion of the sophisticated use of quantitative data to enhance qualitative historical research. This article focuses on the Boerenpartij(Farmers’ Party), the first political party from the far right that entered Dutch parliament after the Second World War (WWII). The Boerenpartij is remembered as being stigmatized by the traditional political parties as wrong ( fout ), as National Socialism and its supporters were dubbed in the Netherlands. However, no systematic research has been conducted on the questions: in what way, how frequently and for what purpose these connections with the wrong past were made. With the available digitized data and the retrieval techniques offered by computer scientists it is now possible to answer these questions. ", War in Parliament: What a Digital Approach Can Add to the Study of Parliamentary History ,,2014,,Digital Humanities Quarterly,008,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2086,,"{'string_id': None, 'id_scheme': None}"," This paper is situated within debates surrounding modes of close and distant reading as they are played out in both the fields of world literature and digital literary studies. It proposes an alternative digital humanities approach to the study of world literature, advocating new methods of close comparative reading rather than the mode of distant reading endorsed by Franco Moretti and Alan Liu . Specifically, the research method proposed here is focused on a close reading of the novel The Secret Scripture (2008) by Irish author Sebastian Barry, with comparative reference to Pat Barker’s well-known war novel Regeneration (1997). Through the development and implementation of a digital humanities research method which facilitates new forms of digital literary analysis, we demonstrate that close reading and digital humanities can too be practicing partners in a way that serves to advance work in both the fields of world literature and digital literary studies. ",A Digital Humanities Approach to Narrative Voice in The Secret Scripture: Proposing a New Research Method,,2014,,Digital Humanities Quarterly,008,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2087,,"{'string_id': None, 'id_scheme': None}"," In the digital humanities we specialize in imagining and launching digital projects, but we rarely consider how to end them. In this paper we propose to discuss the ends of a particular digital project as a case study for the planning of ending. The project we focus on is the Globalization and Autonomy Online Compendium that was developed as a digital outcome of the Globalization and Autonomy project. Specifically, this paper will: Survey the general issues at stake when planning for the end of a digital project; Provide some background on the project and the Compendium; Discuss the underlying technologies that had to be dealt with; Address the specific problem of ending and how we prepared this project for archival deposit; And conclude by talking about some other ends that are really beginnings. ",Burying Dead Projects: Depositing the Globalization Compendium,,2014,"['Archiving', 'Globalization', 'Digital Repositories']",Digital Humanities Quarterly,008,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2088,,"{'string_id': None, 'id_scheme': None}"," Among the many reactions against the digital revolution is a humanitarian movement toward long form online reading in collective and social networks. This movement — visible in online book clubs such as Infinite Summer and 1book140, websites such as longreads.com, and the trend of blogs-to-books publication — is a reaction against superficial increasingly brief headline-driven Internet news. Called to action by the threat of what critic Jessica Helfand has diagnosed as digital culture’s narrative depravation, the deep reading revival has reclaimed narrative and returned it to the populace, transforming reading into an act of mass collaboration on an unprecedented scale. Despite studies corroborating Nicholas Carr’s claim in The Shallows (2011) that the distractions of the digital environment are anathema to immersive linear deep reading, online culture has actually enhanced and accelerated the appreciation of longer richer works through its support of radial reading as described by Jerome McGann. This essay argues that while the intrinsically distracting virtual geography of the Internet has threatened to diminish the role of textured narrative in our intellectual and social lives, the Web has ironically provided the media for the most salient movements in support of the deep reading it threatens to obliterate. ",Escaping the Shallows: Deep Reading’s Revival in the Digital Age,,2014,,Digital Humanities Quarterly,008,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2089,,"{'string_id': None, 'id_scheme': None}"," Systems of canonical references, whereby segments of written works are sequentially labeled with numbers or letters to facilitate cross-referencing, are widely used but seldom studied, undeservedly so. Canonical numbers are complex interpretive mechanisms with a great deal of potential for anyone editing and using electronic texts. In this essay I consider the rationale for and nature of canonical reference systems, to recommend principles to consider when deploying them in digital projects. After briefly reviewing the history of canonical references I note how they have been used so far, emphasizing the advances made by Canonical Text Services (CTS). I argue that the practical and theoretical problems that remain unaddressed require engagement with descriptions of how textual scholarship works and how notional literary works relate to the artefacts that carry them (using Functional Requirements for Bibliographic Records, FRBR). By correlating a theory of canonical reference numbers with those two models — editorial workflow and creative works — I offer key principles that should be addressed when planning, writing, and using digital projects. ",Canonical References in Electronic Texts: Rationale and Best Practices,,2014,"['Canonical numbering, mileposts, FRBR, Canonical Text Services, TEI, XML, text editing, text alignment, digital humanities']",Digital Humanities Quarterly,008,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2090,,"{'string_id': None, 'id_scheme': None}"," For Vilém Flusser, philosopher of technology, the advent of photography heralded the return of the image from its subjection to the linearity of written language. Here we extend his concept of the techno-image (successor of the pre-historical hand-drawn image and the historical printed word), to consider the digital image-text that today dominates reading and writing. Our question: Can we reader-writers think the digitas, or are we doomed to perform its functions in an automati[c] or robotiz[ed] fashion, as Flusser put it, so that, if anything, the digitas now thinks us? The short answer to our question is as follows: we can think the digitas, but only if we consider it, firstly, as a kind of writing (digital orthography) and, secondly, as a caricature of thinking, both impoverished and, dare we say it, funny (digital caricature). ",Digital Caricature,,2014,"['Digital Writing', 'Digital Pedagogy', 'New Media Studies']",Digital Humanities Quarterly,008,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2091,,"{'string_id': None, 'id_scheme': None}"," For digital humanists planning to build tools for cyberinfrastructure several variables ought to be defined for each project. Pay close attention to the balance of traditional methods and new ways of conducting research. When gathering resources to do the job, seek contributions of different domain experts. Also, careful consideration of a tool’s intended scope will help refine the required resources needed to complete a project. This case study illustrates how one project, the Social Networks and Archival Context Project (SNAC), has defined these variables. The process of building a new tool also benefits from an awareness of older infrastructure that has come before it. SNAC illustrates this awareness in the way it has taken advantage of previously existing infrastructure, both cyber and not, by extending its purpose and building new features on top of it. ",Social Networks and Archival Context Project: A Case Study of Emerging Cyberinfrastructure,,2014,,Digital Humanities Quarterly,008,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2092,,"{'string_id': None, 'id_scheme': None}"," The digital era has become inundated with the idea of anonymity as on the Internet where users create avatars in forums and write without obvious material constraint; however when considering documents such as graphic novels and print fiction, the figure of the author remains a nostalgic figure which grants validity to the document. In classic comic book collections such as Watchmen and Batman: Year One by Alan Moore and Frank Miller, the original scripts by the authors are included in special editions in both print and Kindle format. But these original script pages are shrouded in forms of anonymity as they illustrate signs of digitization, either through scanning or during production and thus display various visual clues, such as errors, which relay levels of realness. Furthermore online versions of these script pages, found on fan website databases and authors’ blogs, are complicated by the anonymity the Internet and digital editions produce. Therefore a digital forensics methodology is used to interrogate these script pages in both print and digital format to create an ordering system for digitally manipulated text. It also endeavours to illustrate the possibilities for a forming digital forensics field by using various technical calculations and recreations of text with original software and hardware. ","Computers, Comics and Cult Status: A Forensics of Digital Graphic Novels",,2014,"['digital forensics', 'pop culture', 'comic books', 'quantitative analysis', 'digital literature', 'Internet studies', 'Modern media']",Digital Humanities Quarterly,008,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2093,http://projects.chass.utoronto.ca/chwp/madrigal/Madrigal.htm,"{'string_id': None, 'id_scheme': None}","For three hundred years Shakespeare scholarship has almost unanimously agreed on rejecting a number of plays that were published under William Shakespeare's name during his lifetime, but excluded from the First Folio of 1623. Since the eighteenth century the main argument against their inclusion in the canon has been based on critical taste rather than exhaustive textual examination. However, the use of a computer KWIC program coupled with a search engine like Google shows a significant amount of verbal parallels and very rare word strings shared exclusively between the Shakespeare canonical plays and The Yorkshire Tragedy, The London Prodigal, Lord Cromwell, and The Puritan, which is consistent with tests carried out on texts from the canon.",The Shakespeare Apocrypha in the Time of Google,,2008,"['Shakespeare Apocrypha, idiolect, Linguistic individuation, stylometry']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2094,,"{'string_id': None, 'id_scheme': None}"," In 1986, Susan Harding published The Science Question in Feminism in which she suggests that feminism had moved past questioning What is to be done about the situation of women in science? – or first-wave feminist initiatives — to include more women in the work of science. Aspects of the science question that consider the politics underlying epistemologies of purportedly value-neutral claims and practices resonate for the work (the research, theory, and practices) being done to build information infrastructure in the humanities today — the work that I am defining here as digital humanities work. Reconsidering this work by using the lens of feminist inquiry to understand the concerns common to information science and digital humanities is the perspective I describe here. Specifically, as my title suggests, I am proposing that feminist inquiry can help us articulate and better understand the epistemologies in digital humanities and information science that are shaping the infrastructures we are building and using in the humanities. ",An Information Science Question in DH Feminism,,2015,,Digital Humanities Quarterly,009,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2095,,"{'string_id': None, 'id_scheme': None}"," While research coordinator at the Burney Centre at McGill University in Montreal, I pioneered new digital paleographical methods to support the editorial work on Frances Burney and Samuel Richardson undertaken there. Prior to my interventions, the primary method for reading faint, obscured, and obliterated manuscript texts had been multi-spectral imaging, which is prohibitively expensive, limiting its utility as a general research tool, although it is still sometimes in use. There have not been many alternative digital paleographical methodologies. The potential of image manipulation software, such as Adobe Photoshop, has been noted by a few scholars, but not explored. Working in Adobe Photoshop, I have developed a method of deciphering heavily deleted or obliterated text through the use of layering techniques, altered color levels, and the employment of certain kinds of filters. The method is more advanced than simple image enlargement techniques used by most researchers. Importantly though, it remains far less expensive than multi-spectral imaging. The technique contributed to the recovery of nearly all of the obliterated text in the first two volumes of The Court Journals and Letters of Frances Burney, which were published by Oxford University Press in 2011, and it was also used within in-progress volumes from The Cambridge Edition of the Works of Samuel Richardson. This article discusses the methodology and some of its key results from eighteenth-century manuscripts. ",Adobe Photoshop and Eighteenth-Century Manuscripts: A New Approach to Digital Paleography,,2014,"['image manipulation software', 'digital paleography', 'eighteenth century']",Digital Humanities Quarterly,008,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2096,,"{'string_id': None, 'id_scheme': None}"," Though better known for the novels that eventually led to his being awarded the Nobel Prize for Literature in 2003, there is a strong but less widely recognised case for claiming J. M. Coetzee as a significant figure in the early development of digital humanities. In light of the recent renaissance of computer-assisted statistical approaches to literary analysis, the present article charts in detail a formative period (1969-1973) during which this most important of novelists first adopted, then deconstructed, and eventually rejected one of the discipline’s earliest incarnations. ",J. M. Coetzee's Work in Stylostatistics,,2014,,Digital Humanities Quarterly,008,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2097,,"{'string_id': None, 'id_scheme': None}"," Of all narrative textual forms, the motion picture screenplay may be the most perfectly pre-disposed for computational analysis. Screenplays contain capitalized character names, indented dialogue, and other formatting conventions that enable an algorithmic approach to analyzing and visualizing film narratives. In this article, the authors introduce their new tool, ScripThreads, which parses screenplays, outputs statistical values which can be analyzed, and offers four different types of visualization, each with its own utility. The visualizations represent character interactions across time as a single 3D or 2D graph. The authors model the utility of the tool for the close analysis of a single film (Lawrence Kasdan’s Grand Canyon [1991]). They also model how the tool can be used for distant reading by identifying patterns of character presence across a dataset of 674 screenplays. ",Visualizing and Analyzing the Hollywood Screenplay with ScripThreads,,2014,,Digital Humanities Quarterly,008,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2098,,"{'string_id': None, 'id_scheme': None}"," The recent proliferation of experimental literature has produced a critical and creative exchange between the possibilities of print and digital distribution platforms. Through a focused study of Jonathan Safran Foer’s Tree of Codes, this article confronts the difficulty of citation as an occasion to develop a web based prototype or model of this decidedly paper bound text. The result of this experiment produced a digitized version of Tree of Codes that allows for a further discussion of issues of loss, deformation, and versioning alongside a wider conversation on presentation semantics on the web and browser capabilities. This article argues that experimental features on the web have a great deal to gain in a reciprocal exchange between experimental print media. ",Versioning Loss: Jonathan Safran Foer’s Tree of Codes and the Materiality of Digital Publishing,,2014,"['modelling', 'web development', 'contemporary literature', 'experimental literature', 'materiality', 'US literature']",Digital Humanities Quarterly,008,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2099,,"{'string_id': None, 'id_scheme': None}"," Exhibits focusing specifically on Electronic Literature have been mounted at galleries, libraries, universities, convention spaces, and parks and other outside venues. The Electronic Literature Organization’s 2012 Media Art Show, for example, hosted exhibits in five different locations in Morgantown, including a community arts center, local gallery, the university library, a department’s conference room, and the city’s amphitheater, while the MLA 2012 and 2013 exhibits were held at the Washington State and Hynes convention centers, respectively. The Library of Congress, the most important repository of books in the U.S., hosted Electronic Literature & Its Emerging Forms in April 2013 while Illuminations gallery at University of Ireland Maymooth featured an exhibit of electronic literature in March 2014. This range of venues suggests a flexibility and appeal of electronic literature that is both scalable and broad. With these qualities in mind, this article outlines the various exhibits of electronic literature that the author has curated in order to highlight the two main challenges facing all scholars curating digital –– that is, the challenge of availability and the challenge of presentation. ",Curating Electronic Literature as Critical and Scholarly Practice,,2014,,Digital Humanities Quarterly,008,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2100,,"{'string_id': None, 'id_scheme': None}"," This essay discusses agent-based modeling (ABM) and its potential as a technique for studying history, including literary history. How can a computer simulation tell us anything about the past? This essay has three distinct goals. The first is simply to introduce agent-based modeling as a computational practice to an audience of digital humanists, for whom it remains largely unfamiliar despite signs of increasing interest. Second, to introduce one possible application for social simulation by comparing it to conventional, print-based models of the history of book publishing. Third, and most importantly, I’ll sketch out a theory and preliminary method for incorporating social simulation into an on-going program of humanities research. ",Agent-Based Modeling and Historical Simulation,,2014,"['agent-based modeling', 'model theory', 'historical method']",Digital Humanities Quarterly,008,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2101,,"{'string_id': None, 'id_scheme': None}"," Computer-aided research in the humanities has been inhibited by the prevailing paradigm of software design in humanities computing, namely, the document paradigm. This article discusses the limitations of the document paradigm and contrasts it with the database paradigm. It describes a database-oriented approach that provides a better way to create digital representations of scholarly knowledge, allowing individual observations and interpretations to be shared more widely, analyzed more effectively, and preserved indefinitely. ",Beyond Gutenberg: Transcending the Document Paradigm in Digital Humanities,,2014,,Digital Humanities Quarterly,008,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2102,,"{'string_id': None, 'id_scheme': None}"," As humanities scholars increasingly recognize the value of public engagement, and as the proportion of tenure-track faculty positions available to new graduates continues to decline, many humanities programs are focusing renewed attention on equipping graduate students for careers as scholars both within and beyond academe. To support those efforts, the Scholarly Communication Institute has carried out a study investigating perceptions about career preparation provided by humanities graduate programs. The survey results help to create a more solid foundation on which to base curricular reform and new initiatives by moving the conversation about varied career paths from anecdote to data. The findings make it clear that there are a number of effective interventions that programs can undertake. Many of the skills that people working beyond the tenure track identify as crucial to their positions — things like project management, collaboration, and communication — are also highly beneficial to those working within the professoriate. Structuring courses and projects in a way that emphasizes the acquisition of these skills not only contributes to the success of students who pursue employment outside the tenure track, but also to the vibrant research, teaching, and service of those who pursue academic roles. With the availability of new data to work from and the recommendations above as possible guiding principles, graduate programs have a robust set of tools available that can help facilitate curricular assessment and new initiatives. As the importance of assessing the effectiveness of existing structures and considering potential benefits of reform continues to grow, humanities programs have a strong incentive to demonstrate the ways that their graduate programs contribute to the vitality of the university and the broader public sphere. Equipping graduate students with the skills and literacies needed for 21st century scholarly work — from technical fluency to an understanding of organizational structures — is critical to ensuring continued rigorous and creative research, scholarship, and teaching. ",Humanities Unbound: Supporting Careers and Scholarship Beyond the Tenure Track,,2015,"['careers', 'alt-ac', 'academic labor', 'graduate education']",Digital Humanities Quarterly,009,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2103,,"{'string_id': None, 'id_scheme': None}"," Co-teaching a digital archives course (ENGL-GA.2971) for graduate students in the English Department allowed us to bring together our expertise in both research and pedagogy from two fields: English Literature and Computer Science. The course built on a core pedagogical principle in Computer Science of teaching through projects rather than from unrelated one-off programming or web development assignments. Teaching the Text Encoding Initiative after students had completed hands-on projects (using xHTML, CSS, and a digital archive working in a standard content management system) enabled the building of technological skill sets in a logical and complementary manner. From a literary perspective, building a digital archive — and teaching text encoding — enabled an in-depth consideration of textual materiality, the processes through which literary scholarship must inform technological building decisions, and the ways in which the act of digitization can be used to ask new questions of the text (or to prompt the text to ask new questions of itself). This paper will survey our techniques and approaches to interdisciplinary teaching, culminating in our usage of text encoding for exploring issues of textuality through digital presentation. ",Textual Artifacts and their Digital Representations: Teaching Graduate Students to Build Online Archives,,2015,"['Digital pedagogy', 'computer science', 'online archives', 'graduate DH']",Digital Humanities Quarterly,009,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2104,http://projects.chass.utoronto.ca/chwp/CHC2007/Liu_Smith/Liu_Smith.htm,"{'string_id': None, 'id_scheme': None}","Computer technologies offer us a wider range of nonlinear ways to read, but current markup-based standards of text encoding do not allow us to exploit the full potential of the digital environment. The relational database, another well-established method of encoding information, suggests another definition of text, not as a hierarchical structure but as a ""labelled network of language objects."" This alternative model allows text not only to be stored in a database but also encoded as a database. Some applications of this model are illustrated from our current work with Glyphicus, a database-building tool that we are using to encode the Middle English long poem Sir Perceval of Galles.",A Relational Database Model for Text Encoding,,2008,"['relational databases, text encoding, text modelling, markup, XML']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2105,,"{'string_id': None, 'id_scheme': None}"," When game studies became an area for scholarly inquiry in the academy, feminist game studies soon followed. The first generation of feminist theory in game studies built on the work of Sherry Turkle, Brenda Laurel, and Janet Murray, although some might argue that the legacy of challenging gender norms in game studies goes back even earlier. Now feminist game scholars organize international conferences, edit journals and scholarly collections, and shape trends in the profession, much as their counterparts in the digital humanities attempt to do, but critics in feminist game studies have been able to take advantage of what is seen as a relatively long trajectory of feminist theoretical inquiry and field development. Articulating a need for a feminist corrective in the digital humanities has come at a much slower pace, perhaps because the instrumentalism of a tool seems much less blatantly anti-feminist than the instrumentalism of a gun. Furthermore, calls to action from more radicalized forms of feminist approaches to science and technology studies have been noticeably absent in the literature around digital information retrieval in the humanities. This issue of DHQ indicates that a sea change may finally be taking place. ",What can the digital humanities learn from feminist game studies?,,2015,"['game studies', 'feminism', 'instrumentalism']",Digital Humanities Quarterly,009,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2106,,"{'string_id': None, 'id_scheme': None}"," Widespread ideals about libraries are in conflict with deep-rooted gender-based inequities within the library and gendered perceptions of libraries and librarians by the larger public. These contradictions are particularly striking when we look at gender in conjunction with information technologies that help to structure work-roles in the library, especially as these change. This article uses conventional and fictional timelines to survey the historical junctures of gender and technology in the library and to speculate about the future of the academic library, with particular attention to deployments of the digital humanities in the library and its potential for disrupting these long-standing gender patterns. ",The Shock of the Familiar: Three Timelines about Gender and Technology in the Library,,2015,"['library technology', 'library history', 'gender']",Digital Humanities Quarterly,009,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2107,,"{'string_id': None, 'id_scheme': None}"," A part of the special issue of DHQ on feminisms and digital humanities, this paper takes as its starting place Greg Crane’s exhortation that there is a need to shift from lone editorials and monumental editions to editors ... who coordinate contributions from many sources and oversee living editions. In response to Crane, the exploration of the living edition detailed here examines the process of creating a publicly editable edition and considers what that edition, the process by which it was built, and the platform in which it was produced means for editions that support and promote gender equity. Drawing on the scholarship about the culture of the Wikimedia suite of projects, and the gendered trolling experienced by members of our team in the production of the Social Edition of the Devonshire Manuscript in Wikibooks, and interviews with our advisory group, we argue that while the Wikimedia projects are often openly hostile online spaces, the Wikimedia suite of projects are so important to the contemporary circulation of knowledge, that the key is to encourage gender equity in social behavior, credit sharing, and knowledge organization in Wikimedia, rather than abandon it for a more controlled collaborative environment for edition production and dissemination. ","Enlisting Vertues Noble & Excelent: Behavior, Credit, and Knowledge Organization in the Social Edition",,2015,"['collaboration', 'editing', 'feminism', 'wikimedia', 'wikipedia']",Digital Humanities Quarterly,009,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2108,,"{'string_id': None, 'id_scheme': None}"," Textual bricolage, the unacknowledged re-use of chunks of existing texts within a new composition, spans the liminal space between authorized, publicly shared, and de-authorized texts. While it can result in unique literary juxtapositions, bricolage also challenges the boundaries of authorial ownership. Understanding the methods and responses to textual bricolage reflects how a culture engages with textuality. Yet such study is often hindered by the sheer extent of compared texts. In this article we explore the potential of using Factotum, text similarity recognition software with visual interface, for analysing textual bricolage. Using examples from medieval and recent texts, we discuss different compilation techniques as well as the interaction between the notions of authorship, plagiarism and intertextuality. ",Deconstructing Bricolage: Interactive Online Analysis of Compiled Texts with Factotum,,2015,"['bricolage', 'plagiarism', 'textual similarity software', 'digital text analysis']",Digital Humanities Quarterly,009,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2109,,"{'string_id': None, 'id_scheme': None}"," With the growing volume of user-generated classification systems arising from media tagging-based platforms (such as Flickr and Tumblr) and the advent of new crowdsourcing platforms for cultural heritage collections, determining the value and usability of crowdsourced, folksonomic, or user-generated, freely chosen keywords for libraries, museums and other cultural heritage organizations becomes increasingly essential. The present study builds on prior work investigating the value and accuracy of folksonomies by: (1) demonstrating the benefit of user-generated tags - or unregulated keywords typically meant for personal organizational purposes - for facilitating item retrieval and (2) assessing the accuracy of descriptive metadata generated via a game-based crowdsourcing application. In this study, participants (N = 16) were first tasked with finding a set of five images using a search index containing either a combination of folksonomic and controlled vocabulary metadata or only controlled vocabulary metadata. Data analysis revealed that participants in the folksonomic and controlled vocabulary search condition were, on average, six times faster to search for each image (M = 25.08 secs) compared to participants searching with access only to controlled vocabulary metadata (M = 154.1 secs), and successfully retrieved significantly more items overall. Following this search task, all participants were asked to provide descriptive metadata for nine digital objects by playing three separate single-player tagging games. Analysis showed that 88% of participant-provided tags were judged to be accurate, and that both tagging patterns and accuracy levels did not significantly differ between groups of professional librarians and participants outside of the Library Science field. These findings illustrate the value of folksonomies for enhancing item findability, or the ease with which a patron can access materials, and the ability of librarians and general users alike to contribute valid, meaningful metadata. This could significantly impact the way libraries and other cultural heritage organizations conceptualize the tasks of searching and classification. ","By the People, For the People: Assessing the Value of Crowdsourced, User-Generated Metadata",,2015,"['metadata', 'folksonomies', 'searching', 'findability', 'user-generated metadata', 'libraries', 'gamification', 'crowdsourcing', 'metadata games']",Digital Humanities Quarterly,009,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2110,,"{'string_id': None, 'id_scheme': None}"," Decades of digitisation have made a wealth of digital cultural material available online. Yet search — the dominant interface to these collections — is incapable of representing this abundance. Search is ungenerous: it withholds information, and demands a query. This paper argues for a more generous alternative: rich, browsable interfaces that reveal the scale and complexity of digital heritage collections. Drawing on related work and precedents from information retrieval and visualisation, as well as critical humanistic approaches to the interface, this paper documents and analyses practical experiments in generous interfaces developed in collaboration with Australian cultural institutions. ",Generous Interfaces for Digital Cultural Collections,,2015,"['interface design', 'digital collections', 'visualisation', 'digital cultural heritage']",Digital Humanities Quarterly,009,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2111,,"{'string_id': None, 'id_scheme': None}"," This article examines the relationship between intersectionality and the digital humanities. Intersectionality offers a critical approach to debates between theory and method in the field, transcending simplistic hack vs. yack binaries. This article situates debates over difference in the digital humanities within the context of the culture wars within the U.S. academy during the 1980s and 1990s, locating the stakes for diversity in the digital humanities. It surveys digital humanities projects, outlining the need for alternate histories of the digital humanities told through intersectional lenses. Finally, the article proposes ways of looking forward towards the deeper intersectional analysis needed to expand intellectual diversity in the field and move difference beyond the margins of the digital humanities. ",Beyond the Margins: Intersectionality and the Digital Humanities,,2015,"['digital humanities', 'intersectionality', 'feminism']",Digital Humanities Quarterly,009,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2112,,"{'string_id': None, 'id_scheme': None}", My research highlights the networks contemporary Black trans women create through the production of digital media and in this article I make the emotional and uncompensated labor of this community visible. I provide an added level of insight into my research process as a way to mirror the access I was granted by these collaborators. I use Digital Humanist Mark Sample’s concept of collaborative construction to demonstrate my own efforts to enact a transformative feminist process of writing and researching in the Digital Humanities (DH) while highlighting the ways in which the communities I follow are doing the same in their spheres of influence. ,#transform(ing)DH Writing and Research: An Autoethnography of Digital Humanities and Feminist Ethics ,,2015,"['methodology', 'hashtag activism', 'ethics', 'Voyant', 'Twitter']",Digital Humanities Quarterly,009,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2113,,"{'string_id': None, 'id_scheme': None}"," A response to Jamie ""Skye"" Bianco's ""Man and His Tool, Again? Queer and Feminist Notes on Practices in the Digital Humanities and Object Orientations Everywhere."" ","Orientation: Man and His Tool, Again?",,2015,,Digital Humanities Quarterly,009,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2114,,"{'string_id': None, 'id_scheme': None}"," Materiality Comics is a digital comic produced with a combination of Bitstrips and Comic Life. It argues and visually demonstrates that materiality is an important topic for comics scholars to consider, and that through creating essays in comics form, comics scholars can develop insights about materiality that are unavailable when analyzing comics by others. ",Materiality Comics,,2015,"['comics', 'materiality']",Digital Humanities Quarterly,009,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2115,http://projects.chass.utoronto.ca/chwp/CHC2007/Stoicheff/Stoicheff.htm,"{'string_id': None, 'id_scheme': None}","Early 20th century book collector Otto Ege removed pages from medieval texts and regrouped them into forty boxes now scattered around the United States, Canada and Europe. The University of Saskatchewan, in partnership with other institutions holding the boxes, is in the process of locating as many as possible with the goal of digitally reassembling at least one of the original texts. The project has had the unanticipated effect of focusing scholarly and public attention on the unit of the individual page as opposed to the more familiar object of scrutiny, the book.",Putting Humpty Together Again: Otto Ege's Scattered Leaves,,2008,"['page, Ege, manuscript, leaf, script, Beauvais Missal, digital reconstruction, fragment']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2116,http://projects.chass.utoronto.ca/chwp/CHC2003/Lancashire2.htm,"{'string_id': None, 'id_scheme': None}","The Lexicons of Early Modern English (LEME), an in-progress database of early dictionaries in England from about 1475 to 1700, builds on the EMEDD, now freely available on the Web. LEME is larger in scope, but the reasons for undertaking it remain the same. Language commentary written by Early Modern English lexicographers usefully supplements the OED, as an examination of selected word-entries in John Florio's first Italian-English lexicon (1598) shows. Editorial annotation of works of the period also can glean much from LEME sources. By comparing traditional editorial glosses to Shakespeare's Cymbeline, II.i.15-34, to what early lexicons supply, we can see how unmodern his language was, and how it is (in minutiae) still misunderstood.",The Lexicons of Early Modern English,,2003,"['Lexicons of Early Modern English, Renaissance, lexicon, dictionary, Shakespeare, Cymbeline, John Florio, Randle Cotgrave, database, editing, definition']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2117,,"{'string_id': None, 'id_scheme': None}"," As discussion and debates on the digital humanities continue among scholars, so too does discussion about how academic libraries can and should support this scholarship. Through interviews with digital humanities scholars and academic librarians within the Center for Institutional Cooperation, this study aims to explore some points of common perspective and underlying tensions in research relationships. Qualitative interviews revealed that, while both groups are enthusiastic about the future of faculty-librarian collaboration on digital scholarship, there remain certain tensions about the role of the library and the librarian. Scholars appreciate the specialized expertise of librarians, especially in metadata and special collections, but they can take a more active stance in utilizing current library resources or vocalizing their needs for other resources. This expertise and these services can be leveraged to make the library an active and equal partner in research. Additionally, libraries should address internal issues, such as training and re-skilling librarians as necessary; better-coordinated outreach to academic departments is also needed. ",The Arrival Fallacy: Collaborative Research Relationships in the Digital Humanities,,2015,"['libraries', 'collaboration', 'digital scholarship']",Digital Humanities Quarterly,009,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2118,,"{'string_id': None, 'id_scheme': None}"," Academic writing has generally been understood as operating primarily within the linguistic modality, with writing remediating the voice of an educator or lecturer. Comics, by contrast, are more explicitly multimodal and derive much of their meaning from visual, spatial and linguistic modalities. Because of their multimodality, educational comics challenge the conception of an authoritative author’s voice, as is typically found in traditional educational and academic writing. To examine how authorship and authority function in multimodal educational texts, this paper examines several books in the popular For Beginners and Introducing series of graphic guides, which use images, text, and comics to summarise the work of major philosophers – in this case Derrida and Foucault. The books chosen for this study are all collaborative efforts between writers, illustrators, and designers. In each book, the collaborations function differently, engendering different divisions of authorial labor and forging different constructions of multimodal relationships between image, text, and design. In order to more fully interrogate the ways that these educational comics combine multimodal modes of meaning, this paper itself takes the form of a comic, mimicking at times the books that it is examining. In this way, it serves as a self-reflexive critique of the idea that authorial voice is central to academic writing, and as an example of the challenges and opportunities presented by composing multimodal scholarship which eschews this conception of linguistic authorship. ",Multimodal Authoring and Authority in Educational Comics: Introducing Derrida and Foucault for Beginners,,2015,"['comics studies', 'multimodality', 'adaptation', 'derrida', 'foucault', 'critical pedagogy', 'linguistics']",Digital Humanities Quarterly,009,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2119,,"{'string_id': None, 'id_scheme': None}"," As the diversity of digital humanities practitioners grows, the need to construct a framework allowing for equal acknowledgement to all involved has become more evident. In this article, I argue that the perceived immateriality of scholarship privileges conventional academic labor over similar pursuits, such as data curation, resulting in them being glanced over and ignored in tenure reviews and job evaluations. To counter this, I create a theoretical framework that places materiality at the forefront. More specifically, I draw on and expand Gilles Deleuze's notion of assemblages, as outlined by new materialist philosopher Manuel DeLanda, to posit the idea of data assemblages, which are the result of digital humanities labor and consist of material parts contingent on their contextual relations and always in flux. I use the Digital Public Library as a case study and highlight how the reconceptualization of digital humanities labor moves beyond the merely theoretical to allow us to better understand the interdependency of individuals in the academic ecosystem and has broader implications for the nature of materiality in the digital age. ",Data Assemblages: A Call to Conceptualize Materiality in the Academic Ecosystem,,2015,,Digital Humanities Quarterly,009,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2120,,"{'string_id': None, 'id_scheme': None}"," As Matthew K. Gold acknowledges in his introduction to Debates in the Digital Humanities, there are some gaps, some preferred object orientations, if you will, in the digital humanities. Many of us and our work fall into these gaps, cracks, and in some cases, void space. This work is not intended to indict the two collections examined here, Debates in the Digital Humanities and Companion to Digital Humanities, and in fact I am represented in Debates in a piece entitled, This Digital Humanities That Is Not One. The piece at hand intends, rather, to apply a basic computational humanities method, frequency of keyword occurrence, to bring to the surface what is and what is not visible or embodied across the scope of digital humanisms. ","Man and His Tool, Again? Queer and Feminist Notes on Practices in the Digital Humanities and Object Orientations Everywhere",,2015,,Digital Humanities Quarterly,009,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2121,,"{'string_id': None, 'id_scheme': None}"," For nearly its entire textual life Ezekiel 16 has barely survived. Early Jewish communities were wary of including it in their canon of sacred texts because of the chapter’s explicit and disturbing imagery. Christian communities have likewise wrestled with the text by essentially barring it from communal worship (the text does not appear in any lectionary) as well as nearly bracketing it in scholarship (most, if not all, scholars see the text as a violent and gross misrepresentation of gender roles as well as one of the most exacerbating cases of divine violence. Kathryn Phisterer Darr’s work stands out as one of the few that allows the tension of the text to stand.). Despite all of these objections, however, the text remains in the canon. In this one chapter of the Book of Ezekiel, one finds themes of hope, love, despair, suffering, betrayal, grace, and abandonment — all foundational to the human experience. In addition to a colorful theological tapestry, one also finds a definitively historical text that is troubling to most contemporary readers. Theology, history, and contemporary reader combine to make the text and its message shocking at best and inaccessible at worst; this project proposes a solution by way of sequential art. Because the theological message of Ezekiel 16 is both wrapped up in the larger narratival context and presents itself through forceful and explicit imagery, exegeting the text via sequential art offers the reader a new medium for understanding the text. I propose a retelling of the story found in Ezekiel 16 that consciously creates space for the theological themes, historical realities, and contemporary cultural concerns all to be heard and to stand in tension with one another. Along with a sequential retelling of the story of Israel and YHWH, I will also annotate my work to provide historical, artistic and scholarly perspectives for the reader. I will first create the comic by traditional pencil and ink and then scan the images. The images can then appear in any form needed. ",Graphic Images of YHWH: Exploring and Exploding the Bounds of Sexual Objectification in Ezekiel 16,,2015,"['Ezekiel', 'Ezekiel 16', 'Sexual Objectification', 'Bible', 'Old Testament']",Digital Humanities Quarterly,009,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2122,,"{'string_id': None, 'id_scheme': None}"," This paper documents the results of an intensive data sprint method for undertaking data and algorithmic work using application programming interfaces (APIs), which took place during the Digital Method Initiative 2013 Winter School at the University of Amsterdam. During this data sprint, we developed a method to map the fields of Digital Humanities and Electronic Literature based on title recommendations from the largest online bookseller, Amazon, by retrieving similar purchased items from the Amazon API. A first step shows the overall Amazon recommendation network for Digital Humanities and allows us to detect clusters, aligned fields and bridging books. In a second step we looked into four country-specific Amazon stores (Amazon.com, Amazon.co.uk, Amazon.fr and Amazon.de) to investigate the specificities of the Digital Humanities in these four countries. The third step is a network of all books suggested for the Electronic Literature field in the four Amazon stores we searched, which offers a comparison to the field of Digital Humanities. ",The Data Sprint Approach: Exploring the field of Digital Humanities through Amazon’s Application Programming Interface,,2015,"['Amazon', 'API', 'Data Sprint', 'Digital Humanities', 'Digital Culture', 'Electronic Literature']",Digital Humanities Quarterly,009,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2123,,"{'string_id': None, 'id_scheme': None}"," This article describes a new method for inferring the gender of personal names using large historical datasets. In contrast to existing methods of gender prediction that treat names as if they are timelessly associated with one gender, this method uses a historical approach that takes into account how naming practices change over time. It uses historical data to measure the likelihood that a name was associated with a particular gender based on the time or place under study. This approach generates more accurate results for sources that encompass changing periods of time, providing digital humanities scholars with a tool to estimate the gender of names across large textual collections. The article first describes the methodology as implemented in the gender package for the R programming language. It goes on to apply the method to a case study in which we examine gender and gatekeeping in the American historical profession over the past half-century. The gender package illustrates the importance of incorporating historical approaches into computer science and related fields.Please see the lmullen/gender-article GitHub repository for the code used to create this article. ","Jane, John … Leslie? A Historical Method for Algorithmic Gender Prediction",,2015,"['gender', 'data analysis', 'R', 'text analysis']",Digital Humanities Quarterly,009,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2124,,"{'string_id': None, 'id_scheme': None}"," This essay applies a digital humanities methodology to the study of digital media by framing software history as a problem of big data and textual criticism. While many scholars have already identified source code as an important site for the cultural study of software, this essay proposes that close reading is not suited to the scale and scope of modern application software. Applying text analysis tools to large bodies of source code can help address these problems as well as provide a basis for narrativizing its development and contextualization amidst a complex network of economic, political, and other sociocultural processes. Using source code produced over 15 years of development by the Mozilla foundation, this essay argues that software interfaces act as an extension of public relations narratives, often representing a developer's idealized version of their software which may not necessarily correspond with the state of the application's source code. Topic modeling Mozilla's source code allows us to explore a more nuanced narrative of development that shows how Mozilla has negotiated between the political ideals of open source software and the corporate ideology of its parent companies. ",A Textual History of Mozilla: Using Topic Modeling to Trace Sociocultural Influences on Software Development,,2015,,Digital Humanities Quarterly,009,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2125,,"{'string_id': None, 'id_scheme': None}"," Our comic combines visual literacy, progymnasmata, and critical pedagogy to showcase a classroom study that used comics production to teach visual literacy. The comic first looks at comics criticism, visual rhetoric, and comics scholarship to set a base to build a methodology build in critical pedagogy and ancient rhetoric. Critical pedagogy’s tradition of inviting students to find meaning in the origin of ideas fits in with having students design and study a medium that’s often overlooked during their college experience. Such an approach echoes Freire’s ideas of using critical strategies as an effective model for change. Progymnasmata, and Quintilian’s work in general, allows students to approach the new medium of comics through reading and production through an ancient rhetorical practice that relies on a step-by-step process. Looking at Quintilian's pedagogy, we demonstrate a modern classroom study that uses progymnasmata to make the strange familiar while introducing visuality. The actual study is briefly discussed as well. This amalgamation of ancient rhetoric, comics studies, and critical pedagogy is the basis of the research behind this pieces’ goal of exploring comics as a multimodal means of composition. ",Sequential Rhetoric: Using Freire and Quintilian to Teach Students to Read and Create Comics ,,2015,"['comic studies', 'comics production', 'multimodality', 'critical pedagogy', 'ancient rhetoric', 'progymnasmata', 'qunitilian', 'comics']",Digital Humanities Quarterly,009,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2126,,"{'string_id': None, 'id_scheme': None}"," We present archival liveness as a concept in design and the Digital Humanities and describe its development within a Research Through Design process. Working with a newly acquired archive of contemporary poetry we produced designs that both manifested and geared in to the temporal rhythms of the work and infrastructure of archiving. Drawing on user-centred work with participants, often poets themselves, we focused on marginalia as a material feature of the archive, developing a drawing machine and live Twitter bot. Our work addresses institutional concerns for outreach and engagement while also acknowledging and exploiting the inevitably incomplete or live character of archival collections. For designers working with digital archives, we demonstrate the pragmatic and critical value of liveness as a focus of the design process. ",Archival Liveness: Designing with Collections Before and During Cataloguing and Digitization,,2015,"['archives', 'interface design', 'research through design', 'liveness']",Digital Humanities Quarterly,009,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2127,http://projects.chass.utoronto.ca/chwp/CHC2004/junker/,"{'string_id': None, 'id_scheme': None}","Building an Internet resource for the Native American language East Cree for the last four years has been full of challenges and rewards. We focus here on the technical challenges in order to discuss both the problems encountered in this particular context and the solutions we have considered and explored. We illustrate our discussion with examples from an Oral Stories Database we developed in collaboration with Cree education consultants and speakers in 2002-2004. We advocate an approach that includes fast-prototyping, open-source development, and database engine design that balances speed, availability, features and resources.",The eastcree.org Web Databases: Participatory Action Research with Information Technology,,2009,"['Oral stories database, participatory action research, information technology, aboriginal language preservation']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2128,,"{'string_id': None, 'id_scheme': None}"," This article argues for a speculative, exploratory approach to literary history that incorporates information visualization early on into, and throughout, the research process. The proposed methodology combines different kinds of expertise — including that of fans and scholars in both literary studies and computer science — in processing and sharing unique cultural materials. Working with a vast fan-curated archive, we suggest tempering scholarly approaches to the history of science fiction (SF) with fan perspectives and demonstrate how information visualization can be incorporated into humanistic research processes, supporting exploration and interpretation of little-known cultural collections. ",The Stuff of Science Fiction: An Experiment in Literary History,,2016,"['science fiction studies', 'information visualization', 'literary history', 'fan', 'popular culture', 'archival research']",Digital Humanities Quarterly,010,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2129,,"{'string_id': None, 'id_scheme': None}"," This article introduces a novel approach to textual reuse in order to identify the sources of previously unattributed quotations within the work of Eliza Haywood. The article offers a brief introduction to methods used previously within the field of historical text reuse, describes the combinatorial ngram approach used within the present work, then shows how this method can help us better understand the complex inner workings of Eliza Haywood’s most celebrated novel, Betsy Thoughtless. ",Textual Reuse in the Eighteenth Century: Mining Eliza Haywood’s Quotations,,2016,"['text analysis', 'text mining', 'influence', 'textual reuse', 'intertextuality', 'eighteenth century']",Digital Humanities Quarterly,010,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2130,,"{'string_id': None, 'id_scheme': None}"," The article begins by asking why comics scholars should create comics as scholarship and traces possible answers through a variety of related fields: English, Rhetoric and Composition, Art, and Games. It then investigates the question of whether this article is itself a comic, by reviewing the history of the conversation about defining comics as an imitation of the Bayeux Tapestry. This tapestry section outlines the major camps, positions, and moves that comics scholars have made. The two major threads are the essentialist camp (with Kunzle, Eisner, McCloud, Harvey, Carrier, and Hayman and Pratt) and the constructivist camp (Meskin and Beaty). The section ends with Bart Beaty’s recent (2012) conceptualization of a comics world that does not need to define individual artifacts as being or not being comics. The article ends with a discussion of the importance of distinguishing definition from conceptualization. Building on Beaty’s conceptualization and Gilles Deleuze’s theory of the concept and critique of representation, it offers applications to the comics world. Finally, it returns to the question of whether or not this article is a comic. The answer (yes, and becoming something else), calls for further interventions throughout the comics world that don’t ask what comics are but what comics might become. ",Is this Article a Comic?,,2015,"['comics', ' comics as scholarship ', ' deleuze', 'critical theory', ' digital humanities ', 'defining comics']",Digital Humanities Quarterly,009,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2131,,"{'string_id': None, 'id_scheme': None}"," In this paper, I offer an overview of an idea for a metadata archive, called the Sentences Commentary Text Archive, that attempts to collect and make accessible metadata about the five century-long medieval tradition of commenting on the Sentences of Peter Lombard. If scaled for production, this kind of archive would enhance collaboration among editors, promote previously impossible analyses of large sections of the Sentences commentary tradition, and generally become the backbone of future applications making use of this data. ","The Sentences Commentary Text Archive: Laying the Foundation for the Analysis, Use, and Reuse of a Tradition",,2016,"['archive', 'metadata', 'RDF', 'digital publishing', 'critical editions']",Digital Humanities Quarterly,010,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2132,,"{'string_id': None, 'id_scheme': None}"," This article critiques the rhetoric of openness, accessibility and collaboration that features largely in digital humanities literature by examining the status of student labour, training, and funding within the discipline. The authors argue that the use of such rhetoric masks the hierarches that structure academic spaces, and that a shift to the digital does not eliminate these structural inequalities. Drawing on two surveys that assess student participation in DH projects (one for students, and one for faculty researchers), the article outlines the challenges currently faced by students working in the field, and suggests a set of best practices that might bridge the disparity between rhetoric and reality. ",Student Labour and Training in Digital Humanities,,2016,"['students', 'digital humanities', 'access', 'funding', 'training', 'collaboration', 'pedagogy']",Digital Humanities Quarterly,010,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2133,,"{'string_id': None, 'id_scheme': None}"," A behind the scenes look at the process and practice of the author's dissertation written and drawn entirely in comics form. Specifically, the commentary explores the thinking and sketches behind the opening part of the third chapter titled The Shape of Our Thoughts, which focuses on the interaction between image and text. ",Behind the Scenes of a Dissertation in Comics Form,,2015,"['comics studies', 'arts-based research', 'alternative scholarship', 'comics']",Digital Humanities Quarterly,009,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2134,,"{'string_id': None, 'id_scheme': None}"," Classifying and categorizing the activities that comprise digital humanities has been a longstanding area of interest for many practitioners in this field, fueled by ongoing attempts to define digital humanities both within the academy and in the public sphere. The emergence of directories that cross traditional disciplinary boundaries has also spurred interest in categorization, with the practical goal of helping scholars identify, for instance, projects that take a similar technical approach, even if their subject matter is vastly different. This paper tracks the development of TaDiRAH, the Taxonomy of Digital Research Activities in the Humanities developed by representatives from DARIAH, the European cyberinfrastructure initiative, and DiRT, a digital humanities tool directory. TaDiRAH was created specifically to connect people with information on DiRT and in a DARIAH-DE bibliography, but with the goal of adoption by other directory-like sites. To ensure that TaDiRAH would be usable by other projects, the developers opened drafts for public feedback, a process which fundamentally altered the structure of the taxonomy and improved it in numerous ways. By actively seeking feedback from the digital humanities community and reviewing data about how the source taxonomies are actually used in order to inform term selection, the development of TaDiRAH provides a model that may benefit other taxonomy efforts. ",TaDiRAH: a Case Study in Pragmatic Classification,,2016,"['taxonomy', 'classification', 'directories']",Digital Humanities Quarterly,010,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2135,,"{'string_id': None, 'id_scheme': None}"," This article outlines an innovative photographic technique used to digitize the Malcolm Francis McGregor Epigraphic Squeeze collection held by the Department of Classical, Near Eastern and Religious Studies at the University of British Columbia (UBC) in collaboration with a branch of the UBC library system called Digital Initiatives. The squeezes are photographed using a TTI Repro-Graphic Workstation 4060, Sinar 86H cameraback and digital CMV lenses using CaptureShop image processing software. The images undergo further processing using Photoshop CS6 and the HDR merge and Photomerge tools. The result creates a stunningly clear image which fully captures the details of the squeeze and specifically its three-dimensional quality. ",From Stone to Screen: Digital Revitalization of Ancient Epigraphy,,2016,"['Digital Epigraphy', 'Greek Epigraphy', 'Squeezes']",Digital Humanities Quarterly,010,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2136,,"{'string_id': None, 'id_scheme': None}", This paper presents a quantitative picture of the interactions between poets in the Latin hexameter tradition. The freely available Tesserae website (tesserae.caset.buffalo.edu) automatically searches pairs of texts in a corpus of over 300 works of Latin literature in order to identify instances where short passages share two or more repeated lexemes. We use Tesserae to survey relative rates of text reuse in 24 Latin hexameter works written from the 1st century BCE to the 6th century CE. We compare the quantitative information about text reuse provided by Tesserae to the scholarly tradition of qualitative discussion of allusion by Latinists. ,Comparative rates of text reuse in classical Latin hexameter poetry,,2015,,Digital Humanities Quarterly,009,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2137,,"{'string_id': None, 'id_scheme': None}"," My paper contains original Word Processor Art compositions and an explanation of the theoretical grounding for my work. I compose visual images (comprised of words primarily taken from advertisements about the object they construct) in a program not intended for that purpose (Microsoft Word). I break from the expected form of the Word Processor Document to scrutinize how machines, especially computer programs and the graphical user interface (GUI), influence the consumer's utilization of computers. In particular, my project questions how machines influence users' thinking and how the user-friendly inhibits creativity. The process disassembles the notion of user-friendly as a transparent influence and reveals how media shapes the author's imagination and creations. The ever-present PC, in many ways, parallels the presence of the typewriter in the 1950s, 60s and 70s, which led artists to experiment with the grid-like form of mechanical type to create concrete and, eventually, dirty concrete poetry. My work is inspired by these artists and the typewriter poetry they composed. My work also references art collective JODI's compositions and how our contemporary society resists the limitations of the GUI. My intent is to draw attention to the media of the composition. My work disobeys the prompts of the GUI to emphasize the idea that pre-packaged programs elicit conditioned responses and stifle genuinely creative uses of computing devices. Nietzsche writes that our writing tools are also working on our thoughts (quoted in ). My purpose in producing this work is not a rejection of computing or the GUI, but to draw attention to an interface’s existence and influence on its user. My work points to a creative space outside of conditioned responses to the GUI. ",Word Processor Art: How User-friendly Inhibits Creativity,,2016,"['Graphical User Interface', 'Concrete Poetry', 'Interface', 'GUI', 'Media', 'Creativity', 'Civil Disobedience']",Digital Humanities Quarterly,010,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2138,http://projects.chass.utoronto.ca/chwp/kunstmann/,"{'string_id': None, 'id_scheme': None}","On the web site of the Laboratoire de Français Ancien at the University of Ottawa we intend to offer free access to a query and research tool, both powerful and new, for use on Chrétien de Troyes' masterpiece the Chevalier au Lion, Chrétien being the most famous writer of medieval French romances. We are preparing a presentation of all the Chevalier au Lion manuscripts, with analysis and interpretation, that will constitute an indispensable collection of documents for future critical work – both for critical editions in general and critical editions of this particular work. Our aim is to show the educated public the text itself, accompanied by a series of indices, a study of the language and a complete lexicon of each manuscript copy. When the work is complete, the reader will be able to see images of the manuscript folios, to decipher the text by means of a first, ""archeological"" transcription (the text as is), and to understand it, through a second, critical transcription of the emended manuscripts. The series of indices will allow easy, automatic look-up of the text, and various types of query (phonetic, morphological, syntactic, lexical, semantic, literary, cultural, etc.). Morphosyntactic study will give a systematic framework for determining the meaning and import of textual forms and expressions encountered in reading. The lexicon will provide precise definitions for all the lexical words of each manuscript.","Chrétien de Troyes sur le Web: présentation, analyse et interprétation des manuscrits d'Yvain ou le Chevalier au Lion",,2000,"['manuscript, Old French, Middle Ages']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2139,,"{'string_id': None, 'id_scheme': None}"," The creation and utilization of sound as a methodology for information representation is intriguing for Digital Humanities research, teaching, and practice. However, scholars, researchers, and artists may lack appreciation and/or ability for using sound(s) to enhance or ground their research / presentations. In response, the author created and taught a week-long course, Sound of / in Digital Humanities, offered during the 2014 Digital Humanities Summer Institute. This essay reflects on the course, its planning, implementation, and outcomes and offers insight into the role sound might play in the research, communication, and consumption of Digital Humanities. ",Sound and Digital Humanities: reflecting on a DHSI course,,2016,"['digital pedagogy', 'sound', 'sound editing', 'sound studies', 'sound art', 'digital humanities', 'information representation']",Digital Humanities Quarterly,010,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2140,,"{'string_id': None, 'id_scheme': None}"," The Almanac Archive, a project in its early stages of development, seeks to create a corpus of annotated British almanacs from 1750-1850. Cheap and useful, the almanac was one of the most commonly purchased and frequently read print genres during the eighteenth and nineteenth centuries. By focusing on readers’ annotations in almanacs about everything from social engagements and weather to historical events and the breeding of livestock, The Almanac Archive offers insights into everyday life and ideologies of time. Creating a searchable, digital corpus of high-resolution images from annotated almanacs will encourage new research questions about the relationship between historical events, individuals’ everyday lives, and the materiality of Romantic-era interfaces for tracking time. By theorizing and sharing the ultimate goals and, indeed, challenges of the project even at its early stages, our aim in this paper is to answer Johanna Drucker’s call to pay [m]ore attention to acts of producing and [to put] less emphasis on product during the creation of an interface in order to expose and support the activity of interpretation, rather than to display finished forms . In openly describing the unfinished form of The Almanac Archive and its relationship to current scholarly trends, we outline the technical and theoretical work going into its creation. ",The Almanac Archive: Theorizing Marginalia and Duplicate Copies in the Digital Realm,,2016,"['database design', 'archive', 'marginalia', 'digitization', 'Romanticism', 'almanacs']",Digital Humanities Quarterly,010,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2141,,"{'string_id': None, 'id_scheme': None}"," In this paper we present a statistical method for inferring historical social networks from biographical documents as well as the scholarly aims for doing so. Existing scholarship on historical social networks is scattered across an unmanageable number of disparate books and articles. A researcher interested in how persons were connected to one another in our field of study, early modern Britain (c. 1500-1700), has no global, unified resource to which to turn. Manually building such a network is infeasible, since it would need to represent thousands of nodes and tens of millions of potential edges just to include the relations among the most prominent persons of the period. Our Six Degrees of Francis Bacon project takes up recent statistical techniques and digital tools to reconstruct and visualize the early modern social network. We describe in this paper the natural language processing tools and statistical graph learning techniques that we used to extract names and infer relations from the Oxford Dictionary of National Biography. We then explain the steps taken to test inferred relations against the knowledge of experts in order to improve the accuracy of the learning techniques. Our argument here is twofold: first, that the results of this process, a global visualization of Britain’s early modern social network, will be useful to scholars and students of the period; second, that the pipeline we have developed can, with local modifications, be reused by other scholars to generate networks for other historical or contemporary societies from biographical documents. ",Six Degrees of Francis Bacon: A Statistical Method for Reconstructing Large Historical Social Networks ,,2016,"['historical networks', 'network inference', 'graph learning', 'early modern Britain', 'unstructured data']",Digital Humanities Quarterly,010,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2142,,"{'string_id': None, 'id_scheme': None}"," This paper presents a joint project of the Humboldt Chair of Digital Humanities at the University of Leipzig, the Perseus Digital Library at Tufts University, and the Harvard Center for Hellenic Studies to produce a new open series of Greek and Latin fragmentary authors. Such authors are lost and their works are preserved only thorugh quotations and text reuses by later texts. The project is undertaking two tasks: 1) the digitization of paper editions of fragmentary works linking them to the source texts from which the fragments have been excerpted; 2) the production of born-digital editions of fragmentary works. The ultimate goals are the creation of open, linked, machine-actionable texts for the study and advancement of the Classical textual fragmentary heritage and the development of a collaborative environment for crowdsourced annotations. ",The Leipzig Open Fragmentary Texts Series (LOFTS),,2016,,Digital Humanities Quarterly,010,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2143,,"{'string_id': None, 'id_scheme': None}"," Humanistic uses of digital technologies have opened up new ways to think about, communicate, and discuss historical research. The common use of digital tools to visually represent ancient cultures and sites, however, has also introduced new issues. For example, critics have argued that digital visualisations, largely synonymous with reconstruction in 3D models, often attempt to represent a photorealistic-artificial vision of the past, and may often prove to be a way to communicate history to a large(r) audience . Against this backdrop, this article will discuss precisely how technology may help immerse researchers into historically situated life, and radically advance historical research. Adding to related criticisms of ocularcentric traditions of knowledge production, we contribute to this stream of research by arguing that contemporary visual representations of the past often concentrate on visual representations and seemingly maintain antiquity as a sanitised historio-cultural ideal . More specifically, this article seeks to demonstrate the potential of digital humanities to move beyond mere representations on screen and to mobilize other senses (specifically sound) as a historically situated component for research. For this purpose, we focus on the abstract principles and overall methodology for a recreation of the experience of sounds in the Roman amphitheatre. ",Experiential Analogies: A Sonic Digital Ekphrasis as a Digital Humanities Project,,2016,,Digital Humanities Quarterly,010,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2144,,"{'string_id': None, 'id_scheme': None}"," Coptic represents the last phase of the Egyptian language and is pivotal for a wide range of disciplines, such as linguistics, biblical studies, the history of Christianity, Egyptology, and ancient history. It was also essential for cracking the code of the Egyptian hieroglyphs. Although digital humanities has been hailed as distinctly interdisciplinary, enabling new forms of knowledge by combining multiple forms of disciplinary investigation, technical obtacles exist for creating a resource useful to both linguists and historians, for example. The nature of the language (outside of the Indo-European family) also requires its own approach. This paper will present some of the challenges -- both digital and material -- in creating an online, open source platform with a database and tools for digital research in Coptic. It will also propose standards and methodologies to move forward through those challenges. This paper should be of interest not only to scholars in Coptic but also others working on what are traditionally considered more marginal language groups in the pre-modern world, and researchers working with corpora that have been removed from their original ancient or medieval repositories and fragmented or dispersed. ",Raiders of the Lost Corpus,,2016,"['corpus linguistics', 'digital history', 'digital philology', 'Egypt', 'Coptic', 'digital standards', 'religious studies', 'Classics']",Digital Humanities Quarterly,010,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2145,,"{'string_id': None, 'id_scheme': None}"," The presentation, publication and research platforms used for scholarly work in the Digital Humanities embody argument structures that are not always explicitly acknowledged. This article examines these platforms, and their protocols, as middleware that includes such purpose-designed projects as Omeka, and Scalar, and general purpose ones such as Drupal and PowerPoint, to ask how they embody rhetorical assumptions at every level of production (from back-end assumptions about what constitutes the smallest unit of discourse, to the front-end modes of presentation and organization of display). It extends the concept of middleware to include physical and social presentation spaces, activities (such as witnessing), to ask how these, also, perform the rhetorical activity of enunciation, positionality, and other discursive modalities. ",The Why and How of Middleware,,2016,"['intellectual middleware', 'knowledge production', 'argument structure']",Digital Humanities Quarterly,010,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2146,,"{'string_id': None, 'id_scheme': None}"," At the meeting of the Digital Classics Association in April of 2013, I described my work-in-progress on a digital tutorial for Ancient Greek. My goal is to create a self-guided computer tutorial that people can use to learn Ancient Greek on their own in the the same way that those studying a modern language can use programs such as Rosetta Stone, Duolingo, Babbel.com, Mango Languages, etc. This article briefly details the work that has been completed since 2013, describes some of the ways that the tutorial has been successful, and explores challenges that remain to be addressed for the tutorial. ",Toward an Open Digital Tutorial for Ancient Greek v. 2.0,,2016,,Digital Humanities Quarterly,010,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2147,,"{'string_id': None, 'id_scheme': None}"," The Ancient World in 19th-Century Fiction is a lightly revised version of a lecture delivered at the first meeting of the Digital Classicists Association. The intent of the lecture, in accordance with the invitation to deliver it, was to introduce literary macroanalysis in the context of the ancient world and offer some exploration of how the ancient world is represented in the 19th-century literary imagination. ","The Ancient World in Nineteenth-Century Fiction; or, Correlating Theme, Geography, and Sentiment in the Nineteenth Century Literary Imagination",,2016,"['Natural Language Processing', 'Sentiment Analysis', 'Topic Modeling', 'Literature']",Digital Humanities Quarterly,010,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2148,,"{'string_id': None, 'id_scheme': None}"," The Hellespont project (DAI, Tufts University) aims to structure the text of a passage from the ancient Greek historian Thucydides (1.89-118), in order to highlight events, persons and peoples that populate the world of the author and connect the different digital sources available for their study. Event annotation in the text in particular requires an in-depth linguistic analysis of morphology, syntax and semantics. However, the available resources for Ancient Greek do not provide adequate standards to support the encoding of semantic and pragmatic phenomena in Ancient Greek texts. In this paper, we discuss the motivation of the project and how we adapted the so called tectogrammatical annotation of the Prague Dependency Treebank to identify the events and describe their structure. The linguistic notion of valency, which is central to tectogrammatical sentence representation, proves very useful for this analysis of Ancient Greek. ",Treebanking in the world of Thucydides. Linguistic annotation for the Hellespont Project,,2016,"['treebanks', 'computational linguistics', 'Ancient Greek', 'digital history', 'syntax and semantics']",Digital Humanities Quarterly,010,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2149,http://projects.chass.utoronto.ca/chwp/heimpel2/,"{'string_id': None, 'id_scheme': None}","How can the specificity of scholarly electronic publications (e-publications) be stressed without inviting claims of inferiority vis-à-vis paper publications (p-publications)? The response I am proposing in this article treats resistance to e-publication as an essentially ideological or conceptual problematic. Examples are drawn from both scientific and humanities scholarly publication, since the conceptual problematic analysed and the solution proposed apply to both fields. Section 2 examines how different approaches to the integration of new technologies inevitably make certain assumptions about the nature of change, its ""felicity conditions,"" and limits. In Section 3, ""Metaphor as Strategy,"" the claims of similarity between p-publications and e-publications made by incrementalist integration approaches are analysed as a kind of metaphorical argument. Section 4 describes the extreme strain and pressure exerted on the publication metaphor by the recent barrage of new printing and publishing technologies. In the final section, ""Beyond the Publication Principle,"" I argue for an alternative metaphorisation of scholarly e-publications as virtual communities. In other words, the legitimization of scholarly e-publication must be approached as a discursive or rhetorical issue: we do not need to change what we are doing, but rather how it is framed as a value that can be protected.",Legitimizing Electronic Scholarly Publication: A Discursive Proposal,,2000,"['Electronic publications, paper publications, virtual communities, metaphorisation']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2150,,"{'string_id': None, 'id_scheme': None}"," Over a decade ago, the Collaboratory for GIS and Mediterranean (CGMA) was created with two stated goals: (1) to create an on-line GIS inventory of survey projects in the greater Mediterranean and Europe and (2) to introduce advanced undergraduates to GIS concepts through Mediterranean archaeology. Through a specialized GIS course a small group of advanced undergraduate students learned how to interpret map data, create effective maps, and build map hypothesis - skills that are recognized as necessary for enhancing spatial thinking and thus spatial literacy. In the last decade GIS technologically has changed dramatically. The advent of Geospatial Semantic Web (Web 2.0) now makes it possible to integrate spatial thinking concepts at all levels of the undergraduate curriculum. This paper argues that spatial literacy must be more intentionally integrated into the Classical Studies curriculum and illustrates how we have begun to do that at DePauw University. ",Teaching Spatial Literacy in the Classical Studies Curriculum,,2016,"['archaeology', 'classical studies', 'spatial literacy', 'GIS', 'Geospatial Semantic Web']",Digital Humanities Quarterly,010,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2151,,"{'string_id': None, 'id_scheme': None}"," Digital humanities scholars have made a tradition of problematizing our understanding of textuality through discussions concerning the design of information systems for texts that, in many cases, still look like books. This discussion is concerned with how creating opportunities for studying audio texts further complicates our understanding of the rationale of a textualized document, defined by Jerome McGann as the dynamic structure of a document as it is realized in determinate (artisanal) and determinable (reflective) ways. This discussion frames a rationale of audio text within the context of developing information infrastructures for accessing audio texts. I introduce a tool called ARLO that we have been developing in the High Performance Sound Technologies for Access and Scholarship (HiPSTAS) project (http://www.hipstas.org) for accessing and analyzing sound collections alongside new standards being proposed for the development of audio visual (AV) metadata and content models. The discussion concludes by considering what these interventions tell us about how a rationale of audio textuality helps us rethink rationales of text in digital environments. ",Towards a Rationale of Audio-Text,,2016,"['digital humanities', 'textual studies', 'sound studies', 'library and information studies']",Digital Humanities Quarterly,010,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2152,,"{'string_id': None, 'id_scheme': None}"," Referring constitutes such an essential scholarly activity across disciplines that it has been regarded by as one of the scholarly primitives. In Classics, in particular, the references to passages of the ancient texts — the so-called canonical citations (or references) — play a prominent role. The potential of these citations, however, has not been fully exploited to date, despite the attention that they have recently received in the field of Digital Humanities. In this paper I discuss two aspects of making such citations computable. Firstly, I illustrate how they can be extracted from text by using Natural Language Processing techniques, especially Named Entity Recognition. Secondly, I discuss the creation of a three-level citation network to formalise the web of relations between texts that canonical references implicitly constitute. As I outline in the conclusive section of this paper, the possible uses of the extracted citation network include the development of search applications and recommender systems for bibliography; the enhancement of digital environments to read primary sources with links to related secondary literature; and the application of these network to the study of intertextuality and text reception. ",Exploring Citation Networks to Study Intertextuality in Classics,,2016,,Digital Humanities Quarterly,010,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2153,,"{'string_id': None, 'id_scheme': None}"," In this article it is argued that one of the major transformative factors of the humanities at the beginning of the 21st century is the shift from analogue to digital source material, and that this shift will affect the humanities in a variety of ways. But various kinds of digital material are not digital in the same way, which a distinction between digitized, born-digital, and reborn-digital may help us acknowledge, thereby helping us to understand how each of these types of digital material affects different phases of scholarly work in its own way. This is illustrated by a detailed comparison of the nature of digitized collections and web archives. ",Digital Humanities in the 21st Century: Digital Material as a Driving Force ,,2016,"['digital humanities', 'digitality', 'digital material', 'digitized', 'born-digital', 'reborn-digital', 'web', 'web archive', 'web archiving']",Digital Humanities Quarterly,010,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2154,,"{'string_id': None, 'id_scheme': None}"," Covers and bindings are collapsing in the digital textual world. To begin with, the following paper argues that this is not a genuinely new situation, since all cultural Western history attests to written texts as never having been autonomous from oral discourses and versioning steps. Thenceforth – after analyzing the relationship between paper and body, relying notably on Derrida – this article will claim that we have the right and indeed, an obligation, to capture new covers and bindings. During the 17th century, in the lawless parts of the ocean, buccaneers realized that the right to depart was the condition for the capacity to be bound. Therefore, let us dare to depart from ancient bindings and create new boundaries. ",Covers and Corpus wanted! Some Digital Humanities Fragments ,,2016,"['Paper', 'Cover', 'History of reading', 'Jacques Derrida', 'Closure', 'Book', 'Digital Humanities']",Digital Humanities Quarterly,010,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2155,,"{'string_id': None, 'id_scheme': None}"," In this paper we introduce the syntagm pragmatic modelling as a productive way of contextualising research in Digital Humanities (DH). We define pragmatic modelling as a middle-out approach (neither top down nor bottom up) that combines formal and experimental modelling techniques with an effective use of language. Furthermore, in order to elucidate a pragmatic understanding of model building, we reflect on texts (considered here as objects) and modelling (or strategy of analysis) in DH research (and teaching). This paper does not identify a new practice or approach; rather it offers an explanatory framework for existing practices. As the paper explains, this framework goes beyond existing ones and allows us to think about modelling in a more integral way. Drawing on this framework, we reveal how DH modelling practices challenge epistemological and linguistic restrictions, by, for example, problematising the adoption of terminology belonging to the domain of computer sciences. Reflections on metaphorical reasoning are used to exemplify how polarities and some rigidities DH research could find itself embedded in are overcome in practice. We conclude by advocating the importance of a diachronic and historical analysis of the role of metaphors in DH to further explore the relation between theory and practice as well as to develop models of modelling integral to DH research. ",Circling around texts and language: towards pragmatic modelling in Digital Humanities,,2016,,Digital Humanities Quarterly,010,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2156,,"{'string_id': None, 'id_scheme': None}"," In the Digital Humanities, there is a fast-growing body of research that uses data visualization to explore the structures of language. While new techniques are proliferating they still fall short of offering whole language experimentation. We provide a mathematical technique that maps words and symbols to ordered unique numerical values, showing that this mapping is one-to-one and onto. We demonstrate this technique through linear, planar, and volumetric visualizations of data sets as large as the Oxford English Dictionary and as small as a single poem. The visualizations of this space have been designed to engage the viewer in the analogic practice of comparison already in use by literary critics but on a scale inaccessible by other means. We studied our visualization with expert participants from many fields including English studies, Information Visualization, Human-Computer Interaction, and Computer Graphics. We present our findings from this study and discuss both the criticisms and validations of our approach. ",Language DNA: Visualizing a Language Decomposition ,,2016,['Text Visualization'],Digital Humanities Quarterly,10,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2157,,"{'string_id': None, 'id_scheme': None}"," For the past century, the sciences have made terrific strides in capturing the public imagination. From dedicated television channels to online learning materials to science-related entertainment and educational facilities for families in practically every major city on the planet, the scale of public relations has been impressive and continuous. However, the same cannot be said for the Humanities. Although we certainly have cultural institutions for adults, ranging from art galleries to opera houses, their primary emphasis has been on providing opportunities for passively consuming cultural production, rather than with the core interest of the Humanities, which is in enriching objects of study by analyzing them through a variety of theoretical lenses. In this paper, I argue, as others have been doing for some time, that the Humanities need to learn in this respect from the sciences, in order to increase their public stature. Further, the driving force behind this education is Digital Humanities. Since public relations has been on the back burner for so long, this initiative is both important and daunting; it is one of the ways in which the Digital Humanities are strengthening the Humanities while at the same time encouraging them in a task that seems difficult enough that everyone wishes it was not necessary. ",How do we get to the Humanitarium from here?,,2016,"['public relations', 'public funding', 'humanities research', 'digital humanities', 'humanities computing']",Digital Humanities Quarterly,010,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2158,,"{'string_id': None, 'id_scheme': None}"," Digital Humanities provide the means and methods to research topics in a transdisciplinary and multilayered way. In this paper we combine perspectives from historical research and literary criticism to problematize the categorization, computation and representation of events in cultural heritage. Combining methodologies of narratology, historic research and Digital Humanities we aim to extract, classify and quantify events in a way that preserves their inherent multiplicity and multivocality. In two case studies we exemplify that narrative may be seen as a means to mediate events rather than a classical media-centred tradition to preserve an authorized version of cultural heritage. ","Explaining Events to Computers: Critical Quantification, Multiplicity and Narratives in Cultural Heritage",,2016,"['cultural heritage', 'event', 'narrative', 'multiplicity', 'tellability', 'event extraction']",Digital Humanities Quarterly,010,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2159,,"{'string_id': None, 'id_scheme': None}"," Historians have argued that nationalism spread from elite groups to larger populations through public media, yet this has never been empirically proven. In this paper, we use digital tools to search for expressions of nationalism in Dutch newspaper discourse in the late nineteenth century by text mining in large newspaper repositories. The absence of emotional nationalist rhetoric in Dutch national newspapers suggests that nationalism in the late nineteenth century was much more subtle than the literature based on elite discourse tells us. ",Mining Public Discourse for Emerging Dutch Nationalism,,2016,"['Netherlands', 'Dutch', 'nationalism', 'text mining', 'topic modelling', 'discourse', 'newspaper', 'banal', 'nationalism']",Digital Humanities Quarterly,010,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2160,http://projects.chass.utoronto.ca/chwp/CHC2004/guertin/,"{'string_id': None, 'id_scheme': None}","This article explores the places subjectivity and agency occupy in game spaces. In an interactive environment, subjectivity becomes motion, becomes the way we move and the choices we make through our embodied location in space. By analyzing the use of embodiment in feminist computer games, specifically Diana Reed Slattery's Glide, we can see how interactive spaces allow us to construct the text as we play within its walls, with our choices forming the topology of the space of our voyaging.",From Complicity to Interactivity: Theories of Feminist Game Play,,2009,"['Glide, digital games, feminism, glyphs']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2161,,"{'string_id': None, 'id_scheme': None}"," These days, we are constantly looking for an image of ourselves in the historical past. “The first ever information revolution,” begins a typical book jacket blurb, “began with the advent of the printed book, enabling Renaissance scholars to formulate new ways of organizing and disseminating knowledge” . A communications scholar offers a view of our place in history that has echoes in countless other publications and presentations: “At present we are witnessing an information revolution whose significance parallels and perhaps even surpasses that of the information revolution caused by the printing press in the fifteenth century” . Publishers, both academic and trade, have created a new genre of book titles that confidently find the digital sizzle in the analog past: The Renaissance Computer (on printed books), The Victorian Internet (on the telegraph), Social Media: The First 2,000 Years (on letters, pamphlets, and graffiti). Scholars give talks with titles like Books as Social Media (Leah Price), Blogging Now and Then (250 Years Ago) (Robert Darnton), and What’s in a Visitor’s Book? Social Media and Volcanic Tourism in the Nineteenth Century (John Brewer). Surrounded as we are by these analogies — by these claims in favor of specific ways of construing the relationship between the past and the present, and by the significance claims that go with them — it seems fair to ask what purposes they serve. There is no doubt that we feel ourselves to be in the midst of a period of dramatic change in our media and information environments. My interest in this essay lies not in these environments themselves, but rather in our popular uses of historical metaphor to explain them. Why, in particular, do we turn to the rise of the printing press, out of all the options available, as our standard analogy for the rise of the internet? What value does this metaphor hold for us, what does it enable and what does it constrain, and what value does metaphoric thinking in general hold for analysis in media studies? It is as common for us today to use the printing press as a unit of measure in estimations of information history as it is for us to discuss our current circumstances in terms of an information revolution. But how historically accurate are these kinds of description? What do they mean for the ways in which we as futurists make use of history — and what do they mean for the ways in which we as historians take the long view? ",The Printing Press as Metaphor,,2016,"['digital humanities', 'book history', 'rhetoric', 'history of the discipline']",Digital Humanities Quarterly,010,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2162,,"{'string_id': None, 'id_scheme': None}"," The art history domain is an interesting case for search engines tailored to the digital humanities, because the domain involves different types of sources (primary and secondary; text and images). One example of an art history search engine is RemBench, which provides access to information in four different databases related to the life and works of Rembrandt van Rijn. In the current paper, RemBench serves as a case to (1) discover the requirements for a search engine that is geared towards the art history domain and (2) make recommendations for the design of user observation studies for evaluating the usability of a search engine in the art history domain, and in digital humanities at large. A user observation study with nine participants confirms that the combination of different source types is crucial in the art history domain. With respect to the user interface, both free-text search and facet filtering are actively used by the observed participants but we observe strong individual preferences. Our key recommendation for specialized search engines is the use of faceted search (free text search combined with filtering) in combination with federated search (combining multiple resources behind one interface). In addition, the user study shows that the usability of domain-specific search engines can successfully be evaluated using a thinking-aloud protocol with a small number of participants. ",Information access in the art history domain: Evaluating a federated search engine for Rembrandt research ,,2016,"['domain-specific search engines', 'art history', 'usability study', 'information seeking behaviour']",Digital Humanities Quarterly,010,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2163,,"{'string_id': None, 'id_scheme': None}"," With the Maker Movement promoting a refreshing DIY ethic in regard to creation and epistemology, the time might be ripe for scholars to adopt such techniques into their own research, particularly in the subfield of mobile communication studies. One can now relatively easily participate in the building and implementation of a variety of digital products, such as mobile apps, that can then be used to study user experiences through interactions with rhetorical forms, including a variety of types of informatics. Our experiences in several projects that use both large- and small-scale mobile apps offer a critique and lessons learned directly from engaging in this type of field experimentation, including reflections on observations, survey responses, and other types of data collection made possible through this model. Four larger issues are addressed here, about conducting research through making apps, providing potential research paths, opportunities and challenges to consider. Perhaps most importantly, this research approach offers the ability to tailor an instrument specific to research needs and then test that instrument in a natural setting, affording a true sense of how people interact with their environments in real situations and real settings. ",The App-Maker Model: An Embodied Expansion of Mobile Cyberinfrastructure,,2016,,Digital Humanities Quarterly,010,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2164,,"{'string_id': None, 'id_scheme': None}"," Early modern printed books pose particular challenges for automatic transcription: uneven inking, irregular orthographies, radically multilingual texts. As a result, modern efforts to transcribe these documents tend to produce the textual gibberish commonly known as dirty OCR (Optical Character Recognition). This noisy output is most frequently seen as a barrier to access for scholars interested in the computational analysis or digital display of transcribed documents. This article, however, proposes that a closer analysis of dirty OCR can reveal both historical and cultural factors at play in the practice of automatic transcription. To make this argument, it focuses on tools developed for the automatic transcription of the Primeros Libros collection of sixteenth century Mexican printed books. By bringing together the history of the collection with that of the OCR tool, it illustrates how the colonial history of these documents is embedded in, and transformed by, the statistical models used for automatic transcription. It argues that automatic transcription, itself a mechanical and practical tool, also has an interpretive effect on transcribed texts that can have practical consequences for scholarly work. ",Machine Reading the Primeros Libros,,2016,,Digital Humanities Quarterly,010,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2165,,"{'string_id': None, 'id_scheme': None}"," Digital archives change more quickly than traditional ones: they are adaptable and transient. This has advantages and disadvantages; digital archives can disappear from sight almost instantly but they can also be easily safeguarded and restored. Borrowing the critical vocabulary of performance studies, digital archives could thus be understood as repertoires rather than traditional archives. By treating digital archives as repertoires, this article explores different threats and opportunities presented by their volatile nature and makes policy and technical recommendations on how to ensure their relevance and sustainability. ",The Archive as Repertoire: Transience and Sustainability in Digital Archives ,,2016,"['Digital archives', 'performance studies', 'open source']",Digital Humanities Quarterly,010,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2166,,"{'string_id': None, 'id_scheme': None}"," This paper presents the results of a study of 1,439 users of British History Online (BHO). BHO is a digital library of key printed primary and secondary sources for the history of Britain and Ireland, with a principal focus on the period between 1300 and 1800. The collection currently contains 1,250 volumes, and 120,000 web pages of material. During a website rebuild in 2014, the project team asked its registered users about their preferences for searching and browsing the content in the collection. Respondents were asked about their current search and browsing behaviour, as well as their receptiveness to new navigation options, including fuzzy searching, proximity searching, limiting search to a subset of the collection, searching by publication metadata, and searching entities within the texts such as person names, place names, or footnotes. The study provides insight into the unique and often converging needs of the site’s academic and genealogical users, noting that the former tended to respond in favour of options that gave them greater control over the search process, whereas the latter generally opted for options to improve the efficacy of targeted keyword searching. Results and recommendations are offered. ",Digital library search preferences amongst historians and genealogists: British History Online user survey,,2016,"['user survey', 'digital library', 'digital archives', 'search', 'browse', 'findability', 'genealogy', 'academic history']",Digital Humanities Quarterly,10,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2167,,"{'string_id': None, 'id_scheme': None}"," This article demonstrates the use of data mining methodologies for the study and research of social media in the digital humanities. Drawing from recent convergences in writing, rhetoric, and DH research, this article investigates how trends operate within complex networks. Through a study of trend data mined from Twitter, this article suggests the possibility of identifying a virality threshold for Twitter trends, and the possibility that such a threshold has broader implications for attention ecology research in the digital humanities. This article builds on the theories of Jacques Derrida, Richard Lanham, and Sidney Dobrin to suggest new theories and methodologies for understanding how attention operates within complex media ecologies at a macroscopic level. While many various theories and methods have investigated writing, rhetoric, and digital media at the microscopic level, this article contends that a complimentary macroscopic approach is needed to further investigate how attention functions for network culture. ",Attention Ecology: Trend Circulation and the Virality Threshold,,2016,"['data mining', 'social media', 'twitter', 'viral', 'trend', 'circulation studies', 'writing studies', 'digital media', 'methodology', 'ecology']",Digital Humanities Quarterly,010,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2168,,"{'string_id': None, 'id_scheme': None}"," This article introduces the Seshat: Global History project, the methodology it is based upon and its potential as a tool for historians and other humanists. Seshat is a comprehensive dataset covering human cultural evolution since the Neolithic. The article describes in detail how the Seshat methodology and platform can be used to tackle big questions that play out over long time scales whilst allowing users to drill down to the detail and place every single data point both in its historic and historiographical context. Seshat thus offers a platform underpinned by a rigorous methodology to actually do longue durée history and the article argues for the need for humanists and social scientists to engage with data driven longue durée history. The article argues that Seshat offers a much-needed infrastructure in which different skill sets and disciplines can come together to analyze the past using long timescales. In addition to highlighting the theoretical and methodological underpinnings, Seshat's potential is demonstrated using three case studies. Each of these case studies is centred around a set of longstanding questions and historiographical debates and it is argued that the introduction of a Seshat approach has the potential to radically alter our understanding of these questions. ","A Macroscope for Global History: Seshat Global History Databank, a methodological overview",,2016,,Digital Humanities Quarterly,010,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2169,,"{'string_id': None, 'id_scheme': None}"," This study examines the extent of the use of geographic code words in place of racial terms in daily news reporting. This is a case study of the only daily newspaper, the Toledo Blade, in the midwestern city of Toledo, Ohio. A data set was constructed by searching a nine year collection of Blade articles, available in full-text searchable format in a ProQuest database, that included the most frequently used directional terms and had specific street addresses (a total of 981 stories). Besides bibliographic data, each story was coded for its location and the general nature of the story. Street addresses were used to compile relevant census tract information on the proportion of minorities in each area referenced. These references were then plotted over a street map of Toledo revealing geographic distributions that do not relate to actual cardinal directions. Population data corresponding to each data point was then analyzed to show that directional terminology correlates with the concentration of minority population. Additionally, a comprehensive content analysis of all 21,667 Blade articles published in this period revealed racial differences in reporting. Such quantified observations are reinforced by examination of particular examples of racialized usage of geographic terms. ",Racial Proxies in Daily News: A Case Study of the Use of Directional Euphemisms,,2016,"['race', 'racism', 'journalism', 'racial proxies']",Digital Humanities Quarterly,010,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2170,,"{'string_id': None, 'id_scheme': None}"," There are a number of fruitful digital humanities approaches to cinema and media studies, but most of them only pursue traditional forms of scholarship by extracting a single variable from the audiovisual text that is already legible to scholars. As an alternative, cinema and media studies should pursue a mostly-ignored digital surrealism that uses computer-based methods to transform film texts in radical ways not previously possible. This article describes one such method using the z-projection function of the scientific image analysis software ImageJ to sum film frames in order to create new composite images. Working with the fifty-five feature-length films from Walt Disney Animation Studios, I describe how this method allows for a unique understanding of a film corpus not otherwise available to cinema and media studies scholars. ",Digital Surrealism: Visualizing Walt Disney Animation Studios,,2016,"['film studies', 'animation', 'visualization']",Digital Humanities Quarterly,011,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2171,http://projects.chass.utoronto.ca/chwp/CHC2004/gibson/,"{'string_id': None, 'id_scheme': None}","While faculty members are beginning to use technology in their teaching, much more attention to technology integration is needed. Professional development experiences can play an important part in supporting faculty in these endeavours; however, most faculty professional development at the postsecondary level has been of an ad hoc nature involving interested individuals who have seen a need and sought out someone more skilled with computers to help them. Glenn (2002) calls for more work in the area of identifying viable models for preparing faculty to infuse technology into teacher education programs. In this paper I share some of the elements of a faculty wide professional development model designed for a large faculty of education. The Teaching with Technology in the Faculty of Education (TTFE) website was developed to provide a professional development model that includes practical resources for faculty seeking ways to make effective use of learning technologies. The model, which includes a suite of online tools and resources, integrates ideas both from people in the faculty and from other postsecondary institutions as to different ways to enhance teaching and learning. Faculty members have opportunities to engage in hands-on learning about active student-centred learning approaches as well as learning about effective use of technology for transforming their courses along these lines.",A Web of Possibilities: Faculty Sharing Ideas For Technology Integration In Teaching,,2009,"['Computer technology, faculty, professional development, classroom integration, stories']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2172,,"{'string_id': None, 'id_scheme': None}"," In this paper we expand Stefan Gradmann’s question at WWW2012 Thinking in the graph: will Digital Humanists ever do so? to consider whether humanists, more generally than just digital ones, might do thinking that is, at least to some useful degree, in the graph too. Drawing on the experience of the Pliny project, and recent work done within that project to explore how Pliny materials might connect with the semantic web, we explore ways in which structured graph-like thinking might be revealed in — to peek out from — parts of humanities research that is common to digital and non-digital humanists alike. Out of this, we propose a number of different ways that scholars might engage with the Semantic Web, and provide examples – arising from the building of a prototype extension to Pliny – of how these engagements could be dealt with. We also explore the challenge of ambiguity and incompleteness in scholarship, explain how 2D space operates in Pliny to cope, to some degree at least, with these issues, and consider the boundaries between the expressiveness of 2D space and the formal graph model of the Semantic Web. We end by proposing several possible avenues for future work that arise from our work so far. ",Fitting Personal Interpretation with the Semantic Web: lessons learned from Pliny,,2017,"['tools for humanities research', 'semantic web']",Digital Humanities Quarterly,011,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2173,,"{'string_id': None, 'id_scheme': None}"," Recent State of the Union addresses have included a number of new visual elements, including a running slide show and interactive social media cards. This paper poses a method for collecting and analyzing these new visual elements and incorporating the results of that study into the study of presidential Rhetoric. This article will: (1) situate the enhanced State of the Union within the study of presidential rhetoric, (2) combine aspects of close and distant reading for critique of the address, (3) provide the results of the approach to distant reading taken here, and (4) discuss the implications of the analysis of this particular visual program as they afford future annual addresses different opportunities, and constraints. ","Obama’s Sixth Annual Address: Image, Affordance, Flow",,2016,"['State of the Union', 'Rhetorical Criticism', 'Image Analysis', 'Speech', 'Presidential Studies']",Digital Humanities Quarterly,010,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2174,,"{'string_id': None, 'id_scheme': None}"," This article discusses the culture of digital citation within the humanities, with specific reference to research done on the citation of two well-used digital resources: British History Online and the Early English Books Online Text Creation Partnership. Because these two resources are available in both print and digital form, they provide a good test case of whether academics prefer to cite print sources when they have used digital resources in their research. ",A Culture of non-citation: Assessing the digital impact of British History Online and the Early English Books Online Text Creation Partnership,,2016,"['citation', 'digital citation', 'URL structures', 'digital resources']",Digital Humanities Quarterly,011,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2175,,"{'string_id': None, 'id_scheme': None}"," Exploratory studies have demonstrated the benefits of implementing Geographic Information Systems (GIS) technology in literary and cultural-historical research. These studies have done much to affirm the power and flexibility of GIS technology as a resource for humanities scholarship. At the same time, however, these studies share a common limitation in that they tend to rely on the analysis of point-based cartographic representations. Such representations are suitable for modelling quantitative geographical phenomena, but they are inadequate for modelling qualitative human phenomena. This inadequacy constitutes a significant problem for researchers who aspire to analyse the geographical experiences and spatial relationships represented in works of literature, including works that contain accounts of travel. The present article proposes a solution to this problem by demonstrating how advanced spatial analyses within GIS such as Cost-Surface Analysis (CSA) and Least-Cost-Path Analysis (LCP) can be used to facilitate more nuanced interpretations of historical works of travel writing and topographical literature. Specifically, the article explains how GIS, CSA and LCP can be combined to build coherent spatial models of the journeys recorded in the works of three canonical eighteenth-century British travellers, each of whom composed influential accounts of their travels through the English Lake District: the poet Thomas Gray (1716–1771), the naturalist Thomas Pennant (1726–1798) and the agriculturist Arthur Young (1741–1820). ",GIS and Literary History: Advancing Digital Humanities research through the Spatial Analysis of historical travel writing and topographical literature,,2016,"['Spatial Humanities', 'Literary GIS', 'English Literature', 'Travel writing', 'English Lake District', 'Cost Surface Analysis', 'Least Cost Paths', 'Digital Mapping']",Digital Humanities Quarterly,011,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2176,,"{'string_id': None, 'id_scheme': None}"," A project is a basic unit of digital humanities (DH) scholarship, which suggests that DH as a discipline should pay more attention to project management, and perhaps to develop the project management models, principles, and methods that are more specific to the discipline. However, DH literature deals with this issue merely by listing the basic principles, or offering specific tips and tricks, which seriously simplifies management of DH projects. DH projects involve building (or at least using) digital tools, which brings the complex tensions between digital and humanistic aspects of these projects. In order to address such a complexity, there is a need for a model for managing DH projects that will learn from information studies and methods in software development, while still being based on values of the humanistic tradition and methods. This article combines a model of scholarly information practices with some concepts of agile software development into a hybrid model for managing DH projects. ",A Hybrid Model for Managing DH Projects,,2017,"['digital humanities', 'project management', 'agile methods', 'information behaviour']",Digital Humanities Quarterly,011,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2177,,"{'string_id': None, 'id_scheme': None}"," In this article, we discuss a way for visualizing interactions or solidarities in a past community. The method that we expound is derived from process-oriented views used in the industry. Therefore, this attempt to adopt practices from outside of the traditional humanities framework must be clearly justified in terms of epistemological position and in terms of choice among the numerous available tools. First, we try to define what the process term could mean in the humanities; then we explain our choice of a representation; we also carry out tests of our method on selected case studies to visualize and question scholarly works. Last, we offer the reader a prototype that must be improved to produce automatic visualization of particular situations. ",To Visualize Past Communities: A Solution from Contemporary Practices in the Industry for the Digital Humanities ,,2017,"['past communities', 'process', 'visualization', 'pedagogy']",Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2178,,"{'string_id': None, 'id_scheme': None}"," James Patterson is the world’s best-selling living author, but his approach to writing is heavily criticised for being too commercially driven — in many respects, he is considered the master of the airport novel, a highly-productive source of commuter fiction. A former marketing professional, Patterson uses his business acumen to drive sales of his novels, which are largely written in conjunction with lesser-known co-authors. Using stylometry, this paper analyses the extent to which Patterson actually contributes to the writing of his novels, situating his process within the context of literary capitalism and the novel as a force of modernity. ",Structure over Style: Collaborative Authorship and the Revival of Literary Capitalism,,2017,"['stylometry', 'popular literature', 'text analysis']",Digital Humanities Quarterly,011,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2179,,"{'string_id': None, 'id_scheme': None}"," This article addresses the relationship of the disciplines of Modern Languages and Digital Humanities in Anglophone academia. It briefly compares and contrasts the nature of these disciplines – most frequently conceived of as either inter- or transdisciplines – before going on to examine in some detail the participation of Modern Linguists in Digital Humanities and that of Digital Humanists in Modern Languages. It argues that, while there is growing evidence of work that crosses disciplinary boundaries between DH and ML in both directions, more work of this sort needs to be done to optimise the potential of both disciplines. It also makes a particular case for Digital Humanities to remain open to critical cultural studies approaches to digital materials as pertaining to the discipline rather than focusing exclusively on more instrumental definitions of Digital Humanities. This argument is consistent with the concerns raised by other scholars with regard to the need for heterogeneity of approach and in particular for increased cultural criticism in Digital Humanities scholarship. Furthermore, we argue that this is where Modern Linguists can make their most decisive contribution to Digital Humanities research, offering what we term a critical DHML approach. We illustrate our arguments with a range of examples from the intersection of ML and DH in the broad field of Hispanic Studies, including the major findings of our own research into digital cultural production in a Latin American context conducted over the last ten years. ","Where’s the ML in DH? And Where’s the DH in ML? The Relationship between Modern Languages and Digital Humanities, and an Argument for a Critical DHML ",,2017,,Digital Humanities Quarterly,011,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2180,,"{'string_id': None, 'id_scheme': None}"," This article describes the results of a case study that applies Neural Network-based Optical Character Recognition (OCR) to scanned images of books printed between 1487 and 1870 by training the OCR engine OCRopus on the RIDGES herbal text corpus (in press). Training specific OCR models was possible because the necessary ground truth is available as error-corrected diplomatic transcriptions. The OCR results have been evaluated for accuracy against the ground truth of unseen test sets. Character and word accuracies (percentage of correctly recognized items) for the resulting machine-readable texts of individual documents range from 94% to more than 99% (character level) and from 76% to 97% (word level). This includes the earliest printed books, which were thought to be inaccessible by OCR methods until recently. Furthermore, OCR models trained on one part of the corpus consisting of books with different printing dates and different typesets (mixed models) have been tested for their predictive power on the books from the other part containing yet other fonts, mostly yielding character accuracies well above 90%. It therefore seems possible to construct generalized models trained on a range of fonts that can be applied to a wide variety of historical printings still giving good results. A moderate postcorrection effort of some pages will then enable the training of individual models with even better accuracies. Using this method, diachronic corpora including early printings can be constructed much faster and cheaper than by manual transcription. The OCR methods reported here open up the possibility of transforming our printed textual cultural heritage into electronic text by largely automatic means, which is a prerequisite for the mass conversion of scanned books. ",OCR of historical printings with an application to building diachronic corpora: A case study using the RIDGES herbal corpus,,2017,"['OCR', 'historical linguistics', 'digitization', 'cultural heritage', 'corpus linguistics']",Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2181,,"{'string_id': None, 'id_scheme': None}"," We propose that network visualization is a digital humanities method that can explore and negotiate the space between text and performance in the study of Shakespeare. The networks developed in this project use the language of Shakespearean plays to trace the relationships between characters in space, in effect, translating the literary text into a web of spatial relations, which are difficult to perceive solely in the act of reading. Our analysis presents a particular method of network visualization, and also demonstrates how this technique can be used as a critical tool to revise our understanding of social disorder in Shakespearean tragedy. We therefore propose a dual scope for this paper. At a methodological level, we argue that network visualization is a way to infer staging and the blocking of theatrical space from the language of the playtext. In our case study, we show how this technique can be used as a form of Shakespearean literary criticism deploying this method to reframe the larger question of social disorder in his tragedies. The network visualizations used in this analysis serve as a stable and reproducible way, beyond any single performance, to delineate how the language of Shakespeare’s plays structures the relationships of characters in space. This study represents the beginning of a digital method that aims to bridge text and performance in the study of Shakespeare by reading the dramatic text for the linguistic codes that organize the space of the stage. ",Shakespeare’s Tragic Social Network; or Why All the World’s a Stage,,2017,,Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2182,http://projects.chass.utoronto.ca/chwp/Casta02/Forest_casta02.htm,"{'string_id': None, 'id_scheme': None}","In this paper, we present the results of a research in which our main objective was to explore text mining techniques in their application to thematical analysis of philosophical corpus. In the first part, we present the software (Numexco) used in our experiment, focusing on the modular approach of the software. The second part presents the results obtained when we applied Numexco to the thematical analysis of Descartes' Discours de la méthode and his Méditations métaphysiques. This paper also demonstrates how this computer assisted technology can allow the user to rapidly identify, explore and navigate through the different themes found in humanities texts.",NUMEXCO: A Text Mining Approach to Thematic Analysis of a Philosophical Corpus,,2005,"['Text classification, thematic analysis, categorization, text mining, philosophy']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2183,,"{'string_id': None, 'id_scheme': None}"," We present a case study on visualizing a collection of historic drawings along its metadata structure while also allowing for close examination of the artifacts’ texture. With regards to the specific character of cultural heritage at the intersection of research, education, and public interest, the presented visualization environment aims at meeting the requirements of both researchers as well as a broader public. We present the results from a collaborative interdisciplinary research project that involved a cultural heritage foundation, art historians, designers, and computer scientists. The case study examines the potential of visualization when applied to, and developed for, cultural heritage collections. It specifically explores how techniques aimed at visualizing the quantitative structure of a collection can be coupled with a more qualitative mode that allows for detailed examination of the artifacts and their contexts by displaying high-resolution views of digitized cultural objects with detailed art historical research findings. Making use of latest web technologies, the resulting visualization environment allows for dynamic filtering and zooming of a collection of visual resources that are arranged along a contextualized timeline. We share insights from our collaborative design process and the feedback and usage data gathered during the deployment of the resulting prototype as a web application. We end with a discussion of transferability of carefully crafted and collaboratively negotiated visualizations of cultural heritage and raise questions concerning the applicability of our approach to related strands of humanities research. ","Past Visions and Reconciling Views: Visualizing Time, Texture and Themes in Cultural Collections",,2017,"['Visualization in the Humanities', 'Digital Art History', 'Zooming and Navigation Techniques', 'Digital Collections']",Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2184,,"{'string_id': None, 'id_scheme': None}"," The concept of literary genre is a highly complex one: not only are different genres frequently defined on several, but not necessarily the same levels of description, but consideration of genres as cognitive, social, or scholarly constructs with a rich history further complicate the matter. This contribution focuses on thematic aspects of genre with a quantitative approach, namely Topic Modeling. Topic Modeling has proven to be useful to discover thematic patterns and trends in large collections of texts, with a view to class or browse them on the basis of their dominant themes. It has rarely if ever, however, been applied to collections of dramatic texts. In this contribution, Topic Modeling is used to analyze a collection of French Drama of the Classical Age and the Enlightenment. The general aim of this contribution is to discover what semantic types of topics are found in this collection, whether different dramatic subgenres have distinctive dominant topics and plot-related topic patterns, and inversely, to what extent clustering methods based on topic scores per play produce groupings of texts which agree with more conventional genre distinctions. This contribution shows that interesting topic patterns can be detected which provide new insights into the thematic, subgenre-related structure of French drama as well as into the history of French drama of the Classical Age and the Enlightenment. ",Topic Modeling Genre: An Exploration of French Classical and Enlightenment Drama,,2017,"['text analysis', 'topic modeling', 'literary studies', 'French literature', 'literary genres', 'drama', 'comedy', 'tragedy', 'seventeenth century', 'eighteenth century', 'Classical Age', 'Enlightenment']",Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2185,,"{'string_id': None, 'id_scheme': None}"," This paper describes how to deal with the scarcity of born-digital primary sources while retrieving materials on the recent past of an academic institution. The case study is an analysis of the first 25 years online of the University of Bologna. The focus of this work is primarily methodological: several different issues are presented, starting with the fact that the University of Bologna website has been excluded for thirteen years from the Internet Archive's Wayback Machine, and possible solutions are proposed and applied. Moreover, this study aims at highlighting how web materials could give us new and distinct insights into the recent past of academic institutions, thereby becoming the starting point for several new studies. ",Reconstructing a website’s lost past Methodological issues concerning the history of Unibo.it,,2017,,Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2186,,"{'string_id': None, 'id_scheme': None}"," Textual Communities is a new system for managing and performing all aspects of an online collaborative scholarly editing project. It permits mounting of document images and offers page-by-page transcription and display, with the facility for project leaders to recruit and manage transcribers and other contributors, allocating and reviewing transcription work as it is done. Most distinctively, Textual Communities is built on a comprehensive model of scholarly editing, enabling both document (page-by-page) and work (intellectual structure, or entity) views of the texts edited. Accordingly, multiple texts of a single work, or part of a work (an entity) may be extracted and compared, using an embedded installation of CollateX. While completely conformant with Text Encoding Initiative guidelines, Textual Communities goes beyond TEI and XML in its ability to handle multiple overlapping hierarchies within texts. This paper will outline the thinking behind the development of Textual Communities, and show examples of its use by several major projects. ",Some principles for making collaborative scholarly editions in digital form,,2017,,Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2187,,"{'string_id': None, 'id_scheme': None}"," The William J. Bankes Archive, Dorchester, is an impressive collection of original material concerning the archaeological, anthropological and natural heritage of Nubia and was amassed in the years 1815-1822. In the last two hundred years, many geo-human factors caused radical changes in the region. In a landscape almost untouched for centuries, the signs of the interactions between the ancient human communities and the natural environment were much clearer in Bankes’ times than now. Digital humanities offer powerful tools to manage and visualize large amounts of data and GIS in particular is an effective form of relational database, where all items of data have a position on the earth. This paper presents the methodology and the preliminary results of a research project that aims at a draft reconstruction of ancient Nubia based on the Bankes Archive. Archaeological, historical, natural history and ethnographic information extracted from the documents will be georeferenced in the GIS. Original maps, landscape views and epigraphic copies will also be made available on-line. ",A Historical Geographic Information System (HGIS) of Nubia Based on the William J. Bankes Archive (1815-1822),,2017,,Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2188,,"{'string_id': None, 'id_scheme': None}"," This paper presents and describes a bottom-up methodology for the detection of stylistic traits in the syntax of literary texts. The extraction of syntactic patterns is performed blindly by a sequential pattern mining algorithm, while the identification of significant and interesting features is performed at a later stage by using correspondence analysis and by ranking patterns by contribution. ",Mining for characterising patterns in literature using correspondence analysis: an experiment on French novels,,2017,,Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2189,,"{'string_id': None, 'id_scheme': None}"," This paper addresses exploratory search in large collections of historical texts. By way of example, we apply our method to a collection of documents comprising dossiers of the former East-German Ministry for State Security, and classical texts. The bases of our approach are topic models, a class of algorithms that define and infer themes pervading the corpus as probability distributions over the vocabulary. Our topic-centered visual metaphor supports to explore the corpus following an intuitive methodology: First, determine a topic of interest, second, suggest documents that contain the topic with ""sufficient"" proportion, and third, browse iteratively through related topics and documents. Our main focus lies on providing a suitable bird's eye view onto the data to facilitate an in-depth analysis in terms of the topics contained. ",Exploratory Search Through Visual Analysis of Topic Models,,2017,,Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2190,,"{'string_id': None, 'id_scheme': None}"," In this paper we intend to present a tool we developed for translation studies and diachronically compare various French translations of the Odyssey. This field of study is part of the more general Classical Receptions studies that try to analyse the influence and adaptation of classical texts in modern and contemporary literature, theatre, cinema, and many other artistic fields. While Greek texts have been analysed by scholars for more than two thousand years, research about classical translations is not yet a most renown subject. In recent years this theme has raised a growing interest in the academic community. We developed a program that can align textual sequences (defined as groups of words delimited by a specified grammatical pivot, in our case proper nouns), without need of previous training. We obtained alignments for many different kinds of translationsEven free translations, a problem that wasn’t generally considered by textual aligners since recent studies. While other programs have an upper bound for one-to-many alignments (for example with a maximum of four translated elements aligned to the same original element) this algorithm allows an indefinite number of alignments, both for the source sequences and the target ones. The aligner is based on an implementation of Needleman-Wunsch algorithm and on a string-based similarity approach to textual segments. The aligner needs to establish proper names as anchor words, as they are a relatively stable feature through different translations and tend to be similar in several languages. Thanks to the alignments obtained using the program, we can explore translations in a number of ways. We will illustrate the creation of a graphical interface to visualize French Homeric translations. With our tool, it is possible to highlight aligned portions of texts and show their immediate differences or similarities, both in meaning and in syntactic distribution. We will show some resulting syntactic analyses carried out on a small sample of texts, taken from a corpus of twenty-seven unabridged French translations of the Odyssey and explore how the study of diachronic translations through algorithms of computational linguistics can produce interesting results for literary and linguistic studies. ",Diachronic trends in Homeric translations,,2017,,Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2191,,"{'string_id': None, 'id_scheme': None}"," For the past fifteen year, scholarly communication networks such as H-Soz-Kult – the German Information Service for Historians – and H-ArtHist – a specialized discussion and information network for art history based in Germany with an international reach – have been steadily publishing conference announcements and reports. Since both services were born digitally, starting with the listserv infrastructure of the Michigan based H-Net and later supplemented by database-driven web sites, the archives are easily accessible by electronic means. The aim of this paper is to demonstrate that the archives of scholarly communication provide a suitable basis for conducting an assessment of broad fields such as German historians or German art history, with relatively low technical effort. For the initial analysis of H-Soz-KultFirst presented at the Historical Network Research Conference 2014 in Ghent., editorial practices facilitated the automated extraction of the speakers’ names as a key feature. But even in cases where no such special markup has been applied, freely available Web services such as AlchemyAPIAlchemyAPI Entity Extraction, http://www.alchemyapi.com/products/alchemylanguage/entity-extraction. provide methods that can be used to achieve comparable results.Thanks to Victoria H. Scott for suggesting analyzing H-ArtHist’s conference announcements in similar ways. ",Comparing Disciplinary Patterns: Exploring the Humanities through the Lens of Scholarly Communication,,2017,,Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2192,,"{'string_id': None, 'id_scheme': None}"," This paper approaches Corinne, ou l'Italie through an inter-disciplinary lens, drawing on literary, historical, and digital methodologies. If Corinne is first and foremost a Romantic novel that foregrounds the love plot between Corinne and Oswald, it is as much a novel about the role of women in society, articulated through the figure of the femme de génie. The plot of the femme de génie finds its expression through the trope of sociability, which I analyze through a social network analysis of the characters in the narrative and a computational analysis of collocates. I argue that through the constellation of sociability in the novel and the national allegories that accompany it, Staël evokes an enduring partiality for an Enlightenment ideal of sociability that remains deeply connected with female emancipation. The lens of sociability thus allows for a re-articulation of the novel as a social critique of patriarchal society. ",An Enlightenment Utopia: The Network of Sociability in Corinne,,2017,,Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2193,http://projects.chass.utoronto.ca/chwp/CHC2003/Finn2.htm,"{'string_id': None, 'id_scheme': None}","This paper addresses the question: “Do We Need a New Theory in Order to Theorize Computer Games?” To answer this question, I will expand two points. The first involves the “we” in “do we need”, the second, the nature of the word theory. My solution involves an emphasis on the key compositional and formal component of computer game construction: programming. The most straightforward answer to the first question is that we are the members of the academy and critical community. Following this simple definition, this paper argues that both groups need to come up with new theoretical models to address computer games, each for their own reasons. Once we articulate these positions, we find that underlying their primary purposes is a position that involves both groups. For the second point, I will make a distinction between theory that is more broadly formal and theory that is cultural. Our answer to the question of whether or not a new theory is needed will change depending on which of these definitions we apply. As a point of departure, my paper responds to a quote from Espen Arseth, which sounds rather pessimistic about early attempts to theorize games by transferring existing models to new media. I agree with some of the spirit of Arseth's quote, but I believe that a properly instituted form of critique can answer many of his concerns, while providing for new modes of critical participation.",What you C is What You Get: Formalizing Computer Game Criticism,,2003,"['Computer games, literary theory, ergotic text']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2194,,"{'string_id': None, 'id_scheme': None}"," Since 2014, The Map of Early Modern London (MoEML) has partnered with professors and students around the world in a unique collaboration between a digital humanities (DH) project and humanities classrooms. The model we have developed addresses a sustainability challenge for DH projects, provides professors with a way of meeting administrative demands for engaged learning, and gives students a high-stakes research-based learning opportunity with the potential for an open-access, peer–reviewed publication. The MoEML Pedagogical Partnership Project emerged from a confluence of problems and opportunities. One longstanding problem for DH practitioners is project-based: how do we sustain the projects already begun? Another problem emerges as DH moves out of the big tent and sets up camp in humanities classrooms at smaller, non-R1 institutions. Also, for scholars not trained in the technologies that drive many DH projects, crossing the analog-digital divide might be daunting and discourage them from contributing to DH projects. To address these challenges, the MoEML Pedagogical Partnership takes Research-Based Learning (RBL) models and turns them into high-profile publication opportunities, mobilizing ubiquitous social networking and communication technologies to connect the project with the new demographic of student contributors. This essay will highlight how digital projects and digi-curious professors can collaborate to develop innovative pedagogical practices that provide projects with content, enliven professors’ pedagogy, and invite students to acquire scholarly research skills, gain digital literacy, and engage in an interdisciplinary and international collaboration. We argue that DH projects can be used innovatively and effectively in the classroom to promote RBL. At the same time, DH projects–open-access ones in particular–can provide a home both for humanities research and for the fruits of digital pedagogy across a wide range of institutional settings. ",The MoEML Pedagogical Partnership Program,,2017,"['digital pedagogy', 'research-based learning', 'collaboration']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2195,,"{'string_id': None, 'id_scheme': None}"," Digital humanities projects and methods are becoming increasingly common in undergraduate humanities classrooms. Digital projects and exercises allow students to engage with new technology, collaborate with peers, graduate students, and faculty, and produce tangible scholarship that is publicly visible. The Lab for the Education and Advancement in Digital Research (LEADR), a new student-focused digital humanities initiative at Michigan State University, has introduced digital components into large numbers of of History and Anthropology courses. Through two years of courses, it has proven fruitful to frame these not as Digital Humanities projects, but as part of a digital liberal arts curriculum that seeks to teach students not only about the domain-specific content, but also essential skills for information retrieval and analysis, media literacy, and communication in the digital age. This framework places these skills as extensions of longstanding skills, literacies, and knowledges that humanities and social sciences have contributed towards liberal arts education. ",Digital Humanities Pedagogy as Essential Liberal Education: A Framework for Curriculum Development,,2017,"['digital pedagogy', 'digital literacy', 'data literacy', 'digital liberal arts', 'digital history', 'digital anthropology']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2196,,"{'string_id': None, 'id_scheme': None}"," In the Fall of 2015, I was hired as contract academic staff at Wilfrid Laurier University’s Department of English and Film Studies to teach the foundation course EN245 The English Literary Tradition (Beginnings to 1660) for the first time as a course with a heavy DH component. My paper is a case study investigating the challenges of creating and delivering a partially online course in a university environment where the majority of teaching is done by sessional instructors whose labour is systemically marginalized by administration. Sessional instructors (or educational entrepreneurs) have even more limited resources (in terms of time, access to technical support, and access to administration) than tenure track faculty; however, open-access educational tools aren’t serving merely to level the playing field, but reshape it altogether as technical support and access to administrative support cease to matter in the delivery of an educational product. Today, many of the tools that are sufficient for the creation of a successful online or partially online course, whether generalist - iTunesU, Zotero, YouTube - or specialist - Google NGram, the University of Victoria`s Map of Early Modern London, Internet Shakespeare Editions - are freely available to instructors. Such freely available tools problematize the relationship between the instructor and the university insofar as universities tend to use proprietary systems (e.g. Desire2Learn) for everything, including data management, presentation, communication, and gradebook integration with the registrar’s office. Universities, in insisting on using these universal proprietary systems for every aspect of course delivery, exacerbate the disenfranchisement of sessional instructors, as access to the support required to become experts in these tools is limited and taken on at the instructor’s cost. A sessional instructor can create an entire course using freely available online tools, at minimal cost and reaching a tremendously large and diverse audience, yet cannot then market that course to any university that has a similar course as an educational product. At the present moment, the sessional instructor and the course are both subject to the curriculum of an individual university and department, despite the fact that courses with a heavy DH component tend towards portability, interoperability, and modularity that renders such boundaries largely incoherent. Though there are attempts to provide funding for courses that will bridge interuniversity boundaries such as the $4.5 million put forward by the Ontario Ministry of Training, Colleges and Universities as a part of the eCampus Ontario initiative, such funding models largely exclude the sessional instructor, who cannot apply for funding as an educational entrepreneur. My paper will tell the story of how I tried to navigate a university system that tried to keep me from using free tools, while at the same time promoting my course as a part of the eCampus Ontario initiative. ",The New Itinerancy: Digital Pedagogy and the Adjunct Instructor in the Modern Academy,,2017,"['digital pedagogy', 'Flipped classroom', 'adjunct instruction', 'digital tools']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2197,,"{'string_id': None, 'id_scheme': None}"," How can alternate histories of DH through feminist criticism, participatory art, and design shape undergraduate pedagogy in DH? In this article, we argue for explicitly employing a scholar-citizen model as a principle of pedagogical design, making explicit many of the latent assumptions of DH belonging and community. By adhering to these design principles we have been able to question some of the assumptions of pedagogical theories like Research Based Learning and public–facing scholarship, demonstrating these theories’ complex relationships public, semi–public, or private dissemination; classroom and non–classroom spaces; complexity of the assigned task; and the role of assessment. Our experiences as Director and Assistant Director for a combined Summer intensive undergraduate Field School in DH occasion this article. ",Undergraduate Students and Digital Humanities Belonging: Metaphors and Methods for Including Undergraduate Research in DH Communities,,2017,"['digital pedagogy', 'scholar-citizen', 'design history', 'research-based learning']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2198,,"{'string_id': None, 'id_scheme': None}"," This article argues that digital humanities projects can promote social change, collaboration, equity, and creativity through a focus on pedagogy in the undergraduate classroom. We analyze a pedagogical project that overtly set out to challenge structures of power and privilege in the undergraduate classroom through the use of an open-source online learning community. The Futures Initiative Commons in a Box site was developed and modified by the university faculty, graduate, and undergraduate students that make up the community of users. This learning community invites undergraduate and graduate students to become active knowledge-producers who contribute to their own teaching and learning. As such, the site works to increase students’ agency and reconfigure hierarchical relationships of power and knowledge. ",Building a Student-Centered (Digital) Learning Community With Undergraduates,,2017,"['pedagogy', 'online learning community', 'open source', 'open access']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2199,,"{'string_id': None, 'id_scheme': None}"," Three years after his death Friedrich Kittler’s impact on the Humanities and Media Studies remains a topic of interest to scholars worldwide. The intellectual challenges presented by his theoretical work, however, are now complemented by the practical and archival difficulties of dealing with his personal digital legacy. How are we to preserve, survey and index the complex data collection Kittler bequeathed to the German Literature Archive in Marbach in the shape of old computers and hard drives? How are the Digital Humanities to handle the archive of one of its most important forefathers? To address these questions, this paper will first focus on the estate itself and then describe the design and development of the Indexer, a tool for the initial indexing of technical information. Two especially problematic aspects are the sheer mass of files (more than 1.5 million) and Kittler's idiosyncratic organization, both of which serve to make conventional content evaluation very difficult. Here, the Indexer has proven to be a powerful tool. Finally, a case study using the indexer's web interface will enable us to tackle the question: When and to what purpose did Friedrich Kittler acquire a computer? ","Friedrich Kittler's Digital Legacy – PART I - Challenges, Insights and Problem-Solving Approaches in the Editing of Complex Digital Data Collections",,2017,,Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2200,,"{'string_id': None, 'id_scheme': None}"," Three years after his death Friedrich Kittler’s impact on the Humanities and Media Studies remains a topic of interest to scholars worldwide. The intellectual challenges presented by his theoretical work, however, are now complemented by the practical and archival difficulties of dealing with his personal digital legacy. How are we to preserve, survey and index the complex data collection Kittler bequeathed to the German Literature Archive in Marbach in the shape of old computers and hard drives? How are the Digital Humanities to handle the archive of one of its most important forefathers? To address these questions, the presentation will first focus on the estate itself and then describe the design and development of the ""Indexer"", a tool for the initial indexing of technical information. Two especially problematic aspects are the sheer mass of files (more than 1.5 million) and Kittler's idiosyncratic organization, both of which serve to make conventional content evaluation very difficult. Here, the ""Indexer"" has proven to be a powerful tool. Finally, a case study using the indexer's web interface will enable us to tackle the question: When and to what purpose did Friedrich Kittler acquire a computer? ","Friedrich Kittler's Digital Legacy – PART II - Friedrich Kittler and the Digital Humanities: Forerunner, Godfather, Object of Research. An Indexer Model Research ",,2017,,Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2201,,"{'string_id': None, 'id_scheme': None}"," The question of how to model similarity between gestures plays an important role in current studies in the domain of human communication. Most research into recurrent patterns in co-verbal gestures – manual communicative movements emerging spontaneously during conversation – is driven by qualitative analyses relying on observational comparisons between gestures. Due to the fact that these kinds of gestures are not bound to well-formedness conditions, however, we propose a quantitative approach consisting of a distance-based similarity model for gestures recorded and represented in motion capture data streams. To this end, we model gestures by flexible feature representations, namely gesture signatures, which are then compared via signature-based distance functions such as the Earth Mover's Distance and the Signature Quadratic Form Distance. Experiments on real conversational motion capture data evidence the appropriateness of the proposed approaches in terms of their accuracy and efficiency. Our contribution to gesture similarity research and gesture data analysis allows for new quantitative methods of identifying patterns of gestural movements in human face-to-face interaction, i.e., in complex multimodal data sets. ",Automated Pattern Analysis in Gesture Research: Similarity Measuring in 3D Motion Capture Models of Communicative Action,,2017,,Digital Humanities Quarterly,011,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2202,,"{'string_id': None, 'id_scheme': None}"," Despite the perceived newness of electronic methods in physical classrooms, electricity–and the distributed labor on which it runs–has long powered the spaces of pedagogy. Routing electronic practices in undergraduate teaching through the digital infrastructures with which they operate, this writing tests circuits of power that migrate between disciplinary and physical learning systems. It does so through a discussion of Pedagogy Toolkit, an open source and community-authored teaching repository built with Jekyll and deployed via GitHub Pages. Contributing to an increase of energy for project-based interventions in digital humanities teaching, Pedagogy Toolkit circulates digitized teaching materials, guides to teaching with digital humanities tools, a curated sample of online syllabuses accompanied by a syllabus templating tool, and an accessible website templating framework. An overview of new methods for digital teaching in the undergraduate classroom leads in turn to a reflexive discussion of the design of digital platforms as pedagogical objects, activating issues of labor, diversity, and knowledge transmission along the way. Ultimately, building a toolkit for digital pedagogy constructs infrastructure as a mode of intellectual inquiry, exposing classroom power as a conduit for ethical connections between students, teachers, and digital development teams. Rerouting logics that partition teaching practice and tool development, this article situates building communities at the heart of humanities learning. ",Building a Toolkit for Digital Pedagogy,,2017,"['digital pedagogy', 'tool development', 'cyberinfrastructure']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2203,,"{'string_id': None, 'id_scheme': None}"," Recent years have seen widespread interest in digital humanities (DH) and growing interest in undergraduate-centered digital curricula. However, few undergraduate DH programs resemble those at large research institutions, where digital initiatives tend to be housed in graduate programs and rooted in graduate pedagogy or faculty research. Further, no two undergraduate DH programs are alike. This article seeks to move beyond graduate- and faculty-centered models by exploring new possibilities for undergraduate DH initiatives. It describes a workshop held at the ADHO DH2015 conference. This workshop brought together practitioners of digital pedagogy from small liberal arts colleges and from undergraduate centers within larger institutions. This article details the workshop’s exploration of undergraduate DH education, situating those practices in the context of broader trends in digital pedagogy. Finally, this article charts three broad challenges faced by programs which emphasize undergraduate digital curricula and offers suggestions and strategies to address these common issues. ",Starting From Scratch? Workshopping New Directions in Undergraduate Digital Humanities,,2017,,Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2204,http://projects.chass.utoronto.ca/chwp/Casta02/Esling_casta02.htm,"{'string_id': None, 'id_scheme': None}","This article describes the development of an auditory coding system and an associated web-based XML database to document and analyze speech development in infants from Canada, Morocco, France, and China. The XML database will allow researchers on this international research project, based at the Department of Linguistics, University of Victoria, to collaborate in data collection and analysis, facilitating a better understanding of phonetic development in infants from diverse language backgrounds. We hypothesize that infants from all language backgrounds employ laryngeal constriction as the primary means of phonetic development in the first months of life, using this experience as a base to learn the sounds of their native language in the second half of the first year. Preliminary results from English-speaking infants support this hypothesis.",Phonetic Structure and Acquisition of Laryngeal and Pharyngeal Articulations: Text-Analysis Considerations,,2005,"['Speech acquisition, phonetics, laryngeal constriction, auditory coding, XML database']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2205,,"{'string_id': None, 'id_scheme': None}"," Digital Humanities (DH) is a field of research in which humanists at the National Autonomous University of Mexico (UNAM) can take part and about which they can collaborate in debates and projects. Introducing DH in the academic programs at an undergraduate level can be a difficult path to traverse. Even so, for the academic year 2016, we managed to include a Specialized Seminar-Workshop on this field of study within the History Course at the Facultad de Filosofía y Letras at the UNAM. This article shows and analyzes the context, the methods and the academic, technical and specialization implications DH has in the History field (and in Humanities in general), and also presents the results of our teaching work, of the research project in teaching to which it is connected, and of some other activities which aim at establishing an academic digital culture in this School’s community. ",DH for History Students: A Case Study at the Facultad de Filosofía y Letras (National Autonomous University of Mexico) ,,2017,"['Digital Humanities', 'History', 'Humanities', 'Digital Culture', 'Education']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2206,,"{'string_id': None, 'id_scheme': None}"," We provide a comprehensive introduction to DREaM (Distant Reading Early Modernity), a hybrid text analysis and text archive project that opens up new possibilities for working with the collection of early modern texts in the EEBO-TCP collection (Phases I & II). Key functionalities of DREaM include i) management of orthographic variance; ii) the ability to create specially-tailored subsets of the EEBO-TCP corpus based on criteria such as date, title keyword, or author; and iii) direct export of subsets to Voyant Tools, a multi-purpose environment for textual visualization and analysis. ",Introducing DREaM (Distant Reading Early Modernity),,2017,,Digital Humanities Quarterly,011,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2207,,"{'string_id': None, 'id_scheme': None}"," This essay presents an approach to teaching Digital Humanities through two largely unexplored lenses: electronic literature and foreign languages (Spanish in particular). It offers a practical example of a course taught during the Spring of 2016 at UC Berkeley that combines literary analysis with the teaching of basic programming skills, and DH tools and methods. Concretely, this course is an upper division, undergraduate writing intensive class, where students learn how to write and talk about electronic literature–e.g. hypertext novels, kinetic poetry, automatic generators, social media fictions, etc.–, learning specific terminology and theoretical frameworks, as they gain the skills to build their own digital art pieces in a collaborative workshop setting. By taking this course as a practical example, this essay tackles three important pillars in the humanities. Firstly, the overall concept of literature, and more specifically, the literary; secondly, what we understand by literary studies at the university; and thirdly, and more broadly, what constitutes cultural (beyond technical) literacy in the twenty–first century. This essay’s final claim is that teaching e-it as DH effectively address all three. ",Teaching Electronic Literature as Digital Humanities: A Proposal,,2017,"['digital pedagogy', 'electronic literature', 'literary studies', 'foreign languages']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2208,,"{'string_id': None, 'id_scheme': None}"," Digital Humanities (DH) methods incorporated into traditional (non-DH) humanities classrooms present a fruitful opportunity to help undergraduate students learn digital literacy skills as well as new ways of studying the humanities. In light of the trend of increasing numbers of women entering higher education and choosing humanities and arts degrees, DH can also help women who potentially face gender biases related to digital technology gain competence and confidence with it through their humanities courses. Having more students introduced to DH as a regular part of study may increase diversity in the DH community when they themselves become teachers and researchers. Barriers exist, from reluctance to change to a rising contingent labor force. Therefore, this article offers a selection of accessible DH methods that can be used to positively shape humanities pedagogy. ",A Long-Belated Welcome: Accepting Digital Humanities Methods into Non-DH Classrooms,,2017,"['digital pedagogy', 'undergraduate', 'women', 'feminist', 'diversity', 'technology', 'methods']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2209,,"{'string_id': None, 'id_scheme': None}"," The need for organising and digitally processing the vast amount of Cultural Heritage (CH) information has recently led to the development of formal knowledge representation models (ontologies) for the CH domain. Existing models, however, do not capture gender-related concepts. This article presents an effort to fill this gap by developing a new ontology for the representation of gendered concepts in CH resources. The new ontology, named GenderedCHContents resulted from combined research in women’s studies, gender theory, and computer science. Its primary aim is to draw attention to the presence of women within CH artefacts. The proposed ontology extends the Europeana Data Model (EDM) with twenty-two new classes, sixteen object properties and seven datatype properties. The article presents a demonstration of the GenderedCHContents ontology’s use in five different representation tasks, which describe five resources related to Pandora’s myth. Lastly, the study stresses the benefits of reasoning support (i.e. enabling computers to infer further information from a set of asserted facts) in revealing different gender ideals and inferred relationships between metaphorical concepts, along with the benefits of the Semantic Web in making information about gendered contents more easily retrievable to the users. ",An Ontology for Gendered Content Representation of Cultural Heritage Artefacts,,2017,"['ontology', 'cultural heritage', 'semantic web', 'gender theory']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2210,,"{'string_id': None, 'id_scheme': None}"," Digital technologies offer opportunities for engagement with cultural heritage resources through the development of online platforms and databases. However, questions have been raised about whether this type of engagement is structurally open or bounded by pre-existing institutional frameworks. Michel Foucault’s later work on governmentality speaks to this concern and identifies in modes of government the mutually reinforcing relation of all and each, to develop those elements constitutive of individuals’ lives in such a way that their development also fosters that of the strength of the state . This article takes Foucault’s insight as a point of departure for thinking about how digital technologies are mediating and structuring the relationships between individuals and organizations, using the European Commission-funded Europeana project as a case study. Europeana is the embodiment of all and each as a technique of government: it functions by fostering the contributions of individuals and national audiences in a way that celebrates their diversity, while also engaging in a project to systematically standardize and unify. Examination of the technical elements of Europeana reveals the political imperatives implicit in its technical operations, and how the parameters for audience participation are subsequently defined. In this article, we examine the audiences explicitly and implicitly delimited by Europeana, and then analyze them in relation to the project’s development of the European Data Model (EDM) for the interchange of metadata about cultural heritage objects. The article concludes that a lack of explicit definitions about audiences, what Europeana is, and how its various parts work in concert constitute a definitional void. This void is a technique of government in that it absorbs difference and is deliberately vague. It involves power relations that are hard to center and render visible, and it is thus difficult to detect which actors are occupying a space of privilege. We suggest some tentative strategies for addressing this problem by attending to the sites of awkward engagement and difference that are currently masked in the technical framing of Europeana. ",All and Each: A Socio-Technical Review of the Europeana Project,,2017,"['digital archives', 'heritage studies', 'governmentality']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2211,,"{'string_id': None, 'id_scheme': None}", This paper offers a case study of two contrasting digital scholarship internships at The Pennsylvania State University. We explore the benefits and drawbacks of the internship model as an approach to developing digital scholarship among undergraduates through detailing the challenges and particularities of these experiences and analyzing mentor reflection and student feedback. We conclude with a number of recommendations on best practices for teaching digital scholarship through an internship model and aim to provide a useful roadmap for institutions looking to follow a similar model for undergraduate education in this field. ,A Tale of Two Internships: Developing Digital Skills through Engaged Scholarship,,2017,"['DH Internships', 'practice-based pedagogy']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2212,,"{'string_id': None, 'id_scheme': None}"," This case study outlines how writing for Wikipedia can benefit students in an undergraduate literature class by having them undertake scholarly research, read unmediated texts, and write for a real-life audience. In keeping with the collaborative spirit of Wikipedia, the rubrics provided here were primarily created by the class as a whole. Estill demonstrates how this assignment encouraged students to question received notions of literary canon and to engage critically with Wikipedia itself. Perhaps paradoxically, writing for Wikipedia gave students ownership of their writing and research. Ultimately, this assignment facilitated students to become experts on understudied topics and helped them learn about how to do literary research. ",Collaborative Knowledge Creation and Student-Led Assignment Design: Wikipedia in the University Literature Class,,2017,"['digital pedagogy', 'undergraduate', 'wikipedia', 'wiki']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2213,,"{'string_id': None, 'id_scheme': None}"," This paper traces the little-known history of the London Stage Information Bank, a digital initiative that ran from 1970 to 1978 under the direction of Professor Ben R. Schneider, Jr. at Lawrence University. With support from the National Endowment for the Humanities, the American Council of Learned Societies, and the Mellon Foundation, Schneider’s team produced a database from the multi-volume reference work The London Stage 1660-1800 (Southern Illinois University Press, 1960-68). Today, however, most of the project’s outputs are lost or damaged, and its history has been largely forgotten in both theater studies and eighteenth-century studies. This essay traces the history of the Information Bank and my efforts to recover its damaged data and code, offering the project as an object lesson in questions of access, preservation, and institutional memory that digital humanities practitioners continue to confront in 2017. I argue that the project faded into obscurity, not only because of technological obsolescence, but also because the development team was unable to promote the kinds of research questions and behaviors that would enable their tool's widespread adoption and survival. The indifference of literary and theater scholars to the Information Bank throughout the late 1970s and early 1980s demonstrates how vital it is that digital and computational humanities work articulate its meaningfulness within existing intellectual and disciplinary traditions. While digital scholars build new avenues for inquiry that expand and transform humanities research, the survival of these approaches depends on their relationship to current humanities questions, methods, commitments, and epistemologies. ",Recovering the London Stage Information Bank: Lessons from an Early Humanities Computing Project,,2017,"['databases', 'media archaeology', 'digital preservation', 'theater studies', 'history of humanities computing']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2214,,"{'string_id': None, 'id_scheme': None}"," Collaboration is an ethically charged relationship that challenges traditional modes of authorship attribution, institutional norms, and expectations of teaching and learning. A careful reflection on the needs and expectations of project participants demands an exposure of the seams and social dynamics inherent in research-driven relationships. In this paper, we ask: How does integrating digital scholarship into undergraduate pedagogy challenge systems of evaluation and credit and affect collaboration in research environments tuned to promotion and tenure? Emerging from our participation in the Scholarly Communication Institute (2015) in the Research Triangle of North Carolina, this article presents the findings of a team tasked with evaluating best practices and better understanding how authorship and contributorship models emerge in heterogeneous teams of students, faculty, staff, #alt-ac roles, librarians, programmers, and community partners.We have used the Taxonomy of Digital Research Activities in the Humanities (TaDiRAH) to describe our contributions to this article. Please find the full reference of activities here: https://github.com/dhtaxonomy/TaDiRAH/blob/master/reference/activities.md This taxonomy was brought to our collective attention at the 2015 Scholarly Communication Institute by the Modeling contributorship with TaDiRAH team, comprised of Cassidy Sugimoto, J. Britt Holbrook, Korey Jackson, Zach Coble, April Hathcock, and Micah Vandegrift. ",Towards a Seamful Design of Networked Knowledge: Practical Pedagogies in Collaborative Teams,,2017,"['digital pedagogy', 'collaboration', 'ethics']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2215,http://projects.chass.utoronto.ca/chwp/CHC2007/Dyck_Williams/Dyck_Williams.htm,"{'string_id': None, 'id_scheme': None}","This paper describes an intermediate point in the development of an electronic edition of a unique and complex early modern book. At this stage, we the authors have done extensive research on the book itself and have also begun thinking about appropriate ways of rendering information about the book electronically. This essay describes the book, a gospel harmony or 'concordance' made for King Charles I in the mid-1630s by the Ferrars of Little Gidding, England, particularly in light of its construction and intended functions. It goes on to explore the appropriateness of electronic forms for describing the book, ending by postulating possible tools for delivery. In particular, it describes the complexity of the Little Gidding concordance, which rearranges and combines the text of the four canonical gospels into a single narrative of 150 chapters. We have found, somewhat paradoxically, that XML provides a natural-feeling platform for gathering data about this document (contrasted with a word processor), but that the document's complexity makes it a difficult fit with TEI markup. We note that since the biblical text is already marked up, and that the Ferrars have added another layer of markup, the real difficulty as well as the promise of this project is in using digital markup to further enable the complex and powerful reading machine that is the Little Gidding concordance.",Toward an Electronic Edition of an Early Modern Assembled Book,,2008,"['Little Gidding concordance, Ferrar family, markup']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2216,,"{'string_id': None, 'id_scheme': None}"," This paper describes a digital teaching application that approaches the study of language and literary works from a social semiotic perspective and represents an innovative pedagogical model for world language and literature classes. The Golden Age Literature Glossary Online, known by the acronym GALGO, consists of an online glossary of select keywords, from canonical texts of Golden Age Spanish literature, whose multiple connotations illuminate important linguistic and social concepts of the 16th and 17th centuries. GALGO incorporates British cultural historian Raymond Williams’ methodology in his Keywords: A Vocabulary of Culture and Society: namely, identifying problem-laden words or keywords, charting their distinct usages across texts, and reflecting critically on clusters of associated words. GALGO seeks to instantiate language as social semiotic by linking the semantic configurations of a literary work simultaneously to the cultural environment, the linguistic system, and the social system. Applying the conceptual design of M.A.K. Halliday’s social semiotic model, GALGO’s interpretive apparatus provides the field of discourse or context of situation for the text in which a specific keyword appears. The field of discourse presents clusters, word groupings of semiotic affinity that describe the social action that is taking place in the text. GALGO also performs an interpretation of the tenor of discourse, highlighting sociological variables connected to class status, gender role and racial category that refine a keyword’s meaning from the perspective of interpersonal relationships. Finally, GALGO adds commentary on discursive structures, such as patterns of grammar, syntactic nuances, and figurative language, that surround the keyword in the text. GALGO’s strength resides in the synchronic connectivity that the system facilitates when identifying the constellation of meanings for any given keyword. From a technical perspective, the system has the ability to efficiently identify the absolute position of all uses of a keyword across multiple texts, so that large texts can be managed and search times minimized for both users and system administrators. GALGO is constructed in such a way that users can not only access existing analysis within the database, but also can assume, when instructed to do so, the role of the system administrator and contribute their own annotations. A team of faculty and student administrators are currently building the database with research previously collected in Spanish Golden Age literature seminars. Providence College students have utilized the social semiotic methodology underlying GALGO for several years now with successful outcomes both in terms of their growth as language majors and critical thinkers. ",A Pedagogy for Computer-Assisted Literary Analysis: Introducing GALGO (Golden Age Literature Glossary Online) ,,2017,"['digital textual analysis', 'keyword research', 'social semiotics', 'Spanish Golden Age literature']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2217,,"{'string_id': None, 'id_scheme': None}"," While the massive and difficult task of finding, documenting, and centralizing collections is certainly of great concern to image archivists, and has been the motivating factor for beginning numerous digital humanities projects, strategies and best practices for archiving challenging or offensive visual objects (images that are non-canonical, violent, and ambiguous) remains under-theorized. Using the Pitt Rivers Museum, the Jim Crow Museum of Racist Memorabilia, Harpweek, the Center for Holocaust and Genocide Studies, and Visual Haggard: The Illustration Archive as case studies, I address the question of how digital image archivists ought to approach the task of curating objects with the potential to cause trauma. I bring together several critical strands–most importantly visual culture, race theory, and archival science–to question how the structure of a digital archive database might best achieve the goals of educating the public, supporting social justice, and enabling the researches of humanities scholars. ",From Disclaimer to Critique: Race and the Digital Image Archivist,,2017,"['digital archive', 'race', 'museum', 'visual culture', 'illustration']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2218,,"{'string_id': None, 'id_scheme': None}"," The formally constrained work of the Oulipo (l’Ouvroir de Littérature Potentielle, loosely translated as Workshop of Potential Literature) lends itself particularly well to digital studies, which was quickly recognized by the members of the group. To facilitate its goal of avoiding chance in its literary production, the group was naturally drawn to the determinism of computers, where true chance is simply impossible. In its early years, therefore, the group used algorithmic procedures as a starting point for various texts and also attempted to program these texts on actual computers, creating some of the first electronic literature and embarking on proto-digital humanities work as early as the 1960s and 1970s, later abandoning these efforts and relegating all subsequent activity to a subsidiary group. To understand the Oulipo's forays into computer science and more importantly, why they abandoned them, I designed and carried out one of the inaugural projects of the Princeton Center for Digital Humanities. The goal was twofold: first, through exploratory programming, I intended to create interactive, digital annexes to accompany my doctoral dissertation; more importantly, I hoped that by attempting to reproduce the Oulipo's own algorithmic efforts, I would gain similar insights into the nature of Potential Literature and be able to understand why the group abandoned such efforts after the 1970s. This article describes the content, development, and results of my project. For each of my three Python-based annexes, I offer a historical survey of the Oulipian text or procedure discussed within and the Oulipo’s own proto-digital humanities experiments; then, I will talk about my own experiences as a coder-researcher, what learning Python has brought to my project, and how my exploratory programming offered me a new kind of critical reflection. Establishing these annexes forced me to learn to code, a type of work that does not only produce digital texts, but also helped me to reflect on the notion of chance in a more nuanced way. Finally, coding has allowed me to better understand the Oulipian mentality concerning this sort of digital experimentation. ",Digital Oulipo: Programming Potential Literature,,2017,,Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2219,,"{'string_id': None, 'id_scheme': None}"," This paper demonstrates the use of a user-centred design approach for the development of generous interfaces/rich prospect browsers for an online cultural heritage collection, determining its primary user groups and designing different browsing tools to cater to their specific needs. We set out to solve a set of problems faced by many online cultural heritage collections. These problems are lack of accessibility, limited functionalities to explore the collection through browsing, and risk of less known content being overlooked. The object of our study is the Dutch Folktale Database, an online collection of tens of thousands of folktales from the Netherlands. Although this collection was designed as a research commodity for folktale experts, its primary user group consists of casual users from the general public. We present the new interfaces we developed to facilitate browsing and exploration of the collection by both folktale experts and casual users. We focus on the user-centred design approach we adopted to develop interfaces that would fit the users' needs and preferences. ",Supporting the Exploration of Online Cultural Heritage Collections: The Case of the Dutch Folktale Database,,2018,"['Cultural heritage', 'Exploration', 'Rich interfaces']",Digital Humanities Quarterly,011,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2220,,"{'string_id': None, 'id_scheme': None}"," This paper introduces MERCKX, a Multilingual Entity/Resource Combiner & Knowledge eXtractor. A case study involving the semantic enrichment of a multilingual archive is presented with the aim of assessing the relevance of natural language processing techniques such as named-entity recognition and entity linking for cultural heritage material. In order to improve the indexing of historical collections, we map entities to the Linked Open Data cloud using a language-independent method. Our evaluation shows that MERCKX outperforms similar tools on the task of place disambiguation and linking, achieving over 80% precision despite lower recall scores. These results are encouraging for small and medium-size cultural institutions since they demonstrate that semantic enrichment can be achieved with limited resources. ",Semantic Enrichment of a Multilingual Archive with Linked Open Data,,2018,"['semantic web', 'linked open data', 'natural language processing']",Digital Humanities Quarterly,011,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2221,,"{'string_id': None, 'id_scheme': None}"," This case study describes my experience implementing a digital writing assignment in a traditional undergraduate literature classroom at Fairfield University while in a pedagogical partnership with The Map of Early Modern London, an award-winning, peer-reviewed digital humanities mapping project housed at the University of Victoria. I argue that crowdsourcing opportunities can offer a way for faculty at small liberal arts colleges and universities to increase digital literacy among their students. I suggest that such assignments be framed with supporting undergraduate coursework. I then offer a series of preparatory steps and suggestions on how to modify an existing course in ways that meet student learning outcomes pertaining to digital literacy. ",Getting on the Map: A Case Study in Digital Pedagogy and Undergraduate Crowdsourcing,,2017,"['digital crowdsourcing', 'undergraduate research', 'small universities', 'The Map of Early Modern London']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2222,,"{'string_id': None, 'id_scheme': None}"," This study builds upon previous research that discusses gender and sexual identities and digital narratives by introducing a queer narratological approach to character creation mechanics. First, Lisa Nakamura’s identity tourism and narratological constructs are applied to formalize the concept of the exploration in digital narratives. Second, exploration of gender and sexual identities is demonstrated through a queer narratological analysis of two digital narratives, Always Sometimes Monsters (2014) and Hustle Cat (2016). Third, the development of character creation mechanics in The Sims (2000-2016) series is examined to reflect the advancement toward progressive game designs. Concerns regarding a sexuality blind approach and the downplaying of homophobia are addressed, and Helene Cixous’s poststructuralist other bisexuality as a transgressive product of the fluidity of identities in digital narratives is emphasized. This study elaborates the often-disregarded workings of queer narratology and theory in digital narratives or game designs. ",Playing with Identities: Queering Digital Narratology and the Exploration of Gender and Sexual Identities,,2017,"['game studies', 'gender studies', 'queer theory', 'narrative studies']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2223,,"{'string_id': None, 'id_scheme': None}"," Interest in the role of book cover images in readers’ reading experience and book marketing has been long-standing. This study attempts to compare book covers from different countries with a media visualization tool called ImagePlot. The top 100 bestselling books from 13 Amazon.com’s international sites were identified and their cover images were downloaded. Using ImagePlot, median values of brightness, hue and saturation for each image in the data set were extracted and analyzed. Along with one-way ANOVA tests and the resulting graphs from SPSS, ImagePlot outputs show differences in these graphical properties of bestsellers’ cover images in different countries. From the outputs, with all the book cover images displayed on a single canvas (screen), hidden patterns emerged and findings were clearly confirmed. This study makes a contribution by providing connection between research interests in book cover images and media visualization techniques for further research. ",Media Visualization of Book Cover Images: Exploring Differences among Bestsellers in Different Countries,,2017,"['Media visualization', 'visualization of image collection', 'book cover images', 'cultural analytics', 'digital humanities']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2224,,"{'string_id': None, 'id_scheme': None}"," Named Entity Recognition (NER), search, classification and tagging of names and name-like informational elements in texts, has become a standard information extraction procedure for textual data. NER has been applied to many types of texts and different types of entities: newspapers, fiction, historical records, persons, locations, chemical compounds, protein families, animals etc. In general, the performance of a NER system is genre- and domain-dependent and also used entity categories vary . The most general set of named entities is usually some version of a tripartite categorization of locations, persons, and organizations. In this paper we report trials and evaluation of NER with data from a digitized Finnish historical newspaper collection (Digi). Experiments, results, and discussion of this research serve development of the web collection of historical Finnish newspapers. Digi collection contains 1,960,921 pages of newspaper material from 1771–1910 in both Finnish and Swedish. We use only material of Finnish documents in our evaluation. The OCRed newspaper collection has lots of OCR errors; its estimated word level correctness is about 70–75 % . Our principal NE tagger is a rule-based tagger of Finnish, FiNER, provided by the FIN-CLARIN consortium. We also show results of limited category semantic tagging with tools of the Semantic Computing Research Group (SeCo) of the Aalto University. Three other tools are also evaluated briefly. This paper reports the first large scale results of NER in a historical Finnish OCRed newspaper collection. Results of this research supplement NER results of other languages with similar noisy data. As the results are also achieved with a small and morphologically rich language, they illuminate the relatively well-researched area of Named Entity Recognition from a new perspective. ",Old Content and Modern Tools – Searching Named Entities in a Finnish OCRed Historical Newspaper Collection 1771–1910,,2017,"['named entity recognition', 'historical newspaper collections', 'Finnish']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2225,,"{'string_id': None, 'id_scheme': None}"," There has been much discussion about digital humanities (DH) both as a discipline and as a community of practice. This paper is based on my talk given to open the Leipzig eHumanities Seminar series and is presented here with many thanks to the organisers for their kind and generous invitation. Thanks also to the DHQ reviewers whose valuable comments have helped me to strengthen this paper. Whatever the balance of opinion, the emergence of digital scholarship in the humanities has undoubtedly had considerable impact on many disciplines; one such discipline is Classics and the study of the ancient world more generally. This article uses the Digital Classicist (DC) as an example of a DH community in a case study which traces its development and growth to examine what might be learned. As a community the DC joins together practitioners interested in the application of innovative digital methods and technology to the study of the ancient world (in its widest sense). How has this come about and perhaps more importantly, how has it been sustained and indeed provided the inspiration for other affiliated communities? What do we understand by a community and the association of individual practitioners separated by distance? It is important that members feel that they are stakeholders, that they have a sense of ownership and derive value from participation and contribution. It is argued here that a community could be seen as a symbolic and intellectual construct, one of perception rather than physicality to facilitate the exchange of ideas and so effect growth and strengthen the discipline. ",The Digital Classicist: building a Digital Humanities Community,,2017,"['Digital Classicist', 'communities', 'virtual community', 'Digital Humanities', 'collaboration', 'open', 'networking', 'networks']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2226,http://projects.chass.utoronto.ca/chwp/Casta02/Sinclair_casta02.htm,"{'string_id': None, 'id_scheme': None}","There is a demonstrated need for literary text analysis tools that take advantage of networked resources and the potential graphical interfaces. Despite several initiatives over the years, there has been little success in developing text analysis tools collaboratively or in creating an interoperable framework for tools development. This article presents initial work towards a Text Analysis Markup Language (TAML) that would foster the distributed development of literary text analysis tools. Any standardization of a vocabulary requires difficult choices, but it also requires a potentially beneficial examination of the needs and practices of a community. TAML is both a technical specification and a product of sociological introspection",Toward Next Generation Text Analysis Tools: The Text Analysis Markup Language (TAML),,2005,"['Text analysis tools, interoperability, markup languages']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2227,http://projects.chass.utoronto.ca/chwp/CHC2003/Dubrule2.htm,"{'string_id': None, 'id_scheme': None}","It is commonly thought that courses requiring discussion must be taught in classrooms. This paper describes a design for a scholastic course taught entirely on the world wide web that has at its core customized online discussion forums. The design has been implemented in two Carleton University Philosophy courses, Computer Ethics and Information Ethics, open to qualified students across North America. Evaluations by students and of students indicate that the courses have not only met, but exceeded the learning goals of a corresponding classroom course. The success of the model implies that many courses hitherto assumed to require classrooms can be taught online as distance education courses.",Teaching Scholastic Courses Requiring Discussion On Line,,2003,"['Discussion boards, philosophy, pedagogy, technology, information ethics, computer ethics']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2228,,"{'string_id': None, 'id_scheme': None}"," Interdisciplinary collaboration brings the benefit of multiple perspectives to a research project, yet it also provides opportunities for reflection on each discipline’s knowledge base. This article presents a case study in interdisciplinary collaboration between two disparate fields, web development and anthropology. We explore the challenges of translating between domains with differing values, aims and methodologies, as well as issues that arose for us during the development of a web application designed to provide a digital output of an ethnographic project. We consider our experience using the Agile style of software development, which emphasises rapid prototyping, iteration and even failure. In the long run, we find negative experiences in web development can be more valuable than the positive ones. The concept of ‘knowledge brokerage’ is a useful term to describe the collaboration between the academics – who were forced to conceptualise their data in new ways – and the developer – who negotiated these transitions between abstract information and binary data, and between academia and a public-facing web application. ",Interdisciplinary Collaboration and Brokerage in the Digital Humanities,,2017,"['interdisciplinary collaboration', 'digital humanities', 'sound studies', 'web design', 'research methodology']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2229,,"{'string_id': None, 'id_scheme': None}"," The paper proposes to bridge two areas of inquiry, digital hermeneutics and metaphor within a digital environment, by the analysis of a less studied phenomenon, i.e. how interpretation is supported and shaped by metaphors embedded in an interface. The study is articulated around three use cases for literary, didactic and historical representations of imaginary and existing cities based on a model (z-text) and interface (Z-editor) for zoomable texts. We will try to demonstrate that the zooming and contextualization features of the tool allow creating layers of meaning that can assist interpretation and critical readings of literature and history. ","Metaphors in Digital Hermeneutics: Zooming through Literary, Didactic and Historical Representations of Imaginary and Existing Cities",,2017,"['digital hermeneutics', 'metaphor', 'zooming', 'z-text', 'cities representations']",Digital Humanities Quarterly,011,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2230,,"{'string_id': None, 'id_scheme': None}"," This article analyses advertisements to shed light on the ways in which the Coca-Cola Company tried to shape the Dutch perception of an American way of life, and by extension provided the discursive building blocks for the construction of a mental map of America. Since the National Library of the Netherlands (KB) digitized its newspaper collection, we could analyze newspapers using both computational and traditional means. The central question this article addresses is: Did Coca-Cola advertisements in Dutch newspapers communicate themes that represented the American way of life? Using two separate workflows, we demonstrate how we combined computational and traditional methods in an iterative and transparent manner. These workflows are systematic descriptions of how we used computational tools to answer this paper’s central research question. ",Coca-Cola: An Icon of the American Way of Life. An Iterative Text Mining Workflow for Analyzing Advertisements in Dutch Twentieth-Century Newspapers,,2018,"['text analysis', 'cultural history', 'americanization']",Digital Humanities Quarterly,011,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2231,,"{'string_id': None, 'id_scheme': None}"," This paper analyzes the design space for crafting systems within games. A crafting system is the collection of game mechanics which enable a player to create virtual objects within a game, with examples ranging from making a pickaxe or placing blocks in Minecraft, to combining ingredients to create items in World of Warcraft, or designing a city in SimCity. Within both game design and game journalism there is no systematic understanding of what crafting means in games — rather, the term is used in an informal I’ll know it when I see it way, with different ideas of what constitutes crafting becoming conventional for different game genres. We aim to create a more systematic analysis and language for virtual crafting, using a design space approach where we identify seven features of crafting systems; any given crafting system is understood in terms of the degree and manner in which it exemplifies these features. The features we identify are: recipe definition (how well-defined are the player steps for creating a virtual object), fidelity of action (how detailed is the player-performed enactment of crafting), completion constraints (how is creation constrained by virtual resources), variable outcome (how much can the result vary when the player performs the same crafting actions), system recognition of outcome (to what degree do other game systems understand and take account of what a player has made), player expressiveness (how big is the creative space provided to the player), and progression (how do the possibilities of crafting change over time). We apply these dimensions to the analysis of 64 crafting systems across 47 example games. This more systematic analysis of the crafting system design space provides a deeper understanding of how games function as a creative and artistic medium for players, and provides a language and framework enabling scholars, critics and developers to more deeply understand existing crafting systems as well as unexplored opportunities for future crafting systems. ",Crafting in Games,,2017,"['game studies', 'crafting', 'video games']",Digital Humanities Quarterly,011,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2232,,"{'string_id': None, 'id_scheme': None}"," The past twenty years have witnessed a mounting crisis in academic publishing. Companies such as Reed-Elsevier, Wiley-Blackwell, and Taylor and Francis have earned unprecedented profits by controlling more and more scholarly output while increasing subscription rates to academic journals. Thus publishers have consolidated their influence despite widespread hopes that digital platforms would disperse control over knowledge production. Open access initiatives dating back to the mid-1990s evidence a religious zeal for overcoming corporate interests in academic publishing, with key advocates branding their efforts as archivangelism. Little attention has been given to the legacy or implications of religious rhetoric in open access debates despite its increasing pitch in recent years. This essay shows how the Protestant imaginary reconciles–rather than opposes–open access initiatives with market economics by tracing the rhetoric of openness to free-market liberalism. Working against the tendency to accept the Reformation as an analogy for the relationship between knowledge production, publishers, and academics, we read Protestantism as a counterproductive element of the archivangelist inheritance. ",Open Access and the Theological Imagination,,2017,"['open access', 'open source', 'scholarly communications', 'digital publishing', 'religion', 'rhetoric']",Digital Humanities Quarterly,011,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2233,,"{'string_id': None, 'id_scheme': None}"," In the emerging field of digital style analysis, scholars have been dwelling on a perceived irreconcilability of distant reading and close reading . This situation is somewhat reminiscent of the paradigm wars in social science, where for years, one could either adhere to a quantitative or a qualitative mindset. Taking seriously both the hermeneutic and the empirical traditions of (digital) stylistic research, the present paper proposes a mixed-methods coalition of approaches. To establish links between the mixed-methods paradigm from Social Sciences and stylistic practices within Digital Humanities (DH), the present article discusses common methods of distant, close, and ‘scalable reading’ as well as a flexibly adjustable definition of style . In the practical part of the paper, I report a ‘mixed-methods digital stylistics’ study on Franz Kafka’s prose. Scaling the degree of abstraction and contextuality of the data according to particular research questions, I combine (1) quantitative hypothesis testing (examining Kafka’s stylistic uniqueness by means of a stylometric measure); (2) quantitative exploration (analyzing the first hundred statistically overrepresented words in Kafka); and (3) qualitative text analysis (KWIC and close reading to investigate the functions of a particular style marker in the context of Kafka’s The Judgment [Das Urteil]). Generally, for digital stylistics, I propose (a) raising epistemological and methodological awareness within the field, and (b) framing the research within the mixed-method paradigm that in fact seems very well suited to DH. ",In a test bed with Kafka. Introducing a mixed-method approach to digital stylistics,,2018,"['text analysis', 'digital stylistics', 'stylometry', 'scalable reading', 'distant reading', 'close reading', 'empirical method', 'digital hermeneutics', 'DH', 'epistemology', 'methodology', 'mixed methods', '20th Century German literature', 'style']",Digital Humanities Quarterly,011,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2234,,"{'string_id': None, 'id_scheme': None}"," We investigate what distinguishes reported dreams from other personal narratives. The continuity hypothesis, stemming from psychological dream analysis work, states that most dreams refer to a person’s daily life and personal concerns, similar to other personal narratives such as diary entries. Differences between the two texts may reveal the linguistic markers of dream text, which could be the basis for new dream analysis work and for the automatic detection of dream descriptions. We used three text analytics methods: text classification, topic modeling, and text coherence analysis, and applied these methods to a balanced set of texts representing dreams, diary entries, and other personal stories. We observed that dream texts could be distinguished from other personal narratives nearly perfectly, mostly based on the presence of uncertainty markers and descriptions of scenes. Important markers for non-dream narratives are specific time expressions. Dream texts also exhibit a lower discourse coherence than other personal narratives. ",Unraveling reported dreams with text analytics,,2018,"['automatic text analysis', 'dream reports']",Digital Humanities Quarterly,011,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2235,,"{'string_id': None, 'id_scheme': None}"," This article presents a new model for emotion mining, resulting from the research project Embodied Emotions. This project aims: 1. to trace historical changes in emotion expression and in the embodiment of emotions, and 2. to develop methods to trace these changes in sizeable corpuses of digitized texts. Up to now, the mining of sentiments or emotions has mainly been restricted to modern, or even very recent text corpora, such as social media. Sentiment mining techniques are often based on relatively simple emotion models of binary (positive/negative) oppositions, or limited sets of ‘basic’ emotions and are not yet able to deal adequately with the complexity and the historical contingency of emotions and their expression. To meet these challenges, we have developed the Historic Embodied Emotion Model (HEEM), built on a test case of 29 Dutch language theatre plays written between 1600 and 1800 and annotated manually with HEEM labels for emotions and body terms. In this article, we present this model and compare it with other sentiment mining techniques: 1) off the shelf linguistic analysis software LIWC (Linguistic Inquiry and Word Count), 2) a version of LIWC that has been adapted for the analysis of Dutch historical texts, 3) a new Dutch translation of WordNetAffect. We conclude that, although different forms of sentiment mining have their value and use, HEEM provides new opportunities for emotion mining and analysis of embodied emotions in historical texts. ","Mining Embodied Emotions: A Comparative Analysis of Sentiment and Emotion in Dutch Texts, 1600-1800.",,2018,"['Sentiment mining', 'emotion mining', 'NPL', 'History of Emotions', 'Digital Humanities', 'Emotion Classification']",Digital Humanities Quarterly,011,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2236,,"{'string_id': None, 'id_scheme': None}"," This paper discusses a digital humanities collection called Contemporary Telepathics housed within a larger archive, The Fabric of Digital Life. We argue that through a variety of multimedia representations, the rhetoric of contemporary telepathic devices invokes sublime predictive visions that become instantiated in popular media, news organizations, and disclosures of technology advancement. We discuss the theoretical grounds for a collection of technological forecasts, at a time when they are largely considered ephemera. We also discuss how tracking the telepathic sublime as a dynamically evolving popular communication phenomenon requires a unique, humanities-based metadata scheme and archival practice. ",Tracking the telepathic sublime as a phenomenon in a digital humanities archive,,2017,"['digital humanities archiving', 'archival practice', 'wearable technology', 'rhetoric', 'technoculture', 'digital futures']",Digital Humanities Quarterly,011,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2237,,"{'string_id': None, 'id_scheme': None}"," Over the last few years, the Perseus Digital Library (PDL) and the Open Philology Project (OPP) have been moving towards enabling better interoperability and citability of their texts by implementing the Canonical Text Services URN standard and the Epidoc subset of the TEI P5 guidelines. This is a resource-intensive effort necessitating a scalable workflow centered on continuous curation of these texts, from both within and outside the PDL/OPP ecosystem. Key requirements for such a workflow are ease of maintenance and speed of deployment of texts for use by a wide variety of analytical services and user interfaces. Drawing on software engineering best practices, we have designed an architecture meant for continuous integration with customizable services that test individual files upon each contribution made to our public git repositories. The services can be configured to test and report status on a variety of checkpoints from schema compliance to CTS-ready markup designed for flexibility and interoperability. ",Continuous Integration and Unit Testing of Digital Editions,,2018,"['continuous integration', 'unit test', 'test', 'edition', 'repository', 'open source', 'object oriented programing', 'curation', 'tei', 'epidoc', 'Open Philology', 'Capitains']",Digital Humanities Quarterly,011,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2238,http://projects.chass.utoronto.ca/chwp/CHC2007/Cunningham/Cunningham.htm,"{'string_id': None, 'id_scheme': None}","“dis-Covering the Early Modern Book” is a description of an experiment conducted during a single day spent in Ray Siemens' Electronic Textual Cultures Laboratory at the University of Victoria. The purpose of the experiment was primarily to find out what kind of digital artefact could be generated from an early modern book. Secondarily, we wanted to contemplate potential use for such an artefact, which subsequently was clearly established as teaching bibliography, or book or print culture. The resulting paper started life as a wiki contributed to by three of the members of the group who conducted the experiment. It was then carried further by the present author. It continues to evolve, and two members of the group plan to publish a longer version of this essay in the near future.",Dis-Covering the Early Modern Book: An Experiment in Humanities Computing,,2008,"['Book history, bibliography, print culture, digitization, education, digital artefact, born digital, Gutenberg, electronic text, Electronic Textual Cultures Laboratory, bibliosection, SDH/SEMI, virtual lightbox']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2239,,"{'string_id': None, 'id_scheme': None}"," Collaboration has become a hallmark of Digital Humanities (DH) research. Nonetheless it remains under-discussed and for those not deeply engaged in DH a bit of a mystery. Drawing on recent DH work and publications that engage with questions of DH collaboration in different ways (e.g. ), we analyse three types of DH collaboration: 1) human-human interactions; 2) human-machine/material interactions; and 3) machine/material-machine/material interactions. We argue that engagement with collaboration processes and practices enables us to think through how DH tools and practices reinforce, resist, shape, and encode material realities which both pre-exist, and are co-produced by them. We suggest that understanding these entanglements facilitates a critical DH in which academic hierarchies and disciplinary preconceptions are challenged. ",Collaboration in Digital Humanities Research – Persisting Silences,,2018,"['collaboration in DH', 'human-human DH interaction', 'human-machine interaction']",Digital Humanities Quarterly,012,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2240,,"{'string_id': None, 'id_scheme': None}"," Discussions of digital humanities pedagogy have often focused on discussions of scaffolding and play (alternatively, tinkering) approaches, and methods for assessing student work appropriate to both techniques. While these approaches may seem oppositional, we emphasize the need to balance them in most classroom contexts, and explore challenges with integrating new digital humanities platforms in topically-driven humanities classrooms. We examine our experiences as an instructor and a librarian partnering to include a multimedia publishing assignment sequence in a course on the history of children’s literature, and our assessment of our approach to instruction in a distance graduate education context that is not amenable to the lab-based learning usually used in person for DH learning. While discussions of assessment of DH classroom projects generally focus on the question of how to grade student work, we argue that assessment is as important for reflecting on and evaluating pedagogy, including how to balance and iteratively improve scaffolding and play approaches. These issues are important not just for experienced DH instructors, but also humanities instructors without a DH background beginning to integrate digital assignments as a new norm. ",Scaffolding and Play Approaches to Digital Humanities Pedagogy: Assessment and Iteration in Topically-Driven Courses,,2018,"['digital pedagogy', 'multimodal publishing']",Digital Humanities Quarterly,011,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2241,,"{'string_id': None, 'id_scheme': None}"," This study used three interdependent techniques to help understand the use and distribution of syllabic values of the cuneiform signs during the second half of the third millennium and early second millennium BCE. The results suggest that, during this period, cuneiform syllabaries were variable. That variation can further inform us about the regional, temporal, and dialectical contexts in which they existed. The addition of this research to the wider literature on the early adaptation of cuneiform enhances the field's understanding of how cuneiform syllabic values began to emerge and spread across the ancient Near East, and demonstrates how computational methods of analysis can be applied to research questions in humanities subjects. ",Computational Models for Analyzing Data Collected from Reconstructed Cuneiform Syllabaries,,2018,"['Cuneiform studies', 'computational methods']",Digital Humanities Quarterly,012,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2242,,"{'string_id': None, 'id_scheme': None}"," Thomas Paine's Rights of Man (Part I, 1791; Part II, 1792) is one of the most influential political tracts ever written. Recently, Clark (2015) has queried the long-established notion that Part I is the work of a single author, Paine himself. Clark argues that a passage of approximately 6000 words just prior to the Declaration of the Rights of Man and Citizens, near midway through the book, was written by Gilbert du Motier, Marquis de Lafayette, and inserted by Paine into his book, perhaps with some editing. Clark's assertion rests mainly on judgement of the tone and content of the queried passage, as well as a hint in a letter by Lafayette to George Washington. This paper presents the results of a stylometric study of this question. Three computational approaches to stylistic analysis — one involving a consensus of six attribution techniques — were applied to the text of the Rights of Man, Part I, along with a corpus of comparison texts from the same period. Our findings tend to corroborate Clark's contention, as well as identifying two other short passages in which Lafayette may have had a hand, thus rendering this important document ripe for re-interpretation. ",The Writeprints of Man: a Stylometric Study of Lafayette's Hand in Paine's 'Rights of Man',,2018,"['Authorship Attribution', 'Co-Authorship', 'Computational Stylistics', 'Revolutionary Writings', 'Stylometry', 'Text Classification.']",Digital Humanities Quarterly,012,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2243,,"{'string_id': None, 'id_scheme': None}"," The William Blake Archive emphasizes accuracy and objectivity. The photographed or scanned images are made faithful to their original objects in scale, color, and detail. The editors’ notes are largely bibliographic and informative. The illustration descriptions consist of precise observations deliberately void of interpretation. The transcriptions are as diplomatic as the digital medium allows. And the explicit relationships between objects in the Archive are material; one can see, for example, copies of prints from the same relief-etched copper plate, or a design carried from a water color drawing to an engraved illustration. The scholar must be free to make his or her own interpretations. Since beginning to redesign the back end and front end of the site, the editors and staff have realized that more kinds of relationship might be made explicit. The challenge has been to decide which ones conform to our emphasis on accuracy and objectivity. In a neutral presentation of art and its contextual information, we can’t tell the scholar that object X relates to object Y because they embody the same theme. That would impose on him or her our own interpretation. But we can say that X relates to Y because they contain some of the same text. In this paper we discuss the difficult task of digitally presenting more kinds of relationships in the Archive while staying objective. We explain how the task has compelled us to reevaluate Blake’s methods and reencode the connections amongst his works accordingly. We describe planned enhancements to our tools used for representing and displaying these connections--enhancements that would benefit all archives, not just the Blake Archive. And finally we gesture towards further research into the philosophical context of Blake's practice of making related art and into the idea of relationship in archives in general. ",All Relate to Art: The William Blake Archive and Its Web of Relations,,2018,,Digital Humanities Quarterly,012,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2244,,"{'string_id': None, 'id_scheme': None}"," Even if digitized and born-digital audiovisual material today amounts to a steadily increasing body of data to work with and research, such media modalities are still relatively poorly represented in the field of DH. Streaming media is a case in point, and the purpose of this article is to provide some findings from an ongoing audio (and music) research project, that deals with experiments, interventions and the reverse engineering of Spotify’s algorithms, aggregation procedures, and valuation strategies. One such research experiment, the SpotiBot intervention, was set up at Humlab, Umeå University. Via multiple bots running in parallel our idea was to examine if it is possible to provoke — or even undermine — the Spotify business model (based on the so called 30 second royalty rule). Essentially, the experiment resembled a Turing test, where we asked ourselves what happens when — not if — streaming bots approximate human listener behavior in such a way that it becomes impossible to distinguish between a human and a machine. Implemented in the Python programming language, and using a web UI testing frameworks, our so called SpotiBot engine automated the Spotify web client by simulating user interaction within the web interface. The SpotiBot engine was instructed to play a single track repeatedly (both self-produced music and Abba’s “Dancing Queen”), during less and more than 30 seconds, and with a fixed repetition scheme running from 100 to n times (simultaneously with different Spotify Free ‘bot accounts’). Our bots also logged all results. In short, our bots demonstrated the ability (at least sometimes) to continuously play tracks, indicating that the Spotify business model can be tampered with. Using a single virtual machine — hidden behind only one proxy IP — the results of the intervention hence stipulate that it is possible to automatically play tracks for thousands of repetitions that exceeds the royalty rule. ",SpotiBot — Turing Testing Spotify,,2018,"['intervention', 'Spotify', 'streaming music', 'bots', 'Turing test']",Digital Humanities Quarterly,012,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2245,,"{'string_id': None, 'id_scheme': None}"," In the last decade tremendous advances have been made in the tools and platforms available for the digital study of manuscripts. Much work, however, remains to be done in order to address the wide range of pedagogical, cataloging, preservation, scholarly (individual and collaborative), and citizen science (crowdsourcing) workflows and use cases in a user-friendly manner. This study (1) summarizes the feedback of dozens of technologists, manuscript experts, and curators obtained through survey data and workshop focus groups; (2) provides a state of the field report which assesses the current tools available and their limitations; and, (3) outlines principles to help guide future development. The authors in particular emphasize the importance of producing tool-independent data, fostering intellectual trading zones between technologists, scholars, librarians, and curators, utilizing a code base with an active community of users, and re-conceptualizing tool-creation as a collaborative form of humanistic intellectual labor. ",Manuscript Study in Digital Spaces: The State of the Field and New Ways Forward,,2018,"['digital workspaces', 'digital humanities', 'workflows', 'standards', 'data curation', 'pedagogy', 'crowdsourcing', 'data management']",Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2246,,"{'string_id': None, 'id_scheme': None}"," This article sheds an empirical light on interdisciplinary collaboration within the Digital Humanities by investigating the daily research practice of the Dutch Digital Humanities-project BRIDGE. The project developed and tested methods for automatically creating meaningful links and expanding archival television data. In the project, a high level of collaboration was required between scholars from two different disciplines: computer sciences and media studies. The majority of the epistemological encounters between the two disciplines took place in the design of the developed tools and the user studies to test the tools. The article is based on structured conversations between the two central staff members in the project, i.e. the computer science PhD-student and the media studies postdoctoral researcher. By unravelling the research project as a process of confrontation, identification and acknowledgement of situated knowledges, the article shows when and how the boundaries between the two disciplines have been maintained, crossed and blurred. The authors point to the benefits and challenges of interdisciplinary collaboration in the Digital Humanities, and formulate some best practices for future Digital Humanities-projects. ",Building Bridges: Collaboration between Computer Sciences and Media Studies in a Television Archive Project,,2019,,Digital Humanities Quarterly,013,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2247,,"{'string_id': None, 'id_scheme': None}"," This paper examines the potentialities of networked browsing, a form of faceted searching that visualizes digital archives in the form of a force-directed network graph. Using BigDIVA.org as an example of networked browsing, this paper presents the results of a small usability study that compared how participants (N = 8) engaged with BigDIVA’s networked browsing in comparison to use of a search engine such as Google. In doing so, we situate our study within performative conceptualizations of human-computer interfaces in order to explore the potential becomings when human and nonhuman machinic component are entangled together. Based on the observations from our usability study, we argue that networked browsing is suggestive of Whitelaw’s (2015) generous interfacing that emphasizes browsing as a tool for exploring relationships between nodes in archives, as well as Shneiderman’s (1996) joyous experience for interfacing with the web. ",BigDIVA and Networked Browsing: A Case for Generous Interfacing and Joyous Searching,,2018,"['networked browsing', 'digital archives', 'usability', 'interface studies', 'cybernetics']",Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2248,,"{'string_id': None, 'id_scheme': None}"," Digital humanities have a long tradition of using advanced computational techniques and machine learning to aid humanistic enquiry. In this paper, we concentrate on a specific subfield of machine learning called predictive analytics and its use in digital humanities. Predictive analytics has evolved from descriptive analytics, which creates summaries of data, while predictive analytics predicts relationships within the data that also help to explain new data. Predictive analytics uses machine learning techniques but also traditional statistical methods. It uses properties (or features) of the data to predict another target feature in the data. Machine learning is used by predictive analytics to establish the rules that given a certain combination of features make the target more or less likely. Predictive analytics can thus be considered to be a technique to machine-read data. The paper discusses the background of predictive analytics, its use for predicting the past and finally presents a case study in predicting past gender relations in a historical dataset. Predicting the past is introduced as a method to explore relationships in past data. ",Predicting the Past,,2018,,Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2249,http://projects.chass.utoronto.ca/chwp/CHC2004/cunningham/,"{'string_id': None, 'id_scheme': None}","In this paper I will provide some background to the history of books with moving parts before using examples of such books to show that even in the first century of printing with moveable type the need for moving pictures was perceived. Next, I will describe the editorial principles I am using to guide creation of an incunabular electronic edition of one of the examples discussed in this paper, Richard Eden's The Arte of Navigation. The paper concludes with an invitation to follow a link to that electronic edition, with the caveat that it is a work in progress.",Coincidental Technologies: Moving Parts in Early Books and in Early Hypertext,,2009,"['Incunabula, editing, publishing, image, hypertext ']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2250,,"{'string_id': None, 'id_scheme': None}"," This paper explores the art history, the digital aesthetics, the technical anatomy, and the restoration of Shu Lea Cheang’s early web artwork Brandon (1998-1999). Commissioned two decades ago by the Solomon R. Guggenheim Museum, the work is just back on view to the public after a major restoration. It seems timely to consider Cheang’s artwork: a collaboration between the artist and a network of programmers, designers, authors and other contributors that offers a creative platform for remembering and engaging with histories and issues of identity from the LGBTQ+ community. In the spirit of the piece, this article is a multi-disciplinary collaboration between a curator, a media art conservator, a computer scientist, and a scholar of the digital liberal arts, which brings together these different areas of expertise to offer an innovative and multi-faceted consideration of one artwork from a digital humanities perspective. ",Reconstructing Brandon (1998-1999): A Cross-disciplinary Digital Humanities Study of Shu Lea Cheang’s Early Web Artwork,,2018,"['Brandon', 'Shu Lea Cheang', 'time-based media', 'source code analysis', 'computer-based art', 'CCBA', 'web art', 'digital preservation', 'LGBT']",Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2251,,"{'string_id': None, 'id_scheme': None}"," The Jesuit scholar, Roberto Busa, is often called the founder of humanities computing. In fact, starting as early as 1949, he collaborated with IBM to perform experiments using suites of punched-card machines. These punched-card data systems—with their plug-board setups, clacking machinery, and flurries of perforated rectangular cards—were developed for business accounting and tabulating, and adapted for government censuses, defense calculations, archival management, and information processing of all kinds. The first decade of humanities computing can more accurately be described as an era of humanities data processing—in the historically specific and contextually rich sense of the term. This essay describes an ongoing collabroative project that aims to reverse engineer that center in the attempt to understand better this important site in the history of technology and humanities computing. ",Reverse Engineering the First Humanities Computing Center,,2018,"['History of DH', '3D modeling', 'humanities computing', 'data processing', 'reverse engineering', 'media archaeology']",Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2252,,"{'string_id': None, 'id_scheme': None}"," The fate of the medieval Aesop during the early age of print reveals the tensions that arise between corporeal and proprietary understandings of artistic production. Building upon the highly accumulative, various, and expandable Aesopic tradition that thrived in the Middle Ages within scholastic manuscripts, the fifteenth-century fables of Robert Henryson and William Caxton confront movements to consolidate and transform Aesop’s sprawling literary corpus into a singular printable property. At the same time that a single series of Aesop’s fables, now known as the elegiac Romulus, became increasingly standardized within printed books, woodcut illustrations and textual descriptions increasingly beautified Aesop’s body, transforming him from an inarticulate slave to an eloquent aristocrat. The simultaneous metamorphosis of Aesop’s poetic and visual corpus reveals the premodern underpinnings for current efforts by publishing industries to immunize digital work from rogue acts of appropriation, mashup, and remix, practices which had previously defined Aesopic textuality. As a response to this textual vulnerability, the collapse of Aesopic work into a singular entity reimagines the allegorical relationship between the author or publisher and the work as proprietary, not corporeal. Rather than an extension of a generative, deeply somatic, and grotesque process of multiple fabular authors and commentators, the modernized Aesop obtains value as a property, paving the way for the notion that creative corpuses can be owned, effectively stealing away corporeal features from intellectual production. ",Stealing a Corpus: Appropriating Aesop’s Body in the Early Age of Print ,,2018,,Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2253,,"{'string_id': None, 'id_scheme': None}"," This article provides an overview of some of the complex issues involved in reconstructing and visualizing past landscapes. It discusses the importance of empirical data and introduces some of the terminology necessary for understanding methods which are often considered more in the domain of the natural sciences than humanities. Current methods and practices are put in the context of environmental archaeology, archaeological theory and heritage management as well as related, briefly, to the broader context of archaeological theory, practice and research data infrastructure. Finally, some examples and pointers for the future are given in the hope that the article may provide a point of reference for those looking to gain an entry point into the study of past landscapes, and understand their relevance in archaeological visualisation. ","To tree, or not to tree? On the Empirical Basis for Having Past Landscapes to Experience.",,2018,"['archaeology', 'landscape', 'reconstruction', 'visualization', 'environmental', 'empirical data']",Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2254,,"{'string_id': None, 'id_scheme': None}"," The history of human-environment interaction is embedded in stone. Stones are essential components of daily life and their various usage characterize certain areas or chronological periods. The form of a stone object is the result of a long chain of interactions with distinct bodies but the intangible life story of any artefact is partially registered in its original material properties and gradual physical alteration. Digital systems can be adopted for collecting these invisible records and tracing a stone’s history. Chemical imaging and portable spectroscopy are quick and non-destructive remote sensing techniques that can be used to gather empirical data and track production and use of stone artefacts over time. This article reviews the application of Near Infrared Spectroscopy as a method for geochemical characterization of objects and as a tool for provenance studies within the Mobima project, carried out by an interdisciplinary team of archaeologists and chemists at University of Umeå, Sweden. Near Infrared Spectroscopy can be used for acquiring and processing spectral information directly in the field, modelling datasets of big assemblages and classifying objects. Making stones’ biographies visible will help understanding the entanglement of past societies and their geological landscapes. ",Recording invisible proofs to compose stone narratives. Applications of Near Infrared Spectroscopy in provenance studies.,,2018,,Digital Humanities Quarterly,012,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2255,,"{'string_id': None, 'id_scheme': None}"," Over the past decades the advancement of technology and its subsequent introduction to the humanities has led to the development of several applications that enhance study and research in areas such as Classics, archaeology, epigraphy, and linguistics. The focus of digital humanists and other humanities scholars has turned to the efficient storage of information that facilitates search, comparative studies, accessibility, and consequently research. The common point of reference for the aforementioned applications is that they do not involve the need for the scholar to physically be in a virtual space. In areas, such as archaeology and epigraphy, the creation of projects that provide access to 3D models of the artifacts as well as to virtual replications of ancient sites has resulted in the opening of new areas of research and reconsideration of traditional research issues. Also, argue that digital visualizations may make the ancient world more accessible to a larger audience. One may comprehend the structure of a vase and analyze its artistic patterns by zooming and relighting it. A problem that is yet to be considered, though, is how a scholar and a student are to perceive Classical drama, the theatrical space, the distances between the actors, the chorus, and the audience, the logistics of the performance, and the cultural aspects at play. This paper discusses the importance of experiential learning and the use of virtual reality as a means of promoting traditional edification methods, virtually recreating the actuality of the stage, and presents the Magic Mirror Theater, a web application designed to facilitate the study of Classical drama. ",Reviving Classical Drama: virtual reality and experiential learning in a traditional classroom,,2018,,Digital Humanities Quarterly,012,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2256,,"{'string_id': None, 'id_scheme': None}"," With the increasing proliferation of digital technologies into our daily routines, long-standing debates have resurfaced on the potential of prosthetic devices to extend our cognitive capacities and distribute intelligence across human and non-human agents. This article will propose ways in which we, as educators, can harness the distractions that new technologies can pose to teaching and research towards productive models of distributed attention and collective intelligence. By reflecting on the relationship between play and pedagogy through critical theory, as well as on the benefits of hyper-connected and collaborative learning, this article hopes to expand the branch of Digital Humanities that Holly Willis terms as the cinematic humanities. Remixing, recontextualization, and the non-linear/non-sequential reconfiguration of information can provide us with new modes of distributed attention and critical making in the digital age. This article focuses on low and no-budget process-oriented collaborative projects for Cinema, Media Studies, and Digital Humanities courses (adjustable to other subject matters) that incorporate remixing, social and locative media, and augmented reality into experimental modes of connected and collaborative learning. The first assignment is a live film scoring remix, where students collectively brainstormed on picking sound clips/samples to add new soundtracks to early silent films. Remixing practices can offer new insights into cinema's legacy and challenge ocularcentric notions of film spectatorship by, in this case, reversing the conventional hierarchy of image-sound into sound-image and reflecting on how our minds attempt to process the impulse of sensory synchronization in cinema. The second sample assignment proposes new ways of using easy-to-learn and accessible GIS (Geographic Information System) and AR (Augmented Reality) tools to create projects related to civic engagement and local/online activism. The drifts from pre-determined learning outcomes are meant to be active disruptions to any prescribed limitations that delimit what digital humanities should or should not be, and other issues that often undermine the potential contributions to digital humanities that derive from the intellectual and material labor that happens in the networked, expanded and distributed classroom. The remix aesthetic, the unfinished and makeshift nature of these projects is something that I find adds value to this kind of process-oriented pedagogy because it remains in touch with the pre-institutionalized experimental ethos that prompted the formation of the digital humanities and other disciplinary cross-pollinations. ","From Distracted to Distributed Attention: Expanded Learning through Social Media, Augmented Reality, Remixing, and Activist Geocaching",,2018,,Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2257,,"{'string_id': None, 'id_scheme': None}"," Rather than seeing hypertext as offering only distracted reading, this paper argues that annotation software provides students with the opportunity to perform recursive reading: a process that facilitates knowledge transfer and encourages intersectional critical approaches to texts. As hypertext editions and online reading communities have proliferated, scholars have theorized that these layered, interactive modes of reading produce distracted readings of texts, especially in pedagogical contexts. With annotation software, however, researchers and students have the opportunity not just to consume these distracted readings, but to produce them. Within the classroom, annotation software assignments require a deeper psychological investment from students than simply consuming a hypertext edition. By choosing how and where to annotate a particular text, students’ distributed modes of attention are re-focused on the text itself, pulling disparate threads of thought together. ",From Distracted to Recursive Reading: Facilitating Knowledge Transfer through Annotation Software,,2018,,Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2258,,"{'string_id': None, 'id_scheme': None}"," Distraction is not a problem for scholarly reading in the humanities: it is, in fact, intrinsic to how we work. An openness to distraction facilitates knowledge creation in conversation with and between sources, bringing together disparate times and places, authors and forms of source material, frames of reference and layers of insight, harnessing peripheral vision as much as central focus to create the conditions for serendipitous discovery. This loose hold on focus results in a knowledgescape with a substantial tacit dimension, with the result that humanists most certainly know more than we can tell. This is not to say that the epistemic process of the humanist is all encompassing, however: administrative work, for example, is viewed as a true distraction, and managed accordingly. In spite of the wide availability of digital tools to consolidate and organise the process of extracting information from the reading of sources (be they text, image, landscape or indeed even the self), the notetaking process of the humanist remains stubbornly multimodal, harnessing embodied, material, and spatial dimensions of knowledge. The resulting knowledge environments humanists create around themselves resemble life-sized realisations in mixed media of a Prezi stack, a physical manifestation of the fragmented, capricious, haphazard stream of influences jostling for scholarly attention, and the scholar’s understanding that their system inter-relationships are complex, sometimes tenuous, and largely undiscovered. This article will lay out the evidence for and implications of this understanding of humanistic reading, and then explore possible future paths for using technology to explore and indeed celebrate distraction, including: Technology as blurring the line between the personal and the professional, not so much as a labour practice, but as an epistemic one opening the way to the validation of epistemic emotions as a valid research input, but also to the more effective convergence of humanities research with other inputs to the identity formation process of seemingly distracted undergraduates; and research infrastructural implications for libraries (where remote storage and electronic catalogues diminish the likelihood for serendipity, essentially by reducing distraction) but also for vitual research environments founded upon a new understanding of what the ideal humanistic work environment might be and how to get below the level of that work. ",How Scholars Read Now: When the Signal Is the Noise,,2018,,Digital Humanities Quarterly,012,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2259,,"{'string_id': None, 'id_scheme': None}"," Reading has always been a contentious and political practice, but this is heightened in the contemporary moment both because of the way the environments in which we read are changing so radically. For Katherine Hayles reading is a powerful technology for reconfiguring activity patterns in the brain , a view representative of attempts to connect the new neuroscience of reading with age old practices of literary endeavour. For Sven Birkerts, however, the Internet and the novel are opposites , a view that suggests that a hierarchy of reading that locks digital readers out of higher order thinking and literary experience. Meanwhile, Anne Mangen finds that electronic reading environments negatively aﬀect emotional aspects of reading . But these approaches tend to understand reading as something static that occurs in one space or another. However, in practice our reading is increasingly distributed. Reading can occur in multiple formats, across multiple platforms for the one text or reading experience. A novel begun in print can be read online in a born-digital format and concluded in a scanned digital format, for example. These journeys across platform require deeper investigation. If we think of the printed book as an interface between two orders of thinking, we need to consider how the experience of reading a digitized version of a formerly printed and bound book alters literary reception and student experience. How does the experience of reading across different technological platforms change the reader’s relationship to the content? As more and more electronic reading platforms take on the physical attributes of material reading experiences either by retaining material traces or by emulating them, we might question what experience How do the material traces left on digitised works impact the reading process for reading in literary studies? The lively discourse surrounding Google Books and the human breaches of the material into the immaterial, as the work crosses the borders of formats and interfaces, raises valuable questions about the future of the book, reading in the twenty-first century, and the long and formidable shadow that centuries of material text production casts over Google Books’ electronic utopia. This paper uses both book history and new media interface theory to consider the multitude of diverse experiences that is literary reading across different platforms in and out of the classroom and to consider whether distracted reading can be better understood as distributed reading. It considers critical infrastructure studies as a useful framework through which to think about reading in the digital age. ",Distributed reading: Literary reading in diverse environments,,2019,,Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2260,http://projects.chass.utoronto.ca/chwp/Casta02/Carlin_casta02.htm,"{'string_id': None, 'id_scheme': None}",This paper enumerates the technologically naïve humanist's wish list for the type of text analysis tools to be developed by the TAPoR project.,Drawing Knowledge from Information: Early Modern Texts and Images on the TAPoR Platform,,2005,"['Annotation, anthology, navigability, searchability, usability']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2261,,"{'string_id': None, 'id_scheme': None}"," The advent of handheld digital devices has proved to be revolutionary for the teaching of art history. They make it possible to supplement, even replace, the traditional format of lecturing and communal viewing of a single screen. On their personal screens, students can enter a searchable world of thousands of virtual images: what the radical art theorist and novelist André Malraux, more than half a century ago, called the imaginary museum. For several years, I have been experimenting with my art history classes, showing my students how to use their laptops, tablets and phones to become active viewers and collaborators. Multiple virtual images transform the content, as well as the environment, of learning art history. Rather than being restricted to the illustrations in a textbook, students can use their devices to engage with online image sources such as museum websites, image databanks, and auction houses. Portable digital technology can turn the art history classroom into a collaborative and dynamic experience inconceivable a short time ago. Malraux himself, who believed in the power of art to transcend time and space, might be amazed at what is now not only possible, but commonplace. ",The Imaginary Museum: Teaching Art History with Mobile Digital Technology,,2018,"['digital pedagogy', 'art history', 'higher education']",Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2262,,"{'string_id': None, 'id_scheme': None}"," In this essay, I examine the use of laptops in the classroom through the lens of boredom. Now that we can carry around an incredibly powerful little computer in our pockets or bring a lightweight laptop to class, boredom is a feeling that we almost never have to face anymore. We instantly alleviate the slightest hint of impatience or aimlessness with a single glance at our screens. But what introspective capacities do we neglect when we reflexively give ourselves over to the distractions of the outside world? What might dwelling within our boredom open up to students? Using my undergraduate Introduction to Digital Media course as a case study, I argue that a humanistic approach to digital media provides STEM students in particular with a means of contextualizing technical detail within larger sociopolitical systems, and that a focus on individual habits of attention can provide an important hinge between the granular and the global. Next, I reflect on the pressures produced by the very devices we studied on the classroom environment, and frame our experiences within some of the most influential writings on boredom as a historically and technologically conditioned mood. I then conclude with some speculations on planning for boredom within a laptop policy that is capable of accounting for a range of learning styles. ",Laptop Policy: Notes on Boredom,,2018,"['digital pedagogy', 'media theory', 'cybernetics', 'frankfurt school']",Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2263,,"{'string_id': None, 'id_scheme': None}"," This article examines the potential of employing structured texts, encoded in the Parliamentary Metadata Language XML schema, for the machine-readable analysis of substantial corpora of legislative proceedings. It demonstrates the potential of using PML corpora for combining the results of sentiment analysis with contextual metadata to establish and visualise patterns of divergent attitudes towards a topic such as immigration as they correlate with such features as party affiliation or geographic location. This is readily achieved using such simple techniques as XSLT transformations or XQUERY searches. ",Using structured text corpora in Parliamentary Metadata Language for the analysis of legislative proceedings,,2018,,Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2264,,"{'string_id': None, 'id_scheme': None}"," This paper argues that digital humanities pedagogy can provide unique forms of attention and engagement for students at the outset of their college and university careers. It reports on courses taught at NC State University and elsewhere which use digital humanities pedagogy under the aegis of studying transformations in reading. With attention to the mediation of attention itself, these courses can cultivate habits of self-reflection to aid students in any disciplinary pathway. Furthermore, digital humanities pedagogy can offer a transferable project-based model of instruction, helping students to develop metacognitive skills as well as to acculturate to different environments across campus. Ultimately, this essay underscores the need to articulate strategies of educational transfer in digital pedagogy. ","Reading, Making, and Metacognition: Teaching Digital Humanities for Transfer",,2018,"['digital pedagogy', 'reading', 'comparative media studies', 'metacognition', 'transfer', 'critical making']",Digital Humanities Quarterly,012,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2265,,"{'string_id': None, 'id_scheme': None}"," Digital reconstructions of classical antiquity are generally ocularcentric, appealing only to the sense of vision. We propose that new technologies may be used to engage the other senses in the act of reception, and specifically here we focus on kinaesthesia, or the sense of self-movement. This paper analyses a phase of the project Ancient Dance in Modern Dancers in which participants created performance pieces in a genre of Graeco-Roman dance for use in a motion-capture system. It was necessary for the performers to develop a range of translational strategies in order to communicate their movement to the system, entailing what we term distributed reception, in which the ultimate recipient of ancient source-material is not a human actor but rather the machine with which s/he is in collaboration. ",Ghosts in the Machine: a motion-capture experiment in distributed reception,,2018,,Digital Humanities Quarterly,012,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2266,,"{'string_id': None, 'id_scheme': None}"," This paper proposes a new listening methodology that lies, in some sense, between Charles Bernstein’s close listening and Tanya Clement’s distant listening. While it’s tempting to think of these approaches as presenting an integral of scope, Bernstein’s work proposes the use and tuning of the human ear for understanding the sonic materiality of the phonotext, while Clement’s work, following its namesake of Moretti’s distant reading, calls for a form of surrogate listening — using the machine in place of the ear. Building on Bernstein’s work and toward Clement’s, I suggest Machine–Aided Close Listening as a methodology that uses data visualization as a prosthetic extension of the ear. This is to say that by considering three dimensions of the phonotext — 1) the textual manifestation of the poem, 2) the audio of the poet performing it, and 3) a visualization of that audio — the reader–listener can empirically confirm the ear’s impressions and expose new facets of the sonic materiality of the performed poem. In support of this methodology, I will demonstrate a new digital tool for aligning these three facets of the phonotext. ",Machine-aided close listening: Prosthetic synaesthesia and the 3D phonotext,,2018,"['poetry', 'poetics', 'modernism', 'methodologies', 'phonotextuality', 'poetry audio']",Digital Humanities Quarterly,012,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2267,,"{'string_id': None, 'id_scheme': None}"," Technologies used to study and represent the past are not hermeneutically neutral. Since technologies have illocutionary force, we should ask of any representation of history, In what ways are the associated technologies conveying meaning? In this article, the question will be asked of a particular kind of history, textual history, taking biblical texts as an illustrative example. The goal is to better understand how textual technologies of yesterday, today, or tomorrow capture or obscure the material history of their texts. The article proceeds in two main parts. First, two kinds of technologies commonly used are examined and compared: the non-digital book and its familiar digital counterparts. Second, we will consider ways in which augmented reality and virtual reality have been used and might be used to represent texts. It will be suggested that these new technologies may overcome some of the limitation of previous book technologies in capturing a text’s history. In conclusion, practical suggestions will be made for those who read, study, or produce digital textual objects. ","How Technology Means: Texts, History, and Their Associated Technologies",,2018,,Digital Humanities Quarterly,012,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2268,,"{'string_id': None, 'id_scheme': None}"," This paper presents findings from a set of contextual inquiry sessions completed in 2016 as part of the four year Digital Scholarly Workflow project (2012-2016) conducted at the Pennsylvania State University and funded by the Andrew W. Mellon Foundation. Studying humanists’ workflows through contextual inquiry enabled us to gain valuable empirical insights into digital scholarship from the perspective of individual scholars. This small-scale observational study (N=8) supplemented findings from our larger scale (N=196) and medium scale (N=23) studies previously completed in the project, allowing us to develop deeper qualitative understanding of how humanists’ research practices change in the encounter with digital technologies. Through close contextual analysis of information-based research activities we illustrate how humanists build rich personal collections of digital sources, which become important information resources in their work. We also examine how adoption of digital tools transforms the established methods of managing primary and secondary sources, bringing hastier digital workflows to traditionally more methodical work. Finally, we show that efficiency cannot be qualified as a neutral or inherent characteristic of digital tools, independent of scholars’ practices, since scholars’ interaction with digital artifacts determines what constitutes efficiency of a tool in the context of a scholar’s workflow. We conclude that in humanists’ workflows digital and analog tools and resources do not necessarily replace one another or vie for the dominant position. They rather supplement each other in an interconnected, hybrid workflow that comes to our respondents as an organic manifestation of their work, and as an illustration of the bricolage character of humanists’ digital workflows. ",Researcher as Bricoleur: Contextualizing humanists’ digital workflows,,2018,,Digital Humanities Quarterly,012,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2269,,"{'string_id': None, 'id_scheme': None}"," The article discusses the current situation in the adoption of digital tools and practices in the humanities and arts in Lithuania, based on a major European survey conducted by the Digital Research Infrastructure for the Arts and Humanities (DARIAH) in 2014 and 2015. The survey was aimed at understanding existing scholarly practices, methods and tools that are applied by researchers, as well as attitudes towards digital technologies in research and scholarship. This article analyzes specific aspects of scholarly research activities and digital needs in Lithuania, and provides evidence-based insights on the national digital humanities landscape. ","Defining scholarly practices, methods and tools in the Lithuanian digital humanities research community",,2018,"['digital humanities', 'scholarly practices', 'digital methods', 'digital tools']",Digital Humanities Quarterly,012,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2270,,"{'string_id': None, 'id_scheme': None}"," The proliferation of tools that enable anyone to create visualizations of their data, even with limited experience or skills, has made data visualization more accessible than ever before. This is true in its use in both teaching and learning, as data visualization has increasingly taken on an important pedagogical role in the classroom and in scholarly research. However, with this proliferation of tools there has been a concomitant awareness that visualization needs to be employed through a critical lens that acknowledges its constructedness as explanatory medium and as a product of situated knowledges. Here, I describe one approach to teaching this notion of constructedness via a framework oriented around information literacy, which encourages critical engagement with data, the tools we use to interrogate them, and the visualizations we design to represent them. I describe this approach through a collection of critical dichotomies used to evaluate the authority and value of visualizations, which are mapped to a subset of the core information literacy competencies defined in the ACRL Framework for Information Literacy in Higher Education. To put these dichotomies into practice, I further describe an interactive activity called Choose Your Own Adventure, with Data Visualization, in which participants are given paper and markers to create booklets in the style of Choose Your Own Adventure books and asked to consider the relationship between active choices in the design process of a visualization and how a given visualization is interpreted. In the process, I explore how this framework can encourage us all, as critical practitioners of visualization, to think about the practical relationship between data visualization and information literacy more generally. ",Critically engaging with data visualization through an information literacy framework,,2019,"['data visualization', 'information design', 'information visualization', 'digital pedagogy']",Digital Humanities Quarterly,012,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2271,http://projects.chass.utoronto.ca/chwp/CHC2003/Bonnett2.htm,"{'string_id': None, 'id_scheme': None}","One of the key challenges currently facing humanities and computing scholars is the generation of new conventions, particularly new forms of narration and representation to support student development of critical thinking skills. Drawing inspiration from the writings of the communication theorist Harold Innis, this study argues that conventions supporting information visualization - the production of visual analogues of ideas or patterns represented by text and number - can help students apprehend and interrogate concepts. The experience of the 3D Virtual Buildings Project suggests that a program devoted to 3D computer modelling is one way that Innis' ideas might be realized. Through tutorials students learn to reconstruct models of historic Canadian settlements and to translate historic data into numeric information for a 3D model. Most importantly, the tutorials show students that historical models are inherently imperfect, at best partial reconstructions of the past. Through a process of information translation and visualization, students learn to distinguish historical models from the objects they purport to represent.","The Oral Tradition in 3D: Harold Innis, Information Visualisation and the 3D Historical Cities Project",,2003,"['Harold Innis, 3D, virtual reality, CAD, information visualisation, learning, constructivism, urban architecture, Canada, history, computing, critical thinking']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2272,,"{'string_id': None, 'id_scheme': None}"," Data visualization has rapidly become a standard approach to interrogating and understanding the world around us in domains that extend beyond the technical and scientific to arts, communications and services. In business settings the Data Scientist has become a recognized and valued role . Journalism has re-oriented itself around data-driven storytelling as a potential saviour for an industry in peril . Governments are moving to more data-driven decision making, publishing open data portals and pondering visualization as an opportunity for citizen participation . This journal itself has numerous examples that use visualization tools and techniques within the digital humanities as a tool for exploration . This boom in attention has led large new populations of learners into the field. Formal educational settings have rushed to create new approaches and introductions to this content, but often they fall back on traditional approaches to things such as scientific charting and graphing . Many view data visualization as a new technology, which runs the risks of replicating old approaches without acknowledging the unique affordances and domains that data visualization relies upon. Data visualization is not simply another technology to integrate into education. It is visual argument and persuasion, far more closely associated with rhetoric and writing than spreadsheets . In this paper we present novel approaches to learning technologies and activities, focused on novice learners entering the field of data driven storytelling. We begin with a deeper dive into the problems we see with introducing new learners into a field characterized by inequality, continue with a discussion of approaches for introducing technologies to education, and summarize the inspirational pedagogies we build on. We then offer some design principles and three activities as examples of the concept of creative data literacy. We assert that creative approaches grounded in constructionist educational theories are necessary to empower non-technical learners to be able to tell stories and argue for change with data. ",Creative Data Literacy: A Constructionist Approach to Teaching Information Visualization ,,2018,"['data literacy', 'constructionism', 'constructivism', 'big data', 'pedagogy', 'data visualization']",Digital Humanities Quarterly,012,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2273,,"{'string_id': None, 'id_scheme': None}"," For many years, digital humanists have responded to Alan Liu’s call for critical digital humanities. Projects such as #TornApart/Seperados and #PuertoRicoMapathon and pieces like Where is Cultural Criticism in the Digital Humanities? , All the Digital Humanists Are White, All the Nerds Are Men, but Some of Us Are Brave , and Beyond the Margins: Intersectionality and the Digital Humanities have paved the way for socially conscious developments in the field. At the same time, pressures from administrators, institutions, and funding agencies often discourage critical engagement in favor of tool development and/or high tech projects . As such, we often attempt to adapt highly rewarded tech skills, like text, sentiment, and big data analyses, for use in social justice projects. While it is possible for these two aims to be compatible, we do ourselves a disservice when we try to force them together. So, how should teacher-scholars implement an intersectional digital humanities framework in the classroom? Using my own classroom as a case study, I assert that one effective strategy is through curation, which helps students investigate topics such as race, gender, sexuality, disability, and socioeconomic status through the careful selection, arrangement, and presentation of materials. Doing so teaches students to think more critically about the act of curation, by encouraging them to participate in knowledge construction as well as the dismantling of harmful narratives and power structures. While this approach differs from the tool-based pedagogy often utilized in the field, its emphasis on knowledge production, critical thinking, digital literacy, and social justice gives students proficiency in socially conscious methodologies that can be applied to any project. Linking curation to making and breaking, two digital humanities approaches to meaning-making, provides a method for interrogating archives of humanity and developing a pedagogy grounded in cultural critique and social justice . ",Making and Breaking: Teaching Information Ethics through Curatorial Practice,,2019,"['digital humanities pedagogy', 'intersectionality', 'curation', 'feminism']",Digital Humanities Quarterly,012,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2274,,"{'string_id': None, 'id_scheme': None}"," Typography, the arrangement and use of type, permeates our visual landscape, from printed pages and digital interfaces to physical environments and interactive experience. Those who study and work in the digital humanities are called upon to make typographic choices everyday, yet few have any training in how to effectively design with type. Over the past several years, I have been involved in several initiatives that seek to help educate digital humanities scholars, technologists and students about the value of information design, including typography. This paper will discuss these experiences and some guiding principles for helping a digital humanities audience understand the basic principles of typography and apply them to their projects and research. ",Best Practices: Teaching Typographic Principles to Digital Humanities Audiences,,2019,,Digital Humanities Quarterly,012,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2275,,"{'string_id': None, 'id_scheme': None}"," The popularity of information visualization in academia and practice brings a renewed emphasis on aesthetic values and visual applications to ensure its appeal to a wider audience. This paper focuses on visualization aesthetics and perception by making the case for using graphic design techniques and design languages to understand and create more aesthetically pleasant and functionally effective information design and visualization. It uses cross-disciplinary reviews of background research to demonstrate the value of graphic design principles and methods in the realm of visualization education. A user-centered design framework and student projects are discussed by adapting graphic design elements into the visualization process. It shows that the practice of developing a visualization should be executed with an understanding of graphic design basics in mind, and with a balanced consideration of tangible and conditional design elements, as well as how these design elements fulfill the purpose of the objective, context, content, audience, and the knowledge of the design outcomes. ",Placing Graphic Design at the Intersection of Information Visualization Fields,,2019,,Digital Humanities Quarterly,012,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2276,,"{'string_id': None, 'id_scheme': None}"," The use of new technologies, media and computational methods in Humanities may evolve and indeed it is already changing the contents and forms of contemporary education, opening new scenarios previously unimaginable. Nowadays Visual Mobile Devices, smartphones, tablets, etc, contribute to a range of computer-mediated activities, which are related to specific locations and times. But for such critical reflection, we need a closer look to deeper layers of our culture to unveil the characteristics of new technologies and devices. This paper describes some emerging aspects of the teaching use of visual communication techniques in primary and secondary schooling. In particular it reviews the evidence of the efficiency of visual communication for learning both in varied curriculum disciplines and of transversal socio-ethical affective skills necessary for the promotion of health and the construction of a planetary citizenship. The paper illuminates the didactic applicability and unique potential of images for the epistemological peculiarities of different disciplines, highlighting how educational use of images in creative pedagogy can be more focused given the function they perform in the more general cognitive process of individuals. The theoretical analysis of the teaching validity of the use of visual communication, as reported in the international literature, seems to receive confirmation from a case study, which provides detailed analyses of how this technique can enhance specific projects and demonstrates its significance for wider practitioners. ",Visual Communication and the promotion of Health: an exploration of how they intersect in Italian education,,2019,,Digital Humanities Quarterly,012,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2277,,"{'string_id': None, 'id_scheme': None}"," During the 2016–2017 financial year, King's Digital Lab (King's College London) undertook an extensive archiving and sustainability project to ensure the ongoing management, security, and sustainability of ~100 digital humanities projects, produced over a twenty-year period. Many of these projects, including seminal publications such as Aphrodisias in Late Antiquity, Inscriptions of Roman Tripolitania, Henry III Fine Rolls, Jonathan Swift Archive, Jane Austen Manuscripts, The Gascon Rolls, The Gough Map, and Inquisitions Post Mortem, occupy important positions in the history of digital humanities. Of the projects inherited by the lab, about half are either of exceptionally high quality or seminal in other ways but almost all of them struggled with funding and technical issues that threatened their survival. By taking a holistic approach to infrastructure, and software engineering and maintenance, the lab has resolved the majority of the issues and secured the short to medium term future of the projects in its care. This article details the conceptual, procedural, and technical approaches used to achieve that, and offers policy recommendations to prevent repetition of the situation in the future. ",Managing 100 Digital Humanities Projects: Digital Scholarship & Archiving in King’s Digital Lab ,,2019,,Digital Humanities Quarterly,013,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2278,,"{'string_id': None, 'id_scheme': None}"," Over the past few decades, the ever-expanding media of the digital world, including digital humanities endeavors, have become more reliant on the results of Optical Character Recognition (OCR) software. Yet, unfortunately, medievalists have not had as much success with using OCR software on handwritten manuscripts as scholars using printed books as their sources. While some projects to ameliorate this situation have emerged in recent years, using software to create machine-readable results from medieval manuscripts is still in its infancy. This article presents the results of a series of successful experiments with open-source neural network OCR software on medieval manuscripts. Results over the course of these experiments yielded character and word accuracy rates over 90%, reaching 94% and 97% accuracy in some instances. Such results are not only viable for creating machine-readable texts but also pose new avenues for bringing together manuscript studies and digital humanities in ways previously unrealized. A closer examination of the experiments indicates regular patterns among the OCR results that could potentially allow for use cases beyond pure text recognition, such as for paleographic classifications of script types. ",Modelling Medieval Hands: Practical OCR for Caroline Minuscule,,2019,,Digital Humanities Quarterly,013,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2279,,"{'string_id': None, 'id_scheme': None}"," We present findings from interdisciplinary research at the intersection between literary studies, information visualization, and interface design. Despite a growing interest in text visualization among literary scholars, so far, narrative visualizations are not designed to support the particular tasks involved in narratological analysis and often fail to reveal nuanced narratological features. One major outcome of our iterative research and design process is Narrelations, a novel visualization technique specifically suited for analyzing and interpreting narrative levels of a story and temporal aspects of its narrative representation. The visualization provides an overview of the nesting and distribution of narrative levels, integrates the representation of temporal phenomena, and facilitates the examination of correlations between these aspects. With this research we explore how collaboratively designed visual encodings and interaction techniques may allow for an insightful analysis at a high level coupled with a close inspection of text passages. We discuss prior work relevant to our research objectives and explain the specific characteristics of narrative levels and temporal aspects of narrative representation. After describing the research process and design principles, we apply the visualization on a test corpus of eight annotated German short stories and demonstrate its heuristic value for literary analyses and interpretations. In particular, we explore the intricate connections between the literary content of the novellas and their narrative form. ",Narrelations — Visualizing Narrative Levels and their Correlations with Temporal Phenomena,,2019,,Digital Humanities Quarterly,013,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2280,,"{'string_id': None, 'id_scheme': None}"," This paper explores different ways of modelling and simulating complex spatial and temporal events, such as battles, for which it has been practically impossible to (re)construct the thousands if not hundreds of thousands of variables of which they are comprised. This research utilises as a case study the Battle of Mount Street Bridge of the Irish Easter 1916 Rising, in which the number of British casualties has been fiercely debated. The research is framed within the theory and practice of digital scholarly editions, which provides a new paradigm for approaching virtual worlds in a contextualized and annotated environment. This paper also discusses the challenges of creating virtual worlds for online environments in which there is rapid obsolescence of software and platforms and an absence of standards. ",Towards 3D Scholarly Editions: The Battle of Mount Street Bridge,,2019,"['Virtual worlds', 'digital (re)constructions', '3D modelling', 'digital scholarly editions', 'Unity 3D', 'WebGL', 'battle', 'Easter Rising']",Digital Humanities Quarterly,013,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2281,,"{'string_id': None, 'id_scheme': None}"," This short essay describes the difficulties and impromptu workarounds that emerged when using the video game Minecraft as the central teaching tool in several professional writing seminars. More specifically, the author discusses a key moment in the semester where students needed to move between university and non-university technology infrastructures in order to create multiplayer gamespaces that were accessible to their peers. In narrating this experience, the author will demonstrate how a discourse of access can be used to examine the oft-invisible policies, procedures, and restrictions that shape the way we compose, circulate and make visible digitally-native work. Furthermore, the author will discuss how a critical emphasis on access can help teachers and students better mediate the relationship between internal or university-supplied technological infrastructures and external platforms when creating interactive digital projects. The underlying motivation of this essay is not to lambaste universities for lack of institutional support nor is it to champion commercial organizations as saviors for helping teachers successfully use digital platforms in the classroom. Instead, the goal of this brief essay is to spur discussions surrounding the following questions: how might we use issues regarding access to better examine and navigate the hard-to-define boundaries that separate university-sanction technology use from non-university sanctioned technology use? How might calling students' attention to access refine the larger learning objectives for Digital Humanities or DH-related courses? ","Interlude: Gaining Access, Gaming Access: Balancing Internal and External Support For Interactive Digital Projects ",,2019,,Digital Humanities Quarterly,013,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2282,http://projects.chass.utoronto.ca/chwp/Casta02/Best_casta02.htm,"{'string_id': None, 'id_scheme': None}","The principal aim of the Internet Shakespeare Editions is to ""create and publish works for the student, scholar, actor, and general reader in a form native to the medium of the Internet."" Our focus to date, as texts are being edited, has been to create hypertext editions, capable of being annotated both by text and by multimedia objects. The infrastructure created by TAPoR (Text Analysis Portal for Research), however, provides an opportunity to go further than the hypertext edition, by integrating into it the capabilities offered by textual analysis software. The aim will be to develop an interface that permits a user to go beyond the hard links provided by the editor in the basic hypertext structure to explore in a more fluid way the text of a given play, and to extend the exploration to related texts by Shakespeare and other relevant writers in the period.",'Is this a vision? is this a dream?': Finding New Dimensions in Shakespeare's Texts,,2005,"['TAPoR, Shakespeare, hypertext edition, text analysis software, interface design']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2283,,"{'string_id': None, 'id_scheme': None}"," The work of digital humanists and librarians is often invisible to the larger communities in which they work, particularly in academia. This opinion essay by three librarian-scholar-digital practitioners explores invisible work and life on the hyphen — between the academy and the library and between the human and the digital. In this essay, we illustrate how librarian-scholar-digital practitioners can feel overworked and underappreciated, working in and with multiple fields and communities who have different and sometimes competing methodologies. Through two examples, we look at how living on the hyphen takes its toll for librarian-scholar-digital practitioners. We end our essay by detailing steps faculty and administration can take to help us solve the problem and realize the promise of digital humanities. ","Manifesto: A Life on the Hyphen: Balancing Identities as Librarians, Scholars, and Digital Practitioners",,2019,,Digital Humanities Quarterly,013,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2284,,"{'string_id': None, 'id_scheme': None}"," This essay responds to two questions at the heart of the Invisible Labor in the Digital Humanities 2016 symposium at Florida State University: (1) what is at stake in making unseen work visible, and (2) how can DH projects equally distribute and value the labor involved in their construction? For us, the answer to these questions lies in privileging the pedagogical affordances of data construction by crafting a workflow that included undergraduates as intellectual partners, and using DH methods to visualize and make public this collaborative labor. By drawing on our work with Photogrammar, which visualizes federal New Deal documentary projects including photography and life histories, we highlight three strategies for making labor visible in the digital humanities. First, we discuss how this project served as a tool for teaching undergraduate students key methods in DH by giving them experience with conducting original research with credit on the public site. In this way, we explain how pedagogy can become a part of project development. Second, we argue that DH visualization techniques can make the labor behind DH projects visible. We focus on how Photogrammar uses a timeline and network analysis alongside the traditional About page to make visible all participants in the project. Third, we turn to an open discussion of the challenges faced in the politics of attribution when working with university, governmental and private historical organizations, including domain names and the use of organizational logos. ",Building Pedagogy into Project Development: Making Data Construction Visible in Digital Projects,,2019,,Digital Humanities Quarterly,013,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2285,,"{'string_id': None, 'id_scheme': None}"," Academic libraries have an important role to play in supporting digital humanities projects in their communities. Librarians at Stony Brook University Libraries host Open Mic events for digital humanities researchers, teachers, and students on campus.  Inspired by a desire to better serve digital humanists with existing projects, this event was initially organized to increase the visibility of scholars and students with nascent projects and connect these digital humanists to library supported resources and to one another. For the Libraries, the Open Mic was an opportunity to understand the scope and practices of the digital humanities community at Stony Brook, and to identify ways to make meaningful interventions.   An open mic is a uniquely suitable event format in that it embodies a dynamic, permissive, multidisciplinary presentation space that is as much for exercising new and ongoing research (and technologies) as it is for making discoveries and connections.  The success of these events can be measured in the establishment of the University Libraries as a nexus for digital humanities work, consultations, instruction, workshops, and community on a campus without a designated digital humanities center.   The digital humanities Open Mic event at Stony Brook University locates the digital humanities within the library’s repertoire, while signaling that the library is — in a number of essential ways — open. ",Raising Visibility in the Digital Humanities Landscape: Academic Engagement and the Question of the Library’s Role,,2019,"['libraries', 'collaboration', 'interdisciplinarity']",Digital Humanities Quarterly,013,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2286,,"{'string_id': None, 'id_scheme': None}"," This article illuminates the ways that digital humanities labs might foster experiences for graduate students that fulfill what Alexander Reid (2002) postulates as the central task of the digital humanities graduate education. We argue that while the digital humanities lab as an institutional economic model does not necessarily promote a focus on graduate student professionalization, it uniquely has the capacity to push back against competing discourses of neoliberal vocationalism, funding and labor precarity on one hand, and technological utopianism and tool fetishization on the other, to train students agile, contextual, and rhetorical mindsets with which to enter technologically-mediated workplaces and lives. To begin, we review the discussion of digital humanities labs in the literature: digital humanities institutional models, how these models are practiced, lab funding, and the resultant position of labs as sites of training for graduate students. From there, we offer a teaching case from the Lab’s fall 2015 “Stories from Data” workshop in order to render visible a set of principles to guide professionalization of graduate students in the digital humanities lab. We conclude with reflections on how these principles might alter current discussions of the success or failure of the Mellon Foundation and NEH ODH digital humanities funding initiatives in the United States. ",The Invisible Work of the Digital Humanities Lab: Preparing Graduate Students for Emergent Intellectual and Professional Work,,2019,"['digital humanities labs', 'graduate students', 'professionalization']",Digital Humanities Quarterly,013,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2287,,"{'string_id': None, 'id_scheme': None}"," FemTechNet is a relatively small and loosely affiliated feminist, anti-racist collective which focuses on overlaps between and implications of feminism and technology. It exists as a support and collaboration structure that challenges traditional hierarchies through distributed power and collective creation. In an examination of hidden labor and archival silences, this research addresses how ideological underpinnings shaped the process of envisioning an institutional archive of FemTechNet records, how principles held by FemTechNet reverberated through the archival process, and how FemTechNet members conceptualized the imagined institutionally-held archive. The research reveals that the collective navigated institutional requirements and resources, the risks inherent in the tensions between the personal and the collective, and affective presence as part of the creation of the records that form the archive. ",Affective Absence: Risks in the Institutionalization of the FemTechNet Archive,,2019,,Digital Humanities Quarterly,013,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2288,,"{'string_id': None, 'id_scheme': None}"," Using DIAS and DENDRON, computer applications designed for the study of infectious fungi and cancer cells, we forge a connection in color, composition, and theme between Paolo Veronese’s Les Noces de Cana and Paul Cézanne’s Baigneuse debout, s’essuyant les cheveux. From this connection, we bring to light a hidden compositional structure, heretofore unidentified, in Cézanne’s Bathers as a series. Using computer-assisted systems such as DIAS and DENDRON allowed us to detect relatedness in these paintings not necessarily visible to the human eye. With DIAS and DENDRON, we generated dendrograms that clustered paintings related in brightness, saturation, complexity, and color. After studying color, we applied DIAS to the study of Cézanne’s composition. Because Cézanne expressed Neoplatonic ideals regarding paintings in his conversations and letters, we programmed DIAS to compute axial lines and golden sections according to the dimensions of the square or rectangular painting under scrutiny. Using DIAS we identified the golden rectangle for Veronese’s Les Noces de Cana, Cézanne’s Baigneuse debout, s’essuyant les cheveux and Ingres’ La Source (another likely model for Baigneuse debout, s’essuyant les cheveux). From the similarities and differences between the three paintings we identified, regarding their use of axial symmetry and the golden section, we have formulated a new approach to seeing Cézanne’s composition in his Bather series.   Cézanne’s noumenal bodies, formerly perceived as awkward and composed with baffling imbalance, we can now see as golden. ",Dendrography and Art History: a computer-assisted analysis of Cézanne’s Bathers.,,2019,,Digital Humanities Quarterly,013,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2289,,"{'string_id': None, 'id_scheme': None}"," In this article I call for more recognition of and scholarly engagement with public, volunteer digital humanities projects, using the example of LibriVox.org to consider what public, sustainable, digital humanities work can look like beyond the contexts of institutional sponsorship. Thousands of volunteers are using LibriVox to collaboratively produce free audiobook versions of texts in the US public domain. The work of finding, selecting, and preparing texts to be digitized and published in audio form is complex and slow, and not all of this labor is ultimately visible, valued, or rewarded. Drawing on an ethnographic study of 12 years of archived discourse and documentation, I interrogate digital traces of the processes by which several LibriVox versions of Anne of Green Gables have come into being, watching for ways in which policies and infrastructure have been influenced by variously visible and invisible forms of work. Making visible the intricate, unique, archived experiences of the crowdsourcing community of LibriVox volunteers and their tools adds to still-emerging discussions about how to value extra-institutional, public, distributed digital humanities work. ","The In/Visible, In/Audible Labor of Digitizing the Public Domain",,2019,,Digital Humanities Quarterly,013,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2290,,"{'string_id': None, 'id_scheme': None}"," In this paper, we present a system that automatically adds visualizations and natural language processing applications to analog texts, using any web-based device with a camera. After taking a picture of a particular page or set of pages from a book or uploading an existing image, our system builds an interactive digital object that automatically inserts modular elements in a digital space. Leveraging the findings of previous studies, our framework augments the reading of analog texts with digital tools, making it possible to work with texts in both a digital and analog environment. ",Textension: Digitally Augmenting Document Spaces in Analog Texts,,2019,,Digital Humanities Quarterly,013,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2291,,"{'string_id': None, 'id_scheme': None}"," This essay was written specifically for NYCDHWeek 2018, the theme of which was DH in the Moment: Reaction, Response, Relevance. The author examines how we define digital humanities activism and how we frame its histories. Relying primarily on examples from Caribbean-oriented digital work, the author argues for a broad definition of DH activism that allows for a variety of projects and intended audiences. In particular, the essay responds to the tendency to focus on DH in the moment (projects that can be done quickly and yield a high social impact) as the primary form of activism, arguing that alongside such projects we include as activist projects that have a more cumulative and less immediate effect. Recently, we have begun to ask not just what the digital humanities does, but what the digital humanities does for others. This essay considers why it is that this has become a key question in this DH moment. ","DH Moments, Caribbean Considerations: On Reaction, Response, and Relevance in the Digital Humanities",,2019,,Digital Humanities Quarterly,013,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2292,,"{'string_id': None, 'id_scheme': None}"," This article articulates a digital adaptation of enumerative bibliography and argues for its recuperative potential in feminist literary history. Digital enumerative bibliography uses bibliographical structures within a relational database that allows researchers to track more relevant metadata such as geographical location of subject matter, language, and time period. Whereas traditional enumerative bibliographies use a hierarchy of textual data, a relational database creates a nexus that facilitates new kinds of research queries. As an example, we offer our digital project the Women in Book History Bibliography and use its 1,550 citations as a dataset to trace what is women’s book history. We then advocate for digital enumerative bibliography as a form of feminist recovery efforts that recovers not only primary texts but scholarship about them. ","Building the Women in Book History Bibliography, or Digital Enumerative Bibliography as Preservation of Feminist Labor.",,2019,,Digital Humanities Quarterly,013,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2293,http://projects.chass.utoronto.ca/chwp/baider/,"{'string_id': None, 'id_scheme': None}","In his seminal work l'Emile Rousseau asked his female reader to educate her daughter so that she would become a 'honnête femme' and not a 'honnête homme'. Rousseau was playing on words as the collocation 'honnête homme' refers to a social and intellectual ideal reserved for the masculine gender, whereas the 'honnêteté' described in 'honnête femme' refers to the loyalty of a spouse vis-à-vis her husband. This different meaning of an adjective when it qualifies the noun femme or the noun homme is found in other collocations in French, for instance a grand homme is a ""great man"" but a grande femme is ""a tall woman"". This lexical and semantic dissymmetry is the topic of this essay. I make the hypothesis that the differential meaning can be explained by the systematic derogation of the noun femme; therefore, in the case of the syntagms with a differential meaning the noun modifies to a certain extent the adjective. Computerized databases and on-line texts provide the much-needed data to examine this assumed phenomenon of derogation of the noun femme. I also remark upon the usage of some women writers. Female writers tend to use more positive adjectives with the word woman. Also female writers such as de Navarre and Sévigné use adjectives with the 'male (and positive) meaning' when referring to women and not the 'female (and negative) meaning'.",Sexism And Language : What Can The Web Teach Us ?,,2000,"['Women, Renaissance, language, French, gender, lexicology, dictionaries']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2294,,"{'string_id': None, 'id_scheme': None}"," Our libraries are full of manuscripts, many of them modern. However, the digitisation of these unique documents is currently very expensive. How can we reduce the cost of encoding them in a way that will facilitate their study, annotation, searching, sharing, editing, comparison and reading over the Web? Unlike new documents prepared for the Web, historical manuscripts frequently contain internal variation in the form of erasures, insertions, substitutions and transpositions. Variation is also often expressed externally between copies of one work: in successive print editions, in manuscript copies or successive drafts. Current practice is to prepare separate transcriptions of each physical document and to embed internal variation directly into the transcribed text using complex markup codes. This makes the transcriptions expensive to produce and hard to edit, limits text reuse and requires that transcriptions be first disentangled via customised software for reading, searching, or comparison. An alternative approach, described here, is to separate out the internal variation of each document into notional layers. This is done primarily in order to facilitate the recording of revisions at any one point. The move is, of course, counter-intuitive since these document-wide layers were not intended by the author as texts to be read. But it proves itself in practice by radically simplifying the tasks of editing, searching and comparison. Versions, on the other hand, are higher-level constructs that do represent the state of a text as it was left at some point in time by an author or scribe. By employing layers to record complex revisions, the task of computing differences among intra-document layers and against versions of the same work in multiple documents may be delegated to the machine rather than having to be recorded laboriously by hand. The ensuing simplification of markup reduces transcription and editing costs, boosts text reuse and searching, and, by removing the need for customised software, increases the longevity of digital transcriptions. ",A Model of Versions and Layers,,2019,,Digital Humanities Quarterly,013,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2295,,"{'string_id': None, 'id_scheme': None}"," This article offers a methodological contribution to manually-assisted topic modeling. With the availability of vast amounts of (online) texts, performing full scale literary analysis using a close reading approach is not practically feasible. The set of alternatives proposed by Franco Moretti (2000) under the umbrella term of distant reading aims to show broad patterns that can be found throughout the entire text collection. After a survey of literary-critical practices that combine close and distant reading methods, we use manual annotations of a thread on Reddit, both to evaluate an LDA model, and to provide information that topic modeling lacks. We also make a case for applying these reading techniques that originate in literary reading more broadly to online, non-literary contexts. Given a large collection of posts from a Reddit thread, we compare a manual, close reading analysis against an automatic, computational distant reading approach based on topic modeling using LDA. For each text in the collection, we label the contents, effectively clustering related texts. Next, we evaluate the similarity of the respective outcomes of the two approaches. Our results show that the computational content/topic-based labeling partially overlaps with the manual annotation. However, the close reading approach not only identifies texts with similar content, but also those with similar function. The differences in annotation approaches require rethinking the purpose of computational techniques in reading analysis. Thus, we present a model that could be valuable for scholars who have a small amount of manual annotation that could be used to tune an unsupervised model of a larger dataset. ",Manual Annotation of Unsupervised Models: Close and Distant Reading of Politics on Reddit,,2019,,Digital Humanities Quarterly,013,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2296,,"{'string_id': None, 'id_scheme': None}"," The present paper addresses the problem of literary history as a problem of data comprehensiveness and selection, seeking not to resolve the impossibility of literary historical narrative, but to reframe it through a computational perspective. Our focus is on the Quan Tang shi 全唐詩 (Complete Tang poetry), the massive comprehensive anthology of Tang poetry that was produced at the height of the Qing dynasty (1644–1912). The sheer quantity of Tang poetry preserved in the Quan Tang shi (over 50,000 poems and poem fragments) exceeds the human-scale perspectives of close reading. To make sense of the corpus as a whole, we will show how two related forms of distant reading — topic modeling and divergence measures — allow us to reframe and rethink these literary historical questions and provide a new perspective on what it means to read Tang poetry. ","Reading the Quan Tang shi: Literary History, Topic Modeling, Divergence Measures",,2019,,Digital Humanities Quarterly,013,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2297,,"{'string_id': None, 'id_scheme': None}"," Skillful identification and interpretation of arguments is a cornerstone of learning, scholarly activity and thoughtful civic engagement. These are difficult skills for people to learn, and they are beyond the reach of current computational methods from artificial intelligence and machine learning, despite hype suggesting the contrary. In previous work, we have attempted to build systems that scaffold these skills in people. In this paper we reflect on the difficulties posed by this work, and we argue that it is a serious challenge which ought to be taken up within the digital humanities and related efforts to computationally support scholarly practice. Network analysis, bibliometrics, and stylometrics, essentially leave out the fundamental humanistic skill of charitable argument interpretation because they touch very little on the meanings embedded in texts. We present a problematisation of the design space for potential tool development, as a result of insights about the nature and form of arguments in historical texts gained from our attempt to locate and map the arguments in one corner of the Hathi Trust digital library. ",Finding and Interpreting Arguments: An Important Challenge for Humanities Computing and Scholarly Practice,,2019,,Digital Humanities Quarterly,13,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2298,,"{'string_id': None, 'id_scheme': None}"," With The Library of Digital Latin Texts (LDLT), the Digital Latin Library (DLL) is set to publish critical editions as sets of data independent of a presentational format. These datasets are meant to be reused for a variety of purposes and in a variety of presentational formats, including the DLL’s own user edition viewer and data visualization applications. It is thus vital to have a way of certifying the data as published in accordance with the DLL’s policies and procedures, central to which are its provisions for peer review. Accordingly, to facilitate the use of an edition’s data in different settings, and to promote a high degree of confidence in the quality of the data, we have introduced the DLL Review Registry Service. The DLL Review Registry Service offers a way of associating peer reviewed status with the text data rather than within any particularly visualization of that data. Any visualization of text data can consult this service and discover whether or not this data has been reviewed. In this way, we free the imprimatur from its association with any particular presentation and allow the indicators of quality to travel with the data and be communicated to end users in a plurality of visualizations. ",Decoupling Quality Control and Publication: The Digital Latin Library and the Traveling Imprimatur.,,2019,,Digital Humanities Quarterly,013,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2299,,"{'string_id': None, 'id_scheme': None}"," To examine changes in the diversity of popular songs over time, I have analyzed the lyrics of 99% of all songs sung in English that have appeared on the U.S. Billboard Hot 100 chart from 1956 up to and including 2016 (n=27.108). I have found the number of songs that reached the Hot 100 per year more than halved between 1966 and 2002, which indicates a loss in diversity. I have also found that the variation of valence (or level of pleasantness) in the songs’ lyrics and the variation of a sampled Type Token ratio (an indicator for lexical complexity) decreases over time. This indicates that lyrics of popular songs become more similar and again signify a loss of diversity. ",Changes in Lyrical and Hit Diversity of Popular U.S. Songs 1956-2016,,2019,"['Billboard', 'charts', 'lyrics', 'popular music', 'cultural diversity']",Digital Humanities Quarterly,013,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2300,,"{'string_id': None, 'id_scheme': None}"," The present study will investigate the perceptions of information professionals about their role in the work of digital humanities scholars, as well as the perceptions of digital humanities scholars on the role of information professionals in their research. While other scholarly literature has considered collaborations between these groups via surveys or interviews with small project teams, the present study will provide a large-scale analysis of collaborations using survey responses from more than 500 scholars, librarians, and archivists. Questions sought to determine the extent to which these groups collaborate with one another on project teams; how these collaborations unfold and who initiates them; whether IPs have begun to adjust and adapt their work to support specific DH projects, or to make their content more appealing and easy for potential future DH projects; and what administrative hurdles are faced during the collaboration. The survey also address how information professionals and DH scholars view the success of the collaboration after completing their work, and whether they intend to collaborate in future. To conclude, the article will consider what these responses tell us about how best to support all members of these collaborations. ",Digital Collaborations: A Survey Analysis of Digital Humanities Partnerships Between Librarians and Other Academics,,2019,,Digital Humanities Quarterly,013,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2301,,"{'string_id': None, 'id_scheme': None}"," In this study, we present the first large-scale quantitative analysis of a corpus of censored historical Hebrew manuscripts that have survived through the ages. A new multi-dimensional ontology-based approach was applied to explore the geographic, temporal, actor- and subject-based distribution of censorship events. We adopted an ontology-based approach to apply statistical analysis on the metadata of censored Hebrew manuscripts for estimating the scope and quantifying the extent of the known facts on the censorship activity and its various characteristics over the years. In addition, we revealed some previously unknown phenomena and trends. Particularly, we analysed the relationship of censorship on other types of events in manuscripts’ lifecycle and compared the distribution of censored vs. non-censored manuscripts in different dimensions. We also devised a set of rules to complete the missing locations of over 50% of censorship events, which has substantially changed the big picture of spatial distribution of censorship activity. From the temporal perspective our findings demonstrate that censorship was conducted in waves and there was a decrease in the creation of new manuscripts in periods of high censorship activity. Certain subjects, such as Kabbalah and Philosophy were censored significantly more than others, and the locations and script types’ distribution in censored manuscripts differs from the non-censored manuscripts. ",A multi-dimensional ontology-based analysis of the Censorship of Hebrew Manuscripts,,2020,,Digital Humanities Quarterly,014,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2302,,"{'string_id': None, 'id_scheme': None}"," This article describes a collaboration between two tenured faculty members (one in the library and one in a department of modern languages) at a large, land-grant institution who sought to introduce a mixed undergraduate and graduate seminar in French literature to DH methods in the second-language classroom culminating in a digital mapping project. Lacking explicit previous training in DH, faculty drew on second language (L2) pedagogy, new literacies, and DH pedagogy to develop an inclusive approach to course design and implementation. The approach focused on students’ development of agency and authority as rising scholars while underscoring conceptions of labor and professional development in the humanities. There is limited scholarship addressing implementing DH in a L2 classroom. However, implementing a combined approach where one pedagogy influenced the other afforded the opportunity to critically consider the role of multilingualism and multiculturalism in a global, open DH context. We adopted this approach in concert with lessons drawn from theories of information literacy and new literacies. This transdisciplinary method encouraged careful consideration of design and implementation given that how information is processed, acquired, and communicated are key concerns in both L2 classrooms and new literacies. ","Developing a Process-Oriented, Inclusive Pedagogy: At the Intersection of Digital Humanities, Second Language Acquisition, and New Literacies",,2020,,Digital Humanities Quarterly,014,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2303,,"{'string_id': None, 'id_scheme': None}"," This essay discusses the process of editing a rather simple playtext, the English traveling players' Fortunatus, with a quite complex history of circulation and performance and a vast network of sources and influences. Since the traveling theater was contingent (context-based) and extemporaneous, the associated playtexts, collaborative adaptations and translations of English works, present unique challenges for editors and scholars. In fact, their printed form often misrepresents the kind of theater practiced. I raise the question of how the textual remnants of this theater shaped critical attitudes regarding the travelers, and I suggest that capabilities of digital publication can illuminate the episodic and modular characteristics of early modern performance. ",Episodic Theater and the Digital Text: Editing the Traveling Players’ Fortunatus,,2020,,Digital Humanities Quarterly,014,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2304,,"{'string_id': None, 'id_scheme': None}"," In this paper, I extend theoretical understandings of the gaming activity and literary form called the quest and its relationship to issues of interpretation, focusing primarily on game theory with concrete examples as well as some broadly applicable pedagogical uses of these ideas in literature classrooms. The argument contributes to a recent theoretical and practical discussion of quests by scholars of games studies such as Aarseth 2004, Juul 2005, Tosca 2003, and Tronstad 2001. I build upon and revise these theorists’ understandings by approaching a “quest” as a goal-oriented activity in which players undertake a journey in search of meaning. By demonstrating similarities between the literary traditions in which quests are central and the practice of digital game design, I argue that quests can be better understood theoretically and more productively used in the classroom if meaning and action are regarded as complementary design principles instead of conflicting impulses. A revised understanding of quests can help to mediate between games and narratives by showing strategies by which game designers have created meaningful action, often in ways that are either unconsciously similar to or inspired by the literary traditions of mythology, epic, and romance. Specifically, game designers can use level design to create labyrinthine spaces that encode thematic implications, in the tradition of literary allegory. If these symbolic spaces are coordinated with significant obstacles and challenges, the apparent conflict between meaning and action can be resolved through engaging gameplay that allows players to enact a range of thematic ideas, contributing to the ongoing replay value of a digital game. I also suggest ways in which this understanding of quests can allow literature teachers to plan assignments where students transform literary narratives into interpretative quests taking the form of digital games. As a paradigmatic example, I describe one such assignment, in which my students adapted episodes from Pynchon’s The Crying of Lot 49 into design documents and prototypes, and I suggest some theoretical implications of its results for other instructors. By adapting works of literature into quests, students learn to discover and create meaning through the active exertion of cognitive and imaginative effort rather than absorbing it passively. ",Interpretative Quests in Theory and Pedagogy,,2007,,Digital Humanities Quarterly,001,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2305,,"{'string_id': None, 'id_scheme': None}"," Historians have regularly debated whether advertisements can be used as a viable source to study the past. One of their main concerns centered on the question of agency. Were advertisements a reflection of historical events and societal debates, or were ad makers instrumental in shaping society and the ways people interacted with consumer goods? Using techniques from econometrics (Granger causality test) and complexity science (Adaptive Fractal Analysis), this paper analyzes to what extent advertisements shaped or reflected society. We found evidence that indicates a fundamental difference between the dynamic behavior of word use in articles and advertisements published in a century of Dutch newspapers. Articles exhibit persistent trends. Contrary to this, advertisements have a more irregular behavior characterized by short bursts and fast decay, which, in part, mirrors the dynamic through which advertisers introduced terms into public discourse. On the issue of whether advertisements shaped or reflected society, we found particular product types that seemed to be collectively driven by a Granger causality going from advertisements to articles. Generally, we found support for a complex interaction pattern, analogous to Cowan’s concept of the consumption junction. Finally, we discovered noteworthy patterns in terms of Granger causality and long-range dependencies for specific product groups. All in, this study shows how methods from econometrics and complexity science can be applied to humanities data to improve our understanding of complex cultural-historical phenomena such as the role of advertising in society. ",Tracking the Consumption Junction: Temporal Dependencies between Articles and Advertisements in Dutch Newspapers,,2020,,Digital Humanities Quarterly,014,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2306,,"{'string_id': None, 'id_scheme': None}"," The Palatine Anthology (PA) Project, coordinated by the Canada Research Chair on digital textualities directed by Marcello Vitali-Rosati, collaborates with several international partners, including Italian and French schools, in order to establish a collaborative critical digital edition and a multilingual translation of all the PA's epigrams. In particular, our project aims to further develop this edition of the PA, thereby demonstrating the philological, editorial and pedagogic challenges involved in compiling the diverse fragments of this collection of Greek epigrams. Since its discovery in 1606 by Claude Saumaise in the Palatina Library of Heidelberg in Germany , the PA manuscript (Codex Palatinus 23) has considerably influenced literature and art. As we know it today, the Anthology is the result of successive compilations, modifications, additions, and rearrangements by the compilers. Meleager's collection is a collection of epigrams compiled in the first century B.C., which represents the original source of what is known today as the Greek Anthology . This collection, called The Garland, was not randomly arranged, but according to a series of particular organizational principles . The PA challenges the concept of an oeuvre – as a unified and cohesive body of work –, insofar as it brings together 4 000 epigrams written by more than one hundred different authors from over sixteen centuries of literary production (from the Byzantine empire to the 10th century AD). If we cannot consider the Anthology as a unified and cohesive work, how do we account for it in an edition? To these questions, the digital environment presents tools and possibilities allowing us to organize our research, and ultimately work towards finding some answers. In an attempt to provide tangible solutions to the difficulty of assembling such a fragmented body of work the PA Project harnesses digital tools. We have created an open database searchable via an API that allows one to transcribe the manuscript, propose translations, align translations, transcribe scholia, and link epigrams both to each other and to external literary and artistic references. We aim to demonstrate the importance of philological approaches to texts; redefine the boundaries between scholarly and amateur practices; connect contemporary readers and scholars with Classical texts; supplement the Perseus project; and harness the potential of semantic web technologies. We aim to shed light on the many ways to engage with textual objects, to conceive of a multiple reception of the anthological imaginary . The interface of our digital platform does intend for users to propose such reading pathways and weak ties , because it enables them to associate an epigram with a reference (textual, iconographic, musical, cinematographic, and others) and thereby demonstrate a collective engagement with the epigram . This demonstrates how collective imaginaries are able to enrich our understanding of the anthological material. By enabling the users to engage with this otherwise elusive literary object, the Greek epigrams, and with a cultural object, our project enables contemporary readers to engage with the digital possibilities in order to visualize a collective imaginary or topoï , and to contribute to philological research on the origins and influences of the PA . ",Editorializing the Greek Anthology: The palatin manuscript as a collective imaginary,,2020,"['Digital Classics', 'Digital Edition', 'Palatine Anthology', 'Anthological Imaginary', 'Editorialization', 'Collaborative Process']",Digital Humanities Quarterly,014,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2307,,"{'string_id': None, 'id_scheme': None}"," This article investigates the topic labeling system of a widely used full-text academic publication database, JSTOR, particularly in reference to colonial North American history scholarship. Using insights developed by critical algorithm and critical archival studies, it analyzes how JSTOR’s topics repeatedly misrepresent and erase work in women’s, African diasporic/African American, and Native American and settler colonial histories. The article discusses concerns over the power of metadata, the need for transparent and domain-expert-involved indexing processes, and digital providers’ responsibilities to accurately categorize scholars’ work. It particularly focuses on the potentially disproportionate harm done to traditionally marginalized fields of study through seemingly racist or sexist topical labeling that impedes knowledge discovery. ","Erasure, Misrepresentation and Confusion: Investigating JSTOR Topics on Women’s and Race Histories",,2020,,Digital Humanities Quarterly,014,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2308,,"{'string_id': None, 'id_scheme': None}"," Semantic Web technologies provide the ability to more effectively connect and integrate structured data by disclosing their intended meaning and therefore making explicit their description, context and provenance. Thanks to their nature, Semantic Web technologies have produced insights into the challenges associated with standardizing metadata for manuscripts. Scholars depend on highly specific catalogue records in order to understand a manuscript and raise research questions which take into account either its physicality or its nature of evidence for all aspects of life in the medieval period. However, the heterogeneity of codicological-paleographical records in terms of metadata and terminology is weakening the integration and interoperability within the community. Ontologies in particular have been evaluated as a clever approach towards a better communication. The employment of knowledge representation in the field of medieval manuscript descriptions is still narrow, though. Against the state of the art, this paper attempts to add some evidence: it analyses the impact of a top-level ontology designed for modelling cultural objects, namely CIDOC Conceptual Reference Model, on a set of fifteen Italian manuscript descriptions encoded in TEI standard using the online platform ManusOnLine. In this case study, the semantics underlying the selected TEI XML-encoded manuscript descriptions were visually explored according to a subset of CIDOC CRM’s classes and properties. In doing so, the process of the creation of a manuscript was split and analysed in its internal phases in accordance with the CIDOC CRM event-modelling principle. Within the dataset, tags as <date> and <name> are used sporadically to encode factual information; however, this case study shows that events and relations can be generally deduced from the XML structure of a manuscript description, although they are not expressively identified. Overall, the analysis demonstrates that CIDOC CRM can represent a valuable aid to overcome manuscripts-related issues — i.e. granularity, contradictory knowledge and terminology — and potentially create an interlinked data platform which could greatly enhance the study of human culture. ",Medieval manuscript descriptions and the Semantic Web: analysing the impact of CIDOC CRM on Italian codicological-paleographical data,,2020,,Digital Humanities Quarterly,014,01,"[{'value': '1938-4122', 'type': 'electronic'}]"
2309,,"{'string_id': None, 'id_scheme': None}"," In this paper we run a topic model on over 300 article-length pieces from the extended bibliography of Melissa Terras, Juliette Nyhan, and Edward Vanhoutte’s edited collection Defining Digital Humanities. We use this topic model as a way to think through entry into the digital humanities as a negotiation between warm invitation and gatekeeping, the pull and push of digital humanities. We then analyze the metadata we collected about these pieces to explore how the push and pull manifest themselves unevenly across different demographics. ",The Push and Pull of Digital Humanities: Topic Modeling the What is digital humanities? Genre,,2020,,Digital Humanities Quarterly,014,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2310,,"{'string_id': None, 'id_scheme': None}"," Optical Character Recognition (OCR) on contemporary and historical data is still in the focus of many researchers. Especially historical prints require book specific trained OCR models to achieve applicable results . To reduce the human effort for manually annotating ground truth (GT) various techniques such as voting and pretraining have shown to be very efficient . Calamari is a new open source OCR line recognition software that both uses state-of-the art Deep Neural Networks (DNNs) implemented in Tensorflow and giving native support for techniques such as pretraining and voting. The customizable network architectures constructed of Convolutional Neural Networks (CNNS) and Long-Short-Term-Memory (LSTM) layers are trained by the so-called Connectionist Temporal Classification (CTC) algorithm of Graves et al. (2006). Optional usage of a GPU drastically reduces the computation times for both training and prediction. We use two different datasets to compare the performance of Calamari to OCRopy, OCRopus3, and Tesseract 4. Calamari reaches a Character Error Rate (CER) of 0.11% on the UW3 dataset written in modern English and 0.18% on the DTA19 dataset written in German Fraktur, which considerably outperforms the results of the existing softwares. ",Calamari − A High-Performance Tensorflow-based Deep Learning Package for Optical Character Recognition,,2020,,Digital Humanities Quarterly,014,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2311,,"{'string_id': None, 'id_scheme': None}"," In spite of the increasingly large textual datasets humanities researchers are confronted with, and the need for automatic tools to extract information from them, we observe a lack of communication and diverging goals between the communities of Natural Language Processing (NLP) and Digital Humanities (DH). This contrasts with the wealth of potential opportunities that could arise from closer collaborations. We argue that more efforts are needed to make NLP tools work for DH datasets so that that NLP research applied to humanities data receives more attention, leading to the development of evaluation approaches tailored towards relevant research questions. This has the potential to bring methodological advances to NLP, while at the same time confronting DH datasets with powerful state-of-the-art techniques. ",Digital Humanities and Natural Language Processing: Je t’aime... Moi non plus,,2020,,Digital Humanities Quarterly,014,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2312,,"{'string_id': None, 'id_scheme': None}"," Digital editions are easily modified after they are first published — a state of affairs that poses challenges both for long-term scholarly reference and for various forms of electronic distribution and analysis. This article argues that producers of digital editions should assign meaningful version numbers to their editions and update those version numbers with each change, allowing both humans and computers to know when resources have been modified and how significant the changes are. As an examination of versioning practices in the software industry reveals, version numbers are not neutral descriptors but social products intended for use in specific contexts, and the producers of digital editions must consider how version numbers will be used in developing numbering schemes. It may be beneficial to version different parts of an edition separately, and in particular to version the data objects or content of an edition independently from the environment in which it is displayed. The article concludes with a case study of the development of a versioning policy for the Piers Plowman Electronic Archive, and includes an appendix surveying how a selection of digital editions handle the problem of recording and communicating changes. ",Digital Editions and Version Numbering,,2020,,Digital Humanities Quarterly,014,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2313,,"{'string_id': None, 'id_scheme': None}"," This essay presents quantitative capture and predictive modeling for one of the largest and longest running mass reading programs of the past two decades: One Book One Chicago (OBOC) sponsored by the Chicago Public Library (CPL). The Reading Chicago Reading project uses data associated with OBOC as a probe into city-scale library usage and, by extension, as a window onto contemporary reading behavior. The first half of the essay explains why CPL’s OBOC program is conducive for modeling purposes, and the second half documents the creation of our models, their underlying data, and the results. ",Reading Chicago Reading: Quantitative Analysis of a Repeating Literary Program,,2020,,Digital Humanities Quarterly,014,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2314,,"{'string_id': None, 'id_scheme': None}"," Treating collections in cultural institutions as data encourages novel approaches to the use of historic collections. To reframe collections as data is to focus on how digitized collection material, collection metadata, and transcriptions can be used and reused for various types of computational analysis. Scholars active in the field of digital humanities have long taken advantage of computational data. This paper focuses on the work of cultural heritage institutions, which are increasingly offering collections as data. This paper outlines the collections as data project and examines specific examples of cultural institutions active in this space. The paper then details the practices of data brokers, and explores how the data broker model can frame the use of data in cultural heritage institutions. In closing a number of experiments are described that might help mitigate the harm that data in cultural institutions might cause. As we create and share data, can we be sure we are better than data brokers? ",Open Data in Cultural Heritage Institutions: Can We Be Better Than Data Brokers?,,2020,,Digital Humanities Quarterly,014,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2315,,"{'string_id': None, 'id_scheme': None}"," Lincoln/Net (http://lincoln.lib.niu.edu), a product of the Abraham Lincoln Historical Digitization Project at Northern Illinois University Libraries, represents a new type of historically oriented digital library resource. Like many other digital libraries, it contains a large amount of searchable primary source materials. Like a number of other historically oriented online resources, project staff have organized Lincoln/Net around a specific topic, in this case Abraham Lincoln’s life and times in antebellum Illinois. In addition to Lincoln’s own papers, the project’s databases contain resources shedding light on his context, including letters, diaries, and publications prepared by his peers. Unlike most historically oriented digital libraries however, the project Web site also includes a wealth of multimedia materials, including image, sound, video, and interactive map resources. But Lincoln/Net is perhaps most unique in that it furnishes its users with an extensive set of interpretive materials. This approach suggests that historians may play an expanding role in the development of digital libraries. It can also provide them with a badly-needed means of communicating with an audience beyond their own scholarly community and students. This communication can facilitate what one digital history pioneer has described as the democratization of history, as defined by an expanded user group enjoying primary source materials and using them to engage in historical thinking . "," Webs of Significance: The Abraham Lincoln Historical Digitization Project, New Technology, and the Democratization of History",,2007,,Digital Humanities Quarterly,001,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2316,,"{'string_id': None, 'id_scheme': None}"," We describe the development of web-based software that facilitates large-scale, crowdsourced image extraction and annotation within image-heavy corpora that are of interest to the digital humanities. An application of this software is then detailed and evaluated through a case study where it was deployed within Amazon Mechanical Turk to extract and annotate faces from the archives of Time magazine. Annotation labels included categories such as age, gender, and race that were subsequently used to train machine learning models. The systemization of our crowdsourced data collection and worker quality verification procedures are detailed within this case study. We outline a data verification methodology that used validation images and required only two annotations per image to produce high-fidelity data that has comparable results to methods using five annotations per image. Finally, we provide instructions for customizing our software to meet the needs for other studies, with the goal of offering this resource to researchers undertaking the analysis of objects within other image-heavy archives. ",Crowdsourcing Image Extraction and Annotation: Software Development and Case Study,,2020,,Digital Humanities Quarterly,014,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2317,,"{'string_id': None, 'id_scheme': None}"," The Digital Prosopography of the Roman Republic (DPRR) project has created a freely available structured prosopography of people from the Roman Republic. As a part of this work the materials that were produced by the project have been made available as Linked Open Data (LOD): translated into RDF, and served through an RDF Server. This article explains what it means to present the material as Linked Open Data by means of working, interactive examples. DPRR didn't do some of the work which has been conventionally associated with Linked Open Data. However, by considering the two conceptions of the Semantic Web and Linked Open Data as proposed by Tim Berners-Lee one can see how DPRR's RDF Server fits best into the LOD picture, including how it might serve to facilitate new ways to explore its material. The article gives several examples of ways of exploiting DPRR's RDF dataset, and other similarly structured materials, to enable new research approaches. ",A Prosopography as Linked Open Data: Some Implications from DPRR,,2020,"['Roman Republic', 'Linked Open Data', 'Resource Description Framework', 'Digital Prosopography']",Digital Humanities Quarterly,014,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2318,http://computerphilologie.digital-humanities.de/jg00/christma/christma.html,"{'string_id': None, 'id_scheme': None}","Much praised and much criticized but certainly unique: The forming of the Deutsches Wörterbuch (DWB) by Jacob and Wilhelm Grimm took far more than one hundred years and engaged generations of lexicographers who have contributed a total of approximately 350.000 entries to be collected in 67.744 dictionary columns that were published in sixteen volumes (32 sections). The DWB comprises the most thorough account of the German language and reflects a period of more than one hundred years of lexicographical, cultural, institutional, and also political history; thus it proves indispensable for research and teaching. It marks a further step in the history of this dictionary, to make it available in the electronic medium, and to recognize and encode its heterogeneous structures systematically. Since November 1998 a project team at the University of Trier prepares a computerized version of the DWB to be published on CD-ROM and also be made available via Internet. Accessibility of this outstanding dictionary will be improved thereby enlargening its user community independent of libraries' opening hours. The adaptation of the dictionary's contents will be according to international standards: The use of SGML determined by the TEI-guidelines will support the aim to obtain new possibilities in its usage: A markup of the dictionary's contents taking into account the specific demands of dictionary structures enables to provide selective access to various positions of each dictionary entry, offering far wider possibilities of dictionary usage than conventional full-text search requests can supply. Systematic analysis of quotations and its references will be ensured by interlinking those with the DWB's index volume. After a brief outline of the DWB's structure, history, and value, this paper stresses upon following the steps to a computerized version of the DWB. The stages from data input, correction procedures, and automatic markup according to SGML standards to the provision of a graphical user interface will be discussed, emphasizing on the difficulties arising from the dictionary's hetergeneous structure and contents.",EIN »HAUSBUCH«[1] FÜR ALLE? - DAS DEUTSCHE WÖRTERBUCH DER BRÜDER GRIMM AUF CD-ROM UND IM INTERNET,,2000,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2319,http://computerphilologie.digital-humanities.de/jg00/maxeibl/maxeibl2.html,"{'string_id': None, 'id_scheme': None}","This article describes ergonomical aspects of hypermedia systems. First, the basic concepts of hypertext, multimedia and hypermedia are defined and discussed. Then the main problems of hypertext systems, such as navigation, orientation, and search, and their possible solutions are discussed. Different methods are presented and evaluated according to their usability. The article closes with a short introduction to usability problems of different modalities.","HYPERTEXT, MULTIMEDIA, HYPERMEDIA: ERGONOMISCHE ASPEKTE",,2000,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2320,http://computerphilologie.digital-humanities.de/jg00/landfest/landfest.html,"{'string_id': None, 'id_scheme': None}","This essay suggests a website edition of Bettine von Arnims complete works, including all her correspondances, concepts and other manuscript material. Crucial arguments in favor of such an edition are first von Arnim's constant interweaving of published books and private correspondances which lent her writings their specific network structure; second the problem of handling the sources, von Arnim's papers partly having disappeared after an auction in 1929 and the rest being scattered wide. To give plausibility to these argu-ments the essay sketches the works' net structure as well as the state of the manuscript sources.",FASELEI ONLINE. VORÜBERLEGUNGEN ZU EINER INTERNET-PUBLIKATION VON BETTINE VON ARNIMS WERK,,2000,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2321,http://computerphilologie.digital-humanities.de/jg00/richter/richter.html,"{'string_id': None, 'id_scheme': None}","The project of a new, historical-critical edition of the entire corpus of letters written by Goethe has been a desideratum of German Studies research for decades. A total of more than 14.700 letters by Goethe to more than 1400 different correspondents, excluding official correspondence, have survived. Various particulars in part four of the Weimar Edition (which appeared between 1887 and 1912 and contains the only thorough academic edition of Goethe letters to date) have to be revised. More than 2.000 letters were published incompletely, while about 300 letters are not yet available in print. Besides a full presentation of the letters, there are two other strong arguments in favour of a revised and enlarged edition: improving both the chronology and the annotation of the letters in the Weimar Edition. As an essential preparatory work transmissions of manuscripts and prints of all Goethe letters were brought together in a database. In this complete index are noted all manuscripts, drafts, and contemporary copies of Goethe letters which have survived. It comprises over 15.200 records. In addition to the Goethe and Schiller Archives, where about 20.000 manuscripts of Goethe letters, including drafts and copies, are held, about 200 other libraries, archives, and private owners all over the world were contacted for materials. The database for the electronic index of transmissions of all Goethe letters is published on the internet now and is accessible to all users.",ZUR INTERNETVERÖFFENTLICHUNG. DER DATENBANK: JOHANN WOLFGANG GOETHE. REPERTORIUM SÄMTLICHER BRIEFE 1764-1832. HG. V. DER STIFTUNG WEIMARER KLASSIK/GOETHE- UND SCHILLER-ARCHIV,,2000,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2322,http://computerphilologie.digital-humanities.de/jg01/bein2.html,"{'string_id': None, 'id_scheme': None}",This feature is a summary of two round table talks conducted for several hours on 2 and 22 August 2001 at the Institute of German Literature of RWTH Aachen. The goal of these talks was to identify what practical consequences new computer and Internet based technologies have on research and teaching in the field of Medieval German studies and what requirements different university groups have for the future use of the new media and technologies. The talks deliberately set aside the field of specialist programming or special hardware configurations. The focus was to be on application. (Translation by Alison Dobson-Ottmers.),PC UND INTERNET IN ALTGERMANISTISCHER. FORSCHUNG UND LEHRE. ERFAHRUNGEN UND DESIDERATE,,2001,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2323,http://computerphilologie.digital-humanities.de/jg01/oberhofer.html,"{'string_id': None, 'id_scheme': None}",Distributed learning environments and content often lack common basis for the exchange of learning materials. This delays or even hinders both innovation and delivery of learning technology. Standards for platforms and authoring may provide a way to improve interoperability and cooperative development. This article provides an XML-based approach to this problem promoted by the IMS Global Learning Consortium.,XML-STANDARDS ALS MOTOR. FÜR DEN AUSBAU VIRTUELLER LEHRANGEBOTE,,2001,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2324,http://computerphilologie.digital-humanities.de/jg01/reinhardt-becker.html,"{'string_id': None, 'id_scheme': None}","The following paper deals with the development of a hypertext-study program for personal use, connected with Jochen Vogts 1999 textbook Einladung zur Literaturwissenschaft (Invitation to Literary Studies), which is used for first year-classes of German literature in several universities home and abroad. Our hypertext-program is available in the World Wide Web and basically provides substantial information on authors (both literary and critical) and literary terms (in example genres, literary history, styles, theory) mentioned in the textbook. In addition, short original texts represent the most important critical schools. A system of links gives every user the opportunity to find her or his own path through the jungle of literary facts, concepts and terms. My paper tries to trace mainly the discussions and decisions in our working team, mostly of graduate students, to discuss the didactic possibilities of the hypertext, and to summarize the responses of users both in Essen and worldwide to our work-in-progress.",EINLADUNG ZUR LITERATURWISSENSCHAFT[1]. DAS ESSENER INTERNET-VERTIEFUNGSPROGRAMM ZUM SELBSTSTUDIUM. EIN ERFAHRUNGSBERICHT,,2001,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2325,http://computerphilologie.digital-humanities.de/jg01/rohmer.html,"{'string_id': None, 'id_scheme': None}","The World Wide Web offers a lot of information on poets and literary texts, which may motivate people in continuing and intensifying their reading. The web-catalogue Erlanger Liste – Germanistik im Internet <http://www.erlangerliste.uni-erlangen.de> (4.11.2001), where internet resources on german literature are listed and commented, could help to find such places in the web. In doing so, the list also takes part in the renewal of a cultural memory under the sign of the new media. Literary science should observe both: the way, how readers present their reading in the net, but also the consequences for the literary tradition.",LINKLISTE UND LESELUST. MÖGLICHKEITEN DER LITERATURVERMITTLUNG IM INTERNET,,2001,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2326,,"{'string_id': None, 'id_scheme': None}"," For over a thousand years, Tibet has preserved and translated ancient Buddhist Sutras from India, keeping the tradition of Buddhist philosophy and meditation alive long after it died out in India by the 12th Century. Recent efforts to digitize materials from this textual tradition offer opportunities to broaden the circulation of rare materials to the exiled Tibetan scholarly community, but also suggest conceptual challenges arising from the complexity of the texts and their inherently multimodal character. This paper describes the scholarly and meditative traditions from which these texts come, and discusses possible approaches to their digitization. ",Encoding for Endangered Tibetan Texts,,2007,,Digital Humanities Quarterly,001,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2327,http://computerphilologie.digital-humanities.de/jg01/doerr.html,"{'string_id': None, 'id_scheme': None}","The article gives an overview about digitization activities from a German prospective. The setup of a specialized funding program for retrospective digitization of library holdings by the Deutsche Forschungsgemeinschaft in 1997 played an important role for inciting and promoting digitization projects. Giving direct access to broadly used research and teaching materials or facilitating the use of rare and precious documents are some of the most frequent selection criterias. The main lines of digitization activities of the Bavarian State Library are given as an example for building a digitization profile which is reflecting the missions of the library. Further American and French initiatives are cited in order to illustrate digitization programs of much greater dimensions. The second part of the article refers to the basic technical questions (scanners, resolution, text encoding) and also mentions some copyright issues.",RETRODIGITALISIERUNG IN BIBLIOTHEKEN UND WISSENSCHAFTLICHEN EINRICHTUNGEN,,2001,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2328,http://computerphilologie.digital-humanities.de/jg01/kocher-boehler.html,"{'string_id': None, 'id_scheme': None}","Departing from an aphorism by G. Ch. Lichtenberg - that words stand still while the world designated by them moves on and that therefore words ought to be reexamined periodically for their adequacy vis-à-vis the new realities - the essay explores the term ›Spiel‹ (›play‹/›game‹) for its possible potential as a key-concept linking post-kantian Aesthetics, Gadamer’s Hermeneutics and Iser’s Reader-Response Criticism as analytical tools for the new artistic genres of electronic hyperfiction and computer games. Tracing the history of the term ›Spiel‹ as it oscillates between an abstract figurative meaning as a purely mental interplay between the different cognitive faculties and its reference to concrete performative acts in a leisurely purposeless mood the authors show their branching out into an outright dichotomy in the post-kantian Aesthetics and they postulate the re-convergence of the two meanings in the new artistic forms of literary hyperfictions and computer games.","ÜBER DEN ÄSTHETISCHEN BEGRIFF DES SPIELS ALS LINK ZWISCHEN TRADITIONELLER TEXTHERMENEUTIK, HYPERFICTION UND COMPUTERSPIELEN",,2001,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2329,http://computerphilologie.digital-humanities.de/jg01/tiutenko.html,"{'string_id': None, 'id_scheme': None}","A look from the viewpoint of the linguistic theory of text, as in this article, may result in reconsideration of some key-ideas common to many non-linguistic publications about hypertext. Since hypertext is frequently thought of as a primarily textual phenomenon (single text or system of texts), the authors of this article search for its linguistic definition based upon the knowledge about text production and reception that was gathered by many researchers in text linguistics during past four decades. A genuine concept of hypertext developed by computer science in 1960s-1980s serves in this case as a framework for the discussion. The article shows that verbal components of hypertext have to satisfy the general textuality criteria to become texts. Those criteria, however, clash with the notion of hypertext as a kind of non-constrained text and make the philosophical vision of an unlimited, universal hypertext unrealistic.",TEXT IM HYPERTEXT. EIN TEXTLINGUISTISCHER BLICK AUF DIE INFORMATIONSGESTALTUNG IM INTERNET,,2001,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2330,http://computerphilologie.digital-humanities.de/jg01/rommel.html,"{'string_id': None, 'id_scheme': None}","Advanced Computing in the Humanities (ACO*HUM) was a SOCRATES/ ERASMUS thematic network designed to analyse the state of humanities computing in Europe. Experts from different countries and with different fields of expertise compiled a survey on the influence of humanities computing at European universities. The methodological implications of formal methods in the humanities and the different approaches to teaching advanced computational skills provide valuable insights into the different attitudes towards education and in particular humanities education at universities in Europe. The revision of curricula and the implementation of humanities computing centres seems a necessary development in the information society of the twenty-first century; here ACO*HUM provides a knowledge base for students, teachers, and administrators.",EUROPA UND DIE NEUEN MEDIEN. ACO*HUM EVALUIERT DIE GEISTESWISSENSCHAFTEN,,2001,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2331,http://computerphilologie.digital-humanities.de/jg01/steding.html,"{'string_id': None, 'id_scheme': None}","Many paper based scholarly editions struggle with the limitations of paper and the typographic means of expression. They are difficult to use, potentially incomplete, and in many cases they do not meet the informational needs and expectations of the users. Digital editions offer serious solutions to many of these problems. They should not be seen simply as ›improved‹ editions, but as a new form of philological data storage. The computer allows new types of scholarly editions and new methods for accessing the data. It therefore is no longer necessary to offer all scholarly editions in print.",WARUM NOCH DRUCKEN? ÜBER DIE NACHTEILE GEDRUCKTER EDITIONEN,,2001,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2332,http://computerphilologie.digital-humanities.de/jg02/mccarty.html,"{'string_id': None, 'id_scheme': None}","The question of academic legitimacy for humanities computing is argued from an heuristic, experimental perspective. The basic approach is to privilege what happens in actual practice, to observe the commonalities and generalize from them to a picture of the interrelations between the field and the non-technical disciplines of the humanities. These generalizations also lead to discovery of intellectual kinships with several traditional fields of study: philosophy and history (including the history and philosophy of science), ethnography, sociology, literary criticism and so on. The essay argues that in these kinships are to be found the basic questions with which we may build a computing that is of as well as in the humanities.",NEW SPLASHINGS IN THE OLD POND: THE COHESIBILITY OF HUMANITIES COMPUTING,,2002,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2333,http://computerphilologie.digital-humanities.de/jg02/meister2.html,"{'string_id': None, 'id_scheme': None}","The article's point of departure is a clarification of the principle conditions that would have to be fulfilled by Computer Philology in order to qualify as a discipline. This is followed by reviewing steps taken with a view to the possible institutionalization of Computer Philology at Hamburg University. The above theoretical considerations, as well as the reference to the practical example will form the backdrop for reviewing three articles presented in recent years which have aimed to answer the question »What is Computer Philology?«. The contributions under discussion are seen to differ mainly with regard to the implications which they hold for Computer Philology's development into a discipline proper. The article's conclusion is that we ought to »Think Big» in Computer Philology and aspire to meet the criteria for disciplinarity which are considered an important scholarly benchmark.",»THINK BIG»: DISZIPLINARITÄT ALS WISSENSCHAFTSTHEORETISCHE BENCHMARK DER COMPUTERPHILOLOGIE,,2002,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2334,http://computerphilologie.digital-humanities.de/jg02/orlandi.html,"{'string_id': None, 'id_scheme': None}","At the beginning, the article mentions the results of an enquiry on how humanities computing is being introduced into the curricula of the European universities; and the most important topics recently discussed around the theme of a theory of humanities computing. It appears that most experts agree on the opinion that humanities computing is an independent discipline, and as such it should be introduced into the faculties of humanities. The article then explains how the foundation of the discipline should be understood, on the basis of computing theory and the methodology of the different humanities disciplines.",IS HUMANITIES COMPUTING A DISCIPLINE?,,2002,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2335,http://computerphilologie.digital-humanities.de/jg02/rockwell.html,"{'string_id': None, 'id_scheme': None}","While we are tired of discussing whether humanities computing is a discipline or an interdisciplinary field, it is still worth looking at what it is to become a discipline. Starting with a discussion of disciplinarity I argue that when a field begins to control the means of its academic reproduction through programmes it becomes a discipline. This founding moment of rupture with the surrounding disciplines should not be overlooked, nor should the dangers inherent in the defining of a field that happens through formalization in educational structures. In multimedia, in particular, there is a rupture around the place of the crafts of digital creation that places us in conflict with ideas about the liberal and servile (applied) arts. This will be the contribution of the digital humanities – a reorientation to craft and creative art in the humanities. There is no more stunning fact about the academic profession anywhere in the world than the simple one that academics are possessed by disciplines, fields of study, even as they are located in institutions. With the growth of specialization in the last century, the discipline has become everywhere an imposing, if not dominating, force in the working lives of the vast majority of academics. A discipline is born when a field takes control of its means of reproduction, specifically the ability to produce ›disciples‹ or students. The founding of a discipline is a rupture in the existing structure of institutions, one which is outside the founded discipline, being the condition of its becoming. The founding of a discipline is a liminal moment that defines the relationship between the discipline and those surrounding institutions. Humanities computing (HC) is becoming a discipline as it struggles to understand itself, struggles with existing disciplines and finds a new place within institutions. This paper will look at the emergence of humanities computing programmes as the founding of a discipline and the possibilities for rupture.","MULTIMEDIA, IS IT A DISCIPLINE? THE LIBERAL AND SERVILE ARTS IN HUMANITIES COMPUTING",,2002,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2336,http://computerphilologie.digital-humanities.de/jg02/unsworth.html,"{'string_id': None, 'id_scheme': None}","I'll give the short answer to the question »what is humanities computing?« up front: it is foreshadowed by my two epigraphs. Humanities computing is a practice of representation, a form of modeling or, as Wallace Stevens has it, mimicry. It is also (as Davis and his co-authors put it) a way of reasoning and a set of ontological commitments, and its representational practice is shaped by the need for efficient computation on the one hand, and for human communication on the other. We'll come back to these ideas, but before we do, let's stop for a moment to consider why one would ask a question such as »what is humanities computing?«",WHAT IS HUMANITIES COMPUTING AND WHAT IS NOT?,,2002,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2337,http://projects.chass.utoronto.ca/chwp/CHC2003/Sinclair2.htm,"{'string_id': None, 'id_scheme': None}","This article is a general description of the conception and development of SAToRBase, an online database tool that contains some 3000 occurrences of topoi (recurring motifs) in French literature from about 1200 to 1800. SAToRBase has been developed expressly for the Société d'Analyse de la Topique Romanesque (SAToR), an international literary association. The purpose of this article is not to examine in detail each aspect of SAToRBase; that would be futile for an online tool of this kind which is in constant evolution. Rather, the purpose is to present more broadly the major characteristics of SAToRBase, including a brief description of typical procedures such as searching for occurrences of topoi and proposing new occurrences. The interface of SAToRBase is likely to change considerably over time, but the core concepts will no doubt remain stable.",SAToRBase: A Database of Topoi for French Literature,,2003,"[""Société d'Analyse de Topiques Romanesques (SAToR), topoi, French literature, literary databases""]",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2338,,"{'string_id': None, 'id_scheme': None}"," Recent efforts to reconceptualize text analysis with computers in order to broaden the appeal of humanities computing have invoked the example of the Oulipo. Although there are similarities between the activities of the Oulipo and the new approach to computer-assisted literary analysis, the development of tools for the express purpose of encouraging scholars to play with texts does not follow the model of Oulipian research into potentialities. For the Oulipo, potential text analysis is less a question of interpreting literature than of supplying algorithms for the good use one can make of reading. Producing exemplary interpretations with algorithms is a secondary consideration. Oulipian constraints are better understood as toys with no intended purpose rather than as tools we use with some objective in mind. The procedures for making sense of texts provide for their own interpretation: they are not only instruments for discovering meaning but also reflections on making meaning. ",Reading Potential: The Oulipo and the Meaning of Algorithms,,2007,,Digital Humanities Quarterly,001,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2339,http://computerphilologie.digital-humanities.de/jg02/boesken.html,"{'string_id': None, 'id_scheme': None}","The past decade has seen the rise of quite a few new technologies. The computer and the internet have undoubtedly reached the status of mass media by now and discussions have been led about whether this phenomenon will finally force the exodus of ›the book‹ and of ›reading‹ – discussions that are well known from earlier times as in regard to the assumed competition between other electronic technologies, e.g. television, and the print media. This essay aims to take a closer look at these theories and tries to distinguish two different levels of the discussion that tend to be mixed up easily, as on one hand arguments are presented that relate to the medium itself and on the other hand arguments that relate to its products, such as the hypertext. Further to this, the essay aims to oppose the mentioned theories and assumptions with factual reality and examine their validity. It builds up on the results of a representative study investigating the attitude towards media in Germany in 2000 and it concludes that – at the time being – no extrusion of ›the book‹ or ›reading‹ in favour of the use of new technologies can be observed. Moreover, this essay aims to show that the assumed ›added value‹ of the hypertext (compared to linear presentation of printed text) should be studied more carefully. Hypertext does indeed offer some added value, but also bears the risk of disorientation and stress which might transform ›extra value‹ to ›less value‹.",LESEN AM COMPUTER – MEHRWERT ODER MEHR VERWIRRUNG? UNTERSUCHUNGEN ZUR ›KONKURRENZ‹ ZWISCHEN BUCH UND HYPERTEXT,,2002,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2340,http://computerphilologie.digital-humanities.de/jg02/gallet-martinet.html,"{'string_id': None, 'id_scheme': None}","The technology of hypermedia presentation can be developed so as to highlight the new approaches of cultural geography, which emphasise the complexities of townscape perception. Hypermedia programs on urban history offer the options of relating maps and projects to views of built areas, or documents on social history to their cultural background, through interface elements such as windows, animations, interchangeable pictures, which structure the documents as embedded spaces, appearing as sets of options. This interactive presentation, which makes the unfolding of the program dependent on the user's choices, stresses the importance of the virtual as an essential constituent of the perception of cityscape. Historical studies of urban culture in recent years have stressed the complexities of townscape perception, favouring an interdisciplinary approach in which semiotics, including rhetoric and image analysis, are applied to cultural geography, an approach indebted to the ›linguistic turn‹ now followed by the ›visual turn‹. The technology of hypermedia presentation can be developed so as to highlight this new conceptualisation. The epistemology of hypermedia has itself undergone an evolution in the past ten years, since the first theories defined programs by their governing ›metaphors‹ whereas the more recent ones were indebted to ›constructivism‹, stressing the user's role. The argument will primarily be based on the CD-ROM Georgian Cities (authored by the research centre Cultures Anglophones et Technologies de l'Information – CATI, Université Paris-Sorbonne – Paris IV), which was designed so as to demonstrate the potential of hypermedia for urban studies.",HYPERMEDIA AND URBAN CULTURE: A PRESENTATION ON THE CD-ROM GEORGIAN CITIES,,2002,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2341,http://computerphilologie.digital-humanities.de/jg02/rudman.html,"{'string_id': None, 'id_scheme': None}","Non-traditional authorship attribution studies are those attribution studies that make use of the computer, statistics, and stylistics. The hypothesis behind these studies is that an author has a unique and identifiable style. The computer has now become ubiquitous in eighteenth century literary studies and is the main reason why non-traditional authorship studies have advanced to where they are. David Holmes gives a good overview of the field in, The Analysis of Literary Style – A Review.[1] This article surveys a representative sample of authorship studies of eighteenth century literature and gives an exemplum of an ongoing study.",NON-TRADITIONAL AUTHORSHIP ATTRIBUTION STUDIES IN EIGHTEENTH CENTURY LITERATURE. STYLISTICS STATISTICS AND THE COMPUTER,,2002,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2342,http://computerphilologie.digital-humanities.de/jg02/will.html,"{'string_id': None, 'id_scheme': None}","The transcription and electronic edition of Jean Paul's Exzerpthefte (excerpts), initiated and directed by Helmut Pfotenhauer at the University of Würzburg, is a project which was first sponsored by the Fritz Thyssen Foundation and is now supported by the Deutsche Forschungsgemeinschaft. The following article first gives a description of the manuscripts in question and outlines their special character and importance: in 45 years, Jean Paul has written down more than 12000 pages of excerpts, a very curious private encyclopaedia, which he intensively used during the writing of his novels and other texts. In a second part, the article delimits the philological and digital intentions of the edition: it is mainly conceived as a support for future editors of the revised historico-critical edition of Jean Paul's works, who will use it to comment the text and describe the author's writing method. In a third step, the article concentrates upon the digital realisation of the project, especially on the text markup, but also the used markup model designed by the Text Encoding Initiative (TEI) and the software used in different stages of the work (transcription, encoding, retrieval).",DIE ELEKTRONISCHE EDITION VON JEAN PAULS EXZERPTHEFTEN,,2002,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2343,http://computerphilologie.digital-humanities.de/jg03/froemming-etal.html,"{'string_id': None, 'id_scheme': None}","The Internet-database Literary Life reflects the growing interest in interdependencies between literary production, reception and communication in relation to other fields and discourses. It aims to encourage occupation with and research on all aspects of literary life in the German-speaking countries from 1945 up to the present time. Provided is information on a variety of subjects such as literary prizes and exhibitions, literary debates and scandals, forms and aspects of political engagement of writers or the activities of institutions of the literary field. The form of presentation refrains from giving information bound in narrative structures, thus enabling the user to organize, combine and select materials according to the requirements of the particular question asked. The article gives further insight into problems and advantages of the database's design as well as its technical realization.",LITERARISCHES LEBEN. DATENBANK ZU DEN DEUTSCHSPRACHIGEN LäNDERN 1945-2000. Projektbericht http://www.literarischesleben.uni-goettingen.de,,2004,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2344,http://computerphilologie.digital-humanities.de/jg03/mmeier.html,"{'string_id': None, 'id_scheme': None}","Journals are still the principal means by which new scientific knowledge is disseminated. The recent high and fast-rising cost of journals has had a devastating effect on the flow of scientific communication, the research community, and library collections. The following article provides an overview of the forces in the scientific communication market focusing on the particularly difficult situation for journals in the scientific, technical and medical (STM) fields. The impact of rising journal prices is aggravated by the availability of new media – such as Web editions of existing journals. Researchers – whose work is paid for by the university or the federal government – increasingly give away their research to commercial journals, which then charge universities high subscription fees in order to buy it back. The papers that appear in STM journals are typically provided free by academic researchers and are subject to peer review by academic referees and editors, who also receive little if any remuneration. To alleviate this difficult situation, numerous projects that represent a direct and strong competitive alternative to existing high-priced titles in important established STM fields have been launched. This article describes the far reaching transformation of the STM publishing market under way which will also change other scientific professions such as the humanities.",AUTOREN ALS VERLEGER. DIE ZUKUNFT DER WISSENSCHAFTLICHEN KOMMUNIKATION?,,2004,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2345,http://computerphilologie.digital-humanities.de/jg03/morgenthaler2.html,"{'string_id': None, 'id_scheme': None}","Handwritten documents like Gottfried Keller's ›Studien- und Notizbücher‹ (›study and note books‹) demand an edition, which accounts for the special character of text witnesses. It is not the task to transfer the records into ›edited texts‹ only, but to make them visible inside their topographic embedding. The Historisch-Kritische Gottfried Keller-Ausgabe (Historical-Critical Gottfried Keller Edition) complies with these requirements owing to the complete reproduction of the text witnesses and the complement of each hand written page with a diplomatic transcription. The computer edition on CD-ROM, issued with the book, intends to implement appropriately the interaction of hand writing and transcription as well within the electronic media. This specification includes, aside from a word by word parallelisation, also a search option for character strings, which spans the entire corpus of the text witnesses. Thus, the limitation to sole textual and graphic display, which still rules today's electronic media, is overcome.",GOTTFRIED KELLERS STUDIENBÜCHER - ELEKTRONISCH EDIERT,,2004,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2346,http://computerphilologie.digital-humanities.de/jg03/opitz.html,"{'string_id': None, 'id_scheme': None}","With the help of specialists in the relevant fields the Herzog August Bibliothek is engaged in a pilot project concentrating on five genres characteristic of the 17th century – pamphlets, emblem books, almanacs, prayer books and books on the plague and other contagions – in order to develop document type definitions accessible via the internet. These DTDs can be employed in the future as worldwide standardisation tools for the publication and retrieval of the genres concerned. They will be developed on an empirical basis and demonstrated by using a large and representative sample of titles from each genre. We use the Text Encoding Initiative (TEI) and XML to encode text and metadata. The source XML-data is transformed into static HTML by an XSLT-script and thus offered to the public via the internet. The project is supported by the Deutsche Forschungsgemeinschaft (DFG)",DOCUMENT TYPE DEFINITIONS ZUR ERSCHLIEßUNG VON GATTUNGEN DES BAROCK IM INTERNET. EIN PROJEKT AN DER HERZOG AUGUST BIBLIOTHEK WOLFENBÜTTEL,,2004,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2347,http://computerphilologie.digital-humanities.de/jg03/schicketanz-heiligenhaus.html,"{'string_id': None, 'id_scheme': None}","Historical newspapers and journals are invaluable sources for the study of the past. Especially, Jewish periodicals provide a detailed impression of the cultural development of Jewry and shed a quite different light on German history. Yet, it is tremendously difficult to obtain and to investigate these sources: Besides ›normal‹ losses (paper destruction et cetera), a great stock of Jewish newspapers was destroyed under the Nazi regime or are scattered all over the world – a fact, that forces a researcher to expensive stays in different libraries or archives and compels librarians to an extra burden of work. Within the funding program Scientific Library Services and Information Systems of the Deutsche Forschungsgemeinschaft the Aachen Chair of German-Jewish Literary History, the Cologne library Germania Judaica and the Town and University Library Frankfurt/Main carry out a digitization project, which already provides more than twenty representative periodicals, containing about 300.000 images in the WWW. The essay emphasizes the implications and the broad range of problems as well as their solutions in the process of digitizing periodicals. A discussion of the project's technological goals demonstrates the single strategies und procedures to publish large amounts of images, text and bibliographical data in an economic and structured way.",»INSELN IM MEER DES BELIEBIGEN«. ARCHITEKTUR UND IMPLEMENTIERUNG EINES INTERNETPORTALS DEUTSCH-JÜDISCHE PERIODIKA,,2004,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2348,http://computerphilologie.digital-humanities.de/jg03/schreier.html,"{'string_id': None, 'id_scheme': None}","This contribution focuses on pseudo-documentaries on the internet which are regarded as the most recent example of the present-day media trend towards an increasing blurring of the line between fact and fiction. After outlining the tradition of especially audiovisual hybrid media products in which pseudo-documentaries can be seen to stand, various approaches for [distinguishing between fact and fiction are discussed and a three-pronged model for evaluating the reality status of media products is presented. On the basis of this model, pseudo-documentaries are reconstructed as a type of fiction that lacks any clear paratextual signals as to its fictional status; instead, the impression is created (as part of the plot) that product actually constitutes fact. This impression is further supported by a highly realistic production mode that is achieved by presenting a large amount of seemingly authentic material. With respect to content, however, most pseudo-documentaries contain highly implausible as well as topical elements which in turn can function as signals pointing recipients to the fictional status of these products. Content analysis of a random sample of e-mails from internet newsgroups about the pseudo-documentary horror film The Blair Witch Project shows, however, that approximately one third of these recipients are somewhat confused concerning the reality status of the film, basing their evaluation for the most part on information obtained from other media products. Even those recipients who correctly consider the film to be fiction or a hybrid hardly ever make use of the strongest signals to that effect contained in the product, but rely on mediated information instead. These results are supported by a comparison with two companion studies.",PSEUDO-DOKUMENTATIONEN: ZUM VERSCHWIMMEN DER GRENZE ZWISCHEN REALITÄT UND FIKTION IN DEN MEDIEN,,2004,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2349,,"{'string_id': None, 'id_scheme': None}"," Because so little primary historical work has been done on the classic text computer game Colossal Cave Adventure, academic and popular references to it frequently perpetuate inaccuracies. Adventure was the first in a series of text-based games (interactive fiction) that emphasize exploring, puzzles, and story, typically in a fantasy setting; these games had a significant cultural impact in the late 1970s and a significant commercial presence in the early 1980s. Will Crowther based his program on a real cave in Kentucky; Don Woods expanded this version significantly. The expanded work has been examined as an occasion for narrative encounters and as an aesthetic masterpiece of logic and utility ; however, previous attempts to assess the significance of Adventure remain incomplete without access to Crowther's original source code and Crowther's original source cave. Accordingly, this paper analyzes previously unpublished files recovered from a backup of Woods's student account at Stanford, and documents an excursion to the real Colossal Cave in Kentucky in 2005. In addition, new interviews with Crowther, Woods, and their associates (particularly members of Crowther's family) provide new insights on the precise nature of Woods's significant contributions. Real locations in the cave and several artifacts (such as an iron rod and an axe head) correspond to their representation in Crowther's version; however, by May of 1977, Woods had expanded the game to include numerous locations that he invented, along with significant technical innovations (such as scorekeeping and a player inventory). Sources that incorrectly date Crowther's original to 1972 or 1974, or that identify it as a cartographic data file with no game or fantasy elements, are sourced thinly if at all. The new evidence establishes that Crowther wrote the game during the 1975-76 academic year and probably abandoned it in early 1976. The original game employed magic, humor, simple combat, and basic puzzles, all of which Woods greatly expanded. While Crowther remained largely faithful to the geography of the real cave, his original did introduce subtle changes to the environment in order to improve the gameplay. ",Somewhere Nearby is Colossal Cave: Examining Will Crowther's Original Adventure in Code and in Kentucky ,,2007,,Digital Humanities Quarterly,001,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2350,http://computerphilologie.digital-humanities.de/jg04/buettner/buettner.html,"{'string_id': None, 'id_scheme': None}","The integration of the medium Internet into the traditional system of scientific communication is still in a transitional process. New ways of knowledge transfer, different forms of communication as well as information retrieval are yet to be tested. Depending on the different communication cultures among academic disciplines, the focus of interest might vary and should be adjusted to a variety of needs and individual preferences. For the field of historical science, the example of the internet-project historicum.net and its predecessor Server Frühe Neuzeit may illustrate the search for appropriate concepts as well as the problems that arise.",ERFAHRUNGEN MIT DER VIRTUELLEN WISSENSCHAFT. VOM SERVER FRÜHE NEUZEIT ZUM HISTORICUM.NET,,2004,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2351,http://computerphilologie.digital-humanities.de/jg04/duerhub/duerhub.html,"{'string_id': None, 'id_scheme': None}","This paper focuses on the development of the e-journal IASLonline with special emphasis on the technical development of the publication of reviews. The technical editing process up to the publication of the reviews is presented in detail. As electronic publications can greatly increase the efficiency and pace of academic communication, the article argues for an equal treatment of non-profit electronic publications and print publications with regard to printing cost subsidies for publishing houses.",IASLONLINE – ZUM MEDIENWECHSEL. IN DER FACHKOMMUNIKATION,,2005,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2352,http://computerphilologie.digital-humanities.de/jg04/gross/gross.html,"{'string_id': None, 'id_scheme': None}","The Heinrich-Heine-Portal (<http://www.hhp.uni-trier.de>) is a digital edition of Heinrich Heine's (1797-1856) works and letters. It was established in autumn 2002 as a cooperation between the Heinrich-Heine-Institut (HHI), Düsseldorf, and the Competence Centre for Information Retrieval and Electronic Publishing in the Humanities at the University of Trier and is being supported by the Deutsche Forschungsgemeinschaft (DFG) and the Kunststiftung Nordrhein-Westfalen. In addition to a complete, annotated scientific edition of the poet's works and letters, the Heinrich-Heine-Portal presents digital images of his manuscripts held by the Heinrich-Heine-Institut and other literary archives. Access for users is free of charge. The electronic version is based on the two reference editions of Heine: the Düsseldorfer Heine-Ausgabe (DHA) for the works and the Weimar Heine-Säkular-Ausgabe (HSA) for the letters. The edition of the letters is being revised and updated, newly-discovered letters and corrigenda are added. Not only does the portal interlink edited texts, indices, commentaries et cetera and vice-versa, it also provides the electronic facsimiles of the textual transmission in Heine's manuscripts and printed materials (i.e. First Editions, newspaper publications); it thus allows for a close study of all the primary sources still available. In addition, the portal offers digital images of contemporary pictures and paintings, including the portraits of the poet. An extensive bibliography will be added at a later date and kept up-to-date continually by the Heinrich-Heine-Institut. The HHP thus represents a comprehensive platform for research on Heinrich Heine. This paper describes the essential steps of the project from the close analysis of the printed editions and their structural and typographical characteristics, the digitizing process, the encoding principles and the encoding practices by means of TUSTEP (Tübinger System von Textverarbeitungsprogrammen), the markup according to the TEI Guidelines and, finally, the electronic publication by consistently using Open Source software such as the database management system MySQL and the content management system ZOPE.",DER DIGITALE HEINE – EIN INTERNETPORTAL. ALS INTEGRIERTES INFORMATIONSSYSTEM,,2005,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2353,http://computerphilologie.digital-humanities.de/jg04/jaeger.html,"{'string_id': None, 'id_scheme': None}","The Goethezeitportal has been developed on the initiative of a team based at the Institute of German Literature at the university of Munich. It is a tool that can be used for the innovative processing and dissemination of knowledge by applying the possibilities offered by the new media to the cultural and scientific discourse on the most significant phase in the history of German intellectual history between 1770 and 1830. Further information on the concept, mission and current activities of the portal is available below.",DAS GOETHEZEITPORTAL – EIN WERKSTATTBERICHT,,2004,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2354,http://computerphilologie.digital-humanities.de/jg04/michel.html,"{'string_id': None, 'id_scheme': None}","The Virtual Library for German Studies (ViFa Germanistik) is being developed at the University Library of Frankfurt/Main in cooperation with Deutscher Germanistenverband, Deutsches Literaturarchiv Marbach and with a group of German specialists. Intended as a central portal for German Studies, it will offer access not only to the fully integrated OPAC, containing both conventional and electronic media, but also to links with selective qualified information/internet resources. In addition, ViFa Germanistik will integrate a communication site allowing comprehensive orientation on scholarly and organizational matters.",DIE VIRTUELLE FACHBIBLIOTHEK GERMANISTIK – EINE PROJEKTSKIZZE,,2005,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2355,http://computerphilologie.digital-humanities.de/jg04/dimpel.html,"{'string_id': None, 'id_scheme': None}","The following paper offers an introduction to a set of programs called ErMaStat, which is an instrument for textual analysis of Middle High German rhymed epics. With the help of ErMaStat, texts can be analysed with respect to statistical differences, for example in the case of uncertain authorship or if the relative chronology of works of one author is unclear. ErMaStat is the first set of programs in the area of German Medieval Studies which makes it possible to record a large number of different textual features automatically and without any manual input, and subsequently carries out a statistical analysis. The scope of the textual features that can be examined is wide: It ranges from simple quantitative features such as words per line, function words and grammatical phenomena to an automatic analysis of meter. The power of ErMaStat is demonstrated on a number of scholarly issues in German Medieval Literature. It can be shown that the results of the so-called ›schallanalytische Untersuchungen‹ practised by Elisabeth Karg-Gasterstädt are more plausible than hitherto assumed. Another result is that statistical evidence can be presented for the theory that the first thousand lines of Hartmann von Aue's Iwein were written immediately after Hartmann had finished Erec.",TEXTSTATISTISCHE UNTERSUCHUNGEN AN MITTELHOCHDEUTSCHEN TEXTEN,,2004,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2356,http://computerphilologie.digital-humanities.de/jg04/luedeling/ddd.html,"{'string_id': None, 'id_scheme': None}","This paper describes plans for a diachronic corpus of German, which contains texts from Old High German to Modern German. In order to serve as a resource for research questions from many different fields (linguistics, literature, lexicography et cetera) the corpus must have a flexible architecture as well as a high degree of standardization of content. This flexibility is possible through a multi-layer standoff corpus model where the texts are stored in a central database. Standardization is ensured through common tagsets on each annotation level.",DEUTSCHDIACHRONDIGITAL – EIN DIACHRONES KORPUS DES DEUTSCHEN,,2005,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2357,http://computerphilologie.digital-humanities.de/jg04/simon.html,"{'string_id': None, 'id_scheme': None}","In the German copyright law so far the legislator has basically considered the interests of the industry rightholders for a commercial use of digital media during the implementation of the EC Copyright Directive 2001/29/EC. But the public in science and education is not allowed to use the new potentials of the information society as they need them. Six main German science organisations, more than 230 scientific societies and institutions and more than 3100 distinguished scientists and educa-tors have established the Alliance for Copyright in Education and Science to change this unbalanced situation. The aims of this movement for a better copyright law will be explained in this paper: In a digital and networked information society everyone should have access to world-wide information at any time and from everywhere for educational and scientific purposes.",Das Aktionsbündnis Urheberrecht für Bildung und Wissenschaft und seine Ziele,,2005,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2358,http://computerphilologie.digital-humanities.de/jg04/wirth.html,"{'string_id': None, 'id_scheme': None}","This essay aims to sketching out the task of a future hypernarratology by exploring the narrative function of hyperlinks and other hypertext specific ways of building a hyperstructure within the frame of digital literature. The most important assumption for any hypernarratological analysis is that the interference between the propositional content of the textual elements and the meaning, ascribed to the hypertext specific ways of connecting textual elements, is relevant for the mise en scènce, and hence, for the understanding of any hypernarrative. The performative aspect of these hypertext specific ways of connecting textual elements are examined by a close reading of Susanne Berkenheger's Hypertext Hilfe!. In the focus of interest is herebey the narrative function of performative dynamical connections.",»WEM DER GROSSE WURF GELUNGEN«... ANSÄTZE EINER HYPERNARRATOLOGIE. DARGESTELLT ANHAND VON SUSANNE BERKENHEGERS HYPERTEXT HILFE!,,2004,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2359,http://computerphilologie.digital-humanities.de/jg04/wolber.html,"{'string_id': None, 'id_scheme': None}","The Freiburger Anthologie has been founded in 1999 by Klemens Wolber and Jochen Knaus. Originally it was a database of lyric poetry assembling the »most important« German poems of the period between 1720 and 1933. These data had been based on an evaluation of numerous anthologies and have been published on a web site.As part of an interdisciplinary project funded by the Deutsche Forschungsgemeinschaft (DFG) the Freiburger Anthologie is now being enlarged to cover not only poems but also traditional German folksongs and church hymns. A lot of poems and songs exist in various versions that have to be taken into account, and during the course of the work annotations and commentaries are to be added. Digital data processing of the texts and annotations ensure the special features of the website that offers the use of extensive search functions browsing the fields of the database. – This essay describes the development of the project and explains the principles used in editing and annotating lyrical texts. Finally the user numbers and target groups of the Freiburger Anthologie are presented.",DIE ALTE UND DIE NEUE FREIBURGER ANTHOLOGIE – EINE ANTHOLOGISCHE LYRIKDATENBANK,,2005,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2360,,"{'string_id': None, 'id_scheme': None}"," Among the alternative kinds of narrative opened up by computer technology, one of earliest is interactive fiction (and specifically the text adventure or adventure game), which first came into being in the 1970s. Text-based interactive fiction enjoyed a brief period of commercial success in the 1980s until it was overtaken by advances in computer graphics, but it continues to be read and written by enthusiastic amateurs. Although interactive fiction clearly has roots in computer gaming, it also has potential as a new form of literature. Since interactive fiction may be a medium unfamiliar to some readers, this article will start by defining it, explaining some of its conventions, and outlining its origins. It will then describe how one recent piece of interactive fiction, All Hope Abandon, explores aspects of Biblical Studies through the medium of IF, and will end by suggesting a number of fruitful links between interactive fiction and biblical text. In the course of the article several examples from All Hope Abandon will be given, and instructions on how to install and play the game will be provided at the end. ",All Hope Abandon: Biblical Text and Interactive Fiction,,2007,,Digital Humanities Quarterly,001,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2361,http://computerphilologie.digital-humanities.de/jg05/kornstaedt/kornstaedt.html,"{'string_id': None, 'id_scheme': None}","While computer-based searching techniques are well-established for texts, searching techniques for musicological documents are far from being standardized yet. This stems from the significant differences between what is searched, to what these searches pertain and how musical data is represented. The following provides an overview what these differences are and outlines the perspectives for future developments in the filed of computer-based search techniques in musicology.",Computergestützte Suchverfahren in der Musikwissenschaft,,2005,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2362,http://computerphilologie.digital-humanities.de/jg05/muxeneder/muxeneder.html,"{'string_id': None, 'id_scheme': None}","With digitalization and publication of the heterogenous extant sources of the Arnold Schönberg legacy, a multitude of methodically formulated questions are being raised that serve as examples of interest for similar collections. The virtual presentation of the music and text manuscripts, letters, works of visual art, calendars, diaries, historical documents, programms, reviews, first editions, photographs, as well as audiovisual sources brings together both archival and editorial concerns. The attempt has been made since 1998 on the website of the Arnold Schönberg Center in Vienna to bundle the media formatting possibilities of these sources with a variety of technological strategies, so that the user can both reflect on unique things of the legacy, as well as see their contextual relevance. How these intermediate Internet databases, static websites, digital facsimiles, as well as music and spoken word audio files present a synopsis of one of the most unique œuvres of the cultural history of the 20th century is the topic of this contribution. A main focus lies on the presentation of the source and facsimile database of letters as well as on the online Critical Complete Edition of Arnold Schönberg’s Writings.",Archivierung und virtuelle Edition – Arnold Schönbergs Nachlass als offenes Archiv,,2006,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2363,http://computerphilologie.digital-humanities.de/jg05/veit.html,"{'string_id': None, 'id_scheme': None}","From the perspective of an editor who intends to make use of the new digital technologies some of the digital libraries and numerous musical databases founded within the last years (or, in a few cases, even in the last third of the last century) prove to be very helpful for daily work. But nevertheless, digital editions in a narrow sense are very rare in the field of music. Looking for the real reason for the delay of musicology in this field of research one has to admit that music proves to be very resistant to all endeavours of encoding. The mixture of graphical and letter symbols or ›normal‹ words in musical notation and the ambiguity of many musical signs lead to a lot of problems when trying to find a good, comprehensive and lasting representing system – which is the precondition for all sorts of analytical operations and for long-term editions too. Today all editions are done with proprietary software often totally out-dated a few years after publication. So one of the most urgent tasks is to find a standard representing system for music (notation). MusicXML and the XML-Code of the Music Encoding Initiative (MEI) are important steps on the way to a form of encoding which will allow the integration of versions and variants and their future re-visualization on the screen. A further problem is the electronic ›mapping‹ of music facsimiles. (Such facsimiles are of the utmost importance for editorial arguing.) But musicologists share these problems with all scholars dealing with ›written‹ documents, and this should be a cause to take joint action. A pool of digitally accessible sources and editions following the same electronic standards may be a starting point for basic research in musicology in a form which was unthinkable until now.",Musikwissenschaft und Computerphilologie – eine schwierige Liaison?,,2005,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2364,http://computerphilologie.digital-humanities.de/jg05/eskelinen.html,"{'string_id': None, 'id_scheme': None}","If we were to apply probably the most typical intertextual slogan to games and try to model our approach to them highlighting the idea that a game is a mosaic of other games, we are on the verge of seeing what's missing both from video game theories and the actual scholarly practice not to mention methodology. In short, how should we handle the complex co-presences, imitations, implementations, adaptations and transformations between and among different rule systems, goals, quest structures, tasks, challenges and obstacles, player representations, resources, non-playing characters, game worlds, game engines and platforms etc.? This is the conceptual territory this paper tries to begin to chart by crafting and exploring a general game ecology and its multiple relations to other transmedial modes and entertainment ecologies that also shape and affect individual games.","Explorations in game ecology, part 1",,2005,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2365,http://computerphilologie.digital-humanities.de/jg05/furtwae.html,"{'string_id': None, 'id_scheme': None}","This article suggests an analysis of computer and video games based on very general concepts and categories, e.g. causality between events, spatial and temporal structures, to get away from the application of traditional theories on computer games. This is only possible with a strong focus on players’ experiences. In which way does the medium of the game use players’ past experiences and knowledge to complement its own deficient structures?",Mensch-Maschine Computerspiel. Über eine notwendige Paartherapie in der Medienrealität,,2006,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2366,http://computerphilologie.digital-humanities.de/jg04/hamburg.html,"{'string_id': None, 'id_scheme': None}","The present paper introduces central paradigms, selection criteria and special topics for a introductory lecture on Computing in Text-oriented Humanities (CP). The term ›Computing in Text-oriented Humanities‹ (German ›Computerphilologie‹) should be understood as follows: ›Computer-‹ means some basic knowledge of non-technical user-applications while ›text oriented humanities‹ (germ. ›Philologie‹) is addressing those students and scientists dealing with language research themes or texts in any form. The paper is referring to a severalfold held course at the Language Department of the University of Hamburg.",COMUTERPHILOLOGIE IN HAMBURG. THEMATISCHE MODULE EINER COMPUTERPHILOLOGISCHEN GRUNDVORLESUNG,,2005,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2367,http://computerphilologie.digital-humanities.de/jg05/hulle.html,"{'string_id': None, 'id_scheme': None}","To study a work’s creative process and reconstruct the dynamics of the composition history, ›genetic criticism‹ is particularly interested in so-called paralipomena (short jottings, notes, etc.). Because these paralipomena – by definition – do not belong to any specific version, they are often left out of critical editions. In scholarly editing, the notion of ›version‹ is usually defined as a text that already possesses a minimum of syntactic and structural characteristics of the published text. But in a work like James Joyce’s Finnegans Wake, characterized by numerous linguistic distortions, a single word can undergo several stages of transformation even before it is incorporated into a draft or ›version‹ of the text. This necessitates a smaller concept of the ›version‹ (›word version‹ or ›Wortfassung‹ as opposed to ›Textfassung‹). By using a separate XML tag to indicate paralipomena and external reference tags to indicate their relation to previous and subsequent word versions, it is possible to chart the development of verbal transformations and include paralipomena as an integral part of a work’s genesis.",The Inclusion of Paralipomena in Genetic Editions,,2005,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2368,http://computerphilologie.digital-humanities.de/jg05/kehrein/kehrein.html,"{'string_id': None, 'id_scheme': None}","The Digital Wenker Atlas (DiWA <http://www.diwa.info>) is primarily dedicated to the internet publication of 19th century language maps. Using satellite imaging technology, graphic files of about 1.6 GB each are linked to databases and placed at users' disposal via decompression on the fly. DiWA has grown to become a powerful information system that opens up hitherto unknown linguistic research possibilities. After an introduction to the technical aspects of the system and the range of contents, some of DiWA's capabilities are demonstrated in a case study.",Möglichkeiten der computergestützten Regionalsprachen- forschung am Beispiel des Digitalen Wenker-Atlas (DiWA),,2006,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2369,http://computerphilologie.digital-humanities.de/jg05/wittdiewald.html,"{'string_id': None, 'id_scheme': None}","In the mid-1990s, the Faculty of Linguistics and Literary-Studies at Bielefeld University began to establish the field Text technology, both in research and education. Text technology is a new field of research on the border of Computational Linguistics and Computational Philology. This paper focuses on Text technology in academic education. In 2002, Text Technology was introduced as a minor subject for B.A. Programs. It is organized in modules: Module 1 introduces the characteristics of electronic texts and documents, typography, typesetting systems and hypertext. Module 2 introduces one or two programming languages relevant to the field of humanities computing. Markup languages and the principles of information structuring are the main topics of Module 3. The formal fundamentals of computer-based text processing, as formal languages and their grammars, Logics et cetera are subjects of another module. The paper ends with a short description of other Bachelor- and Master-Programs at Bielefeld University which contain text technological themes.",Texttechnologie an der Universität Bielefeld,,2006,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2370,http://computerphilologie.digital-humanities.de/jg05/zoellner-weber.html,"{'string_id': None, 'id_scheme': None}","This paper presents a combination of literary studies and text technology. The goal of this work is to develop a formal representation and description system for literary characters. It should enable users either to search and compare literary characters and to integrate newly analysed characters. Technically, the system for description and representation is based on an ontology. The class system within the ontology is derived using literary character theories and own developed concepts. By using this description system new perspectives of literature and literary studies should be possible.",Formale Repräsentation und Beschreibung von literarischen Figuren,,2006,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2371,,"{'string_id': None, 'id_scheme': None}"," This article maps two divergent trajectories within a narrowly defined sphere of short-form, time-based digital media created between 1995 and 2005. These works are considered in relation to the historical avant-garde - particularly the Structural film movement of the 1960s and 70s - and analyzed as responses to a range of cultural concerns specific to the digital age. The analysis identifies movement toward two terminal points: first, a mode of remix-based montage inspired by open source programming communities and peer-to- peer networks; and second, the emergence of a mode of imaging termed the digital analogue, which foregrounds the material basis of digital production. ",Aporias of the Digital Avant-Garde,,2007,,Digital Humanities Quarterly,001,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2372,http://computerphilologie.digital-humanities.de/jg06/boot.html,"{'string_id': None, 'id_scheme': None}","Robinson and others have recently called for dynamic and collaborative digital scholarly editions. Annotation is a key component for editions that are not merely passive, read-only repositories of knowledge. Annotation facilities (both annotation creation and display), however, require complex software, and if only for that reason it makes sense to share these facilities among edition sites. The paper argues we therefore need a protocol for requesting (and serving) annotations, and a markup vocabulary for storing annotations. The paper proposes a SANE (Scholarly ANnotation Exchange) Request Protocol (SANE-RP) and a SANE Markup Language (SANE-ML) and shows an experimental implementation of these in annotations to a digitised emblem book. The edition (the process that generates the edition HTML from the edition XML source) requests annotation information from an annotation server using SANE-RP. The annotation server serves out the annotations (created using the annotation tool EDITOR) stored in SANE-ML.",A SANE approach to annotation in the digital edition,,2007,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2373,http://computerphilologie.digital-humanities.de/jg06/fanta.html,"{'string_id': None, 'id_scheme': None}","Walter Fanta is the co-editor of the ‚Klagenfurt Robert Musil Edition’ due to appear on November 6th, 2008 in DVD format, of which a twenty-volume edition of the reading text is also planned in book form. In the article the annotated digital Musil edition is offered as a model for historic-critical complete editions and presented for discussion. The particularity of the Austrian author Robert Musil is shown in his most important work, the novel ‚The Man without Qualities’, published by Musil in two volumes (1930 and 1932). For this work Musil left behind more than 12.000 unpublished pages of manuscript which partly contain preliminary sketches (1903-1925) and partly various options for the continuation of the book (1933-1942). In the protracted editing history of this book hitherto this has led to incomplete, one-sided editorial solutions satisfying only a restricted portion of the reading public. The new Klagenfurt Edition wishes to unite all the various approaches and offer a satisfactory solution for all those interested in the literary text, as well as for translators and scholars active in this field, by offering a reading text and in addition more than 15.000 pages of original manuscript and printed matter all connected with hypertext commentaries. Detailed reasons are given why this goal can only be attained by a digital edition: the complex relations in the texts and between the texts can only be depicted identically in this way - through the use of hyperlinks. Three fields of discussion with open and partially controversial questions concern not just Musil alone but rather general questions of digital editing and are still of concern to the editors of the Klagenfurt Edition. These are as follows: 1) The possibilities of text-genetic depiction, reconstruction of the reading text and the presentation of all levels through original documents, in the Klagenfurt Edition both of which are put into effect at the same time. Here the generating of ‚open’ reading texts with a multitude of undecided variants for the continuation of the novel turns out to be a creative game with many options in which the users of the Edition can also be involved. 2) Secondly there is the question of ‚open’ commentary which in the depiction of intertextuality through weblinks always appears to be in need of supplementation and extrapolation. 3) Thirdly the ‚open source’ discussion, the question of independence from commercial systems and the difficulties involved in the long-term-filing of highly complex hypertext systems, still unsolved in the case of the Musil Edition as conceded by the editors.",Die Klagenfurter Ausgabe Robert Musil Historisch-kritisches Edieren am Computer,,2007,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2374,http://computerphilologie.digital-humanities.de/jg06/kegelel.html,"{'string_id': None, 'id_scheme': None}","A large edition project is nowadays bound to investigate means to bridge the gap between computer science and traditional scholarly research. In this paper we give an overview of ways in which the use of structural markup and computer based collation, analysis and presentation has informed the edition of the complete works of the Dutch writer W.F. Hermans.","»A collection, an enormous accumulation of movements and ideas«. Research documentation for the digital edition of the Volledige Werken (Complete Works) of Willem Frederik Hermans",,2007,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2375,http://computerphilologie.digital-humanities.de/jg06/schmitzstark.html,"{'string_id': None, 'id_scheme': None}","The academy-edition of Immanuel Kant’s writings is the general reference edition for philological and philosophical investigation concerning Immanuel Kant. The first three sections (works, correspondence and handwritten remains) have been digitized and serve as the basis for an electronic edition which is freely available at <http://www.ikp.uni-bonn.de/kant>. The fourth section (lectures) is still work in progress. In this paper, we describe the academy-edition, the electronic edition and the computer-assisted edition of the lectures. We explicate problems regarding the arrangement and documentation of Kant’s texts, and – in this context – we explain the importance of hyperlinking texts and other documents for the proper editing and interpretation.",Herstellen von Verknüpfungen – Zur elektronischen Edition und zum elektronischen Edieren von Immanuel Kants Schriften und Vorlesungen,,2007,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2376,http://computerphilologie.digital-humanities.de/jg06/czfrineu.html,"{'string_id': None, 'id_scheme': None}","According to the Berlin Declaration on Open Access in the Sciences and Humanities, which was signed by the President of the Academy, an initiative was founded to provide a sustainable, interactive and transparent way to inspire activities in supporting research, communication and presentation with electronic media. This initiative is called Telota – the electronic life of the academy. One part of Telota is the so called Telota – Project of the month (TPM) working group which started work in February 2005. The main aims of this working group are to provide solutions for the mentioned issues which are cost efficient, future proven (especially to be independent from commercial vendors) and freely extensible. Every month the data of a selected academy project is processed, (re)structured and presented on the web to give humanities researchers, scholars and the interested public a new view in the extensive knowledge inventory of the academy. Identification and processing information of the long term projects is one of the central features of TPM. All the applied technologies and third party tools are reused, like all the gained experience is transferred from one project to the next. In addition new technologies are adopted to the working group’s portfolio. Thus the Project of the month is able to react properly to the monthly changing requirements. This paper exemplarily presents two systems for humanities projects from the viewpoint of an »in-house« working group. It tries to show the possibilities of developing electronic resources of long term projects in a very short time period.",Mehr XML – Die digitalen Projekte an der Berlin-Brandenburgischen Akademie der Wissenschaften.,,2007,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2377,http://computerphilologie.digital-humanities.de/jg06/kueck.html,"{'string_id': None, 'id_scheme': None}","The aim of this paper is to show up ways in which the practice, or rather practices, of cheating can be integrated into the work of game researchers. Apart from the obvious ways, in which cheats can be employed, exempli grati as time-savers, it is my intention to demonstrate how cheating can actually add to our understanding of games. To this end, I study the areas of game analysis, genre, and games culture. In all these examples, I attempt to point to ways in which the practice of cheating can not only make us understand the things we already see, but also enable us to see things that have escaped our attention so far.",Homo Deludens – Cheating als methodisches Werkzeug in der Computerspielforschung,,2007,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2378,http://computerphilologie.digital-humanities.de/jg07/BassKarcz.html,"{'string_id': None, 'id_scheme': None}","In view of the methodological premises informed by the cultural turn, researchers in literary and cultural studies are engaging in a promising critical endeavor that demands computer-assisted research techniques in order to fulfill its enormous potential. To analyze the delicate relationships between literary works and their cultural context(s) in a comprehensive and traceable manner scholars of literature and culture need convenient computational environments and standards that meet the specific needs of these fields of research. This article introduces a wish list to propose a number of basic requirements for computer-aided research practices and applications in literary and cultural studies and closes with an open invitation to critical discussion and continuation of that list.",Computergestützte Literaturwissenschaft als Kulturwissenschaft. Eine Wunschliste,,2009,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2379,http://computerphilologie.digital-humanities.de/jg07/annobiblio.html,"{'string_id': None, 'id_scheme': None}","The database Annotierte Bibliographie zur Literaturtheorie has been online since July 2007. It contains entries of recent contributions to the field of literary theory and it aims to facilitate a survey of fundamental theories, theories of interpretation and methodological approaches that are relevant to literary theory. Each entry comprises of bibliographical data as well as annotations of several types. In the next few years the database will continue to expand and improve. Our aim is to compile a comprehensive bibliography of literary theory applying the »Creative Commons License« and being globally accessible for users to search for titles within a particular theoretical field and to contribute to the database. This article introduces the idea, implementation, and the utilization of the database and outlines its further development.",Das Projekt Annotierte Bibliographie zur Literaturtheorie – Vorstellung der Datenbank,,2008,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2380,http://computerphilologie.digital-humanities.de/jg07/bobgehl.html,"{'string_id': None, 'id_scheme': None}",Metrical markup done manually demands an enormous effort of time. Wouldn’t it be smart to let computers do the work automatically – and would that be possible? The answer is »Yes«. The following text describes how automatic metrical markup for stressed and unstressed syllables in German verse text can be achieved on the bases of theoretical postulations and their methodical realization.,Automatisches metrisches Markup deutschsprachiger Gedichte,,2009,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2381,http://computerphilologie.digital-humanities.de/jg07/bubenh.html,"{'string_id': None, 'id_scheme': None}","Whereas editors of printed collections of historical sources have to decide whether to create a critical edition for a scientific audience or one for a wider audience, which is easier to read, digital editions have the potential to combine high standards of textual criticism with a high degree of readability. By means of an edition of primary sources on the 1968 movement in Zurich, Switzerland, the article introduces the utilization of XML as tool for the creation of dynamic documents. The application allows the user to switch between different modes of the representation of the sources ranging from a plain to a critically edited text with facsimile. Moreover, XML can be very useful for structuring and arranging large amounts of texts and present them in different modes. XML is thus an ideal markup language for critical editions, although minor problems concerning the processing of large data sets must be dealt with. The article illustrates, how XML and XSLT are put into practice. On the one hand XSLT is used to dynamically produce different views on the sources of the edition. On the other hand XSLT extracts meta information from the data to get the various indices and tables of content. By these means the edition organizes its source documents in different ways to ease the access for the users.",XML-Technologien als Grundlage dynamischer Textpräsentation. Die digitale Quellenedition Der Zürcher Sommer 1968,,2008,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2382,,"{'string_id': None, 'id_scheme': None}"," The close study of literary texts has a long and illustrious history. But the popularity of textual analysis has waned in recent decades, just at the time that widely available electronic texts were making traditional analytic tools easier to apply and encouraging the development of innovative computer-assisted tools. Without claiming any simple causal relationship, I argue that the marginalization of textual analysis and other text-centered approaches owes something to the dominance of Chomskyan linguistics and the popularity of high theory. Certainly both an introspective, sentence-oriented, formalist linguistic approach and literary theories deeply influenced by ideas about the sign's instability and the tendency of texts to disintegrate under critical pressure minimize the importance of the text. Using examples from Noam Chomsky, Jerome McGann, and Stanley Fish, I argue for a return to the text, specifically the electronic, computable text, to see what corpora, text-analysis, statistical stylistics, and authorship attribution can reveal about meanings and style. The recent resurgence of interest in scholarly editions, corpora, text- analysis, stylistics, and authorship suggest that the electronic text may finally reach its full potential. ","The End of the Irrelevant Text: Electronic Texts, Linguistics, and Literary Theory",,2007,,Digital Humanities Quarterly,001,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2383,http://computerphilologie.digital-humanities.de/jg07/kuwalu.html,"{'string_id': None, 'id_scheme': None}","Over the last years, the digital archiving, administration, and processing of texts has become more and more important in various fields of academic research. Particularly for literary and cultural studies, the digital storage and handling of great amounts of texts – an endeavor which has already been labelled with the keyword eHumanities (enhanced Humanities) – bears an enormous potential. There is, however, still a lack of applications appropriately supporting specific approaches in these fields of research. In this article, we thus propose basic design principles of a computational environment for literary and cultural investigation, id est a platform providing advanced search facilities which are tailored to the needs of scholars of literature and culture (KUWALU = Kulturwissenschaftliche Arbeits- und Lern-Umgebung). We both sketch the theoretical fundamentals of such a platform and outline specific requirements it has to fulfil. Furthermore, we discuss possible ways of implementing these requirements, in particular focussing on computational linguistic methods from Computational Linguistics supporting »intelligent« search facilities and the need for comprehensibility and traceability of complex search queries and results.",KUWALU – Motivation und Grundzüge einer computergestützten Umgebung für die literatur- und kulturwissenschaftliche Recherche- und Analysearbeit,,2007,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2384,http://computerphilologie.digital-humanities.de/jg07/koenigrusch.html,"{'string_id': None, 'id_scheme': None}","This article explores the potential of computer games to expand their emotional range and to provide their players with thought provoking and deeply moving experiences. While traditional narrative is well equipped to offer an inside view on characters and events, the fictional components in computer games tend to recede into the background during game-play. We will argue that a systematic approach on coupling rules and fiction may offer a solution to this problem. By drawing on Roland Barthes’ research on the functional units of literary texts, adapting it for the interactive medium, we will identify particular aspects of fiction that are most apt to heighten emotional game-play, if strategically integrated into the rule-system.",Barthes Revisited: Perspectives on Emotion Strategies in Computer Games,,2008,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2385,http://computerphilologie.digital-humanities.de/jg07/luedzeldes.html,"{'string_id': None, 'id_scheme': None}","Digital corpora are used as a data source in corpus linguistics, literary computing and computational linguistics. Although differences in these disciplines dictate different kinds of work with corpora, many of their respective methods either are applied or could be applicable in the other disciplines. With the recent emergence of richly annotated multi-level and multi-purpose corpora in mind, we review differences and similarities in research questions, corpus resources and their qualitative and quantitative exploitation in the three disciplines, along with suggestions for further development and mutual enrichment.","Three Views on Corpora: Corpus Linguistics, Literary Computing, and Computational Linguistics",,2008,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2386,http://computerphilologie.digital-humanities.de/jg07/rischuetei.html,"{'string_id': None, 'id_scheme': None}","Digital methods are essential for production, revision and publication of philological editions. The TEI-guidelines for XML coding are considered an approved standard for the representation of literary texts. The research group responsible for publishing Karl Ferdinand Gutzkow’s (1811–1878) dramatic works and the Institute of Computer Science – both located at the Martin-Luther-University Halle-Wittenberg, Germany – are working together on a configurable platform that simplifies and automates the work of editors. The product of this cooperation is KRONOS, a TEI-conform platform supporting drama edition[1]. The tool offers a GUI adapted to the structure of drama, XML-encoding that is as easy as formatting text even without knowledge of XML/TEI, and the extraction of publication formats (HTML, PDF) at the push of a button.",Entwicklung und Einsatz einer TEI-konformen Arbeitsumgebung für die Edition der Dramen von Karl Ferdinand Gutzkow,,2008,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2387,http://computerphilologie.digital-humanities.de/jg07/schmieder.html,"{'string_id': None, 'id_scheme': None}","The article discusses the meaning of the new digital media for the History of Concepts. Although it is evident that new technics and methods of data processing will become more and more relevant it seems surprising to find that the trial and methodical reflexion of digitally supported methods have not found too much interest among researchers in History of Concepts in the past. This neglect seems even more striking as the digital media offer a broad set of methodically and practically highly interesting possibilities for projects on the History of Concepts. The article discusses the meaning of the new digital media for the History of Concepts in relation to three dimensions: to new possibilities of data aquisition and systematisation, concerning new forms of archive systems and, not least, new forms of presentation of the results. The article demonstrates special demands and desiderats; its aim is to stimulate discussions between the History of Concepts, Computerphilologists and Programmers.",Zur Bedeutung der Computertechnik für die Begriffsgeschichte,,2007,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2388,http://computerphilologie.digital-humanities.de/jg08/tonra.html,"{'string_id': None, 'id_scheme': None}","This paper focuses on the theoretical and practical decisions taken in the creation of a genetic hypermedia edition of Thomas Moore’s 1817 poetic work, Lalla Rookh for the IRCHSS-funded project, the Thomas Moore Hypermedia Archive. It evaluates the appropriateness of hypermedia for hosting such a work from the perspective of presenting its many associated materials (translations, illustrations, adaptations), and demonstrating the genetic development of the work from its first manuscript draft through to publication. The author compares the different editorial decisions that are associated with presenting literary texts in print and electronic formats, concluding that the latter eliminates the need to choose a copy-text and establish a hierarchy among the textual witnesses. The article illustrates how this fact permits a more effective visualisation of the genetic development of the work, and reports on how the Lalla Rookh edition has achieved this, referring to the encoding practice adopted by the editor on the basis of the standards that are outlined in the TEI P5 Guidelines.",Textual Studies and the TEI: Encoding Thomas Moore’s Lalla Rookh,,2009,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2389,http://computerphilologie.digital-humanities.de/jg08/mclough.pdf,"{'string_id': None, 'id_scheme': None}","The paper looks at the challenges facing an editor who moves from print editions to working on an electronic edition. Among the first are the different kinds of language usage among editors of print and electronic editions; these – particularly that of ›reader‹ and ›user‹ – point up different expectations and different goals in the two mediums. Questions thus arise as to the purpose in editing an electronic edition and their impact on the way the electronic editor thinks about the text. The implications of the word ›user‹, are explored especially in relation to annotation. Annotation in turn calls for attention to how the text is marked-up in TEI. Throughout the change from the one medium to the other, the editor has to reassess his relation to what he thought were the critical priorities of the text, as well as to his user. The final part of the paper looks at issues of navigation and the demands to think critically about the dynamics of the edition. That takes the editor back to the fundamental question about the purpose of an electronic edition",Bridging the Gap ,,2009,,Jahrbuch für Computerphilologie,10,,"[{'value': None, 'type': None}]"
2390,http://computerphilologie.digital-humanities.de/jg08/rehbein.pdf,"{'string_id': None, 'id_scheme': None}","The purpose of this article is two-fold: firstly, to illustrate the collaborative work of the TEXTE programme at NUI, Galway, in order to facilitate the move that an »old-style editor« had to undertake to electronically edit the correspondence of Irish painter James Barry, and secondly to discuss certain aspects that arose from this work in reflection to editor Tim McLoughlin’s observations which he describes by himself elsewhere in this volume. This article focuses mainly on what ›using‹ a scholarly edition means and problematizes what impact this has on the shift from hard-copy to electronic editing. It is argued that there is not so much a gap between print and electronic medium but much more a transition in a thinking process: from the output orientation of the classical style of editing towards a data- and user-driven new approach. ","The transition from classical to digital thinking Reflections on Tim McLoughlin, James Barry and collaborative work",,2009,,Jahrbuch für Computerphilologie,10,,"[{'value': None, 'type': None}]"
2391,http://computerphilologie.digital-humanities.de/jg08/romary.pdf,"{'string_id': None, 'id_scheme': None}","This paper provides an introduction to the Text Encoding Initiative (TEI), focused at bringing in newcomers who have to deal with a digital document project and are investigating the capacity that the TEI environment may have to fulfil their needs. To this end, we avoid a strictly technical presentation of the TEI and concentrate on the actual issues that such projects face, with a parallel investigation of the situation within two institutions. While a quick walkthrough the TEI technical framework is provided, the paper concludes by showing the essential role played by the TEI community in providing technical and practical advice.",Questions & Answers for TEI Newcomers ,,2009,,Jahrbuch für Computerphilologie,10,,"[{'value': None, 'type': None}]"
2392,http://computerphilologie.digital-humanities.de/jg08/pierazzo.html,"{'string_id': None, 'id_scheme': None}","The digital and print edition of the correspondence of Giacomo Puccini, a research project hosted in Lucca (Italy), gives the opportunity to discuss more general issues of project management, workflow and teamwork. The project is intended to run for the next ten years, a time-scale that also raises issues of long-term sustainability and controllable editorial practices. The project team consists of members who are mostly voluntary and who are based in five countries on two continents. These two factors further compound existing difficulties in dealing with unfamiliar frameworks, while the limited budget leads to unsystematic technical development. The need for user-friendly editorial tools and the budget-driven technical limitations combine to demand creative solutions in project management. The choice of a hybrid publication (print and digital) aroused much controversy with the publishing house, probably due on the one hand to the novelty of the form, and on the other hand to the unclear theoretical and practical understanding of the relative strengths and weaknesses of the two formats and of the advantages of having the two side by side.",Editorial Teamwork in a Digital Environment: The Edition of the Correspondence of Giacomo Puccini,,2009,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2393,,"{'string_id': None, 'id_scheme': None}"," This article describes the evolution of the design of Vannevar Bush's Memex, tracing its roots in Bush's earlier work with analog computing machines, and his understanding of the technique of associative memory. It argues that Memex was the product of a particular engineering culture, and that the machines that preceded Memex — the Differential Analyzer and the Selector in particular — helped engender this culture, and the discourse of analogue computing itself. ",The Technical Evolution of Vannevar Bush’s Memex ,,2008,,Digital Humanities Quarterly,002,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2394,http://computerphilologie.digital-humanities.de/jg08/cummings.pdf,"{'string_id': None, 'id_scheme': None}","This article reports on a project in progress at the University of Oxford that is editing the forty-eight years of detailed diaries written by William Godwin from 1788 to 1836. Support has been provided in the form of TEI training, schema customisation, website development, and general technical support. This work so far has included tutorials on TEI (and related technologies), the creation of a Godwin-specific TEI ODD in order to customise the TEI to their needs, and numerous XSLT stylesheets for producing various lists and statistics to help them proofread and understand the markup that they’ve already applied and prepare for the next phases of encoding they are undertaking. This article is concerned with some of the the new features of TEI P5 they have used, how these have been transformed (with XSLT2), and the benefits this has accrued to their project methodology. The project, while it is an inter-departmental one, is not located in the History or English faculties as one might expect, but the Department of Politics and International Relations. Hence, the central concerns of those working on the project are the networked relationships between people, the meetings they attended, and prosopographical networks. For example, in addition to just marking up the very regular diary entries, they are also pointing from each of the over 64,000 instances of a person’s name to a TEI <person> element in a separate file where metadata is stored about that person. This form of URI-based pointing combined with prosopographical data is one of the new areas in TEI P5. The article uses this as an example to look at how the particular needs of a scholarly editing project were reflected in the TEI P5 markup chosen and how that has been chosen to be processed for a variety of different purposes, before discussing the directions the project will be going in the future. ",The William Godwin’s Diaries Project: Customising and transforming TEI P5 XML for project work,,2009,,Jahrbuch für Computerphilologie,10,,"[{'value': None, 'type': None}]"
2395,http://computerphilologie.digital-humanities.de/jg08/moran.pdf,"{'string_id': None, 'id_scheme': None}","Students of early Irish history, archaeology, language and literature are in many ways well served with digital resources. This paper outlines some of the key resources already available, and discusses a contribution currently under preparation by the Early Irish Glossaries Project at the University of Cambridge. It also makes some general remarks on potential future directions for electronic resources in the field. ",Irish glossaries and other digital resources for early Irish studies ,,2009,,Jahrbuch für Computerphilologie,10,,"[{'value': None, 'type': None}]"
2396,http://computerphilologie.digital-humanities.de/jg08/fischer.html,"{'string_id': None, 'id_scheme': None}","What should a scholarly edition look like? A methodology of scholarly editing has, on the one hand, to be developed under conditions of ongoing change in text theory – as has taken place in a Material Philology – and concomitant changes of perspective on the extant documents. On the other hand, the shift to digital media is augmenting the options for representing and publishing texts and documents, and thus is a decisive factor in methodological enhancement. Stimulated by the fertile interaction of both theoretical and technical options, the digital edition of William of Auxerre’s treatise on liturgy is intended to be a contribution, drawn from practical experience, to the discussion of objectives and methodological standards in scholarly editing. The present article (1) gives some basic information on the life and work of William of Auxerre and (2) presents the features of the digital edition of his treatise on liturgy. This edition then (3) is placed in a theoretical framework; finally (4) some technical issues and problems that appeared during encoding are pointed out.",The pluralistic approach – The first scholarly edition of William of Auxerre’s treatise on liturgy,,2009,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2397,http://computerphilologie.digital-humanities.de/jg08/keating1.html,"{'string_id': None, 'id_scheme': None}","In this, part one of a two part paper, we will discuss the approach taken during the creation of a digital edition of the Alcalá Account Book manuscript. The Alcalá Project was originally proposed as a digital humanities project to mark a humanities collaboration between the University of Alcalá de Henares (UAH), Spain, and the National University of Ireland, Maynooth (NUIM). The source was to be a Spanish eighteenth century account book recording the monthly expenses of the Royal Irish College of Saint George the Martyr. In the given time-frame, the source manuscript was chosen, encoded and made available in a web based, dual language, searchable and interactive environment. More importantly, a virtual framework was developed to aid the historian in answering historically pertinent research questions that are specifically prompted by the historical object – an account book. We contend that by creating a digital edition that represents the original Alcalá Account Book manuscript and its functionality, we have provided the end-user (for example the historian) with a richer environment for performing research, and a research tool that is specifically designed to be fit for purpose. The approach is informed by the discipline of the participants, and also takes account of current best practice in humanities and digital humanities. We aim here to provide a description of that digitisation process and the methodologies we used to make design decisions. Part two, Formalisation and Encoding, will address the theoretical framework and practicalities of formalising and encoding the source.",A Digital Edition of a Spanish 18th Century Account Book: User Driven Digitisation,,2009,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2398,http://computerphilologie.digital-humanities.de/jg08/keating2.html,"{'string_id': None, 'id_scheme': None}","In this, part two of a two part paper, we will discuss our approach to the formalisation of our document encoding approach, derived from software engineering, which treats of the three classes of a digital edition; the Logical, the Physical and the Interaction Classes. We specifically address our decision to use XML (Extensible Mark-up Language), not TEI (Text Encoding Initiative), as our encoding language. An argument is provided as to why TEI is unsuitable for function-based documents, this addresses both source integrity and the restrictive nature of TEI. TEI does not support our forward engineering approach, which allows us to simultaneously produce the model, the encoding and the software environment.",A Digital Edition of a Spanish 18th Century Account Book: Formalisation and Encoding,,2009,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2399,http://computerphilologie.digital-humanities.de/jg09/walkowski.html,"{'string_id': None, 'id_scheme': None}","The article discusses possibilities of data-orientated research with digital data environments in the humanities. Starting with problems of representing meaning in data form in the context of person data, the perspectives of the Person Data Repositoryby the Berlin-Brandenburgische Akademie der Wissenschaften are presented. These problems are related to well known issues concerning the standardization and formalization of content when storing it in the database. This is seen as a loss of value in many humanities projects. The PDR developed an approach in which this loss is compensated by bringing together data and context of the data into the database. The main idea of the PDR is then worked out to the concept of a polysemic database.",Das Konzept einer polysemischer Datenbank und seine Konkretisierung im Personendaten-Repositorium der BBAW,,2011,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2400,http://computerphilologie.digital-humanities.de/jg09/boot.html,"{'string_id': None, 'id_scheme': None}","In 2009, the Van Gogh Museum and the Huygens Institute published a new edition of the correspondence of Vincent van Gogh, both online and in print. This article discusses how the online edition dealt with the richness of material (using the strategies of flexibility and user control, powerful search and filter facilities and indexing and cross-referencing). It describes the technical infrastructure and the conversion that was needed from word processor documents into XML. It also examines the relation between the book and web versions of the edition. In conclusion, the article mentions some of the user reactions to the edition and briefly mentions some of the prospects for future scholarly editions.",Vincent Van Gogh - The Letters,,2011,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2401,http://computerphilologie.digital-humanities.de/jg09/fanta.html,"{'string_id': None, 'id_scheme': None}","Considering the immortality of electronic corpora in the literary edition of the Musil estate The digital Klagenfurt edition of the works of Robert Musil contains the complete transcript of the writers Nachlass, his literary estate.[1] The estate compiles about 12,000 pages from Musils manuscripts; it includes all data collected since 1985, when work on the project began, until the year of publication of the first full edition of his work on DVD in 2009. The transcribed data underwent several interesting metamorphoses throughout the process; the transformation reflects the history of digital philology itself. Out review of the project is geared both towards philological aspects, like the changed status of transcripts within edition projects, and the different transcription systems; as well as technical aspects, like changes in hardware, software and publication formats.",Zur Immortalität elektronischer Korpora am Beispiel der Musil-Edition,,2011,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2402,http://computerphilologie.digital-humanities.de/jg09/geykenetal.html,"{'string_id': None, 'id_scheme': None}","This paper deals with the issue of quality assurance in very large, XML/TEI-encoded full-text collections. The text corpus edited by the DFG-funded project Deutsches Textarchiv (henceforth: DTA), a large and still growing reference corpus of historical German, is a fine example of such a collection. The following remarks focus on text prepared in a Double-Keying-process, since the major part of the DTA-corpus is compiled by applying this highly accurate method. An extensive and multi-tiered approach, which is currently applied by the DTA for the analysis and correction of errors in double-keyed text, is introduced. The process of quality assurance is pursued in a formative way in order to prevent as many errors as possible, as well as in a summative way in order to track errors which nevertheless may have occurred in the course of full-text digitization. To facilitate the latter, DTAQ, a web-based, collaborative tool for finding and commenting errors in the corpus, was developed. On the profound basis of practical experience in the past four years, the preliminaries and possible methods of conducting a widespread quality assurance are being discussed.","TEI und Textkorpora: Fehlerklassifikation und Qualitätskontrolle vor, während und nach der Texterfassung im Deutschen Textarchiv",,2012,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2403,http://computerphilologie.digital-humanities.de/jg09/Ilsemann.html,"{'string_id': None, 'id_scheme': None}","The main task of this paper deals with the exploration of frequency distributions of speech lengths. Initial investigations aimed at authorship questions and tried to contribute seriously to the recently flared up Marlowe-Shakespeare debate. Distribution curves were compared and checked by chi-square tests relying on text corpora by Marlowe, Shakespeare, and other contemporaries. The all-all text curves showed congruence between Marlowe and Shakespeare, but also conformity between Marlowe/Shakespeare and their contemporaries. Single text vs. all text curve relations resulted in cross-overs when Marlowe texts fitted better into the Shakespeare canon up to 1599 and Titus Andronicus was closer to the Marlowe range. The overall result was more than clear. Frequency distributions of speech lengths could not contribute to authorship questions. However, as an indication of style changes, the maxima values in their reduction from nine to four words opened up a totally different artistic and aesthetic concept which started around 1599 and marked the beginning of British drama in its own right after the process of incorporation of Roman plays and antique mythology. The stylistic shift certainly occurs in the wider context of rhetorical speeches and dramatic actions, as rhetoric and declamations receded and were replaced by refined acting using the spatial dimensions of the stage and speeding up events by shorter speeches. Whereas before monologues had to be seen within the framework of rhetorical devices that kept actors rambling and lamenting they now appeared as functional components of plays, adding to the aesthetic values of varietas and copia – or variety and abundance thus bearing witness to the process of commercialization and of cultural establishment of the literary genre drama at the turn of the century. All quantitative dates were derived from IDAP [1].",Quantitativ-statistische Dramenanalyse: Welche Aussagekraft haben Häufigkeitsverteilungen der Replikenlängen?,,2013,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2404,,"{'string_id': None, 'id_scheme': None}"," With Humanities Computing and New Media identified as emerging fields of significant strength, it is time for well-funded and fully supported programs in Digital Humanities to be described, developed, and implemented in the university. This article is a description of an attempt to build such a program from the ground up, rather than from the top down. That is, the authors and others created a series of courses, both multi-disciplinary and disciplinary, a database, and a core course designed to make digital humanities a reality, even without having it certified as a program by the governing bodies of their faculty and university. In this article, the database and core course are described in some detail in order to offer what the authors believe to be worthwhile ideas to others who would advance the cause of digital humanities. The article concludes with some concrete suggestions on how to ensure support, to make faculty participation possible, to measure success, and to motivate students. ",The Humanities HyperMedia Centre @ Acadia University: An Invitation to Think About Higher Education,,2008,,Digital Humanities Quarterly,002,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2405,http://computerphilologie.digital-humanities.de/jg09/viehhauser.html,"{'string_id': None, 'id_scheme': None}","Up to now, Wolfram von Eschenbach’s Parzival, one of the most important texts of Middle High German literature, hasn’t been accessible but in one edition, which is over 170 years old and thus follows editorial principles that are by now outdated. Hence, in 2001, the Parzival-Project, currently settled at the University of Berne, was grounded to present a new edition that would visualize the plurality of manuscript transmission by applying modern computer technology. The edition is based on digital transcriptions that are annotated according to a specifically developed markup-system. Due to the fact that the guidelines of the Text Encoding Initiative (TEI) have become a standard for the annotation of digital editions it would be desirable to transform the Bernese project material into the TEI-code. The paper demonstrates what methods can be used to convert the data, and what problems might occur in the transformation process.",Standardisierung und proprietäre Annotation im Berner Parzival-Projekt.,,2013,,Jahrbuch für Computerphilologie,,,"[{'value': None, 'type': None}]"
2406,http://computerphilologie.digital-humanities.de/jg09/lukasosthof.pdf,"{'string_id': None, 'id_scheme': None}","Spatiality – the topographical distribution of text on the manuscript page – represents one of the central aspects of the epistolographical ›material text‹. In contrast to a diplomatic-mimetic text reproduction, the C.F.Meyer edition distinguishes between significant (structural) vs. meaningless (contingent) spatiality. The contribution reflects the theoretical foundations and implications of this concept in the context of the current materiality debate in textual scholarship and presents a solution for the implementation of the philological model in TEI.",Physische vs. gedeutete Räumlichkeit. Zur Auszeichnung spatialer Informationen in der historisch-kritischen Ausgabe C.F. Meyers Briefwechsel,,2016,,Jahrbuch für Computerphilologie,30,,"[{'value': None, 'type': None}]"
2407,,"{'string_id': None, 'id_scheme': None}"," Art historically relevant visual knowledge can be deconstructed and the resulting components of this visual knowledge — visual discernments — lend themselves to be socially negotiated. Individual visual experts (like connoisseurs) do not share some grand and undividable cognitive cataloguing system; they are attentive to piecemeal visual discernments and the patterns in which these occur in reality. In conventional scholarly communication sophisticated tools to discuss perceptual patterns are lacking. This paper not only proposes a theoretical model of visual knowledge accumulation, but also describes a practical implementation, Art.Similarities, which is designed as a prototype of such a sophisticated tool. Using a custom-made interface it records visual behavior: the non-verbally expressed visual similarity judgments of distributed individuals. Users can be assigned to groups according to the qualities of their judgments. These qualities may be distilled from emerging similarity patterns. The implications of individual judgments in different user groups may vary considerably. Emerging patterns can be assessed both according to human analysis and statistical procedures. Most studies on art evaluation are attentive to either the characteristics of works, or the characteristics of observers. In this study both are considered as interdependent entities consistently. ",As You Can See: Applying Visual Collaborative Filtering to Works of Art,,2008,,Digital Humanities Quarterly,002,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2408,,"{'string_id': None, 'id_scheme': None}"," No humanists have moved more aggressively in the digital world than students of the Greco-Roman world but the first generation of digital classics has seen relatively superficial methods to address the problems of print culture. We are now beginning to see new intellectual practices for which new terms, eWissenschaft and eClassics, and a new cyberinfrastructure are emerging. ",Cyberinfrastructure for Classical Philology,,2009,,Digital Humanities Quarterly,003,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2409,,"{'string_id': None, 'id_scheme': None}"," The Suda On Line (SOL) is a collaborative translation of a Byzantine Greek encyclopedia. It makes this difficult but useful text available to non-specialists and, with annotations and search facilities, makes the Suda easier to use than it is in print. As a collaboration, SOL demonstrates open peer review and the feasibility of a large, but closely focused, humanities project. ",Tachypaedia Byzantina: The Suda On Line as Collaborative Encyclopedia,,2009,,Digital Humanities Quarterly,003,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2410,http://projects.chass.utoronto.ca/chwp/CHC2004/sinclair/,"{'string_id': None, 'id_scheme': None}","With the advent of miniaturization and mass data storage, it becomes possible to collect relatively large amounts of data based on aspects of everyday life. With data sets placed in flexible, organized database contexts, it becomes possible to explore data in unique and interesting ways. In this paper I will discuss my new media art works based on a data set I have created called oh, those everyday spaces (OTES). The OTES data set contains material collected while cycling in and around Toronto, Canada. During the 15 month collection period one image was captured each minute, GPS (Global Positioning System) position about three times each minute, and local weather conditions were downloaded from Environment Canada's web site each hour. OTES contains about 25,000 images. The data set becomes the basis for my explorations of place through creative works based on the data. To illuminate the variety of approaches I am using to explore place, I will discuss works based on OTES.",The database as a vehicle for reconceiving place through the new media art interface,,2009,"['Database art, sousveillance, sonification, new media art']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2411,,"{'string_id': None, 'id_scheme': None}"," The Web, though full of historical information, lacks a means of organizing that information, searching on it or visualizing it. The Historical Event Markup and Linking Project (Heml) was begun six years ago to explore how disparate historical materials on the Internet can be navigated and visualized, and for the past four years has used an XML data format defined in W3C Schemas. This format aims for conforming data that can be quickly parsed but provide a variety of facets on which to search for historical materials. While the project's graphical visualizations are in some respects successful, they have revealed some deficiencies in the underlying data format: it ought to provide for nested events, it ought to represent relations of causality between events and it ought to express the varieties of scholarly opinion about the attributes of events. By encoding the Heml data in the Resource Description Framework (RDF) it is possible to undertake these improvements. Moreover, an RDF-encoded Heml process provides easier access to CIDOC-CRM data into Heml events. Finally, a historical RDF language would simplify the discovery of references to historical events in digitized texts, thereby automating a growing network of historical information on the Web. ",Exploring Historical RDF with Heml,,2009,,Digital Humanities Quarterly,003,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2412,,"{'string_id': None, 'id_scheme': None}"," Incunabula, or books printed before 1500, are extremely difficult and expensive to convert to digital form. The primary challenges arise from the use of non-standard typographical glyphs based on medieval handwriting to abbreviate words. Further difficulties are also posed by the practice of inconsistently marking word breaks at the end of lines and reducing or even eliminating spacing between some words. As such, these documents form a distinct genre of electronic document that poses unique challenges for conversion to digital form. From 2005–2007, the Preservation and Access Research and Development Program at the National Endowment for the Humanities funded a study to explore methods for digitizing these difficult texts. This paper describes some of the results of that project. The work described in this paper was completed by the Approaching the Problems of Digitizing Latin Incunables project funded by the National Endowment for the Humanities Division of Preservation and Access. The material in this paper is drawn from the project application, internal technical reports, grant project reports and the project descriptions included in and . Much of this work was inspired by Ross Scaife and his work building a corpora of Latin Colloquia. I am deeply grateful for Ross's comments, advice and support. A version of this paper will also be published as part of the project web site. ","Digitizing Latin Incunabula: Challenges, Methods, and Possibilities",,2009,,Digital Humanities Quarterly,003,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2413,,"{'string_id': None, 'id_scheme': None}"," Citation practice reflects a model of a scholarly domain. This paper first considers traditional citation practice in the humanities as a description of our subjects of study. It then describes work at the Center for Hellenic Studies on an architecture for digital scholarship that is explicitly based on this model, and proposes a machine-actionable but technologically independent notation for citing texts, the Canonical Text Services URN. ",Citation in Classical Studies,,2009,,Digital Humanities Quarterly,003,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2414,,"{'string_id': None, 'id_scheme': None}"," In this article we argue for the necessity of a digital edition to most accurately represent the textual tradition of the Homeric epics and to better understand the oral performance tradition that created the poems. We demonstrate how such a digital criticism would differ from the traditional textual criticism as practiced for editions in print and suggest how a digital criticism might open new avenues for the interpretation of the poetry. In defining our needs and goals for a digital edition, we discuss what our project has in common with other digital editions of literary works, but how the oral, traditional nature of the poetry creates special requirements as well. In addition to elaborating the editorial approach for the project, we reaffirm the principles of collaboration, international standards, and open access that we have learned from Ross Scaife, the founder of the Stoa Consortium. ",Digital Criticism: Editorial Standards for the Homer Multitext ,,2009,,Digital Humanities Quarterly,003,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2415,,"{'string_id': None, 'id_scheme': None}"," Epigraphy as a discipline has evolved greatly over the centuries. Many epigraphists in the last 20 years have been investigating how to use digital technology to advance their research, but until the present decade, these undertakings were restrained by aspects of the technology. The late 1990s will be seen as a watershed moment in the transition from print-based to born-digital epigraphic publication. At present, the majority of new editions are still published solely in print, but by 2017 we believe this circumstance will change drastically. The history of epigraphy makes it quite clear that such transitions are natural to the discipline. ",Epigraphy in 2017,,2009,,Digital Humanities Quarterly,003,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2416,,"{'string_id': None, 'id_scheme': None}"," The authors open by imagining one possible use of digital geographic techniques in the context of humanities research in 2017. They then outline the background to this vision, from early engagements in web-based mapping for the Classics to recent, fast-paced developments in web-based, collaborative geography. The article concludes with a description of their own Pleiades Project (http://pleiades.stoa.org), which gives scholars, students and enthusiasts worldwide the opportunity to use, create and share historical geographic information about the Greek and Roman World in digital form. ",Digital Geography and Classics,,2009,,Digital Humanities Quarterly,003,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2417,,"{'string_id': None, 'id_scheme': None}"," We describe two different strategies for generating the morphology of Latin verbs. First, we hand-code default inheritance hierarchies in the KATR formalism, treating inflectional exponents as markings associated with the application of rules by which complex word forms are deduced from simpler roots or stems. The high degree of similarity among verbs of different conjugation classes allows us to formulate general rules; these general rules are, however, sometimes overridden by conjugation-specific rules. This approach allows linguists to gain an appreciation for the structure of verbs, gives teachers a foundation for organizing lessons in morphology, and provides students a technique for generating forms of any verb. Second, we start with a paradigm chart, then automatically remove common parts and redundant morphosyntactic property sets (columns), combine similar conjugations (rows), and generate the KATR theory that produces a complete table of forms for a set of lexemes. This second approach automatically determines principal parts (for Latin, we verify that there are four), groups inflection classes into super-classes, and builds full paradigm charts. ",What Your Teacher Told You is True: Latin Verbs Have Four Principal Parts ,,2009,,Digital Humanities Quarterly,003,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2418,,"{'string_id': None, 'id_scheme': None}"," Manual lexicography has produced extraordinary results for Greek and Latin, but it cannot in the immediate future provide for all texts the same level of coverage available for the most heavily studied materials. As we build a cyberinfrastructure for Classics in the future, we must explore the role that automatic methods can play within it. Using technologies inherited from the disciplines of computational linguistics and computer science, we can create a complement to these traditional reference works - a dynamic lexicon that presents statistical information about a word’s usage in context, including information about its sense distribution within various authors, genres and eras, and syntactic information as well. ",Computational Linguistics and Classical Lexicography,,2009,,Digital Humanities Quarterly,003,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2419,,"{'string_id': None, 'id_scheme': None}"," In October 2008, Google announced a settlement that will provide access to seven million scanned books while the number of books freely available under an open license from the Internet Archive exceeded one million. The collections and services that classicists have created over the past generation place them in a strategic position to exploit the potential of these collections. This paper concludes with research topics relevant to all humanists on converting page images to text, one language to another, and raw text into machine actionable data. ",Classics in the Million Book Library,,2009,,Digital Humanities Quarterly,003,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2420,,"{'string_id': None, 'id_scheme': None}"," We can already begin to envision research projects that were scarcely, if at all, feasible in print culture. The papers in this collection allow us as well to enumerate the services and publication types on which emerging scholarship depends. We also need models for publication that meet the needs and realize the potential of the digital media and we describe here the Scaife Digital Library, a concrete example of true digital publication. ","Conclusion: Cyberinfrastructure, the Scaife Digital Library and Classics in a Digital age",,2009,,Digital Humanities Quarterly,003,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2421,http://projects.chass.utoronto.ca/chwp/Casta02/Siemens_casta02.htm,"{'string_id': None, 'id_scheme': None}","Beginning with the assumption that text analysis and ‘algorithmic’ approaches to textual navigation provide a foundation for the dynamic scholarly edition, in 2002 I began a number of conversations of varying degrees of formality chiefly, but not exclusively, with researchers associated with the Text Analysis Portal for Research (TAPoR) initiative. My working paper is intended to recount, generically, the salient points of these discussions, as they document an important stage in our thinking about the future of the scholarly edition.","Text Analysis and the Dynamic Edition? A Working Paper, Briefly Articulating Some Concerns with an Algorithmic Approach to the Electronic Scholarly Edition",,2005,"['Electronic editing, text analysis, TAPoR, dynamic edition']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2422,,"{'string_id': None, 'id_scheme': None}", How do we know when we're done? This cluster of articles explores completion and incompletion in the digital humanities from a variety of perspectives. ,Done: Finishing Projects in the Digital Humanities,,2009,,Digital Humanities Quarterly,003,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2423,,"{'string_id': None, 'id_scheme': None}"," The word finish can mean two things that have quite different implications for large-scale humanities computing projects: to bring to completion; to make or perform completely; to complete and also to perfect finally or in detail; to put the final and completing touches to (a thing). The word finish is just not part of the deal for the Linguistic Atlas Project in either sense. However, granting agencies must ask what do you want money for this time? and, from this viewpoint, the Atlas Project consists of a series of particular tasks or experiments, each one of which is capable of being finished in both senses of the word. This paper discusses the reality of funding, deadlines, and deliverables, as they relate to the sequence of tasks that make up the larger Atlas Project. There are no once-and-done, permanent solutions. The largest humanities computing projects require continuing care and maintenance, and the best way forward is to create some sort of stable institutional setting for large projects that will provide continuity and baseline resources for the work. ","Large-Scale Humanities Computing Projects: Snakes Eating Tails, or Every End is a New Beginning? ",,2009,,Digital Humanities Quarterly,003,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2424,,"{'string_id': None, 'id_scheme': None}"," Since the early 1990s, theorizing in the digital humanities has often celebrated open-endedness and incompletion as inherent qualities of digital work. But a scholarly publisher undertaking preparation and sale of digital objects cannot altogether dispense with traditional notions of deadlines and completion if those publications are to enter the dual marketplaces of peer review and institutional purchase. The Electronic Imprint of the University of Virginia Press was funded in 2001 with the goal of bringing born-digital scholarly projects under the aegis of the same review and marketing system that applies to books. In this article I describe how we defined the criteria for done-ness in creating two very different projects, a born-digital edition of Herman Melville’s Typee manuscript and a conversion of the letterpress Papers of George Washington into a digital edition. Our experience suggests that it is possible to categorize different genres of digital creations based on the extent to which intrinsic criteria for “done-ness” can be applied to them, and that decisions about completeness are always subject to extrinsic factors as well, such as budgetary constraints and the pressures created by competition and the evolution of standards. ","It’s For Sale, So It Must Be Finished: Digital Projects in the Scholarly Publishing World",,2009,,Digital Humanities Quarterly,003,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2425,,"{'string_id': None, 'id_scheme': None}"," The case of the Orlando Project offers a useful interrogation of concepts like completion and finality, as they emerge in the arena of electronic publication. The idea of doneness circulates discursively within a complex and evolving scholarly ecology where new modes of digital publication are changing our conceptions of textuality, at the same time that models of publication, funding, and archiving are rapidly changing. Within this ecology, it is instrumental and indeed valuable to consider particular tasks and stages done, even as the capacities of digital media push against a sense of finality. However, careful interrogation of aims and ends is required to think through the relation of a digital project to completion, whether modular, provisional, or of the project as a whole. ",Published Yet Never Done: The Tension Between Projection and Completion in Digital Humanities Research,,2009,,Digital Humanities Quarterly,003,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2426,,"{'string_id': None, 'id_scheme': None}"," This introduces the set of papers reflecting initial collaborative work between the ARTFL Project at the University of Chicago and the Linguistic Cognition Laboratory at the Illinois Institute of Technology on the intersection of machine learning, text mining and text analysis. ","Words, Patterns and Documents: Experiments in Machine Learning and Text Analysis",,2009,,Digital Humanities Quarterly,003,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2427,,"{'string_id': None, 'id_scheme': None}"," In this study, a corpus of 300 male-authored and 300 female-authored French literary and historical texts is classified for author gender using the Support Vector Machine (SVM) implementation SVMLight, achieving up to 90% classification accuracy. The sets of words that were most useful in distinguishing male and female writing are extracted from the support vectors. The results reinforce previous findings from statistical analyses of the same corpus, and exhibit remarkable cross-linguistic parallels with the results garnered from SVM models trained in gender classification on selections from the British National Corpus. It is found that female authors use personal pronouns and negative polarity items at a much higher rate than their male counterparts, and male authors demonstrate a strong preference for determiners and numerical quantifiers. Among the words that characterize male or female writing consistently over the time period spanned by the corpus, a number of cohesive semantic groups are identified. Male authors, for example, use religious terminology rooted in the church, while female authors use secular language to discuss spirituality. Such differences would take an enormous human effort to discover by a close reading of such a large corpus, but once identified through text mining, they frame intriguing questions which scholars may address using traditional critical analysis methods. ",Vive la Différence! Text Mining Gender Difference in French Literature ,,2009,,Digital Humanities Quarterly,003,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2428,,"{'string_id': None, 'id_scheme': None}"," Machine learning and text mining offer new models for text analysis in the humanities by searching for meaningful patterns across many hundreds or thousands of documents. In this study, we apply comparative text mining to a large database of 20th century Black Drama in an effort to examine linguistic distinctiveness of gender, race, and nationality. We first run tests on the plays of American versus non-American playwrights using a variety of learning techniques to classify these works, identifying those which are incorrectly classified and the features which distinguish the plays. We achieve a significant degree of performance in this cross-classification task and find features that may provide interpretative insights. Turning our attention to the question of gendered writing, we classify plays by male and female authors as well as the male and female characters depicted in these works. We again achieve significant results which provide a variety of feature lists clearly distinguishing the lexical choices made by male and female playwrights. While classification tasks such as these are successful and may be illuminating, they also raise several critical issues. The most successful classifications for author and character genders were accomplished by normalizing the data in various ways. Doing so creates a kind of distance from the text as originally composed, which may limit the interpretive utility of classification tools. By framing the classification tasks as binary oppositions (male/female, etc), the possibility arises of stereotypical or lowest common denominator results which may gloss over important critical elements, and may also reflect the experimental design. Text mining opens new avenues of textual and literary research by looking for patterns in large collections of documents, but should be employed with close attention to its methodological and critical limitations. ","Gender, Race, and Nationality in Black Drama, 1950-2006: Mining Differences in Language Use in Authors and their Characters",,2009,,Digital Humanities Quarterly,003,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2429,,"{'string_id': None, 'id_scheme': None}"," The Encyclopédie of Denis Diderot and Jean le Rond d'Alembert was one of the most important and revolutionary intellectual products of the French Enlightenment. Mobilizing many of the great – and the not-so-great – philosophes of the 18th century, the Encyclopédie was a massive reference work for the arts and sciences, which sought to organize and transmit the totality of human knowledge while at the same time serving as a vehicle for critical thinking. In its digital form, it is a highly structured corpus; some 55,000 of its 77,000 articles were labeled with classes of knowledge by the editors making it a perfect sandbox for experiments with supervised learning algorithms. In this study, we train a Naive Bayesian classifier on the labeled articles and use this model to determine class membership for the remaining articles. This model is then used to make binary comparisons between labeled texts from different classes in an effort to extract the most important features in terms of class distinction. Re-applying the model onto the original classified articles leads us to question our previous assumptions about the consistency and coherency of the ontology developed by the Encyclopedists. Finally, by applying this model to another corpus from 18th century France, the Journal de Trévoux, or Mémoires pour l'Histoire des Sciences & des Beaux-Arts, new light is shed on the domain of Literature as it was understood and defined by 18th century writers. ",Mining Eighteenth Century Ontologies: Machine Learning and Knowledge Classification in the Encyclopédie ,,2009,,Digital Humanities Quarterly,003,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2430,,"{'string_id': None, 'id_scheme': None}"," A response to the Data Mining cluster, exploring the role of machine learning in textual study. ","Text Minding: A Response to Gender, Race, and Nationality in Black Drama, 1850-2000: Mining Differences in Language Use in Authors and their Characters ",,2009,,Digital Humanities Quarterly,003,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2431,,"{'string_id': None, 'id_scheme': None}"," Electronic literature is an important evolving field of artistic practice and literary study. It is a sector of digital humanities focused specifically on born-digital literary artifacts, rather than on using the computer and the network to redistribute, analyze, or recontextualize artifacts of print culture. Works of electronic literature appeal to configurative reading practices. The field of electronic literature is based on a gift economy and developing a network-based literary culture built on the collaborative practices of a globally distributed community of artists, writers, and scholars. This article situates the development of the field of electronic literature within academe, some of the institutional challenges currently confronting the field, and its potential for further development. ",Communitizing Electronic Literature,,2009,,Digital Humanities Quarterly,003,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2432,http://projects.chass.utoronto.ca/chwp/CHC2003/Santos_Fortier2.htm,"{'string_id': None, 'id_scheme': None}","The Trésor de la Langue Française (TLF) database was designed, and texts chosen for inclusion in it, in the late 1950s. This paper evaluates the choices of texts made by the TLF committee in the light of a contemporary encyclopaedia of French literature and of subsequent published research in the field of French literature, provided by the MLA (Modern Language Association) online bibliography. Spearman?s rank correlation coefficient, and outlier analysis based on Mahalanobis distance evaluate similarities in the sets of data. The conclusion is that the choices made in the mid-twentieth century were a reasonable reflection of scholarly interests both at the time the database was constituted, and subsequently up to the present. The method described in this paper is applicable to the evaluation of other full-text databases.",Validating choices: Texts in the Trésor de la Langue Française,,2003,"['Literature, corpora, French, statistics, Trésor de la langue française, Modern Language Association']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2433,,"{'string_id': None, 'id_scheme': None}"," This paper examines the internet journal Southern Spaces, launched in February 2004 and the online learning community SouthComb, started in 2006. We examine the development of these online tools, exploring pedagogical implications as well as the tools and avenues they bring to the field of Southern Studies, American Studies and scholarly communication online. We also explore the potential uses for these resources as well as their efforts to elucidate a broader understanding of the U.S. South in regional, national and global contexts. ",Teaching and Learning from the U.S. South in Global Contexts: A Case Study of Southern Spaces and Southcomb,,2009,"['Online learning environments', 'scholarly communication', 'online learning communities', 'U.S. South', 'Southern Studies', 'American Studies', 'Global Souths']",Digital Humanities Quarterly,003,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2434,,"{'string_id': None, 'id_scheme': None}"," The nature of the academic lecture has changed with the introduction of wi-fi and cellular technologies. Interacting with personal screens during a lecture or other live event has become commonplace and, as a result, the economy of attention that defines these situations has changed. Is it possible to pay attention when sending a text message or surfing the web? For that matter, does distraction always detract from the learning that takes place in these environments? In this article, we ask questions concerning the texture and shape of this emerging economy of attention. We do not take a position on the efficiency of new technologies for delivering educational content or their efficacy of competing for users’ time and attention. Instead, we argue that the emerging social media provide new methods for choreographing attention in line with the performative conventions of any given situation. Rather than banning laptops and phones from the lecture hall and the classroom, we aim to ask what precisely they have on offer for these settings understood as performative sites, as well as for a culture that equates individual attentional behavior with intellectual and moral aptitude. ",Designing Choreographies for the New Economy of Attention ,,2009,,Digital Humanities Quarterly,003,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2435,,"{'string_id': None, 'id_scheme': None}"," Many ends of editing in the digital world may be distinguished. One may speak of end as in the supersession of one model of editing (the intentionalist, definitive-text model) by another (the multiple texts, multiply-intentioned views enabled by digital methods). One may speak of end as in aim: not only the aim of the author or authors, but also the aims of the editor or editors. These questions were already complicated in the print world; the advent of digital methods has both focussed and widened the contests around these concepts. The essay reviews some of these questions, with examples drawn from (inter alia) the utterances of the two George Bushes, from editions with which the author is associated of Chaucer, Dante, and of Armenian texts, from the eColi genome, and from Barack Obama's discussion of different viewpoints on the Constitution of the United States. The essay concludes that a huge shift is indeed underway in the editing world, towards a more open and participatory model of editing and reading. ",The Ends of Editing,,2009,,Digital Humanities Quarterly,003,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2436,,"{'string_id': None, 'id_scheme': None}"," After centuries of image deprivation, we now bathe in a sea of pictures, most of them digitized at some stage. In the 1990s, as humanists began to sense the advantages of networked computing on the web, they conceived major new editorial projects that would depend to an extraordinary degree upon the documentary power of pictures. Despite evident progress in devising sturdy and responsive standards, images, and tools, stubborn problems persist in several key areas that are explored here through an overview of issues that arise as the William Blake Archive acquires images, prepares them for reproduction, and makes them available for manipulation by its users. Editing electronic images in so unsettled and unsettling an environment generates the provisional success — weak success — that is utterly characteristic of X-editing, electronic scholarly editing in our time. Our dependence on current technology and the expertise of others is not a remediable condition. We must play the game as it presents itself, making the compromises that are necessary, and move ahead. ",Picture Problems: X-Editing Images 1992-2010,,2009,,Digital Humanities Quarterly,003,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2437,,"{'string_id': None, 'id_scheme': None}"," What are the implications of the terms we use to describe large-scale text-based electronic scholarship, especially undertakings that share some of the ambitions and methods of the traditional multi-volume scholarly edition? And how do the conceptions inherent in these choices of language frame and perhaps limit what we attempt? How do terms such as edition, project, database, archive, and thematic research collection relate to the past, present, and future of textual studies? Kenneth M. Price considers how current terms describing digital scholarship both clarify and obscure our collective enterprise. Price argues that the terms we use have more than expressive importance. The shorthand we invoke when explaining our work to others shapes how we conceive of and also how we position digital scholarship. ","Edition, Project, Database, Archive, Thematic Research Collection: What's in a Name?",,2009,,Digital Humanities Quarterly,003,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2438,,"{'string_id': None, 'id_scheme': None}"," This is the second of two essays on the nature of electronic representations of literary texts, the first focusing primarily on the original textual material and the materiality of literary works, and the present essay focusing first on the nature of the electronic surrogates to those material forms and, second, on the ways our decisions about how to create them can be affected by our notions of use. ",How Literary Works Exist: Convenient Scholarly Editions,,2009,,Digital Humanities Quarterly,003,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2439,,"{'string_id': None, 'id_scheme': None}"," Despite prevailingly progressive narratives surrounding the impact of digital technology on modern academic culture, the field of digital humanities is characterized at a deeper level by a more critical engagement with technology. This engagement, which I characterize as a kind of productive unease, is focused around issues of representation, medium, and structures of scholarly communication. ",The Productive Unease of 21st-century Digital Scholarship,,2009,,Digital Humanities Quarterly,003,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2440,,"{'string_id': None, 'id_scheme': None}"," This article presents the results of a semester-long project designed to determine how effectively interactive Web 2.0 technology can facilitate collaborative research in undergraduate learners. The study was conducted during a 2007 advanced Shakespeare course at the University of Central Florida that focused heavily on a new historicist approach to studying literature. In this paper we first establish the theoretical foundation for this particular approach to literary studies, then discuss more in-depth how the collaborative, inter-connective nature of wikis allowed students to witness first-hand some of the concealed assumptions enmeshed in the creation of historical explanation or narrative. We also discuss how, in thinking about the past, this technology allowed our students realize some of the stakes in describing history for the present. In other words, having students create wikis based on the social identities that recur in Shakespeare’s works developed an implicit awareness of motives for doing history. We also show how employing open source technology in a localized classroom setting can assuage some of the gaps we experience in trying to provide enough period coverage while also attending to theoretical apparatus and students’ experience of meaningful connections to material. On a larger scale, creating inquiry-based projects can alleviate some of the humanities’ disengagement from the real world that many have been suggesting of late. ",The Radical Historicity of Everything: Exploring Shakespearean Identity with Web 2.0,,2009,,Digital Humanities Quarterly,003,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2441,,"{'string_id': None, 'id_scheme': None}"," The past decade has seen XML widely adopted within a variety of communities, including the digital library community. While it now plays a critical role in the infrastructure of many digital library operations, XML's promise of interoperability of data across systems and organizations has not been fully realized within digital libraries. The reasons for this are not primarily technical in origin, but social, and relate to the cultures of XML's designers and XML language implementors, and a failure on the part of the digital library community to grapple with the sociotechnical nature of XML and its implementation. Possible strategies for addressing these issues of interoperability might include reduction of the flexibility afforded by specific XML-based markup languages used by the digital library community, and an increased focus on standardizing translations between various communities of practice use of such markup languages. ","XML, Interoperability and the Social Construction of Markup Languages: The Library Example",,2009,,Digital Humanities Quarterly,003,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2442,,"{'string_id': None, 'id_scheme': None}"," This article presents an examination of how digital humanities is currently conceived and described, and examines the discursive shift from humanities computing to digital humanities. It is argued that this renaming of humanities computing as digital humanities carries with it a set of epistemic commitments that are not necessarily compatible with a broad and inclusive notion of the digital humanities. In particular, the author suggests that tensions arise from the instrumental, textual and methodological focus of humanities computing as well as its relative lack of engagement with the digital as a study object. This article is the first in a series of four articles attempting to describe and analyze the field of digital humanities and digital humanities as a transformative practice. ",Humanities Computing as Digital Humanities,,2009,,Digital Humanities Quarterly,003,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2443,http://projects.chass.utoronto.ca/chwp/CHC2007/Ruecker_etal/Ruecker_etal.htm,"{'string_id': None, 'id_scheme': None}","This paper discusses our recent research projects in the area of information visualization systems for studying repetition in literary documents. Our rationale for these experiments is that readers typically notice repetition as part of the normal process of reading, and such moments of recognition can serve as useful means of identifying interesting elements for further inquiry; however, while ordinary reading skills are often all that are necessary for spotting repeated patterns within a single document, if the document is long or complicated, or once there are many documents involved, an automated approach combined with an appropriate interface to visualize the results can make the experience not only possible, but also interesting and pleasurable in its own right. We describe three design projects in this area, dealing respectively with an approach that accommodates the contemporary hardware configuration of monitor, keyboard, and mouse; a set of designs that strip away codex conventions; and a design that attempts to take advantage of the new affordances offered by a high-resolution wall-sized display.",Visualizing Repetition in Text,,2008,"['Computer-human interaction, interface design, large screen displays, text mining, data mining, knowledge discovery']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2444,,"{'string_id': None, 'id_scheme': None}"," This article offers an analysis of the video game in terms of the experience of gameplay, starting with the concept of the avatar, which is central to all video games. The avatar is typically described as our second self in the virtual world of the game. The paper challenges this theory, suggesting that the avatar is better understood as a set of possible interactions, and proposes a second concept, the avatari. If the avatar is our second self, the avatari is the rebellious figure on the screen that we cannot quite control, and that jumps into the pit, gets hit by the enemy, or otherwise fails despite our best efforts to succeed. Conceptualizing video games in terms of the avatar and avatari enables thinking about video games via a more sophisticated and productive model of interactivity than many of the existing paradigms. ",Avatari: Disruption and Imago in Video Games,,2009,,Digital Humanities Quarterly,003,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2445,,"{'string_id': None, 'id_scheme': None}"," In this paper, we describe the design of a number of alternative interface droplets that are intended for use by humanities scholars interested in applying data mining and information visualization tools to the task of hypothesis formulation. The trained droplets provide several functions. Their primary purpose is to encapsulate the results of the software training phase. They can be saved for future re-use against other collections or combinations of collections. They can be modified by having the user accept or reject features identified by the data mining software. Finally, they can also contain choices for how to display and organize items in the collection. The opportunity to develop a new interface object presents the designer with the challenge of effectively communicating what the tool is good for and how it is used. This paper outlines the design process we followed in creating the visual representations of these interface objects, describes the communicative strengths and weaknesses of a number of alternative designs, and discusses the importance of the study of new interface objects as the means of providing the user with new interface affordances. ",Designing Data Mining Droplets: New Interface Objects for the Humanities Scholar,,2009,,Digital Humanities Quarterly,003,3,"[{'value': '1938-4122', 'type': 'electronic'}]"
2446,,"{'string_id': None, 'id_scheme': None}"," In this article, we evaluate approaches using logic reasoning applied to an ontology for literary characters. The inference tool Racer and the programming language Prolog were tested and compared to see if they can serve as a computer assisted approach in this scenario. Both offer options to draw inferences, but the usage requests a good comprehension of logics. Intuitive and individual queries are also difficult to produce with solely logic constructs. Furthermore, information in humanities is often vague, ambiguous, or even contradictory. Solving such problems by logic reasoning which is simply based on true or false conclusions will become challenging and might exceed the limits of logic reasoning. At the moment, to the author’s knowledge, only few such approaches, especially in literature studies, have been published. Existing approaches show promising results in modelling information in humanities. Therefore, further research should be directed to make ontologies and further approaches like logic reasoning even more popular and applicable in the humanities and literature studies. ",Ontologies and Logic Reasoning as Tools in Humanities?,,2010,,Digital Humanities Quarterly,003,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2447,,"{'string_id': None, 'id_scheme': None}"," Broadly conceived, this article re-imagines the role of conjecture in textual scholarship at a time when computers are increasingly pressed into service as tools of reconstruction and forecasting. Examples of conjecture include the recovery of lost readings in classical texts, and the computational modeling of the evolution of a literary work or the descent of a natural language. Conjectural criticism is thus concerned with issues of transmission, transformation, and prediction. It has ancient parallels in divination and modern parallels in the comparative methods of historical linguistics and evolutionary biology. The article develops a computational model of textuality, one that better supports conjectural reasoning, as a counterweight to the pictorial model of textuality that now predominates in the field of textual scholarship. Computation is here broadly understood to mean the manipulation of discrete units of information, which, in the case of language, entails the grammatical processing of strings rather than the mathematical calculation of numbers to create puns, anagrams, word ladders, and other word games. The article thus proposes that a textual scholar endeavoring to recover a prior version of a text, a diviner attempting to decipher an oracle by signs, and a poet exploiting the combinatorial play of language collectively draw on the same library of semiotic operations, which are amenable to algorithmic expression. The intended audience for the article includes textual scholars, specialists in the digital humanities and new media, and others interested in the technology of the written word and the emerging field of biohumanities. ",Conjectural Criticism: Computing Past and Future Texts,,2010,,Digital Humanities Quarterly,003,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2448,,"{'string_id': None, 'id_scheme': None}"," e-Science and high performance computing (HPC) have the potential to allow large datasets to be searched and analysed quickly, efficiently, and in complex and novel ways. Little application has been made of the processing power of grid technologies to humanities data, due to lack of available large-scale datasets, and little understanding of or access to e-Science technologies. The Researching e-Science Analysis of Census Holdings (ReACH) scoping study, an AHRC-funded e-science workshop series, was established to investigate the potential application of grid computing to a large dataset of interest to historians, humanists, digital consumers, and the general public: historical census records. Consisting of three one-day workshops held at UCL in Summer 2006, the workshop series brought together expertise across different domains to ascertain how useful, possible, or feasible it would be to analyse datasets from Ancestry and The National Archives using the HPC facilities available at UCL. This article details the academic, technical, managerial, and legal issues highlighted in the project when attempting to apply HPC to historical data sets. Additionally, generic issues facing humanities researchers attempting to utilise HPC technologies in their research are presented. ",The Potential and Problems in using High Performance Computing in the Arts and Humanities: the Researching e-Science Analysis of Census Holdings (ReACH) Project.,,2010,,Digital Humanities Quarterly,003,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2449,,"{'string_id': None, 'id_scheme': None}"," Medievalists typically resort to parchment for primary research and when editing their sources. Not always accurately catalogued, manuscripts copied onto animal skins may have started life in the same workshop but over the centuries have become dispersed, coming to rest in libraries all over the world; bringing these together entails travel, microfilm purchases and reassembly and collation of the data within reach of a microfilm reader. These unwieldy machines afford only moderate scope for exploring single manuscripts at close quarters. High-resolution digitisation yields not just better surrogates in full colour; it allows for the development of additional research tools using image compression and manipulation, and new modes of representation, e.g. juxtaposed display of several related witnesses. This paper outlines research questions underpinning the development of an electronic tool for viewing, transcribing and manipulating manuscripts; it moves on to show how the viewer can be adapted for access from remote sites, to compare and annotation one or more witnesses (interactively and in real time), and for use as an integral part of an online edition. Finally, it explores how it can be deployed for use on projects taking knowledge outside the academy: in museums, galleries and other public spaces. ","e-Science for Medievalists: Options, Challenges, Solutions and Opportunities",,2010,,Digital Humanities Quarterly,003,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2450,,"{'string_id': None, 'id_scheme': None}"," Software Engineering, as a sub-discipline of the broader field of computer science, is concerned with the production, use, and maintenance of large, complex software systems. On first inspection, the set of managerial and technical activities involved in software engineering appears to be somewhat orthogonal to core research activity in the humanities, being concerned more with the production of research-enabling software systems than the research itself. However, as the scale of software used in digital humanities has increased, it is becoming clear that there are ways in which software engineering can inform, inspire, and aid in the management of the larger-scale software systems now being constructed in these disciplines. In particular, the development of service technology to aid in the production of flexible software systems for business now offers opportunities, not only for collaborative data sharing, but also the modelling, capture, provenancing, and replay of the research (and possibly creative) process itself. This paper examines, from the perspective of a software engineer relatively new to the digital humanities, how the recent developments in service-oriented architectures could be used to enable new approaches to digital enquiry in the arts and humanities. The first part of the paper presents a brief history of software engineering, with particular reference to the aspects that have led to service-oriented architectures. In the second part, the paper offers some thoughts on how certain aspects of service-oriented architectures could be used to enable new kinds of computer-based research and practice in the arts and humanities. It also introduces important national initiatives in this area, such as the JISC e-Framework programme for Higher Education. ",Service-Oriented Software in the Humanities: A Software Engineering Perspective,,2010,,Digital Humanities Quarterly,003,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2451,,"{'string_id': None, 'id_scheme': None}"," What does it mean to put an e ahead of a concept? This essay discusses the purpose of doing such a thing, arguing there is a distinct method in the apparent randomness of labelling something e this or that. Far from simply denoting that it might be done with computers (and, indeed, what isn't today), Sporton argues that beyond the effect of explaining this is something to do with technology, there is an emergent e-culture that reunites the arts and sciences after two hundred years of separate development within the academy. An e-Culture emerges that reflects the values, opportunities and restrictions of Internet as a research environment. The potential of that environment requires a mindset focussed on collaboration to achieve anything of creative significance. ","The e Prefix: e-Science, e-Art & the New Creativity",,2010,,Digital Humanities Quarterly,003,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2452,,"{'string_id': None, 'id_scheme': None}"," As the amount of scholarly material published in digital form increases, there is growing pressure on content producers to identify the needs of expert readers and to create online tools that satisfy their requirements. Based on the results of a study conducted by the Public Knowledge Project and introduced at Digital Humanities 2006 (Siemens, Willinsky and Blake), continued and augmented since, this paper discusses the reactions of Humanities Computing scholars and graduate students to using a set of online reading tools. The results of our study reveal both the potential strengths and perceived weaknesses of online reading environments. Understanding how users read and evaluate research materials, anticipating users’ expectations of the reading tools and resources, and addressing user concerns about the availability of online material will lead to improvements in the design and features of online publishing. ", It May Change My Understanding of the Field: Understanding Reading Tools for Scholars and Professional Readers,,2010,,Digital Humanities Quarterly,003,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2453,,"{'string_id': None, 'id_scheme': None}"," This paper arises out of a brief period in the early- to mid-2000s when the British funding and research climate facilitated a relationship between the technical, operational language of e-Science and the creative and performing arts. It concerns the ways in which live creative practices produce media traces that are fractured across screens and networks to produce new spatial relations between live events and their records. The split and contradictory subjectivities produced in these highly mediatized environments bring to the fore creative tensions between the live event and the recorded document. That is, the discourses, technologies and practices (if we may separate these) of e-Science not only produce new, spatial connections between events and their archives, they enact the liveness of archives as they are accessed and recombined to produce new art forms. Locating Grid Technologies: Performativity, Place, Space, a research workshop series funded by the A&H e-Science Initiative in its 2006 round, aimed to investigate how e-Science technologies might inform new understandings of space and time for distributed, creative research practices. Arts and technology researchers from the UK, US and Japan met to generate, analyze and re-use audio-visual documents of distributed practice-led research. Specifically, the project sought to combine and repurpose e-Science tools in order to investigate the spatial relationships produced between time-based, live events and their immediately mediatized traces. This paper investigates those performative fragmentations of place and space. It suggests that the potentialities and pitfalls of e-Science tools and technologies present fertile material for the arts researcher, particularly within the area of practice-based research: from the politics of surveillance to the aesthetics of video compression, from the ethics of multidisciplinary collaboration to the theoretical implications of mixing video time and space with the time and space of the performance event. ","Locating Grid Technologies: Performativity, Place, Space: Challenging the Institutionalized Spaces of e-Science",,2010,,Digital Humanities Quarterly,003,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2454,http://projects.chass.utoronto.ca/chwp/Casta02/Rockwell_casta02.htm,"{'string_id': None, 'id_scheme': None}","One problem in humanities computing is the automatic interpretation of electronic texts. Can computers interpret texts or do they aide in the interpretation? In this paper we propose variants on the Turing Test to imagine if computer-assisted interpretation is possible. We argue that a way forward for developing models for successful machine interpreters is to work with regular expression recognition and manipulation, which is built into most programming languages in a common fashion.",MIMes and MeRMAids: On the Possibility of Computer-aided Interpretation,,2005,"['Humanities Computing, Computer-Assisted Text Analysis, Interpretation']",CH Working Papers,,,"[{'value': '1205-5743', 'type': 'print'}]"
2455,,"{'string_id': None, 'id_scheme': None}"," The digital humanities are at a critical moment in the transition from a specialty area to a full-fledged community with a common set of methods, sources of evidence, and infrastructure — all of which are necessary for achieving academic recognition. As budgets are slashed and marginal programs are eliminated in the current economic crisis, only the most articulate and productive will survive. Digital collections are proliferating, but most remain difficult to use, and digital scholarship remains a backwater in most humanities departments with respect to hiring, promotion, and teaching practices. Only the scholars themselves are in a position to move the field forward. Experiences of the sciences in their initiatives for cyberinfrastructure and eScience offer valuable lessons. Information- and data-intensive, distributed, collaborative, and multi-disciplinary research is now the norm in the sciences, while remaining experimental in the humanities. Discussed here are six factors for comparison, selected for their implications for the future of digital scholarship in the humanities: publication practices, data, research methods, collaboration, incentives, and learning. Drawing upon lessons gleaned from these comparisons, humanities scholars are called to action with five questions to address as a community: What are data? What are the infrastructure requirements? Where are the social studies of digital humanities? What is the humanities laboratory of the 21st century? What is the value proposition for digital humanities in an era of declining budgets? ",The Digital Future is Now: A Call to Action for the Humanities,,2010,,Digital Humanities Quarterly,003,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2456,,"{'string_id': None, 'id_scheme': None}"," The term grid-enabling is sometimes (or even often) used without aclear idea of what is meant. In this article we attempt to clarifysome of the possible meanings of grid-enabling data resources. Inparticular, we examine how researchers in the humanities may benefitfrom using such approaches, and examine some concrete case studies inwhich grid technologies have been used to support data-driven researchin the humanities. ",Grid-enabling Humanities Datasets,,2010,,Digital Humanities Quarterly,003,4,"[{'value': '1938-4122', 'type': 'electronic'}]"
2457,,"{'string_id': None, 'id_scheme': None}"," The digital humanities is increasingly becoming a buzzword, and there is more and more talk about a broadly conceived, inclusive digital humanities. The field is expanding and at the same time being negotiated, and this article explores the idea of a broadly conceived landscape of digital humanities in some depth. It is argued that awareness across this landscape is important to the future of the field. The study starts out from typologies of digital humanities, a flythrough of the landscape, and a discussion of what being a digital humanist entails. The second part is an exploration of four concrete encounters: ACTLab at University of Texas at Austin, the Humanities Arts Science Technology Advanced Collaboratory (HASTAC), the Humanities Computing Program at the University of Alberta, and Internet Studies. In the third part of the article, it is suggested that a model based on paradigmatic modes of engagement between the humanities and information technology can help chart and understand the digital humanities. The modes of engagement analyzed are technology as a tool, study object, expressive medium, exploratory laboratory and activist venue. ",The Landscape of Digital Humanities,,2010,,Digital Humanities Quarterly,004,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2458,,"{'string_id': None, 'id_scheme': None}"," This article traces the development of two important hypertext systems in the history of computing, and the new paradigms they created: the Hypertext Editing System (HES) and the File Retrieval and Editing System (FRESS). HES was the world's first word processor to run on commercial equipment. It was also the first hypertext system that beginners could use, and pioneered many modern hypertext concepts for personal use. Although the idea of hypertext predates HES and FRESS, this article argues that these two systems were successful because they demonstrated hypertext to a sceptical public; they were both working prototypes. ",Crafting the User-Centered Document Interface: The Hypertext Editing System (HES) and the File Retrieval and Editing System (FRESS),,2010,,Digital Humanities Quarterly,004,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2459,,"{'string_id': None, 'id_scheme': None}"," In this article we propose different methods of encoding, according to the TEI Guidelines, three different cases of genetic or compositional textual variants found in the autographs of the Italian contemporary poet Valerio Magrelli. These encoding experiments reflect the diverse nature of the artifacts and represent a critical assessment of the effectiveness of present encoding practices for the multidimensional and pragmatic aspects of authorial drafts. Thus far, it seems that the TEI has yet to offer a convincing theoretical model and adequate practical solutions for representing the complex temporal structures normally present in manuscripts, and in fluid textual traditions in general. Our conclusion is that there is a potential conflict between the linear and hierchical nature of current formal language systems such as XML, and the intrinsic dynamic nature of the writing process. In such cases we may have to rethink present models of document modeling, and to develop, within an adequate epistemological framework, a new theory of digital text. ",Digital Encoding as a Hermeneutic and Semiotic Act: The Case of Valerio Magrelli,,2010,,Digital Humanities Quarterly,004,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2460,,"{'string_id': None, 'id_scheme': None}"," While the purpose and direction of tools and tool development for the Digital Humanities have been debated in various forums, the value of tool development as a scholarly activity has seen little discussion. As a way of filling this gap, the authors conducted an online survey of developers of digital humanities tools in March 2008. The survey focused on their perceptions of their work, how they felt their tool development fit into a structure of academic rewards, and the value of tool development as a scholarly pursuit. Survey results indicate that tool development is indeed considered a scholarly activity by developers, but recognition of this work and rewards for it lag behind rewards for traditional scholarly pursuits (such as journal articles and book publication). This paper presents a summary of the results of the survey, ending with some suggestions for further research. ",Determining Value for Digital Humanities Tools: Report on a Survey of Tool Developers ,,2010,,Digital Humanities Quarterly,004,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2461,,"{'string_id': None, 'id_scheme': None}"," This extended interview between founding H-Urban editor Wendy Plotkin and H-Urban reviews editor Sharon Irish traces the early history of online scholarly communication via H-Net, H-Urban, and COMM-ORG, informed by Plotkin’s background as a planner and community activist in the 1970s and 1980s. After work with community development corporations on the East Coast, Plotkin entered graduate school in urban history at the University of Illinois at Chicago. During this period in the early nineties, Plotkin had a job with the Text Encoding Initiative (TEI), and then collaborated on the development of the forum that became H-Net. In addition to standards and protocols about new communication technologies, face-to-face relationships grew out of online exchanges, often with lively disagreements about the direction of H-Net. Plotkin’s own broadening use of digital tools prompts her concluding reflections on historians’ continuing need to use the Internet to overcome physical and intellectual fragmentation, and how the Internet has democratized the field of history. ","History, People, and Informatics: A Conversation between Sharon Irish and Wendy Plotkin",,2010,,Digital Humanities Quarterly,004,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2462,,"{'string_id': None, 'id_scheme': None}"," Humanities scholars and librarians both confront questions regarding the boundaries of texts and the relationships between various editions, translations and adaptations. The Functional Requirements for Bibliographic Records (FRBR) Final Report from the International Federation of Library Associations has provided the library community with a model for addressing these questions in the bibliographic systems they create. The Preserving Virtual Worlds project has been investigating FRBR's potential as a model for the description of computer games and interactive fiction. While FRBR provides an attractive theoretical model, the complexity of computer games as works makes its application to such software creations problematic in practice. ",Twisty Little Passages Almost All Alike: Applying the FRBR Model to a Classic Computer Game,,2010,,Digital Humanities Quarterly,004,2,"[{'value': '1938-4122', 'type': 'electronic'}]"
2463,,"{'string_id': None, 'id_scheme': None}"," There is currently an infrastructure turn with very real implications for the humanities and digital humanities. It comes not only with presumed technology or infrastructure, but also with certain assumptions, discursive patterns, and models. This paper analyzes these critically and advocates a humanities-based notion of cyberinfrastructure, not necessarily built on a science-and-engineering paradigm or exclusively grounded in existing humanities infrastructure. It is argued that we need to maintain a critical stance while simultaneously engaging in the exploration of research issues and technologies. There is often a gap between the material details of infrastructure and underlying, foundational ideas, and it is suggested that a model based on conceptual cyberinfrastructure and design parameters can be one way of connecting the ideational level with actual implementation. HUMlab at Umeå University serves as a case study. ",From Optical Fiber To Conceptual Cyberinfrastructure,,2011,,Digital Humanities Quarterly,005,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
2464,,"{'string_id': None, 'id_scheme': None}"," As digital humanists have adopted visualization tools in their work, they have borrowed methods developed for the graphical display of information in the natural and social sciences. These tools carry with them assumptions of knowledge as observer-independent and certain, rather than observer co-dependent and interpretative. This paper argues that we need a humanities approach to the graphical expression of interpretation. To begin, the concept of data as a given has to be rethought through a humanistic lens and characterized as capta, taken and constructed. Next, the forms for graphical expression of capta need to be more nuanced to show ambiguity and complexity. Finally, the use of a humanistic approach, rooted in a co-dependent relation between observer and experience, needs to be expressed according to graphics built from interpretative models. In summary: all data have to be understood as capta and the conventions created to express observer-independent models of knowledge need to be radically reworked to express humanistic interpretation. ",Humanities Approaches to Graphical Display,,2011,,Digital Humanities Quarterly,005,1,"[{'value': '1938-4122', 'type': 'electronic'}]"
