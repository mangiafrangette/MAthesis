<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 220. Laubrock-Design of a Corpus and an Interactive Annotation Tool-220.docx</title><link rel="stylesheet" href="220_files/220.css" type="text/css"/>
</head>
<body><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font3" style="font-weight:bold;">Design of a corpus and an interactive annotation tool&nbsp;for graphic literature</span></h1>
<p><span class="font1" style="font-weight:bold;">Jochen Laubrock</span></p>
<p><span class="font1"><a href="mailto:laubrock@uni-potsdam.de">laubrock@uni-potsdam.de</a></span></p>
<p><span class="font1">University of Potsdam, Germany</span></p>
<p><span class="font1" style="font-weight:bold;">Sven Hohenstein</span></p>
<p><span class="font1"><a href="mailto:hohenstein@uni-potsdam.de">hohenstein@uni-potsdam.de</a></span></p>
<p><span class="font1">University of Potsdam, Germany</span></p>
<p><span class="font1" style="font-weight:bold;">Eike Martin Richter</span></p>
<p><span class="font1"><a href="mailto:eike.richter@uni-potsdam.de">eike.richter@uni-potsdam.de</a> University of Potsdam, Germany</span></p>
<p><span class="font1">The project &quot;Hybrid Narrativity&quot; combines work by language and literature studies, cognitive psychology,&nbsp;and computer science with the overarching goal to&nbsp;arrive at an empirically founded narratology of&nbsp;graphic literature, including comics and graphic&nbsp;novels. Comics and graphic novels provide a unique&nbsp;cultural form that has developed its own vocabulary,&nbsp;allowing for a fascinating interplay of text and visual&nbsp;art. After a period of neglect, they have recently been&nbsp;theoretically analyzed in detail by scholars in the arts,&nbsp;humanities and linguistics (McCloud, 1993;&nbsp;Groensteen, 2007; Cohn, 2013). Our aim is to provide&nbsp;an empirical testbed for these theories. The&nbsp;foundation of this endeavor is a large collection of&nbsp;graphic novels, which are annotated using a variety of&nbsp;methods. These include high-level descriptions of the&nbsp;work, mid-level descriptions of pages and panels,&nbsp;including the actors / characters, text, objects, and&nbsp;panel transitions, and low-level descriptions of visual&nbsp;elements in terms of descriptors developed in&nbsp;computer vision such as color histograms, GIST, SIFT,&nbsp;and SURF features. We are currently evaluating the&nbsp;addition of mid-level features from deep networks&nbsp;trained on photographs of real-world scenes, with&nbsp;quite promising first results.</span></p>
<p><span class="font1">These descriptions in terms of material properties are complemented by eye-tracking data, providing an&nbsp;empirical measure of the reader-level attention&nbsp;distribution and the time course of attention shifts.&nbsp;Thus, the digitized representation of literary and&nbsp;artistic works includes information on the side of&nbsp;&quot;recipients&quot;, that is, readers, viewers, spectators and&nbsp;appreciators with their psychological and&nbsp;physiological responses. In a first step, eye-tracking&nbsp;data on a small sample of pages from selected works&nbsp;were collected from a large number of participants in&nbsp;order to evaluate general principles of attentional&nbsp;selection in graphic literature. Results show that&nbsp;reading of graphic novels is primarily governed by&nbsp;reading of text, and that inspection of graphical&nbsp;elements is apparently governed by top-down&nbsp;selection of story-relevant elements. In perspective,&nbsp;eye-tracking data will be collected for each of the&nbsp;works in the corpus, using a sample of pages and a&nbsp;smaller number of readers.</span></p>
<p><span class="font1">A graphical annotation tool is in development and has first been released to the public at DH 2016. This&nbsp;tool is based on an XML dialect that allows for the&nbsp;annotation of language as well as graphical elements.&nbsp;Future versions will include OCR support for comics&nbsp;fonts, and provide customizable annotation schemes,&nbsp;allowing other researchers to implement their own&nbsp;research ideas. We will also briefly present ideas on&nbsp;the potential to incorporate gaze-based interaction in&nbsp;the user interface of the tool, e.g., for the intuitive&nbsp;selection of objects, which will become important with&nbsp;the projected availability of low-cost eye trackers in&nbsp;the near future.</span></p>
<p><span class="font1">We have developed the Graphic Novel Markup Language (GBML) as an extension of John Walsh's&nbsp;Comic Book Markup Language (CBML; Walsh, 2012) to&nbsp;facilitate the description of graphical elements. These&nbsp;descriptions are imperative for defining regions-of-interest based mapping of eye movement data to the&nbsp;stimulus material. Material has been annotated using&nbsp;our editor, and a custom R package is under&nbsp;development and in use for statistical analysis of eye&nbsp;movements. Visual features are currently extracted&nbsp;using OpenCV (Bradski, 2000, 2016) and VLFEAT&nbsp;(Vedaldi &amp; Fulkerson, 2008) libraries from Python and&nbsp;Matlab, since R does not yet provide sufficiently&nbsp;extensive packages for this purpose (for a promising&nbsp;approach see imager, Barthelme, 2016). Deep features&nbsp;are based on Deep Gaze II (Kümmerer, Wallis, &amp;&nbsp;Bethge, 2016), which is in turn based on the VGG-19&nbsp;network (Simonyan &amp; Zisserman, 2014). A description&nbsp;of artworks in terms of visual features has shown&nbsp;promising results in other domains (Elgammal &amp;&nbsp;Saleh, 2015).</span></p>
<p><span class="font1">A number of analyses using the corpus data show the potential for comparative studies as well as&nbsp;detailed study of individual works. Many of the&nbsp;testable hypotheses can be derived from the&nbsp;theoretical work cited above. For example, McCloud&nbsp;(1993) speculated that the cognitive effort of the&nbsp;recipient depends on the kind of panel transition, or&nbsp;that the empty space between panels is used to signal&nbsp;the passage of time. We provide empirical support for&nbsp;both of these hypotheses. Other examples include&nbsp;visual trends that can be identified across time or&nbsp;between regions, linguistic and visual analyses that&nbsp;can be used to compare text and visual complexity&nbsp;between different genres, and network analyses of&nbsp;interactions between characters that allow for an easy&nbsp;quantification and visualization of roles within a work,&nbsp;and for a comparison between works. They can also be&nbsp;used, e.g., to compare the complexity of a novel and its&nbsp;adaptation to the graphic novel format.</span></p>
<p><span class="font1">An in-depth analysis of a single work, the graphic novel adaptation of Paul Auster's City of Glass by Paul&nbsp;Karasik and David Mazzucchelli, shows that readers of&nbsp;graphic literature benefit from a specific expertise in&nbsp;decoding the different channels of information&nbsp;conveyed by image and text. Comics experts spend&nbsp;significantly more time on the image part of the panels,&nbsp;and this is correlated with a significantly deeper&nbsp;understanding of the narrative. New data suggests&nbsp;that this pattern replicates across samples, labs, and&nbsp;languages.</span></p>
<p><span class="font1">Taken together, we present the design of a corpus of graphic literature that is annotated using a variety&nbsp;of levels, including readers' eye movements. Ideas for&nbsp;how to make use of these data for interactive future&nbsp;versions are developed, and analyses of the collected&nbsp;data in terms of description as well as reception of the&nbsp;works of art are presented.</span></p>
<p><span class="font0" style="font-weight:bold;">Groensteen, T. </span><span class="font0">(2007). The System of Comics. Translated by B. Beaty and N. Nguyen. Jackson, MI: University of&nbsp;Mississippi Press.</span></p>
<p><span class="font0" style="font-weight:bold;">Kümmerer, M., Wallis, T.S.A., &amp; Bethge, M </span><span class="font0">(n.d.).:</span></p>
<p><span class="font0">DeepGaze II: Reading fixations from deep features trained on object recognition. arXiv:1610.01563</span></p>
<p><span class="font0" style="font-weight:bold;">McCloud, S. </span><span class="font0">(1993). Understanding Comics: The Invisible Art. New York: Harper Collins.</span></p>
<p><span class="font0" style="font-weight:bold;">Simonyan, K. &amp; Zisserman, A. </span><span class="font0">(2014). Very Deep Convolutional Networks for Large-Scale Image&nbsp;Recognition. In: CoRR abs/1409.1556. url:&nbsp;<a href="http://arxiv.org/abs/1409.1556">http://arxiv.org/abs/1409.1556</a>.</span></p>
<p><span class="font0" style="font-weight:bold;">Vedaldi, A. &amp; Fulkerson, B. </span><span class="font0">(2008). VLFeat: An open and portable library of computer vision algorithms.&nbsp;[Computer Software: <a href="http://www.vlfeat.org/">http://www.vlfeat.org/</a> ]</span></p>
<p><span class="font0" style="font-weight:bold;">Walsh, J. </span><span class="font0">(2012). Comic Book Markup Language: An Introduction and Rationale. Digital Humanities&nbsp;Quarterly, 6(1).</span></p>
<p><span class="font2" style="font-weight:bold;">Bibliography</span></p>
<p><span class="font0" style="font-weight:bold;">Barthelme, S. </span><span class="font0">(2016). imgager: Image Processing Library Based on 'CImg'. R package version 0.31. [Computer&nbsp;Software:&nbsp;&nbsp;&nbsp;&nbsp;https://CRAN.R-</span></p>
<p><span class="font0">project.org/package=imager]</span></p>
<p><span class="font0" style="font-weight:bold;">Bradski, G. </span><span class="font0">(2000). The OpenCV library. Dr. Dobb’s Journal of Software Tools, 25(11):120, 122-125.</span></p>
<p><span class="font0" style="font-weight:bold;">Cohn, N. </span><span class="font0">(2013). The Visual Language of Comics. Introduction to the Structure and Cognition of&nbsp;Sequential Images. London: Bloomsbury.</span></p>
<p><span class="font0" style="font-weight:bold;">Elgammal, A. &amp; Saleh, B. </span><span class="font0">(2015). Quantifying Creativity in Art Networks. 6th International Conference on&nbsp;Computational Creativity (ICCC’15).</span></p>
</body>
</html>