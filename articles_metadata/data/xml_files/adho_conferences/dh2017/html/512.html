<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 512.Devaney-Leveraging Expert Domain Knowledge-512.docx</title><link rel="stylesheet" href="512_files/512.css" type="text/css"/>
</head>
<body>
<p><span class="font4" style="font-weight:bold;">Leveraging Expert Domain Knowledge to Learn a&nbsp;Representation of Symbolic&nbsp;Music</span></p>
<p><span class="font2" style="font-weight:bold;font-variant:small-caps;">JoHanna Devaney</span></p>
<p><span class="font2"><a href="mailto:devaney.12@osu.edu">devaney.12@osu.edu</a></span></p>
<p><span class="font2">Ohio State University, United States of America</span></p>
<p><span class="font2">Music manuscripts offer as much potential as text manuscripts for data mining and, as with resources&nbsp;like Google Books, there is a wealth of data available&nbsp;online. Currently the largest resource, the International Music Score Library Project (IMSLP), has more&nbsp;than 370,000 scores in its database. While many of&nbsp;these scores exist only in image-based formats, ongoing improvements in the area of Optical Music Recognition (OMR) allow for automatic conversion from images to symbolic representations, which include information such as instrumentation, key signature, time&nbsp;signature, and the notes' pitches, metrical positions,&nbsp;and durations. In order to exploit the research potential of these symbolic music databases, a representation that captures temporal relationships within the&nbsp;music is needed that highlights the structurally significant parts of the musical surface, while ignoring ornamentation.</span></p>
<p><span class="font2">This paper describes a representation that emphasizes the more structurally significant parts of the musical surface and de-emphasizes less significant parts, such as ornamentation, by integrating human domain&nbsp;expertise and data-driven approaches within a temporal machine learning model. The representation&nbsp;contains less information than the musical surface but&nbsp;more than corresponding chord labels, which discard&nbsp;information about musical texture and are too generalized to use for detailed similarity and classification&nbsp;tasks. The weighting for the various components of the&nbsp;musical surface is determined from an initial harmonic&nbsp;analysis. This harmonic analysis will be performed by&nbsp;a hierarchical model of chord labels and phrases,&nbsp;which will function like a “language model” in speech&nbsp;recognition. In music theory, phrase models describe&nbsp;musical phrases in terms of the tonic, predominant,&nbsp;and dominant functions (Laitz 2012). The inclusion of&nbsp;the expert domain knowledge expressed in the phrase&nbsp;function model helps to resolve the ambiguity between the musical surface and appropriate chord labels in the harmonic analysis, namely whether a particular chord is likely to occur in a particular part of a&nbsp;phrase. Taken in combination with OMR, this representation could be used to render searchable all available scanned music. These searches would not be limited to melody, as is the current state-of-the-art, but&nbsp;would also allow for querying by chord progressions&nbsp;and/or formal structures. The representation can also&nbsp;facilitate automatic hierarchical analysis of musical&nbsp;structures and provides a basis from which to undertake classifications and similarity tasks. Classification&nbsp;tasks include harmonic analysis or assessing the likelihood of a particular composer having composed a&nbsp;piece of unknown provenance, while similarity tasks&nbsp;include longitudinal studies over a composer's career&nbsp;or across composers.</span></p>
<p><span class="font2">Much of the work on analyzing the growing wealth of music data has been heavily influenced by text retrieval methods through their use of N-grams, sequences of N contiguous symbols. N-grams work well&nbsp;in modeling monophonic sequences, such as when directly applied to the musical surface for monophonic&nbsp;melody retrieval (Pickens, 2001) and for chord retrieval when the chords occur as distinct vertical units&nbsp;(Scholz et al., 2009). This has been demonstrated effectively on peachnote.com (Viro, 2011) with an N-gram viewer similar to the one Google makes available&nbsp;for Google Books. One significant area where N- grams&nbsp;have problems, however, is for more complex textures&nbsp;where the notes of chords are not played simultaneously, which is true of a large proportion of western&nbsp;art music since 1750. One way to address this problem&nbsp;is to automatically segment the musical surface into&nbsp;beat-length frames and treat the contents of each&nbsp;frame as a “chord” (Radicioni and Espositio, 2006),&nbsp;which is well suited to chorale textures but is problematic for arpeggiations or other textures where the&nbsp;chords notes don't occur simultaneously. Another approach is to analyze chord labels rather than the musical surface, such as the system of de Haas et. al (2011),&nbsp;although these are often not available.</span></p>
<p><span class="font2">The representation described in this paper takes a different approach, using a conditional random fields&nbsp;(CRFs) model for developing both a data-driven&nbsp;model, where all of the feature functions and potentials are learned from the data, and a hybrid data-driven/rule- driven approach, where domain&nbsp;knowledge “rules&quot; are used to design feature and potential functions. Data for the purely data-driven approach comes from a domain expert-labeled dataset of</span></p>
<p><span class="font2">Mozart and Beethoven piano works in theme and variation form (Devaney et al. 2015). The rule-driven approach incorporates the rules presented in textbooks used in undergraduate music theory curricula, primarily Laitz (2012). This hybrid data- and rule- driven approach is motivated by previous work that demonstrated that a combination of data- and rule-driven&nbsp;models performed better than either approach alone&nbsp;on music analysis tasks (Devaney and Shanahan&nbsp;2013).</span></p>
<p><span class="font2">This paper will also discuss the implications of this use CRFs for analyzing other metrically structured cultural products, such as poetry or song lyrics, as well as&nbsp;how this approach could be generalized to other digital humanities projects, specifically for relatively&nbsp;“data- poor” problems where there is a large amount&nbsp;of domain expertise that can be modeled, such as the&nbsp;study of narrative in natural language. More broadly,&nbsp;this work presents a vision of the digital humanities,&nbsp;where large-scale data-driven approaches are balanced by deep domain knowledge and the types of humanistic questions being asked require the development of more sophisticated technology than is currently available.</span></p>
<p><span class="font1">Technical Report.</span></p>
<p><span class="font1" style="font-weight:bold;">Radicioni, D.P. and Esposito, R. </span><span class="font1">(2006). Learning tonal harmony from Bach chorales. In </span><span class="font1" style="font-style:italic;">Proceedings of the International Conference on Cognitive Modelling</span><span class="font1">.</span></p>
<p><span class="font1" style="font-weight:bold;">Scholz, R., Vincent, E., and F. Bimbot, F</span><span class="font1">. (2009). Robust modeling of musical chord sequences using probabilistic N-grams. In </span><span class="font1" style="font-style:italic;">Proceedings of the International Conference on Acoustics, Speech and Signal Processing</span></p>
<p><span class="font1" style="font-style:italic;">(ICASSP).</span><span class="font1"> 53-6.</span></p>
<p><span class="font1" style="font-weight:bold;">Viro, V. </span><span class="font1">(2011). Peachnote: Music score search and analysis platform. In </span><span class="font1" style="font-style:italic;">Proceedings of the International Society for&nbsp;Music Information Retrieval Conference (ISMIR)</span><span class="font1">. 35962.</span></p>
<p><span class="font0" style="font-weight:bold;">Bibliography</span></p>
<p><span class="font1" style="font-weight:bold;">de Haas, W.B., J.P. Magalhaes, R.C. Veltkamp, F. Wiering.</span></p>
<p><span class="font1">(2011). HarmTrace: Improving Harmonic similarity estimation using functional harmony analysis. In </span><span class="font1" style="font-style:italic;">Proceedings of International Society of Music Information</span></p>
<p><span class="font1" style="font-style:italic;">Retrieval conference (ISMIR).</span><span class="font1"> 67-72.</span></p>
<p><span class="font1" style="font-weight:bold;">Devaney, J., C. Arthur, N. Condit-Schultz, and K. Nisula.</span></p>
<p><span class="font1">(2015). Theme And Variation Encodings with Roman</span></p>
<p><span class="font1">Numerals (TAVERN): A new data set for symbolic music analysis. In </span><span class="font1" style="font-style:italic;">Proceedings of the International Society for Music Information Retrieval (ISMIR) conference</span><span class="font1">,&nbsp;728-34.</span></p>
<p><span class="font1" style="font-weight:bold;">Devaney, J., and D. Shanahan. </span><span class="font1">(2014). Evaluating Rule- and</span></p>
<p><span class="font1">Exemplar-Based Computational Approaches to Modeling Harmonic Function in Music Theory Pedagogy. In</span></p>
<p><span class="font1" style="font-style:italic;">Proceedings of the 9th Conference on Interdisciplinary Musicology.</span></p>
<p><span class="font1" style="font-weight:bold;">Laitz, S. G. </span><span class="font1">(2011). </span><span class="font1" style="font-style:italic;">The Complete Musician</span><span class="font1">. Oxford: Oxford University Press, 3rd edition. Pickens, J. 2001. A survey</span></p>
<p><span class="font1">of feature selection techniques for music information retrieval.</span></p>
<p><span class="font1" style="font-weight:bold;">Center for Intelligent Information Retrieval, </span><span class="font1">Department of Computer Science, University of Massachusetts,</span></p>
</body>
</html>