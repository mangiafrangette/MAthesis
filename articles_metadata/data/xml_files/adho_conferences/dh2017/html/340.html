<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 340. Krug-Overcoming Data Sparsity for Relation Detection in German Novels-340.docx</title><link rel="stylesheet" href="340_files/340.css" type="text/css"/>
</head>
<body><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font3" style="font-weight:bold;">Overcoming Data Sparsity for Relation Detection in&nbsp;German Novels</span></h1><h3><a name="bookmark1"></a><span class="font7" style="font-weight:bold;">Markus Krug</span></h3>
<p><span class="font7"><a href="mailto:markus.krug@uni-wuerzburg.de">markus.krug@uni-wuerzburg.de</a></span></p>
<p><span class="font7">University of Wuerzburg, Germany</span></p><h3><a name="bookmark2"></a><span class="font7" style="font-weight:bold;">Isabella Reger</span></h3>
<p><span class="font7"><a href="mailto:isabella.reger@uni-wuerzburg.de">isabella.reger@uni-wuerzburg.de</a></span></p>
<p><span class="font7">University of Wuerzburg, Germany</span></p><h3><a name="bookmark3"></a><span class="font7" style="font-weight:bold;">Fotis Jannidis</span></h3>
<p><span class="font7"><a href="mailto:fotis.jannidis@uni-wuerzburg.de">fotis.jannidis@uni-wuerzburg.de</a></span></p>
<p><span class="font7">University of Wuerzburg, Germany</span></p><h3><a name="bookmark4"></a><span class="font7" style="font-weight:bold;">Lukas Weimer</span></h3>
<p><span class="font7"><a href="mailto:lukas.weimer@uni-wuerzburg.de">lukas.weimer@uni-wuerzburg.de</a></span></p>
<p><span class="font7">University of Wuerzburg, Germany</span></p><h3><a name="bookmark5"></a><span class="font7" style="font-weight:bold;">Nathalie Madarasz</span></h3>
<p><span class="font7"><a href="mailto:nathalie.madarasz@stud-mail.uni-wuerzburg.de">nathalie.madarasz@stud-mail.uni-wuerzburg.de</a> University of Wuerzburg, Germany</span></p><h3><a name="bookmark6"></a><span class="font7" style="font-weight:bold;">Frank Puppe</span></h3>
<p><span class="font7"><a href="mailto:frank.puppe@uni-wuerzburg.de">frank.puppe@uni-wuerzburg.de</a></span></p>
<p><span class="font7">University of Wuerzburg, Germany</span></p><h2><a name="bookmark7"></a><span class="font2" style="font-weight:bold;">Introduction</span></h2>
<p><span class="font7">Within the context of social network analysis (SNA) for literary texts the automatic detection of family&nbsp;relations and similar social relations between&nbsp;characters in novels would be an important step for&nbsp;any macroscopic analysis. Manual labeling is rather&nbsp;inefficient since the text snippets that explicitly&nbsp;describe a relation are sparse within the long text&nbsp;documents; therefore we combine two techniques,&nbsp;active learning and distant supervision, which are&nbsp;often used to overcome data sparsity.</span></p>
<p><span class="font7">Inspired by distant supervision which uses a high quality information resource to support information&nbsp;extraction from other data, we used expert summaries&nbsp;of literary texts, German novels mainly from the 19th&nbsp;century, since relevant text snippets are much more&nbsp;frequent in summaries. Then we applied an&nbsp;uncertainty-based active learning strategy labeling&nbsp;selected sentences from the novels and the complete&nbsp;summaries. The results show that training on&nbsp;summaries and evaluating on data derived from&nbsp;novels yields reasonable results with high precision&nbsp;and low recall similar to humans solving this task.&nbsp;After a brief discussion of related work in the next&nbsp;section, the data set and the necessary preprocessing&nbsp;for this work are explained in section three. Section&nbsp;four describes our method in detail and shows&nbsp;strengths and weaknesses.</span></p><h2><a name="bookmark8"></a><span class="font2" style="font-weight:bold;">Related work</span></h2>
<p><span class="font7">The challenge of training an algorithm capable of generalizing from a small set of manually labeled data&nbsp;has created a multitude of approaches like active&nbsp;learning and distant supervision. A good survey on&nbsp;active learning is given in (Finn et al., 2003). Usually it&nbsp;starts with a seed set of manually annotated data. A&nbsp;classifier is then trained and new instances that&nbsp;appear to be very different from the current training&nbsp;data are proposed for manual labeling until the quality&nbsp;of the classifier stops improving. Successful&nbsp;algorithms include Multi-instance Multi-label Relation&nbsp;Extraction (Surdeanu et al., 2012).</span></p>
<p><span class="font7">Another method specifically used for relation detection in newspapers is distant supervision (Mintz&nbsp;et al., 2009): Given some facts (e.g. Michelle Obama is&nbsp;the wife of Barack Obama), usually stored in a&nbsp;database, the aim is to match those facts to the text&nbsp;(e.g. every sentence containing Michelle and Barack&nbsp;Obama indicates that they are married). The training&nbsp;of the classifier is then performed on the pseudo gold&nbsp;data. Even though the idea appears to be simplistic, the&nbsp;results are comparable to those obtained by active&nbsp;learning.</span></p>
<p><span class="font7">Jing et al. (Jing et al. 2007) successfully applied relation extraction for SNA in an end-to-end manner&nbsp;and reported that most problems were caused by&nbsp;coreference resolution.</span></p><h2><a name="bookmark9"></a><span class="font2" style="font-weight:bold;">Data and preprocessing</span></h2>
<p><span class="font7">We created three datasets from 213 expert summaries, available from Kindler Literary Lexicon&nbsp;Online, and 1700 novels derived from project&nbsp;Gutenberg and annotated relations between&nbsp;characters:</span></p>
<p><span class="font7">• We split 500 novels into sentences and applied an uncertainty-based active learning&nbsp;strategy (explained below) to iteratively&nbsp;select new examples (in this case full&nbsp;sentences) using a MaxEnt classifier. In total,&nbsp;about 1100 sentences were labeled in this&nbsp;way. This was labeled by annotator 1. (From&nbsp;now on, we refer to this as the novel data set)</span></p>
<p><span class="font7">• &nbsp;&nbsp;&nbsp;We split our summaries into sentences and&nbsp;applied the same active learning strategy to&nbsp;select new examples, thereby generating&nbsp;about 1300 labeled sentences. They were&nbsp;labeled by annotator 1. (From now on, this is&nbsp;called summaries I)</span></p>
<p><span class="font7">• &nbsp;&nbsp;&nbsp;Each of the 213 summaries has been&nbsp;manually labeled with all character&nbsp;references, the co-reference chains amongst&nbsp;them and relation annotations for pairs of&nbsp;entities that are explicitly mentioned to be in&nbsp;a relation. They have been labeled by&nbsp;annotator 2. (From now on, we call this&nbsp;summaries II)</span></p>
<p><span class="font7">The applied active learning strategy started by manually selecting about 20 seed training sentences&nbsp;which were manually labeled with information about&nbsp;relations between character references. The seed&nbsp;examples were chosen by matching a wordlist&nbsp;containing indicative expressions (such as “mother”,&nbsp;“father”, “servant” or “loves”) to the text, to enable the&nbsp;classifier to learn relations from different relation&nbsp;types in an unbiased fashion (which usually changes&nbsp;during training because the underlying distribution of&nbsp;relations is heavily biased towards family relations).&nbsp;On those seed examples we trained a binary Maximum&nbsp;Entropy classifier which was applied to thousands of&nbsp;unlabeled sentences. The sentences were then ranked&nbsp;by uncertainty of the classifier. Uncertainty for a&nbsp;sentence, in our case, was defined by extracting all&nbsp;pairs of character references first, applying the&nbsp;classifier to every pair and then assigning the&nbsp;minimum probability to the sentence. The classifier&nbsp;was retrained on command of the user and the ranking&nbsp;of the unlabeled sentences restarted. By applying this&nbsp;strategy, we observed that the average certainty of a&nbsp;sentence rises with every iteration and decided to stop&nbsp;the manual labeling once there was no sentence with&nbsp;a classifier probability below 60% for the novels and&nbsp;70% for the summaries (this does not mean we&nbsp;reached saturation in classification gain).</span></p>
<p><span class="font7">For the labeling, we used a total of 57 hierarchically ordered relation labels, inspired by (Massey et al.,&nbsp;2015) (see figure 1). All these labels relate person-entities with each other, such as “motherOf” or “loves”.</span></p>
<p><span class="font4">hasRelation</span></p><img src="340_files/340-1.jpg" style="width:166pt;height:14pt;"/>
<p><span class="font4">hasFamilyRelation hasLoveRelation hasProfessionalRelation hasSocialRelation</span></p>
<p><span class="font1">Figure 1: The four main relation types which are further differentiated in 57 relation types in total</span></p>
<p><span class="font7">The inter-annotator-agreement (IAA) between summary data set I and summary data set II was&nbsp;measured in two ways:</span></p>
<p><span class="font7">1. &nbsp;&nbsp;&nbsp;A true positive appears when both&nbsp;annotators mark the correct span of the&nbsp;annotation as well as the correct label and&nbsp;the correct arc direction where a correct arc&nbsp;links the two entities in the direction as it is&nbsp;expressed in the text (labeled interannotator agreement).</span></p>
<p><span class="font7">2. &nbsp;&nbsp;&nbsp;A true positive appears when both&nbsp;annotators mark the correct span and arc&nbsp;direction of the relation (unlabeled interannotator agreement).</span></p>
<p><span class="font7">Table 1 gives an overview of the IAA results.</span></p>
<table border="1">
<tr><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font7" style="font-weight:bold;">Precision</span></p></td><td style="vertical-align:middle;">
<p><span class="font7" style="font-weight:bold;">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font7" style="font-weight:bold;">FI</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">ilalabekd iaa</span></p></td><td style="vertical-align:middle;">
<p><span class="font7">75.6%</span></p></td><td style="vertical-align:middle;">
<p><span class="font7">43.7%</span></p></td><td style="vertical-align:middle;">
<p><span class="font7">55.4%</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font2">Labeled iaa</span></p></td><td style="vertical-align:middle;">
<p><span class="font7">60.9%</span></p></td><td style="vertical-align:middle;">
<p><span class="font7">35.2%</span></p></td><td style="vertical-align:middle;">
<p><span class="font7">44.6%</span></p></td></tr>
</table>
<p><span class="font1">Table 1: IAA results, the comparison assumed summaries II as gold and compared summaries I to it.</span></p>
<p><span class="font7">Additionally, we determined 55.5% as the normalized Cohen's Kappa between our annotators.&nbsp;The results for the IAA are surprisingly low (amount&nbsp;of labeled relations in summaries I compared to the&nbsp;relations in summaries II). The reasons are yet unclear&nbsp;and have to be investigated; we assume that one of&nbsp;them is the high variance of possibilities to express&nbsp;social relations. Labeling the complete summaries may&nbsp;also be more difficult because the annotator needs to&nbsp;read the text completely and might use background&nbsp;knowledge to annotate relations which are only&nbsp;implicit in the text.</span></p><h2><a name="bookmark10"></a><span class="font2" style="font-weight:bold;">Method and evaluation</span></h2>
<p><span class="font7">To compare the transfer from summaries to novels, we trained a classifier, specifically a maximum entropy&nbsp;classifier based on boolean features generated from&nbsp;rule templates because previous work has shown that&nbsp;this classifier is superior in classification accuracy&nbsp;compared to kernel machines, pure rule based&nbsp;approaches or other supervised classifiers such as&nbsp;support vector machines (Krug et al., 2017). Training&nbsp;was done on a data set using the annotations as&nbsp;features and the classifier was applied either to test&nbsp;data from the same set or to a different data set&nbsp;resulting in three evaluations:</span></p>
<p><span class="font7">• &nbsp;&nbsp;&nbsp;A 5-fold cross evaluation within the novel&nbsp;data set.</span></p>
<p><span class="font7">• &nbsp;&nbsp;&nbsp;Training on the snippets of the summaries&nbsp;(summaries I or summaries II) and&nbsp;evaluation on the novel data set.</span></p>
<p><span class="font7">Table 2 shows the result of this experiment for the indata and cross data evaluation of the relation detection component.</span></p>
<table border="1">
<tr><td>
<p><span class="font5" style="font-weight:bold;">Evaluation</span></p></td><td>
<p><span class="font5" style="font-weight:bold;">Precision</span></p></td><td>
<p><span class="font5" style="font-weight:bold;">Recall</span></p></td><td style="vertical-align:middle;">
<p><span class="font5" style="font-weight:bold;">Fl-Score</span></p>
<p><span class="font5" style="font-weight:bold;">(micro)</span></p></td><td style="vertical-align:middle;">
<p><span class="font5" style="font-weight:bold;">Labeling efficiency in relations per sentence</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font5" style="font-weight:bold;">nexsl data set</span></p></td><td style="vertical-align:middle;">
<p><span class="font5" style="font-weight:bold;">78.4%</span></p></td><td style="vertical-align:middle;">
<p><span class="font5" style="font-weight:bold;">48.9%</span></p></td><td style="vertical-align:middle;">
<p><span class="font5" style="font-weight:bold;">60.2%</span></p></td><td style="vertical-align:middle;">
<p><span class="font5" style="font-weight:bold;">0.56</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font5" style="font-weight:bold;">summaries i -&gt; </span><span class="font0">asœte</span></p></td><td>
<p><span class="font5" style="font-weight:bold;">75.6%</span></p></td><td>
<p><span class="font5" style="font-weight:bold;">52.7%</span></p></td><td>
<p><span class="font5" style="font-weight:bold;">62.1%</span></p></td><td>
<p><span class="font5" style="font-weight:bold;">0.52</span></p></td></tr>
<tr><td style="vertical-align:middle;">
<p><span class="font5" style="font-weight:bold;">summums n -&gt;</span></p>
<p><span class="font0">asœte</span></p></td><td>
<p><span class="font5" style="font-weight:bold;">65.5%</span></p></td><td>
<p><span class="font5" style="font-weight:bold;">54.1%</span></p></td><td>
<p><span class="font5" style="font-weight:bold;">59.2%</span></p></td><td>
<p><span class="font5" style="font-weight:bold;">0.75</span></p></td></tr>
</table>
<p><span class="font1">Table 2: The results of a 5-fold in-data set evaluation for both of the data sets and the results for a cross-data set&nbsp;evaluation. Each number represents a micro-average score,&nbsp;i.e. we count every true-positive, false-positive and falsenegative in a document and calculate the average scores&nbsp;based on these quantities. We choose the micro score since&nbsp;the label set is rather unbalanced between classes. The&nbsp;efficiency of an approach is measured by calculating&nbsp;number of relations / number of sentences.</span></p>
<p><span class="font7">Results that are very similar to working directly on the novel (60.2% F1) are achieved by using the model&nbsp;trained on the extracted sentences from the&nbsp;summaries to retrieve information about character&nbsp;relations in the novels (62.1% resp. 59.2%). Since our&nbsp;test data is generated by active learning and only the&nbsp;most difficult examples were chosen for labeling, we&nbsp;expect our results to be a lower bound compared to&nbsp;data in complete novels.</span></p>
<p><span class="font7">If we use a model trained on the complete summaries, we experience a drop in precision. This&nbsp;drop was to be expected, since the amount of&nbsp;additional labeled relations in the novels is high&nbsp;according to the IAA results (this manifests in the low&nbsp;recall in table 1) as well as can be seen in the labeling&nbsp;efficiency. Altogether, the quality and efficiency of&nbsp;using a classifier trained on summaries are&nbsp;comparable to training on the novels directly based on&nbsp;our data.</span></p><h2><a name="bookmark11"></a><span class="font2" style="font-weight:bold;">Summary</span></h2>
<p><span class="font7">We presented an approach to increase labeling efficiency for relation detection in German novels by&nbsp;transferring knowledge from summaries to novels. It&nbsp;could be shown that using the summaries as trainings&nbsp;data will achieve similar results to using the novels,&nbsp;but the summaries are much shorter and relevant&nbsp;sentences are much more frequent. The interannotator agreement for this task is also relatively low&nbsp;which may point to an explanation for the&nbsp;comparatively low results of the automatic approach.</span></p><h2><a name="bookmark12"></a><span class="font2" style="font-weight:bold;">Bibliography</span></h2>
<p><span class="font6" style="font-weight:bold;">Finn, A., and Kushmerick, N. </span><span class="font6">(2003). “Active learning selection strategies for information extraction.”&nbsp;</span><span class="font6" style="font-style:italic;">Proceedings of the International Workshop on Adaptive&nbsp;Text Extraction and Mining (ATEM-03).</span></p>
<p><span class="font6" style="font-weight:bold;">Jing, H., Kambhatla, N. and Roukos, S</span><span class="font6">. (2007). “Extracting social networks and biographical facts from&nbsp;conversational speech transcripts.” </span><span class="font6" style="font-style:italic;">Proceedings of the&nbsp;45th Annual Meeting of the Association for Computational&nbsp;Linguistics,</span><span class="font6"> Prague: ACL, pp. 1040-48.</span></p>
<p><span class="font6" style="font-weight:bold;">Krug, M., Wick, C., Jannidis, F., Reger, I. Weimer, L., Madarasz, N. and Puppe, F. </span><span class="font6">(2017). “Comparison of&nbsp;Methods for Automatic Relation Extraction in German&nbsp;Novels.” 4. </span><span class="font6" style="font-style:italic;">Tagung Digital Humanities im&nbsp;deutschsprachigen Raum.</span><span class="font6"> Bern: DHd, pp. 223-26.</span></p>
<p><span class="font6" style="font-weight:bold;">Massey, P., Xia, P., Bamman, D. and Smith, N. A. </span><span class="font6">(2015). “Annotating character relationships in literary texts.”&nbsp;arXiv preprint: arXiv:1512.00728.</span></p>
<p><span class="font6" style="font-weight:bold;">Mintz, M., Bills, S., Snow, R. and Jurafsky, D. </span><span class="font6">(2009). “Distant supervision for relation extraction without&nbsp;labeled data.” </span><span class="font6" style="font-style:italic;">Proceedings of the Joint Conference of the&nbsp;47th Annual Meeting of the ACL and the 4th International&nbsp;Joint Conference on Natural Language Processing of the&nbsp;AFNLP.</span><span class="font6"> Singapore: ACL, pp. 1003-11.</span></p>
<p><span class="font6" style="font-weight:bold;">Surdeanu, M., Tibshirani, J., Nallapati, R. and Manning, C.</span></p>
<p><span class="font6" style="font-weight:bold;">D. </span><span class="font6">(2012). “Multi-instance multi-label learning for relation extraction.” </span><span class="font6" style="font-style:italic;">Proceedings of the 2012 Joint&nbsp;Conference on Empirical Methods in Natural Language&nbsp;Processing and Computational Natural Language&nbsp;Learning.</span><span class="font6"> Jeju Island, Korea: ACL, pp. 455-65.</span></p>
</body>
</html>