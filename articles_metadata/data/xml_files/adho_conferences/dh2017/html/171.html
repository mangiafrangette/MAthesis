<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 171. Du-Authorship of Dream of the Red Chamber-171.docx</title><link rel="stylesheet" href="171_files/171.css" type="text/css"/>
</head>
<body><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font2" style="font-weight:bold;">Authorship of Dream of the Red Chamber: A Topic&nbsp;Modeling Approach</span></h1>
<p><span class="font4" style="font-weight:bold;">Keli Du</span></p>
<p><span class="font4"><a href="mailto:keli.du@stud-mail.uni-wuerzburg.de">keli.du@stud-mail.uni-wuerzburg.de</a> University of Wurzburg, Germany</span></p>
<p><span class="font4" style="font-style:italic;">Dream of the Red Chamber</span><span class="font4"> (DRC, </span><span class="font5">^^^</span><span class="font4">) is one of the most famous Chinese classic novels, written by Cao&nbsp;Xueqin (</span><span class="font3">WW^</span><span class="font4">) during the 18th century. The original&nbsp;version of DRC had 80 chapters. But in 1791, Gao E (</span><span class="font3">^&nbsp;</span><span class="font5">W</span><span class="font4">) and Cheng Weiyuan (®)</span><span class="font5">^</span><span class="font3">x</span><span class="font4">) claimed that they&nbsp;had found more manuscripts of Cao and published&nbsp;another edition with 120 chapters. Since then, there&nbsp;has been a lot of discussions regarding the number of&nbsp;authors of DRC. Many scholars see the last 40 chapters&nbsp;as a later addition. Currently Hu Shih’s (</span><span class="font3">^^</span><span class="font4">) (Hu,&nbsp;1988) research is most widely accepted, where he&nbsp;argued these last 40 chapters were written by Gao E.&nbsp;According to some modern research approaches, the&nbsp;first 80 and the last 40 chapters are written by two&nbsp;authors. Evidence also suggest that Chapters 64 and&nbsp;67 may not be written by Cao (Hu, Wang, &amp; Wu, 2014;&nbsp;Tu, &amp; Hsiang, 2013).</span></p>
<p><span class="font4">Using Delta (Burrows, 2002), a measure of difference between two texts, the same conclusion has&nbsp;been obtained (Du, 2016). The 120 chapters are&nbsp;written by two authors. Red texts are the first 80&nbsp;chapters and green texts are the last 40 chapters. Delta&nbsp;also suggest that chapter 6, 10, 11 and 67 might be&nbsp;written by the second author (see Fig. 1, 2, 3).</span></p>
<p><span class="font4">Although most the results obtained are within expectations, the presence of the four chapters in the&nbsp;second group certainly deserves further investigation.&nbsp;Three hypotheses are proposed in this paper as the&nbsp;cause for this situation:</span></p>
<p><span class="font4">• &nbsp;&nbsp;&nbsp;This test result from the Delta method is not&nbsp;100% accurate.</span></p>
<p><span class="font4">• &nbsp;&nbsp;&nbsp;These four chapters shares many names or&nbsp;plot related terms with the last 40 chapters.</span></p>
<p><span class="font4">• &nbsp;&nbsp;&nbsp;Stylistic difference. Compared to the other&nbsp;chapters, the use of some less plot related&nbsp;terms indicates that the second author wrote&nbsp;Chapter 6, 10, 11 and 67.</span></p>
<p><span class="font4">Topic Modeling was used to test DRC on this regard. I used the </span><span class="font4" style="text-decoration:underline;">version</span><span class="font4"> of the DRC that Tu (2013)&nbsp;deemed “the closest to the earliest editions” for this&nbsp;study. Topic Modeling can automatically discover the&nbsp;contents of a large collection of documents, and is&nbsp;often used as an alternative method to explore the&nbsp;documents. A topic is a probabilistic distribution over&nbsp;words appearing in the corpus. The model finds&nbsp;groups of related words, and words that occur&nbsp;frequently together will be clustered in the same&nbsp;group. If some words tend to co-occur in the last 40&nbsp;chapters of DRC, Topic Modeling would be effective in&nbsp;highlighting their presence. LDA (Latent Dirichlet&nbsp;Allocation) (Blei et al., 2003) was used to model my&nbsp;corpus and do my test, and MAchine Learning for&nbsp;LanguagE Toolkit (</span><span class="font4" style="text-decoration:underline;">MALLET</span><span class="font4">) was used as the topicmodeling tool.</span></p>
<p><span class="font4">At the preprocessing step, tokenizing and chunking were performed. Tokenizing is required to process&nbsp;texts written in the Chinese Language to spilt texts into&nbsp;words, as there are no spaces to mark word&nbsp;boundaries. Tools like Stanford Chinese Word&nbsp;Segmenter can only be used on modern Chinese texts,&nbsp;as the segmentation standards are not suitable for&nbsp;classic Chinese. Character bigrams were hence&nbsp;selected as the “word”. Breaking the texts up allows&nbsp;the relationship among words to be explored more&nbsp;thoroughly, hence the chapters were split, where each&nbsp;document for the test contains 500 bigrams.&nbsp;Stopwords present another issue. A test run was first&nbsp;performed with MALLET to acquire some topics. The&nbsp;results of the test run show the bigram words assigned&nbsp;to the topics. In these topics the correct bigram words&nbsp;and person names were observed. Besides, a&nbsp;significant amount of meaningless bigrams in the&nbsp;topics were also found, for example:&nbsp;&nbsp;&nbsp;&nbsp;(writings,</span></p>
<p><span class="font4">one), (and so on, do), (cloud, smile). Thereby a stopword list with the following was compiled: a&nbsp;collection of meaningless bigrams and person names&nbsp;and function words like&nbsp;&nbsp;&nbsp;&nbsp;(a),&nbsp;&nbsp;&nbsp;&nbsp;(this), </span><span class="font3">A#</span></p>
<p><span class="font4">(have to) and so on.</span></p>
<p><span class="font4">MALLET was run after the preprocessing stage to generate 50 topics. The topic-document distribution&nbsp;output was aggregated as the chapters were split into&nbsp;chunks at the preprocessing step. The average of the&nbsp;topic shares associated with each chapter were then&nbsp;computed and the shares were transformed into a&nbsp;document-topic matrix. Then the visualization of the&nbsp;topic proportions associated with the chapters was&nbsp;created (see Fig. 4). The x-axis are the topics and the y-axis are the documents. The red line divides the figure</span></p>
<p><span class="font4">into two parts, the first 80 chapters (above the line) and the last 40 chapters (under the line). Topic #26&nbsp;became the main focus from this result: the last 40&nbsp;chapters are all strongly associated with it, while the&nbsp;first 80 chapters (except Chapter 11 and 67) are&nbsp;associated to this topic with much lower probability.&nbsp;The topic-word assignments were then computed. The&nbsp;top 20 words and their unnormalized weights of the&nbsp;topic #26 are:</span></p>
<p><span class="font4">357.0: </span><span class="font3">AA </span><span class="font4">(lady), 246.0: </span><span class="font3">0A </span><span class="font4">(back), 233.0: </span><span class="font3">A</span></p>
<p><span class="font3">T </span><span class="font4">(coming), 218.0: </span><span class="font3">TW </span><span class="font4">(slave girl), 190.0:</span></p>
<p><span class="font3">A </span><span class="font4">(come over), 184.0: &nbsp;&nbsp;&nbsp;(answer), 146.0: </span><span class="font3">X</span></p>
<p><span class="font3">M </span><span class="font4">(seen), 125.0: </span><span class="font3">XX </span><span class="font4">(go), 122.0:</span></p>
<p><span class="font4">(moment), 114.0: </span><span class="font3">XA </span><span class="font4">(call someone), 108.0: </span><span class="font3">X</span></p>
<p><span class="font3">A </span><span class="font4">(come out), 101.0: &nbsp;&nbsp;&nbsp;(speech), 100.0:</span></p>
<p><span class="font4">(tell), 98.0: </span><span class="font3">40 </span><span class="font4">(today), 98.0: </span><span class="font3">XX </span><span class="font4">(talk), 95.0:</span></p>
<p><span class="font4">(see), 92.0: </span><span class="font3">0^ </span><span class="font4">(answer), 91.0: &nbsp;&nbsp;&nbsp;(over</span></p>
<p><span class="font4">there), 91.0: </span><span class="font3">Wt </span><span class="font4">(promptly), 85.0: &nbsp;&nbsp;&nbsp;(think</span></p>
<p><span class="font4"><sup>of</sup>)</span></p>
<p><span class="font4">All the words are not person names or plot-related. The result obtained from the Topic Modelling shows&nbsp;that Topic #26 is a “style”-Topic. Most of them can be&nbsp;presented in a scene: an interaction (or a&nbsp;conversation) between the lady and the slave girl. DRC&nbsp;is an observation of the Chinese society in 18th-century. The story is about the life of two large,&nbsp;wealthy family compounds in the capital of China.&nbsp;Such scenes or plot are hence very common in the&nbsp;whole novel.</span></p>
<p><span class="font4">In conclusion, Topic Modeling was used in this paper to find the specific topic, which represents the&nbsp;difference between the first 80 and the last 40&nbsp;chapters of DRC. The test results indicate that both&nbsp;hypotheses, a) the Delta test result is not 100%&nbsp;accurate, b) The four chapters share many names or&nbsp;plot related terms with the last 40 chapters, are not&nbsp;true. The first 80 chapters (except Chapter 11 and 67)&nbsp;are stylistically different from the last 40 chapters.&nbsp;According to the results of Delta and Topic Modeling,&nbsp;both Chapter 11 and 67 are definitely not written by&nbsp;the first author. They might be written. or at least&nbsp;edited by the second author of DRC.</span></p><div><img src="171_files/171-1.jpg" style="width:221pt;height:294pt;"/></div><br clear="all"/><div>
<p><span class="font0">Figure 1. Delta test results of DRC, (300 MFC, 2-grams)</span></p><img src="171_files/171-2.jpg" style="width:220pt;height:294pt;"/></div><br clear="all"/><div>
<p><span class="font0">Figure 2, Delta test results of DRC, (400 MFC, 2-grams)</span></p></div><br clear="all"/><div><img src="171_files/171-3.jpg" style="width:212pt;height:282pt;"/>
<p><span class="font0">Figure 3. Delta test results of DRC, (500 MFC, 2-grams)</span></p></div><br clear="all"/><div><img src="171_files/171-4.jpg" style="width:201pt;height:321pt;"/>
<p><span class="font0">Figure 4. Topic-chapter distribution of DRC (50 topics, 120 documents)</span></p></div><br clear="all"/><h2><a name="bookmark1"></a><span class="font1" style="font-weight:bold;">Bibliography</span></h2>
<p><span class="font3" style="font-weight:bold;">Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. </span><span class="font3">(2003). Latent dirichlet allocation. Journal of machine Learning research, 9931022.</span></p>
<p><span class="font3" style="font-weight:bold;">Burrows, J. </span><span class="font3">(2002). 'Delta': A measure of stylistic difference and a guide to likely authorship. In: Literary and&nbsp;Linguistic Computing, 17(3), (pp. 267-287).</span></p>
<p><span class="font3" style="font-weight:bold;">Du, K. </span><span class="font3">(2016). Testing Delta on Chinese Texts. In Digital Humanities 2016: Conference Abstracts. Jagiellonian&nbsp;University &amp; Pedagogical University, Krakow, pp. 781783.</span></p>
<p><span class="font3" style="font-weight:bold;">Hu Shhi </span><span class="font3">(1988). &nbsp;&nbsp;&nbsp;[Hu Shihs</span></p>
<p><span class="font3">Analysis of Dream of Red Chamber], Shanghai Guji Chubanshe (Shanghai Classics Publishing House)</span></p>
<p><span class="font3" style="font-weight:bold;">Hu, X., Wang, Y., &amp; Wu, Q. </span><span class="font3">(2014). Multiple Authors Detection: A Quantitative Analysis of Dream of the Red&nbsp;Chamber. Advances in Adaptive Data Analysis, 1450012.</span></p>
<p><span class="font3" style="font-weight:bold;">Tu, H. C., &amp; Hsiang, J. </span><span class="font3">(2013). A Text-Mining Approach to the Authorship Attribution Problem of Dream of the Red</span></p>
<p><span class="font3">Chamber. In: Digital Humanities 2013: Conference Abstracts (pp. 441-443)</span></p>
</body>
</html>