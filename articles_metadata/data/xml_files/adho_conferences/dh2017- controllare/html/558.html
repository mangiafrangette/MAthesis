<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 558. Giannella-ObservatoR!O2016-558.docx</title><link rel="stylesheet" href="558_files/558.css" type="text/css"/>
</head>
<body><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font6" style="font-weight:bold;">OBSERVATÓR!O2016</span></h1>
<p><span class="font10" style="font-weight:bold;">Julia Gianella</span></p>
<p><span class="font10"><a href="mailto:julia@impa.br">julia@impa.br</a></span></p>
<p><span class="font10">Instituto de Matemática Pura e Aplicada, Brazil</span></p>
<p><span class="font10" style="font-weight:bold;">Luiz Velho</span></p>
<p><span class="font10"><a href="mailto:lvelho@impa.br">lvelho@impa.br</a></span></p>
<p><span class="font10">Instituto de Matemática Pura e Aplicada, Brazil</span></p>
<p><span class="font10">This poster focuses on OBSERVATOR!O2016, a web-based platform for collecting, structuring and visualizing the online response to Rio-2016 from content&nbsp;shared on Twitter. This project was developed at the&nbsp;</span><span class="font10" style="text-decoration:underline;">Vision and Computer Graphics Laboratory</span><span class="font10"> and is&nbsp;based on two cross related research lines. First, the&nbsp;conception and design of the OBSERVATORJO2016&nbsp;</span><span class="font10" style="text-decoration:underline;">website,</span><span class="font10"> which provides a space to explore comments&nbsp;and images about the Olympics through structured&nbsp;visualizations. Second, the ongoing deployment of </span><span class="font10" style="text-decoration:underline;">research</span><span class="font10"> regarding the application of a digital method to&nbsp;explore large sets of images. The goal is to highlight&nbsp;how we use </span><span class="font10" style="font-style:italic;">Deep Learning</span><span class="font10"> applications - such as automatic image classification - and visualization techniques to enhance discoverability and expression of&nbsp;subject features within an extensive image collection.</span></p>
<p><span class="font11" style="font-weight:bold;">Method: </span><span class="font3" style="font-weight:bold;font-style:italic;">Deep Learning</span><span class="font11" style="font-weight:bold;"> and mosaic visualization</span></p>
<p><span class="font10">OBSERVATOR!O2016 collected around 1 million tweets from April 18th to August 25th, 2016. In other&nbsp;to gather different perspectives on the debate about&nbsp;the Olympics we created seven Twitter search queries.&nbsp;The data was presented in eight interactive visualizations:</span></p><img src="558_files/558-1.jpg" style="width:238pt;height:100pt;"/>
<p><span class="font10">digital images inaugurates new avenues for researchers interested in the human creative practice. In this sense, the investigation of Lev Manovich on digital&nbsp;methods to study visual culture is quite relevant. With&nbsp;this in mind, we explored Rio-2016 images through&nbsp;</span><span class="font10" style="font-style:italic;">Deep Learning</span><span class="font10"> approaches. As the investigation is currently ongoing, we will report the research process of&nbsp;a single task, which resulted in the </span><span class="font10" style="font-style:italic;">Torch Mosaic</span><span class="font10"> visualization.</span></p>
<p><span class="font10">During the pre-Olympics, it became evident that many of the images gathered by our query scripts&nbsp;were related to the Olympic torch relay and depicted&nbsp;the iconic object. Part of these images were accompanied by texts that mentioned the torch, but not all. In&nbsp;addition, some tweets mentioning the torch relay incorporated images that didn't depict the object. In&nbsp;other words, text analysis alone was not sufficient to&nbsp;detect a set of images containing the torch.</span></p>
<p><span class="font10">Thus, we referred to a </span><span class="font10" style="font-style:italic;">Deep Learning</span><span class="font10"> approach to recognize the Olympic torch in our database. The field&nbsp;of visual pattern recognition has been recently improved by the efficient performance of </span><span class="font10" style="font-style:italic;">Convolutional&nbsp;Neural Networks</span><span class="font10"> (CNN). In 2012, the work of Krizhev-sky et al. on training a deep convolutional neural network to classify the 1.2 million images in the </span><span class="font10" style="font-style:italic;">ImageNet&nbsp;</span><span class="font10">LSVRC-2010 contest into 1000 different classes had a&nbsp;substantial impact on the computer vision community.</span></p>
<p><span class="font10">More recently, and thanks to Google, computer vision tasks such as image classification have become more accessible and applicable. That's because the&nbsp;company released last year their open source software&nbsp;library </span><span class="font10" style="font-style:italic;">TensorFlow.</span><span class="font10"> This library runs code for image&nbsp;classification on </span><span class="font10" style="font-style:italic;">Inception-v3</span><span class="font10"> CNN model, which can be&nbsp;retrained on a distinct image classification task (this&nbsp;quality is referred to as </span><span class="font10" style="font-style:italic;">transfer learning</span><span class="font10">)</span><span class="font10" style="font-style:italic;">.</span><span class="font10"> By creating&nbsp;a set of training images, it is possible to update the parameters of the model and use it to recognize a new&nbsp;image category. That said, we retrained the network&nbsp;by showing it a sample of 100 manually labeled images&nbsp;containing the torch. Finally, the retrained network&nbsp;ran over our database and returned a set of images&nbsp;with their corresponding confidence score for the new&nbsp;category.</span></p><div>
<p><span class="font10">Approximately 180,000 of the collected tweets included unique images. The production of large sets of</span></p></div><br clear="all"/><div>
<p><span class="font0" style="font-weight:bold;">R!02016</span></p>
<p><span class="font0" style="font-weight:bold;">DEEP LABELS</span></p><img src="558_files/558-2.jpg" style="width:175pt;height:82pt;"/>
<p><span class="font2">Figure 2. Confidence score over 83%</span></p></div><br clear="all"/><div>
<table border="1">
<tr><td>
<p></p></td><td>
<p></p></td><td>
<p><span class="font4" style="font-style:italic;">•-</span></p></td><td style="vertical-align:bottom;">
<p><span class="font5">T</span></p></td><td rowspan="2">
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font5">il <sup>1</sup></span></p></td><td style="vertical-align:bottom;">
<p><span class="font5">¡jííi J</span></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font5">?? A *</span></p></td><td>
<p></p></td><td>
<p><span class="font4" style="font-style:italic;">?</span></p></td><td>
<p></p></td><td>
<p><span class="font12" style="font-weight:bold;">p </span><span class="font7" style="font-style:italic;">r</span></p></td><td>
<p></p></td></tr>
<tr><td style="vertical-align:bottom;">
<p><span class="font5">W'</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font7" style="font-style:italic;">Ï</span></p></td><td style="vertical-align:bottom;">
<p><span class="font5">r»</span></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font5">z ¿A</span></p></td><td style="vertical-align:bottom;">
<p><span class="font5">s </span><span class="font12" style="font-weight:bold;">y</span></p></td></tr>
<tr><td>
<p></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font5">y</span></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p></p></td><td style="vertical-align:bottom;">
<p><span class="font5">i</span></p></td></tr>
<tr><td>
<p></p></td><td>
<p><span class="font12" style="font-weight:bold;">« -</span></p>
<p><span class="font1">r.</span></p></td><td style="vertical-align:bottom;">
<p><span class="font5">j</span></p></td><td style="vertical-align:bottom;">
<p><span class="font5">£</span></p></td><td style="vertical-align:bottom;">
<p><span class="font1">’•j &nbsp;&nbsp;&nbsp;•&nbsp;&nbsp;&nbsp;&nbsp;*7^'*</span></p></td><td style="vertical-align:middle;">
<p><span class="font1">í-</span></p></td><td>
<p><span class="font5">»w*'&quot;</span></p></td></tr>
<tr><td>
<p></p></td><td style="vertical-align:middle;">
<p><span class="font5">,y '</span></p></td><td style="vertical-align:bottom;">
<p><span class="font5">ia</span></p></td><td>
<p></p></td><td>
<p></p></td><td>
<p><span class="font8" style="font-style:italic;">&amp;</span></p></td><td style="vertical-align:bottom;">
<p><span class="font12" style="font-weight:bold;">7</span></p></td></tr>
</table>
<p><span class="font2">Figure 5. Tile images</span></p></div><br clear="all"/>
<p><span class="font10">Until June 25th, around 1500 images with over 85% confidence score for the Olympic torch category&nbsp;had been classified by our network. We used them to&nbsp;create a </span><span class="font10" style="text-decoration:underline;">mosaic visualization</span><span class="font10"> that can be zoomed and&nbsp;panned. The mosaic idea is that, given an image (target&nbsp;image), another image (mosaic) is automatically build&nbsp;up from several smaller images (tile images). To implement the mosaic, we used a web-based viewer for&nbsp;high-resolution zoomable images called&nbsp;</span><span class="font10" style="font-style:italic;">OpenSeadragon</span><span class="font10">.</span></p><img src="558_files/558-3.jpg" style="width:239pt;height:126pt;"/>
<p><span class="font2">Figure 3. The target image</span></p><img src="558_files/558-4.jpg" style="width:234pt;height:143pt;"/>
<p><span class="font2">Figure 4. The mosaic</span></p>
<p><span class="font10">The organization nature of the mosaic visualization is mainly aesthetic. Nevertheless, zooming and panning the mosaic allows the user to explore a wide variety of views, and to discover image details and surprises such as the spoof picture of </span><span class="font10" style="font-style:italic;">Fofao,</span><span class="font10"> a Brazilian&nbsp;fictional character, carrying the torch.</span></p>
<p><span class="font10">For DH2017 poster session, we expect to present the results for another subject feature - the sporting&nbsp;disciplines - and visualisation technique - the </span><span class="font10" style="font-style:italic;">videosphere</span><span class="font10"> - we are working on at the moment. In addition, we plan to discuss with participants some possible scenarios in which </span><span class="font10" style="font-style:italic;">Deep Learning</span><span class="font10"> models could be&nbsp;applied to help image collections become more discoverable and expressive.</span></p>
<p><span class="font11" style="font-weight:bold;">Bibliography</span></p>
<p><span class="font9" style="font-weight:bold;">Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. </span><span class="font9">(2012).</span></p>
<p><span class="font9">&quot;Imagenet classification with deep convolutional neural networks&quot;. </span><span class="font9" style="font-style:italic;">Advances in neural information processing&nbsp;systems,</span><span class="font9"> pp. 1097-1105.</span></p>
<p><span class="font9" style="font-weight:bold;">Manovich, L </span><span class="font9">(2012). &quot;How to Compare One Million Images?”. In Berry, D. (ed), </span><span class="font9" style="font-style:italic;">Understanding Digital Humanities.</span><span class="font9"> Palgrave, pp. 249-278.</span></p>
<p><span class="font9" style="font-weight:bold;">Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Angue-lov, D., ... &amp; Rabinovich, A. </span><span class="font9">(2015). &quot;Going deeper with convolutions&quot;. </span><span class="font9" style="font-style:italic;">Proceedings of the IEEE Conference on&nbsp;Computer Vision and Pattern Recognition</span><span class="font9">, pp. 1-9.</span></p>
</body>
</html>