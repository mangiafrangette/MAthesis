<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 562. Seguin-Tracking Transmission of Details in Paintings-562.docx</title><link rel="stylesheet" href="562_files/562.css" type="text/css"/>
</head>
<body><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font5" style="font-weight:bold;">Tracking transmission of details in paintings</span></h1>
<p><span class="font8" style="font-weight:bold;">Benoit Seguin</span></p>
<p><span class="font8"><a href="mailto:benoit.seguin@epfl.ch">benoit.seguin@epfl.ch</a></span></p>
<p><span class="font8">Digital Humanities Laboratory</span></p>
<p><span class="font8">Ecole Polytechnique Fédérale de Lausanne, Switzerland</span></p>
<p><span class="font8" style="font-weight:bold;">Isabella di Lenardo</span></p>
<p><span class="font8"><a href="mailto:isabella.dilenardo@epfl.ch">isabella.dilenardo@epfl.ch</a></span></p>
<p><span class="font8">Digital Humanities Laboratory</span></p>
<p><span class="font8">Ecole Polytechnique Fédérale de Lausanne, Switzerland</span></p>
<p><span class="font8" style="font-weight:bold;">Frédéric Kaplan</span></p>
<p><span class="font8"><a href="mailto:frederic.kaplan@epfl.ch">frederic.kaplan@epfl.ch</a></span></p>
<p><span class="font8">Digital Humanities Laboratory</span></p>
<p><span class="font8" style="text-decoration:underline;">Ecole Polytechnique Fédérale de Lausanne, Switzerland</span></p><h2><a name="bookmark1"></a><span class="font4" style="font-weight:bold;">Introduction</span></h2>
<p><span class="font8">In previous articles (di Lenardo et al, 2016; Seguin et al, 2016), we explored how efficient visual search&nbsp;engines operating not on the basis of textual metadata&nbsp;but directly through visual queries, could fundamentally change the navigation in large databases of work&nbsp;of arts. In the present work, we extended our search&nbsp;engine in order to be able to search not only for global&nbsp;similarity between paintings, but also for matching details. This feature is of crucial importance for retrieving the visual genealogy of a painting, as it is often the&nbsp;case that one composition simply reuses a few elements of other works. For instance, some workshops&nbsp;of the 16th century had repertoires of specific characters (a peasant smoking a pipe, a couple of dancing,&nbsp;etc.) and anatomical parts (head poses, hands, etc.)&nbsp;,that they reused in many compositions (van den&nbsp;Brink, 2001; Tagliaferro et al, 2009). In some cases it&nbsp;is possible to track the circulation of these visual patterns over long spatial and temporal migrations, as&nbsp;they are progressively copied by several generations&nbsp;of painters. Identifying these links permits to reconstruct the production context of a painting, and the&nbsp;connections between workshops and artists. In addition, it permits a fine-grained study of taste evolution&nbsp;in the history of collections, following specific motives&nbsp;successfully reused in a large number of paintings.</span></p>
<p><span class="font8">Tracking these graphical replicators is challenging as they can vary in texture and medium. For instance,&nbsp;a particular character or a head pose of a painting may&nbsp;have been copied from a drawing, an engraving or a&nbsp;tapestry. It is therefore important that the search for&nbsp;matching details still detects visual reuse even across&nbsp;such different media and styles. In the rest of the paper, we describe the matching method and discuss&nbsp;some results obtained using this approach.</span></p><h2><a name="bookmark2"></a><span class="font4" style="font-weight:bold;">Method</span></h2>
<p><span class="font8">Matching patterns in a bank of images is a problem that has been extensively studied in the Computer Vision community as « Visual instance retrieval » (Sivic&nbsp;and Zisserman, 2003). The definition of the task is :&nbsp;</span><span class="font8" style="font-style:italic;">given a region of a query image, can we identify matching regions in other images of a large iconographic collection</span><span class="font8"> ?</span></p>
<p><span class="font8">Historically, the most successful methods were based on feature point descriptors (like SIFT, see&nbsp;Lowe, 2004) used in a Bag-of-Words (Jegou et al,&nbsp;2008) fashion. The global architecture can be summarized as follows:</span></p>
<p><span class="font8" style="font-style:italic;">For each image in the collection:</span></p>
<p><span class="font3">• </span><span class="font8">Extract the feature points for the image.</span></p>
<p><span class="font3">• </span><span class="font8">Quantize the point descriptors to a Visual-</span></p>
<p><span class="font8">Bag-of-Words representation.</span></p>
<p><span class="font8" style="font-style:italic;">Given a query region</span><span class="font8">:</span></p>
<p><span class="font3">• </span><span class="font8">Use the Bag-of-Words signatures to rank the first N most likely candidates of the collection.</span></p>
<p><span class="font3">• </span><span class="font8">For each candidate, re-rank them according to a spatial verification of the matching&nbsp;points with the query region.</span></p>
<p><span class="font8">In practice, such an approach works extremely well and numerous improvements (Shen et al, 2009; Crowley and Zisserman, 2014) have been brought over the&nbsp;years to the different steps of the procedure. Hence it&nbsp;is still considered state of the art for the traditional datasets which focus on building and object retrieval in&nbsp;photographs.</span></p><img src="562_files/562-1.jpg" style="width:236pt;height:168pt;"/>
<p><span class="font2">Figure 1: Working Bag-of-Words matching for buildings.</span></p>
<p><span class="font8">However, it was shown recently (Seguin et al, 2016; Crowley and Zisserman, 2014) that such approaches&nbsp;break completely when not dealing with the same&nbsp;physical objects and important style variations like we&nbsp;do in paintings. An example can be seen on Figure 2.</span></p><img src="562_files/562-2.jpg" style="width:228pt;height:160pt;"/>
<p><span class="font2">Figure 2: Feature point matching breaks when local features and style vary.</span></p>
<p><span class="font8">More recently, Convolutional Neural Networks (CNN) have had tremendous success in almost all areas of Computer Vision (object detection, recognition,&nbsp;segmentation, face identification, etc.) and CNN have&nbsp;established themselves over the last couple of years as&nbsp;an extremely powerful tool for almost any vision&nbsp;based problem.</span></p>
<p><span class="font8">A CNN is a multi-layer architecture where each layer transforms its input according to some parameters (also called weights). What makes them so powerful is that all these parameters can be learned « end-to-end » (for example in the case of object classification, just with images and their corresponding labels).</span></p><img src="562_files/562-3.jpg" style="width:235pt;height:64pt;"/>
<p><span class="font2">Figure 3: Simplified structure of a Convolutional Neural Network.</span></p>
<p><span class="font8">It has been shown in Donahue et al (2014) that CNN pretrained on very large datasets, like Deng et al&nbsp;(2009), for object classification tasks offer a very good&nbsp;abstract representation of the image information, and&nbsp;are thus applicable for other vision problems. They&nbsp;generalize much better when transferring from photographs to paintings, contrary to the traditional Bag-of-Words techniques (Seguin et al, 2016; Crowley and&nbsp;Zisserman, 2014). However, its application to visual&nbsp;instance retrieval was always hindered by the fact that&nbsp;they traditionally output a single global descriptor for&nbsp;the image, hence not directly allowing for region (subimage) retrieval (Babenko et al, 2014).</span></p>
<p><span class="font8">In Razavian et al (2014), the authors proposed to just precompute the CNN descriptors for some subdivision of the image. However, such approach limits the&nbsp;possible granularity of the windows, and multiply the&nbsp;memory requirement by a huge factor.</span></p>
<p><span class="font8">Another more promising approach introduced in Tolias et al (2015) is to work directly on the CNN feature maps. More precisely, a common way of extracting a CNN descriptor for image retrieval is to take an&nbsp;image of size </span><span class="font8" style="font-style:italic;">(H, W, 3)</span><span class="font8"> (height of </span><span class="font8" style="font-style:italic;">H</span><span class="font8"> pixels, width of </span><span class="font8" style="font-style:italic;">W&nbsp;</span><span class="font8">pixels and 3 color channels RGB) go through all the&nbsp;convolutional layers of CNN, which outputs the </span><span class="font8" style="font-style:italic;">feature&nbsp;maps</span><span class="font8"> : a structure of size </span><span class="font8" style="font-style:italic;">(H', W' F) (F</span><span class="font8"> channels of size&nbsp;</span><span class="font8" style="font-style:italic;">H'</span><span class="font8"> and </span><span class="font8" style="font-style:italic;">W</span><span class="font8">Q. From these feature maps, taking the sum&nbsp;(or the max) of each channel gives a signature of size F&nbsp;which is (after normalization) the descriptor of the image.</span></p>
<p><span class="font8">Traditionally, the network used is the VGG16 (Simonyan and Zisserman, 2014) architecture which given an image of size </span><span class="font8" style="font-style:italic;">(H, W, 3)</span><span class="font8"> creates feature maps&nbsp;of size </span><span class="font8" style="font-style:italic;">(H/32, W/32, 512).</span></p><img src="562_files/562-4.jpg" style="width:216pt;height:120pt;"/>
<p><span class="font2">Figure 4: Region evaluation architecture.</span></p>
<p><span class="font8">Now, starting from the feature maps, computing the signature of a sub-part of an image is already easier, we just need to compute the sum of the corresponding region in the feature maps to obtain the descriptor of size </span><span class="font8" style="font-style:italic;">F</span><span class="font8">. However, evaluating many different&nbsp;regions would still be prohibitive from a computation&nbsp;point of view.</span></p>
<p><span class="font8">In order to alleviate the performance problem, To-</span></p>
<p><span class="font8">lias et al (2015) proposed to use </span><span class="font8" style="font-style:italic;">integral images</span><span class="font8">. Given</span></p>
<p><span class="font8">an image </span><span class="font8" style="font-style:italic;">I,</span><span class="font8"> the integral image /</span><span class="font6">j </span><span class="font8">is </span><span class="font8" style="font-style:italic;">I</span><span class="font6" style="font-style:italic;">j</span><span class="font8" style="font-style:italic;">(y,x) = </span><span class="font6" style="font-style:italic;">T,+&lt;-j&lt;/</span><span class="font8" style="font-style:italic;">^(j,</span><span class="font8"> Q- This allow for extremely quick computation of the sum of an image for a given area&nbsp;</span><span class="font8" style="font-style:italic;">(y</span><span class="font6" style="font-style:italic;">o</span><span class="font8" style="font-style:italic;">,V</span><span class="font6" style="font-style:italic;">i</span><span class="font8" style="font-style:italic;">,X</span><span class="font6" style="font-style:italic;">0</span><span class="font8" style="font-style:italic;">,X</span><span class="font6" style="font-style:italic;">i</span><span class="font8" style="font-style:italic;">)</span><span class="font8"> (Fig.5) :</span></p>
<p><span class="font9" style="font-weight:bold;font-style:italic;">I(J’Q = Ij(y2'<sup>X</sup>2') + &nbsp;&nbsp;&nbsp;</span><span class="font1">- // (y<sub>V</sub>X<sub>2</sub>) - /; </span><span class="font9" style="font-weight:bold;font-style:italic;">(y^xj</span></p>
<p><span class="font9" style="font-weight:bold;font-style:italic;">x<sub>i</sub>si&lt;x<sub>2</sub>.y<sub>1</sub>sj&lt;y<sub>2</sub></span></p>
<p><span class="font8">X1 X<sub>2</sub></span></p><img src="562_files/562-5.jpg" style="width:75pt;height:53pt;"/>
<p><span class="font0" style="font-weight:bold;">■lnl(x..y<sub>2</sub>) &nbsp;&nbsp;&nbsp;+Inl(x?.y2)</span></p>
<p><span class="font2">Figure 5: Integral images</span></p>
<p><span class="font8">This trick allows for extensive evaluation for the best matching window for the query image in the target&nbsp;collection. The global procedure for searching is the&nbsp;following, quite close to the Bag-of-Word approach:</span></p>
<p><span class="font8" style="font-style:italic;">For each image in the collection</span><span class="font8">:</span></p>
<p><span class="font3">• &nbsp;&nbsp;&nbsp;</span><span class="font8">Extract the feature maps of the image, and&nbsp;compute the corresponding integral images.</span></p>
<p><span class="font3">• &nbsp;&nbsp;&nbsp;</span><span class="font8">Compute the global signature (with the&nbsp;whole image as window).</span></p>
<p><span class="font8" style="font-style:italic;">Given a query region</span><span class="font8">:</span></p>
<p><span class="font3">• &nbsp;&nbsp;&nbsp;</span><span class="font8">Use the global signatures to rank the first N&nbsp;most likely candidates of the collection.</span></p>
<p><span class="font3">• &nbsp;&nbsp;&nbsp;</span><span class="font8">For each candidate, re-rank them according&nbsp;to an extensive look of the sub-windows in&nbsp;the images using their pre-computed integral images.</span></p>
<p><span class="font8">In order to greatly improve the results, we add the following improvements:</span></p>
<p><span class="font3">• &nbsp;&nbsp;&nbsp;</span><span class="font8">The parameters of the network we use were&nbsp;fine-tuned using the Replica dataset (Seguin&nbsp;et al, 2016) (image retrieval in paintings).&nbsp;This dramatically improves the system resilience to color and style.</span></p>
<p><span class="font3">• &nbsp;&nbsp;&nbsp;</span><span class="font8">We use Spatial Pooling according to Ra-zavian et al (2014) which consists of extracting 4 blocks per evaluated region instead of&nbsp;1. It makes the search roughly 4 times slower&nbsp;but allow for much better retrieval of complex patterns by directly encoding spatiality.</span></p><h2><a name="bookmark3"></a><span class="font4" style="font-weight:bold;">Results</span></h2>
<p><span class="font8">The following experiment was run on the whole Web Gallery of Art collection (38'000 elements). Each&nbsp;image was resized so that its smaller dimension is 512&nbsp;pixels, and the integral images of the feature maps&nbsp;computed on it. Given a query region for an image, the&nbsp;300 most likely candidates are extracted from the&nbsp;WGA collection, and re-ranked according to the best&nbsp;matching window on each of them. Using 35 cores on&nbsp;a server machine, the complete request takes less than&nbsp;4 seconds.</span></p>
<p><span class="font8">Examples of queries and their results are shown on Figure 6. In query 1 and 2, the starting image is Leonardo da Vinci's </span><span class="font8" style="font-style:italic;">Virgine delle Rocce</span><span class="font8"> (Paris, Louvres),&nbsp;first version of this subject (1483-1486). Leornado is&nbsp;an interesting case study and his influence in Europe&nbsp;is known to be extremely important for the general&nbsp;compositions of paintings, character typologies and&nbsp;landscape patterns. In query 1, the group of Mary, the&nbsp;angel and the two children is selected. The first result&nbsp;is the version of the same subject (London, National&nbsp;Gallery) finished a decade after the Paris version, with&nbsp;the contribute of Ambrogio de Predis. The painting is&nbsp;different in color but has the same composition. The&nbsp;second and third results are other versions of the same&nbsp;subject by unidentified painters of the XVIth century.&nbsp;This constitutes typical examples of the propagation of&nbsp;a complex theme.</span></p>
<p><span class="font8">In query 2, only a detail from the landscape is chosen. Interestingly, the second result is a painting from</span></p>
<p><span class="font8">Bernardino de Conti, which is a variation on the same theme, reusing the landscape but without the angel&nbsp;and with the two children kissing.</span></p>
<p><span class="font8">For query 3, we use a painting by Marco d'Oggiono, a follower of Leonardo, very similar to the one by de&nbsp;Conti and we select only the two children kissing. Results 1 and 3 feature paintings where only the children&nbsp;are present, showing that this replicator has an autonomy of its own. The third result by Joos van Cleve confirms the historical migration of this subject, as autonomous, from Italy to Flanders.</span></p>
<p><span class="font8">These 3 simple queries illustrate how the detail matching method can easily unveil the transmission&nbsp;network between different series of paintings.</span></p><h2><a name="bookmark4"></a><span class="font4" style="font-weight:bold;">Perspectives</span></h2>
<p><span class="font8">The search of matching details in large-scale databases of paintings may enable to find undocumented links and therefore new historical connections between paintings. By tracking the propagation and&nbsp;transformations of a replicator, it becomes possible to&nbsp;follow the evolution through time of repertoires of&nbsp;forms and view each painting as a temporary vehicle&nbsp;playing the role of an intermediary node in a long history of images transmissions. Although in continuation with traditional methods in Art History, such a&nbsp;tool opens the avenue for research at a much larger&nbsp;scale, searching for patterns and finding new links&nbsp;simultaneously in millions of digitized paintings.</span></p><div><img src="562_files/562-6.jpg" style="width:216pt;height:297pt;"/>
<p><span class="font2">Figure 6: Examples of results of detail search.</span></p></div><br clear="all"/><img src="562_files/562-7.jpg" style="width:210pt;height:335pt;"/><img src="562_files/562-8.jpg" style="width:209pt;height:86pt;"/>
<p><span class="font2">Figure 7: Additional examples of results of detail search.</span></p><h2><a name="bookmark5"></a><span class="font4" style="font-weight:bold;">Bibliography</span></h2>
<p><span class="font7" style="font-weight:bold;">Babenko, A., Slesarev, A., Chigorin, A., and Lempitsky, V. </span><span class="font7">(2014) “Neural codes for image retrieval,” in </span><span class="font7" style="font-style:italic;">ECCV</span><span class="font7">.</span></p>
<p><span class="font7" style="font-weight:bold;">Crowley, E. J., and Zisserman, A. </span><span class="font7">(2014) “In search of art,” </span><span class="font7" style="font-style:italic;">ECCV Workshops</span><span class="font7">, 2014.</span></p>
<p><span class="font7" style="font-weight:bold;">Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N.,</span></p>
<p><span class="font7" style="font-weight:bold;">Tzeng, E., and Darrell, T. </span><span class="font7">(2014)“DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition,” </span><span class="font7" style="font-style:italic;">ICML</span><span class="font7">.</span></p>
<p><span class="font7" style="font-weight:bold;">Deng, J., Dong, W., Socher, R. Li, L.-J., Li, K., and Fei-Fei, L.</span></p>
<p><span class="font7">(2009) “ImageNet: A large-scale hierarchical image database,” </span><span class="font7" style="font-style:italic;">CVPR</span><span class="font7">.</span></p>
<p><span class="font7" style="font-weight:bold;">Jegou, H., Douze, M., and Schmid, C. (</span><span class="font7">2008) “Hamming Embedding and Weak Geometry Consistency for Large Scale Image Search ”, </span><span class="font7" style="font-style:italic;">ECCV</span><span class="font7">.</span></p>
<p><span class="font7" style="font-weight:bold;">di Lenardo, I., Seguin, B. L. A., and Kaplan, F. </span><span class="font7">(2016). Visual Patterns Discovery in Large Databases of Paintings. Digital Humanities 2016, Krakow, Polland, July 11-16,&nbsp;2016.</span></p>
<p><span class="font7" style="font-weight:bold;">Lowe, D. G. </span><span class="font7">(2004) “Distinctive Image Features from ScaleInvariant Keypoints,” </span><span class="font7" style="font-style:italic;">Int. J. Comput. Vis.</span><span class="font7">, Nov. 2004.</span></p>
<p><span class="font7" style="font-weight:bold;">Razavian, A. S., Sullivan, J., Maki, A., and Carlsson, S.</span></p>
<p><span class="font7">(2014) “A Baseline for Visual Instance Retrieval with Deep Convolutional Networks,” Dec.</span></p>
<p><span class="font7" style="font-weight:bold;">Seguin, B., Striolo, C., di Lenardo, I., and Kaplan, F. </span><span class="font7">(2016) Visual link retrieval in a database of paintings, VISART :&nbsp;Where Computer Vision Meets Art, 3rd Workshop on&nbsp;Computer Vision for Art Analysis, October 2016, Amsterdam, The Netherlands</span></p>
<p><span class="font7" style="font-weight:bold;">Shen, X., Lin, Z., Brandt, J., Avidan, S., and Wu, Y. </span><span class="font7">(2012) “Object retrieval and localization with spatially-constrained similarity measure and k -NN re-ranking,”&nbsp;</span><span class="font7" style="font-style:italic;">CVPR</span><span class="font7">.</span></p>
<p><span class="font7" style="font-weight:bold;">Simonyan, K., and Zisserman, A. </span><span class="font7">(2014) “Very Deep Convolutional Networks for Large-Scale Image Recognition,” </span><span class="font7" style="font-style:italic;">arXiv Prepr.</span></p>
<p><span class="font7" style="font-weight:bold;">Sivic, J., and Zisserman, A. </span><span class="font7">(2003) “{Video Google:} A text retrieval approach to object matching in videos,” </span><span class="font7" style="font-style:italic;">CVPR</span><span class="font7">.</span></p>
<p><span class="font7" style="font-weight:bold;">Tagliaferro, G., Aikema, B., Mancini, M., Martin, A. J..</span></p>
<p><span class="font7">(2009) </span><span class="font7" style="font-style:italic;">Le Botteghe di Tiziano Alinari</span><span class="font7">, Firenze</span></p>
<p><span class="font7" style="font-weight:bold;">Tolias, G., Sicre, R., and Jégou, H. </span><span class="font7">(2015) “Particular object retrieval with integral max-pooling of CNN activations,”&nbsp;</span><span class="font7" style="font-style:italic;">arXiv Prepr. arXiv1511.05879</span></p>
<p><span class="font7" style="font-weight:bold;">van den Brink, H. M., </span><span class="font7">ed. (2001) L'entreprise Brueghel, , Maastricht, Bonnefantenmuseum- Bruxelles, Musée royaux des beaux-arts, Beaux-Arts Collection 2001</span></p>
</body>
</html>