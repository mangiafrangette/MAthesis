[
{"url": "https://hal.archives-ouvertes.fr/hal-02280013v2", "identifier": {"string_id": "hal-02280013", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : While online crowdsourced text transcription projects have proliferated in the last decade, there is a need within the broader field to understand differences in project outcomes as they relate to task design, as well as to experiment with different models of online crowdsourced transcription that have not yet been explored. The experiment discussed in this paper involves the evaluation of newly-built tools on the Zooniverse.org crowdsourcing platform, attempting to answer the research question: \"Does the current Zooniverse methodology of multiple independent transcribers and aggregation of results render higher-quality outcomes than allowing volunteers to see previous transcriptions and/or markings by other users? How does each methodology impact the quality and depth of analysis and participation?\" To answer these questions, the Zooniverse team ran an A/B experiment on the project Anti-Slavery Manuscripts at the Boston Public Library. This paper will share results of this study, and also describe the process of designing the experiment and the metrics used to evaluate each transcription method. These include the comparison of aggregate transcription results with ground truth data; evaluation of annotation methods; the time it took for volunteers to complete transcribing each dataset; and the level of engagement with other project elements such as posting on the message board or reading supporting documentation. Particular focus will be given to the (at times) competing goals of data quality, efficiency, volunteer engagement, and user retention, all of which are of high importance for projects that focus on data from galleries, libraries, archives and museums. Ultimately, this paper aims to provide a model for impactful, intentional design and study of online crowdsourcing transcription methods, as well as shed light on the associations between project design, methodology and outcomes.                    </div>", "article_title": "\n                Individual vs. Collaborative Methods of Crowdsourced Transcription            ", "authors": [{"given": "Samantha Blickhan", "family": null, "affiliations": ["\n                            Adler Planetarium [Chicago]                        "]}, {"given": "Coleman Krawczyk", "family": null, "affiliations": ["\n                            University of Portsmouth                        "]}, {"given": "Daniel Hanson", "family": null, "affiliations": ["\n                            University of Minnesota [Twin Cities]                        "]}, {"given": "Amy Boyer", "family": null, "affiliations": null}, {"given": "Andrea Simenstad", "family": null, "affiliations": ["\n                            University of Minnesota [Twin Cities]                        "]}, {"given": "Victoria Hyning", "family": null, "affiliations": null}, {"given": "Victoria van Hyning", "family": null, "affiliations": ["\n                            Library of Congress                        "]}], "publisher": null, "date": "2019/12/03", "keywords": [], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Collecting, Preserving, and Disseminating Endangered Cultural Heritage for New Understandings through Multilingual Approaches", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01281266v2", "identifier": {"string_id": "hal-01281266", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : The ancient commentaries provide a large sample of quotations from classical or biblical texts for which Latin gramamrians developed a complex system of insertion of quoted texts. The paper examines how to encode these places using XML Tei, and focuses on difficult cases, such as inaccurate quotations, or quotations of partly or wholly lost texts.                    </div>", "article_title": "\n                Encoding (inter)textual insertions in Latin \"grammatical commentary\"            ", "authors": [{"given": "Bruno Bureau", "family": null, "affiliations": ["\n                            HiSoMA - Histoire et Sources des Mondes antiques                        "]}, {"given": "Christian Nicolas", "family": null, "affiliations": ["\n                            HiSoMA - Histoire et Sources des Mondes antiques                        "]}, {"given": "Ariane Pinche", "family": null, "affiliations": ["\n                            CIHAM - Histoire, Archéologie et Littératures des mondes chrétiens et musulmans médiévaux                        "]}], "publisher": null, "date": "2017/11/30", "keywords": ["Ancient commentary ; allusion ; fragmentary texts ; mentioned words ; Latin grammar ; XML Tei ; quotation"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01283638v2", "identifier": {"string_id": "hal-01283638", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : Colophons of Armenian manuscripts are replete with yet untapped riches. Formulae are not the least among them: these recurrent stereotypical patterns conceal many clues as to the schools and networks of production and diffusion of books in Armenian communities. This paper proposes a methodology for exploiting these sources, as elaborated in the framework of a PhD research project about Armenian colophon formulae. Firstly, the reader is briefly introduced to the corpus of Armenian colophons and then, to the purposes of our project. In the third place, we describe our methodology, relying on lemmatization and modelling of patterns into automata. Fourthly and finally, the whole process is illustrated by a basic case study, the occasion of which is taken to outline the kind of results that can be achieved by combining this methodology with a philologico-historical approach to colophons.                    </div>", "article_title": "\n                Recurrent Pattern Modelling in a Corpus of Armenian Manuscript Colophons            ", "authors": [{"given": "Emmanuel van Elverdinghe", "family": null, "affiliations": ["\n                            Fonds de la Recherche Scientifique [FNRS]                        ", "\n                            INCAL - Institut des Civilisations Arts et Lettres (INCAL)                        "]}], "publisher": null, "date": "2017/12/22", "keywords": ["codicology ; Unitex ; finite state transducers ; Armenian colophons ; automata ; colophon formula ; formulaic patterns ; lemmatization ; manuscript studies"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-02520508v3", "identifier": {"string_id": "hal-02520508", "id_scheme": "hal"}, "abstract": null, "article_title": "\n                                    ", "authors": [{"given": "Régis Schlagdenhauffen", "family": null, "affiliations": ["\n                            IRIS - Institut de Recherche Interdisciplinaire sur les enjeux Sociaux - sciences sociales, politique, santé                        "]}], "publisher": null, "date": "2020/07/14", "keywords": ["TEI ; User Experience ; Human Text Recognition ; Learning process ; OCR"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Atelier Digit_Hum", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01671592v1", "identifier": {"string_id": "hal-01671592", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : This paper presents some computer tools and linguistic resources of the GREgORI project. These developments allow automated processing of texts written in the main languages of the Christian Middel East, such as Greek, Arabic, Syriac, Armenian and Georgian. The main goal is to provide scholars with tools (lemmatized indexes and concordances) making corpus-based linguistic information available. It focuses on the questions of text processing, lemmatization, information retrieval, and bitext alignment.                    </div>", "article_title": "\n                Processing Tools for Greek and Other Languages of the Christian Middle East            ", "authors": [{"given": "Bastien Kindt", "family": null, "affiliations": ["\n                            INCAL - Institut des Civilisations Arts et Lettres (INCAL)                        "]}], "publisher": null, "date": "2017/12/22", "keywords": ["Lemmatization ; Greek ; Syriac ; Arabic ; Armenian ; Georgian ; lexical tagging ; POS tagging ; concordances ; indexes ; bitext ; bilingual alignment ; translation memories ; mkAlign"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01913435v3", "identifier": {"string_id": "hal-01913435", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : The Foucault Fiches de Lecture (FFL) project aims both to explore and to make available online a large set of Michel Foucault’s reading notes (organized citations, references and comments) held at the BnF since 2013. Therefore, the team is digitizing, describing and enriching the reading notes that the philosopher gathered while preparing his books and lectures, thus providing a new corpus that will allow a new approach to his work. In order to release the manuscripts online, and to collectively produce the data, the team is also developing a collaborative platform, based on RDF technologies, and designed to link together archival content and bibliographic data. This project is financed by the ANR (2017-2020) and coordinated by Michel Senellart, professor of philosophy at the ENS Lyon. It benefits from the partnerships of the ENS/PSL and the BnF. In addition, a collaboration with the European READ/Transkribus project has been started so as to produce automatic transcription of the reading notes.                    </div>", "article_title": "\n                                    ", "authors": [{"given": "Marie-Laure Massot", "family": null, "affiliations": ["\n                            CAPHÉS - Centre d’Archives en Philosophie, Histoire et Édition des Sciences                        "]}, {"given": "Arianna Sforzini", "family": null, "affiliations": ["\n                            TRIANGLE - Triangle : action, discours, pensée politique et économique                        "]}, {"given": "Vincent Ventresque", "family": null, "affiliations": ["\n                            TRIANGLE - Triangle : action, discours, pensée politique et économique                        "]}], "publisher": null, "date": "2019/02/18", "keywords": ["transkribus ; reading notes ; automatic transcription of manuscripts ; artificial intelligence"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Atelier Digit_Hum", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-02109972v2", "identifier": {"string_id": "hal-02109972", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : The creation of the Artist Libraries Project was sparked by the observation that artist libraries are still not well known, yet many art historians are interested in this archive for the value it adds to understanding the person behind the artist and his or her creative process. The problem is that these libraries are rarely physically preserved. To remedy this dispersion, we built an online database and a website www.lesbibliothequesdartistes.org that house this valuable source in the form of lists of books and their electronic versions. First data on Monet's library have been made available, and several additional artist libraries from the 19 th and 20 th centuries are on the way for 2019. By gathering all these bibliographical data in a central database, it's possible to explore one library and to compare several. This article explains how we built the database and the website and how the implementation of those IT tools has raised questions about the use of this resource as an archive on the one hand, as well as its value for art history on the other.                    </div>", "article_title": "\n                The Artist Libraries Project in the Labex Les passés dans le présent            ", "authors": [{"given": "Félicie Faizand de Maupeou", "family": null, "affiliations": ["\n                            HAR - Histoire des Arts et des Représentations                        "]}, {"given": "Ségolène Le Men", "family": null, "affiliations": ["\n                            HAR - Histoire des Arts et des Représentations                        "]}], "publisher": null, "date": "2019/04/25", "keywords": [], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Atelier Digit_Hum", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01287195v4", "identifier": {"string_id": "hal-01287195", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : This contribution to a special issue on “Computer-aided processing of intertextuality” in ancient texts will illustrate how using digital tools to interact with the Hebrew Bible offers new promising perspectives for visualizing the texts and for performing tasks in education and research. This contribution explores how the corpus of the Hebrew Bible created and maintained by the Eep Talstra Centre for Bible and Computer can support new methods for modern knowledge workers within the field of digital humanities and theology be applied to ancient texts, and how this can be envisioned as a new field of digital intertextuality. The article first describes how the corpus was used to develop the Bible Online Learner as a persuasive technology to enhance language learning with, in, and around a database that acts as the engine driving interactive tasks for learners. Intertextuality in this case is a matter of active exploration and ongoing practice. Furthermore, interactive corpus-technology has an important bearing on the task of textual criticism as a specialized area of research that depends increasingly on the availability of digital resources. Commercial solutions developed by software companies like Logos and Accordance offer a market-based intertextuality defined by the production of advanced digital resources for scholars and students as useful alternatives to often inaccessible and expensive printed versions. It is reasonable to expect that in the future interactive corpus technology will allow scholars to do innovative academic tasks in textual criticism and interpretation. We have already seen the emergence of promising tools for text categorization, analysis of translation shifts, and interpretation. Broadly speaking, interactive tools and tasks within the three areas of language learning, textual criticism, and Biblical studies illustrate a new kind of intertextuality emerging within digital humanities.                     </div>", "article_title": "\n                Interactive Tools and Tasks for the Hebrew Bible: From Language Learning to Textual Criticism            ", "authors": [{"given": "Nicolai Winther-Nielsen", "family": null, "affiliations": ["\n                            FIUC-Dk - Fjellhaug International University College Denmark                        ", "\n                            FIUC-Dk - Fjellhaug International University College Denmark                        "]}], "publisher": null, "date": "2017/10/18", "keywords": ["Bible Online Learner ; Joshua 24 ;  Hebrew Bible ; Digital intertextuality ; Corpus ; Logos Bible software ; language learning ; Hebrew Bible ; ETCBC ; textual criticism ; Joshua"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01294158v2", "identifier": {"string_id": "hal-01294158", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : The project is to develop a database, which is planned to include all available information on the use of the Bible in the patristic works of Migne's Patrologia Graeca. Utilization of the data will be available through a web page equipped with necessary tools for developing data mining techniques and other methods of analysis. The main aim of the project is to revive the catenae, the ancient exegetical tool for biblical interpretation.                    </div>", "article_title": "\n                Digital Greek Patristic Catena (DGPC). A brief presentation            ", "authors": [{"given": "Athanasios Paparnakis", "family": null, "affiliations": ["\n                            Department of Pastoral and Social Theology                        "]}, {"given": "Constantinos Domouchtsis", "family": null, "affiliations": ["\n                            Department of Pastoral and Social Theology                        "]}], "publisher": null, "date": "2017/07/24", "keywords": ["patristic authors ; Catena ; bible references ; biblical exegesis ; database ; Patrologia Graeca"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01915730v2", "identifier": {"string_id": "hal-01915730", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : British philosopher and reformer Jeremy Bentham (1748-1832) left over 60,000 folios of unpublished manuscripts. The Bentham Project, at University College London, is creating a TEI version of the manuscripts, via crowdsourced transcription verified by experts. We present here an interface to navigate these largely unedited manuscripts, and the language technologies the corpus was enriched with to facilitate navigation, i.e Entity Linking against the DBpedia knowledge base and keyphrase extraction. The challenges of tagging a historical domain-specific corpus with a contemporary knowledge base are discussed. The concepts extracted were used to create interactive co-occurrence networks, that serve as a map for the corpus and help navigate it, along with a search index. These corpus representations were integrated in a user interface. The interface was evaluated by domain experts with satisfactory results , e.g. they found the distributional semantics methods exploited here applicable in order to assist in retrieving related passages for scholarly editing of the corpus.                    </div>", "article_title": "\n                Mapping the Bentham Corpus: Concept-based Navigation            ", "authors": [{"given": "Pablo Ruiz", "family": null, "affiliations": ["\n                            LILPA - Linguistique, Langues et Parole                         "]}, {"given": "Thierry Poibeau", "family": null, "affiliations": ["\n                            LattIce - Lattice - Langues, Textes, Traitements informatiques, Cognition - UMR 8094                        "]}], "publisher": null, "date": "2019/02/12", "keywords": ["Jeremy Bentham ; manuscripts ; corpus navigation ; entity linking ; keyphrase extraction"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue: Digital Humanities between knowledge and know-how (Atelier Digit_Hum)", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-02513038v2", "identifier": {"string_id": "hal-02513038", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : The study of watermarks is a key step for archivists and historians as it enables them to reveal the origin of paper. Although highly practical, automatic watermark recognition comes with many difficulties\r\nand is still considered an unsolved challenge. Nonetheless, Shen et al. [2019] recently introduced a new\r\napproach for this specific task which showed promising results. Building upon this approach, this work\r\nproposes a new public web application dedicated to automatic watermark recognition entitled Filigranes\r\npour tous. The application not only hosts a detailed catalog of more than 17k watermarks manually collected from the French National Archives (Minutier central) or extracted from existing online resources\r\n(Briquet database), but it also enables non-specialists to identify a watermark from a simple photograph\r\nin a few seconds. Moreover, additional watermarks can easily be added by the users making the enrichment of the existing catalog possible through crowdsourcing. Our Web application is available at\r\nhttp://filigranes.inria.fr/.                    </div>", "article_title": "\n                A Web Application for Watermark Recognition            ", "authors": [{"given": "Oumayma Bounou", "family": null, "affiliations": ["\n                            SED - Service Expérimentation et Développement  [Paris Rocquencourt]                        ", "\n                            ENC - École nationale des chartes                        ", "\n                            LIGM - Laboratoire d'Informatique Gaspard-Monge                        "]}, {"given": "Tom Monnier", "family": null, "affiliations": ["\n                            ENC - École nationale des chartes                        ", "\n                            LIGM - Laboratoire d'Informatique Gaspard-Monge                        ", "\n                            WILLOW - Models of visual object recognition and scene understanding                        "]}, {"given": "Ilaria Pastrolin", "family": null, "affiliations": ["\n                            ENC - École nationale des chartes                        "]}, {"given": "Xi Shen", "family": null, "affiliations": ["\n                            LIGM - Laboratoire d'Informatique Gaspard-Monge                        "]}, {"given": "Christine Benevent", "family": null, "affiliations": ["\n                            ENC - École nationale des chartes                        "]}, {"given": "Marie-Françoise Limon-Bonnet", "family": null, "affiliations": ["\n                            ENC - École nationale des chartes                        ", "\n                            Archives nationales                        "]}, {"given": "François Bougard", "family": null, "affiliations": ["\n                            IRHT - Institut de recherche et d'histoire des textes                        "]}, {"given": "Mathieu Aubry", "family": null, "affiliations": ["\n                            LIGM - Laboratoire d'Informatique Gaspard-Monge                        "]}, {"given": "Marc H. Smith", "family": null, "affiliations": ["\n                            ENC - École nationale des chartes                        "]}, {"given": "Olivier Poncet", "family": null, "affiliations": ["\n                            ENC - École nationale des chartes                        "]}, {"given": "Pierre-Guillaume Raverdy", "family": null, "affiliations": ["\n                            SED - Service Expérimentation et Développement  [Paris Rocquencourt]                        "]}], "publisher": null, "date": "2020/06/16", "keywords": ["cross-domain recognition ; deep learning ; watermark recognition ; web application ; paper analysis"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "24", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/halshs-01543050v2", "identifier": {"string_id": "halshs-01543050", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : This paper discusses markup strategies for the identification and description of text reuses in a corpus of patristic texts related to the BIBLINDEX Project, an online index of biblical references in Early Christian Literature. In addition to the development of a database that can be queried by canonical biblical or patristic references, a sample corpus of patristic texts has been encoded following the guidelines of the TEI (Text Encoding Initiative), in order to provide direct access to quoted and quoting text passages to the users of the https://www.biblindex.info platform.                    </div>", "article_title": "\n                TEI-encoding of text reuses in the BIBLINDEX Project            ", "authors": [{"given": "Elysabeth Hue-Gay", "family": null, "affiliations": ["\n                            HiSoMA - Histoire et Sources des Mondes antiques                        "]}, {"given": "Laurence Mellerin", "family": null, "affiliations": ["\n                            HiSoMA - Histoire et Sources des Mondes antiques                        "]}, {"given": "Emmanuelle Morlock", "family": null, "affiliations": ["\n                            HiSoMA - Histoire et Sources des Mondes antiques                        "]}], "publisher": null, "date": "2017/10/07", "keywords": ["patristics ;  Bernard of Clairvaux ;  Sources Chrétiennes ;  text reuses ; BIBLINDEX ;  quotations ;  Biblia Patristica ; TEI ;  Text Encoding Initiative ;  text markup ;  Bible ;  Greek ;  Latin ;  Septuagint ;  Vulgata"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01528092v2", "identifier": {"string_id": "hal-01528092", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : The Text Alignment Network (TAN) is a suite of XML encoding formats intended to serve anyone who wishes to encode, exchange, and study multiple versions of texts (e.g., translations, paraphrases), and annotations on those texts (e.g., quotations, word-for-word correspondences). This article focuses on TAN’s innovative intertextual pointers, which, I argue, provide an unprecedented level of readability, interoperability, and semantic context. Because TAN is a new, experimental format, this article provides a brief introduction to the format and concludes with comments on progress and future prospects.                    </div>", "article_title": "\n                Intertextual Pointers in the Text Alignment Network            ", "authors": [{"given": "Joel Kalvesmaki", "family": null, "affiliations": ["\n                            DO - Dumbarton Oaks                        "]}], "publisher": null, "date": "2017/10/20", "keywords": ["alignment ;  TEI ; XML ;  TAN ;  intertextuality ;  canonical references"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01265297v2", "identifier": {"string_id": "hal-01265297", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : The production of digital critical editions of texts using TEI is now a widely-adopted procedure within digital humanities. The work described in this paper extends this approach to the publication of gnomologia (anthologies of wise sayings) , which formed a widespread literary genre in many cultures of the medieval Mediterranean. These texts are challenging because they were rarely copied straightforwardly ; rather , sayings were selected , reorganised , modified or re-attributed between manuscripts , resulting in a highly interconnected corpus for which a standard approach to digital publication is insufficient. Focusing on Greek and Arabic collections , we address this challenge using semantic web techniques to create an ecosystem of texts , relationships and annotations , and consider a new model – organic , collaborative , interconnected , and open-ended – of what constitutes an edition. This semantic web-based approach allows scholars to add their own materials and annotations to the network of information and to explore the conceptual networks that arise from these interconnected sayings .                    </div>", "article_title": "\n                Computer - Assisted Processing of Intertextuality in Ancient Languages            ", "authors": [{"given": "Mark Hedges", "family": null, "affiliations": ["\n                            King‘s College London                        "]}, {"given": "Anna Jordanous", "family": null, "affiliations": ["\n                            University of Kent [Canterbury]                        "]}, {"given": "K. Faith Lawrence", "family": null, "affiliations": ["\n                            King‘s College London                        "]}, {"given": "Charlotte Roueché", "family": null, "affiliations": ["\n                            King‘s College London                        "]}, {"given": "Charlotte Tupman", "family": null, "affiliations": ["\n                            University of Exeter                        "]}], "publisher": null, "date": "2017/08/03", "keywords": ["TEI ; gnomologia ; RDF ; manuscripts ; ontology ; linked data ; semantic web ; digital edition ; anthologies"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1912.05082v3", "identifier": {"string_id": "1912.05082", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  Scholarship on underresourced languages bring with them a variety of\nchallenges which make access to the full spectrum of source materials and their\nevaluation difficult. For Coptic in particular, large scale analyses and any\nkind of quantitative work become difficult due to the fragmentation of\nmanuscripts, the highly fusional nature of an incorporational morphology, and\nthe complications of dealing with influences from Hellenistic era Greek, among\nother concerns. Many of these challenges, however, can be addressed using\nDigital Humanities tools and standards. In this paper, we outline some of the\nlatest developments in Coptic Scriptorium, a DH project dedicated to bringing\nCoptic resources online in uniform, machine readable, and openly available\nformats. Collaborative web-based tools create online 'virtual departments' in\nwhich scholars dispersed sparsely across the globe can collaborate, and natural\nlanguage processing tools counterbalance the scarcity of trained editors by\nenabling machine processing of Coptic text to produce searchable, annotated\ncorpora.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>A Collaborative Ecosystem for Digital Coptic Studies</h1>", "authors": [{"given": "Schroeder", "family": "Caroline T.", "affiliations": null}, {"given": "Zeldes", "family": "Amir", "affiliations": null}], "publisher": null, "date": "2019/12/11", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01276243v3", "identifier": {"string_id": "hal-01276243", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : The Project The literary tradition in the third and fourth centuries CE: Grammarians, rhetoricians and sophists as sources of Graeco-Roman literature (FFI2014-52808-C2-1-P) aims to trace and classify all types of quotations, both explicit (with or without mention of the author and/or title) and hidden, in a corpus comprising the Greek grammarians, rhetoricians and \" sophists \" of the third and fourth centuries CE. At the same time, we try to detect whether or not these are first-hand quotations, and if our quoting authors (28 in all) are, in turn, secondary sources for the same citations in later authors. We also study the philological (textual) aspects of the quotations in their context, and the problems of limits they sometimes pose. Finally, we are interested in the function of the quotation in the citing work. This is the first time that such a comprehensive study of this corpus is attempted. This paper explains our methodology, and how we store all these data in our electronic card-file.                     </div>", "article_title": "\n                Dealing with all types of quotations (and their parallels) in a closed corpus: The methodology of the Project The literary tradition in the third and fourth centuries CE: Grammarians, rhetoricians and sophists as sources of Graeco-Roman literature            ", "authors": [{"given": "Lucía Rodríguez-Noriega", "family": null, "affiliations": ["\n                            Universidad de Oviedo [Oviedo]                        "]}], "publisher": null, "date": "2017/06/13", "keywords": ["Intertextuality ; Greco-Roman scholars of the Empire ; Fragmentary literature"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01279493v2", "identifier": {"string_id": "hal-01279493", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : Greek documentary papyri form an important direct source for Ancient Greek. It has been exploited surprisingly little in Greek linguistics due to a lack of good tools for searching linguistic structures. This article presents a new tool and digital platform, “Sematia”, which enables transforming the digital texts available in TEI EpiDoc XML format to a format which can be morphologically and syntactically annotated (treebanked), and where the user can add new metadata concerning the text type, writer and handwriting of each act of writing. An important aspect in this process is to take into account the original surviving writing vs. the standardization of language and supplements made by the editors. This is performed by creating two different layers of the same text. The platform is in its early development phase. Ongoing and future developments, such as tagging linguistic variation phenomena as well as queries performed within Sematia, are discussed at the end of the article.                    </div>", "article_title": "\n                Preprocessing Greek Papyri for Linguistic Annotation            ", "authors": [{"given": "Marja Vierros", "family": null, "affiliations": ["\n                            Department of World Cultures                        "]}, {"given": "Erik Henriksson", "family": null, "affiliations": ["\n                            Department of World Cultures                        "]}], "publisher": null, "date": "2017/06/09", "keywords": ["JavaScript ; Python ; MySQL ; TEI EpiDoc XML ; Greek ; papyri ; linguistic annotation ; treebank ; dependency grammar"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1602.08844v2", "identifier": {"string_id": "1602.08844", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  This paper describes the Quantitative Criticism Lab, a collaborative\ninitiative between classicists, quantitative biologists, and computer\nscientists to apply ideas and methods drawn from the sciences to the study of\nliterature. A core goal of the project is the use of computational biology,\nnatural language processing, and machine learning techniques to investigate\nauthorial style, intertextuality, and related phenomena of literary\nsignificance. As a case study in our approach, here we review the use of\nsequence alignment, a common technique in genomics and computational\nlinguistics, to detect intertextuality in Latin literature. Sequence alignment\nis distinguished by its ability to find inexact verbal similarities, which\nmakes it ideal for identifying phonetic echoes in large corpora of Latin texts.\nAlthough especially suited to Latin, sequence alignment in principle can be\nextended to many other languages.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Bioinformatics and Classical Literary Study</h1>", "authors": [{"given": "Chaudhuri", "family": "Pramit", "affiliations": null}, {"given": "Dexter", "family": "Joseph P.", "affiliations": null}], "publisher": null, "date": "2016/02/29", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01371751v3", "identifier": {"string_id": "hal-01371751", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : We describe the course of a hackathon dedicated to the development of linguistic tools for Tibetan Buddhist studies. Over a period of five days, a group of seventeen scholars, scientists, and students developed and compared algorithms for intertextual alignment and text classification, along with some basic language tools, including a stemmer and word segmenter.                    </div>", "article_title": "\n                A Hackathon for Classical Tibetan            ", "authors": [{"given": "Orna Almogi", "family": null, "affiliations": ["\n                            UHH - Universität Hamburg                        "]}, {"given": "Lena Dankin", "family": null, "affiliations": ["\n                            TAU-CS - School of Computer Science                         "]}, {"given": "Nachum Dershowitz", "family": null, "affiliations": ["\n                            TAU-CS - School of Computer Science                         "]}, {"given": "Lior Wolf", "family": null, "affiliations": ["\n                            TAU-CS - School of Computer Science                         "]}], "publisher": null, "date": "2018/12/30", "keywords": ["Tibetan ; Buddhist studies ; hackathon ; stemming ; segmentation ; intertextual alignment ; text classification"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1603.01207v1", "identifier": {"string_id": "1603.01207", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  Despite increasing interest in Syriac studies and growing digital\navailability of Syriac texts, there is currently no up-to-date infrastructure\nfor discovering, identifying, classifying, and referencing works of Syriac\nliterature. The standard reference work (Baumstark's Geschichte) is over ninety\nyears old, and the perhaps 20,000 Syriac manuscripts extant worldwide can be\naccessed only through disparate catalogues and databases. The present article\nproposes a tentative data model for <a class=\"link-external link-http\" href=\"http://Syriaca.org\" rel=\"external noopener nofollow\">this http URL</a>'s New Handbook of Syriac\nLiterature, an open-access digital publication that will serve as both an\nauthority file for Syriac works and a guide to accessing their manuscript\nrepresentations, editions, and translations. The authors hope that by\npublishing a draft data model they can receive feedback and incorporate\nsuggestions into the next stage of the project.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>From manuscript catalogues to a handbook of Syriac literature: Modeling an infrastructure for Syriaca.org</h1>", "authors": [{"given": "Gibson", "family": "Nathan P.", "affiliations": null}, {"given": "Michelson", "family": "David A.", "affiliations": null}, {"given": "Schwartz", "family": "Daniel L.", "affiliations": null}], "publisher": null, "date": "2016/03/03", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1603.01597v2", "identifier": {"string_id": "1603.01597", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  In this paper we consider two sequence tagging tasks for medieval Latin:\npart-of-speech tagging and lemmatization. These are both basic, yet\nfoundational preprocessing steps in applications such as text re-use detection.\nNevertheless, they are generally complicated by the considerable orthographic\nvariation which is typical of medieval Latin. In Digital Classics, these tasks\nare traditionally solved in a (i) cascaded and (ii) lexicon-dependent fashion.\nFor example, a lexicon is used to generate all the potential lemma-tag pairs\nfor a token, and next, a context-aware PoS-tagger is used to select the most\nappropriate tag-lemma pair. Apart from the problems with out-of-lexicon items,\nerror percolation is a major downside of such approaches. In this paper we\nexplore the possibility to elegantly solve these tasks using a single,\nintegrated approach. For this, we make use of a layered neural network\narchitecture from the field of deep representation learning.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Integrated Sequence Tagging for Medieval Latin Using Deep Representation Learning</h1>", "authors": [{"given": "Kestemont", "family": "Mike", "affiliations": null}, {"given": "De Gussem", "family": "Jeroen", "affiliations": null}], "publisher": null, "date": "2016/03/04", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/halshs-01532877v2", "identifier": {"string_id": "halshs-01532877", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : The ‘Version Variation Visualization’ project has developed online tools to support comparative, algorithm-assisted investigations of a corpus of multiple versions of a text, e.g. variants, translations, adaptations (Cheesman, 2015, 2016; Cheesman et al., 2012, 2012-13, 2016; Thiel, 2014; links: www.tinyurl.com/vvvex). A segmenting and aligning tool allows users to 1) define arbitrary segment types, 2) define arbitrary text chunks as segments, and 3) align segments between a ‘base text’ (a version of the ‘original’ or translated text), and versions of it. The alignment tool can automatically align recurrent defined segment types in sequence.\r\nSeveral visual interfaces in the prototype installation enable exploratory access to parallel versions, to comparative visual representations of versions’ alignment with the base text, and to the base text visually annotated by an algorithmic analysis of variation among versions of segments. Data can be filtered, viewed and exported in diverse ways. Many more modes of access and analysis can be envisaged. The tool is language neutral. Experiments so far mostly use modern texts: German Shakespeare translations. Roos is working on a collection of approx. 100 distinct English-language translations of a Hebrew text with ancient Hebrew and Aramaic passages: the Haggadah (Roos, 2015)                    </div>", "article_title": "\n                Version Variation Visualization (VVV): Case Studies on the Hebrew Haggadah in English            ", "authors": [{"given": "Tom Cheesman", "family": null, "affiliations": ["\n                            Swansea University                        "]}, {"given": "Avraham Roos", "family": null, "affiliations": ["\n                            UvA - Universiteit van Amsterdam                        "]}], "publisher": null, "date": "2017/06/22", "keywords": ["digital humanities ; Social science ; translation ;  corpus ;  Judaism ;  haggadah ;  VVV ;  visualization ;  algorithm ;  retranslation ;  4 sons ; humanities"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1602.08657v2", "identifier": {"string_id": "1602.08657", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  The software programs generally used with the TLG (Thesaurus Linguae Graecae)\nand the CLCLT (CETEDOC Library of Christian Latin Texts) CD-ROMs are not well\nsuited for finding quotations and allusions. QuotationFinder uses more\nsophisticated criteria as it ranks search results based on how closely they\nmatch the source text, listing search results with literal quotations first and\nloose verbal parallels last.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>QuotationFinder - Searching for Quotations and Allusions in Greek and Latin Texts and Establishing the Degree to Which a Quotation or Allusion Matches Its Source</h1>", "authors": [{"given": "Herren", "family": "Luc", "affiliations": null}], "publisher": null, "date": "2016/02/28", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1602.08715v2", "identifier": {"string_id": "1602.08715", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  We propose a method for efficiently finding all parallel passages in a large\ncorpus, even if the passages are not quite identical due to rephrasing and\northographic variation. The key ideas are the representation of each word in\nthe corpus by its two most infrequent letters, finding matched pairs of strings\nof four or five words that differ by at most one word and then identifying\nclusters of such matched pairs. Using this method, over 4600 parallel pairs of\npassages were identified in the Babylonian Talmud, a Hebrew-Aramaic corpus of\nover 1.8 million words, in just over 30 seconds. Empirical comparisons on\nsample data indicate that the coverage obtained by our method is essentially\nthe same as that obtained using slow exhaustive methods.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Identification of Parallel Passages Across a Large Hebrew/Aramaic Corpus</h1>", "authors": [{"given": "Shmidman", "family": "Avi", "affiliations": null}, {"given": "Koppel", "family": "Moshe", "affiliations": null}, {"given": "Porat", "family": "Ely", "affiliations": null}], "publisher": null, "date": "2016/02/28", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01280627v4", "identifier": {"string_id": "hal-01280627", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : If one is convinced that \" quantitative research provides data not interpretation \" [Moretti, 2005, 9], close reading should thus be considered as not only the necessary bridge between big data and interpretation but also the core duty of the Humanities. To test its potential in a neglected field – the Arabic manuscripts of the Letters of Paul of Tarsus – an enhanced, digital edition has been in development as a progression of a Swiss National Fund project. This short paper presents the development of this edition and perspectives regarding a second project. Based on the Edition Visualization Technology tool, the digital edition provides a transcription of the Arabic text, a standardized and vocalized version, as well as French translation with all texts encoded in TEI XML. Thanks to another Swiss National Foundation subsidy, a new research project on the unique New Testament, trilingual (Greek-Latin-Arabic) manuscript, the Marciana Library Gr. Z. 11 (379), 12th century, is currently underway. This project includes new features such as \" Textlink \" , \" Hotspot \" and notes: HumaReC.                    </div>", "article_title": "\n                Editing New Testament Arabic Manuscripts in a TEI-base: fostering close reading in Digital Humanities            ", "authors": [{"given": "Claire Clivaz", "family": null, "affiliations": ["\n                            SIB - Swiss Institute of Bioinformatics [Lausanne]                        "]}, {"given": "Sara Schulthess", "family": null, "affiliations": ["\n                            SIB - Swiss Institute of Bioinformatics [Lausanne]                        "]}, {"given": "Martial Sankar", "family": null, "affiliations": ["\n                            SIB - Swiss Institute of Bioinformatics [Lausanne]                        "]}], "publisher": null, "date": "2017/06/03", "keywords": ["Paul of Tarsus ; Letters ; Digital edition ; Arabic TEI ; New Testament"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01282568v4", "identifier": {"string_id": "hal-01282568", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : Most intertextuality in classical poetry is unmarked, that is, it lacks objective signposts to make readers aware of the presence of references to existing texts. Intergeneric relationships can pose a particular problem as scholarship has long privileged intertextual relationships between works of the same genre. This paper treats the influence of Latin love elegy on Lucan’s epic poem, Bellum Civile, by looking at two features of unmarked intertextuality: frequency and distribution. I use the Tesserae project to generate a dataset of potential intertexts between Lucan’s epic and the elegies of Tibullus, Propertius, and Ovid, which are then aggregrated and mapped in Lucan’s text. This study draws two conclusions: 1. measurement of intertextual frequency shows that the elegists contribute fewer intertexts than, for example, another epic poem (Virgil’s Aeneid), though far more than the scholarly record on elegiac influence in Lucan would suggest; and 2. mapping the distribution of intertexts confirms previous scholarship on the influence of elegy on the Bellum Civile by showing concentrations of matches, for example, in Pompey and Cornelia’s meeting before Pharsalus (5.722-815) or during the affair between Caesar and Cleopatra (10.53-106). By looking at both frequency and proportion, we can demonstrate systematically the generic enrichment of Lucan’s Bellum Civile with respect to Latin love elegy.                    </div>", "article_title": "\n                Measuring and Mapping Intergeneric Allusion in Latin Poetry using Tesserae            ", "authors": [{"given": "Patrick J. Burns", "family": null, "affiliations": ["\n                             Institute for the Study of the Ancient World                        ", "\n                             Institute for the Study of the Ancient World                        "]}], "publisher": null, "date": "2017/07/31", "keywords": ["allusion ;  Lucan ;  Latin epic ;  Latin love elegy ;  intertextuality ;  generic enrichment ;  Tesserae"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01645124v2", "identifier": {"string_id": "hal-01645124", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : This article explores whether and how network visualization can benefit philological and historical-linguistic study. This is illustrated with a corpus-based investigation of scribes' language use in a lemmatized and morphologically annotated corpus of documentary Latin (Late Latin Charter Treebank, LLCT2). We extract four continuous linguistic variables from LLCT2 and utilize a gradient colour palette in Gephi to visualize the variable values as node attributes in a trimodal network which consists of the documents, writers, and writing locations underlying the same corpus. We call this network the \"LLCT2 network\". The geographical coordinates of the location nodes form an approximate map, which allows for drawing geographical conclusions. The linguistic variables are examined both separately and as a sum variable, and the visualizations presented as static images and as interactive Sigma.js visualizations. The variables represent different domains of language competence of scribes who learnt written Latin practically as a second-language. The results show that the network visualization of linguistic features helps in observing patterns which support linguistic-philological argumentation and which risk passing unnoticed with traditional methods. However, the approach is subject to the same limitations as all visualization techniques: the human eye can only perceive a certain, relatively small amount of information at a time.                    </div>", "article_title": "\n                Visualizing linguistic variation in a network of Latin documents and scribes            ", "authors": [{"given": "Timo Korkiakangas", "family": null, "affiliations": ["\n                            UiO - University of Oslo                        "]}, {"given": "Matti Lassila", "family": null, "affiliations": ["\n                            Open Science Centre [Jyväskylä]                        "]}], "publisher": null, "date": "2018/04/24", "keywords": ["network visualization ;  Latin linguistics ;  Early Middle Ages ;  philology"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01466986v2", "identifier": {"string_id": "hal-01466986", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : Digital humanities require IT Infrastructure and sophisticated analytical tools, including data\r\nvisualization, data mining, statistics, text mining and information retrieval. Regarding funding, to\r\nbuild a local data center will necessitate substantial investments. Fortunately, there is another option\r\nthat will help researchers take advantage of these IT services to access, use and share information\r\neasily. Cloud services ideally offer on-demand software and resources over the Internet to read and\r\nanalyze ancient documents. More interestingly, billing system is completely flexible and based on\r\nresource usage and Quality of Service (QoS) level. In spite of its multiple advantages, outsourcing\r\ncomputations to an external provider arises several challenges. Specifically, security is the major\r\nfactor hindering the widespread acceptance of this new concept. As a case study, we review the use of\r\ncloud computing to process digital images safely. Recently, various solutions have been suggested to\r\nsecure data processing in cloud environement. Though, ensuring privacy and high performance needs\r\nmore improvements to protect the organization's most sensitive data. To this end, we propose a\r\nframework based on segmentation and watermarking techniques to ensure data privacy. In this respect,\r\nsegementation algorithm is used to to protect client's data against untauhorized access, while\r\nwatermarking method determines and maintains ownership. Consequentely, this framework will\r\nincrease the speed of development on ready-to-use digital humanities tools.                    </div>", "article_title": "\n                A Secured Data Processing Technique for Effective Utilization of Cloud Computing            ", "authors": [{"given": "Mbarek Marwan", "family": null, "affiliations": ["\n                            UCD - Université Chouaib Doukkali                        ", "\n                            Département Télécommunications, Réseaux et Informatique                        "]}, {"given": "Ali Kartit", "family": null, "affiliations": ["\n                            Département Télécommunications, Réseaux et Informatique                        ", "\n                            UCD - Université Chouaib Doukkali                        "]}, {"given": "Hassan Ouahmane", "family": null, "affiliations": ["\n                            Département Télécommunications, Réseaux et Informatique                        ", "\n                            UCD - Université Chouaib Doukkali                        "]}], "publisher": null, "date": "2017/11/21", "keywords": ["data processing ; security ; digital humanities ; cloud computing"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Scientific and Technological Strategic Intelligence (2016)", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01458216v1", "identifier": {"string_id": "hal-01458216", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : In this paper we present a system for offline recognition cursive Arabic handwritten text which is analytical without explicit segmentation based on Hidden Markov Models (HMMs). Extraction features preceded by baseline estimation are statistical and geometric to integrate both the peculiarities of the text and the pixel distribution characteristics in the word image. These features are modelled using hidden Markov models. The HMM-based classifiercontains a training module and a recognition module. The training module estimates the parameters of each of the character HMMs uses the Baum-Welchalgorithm. In the recognition phase, feature vectors extracted from an image are passed to a network of word lexicon entries formed of character models. The character sequence providing the maximumlikelihood identifies the recognized entry. If required, the recognition can generate N best output hypotheses rather than just the single best one. To determine the best output hypotheses, the Viterbi algorithm is used.The experiments on images of the benchmark IFN/ENIT database show that the proposed system improves recognition.                    </div>", "article_title": "\n                Cursive Arabic Handwriting Recognition System Without Explicit Segmentation Based on Hidden Markov Models            ", "authors": [{"given": "Mouhcine Rabi", "family": null, "affiliations": ["\n                            IRF-SIC - Laboratoire Image et Reconnaissance de Formes - Systèmes Intelligents et Communicants                        "]}, {"given": "Mustapha Amrouch", "family": null, "affiliations": ["\n                            IRF-SIC - Laboratoire Image et Reconnaissance de Formes - Systèmes Intelligents et Communicants                        "]}, {"given": "Zouhair Mahani", "family": null, "affiliations": ["\n                            IRF-SIC - Laboratoire Image et Reconnaissance de Formes - Systèmes Intelligents et Communicants                        "]}], "publisher": null, "date": "2017/02/09", "keywords": ["HMMs ; Arabic text ; handwriting ; Recognition"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Scientific and Technological Strategic Intelligence (2016)", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01294591v2", "identifier": {"string_id": "hal-01294591", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : This paper discusses the word level alignment of lemmatised bitext consisting of the Oratio I of Gregory of Nazianzus in its Greek model and Georgian translation. This study shows how the direct and empirical observations offered by an aligned text enable an accurate analysis of techniques of translation and many philological parameters of the text.                    </div>", "article_title": "\n                Text Alignment in Ancient Greek and Georgian: A Case-Study on the First Homily of Gregory of Nazianzus            ", "authors": [{"given": "Tamara Pataridze", "family": null, "affiliations": ["\n                            Fonds de la Recherche Scientifique [FNRS]                        ", "\n                            INCAL - Institut des Civilisations Arts et Lettres (INCAL)                        ", "\n                            Fonds de la Recherche Scientifique [FNRS]                        "]}, {"given": "Bastien Kindt", "family": null, "affiliations": ["\n                            INCAL - Institut des Civilisations Arts et Lettres (INCAL)                        "]}], "publisher": null, "date": "2017/12/22", "keywords": ["lexical tagging ; corpus ; bilingual dictionaries ; bitext ; Ancient Greek ; Ancient Georgian ; morphological tagging ; text alignment ; lemmatisation ;  morphological tagging"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01762730v5", "identifier": {"string_id": "hal-01762730", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : This work applies knowledge engineering’s techniques to medieval illuminations. Inside it, an illumination is considered as a knowledge graph which was used by some elites in the Middle Ages to represent themselves as a social group and exhibit the events in their lives, and their cultural values. That graph is based on combinations of symbolic elements linked each to others with semantic relations. Those combinations were used to encode visual metaphors and influential messages whose interpretations are sometimes tricky for not experts. Our work aims to describe the meaning of those elements through logical modelling using ontologies. To achieve that, we construct logical reasoning rules and simulate them using artificial intelligence mechanisms. The goal is to facilitate the interpretation of illuminations and provide, in a future evolution of current social media, logical formalisation of new encoding and information transmission services.                    </div>", "article_title": "\n                                    ", "authors": [{"given": "Djibril Diarra", "family": null, "affiliations": ["\n                            Checksem                        "]}, {"given": "Martine Clouzot", "family": null, "affiliations": ["\n                            ARTeHiS - Archéologie, Terre, Histoire, Sociétés [Dijon]                        "]}, {"given": "Christophe Nicolle", "family": null, "affiliations": ["\n                            Checksem                        "]}], "publisher": null, "date": "2019/06/03", "keywords": ["Semantic relation ; Symbolic relation ; Ontology ; Social network ; Medieval illumination"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Data Science and Digital Humanities @ EGC 2018", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/halshs-01557447v1", "identifier": {"string_id": "halshs-01557447", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : A new method for grouping manuscripts in clusters is presented with the calculation of distances between readings, then between witnesses. A classification algorithm (\" Hierarchical Ascendant Clustering \"), achieved through computer-aided processing, enables the construction of trees illustrating the textual taxonomy obtained. This method is applied to the Old Latin witnesses of the Gospel of John, and, in order to provide a study of a reasonable size, to a chapter as a whole (chapter 14). The result basically confirms the text-types identified by Bonatius Fischer, founder of the Vetus Latina Institute, while it invalidates the classification adopted by the current edition of the Vetus Latina of the Gospel of John.                    </div>", "article_title": "\n                A Classification of Manuscripts Based on A New Quantitative Method. The Old Latin Witnesses of John's Gospel as Text Case            ", "authors": [{"given": "David Pastorelli", "family": null, "affiliations": ["\n                            AU - Avignon Université                        "]}], "publisher": null, "date": "2017/07/06", "keywords": ["Manuscripts ; Gospel of John ; latin witnesses"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Computer-Aided Processing of Intertextuality in Ancient Languages", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01456090v2", "identifier": {"string_id": "hal-01456090", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : This paper proposes an ontological integration model for credit risk management. It is based on three ontologies; one is global describing credit risk management process and two other locals, the first, describes the credit granting process, and the second presents the concepts necessary for the monitoring of credit system. This paper also presents the technique used for matching between global ontology and local ontologies.                     </div>", "article_title": "\n                Applying ontologies to data integration systems for bank credit risk management            ", "authors": [{"given": "Jalil Elhassouni", "family": null, "affiliations": ["\n                            GSCM-LRIT - Laboratoire de Recherche en Informatique et Télécommunications [Rabat]                        "]}, {"given": "Mehdi Bazzi", "family": null, "affiliations": ["\n                            LIAD Laboratory, FSAC, Hassan II University, Morocco                        "]}, {"given": "Abderrahim Qadi", "family": null, "affiliations": ["\n                            Ecole Supérieure de Technologie de Meknès                        ", "\n                            GSCM-LRIT - Laboratoire de Recherche en Informatique et Télécommunications [Rabat]                        "]}, {"given": "Mohamed Haziti", "family": null, "affiliations": ["\n                            GSCM-LRIT - Laboratoire de Recherche en Informatique et Télécommunications [Rabat]                        "]}], "publisher": null, "date": "2017/11/26", "keywords": ["credit risk management ;  data integration ;  ontologies alignment"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Scientific and Technological Strategic Intelligence (2016)", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01759191v2", "identifier": {"string_id": "hal-01759191", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : The EUROPANGE project, involving both medievalists and computer scientists, aims to study the emergence of a corps of administrators in the Angevin controlled territories in the XIII–XV centuries. Our project attempts to analyze the officers' careers, shared relation networks and strategies based on the study of individual biographies. In this paper, we describe methods and tools designed to analyze these prosopographical data. These include OLAP analyzes and network analyzes associated with cartographic and chronological visualization tools.                    </div>", "article_title": "\n                Prosopographical data analysis. Application to the Angevin officers (XIII–XV centuries)            ", "authors": [{"given": "Anne Tchounikine", "family": null, "affiliations": ["\n                            BD - Base de Données                        "]}, {"given": "Maryvonne Miquel", "family": null, "affiliations": ["\n                            BD - Base de Données                        "]}, {"given": "Thierry Pécout", "family": null, "affiliations": ["\n                            LEM - Laboratoire d'Etudes sur les Monothéismes                        "]}, {"given": "Jean-Luc Bonnaud", "family": null, "affiliations": null}], "publisher": null, "date": "2018/05/24", "keywords": [], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Data Science and Digital Humanities @ EGC 2018", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1312.6675v2", "identifier": {"string_id": "1312.6675", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  Social media and social networks have already woven themselves into the very\nfabric of everyday life. This results in a dramatic increase of social data\ncapturing various relations between the users and their associated artifacts,\nboth in online networks and the real world using ubiquitous devices. In this\nwork, we consider social interaction networks from a data mining perspective -\nalso with a special focus on real-world face-to-face contact networks: We\ncombine data mining and social network analysis techniques for examining the\nnetworks in order to improve our understanding of the data, the modeled\nbehavior, and its underlying emergent processes. Furthermore, we adapt, extend\nand apply known predictive data mining algorithms on social interaction\nnetworks. Additionally, we present novel methods for descriptive data mining\nfor uncovering and extracting relations and patterns for hypothesis generation\nand exploration, in order to provide characteristic information about the data\nand networks. The presented approaches and methods aim at extracting valuable\nknowledge for enhancing the understanding of the respective data, and for\nsupporting the users of the respective systems. We consider data from several\nsocial systems, like the social bookmarking system BibSonomy, the social\nresource sharing system flickr, and ubiquitous social systems: Specifically, we\nfocus on data from the social conference guidance system Conferator and the\nsocial group interaction system MyGroup. This work first gives a short\nintroduction into social interaction networks, before we describe several\nanalysis results in the context of online social networks and real-world\nface-to-face contact networks. Next, we present predictive data mining methods,\ni.e., for localization, recommendation and link prediction. After that, we\npresent novel descriptive data mining methods for mining communities and\npatterns.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Data Mining on Social Interaction Networks</h1>", "authors": [{"given": "Atzmueller", "family": "Martin", "affiliations": null}], "publisher": null, "date": "2013/12/23", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01443713v1", "identifier": {"string_id": "hal-01443713", "id_scheme": "hal"}, "abstract": null, "article_title": "\n                                    ", "authors": [{"given": "Hicham Gibet Tani", "family": null, "affiliations": ["\n                            LIST - Laboratoire d'Informatique, Système et Télécommunications                        "]}, {"given": "Chaker El Amrani", "family": null, "affiliations": ["\n                            LIST - Laboratoire d'Informatique, Système et Télécommunications                        "]}], "publisher": null, "date": "2017/01/23", "keywords": ["Scheduling Algorithms ; Cloud Computing Simulation ; First Come First Served", "Cloud Computing ; Big Data ; Algorithme de planification ; Simulation Cloud Computing ; Round Robin ; Premier Arrivé Premier Servi"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Scientific and Technological Strategic Intelligence (2016)", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1402.2003v2", "identifier": {"string_id": "1402.2003", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  The project presents the strategy adopted by the Rough Cilicia Archaeological\nSurvey team for publishing its primary data and reports via three potentially\ntransformative strategies for digital humanities: Loose coupling of digital\ndata curation and publishing platforms. In loosely coupled systems, components\nshare only a limited set of simple assumptions, which enables systems to evolve\ndynamically. Collaborative creation of map based narrative content. Connecting\nprint scholarship (book, reports, article) to online resources via\ntwo-dimensional barcodes (2D codes) that can be printed on paper and can call\nup hyperlinks when scanned with a Smartphone. The three strategies are made\npossible by loosely coupling two autonomous services: Visible Past, dedicated\nto web collaboration and digital-print publishing and Open Context, which is a\ngeo-historical data archiving and publishing service. The Rough Cilicia\nArchaeological Survey, Visible Past, and Open Context work together to\nillustrate a new genre of scholarship, which combine qualitative narratives and\nquantitative representations of space and social phenomena. The project\nprovides tools for collaborative creation of rich scholarly narratives that are\nspatially located and for connecting print publications to the digital realm.\nThe project is a case study for utilizing the three new strategies for creating\nand publishing spatial humanities scholarship more broadly for ancient\nhistorians.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>A New Approach to Reporting Archaeological Surveys: Connecting Rough Cilicia, Visible Past and Open Context through loose coupling and 3d codes</h1>", "authors": [{"given": "Matei", "family": "Sorin Adam", "affiliations": null}, {"given": "Rauh", "family": "Nicholas K.", "affiliations": null}, {"given": "Kansa", "family": "Eric C.", "affiliations": null}], "publisher": null, "date": "2014/02/09", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01466986v1", "identifier": {"string_id": "hal-01466986", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : Digital humanities require IT Infrastructure and sophisticated analytical tools, including data\r\nvisualization, data mining, statistics, text mining and information retrieval. Regarding funding, to\r\nbuild a local data center will necessitate substantial investments. Fortunately, there is another option\r\nthat will help researchers take advantage of these IT services to access, use and share information\r\neasily. Cloud services ideally offer on-demand software and resources over the Internet to read and\r\nanalyze ancient documents. More interestingly, billing system is completely flexible and based on\r\nresource usage and Quality of Service (QoS) level. In spite of its multiple advantages, outsourcing\r\ncomputations to an external provider arises several challenges. Specifically, security is the major\r\nfactor hindering the widespread acceptance of this new concept. As a case study, we review the use of\r\ncloud computing to process digital images safely. Recently, various solutions have been suggested to\r\nsecure data processing in cloud environement. Though, ensuring privacy and high performance needs\r\nmore improvements to protect the organization's most sensitive data. To this end, we propose a\r\nframework based on segmentation and watermarking techniques to ensure data privacy. In this respect,\r\nsegementation algorithm is used to to protect client's data against untauhorized access, while\r\nwatermarking method determines and maintains ownership. Consequentely, this framework will\r\nincrease the speed of development on ready-to-use digital humanities tools.                    </div>", "article_title": "\n                A Secured Data Processing Technique for Effective Utilization of Cloud Computing            ", "authors": [{"given": "Mbarek Marwan", "family": null, "affiliations": ["\n                            UCD - Université Chouaib Doukkali                        ", "\n                            Département Télécommunications, Réseaux et Informatique                        "]}, {"given": "Ali Kartit", "family": null, "affiliations": ["\n                            Département Télécommunications, Réseaux et Informatique                        ", "\n                            UCD - Université Chouaib Doukkali                        "]}, {"given": "Hassan Ouahmane", "family": null, "affiliations": ["\n                            Département Télécommunications, Réseaux et Informatique                        ", "\n                            UCD - Université Chouaib Doukkali                        "]}], "publisher": null, "date": "2017/02/16", "keywords": ["cloud computing ; digital humanities ; security ; data processing"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "Special Issue on Scientific and Technological Strategic Intelligence (2016)", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-00919370v3", "identifier": {"string_id": "hal-00919370", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : This research study tested three different NLP technologies to analyze representative journalistic discourse used in the 2007 and 2012 presidential campaigns in France. The analysis focused on the discourse in relation to the candidate's gender and/ or political party. Our findings suggest that using specific software to examine a journalistic corpus can reveal linguistic patterns and choices made on the basis of political affiliation and/or gender stereotypes. These conclusions are drawn from quantitative and qualitative analysis carried out with three different software programs: SEMY, which semi-automatically provides semantic profiles; ANTCONC, which provides useful Keywords in Context (KWIC) or abstracts of texts, as well as collocations; TERMOSTAT, which reveals discourse specificities, frequencies and the most common morpho-syntactic patterns. Analysis of our data point to convergent asymmetries between female and male candidates in journalistic discourse (however conditionally) for the 2007 and the 2012 French presidential campaigns. We conclude that social gender (i.e., stereotypical expectations of who will be a typical member of a given category) and / or political favoritism may affect the representation of leadership in discourse, which, in turn, may influence the readership, hence the electorate. Thus the study recommends the use of corpus linguistic tools for the semi-automatic investigation of political texts.                    </div>", "article_title": "\n                ANALYSING JOURNALISTIC DISCOURSE AND FINDING OPINIONS SEMI-AUTOMATICALLY?: A CASE STUDY OF THE 2007 AND 2012 PRESIDENTIAL FRENCH CAMPAIGNS            ", "authors": [{"given": "Fabienne Baider", "family": null, "affiliations": ["\n                            UCY - University of Cyprus                        "]}], "publisher": null, "date": "2014/05/02", "keywords": ["semi-automatic reading ; gender stereotypes ; political favoritism ; journalistic discourse ; French presidential campaign", "lecture automatique ; stéréotypes de genre ; biais politique ; discours journalistique ; campagne présidentielle française"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "2014", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1312.5817v3", "identifier": {"string_id": "1312.5817", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  This study analyzed references and source papers of the Proceedings of\n2009-2012 International Conference of Digital Archives and Digital Humanities\n(DADH), which was held annually in Taiwan. A total of 59 sources and 1,104\nreferences were investigated, based on descriptive analysis and subject\nanalysis of library practices on cataloguing. Preliminary results showed\nhistorical materials, events, bureaucracies, and people of Taiwan and China in\nthe Qing Dynasty were the major subjects in the tempo-spatial dimensions. The\nsubject-date figure depicted a long-low head and short-high tail curve, which\ndemonstrated both characteristics of research of humanities and application of\ntechnology in digital humanities. The dates of publication of the references\nspanned over 360 years, which shows a long time span in research materials. A\nmajority of the papers (61.41%) were single-authored, which is in line with the\ncommon research practice in the humanities. Books published by general\npublishers were the major type of references, and this was the same as that of\nestablished humanities research. The next step of this study will focus on the\ncomparison of characteristics of both sources and references of international\njournals with those reported in this article.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Exploring Regional Development of Digital Humanities Research: A Case Study for Taiwan</h1>", "authors": [{"given": "Chen", "family": "Kuang-hua", "affiliations": null}, {"given": "Hsueh", "family": "Bi-Shin", "affiliations": null}], "publisher": null, "date": "2013/12/20", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01024985v4", "identifier": {"string_id": "hal-01024985", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : The contribution of this article is twofold: the adaptation and application of models of deception from psychology, combined with data-mining techniques, to the text of speeches given by candidates in the 2008 U.S. presidential election; and the observation of both short-term and\r\nmedium-term differences in the levels of deception. Rather than considering the effect of deception on voters, deception is used as a lens through which to observe the self-perceptions of candidates and campaigns. The method of analysis is fully automated and requires no human coding, and so can be applied to many other domains in a straightforward way. The authors posit explanations for the observed variation in terms of a dynamic tension between the goals of campaigns at each moment in time, for example gaps between their view of the candidate’s persona and the persona expected for the position; and the difficulties of crafting and sustaining a persona, for example, the cognitive cost and the need for apparent continuity with past actions and perceptions. The changes in the resulting balance provide a new channel by which to understand the drivers of political campaigning, a channel that is hard to manipulate because its markers are created subconsciously.                    </div>", "article_title": "\n                Deception in Speeches of Candidates for Public Office            ", "authors": [{"given": "David Skillicorn", "family": null, "affiliations": ["\n                            School of computing [Kingston]                        "]}, {"given": "Christian Leuprecht", "family": null, "affiliations": ["\n                            RMCC - Royal Military College of Canada                        "]}], "publisher": null, "date": "2015/08/16", "keywords": ["political discourse ; corpus analytics ; U.S. presidential elections ; deception ; singular value decomposition"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1312.4617v2", "identifier": {"string_id": "1312.4617", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  Social network has gained remarkable attention in the last decade. Accessing\nsocial network sites such as Twitter, Facebook LinkedIn and Google+ through the\ninternet and the web 2.0 technologies has become more affordable. People are\nbecoming more interested in and relying on social network for information, news\nand opinion of other users on diverse subject matters. The heavy reliance on\nsocial network sites causes them to generate massive data characterised by\nthree computational issues namely; size, noise and dynamism. These issues often\nmake social network data very complex to analyse manually, resulting in the\npertinent use of computational means of analysing them. Data mining provides a\nwide range of techniques for detecting useful knowledge from massive datasets\nlike trends, patterns and rules [44]. Data mining techniques are used for\ninformation retrieval, statistical modelling and machine learning. These\ntechniques employ data pre-processing, data analysis, and data interpretation\nprocesses in the course of data analysis. This survey discusses different data\nmining techniques used in mining diverse aspects of the social network over\ndecades going from the historical techniques to the up-to-date models,\nincluding our novel technique named TRCM. All the techniques covered in this\nsurvey are listed in the Table.1 including the tools employed as well as names\nof their authors.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>A Survey of Data Mining Techniques for Social Media Analysis</h1>", "authors": [{"given": "Adedoyin-Olowe", "family": "Mariam", "affiliations": null}, {"given": "Gaber", "family": "Mohamed Medhat", "affiliations": null}, {"given": "Stahl", "family": "Frederic", "affiliations": null}], "publisher": null, "date": "2013/12/17", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1311.5401v2", "identifier": {"string_id": "1311.5401", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  Text data is often seen as \"take-away\" materials with little noise and easy\nto process information. Main questions are how to get data and transform them\ninto a good document format. But data can be sensitive to noise oftenly called\nambiguities. Ambiguities are aware from a long time, mainly because polysemy is\nobvious in language and context is required to remove uncertainty. I claim in\nthis paper that syntactic context is not suffisant to improve interpretation.\nIn this paper I try to explain that firstly noise can come from natural data\nthemselves, even involving high technology, secondly texts, seen as verified\nbut meaningless, can spoil content of a corpus; it may lead to contradictions\nand background noise.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Clustering and Relational Ambiguity: from Text Data to Natural Data</h1>", "authors": [{"given": "Turenne", "family": "Nicolas", "affiliations": null}], "publisher": null, "date": "2013/11/21", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1010.0803v3", "identifier": {"string_id": "1010.0803", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  How are people linked in a highly connected society? Since in many networks a\npower-law (scale-free) node-degree distribution can be observed, power-law\nmight be seen as a universal characteristics of networks. But this study of\ncommunication in the Flickr social online network reveals that power-law\nnode-degree distributions are restricted to only sparsely connected networks.\nMore densely connected networks, by contrast, show an increasing divergence\nfrom power-law. This work shows that this observation is consistent with the\nclassic idea from social sciences that similarity is the driving factor behind\ncommunication in social networks. The strong relation between communication\nstrength and node similarity could be confirmed by analyzing the Flickr\nnetwork. It also is shown that node similarity as a network formation model can\nreproduce the characteristics of different network densities and hence can be\nused as a model for describing the topological transition from weakly to\nstrongly connected societies.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Node similarity as a basic principle behind connectivity in complex networks</h1>", "authors": [{"given": "Scholz", "family": "Matthias", "affiliations": null}], "publisher": null, "date": "2010/10/05", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1405.3539v3", "identifier": {"string_id": "1405.3539", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  Using geometric data analysis, our objective is the analysis of narrative,\nwith narrative of emotion being the focus in this work. The following two\nprinciples for analysis of emotion inform our work. Firstly, emotion is\nrevealed not as a quality in its own right but rather through interaction. We\nstudy the 2-way relationship of Ilsa and Rick in the movie Casablanca, and the\n3-way relationship of Emma, Charles and Rodolphe in the novel {\\em Madame\nBovary}. Secondly, emotion, that is expression of states of mind of subjects,\nis formed and evolves within the narrative that expresses external events and\n(personal, social, physical) context. In addition to the analysis methodology\nwith key aspects that are innovative, the input data used is crucial. We use,\nfirstly, dialogue, and secondly, broad and general description that\nincorporates dialogue. In a follow-on study, we apply our unsupervised\nnarrative mapping to data streams with very low emotional expression. We map\nthe narrative of Twitter streams. Thus we demonstrate map analysis of general\nnarratives.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Pattern Recognition in Narrative: Tracking Emotional Expression in Context</h1>", "authors": [{"given": "Murtagh", "family": "Fionn", "affiliations": null}, {"given": "Ganz", "family": "Adam", "affiliations": null}], "publisher": null, "date": "2014/05/14", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-02324617v2", "identifier": {"string_id": "hal-02324617", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : In this study, we address the interesting task of classifying historical texts by their assumed period of writ-ing. This task is useful in digital humanity studies where many texts have unidentified publication dates.For years, the typical approach for temporal text classification was supervised using machine-learningalgorithms.  These algorithms require careful feature engineering and considerable domain expertise todesign a feature extractor to transform the raw text into a feature vector from which the classifier couldlearn to classify any unseen valid input.  Recently, deep learning has produced extremely promising re-sults for various tasks in natural language processing (NLP). The primary advantage of deep learning isthat human engineers did not design the feature layers, but the features were extrapolated from data witha general-purpose learning procedure. We investigated deep learning models for period classification ofhistorical texts. We compared three common models: paragraph vectors, convolutional neural networks (CNN) and recurrent neural networks (RNN), and conventional machine-learning methods. We demon-strate that the CNN and RNN models outperformed the paragraph vector model and the conventionalsupervised machine-learning algorithms.  In addition, we constructed word embeddings for each timeperiod and analyzed semantic changes of word meanings over time.                    </div>", "article_title": "\n                Deep Learning for Period Classification of Historical Hebrew Texts            ", "authors": [{"given": "Chaya Liebeskind", "family": null, "affiliations": ["\n                            JCT - Jerusalem College of Technology                        "]}, {"given": "Shmuel Liebeskind", "family": null, "affiliations": null}], "publisher": null, "date": "2020/06/01", "keywords": ["Machine Learning ; Deep Learning ; Diachronic Corpus ; Period Classification"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "2020", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1808.10685v3", "identifier": {"string_id": "1808.10685", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  Open-ended survey data constitute an important basis in research as well as\nfor making business decisions. Collecting and manually analysing free-text\nsurvey data is generally more costly than collecting and analysing survey data\nconsisting of answers to multiple-choice questions. Yet free-text data allow\nfor new content to be expressed beyond predefined categories and are a very\nvaluable source of new insights into people's opinions. At the same time,\nsurveys always make ontological assumptions about the nature of the entities\nthat are researched, and this has vital ethical consequences. Human\ninterpretations and opinions can only be properly ascertained in their richness\nusing textual data sources; if these sources are analyzed appropriately, the\nessential linguistic nature of humans and social entities is safeguarded.\nNatural Language Processing (NLP) offers possibilities for meeting this ethical\nbusiness challenge by automating the analysis of natural language and thus\nallowing for insightful investigations of human judgements. We present a\ncomputational pipeline for analysing large amounts of responses to open-ended\nquestions in surveys and extract keywords that appropriately represent people's\nopinions. This pipeline addresses the need to perform such tasks outside the\nscope of both commercial software and bespoke analysis, exceeds the performance\nto state-of-the-art systems, and performs this task in a transparent way that\nallows for scrutinising and exposing potential biases in the analysis.\nFollowing the principle of Open Data Science, our code is open-source and\ngeneralizable to other datasets.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Extracting Keywords from Open-Ended Business Survey Questions</h1>", "authors": [{"given": "McGillivray", "family": "Barbara", "affiliations": null}, {"given": "Jenset", "family": "Gard", "affiliations": null}, {"given": "Heil", "family": "Dominik", "affiliations": null}], "publisher": null, "date": "2018/08/31", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-02154122v2", "identifier": {"string_id": "hal-02154122", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : Tokenization of modern and old Western European languages seems to be fairly simple, as it stands on the presence mostly of markers such as spaces and punctuation. However, when dealing with old sources like manuscripts written in scripta continua, antiquity epigraphy or Middle Age manuscripts, (1) such markers are mostly absent, (2) spelling variation and rich morphology make dictionary based approaches difficult. Applying convolutional encoding to characters followed by linear categorization to word-boundary or in-word-sequence is shown to be effective at tokenizing such inputs. Additionally, the software is released with a simple interface for tokenizing a corpus or generating a training set.                    </div>", "article_title": "\n                                    ", "authors": [{"given": "Thibault Clérice", "family": null, "affiliations": ["\n                            PSL - Université Paris sciences et lettres                        ", "\n                            ENC - École nationale des chartes                        ", "\n                            CJM - Centre Jean Mabillon                        ", "\n                            HiSoMA - Histoire et Sources des Mondes antiques                        "]}], "publisher": null, "date": "2020/04/05", "keywords": ["convolutional network ; scripta continua ; tokenization ; Old French ; word segmentation"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "2020", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1807.04892v4", "identifier": {"string_id": "1807.04892", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  In the past few years, computer vision and pattern recognition systems have\nbeen becoming increasingly more powerful, expanding the range of automatic\ntasks enabled by machine vision. Here we show that computer analysis of\nbuilding images can perform quantitative analysis of architecture, and quantify\nsimilarities between city architectural styles in a quantitative fashion.\nImages of buildings from 18 cities and three countries were acquired using\nGoogle StreetView, and were used to train a machine vision system to\nautomatically identify the location of the imaged building based on the image\nvisual content. Experimental results show that the automatic computer analysis\ncan automatically identify the geographical location of the StreetView image.\nMore importantly, the algorithm was able to group the cities and countries and\nprovide a phylogeny of the similarities between architectural styles as\ncaptured by StreetView images. These results demonstrate that computer vision\nand pattern recognition algorithms can perform the complex cognitive task of\nanalyzing images of buildings, and can be used to measure and quantify visual\nsimilarities and differences between different styles of architectures. This\nexperiment provides a new paradigm for studying architecture, based on a\nquantitative approach that can enhance the traditional manual observation and\nanalysis. The source code used for the analysis is open and publicly available.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Computer Analysis of Architecture Using Automatic Image Understanding</h1>", "authors": [{"given": "Wei", "family": "Fan", "affiliations": null}, {"given": "Li", "family": "Yuan", "affiliations": null}, {"given": "Shamir", "family": "Lior", "affiliations": null}], "publisher": null, "date": "2018/07/13", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/2001.01863v7", "identifier": {"string_id": "2001.01863", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  The goal of this work is to build a classifier that can identify text\ncomplexity within the context of teaching reading to English as a Second\nLanguage (ESL) learners. To present language learners with texts that are\nsuitable to their level of English, a set of features that can describe the\nphonological, morphological, lexical, syntactic, discursive, and psychological\ncomplexity of a given text were identified. Using a corpus of 6171 texts, which\nhad already been classified into three different levels of difficulty by ESL\nexperts, different experiments were conducted with five machine learning\nalgorithms. The results showed that the adopted linguistic features provide a\ngood overall classification performance (F-Score = 0.97). A scalability\nevaluation was conducted to test if such a classifier could be used within real\napplications, where it can be, for example, plugged into a search engine or a\nweb-scraping module. In this evaluation, the texts in the test set are not only\ndifferent from those from the training set but also of different types (ESL\ntexts vs. children reading texts). Although the overall performance of the\nclassifier decreased significantly (F-Score = 0.65), the confusion matrix shows\nthat most of the classification errors are between the classes two and three\n(the middle-level classes) and that the system has a robust performance in\ncategorizing texts of class one and four. This behavior can be explained by the\ndifference in classification criteria between the two corpora. Hence, the\nobserved results confirm the usability of such a classifier within a real-world\napplication.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>Text Complexity Classification Based on Linguistic Information: Application to Intelligent Tutoring of ESL</h1>", "authors": [{"given": "Kurdi", "family": "M. Zakaria", "affiliations": null}], "publisher": null, "date": "2020/01/07", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://hal.archives-ouvertes.fr/hal-01981922v3", "identifier": {"string_id": "hal-01981922", "id_scheme": "hal"}, "abstract": "<div class=\"abstract-content\">\n                        <strong>Abstract</strong> : This paper addresses the integration of a Named Entity Recognition and Disambiguation (NERD) service within a group of open access (OA) publishing digital platforms and considers its potential impact on both research and scholarly publishing. The software powering this service, called entity-fishing, was initially developed by Inria in the context of the EU FP7 project CENDARI and provides automatic entity recognition and disambiguation using the Wikipedia and Wikidata data sets. The application is distributed with an open-source licence, and it has been deployed as a web service in DARIAH's infrastructure hosted by the French HumaNum. In the paper, we focus on the specific issues related to its integration on five OA platforms specialized in the publication of scholarly monographs in the social sciences and humanities (SSH), as part of the work carried out within the EU H2020 project HIRMEOS (High Integration of Research Monographs in the European Open Science infrastructure). In the first section, we give a brief overview of the current status and evolution of OA publications, considering specifically the challenges that OA monographs are encountering. In the second part, we show how the HIRMEOS project aims to face these challenges by optimizing five OA digital platforms for the publication of monographs from the SSH and ensuring their interoperability. In sections three and four we give a comprehensive description of the entity-fishing service, focusing on its concrete applications in real use cases together with some further possible ideas on how to exploit the annotations generated. We show that entity-fishing annotations can improve both research and publishing process. In the last chapter, we briefly present further possible application scenarios that could be made available through infrastructural projects.                    </div>", "article_title": "\n                Leveraging Concepts in Open Access Publications            ", "authors": [{"given": "Andrea Bertino", "family": null, "affiliations": ["\n                            SUB Göttingen - Göttingen State and University Library                        "]}, {"given": "Luca Foppiano", "family": null, "affiliations": ["\n                            ALMAnaCH - Automatic Language Modelling and ANAlysis & Computational Humanities                        "]}, {"given": "Laurent Romary", "family": null, "affiliations": ["\n                            ALMAnaCH - Automatic Language Modelling and ANAlysis & Computational Humanities                        "]}, {"given": "Pierre Mounier", "family": null, "affiliations": ["\n                            CLEO - Centre pour l'édition électronique ouverte                        ", "\n                            EHESS - École des hautes études en sciences sociales                        "]}], "publisher": null, "date": "2019/03/25", "keywords": ["Open Access ; Named Entity Recognition and Disambiguation (NERD) ; Entity-Fishing ; Monographs ; Digital Publishing Platforms"], "journal_title": "Journal of Data Mining & Digital Humanities", "volume": "2019", "issue": null, "ISSN": [{"value": null, "type": null}]},
{"url": "https://arxiv.org/abs/1801.00912v3", "identifier": {"string_id": "1801.00912", "id_scheme": "arXiv"}, "abstract": "<blockquote class=\"abstract mathjax\">\n      <span class=\"descriptor\">Abstract:</span>  With the rapid evolution of cross-strait situation, \"Mainland China\" as a\nsubject of social science study has evoked the voice of \"Rethinking China\nStudy\" among intelligentsia recently. This essay tried to apply an automatic\ncontent analysis tool (CATAR) to the journal \"Mainland China Studies\"\n(1998-2015) in order to observe the research trends based on the clustering of\ntext from the title and abstract of each paper in the journal. The results\nshowed that the 473 articles published by the journal were clustered into seven\nsalient topics. From the publication number of each topic over time (including\n\"volume of publications\", \"percentage of publications\"), there are two major\ntopics of this journal while other topics varied over time widely. The\ncontribution of this study includes: 1. We could group each \"independent\" study\ninto a meaningful topic, as a small scale experiment verified that this topic\nclustering is feasible. 2. This essay reveals the salient research topics and\ntheir trends for the Taiwan journal \"Mainland China Studies\". 3. Various\ntopical keywords were identified, providing easy access to the past study. 4.\nThe yearly trends of the identified topics could be viewed as signature of\nfuture research directions.\n\n    </blockquote>", "article_title": "<h1 class=\"title mathjax\"><span class=\"descriptor\">Title:</span>How the Taiwanese Do China Studies: Applications of Text Mining</h1>", "authors": [{"given": "Shao", "family": "Hsuan-Lei", "affiliations": null}, {"given": "Huang", "family": "Sieh-Chuen", "affiliations": null}, {"given": "Tsai", "family": "Yun-Cheng", "affiliations": null}], "publisher": null, "date": "2018/01/03", "keywords": null, "journal_title": "Journal of Data Mining & Digital Humanities", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}
]