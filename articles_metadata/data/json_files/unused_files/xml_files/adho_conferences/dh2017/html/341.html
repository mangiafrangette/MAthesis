<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN"
	"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head><meta http-equiv="content-type" content="text/html; charset=UTF-8"/><meta name="generator" content="ABBYY FineReader 14"/><title>Microsoft Word - 341. Eder-Short Samples in Authorship Attribution-341.docx</title><link rel="stylesheet" href="341_files/341.css" type="text/css"/>
</head>
<body><h1><a name="caption1"></a><a name="bookmark0"></a><span class="font4" style="font-weight:bold;">Short samples in authorship attribution: a new approach</span></h1>
<p><span class="font6" style="font-weight:bold;">Maciej Eder</span></p>
<p><span class="font6"><a href="mailto:maciejeder@gmail.com">maciejeder@gmail.com</a></span></p>
<p><span class="font6">Institute of Polish Language</span></p>
<p><span class="font6">Polish Academy of Sciences, Poland</span></p><h2><a name="bookmark1"></a><span class="font3" style="font-weight:bold;">Introduction</span></h2>
<p><span class="font6">The question of minimal sample size is one of the most important issues in stylometry and nontraditional authorship attribution. In the last decade or&nbsp;so, a few studies concerning different aspects of&nbsp;scalability in stylometry have been published (Zhao&nbsp;and Zobel, 2005; Hirst and Feiguina, 2007; Stamatatos,&nbsp;2008; Koppel et al., 2009; Mikros, 2009; Luyckx and&nbsp;Daelemans, 2011), but the question has not been&nbsp;answered comprehensively. In his recent study, Eder&nbsp;proposed a systematic approach to solve the problem&nbsp;in a series of experiments, claiming that a sample&nbsp;should have at least 5,000 running words to be&nbsp;attributable (Eder, 2015).</span></p>
<p><span class="font6">The above studies (and many other as well) tacitly assume that there exists a certain amount of linguistic&nbsp;data that allows for reliable authorial recognition, and&nbsp;the real problem at stake is to determine that very&nbsp;value. However, one can assume that the authorial&nbsp;fingerprint is not distributed evenly in a collection of&nbsp;texts. Just the contrary, many experiments seem to&nbsp;suggest that the authorial voice is sometimes&nbsp;overshadowed by other signals, such as genre, gender,&nbsp;chronology, or translation. Some authors, say&nbsp;Chandler, should be easily attributable, while some&nbsp;other authors, say Virginia Woolf, will probably have&nbsp;their fingerprint somewhat hidden. Moreover,&nbsp;authorship attribution is ultimately a matter of&nbsp;context: telling apart Hemingway and Dickens will&nbsp;always be easier than distinguishing the Bronte sisters.&nbsp;On theoretical grounds, then, the minimal sample size&nbsp;can not be determined once and forever for the entire&nbsp;corpus, but may be different for different texts in the&nbsp;corpus.</span></p><h2><a name="bookmark2"></a><span class="font3" style="font-weight:bold;">Method and Data</span></h2>
<p><span class="font6">To scrutinize the above intuition, a controlled experiment has been designed, in which particular text&nbsp;samples were assessed independently (one by one)&nbsp;and compared against the corpus. A following&nbsp;procedure was applied: the entire corpus served as a&nbsp;training set, out of which one text at a time was&nbsp;excluded. This temporarily excluded text was further&nbsp;pre-processed: in many iterations, longer and longer&nbsp;samples of randomly chosen words were excerpted&nbsp;(100 independent samples in each iteration), and then&nbsp;tested against the training set. In each iteration, the&nbsp;total number of correctly “guessed” authorial classes -a single value between 0 and 100 - was recorded,&nbsp;resulting in a row of accuracy scores for a given text as&nbsp;a function of its sample size. The same procedure was&nbsp;repeated for each text in the corpus. The above setup&nbsp;does not need to be supplemented by any crossvalidation, because the experiment itself is a variant of&nbsp;a leave-one-out cross-validation scenario. Moreover,&nbsp;each text is re-sampled several times, which can be&nbsp;perceived as an additional way of neutralizing&nbsp;potential model overfitting.</span></p>
<p><span class="font6">The experiments were repeated a few times. Firstly, three different classification methods have been&nbsp;tested: Support Vector Machines (SVM), Nearest&nbsp;Shrunken Centroids (NSC), and a distance-based&nbsp;learner that is routinely used in authorship attribution&nbsp;tests, namely Burrows’s Delta (Burrows, 2002).&nbsp;However, Delta was used as a general classification&nbsp;framework combined with a few custom kernels that&nbsp;seem to outperform the original setup. These included&nbsp;Cosine Delta (Evert et al., 2016), min-max measure&nbsp;(Kestemont et al., 2016), Eder’s Delta (Eder et al.,&nbsp;2016), and, obviously, the original measure as&nbsp;introduced by Burrows and mathematically justified&nbsp;by Argamon (2011). Secondly, all the tests have been&nbsp;repeated for different vectors of input features, or&nbsp;most frequent words: 100, 200, 300, 500, 750 and&nbsp;1,000. While the choice of the vectors’ lengths was&nbsp;arbitrary, it was aimed to follow usual stylometric&nbsp;scenarios in their various flavors, ranging from a&nbsp;considerably short list of mostly frequent words, to a&nbsp;longish vectors overwhelmed by content words.</span></p>
<p><span class="font6">The aforementioned method of testing was applied into two roughly similar corpora (one at a time): a&nbsp;corpus of 100 English novels by 33 authors (male and&nbsp;female), covering the years 1840-1940, and a similar&nbsp;corpus of 100 Polish novels. Both corpora, referred to&nbsp;as the Benchmark Corpus of English and the&nbsp;Benchmark Corpus of Polish, have been compiled by&nbsp;Jan Rybicki (pers. comm.). The corpora used in the&nbsp;experiment, as well as the complete code needed to&nbsp;replicate the study, will be available in a </span><span class="font6" style="text-decoration:underline;">GitHub&nbsp;repository</span><span class="font6">.</span></p><div>
<p><span class="font3" style="font-weight:bold;">Results</span></p></div><br clear="all"/>
<p><span class="font6">A lion’s share of tested samples revealed a very consistent and clear picture. According to intuition, the&nbsp;performance for short samples falls far beyond any&nbsp;acceptance rate, sometimes showing no correct&nbsp;“guesses” at all. This is followed, however, by a very&nbsp;steep increase of performance which immediately&nbsp;turns into a plateau of statistical saturation, despite&nbsp;the number of analyzed features (frequent words). An&nbsp;example of such a behavior is </span><span class="font6" style="font-style:italic;">The Ambassadors</span><span class="font6"> by&nbsp;Henry James (Figure 1), as well as many other novels&nbsp;by Blackmore, Chesterton, Foster, Lytton, Meredith,&nbsp;Morris, Thackeray, and Trollope. As one can see, the&nbsp;amount of text needed for a reliable attribution is less&nbsp;than 2,000 words (!), an amount radically smaller than&nbsp;the previous study suggests (Eder, 2015). Sometimes&nbsp;the picture is somewhat blurry, nevertheless the same&nbsp;general shape reappears, as in the case of </span><span class="font6" style="font-style:italic;">Felix Holt</span><span class="font6"> by&nbsp;George Elliot (Figure 2). As one can see, using shorter&nbsp;vectors of features requires longer samples to extract&nbsp;the authorial profile.</span></p><div>
<p><span class="font1" style="font-weight:bold;">James_Ambassadors_1903</span></p><img src="341_files/341-1.jpg" style="width:225pt;height:189pt;"/>
<p><span class="font0" style="font-weight:bold;">sample size</span></p>
<p><span class="font2">Figure 1: </span><span class="font2" style="font-style:italic;">The Ambassadors</span><span class="font2"> by Henry James contrasted against a corpus of 100 English novels: the attribution&nbsp;accuracy as a function of sample size (in words). Colors&nbsp;represent the results for different vectors of MFWs: 100</span></p>
<p><span class="font2">(red), 200 (yellow), 300 (green), 500 (cyan), 750 (blue), and 1,000 (violet).</span></p></div><br clear="all"/><div><img src="341_files/341-2.jpg" style="width:242pt;height:242pt;"/>
<p><span class="font2">Figure 2: </span><span class="font2" style="font-style:italic;">Felix Holt</span><span class="font2"> by George Eliot: the dependence of authorship recognition and sample size.</span></p></div><br clear="all"/>
<p><span class="font6">Optimistic as they are, however, the results might differ significantly. E.g., in some cases, the statistical&nbsp;saturation does not really take place, even if very long&nbsp;samples are used (Figure 3: scores for </span><span class="font6" style="font-style:italic;">Saints Progress&nbsp;</span><span class="font6">by John Galsworthy). What is more important,&nbsp;however, the final results additionally depend on the&nbsp;number of analyzed features. In Figure 4, a&nbsp;representative example of this behavior has been&nbsp;shown, namely </span><span class="font6" style="font-style:italic;">Bleak House</span><span class="font6"> by Dickens.</span></p><div><img src="341_files/341-3.jpg" style="width:225pt;height:221pt;"/>
<p><span class="font2">Figure 3: </span><span class="font2" style="font-style:italic;">Saints Progress</span><span class="font2"> by John Galsworthy: the dependence of authorship recognition and sample size.</span></p></div><br clear="all"/><div>
<p><span class="font1" style="font-weight:bold;">Stevenson_Catriona_1893</span></p><img src="341_files/341-4.jpg" style="width:225pt;height:204pt;"/>
<p><span class="font2">Figure 5: </span><span class="font2" style="font-style:italic;">Catriona</span><span class="font2"> by Robert Louis Stevenson: the dependence of authorship recognition and sample size.</span></p></div><br clear="all"/><div><img src="341_files/341-5.jpg" style="width:242pt;height:241pt;"/>
<p><span class="font2">Figure 4: </span><span class="font2" style="font-style:italic;">Bleak House</span><span class="font2"> by Charles Dickens: the dependence of authorship recognition and sample size.</span></p></div><br clear="all"/>
<p><span class="font6">Last but definitely not least, there are a few texts that are never correctly attributed, no matter how long&nbsp;the extracted samples are (Figure 5). The question&nbsp;why some novels were misclassified will be addressed&nbsp;in a separate study. Here, it should be emphasized that&nbsp;such a behavior is unpredictable. Certainly, it can be&nbsp;easily detected, as long as one tests novels of known&nbsp;authorship; it becomes an obstacle, however, when&nbsp;one tries to scrutinize an anonymous text.</span></p><h2><a name="bookmark3"></a><span class="font3" style="font-weight:bold;">Detecting Outliers</span></h2>
<p><span class="font6">The outcome of the above experiment shows that the minimal sample size can be lowered substantially,&nbsp;from ca. 5,000 running words as suggested previously&nbsp;(Eder, 2015), to less than 2,000 words. However, this is&nbsp;true only for those texts that exhibit a clear authorial&nbsp;signal; otherwise the risk of severe misclassification&nbsp;appears. To take advantage of the above results, then,&nbsp;one has to be sure which category an analyzed text&nbsp;belongs to. In a controlled experiment, the task is&nbsp;simple, in a real-case attribution study, however, one&nbsp;has no chance to fine-tune the model by testing the&nbsp;disputed sample against the corpus. What if an&nbsp;anonymous text does not reveal a clear accuracy curve,&nbsp;as the one in Figure 1?</span></p>
<p><span class="font6">To overcome the sample size issue of unknown texts, an additional measure can be involved to&nbsp;supplement the accuracy scores. (Due to limited space&nbsp;in this abstract, a compact outline of the proposed</span></p>
<p><span class="font6">solution will be presented, rather than a complete algorithm). In the case of misclassification, one would&nbsp;like to know if the wrong response is consistent, or if&nbsp;different classes were assigned chaotically. To address&nbsp;this question, an indicator of consistency would be&nbsp;useful. The Simpson index is a very simple measure of&nbsp;concentration when observations are classified into a&nbsp;certain number of types (Simpson, 1949):</span></p>
<p><span class="font6">A = Spi2</span></p>
<p><span class="font6">where pi is the proportion of observations belonging to the ith type. The index can be easily adopted to&nbsp;indicate imbalance between assigned classes in&nbsp;supervised classification. To this end, the obtained&nbsp;classification scores (for a given sample size) have to&nbsp;be divided by the total number of trials (in this case,&nbsp;100). The value 1 reflects purely consistent results,&nbsp;lower values mean that the assigned classes were&nbsp;fuzzy.</span></p><img src="341_files/341-6.jpg" style="width:242pt;height:241pt;"/>
<p><span class="font2">Figure 6: Diversity scores (Simpson index) as a function of sample size.</span></p>
<p><span class="font6">To make a long story short: the texts that distribute their accuracy curves as in Figure 1 will also exhibit the&nbsp;same shape of the diversity index (see Figure 6).&nbsp;However, when the accuracy scores are low and/or&nbsp;ambiguous, the diversity index might provide a&nbsp;priceless hint. It is especially important when the&nbsp;accuracy scores are consistent (Figure 5), and the&nbsp;Simpson index is not (Figure 7). Instead of being&nbsp;mislead (“Stevenson did not write Catriona”, which is&nbsp;not true), we are warned that the classification is&nbsp;inconsistent. Thus, to reliably test a minimal size of a&nbsp;disputed text, one has to take into account two values&nbsp;(accuracy and diversity). The bigger the dispersion&nbsp;between the indices, the smaller the probability that&nbsp;the text is attributable - perhaps a longer sample has&nbsp;to be involved, or a different set of features?&nbsp;</span><span class="font3" style="font-weight:bold;">Conclusion</span></p>
<p><span class="font6">The study was aimed at re-considering the minimum sample size for reliable authorship&nbsp;attribution. The results of the experiments suggest&nbsp;that a sufficient amount of textual data may be as little&nbsp;as 2,000 words in many cases. However, sometimes the&nbsp;authorial fingerprint is so vague, that one needs to use&nbsp;substantially longer samples to make the attribution&nbsp;feasible. A question of some importance is to which&nbsp;category an unknown (disputed) text belongs.</span></p><h2><a name="bookmark4"></a><span class="font3" style="font-weight:bold;">Bibliography</span></h2>
<p><span class="font5" style="font-weight:bold;">Argamon, S. </span><span class="font5">(2011). Interpreting Burrows’s delta: geometric and probabilistic foundations. </span><span class="font5" style="font-style:italic;">Literary and&nbsp;Linguistic Computing,</span><span class="font5"> 23(2): 131-47.</span></p>
<p><span class="font5" style="font-weight:bold;">Burrows, J. </span><span class="font5">(2002). ‘Delta’: a measure of stylistic difference and a guide to likely authorship. </span><span class="font5" style="font-style:italic;">Literary and Linguistic&nbsp;Computing,</span><span class="font5"> 17(3): 267-87.</span></p>
<p><span class="font5" style="font-weight:bold;">Eder, M. </span><span class="font5">(2015). Does size matter? Authorship attribution, small samples, big problem. </span><span class="font5" style="font-style:italic;">Digital Scholarship in the&nbsp;Humanities,</span><span class="font5"> 30(2): 167-82.</span></p>
<p><span class="font5" style="font-weight:bold;">Eder, M., Rybicki, J. and Kestemont, M. </span><span class="font5">(2016). Stylometry with R: a package for computational text analysis. </span><span class="font5" style="font-style:italic;">R&nbsp;Journal,</span><span class="font5"> 8(1): 107-21.</span></p>
<p><span class="font5" style="font-weight:bold;">Evert, S., Jannidis, F., Proisl, T., Thorsten, V., Schoch, C., Pielstrom, S. and Reger, I. </span><span class="font5">(2016). Outliers or key&nbsp;profiles? Understanding distance measures for&nbsp;authorship attribution. </span><span class="font5" style="font-style:italic;">Digital Humanities 2016:&nbsp;Conference Abstracts.</span><span class="font5"> Krakow: Jagiellonian University &amp;&nbsp;Pedagogical University, pp. 188-91.</span></p>
<p><span class="font5" style="font-weight:bold;">Hirst, G. and Feiguina, O. </span><span class="font5">(2007). Bigrams of syntactic labels for authorship discrimination of short texts.&nbsp;</span><span class="font5" style="font-style:italic;">Literary and Linguistic Computing,</span><span class="font5"> 22(4): 405-17.</span></p>
<p><span class="font5" style="font-weight:bold;">Kestemont, M., Stover, J., Koppel, M., Karsdorp, F. and Daelemans, W. </span><span class="font5">(2016). Authenticating the writings of&nbsp;Julius Caesar. </span><span class="font5" style="font-style:italic;">Expert Systems With Applications,</span><span class="font5"> 63: 8696.</span></p>
<p><span class="font5" style="font-weight:bold;">Koppel, M., Schler, J. and Argamon, S</span><span class="font5">. (2009). Computational methods in authorship attribution.</span></p>
<p><span class="font5" style="font-style:italic;">Journal of the American Society for Information Science and Technology,</span><span class="font5"> 60(1): 9-26.</span></p>
<p><span class="font5" style="font-weight:bold;">Luyckx, K. and Daelemans, W. </span><span class="font5">(2011). The effect of author set size and data size in authorship attribution. </span><span class="font5" style="font-style:italic;">Literary&nbsp;and Linguistic Computing,</span><span class="font5"> 26(1): 35-55.</span></p>
<p><span class="font5" style="font-weight:bold;">Mikros, G. K. </span><span class="font5">(2009). Content words in authorship attribution: an evaluation of stylometric features in a&nbsp;literary corpus. In Kohler, R. (ed), </span><span class="font5" style="font-style:italic;">Studies in Quantitative&nbsp;Linguistics,</span><span class="font5"> vol. 5. Ludenscheid: RAM, pp. 61-75.</span></p>
<p><span class="font5" style="font-weight:bold;">Rybicki, J. and Eder, M. </span><span class="font5">(2011). Deeper Delta across genres and languages: do we really need the most frequent&nbsp;words?. </span><span class="font5" style="font-style:italic;">Literary and Linguistic Computing,</span><span class="font5"> 26(3): 31521.</span></p>
<p><span class="font5" style="font-weight:bold;">Simpson, E. H. </span><span class="font5">(1949). Measurement of diversity. </span><span class="font5" style="font-style:italic;">Nature, </span><span class="font5">163: 688.</span></p>
<p><span class="font5" style="font-weight:bold;">Stamatatos, E</span><span class="font5">. (2008). Author identification: Using text sampling to handle the class imbalance problem.&nbsp;</span><span class="font5" style="font-style:italic;">Information Processing and Management,</span><span class="font5"> 44(2): 790-99.</span></p>
<p><span class="font5" style="font-weight:bold;">Zhao, Y. and Zobel, J. </span><span class="font5">(2005). Effective and scalable authorship attribution using function words.&nbsp;</span><span class="font5" style="font-style:italic;">Proceedings of the Second Asia Conference on Asia&nbsp;Information Retrieval Technology.</span><span class="font5"> (AIRS’05). Berlin,&nbsp;Heidelberg: Springer-Verlag, pp. 174-89.</span></p>
</body>
</html>