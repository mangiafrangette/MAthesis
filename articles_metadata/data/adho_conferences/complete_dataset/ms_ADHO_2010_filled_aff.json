[
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The Text Encoding Initiative’s (TEI) Extramural Journal (EJ) project was conceived early in 2009 when the conveners of the TEI Education Special Interest Group (SIG) proposed, as a matter of urgency, the development of an online publishing suite to address the shortage of TEI educational resources.Two of the notable TEI teaching resources which are available include the Women Writers Project’s NEH-funded series of “Advanced Seminars on Scholarly Text Encoding” (see <http://www.wwp.brown.edu/encoding/seminars/neh_advanced.html>) and TEI by Example (see <http://www.kantl.be/ctb/project/2006/tei-ex.htm> and Terras et al.). Following approval by the TEI Board and Council and the receipt of a small SIG grant in support of the project, TEI-EJ advanced into development. Because TEI-EJ is being researched and developed in an era where “print is no longer the exclusive or the normative medium in which knowledge is produced and/or disseminated” (“A Digital Humanities Manifesto”) and where electronic publication is increasingly common (see Waltham; Maron and Smith; Willett), it is crucial to point out that typologically, TEI-EJ is positioned outside of two disparate points on the web publishing continuum, media-driven journals which are designed primarily for the publication of media-driven content (e.g. Vectors Journal <http://www.vectorsjournal.org/>, Southern Spaces <http://www.southernspaces.org/>; also see Toton and Martin) and text-driven journals which are designed to reproduce the print journal model in a web publication (e.g. Journal of Writing Research <>, International Journal of Teaching and Learning in Higher Education <http://www.isetl.org/ijtlhe/>). Although TEI-EJ’s ‘journal’ designation is suggestive of a single aim, from the outset, project objectives have been defined as both experimental and extramural, and TEI-EJ has been envisaged not only as a publishing venue but also as a community-driven online forum that offers members of the TEI, whether novice or expert, as well as the broader DH community new educational insights into the TEI. Significantly, the steps being taken to achieve these objectives contribute to a newly emerging body of scholarship which explores the development of new digital environments and which defines a new genre in academic publishing.   The first stage of the project has been the development of TEI-EJ as a born digital, open access, peer reviewed scholarly journal where communicative modes are bidirectional rather than exclusively unidirectional, articles are media-driven (including video, audio, and image) as well as text-driven, and where the aims of publication extend beyond print journal mimesis to include education and community building. Given the hybrid nature of the project’s goals (publishing and learning community; see Dal Fiore, Koku and Wellman) and the novel design for implementation, TEI-EJ’s site infrastructure (see Fig. 1) was designed to be extensible, capable of managing text articles (e.g. TEI-XML as well as formats such as .txt and .doc which are converted to TEI-XML), multimedia articles (e.g. video, audio, image), moderated responses to articles, and community-driven communications (e.g. forum and blog) (see Fig. 2). This is achieved through Drupal,For a good, comparative discussion of content management systems, see “Comparing Open Source Content Management Systems: WordPress, Joomla, Drupal, and Plone,” available at <http://idealware.org/comparing_os_cms/>. TEI-XML content is being handled by the Drupal XML Content module, and the journal’s publishing workflow is being handled by the Drupal E- Journal module. a customizable, open source content management system, which facilitates the social media as well as the publishing and educational aspects of the project.A preview of the publishing website and a fully functional mock journal issue are being presented at the TEI Members’ Meeting in November 2009.   Fig. 1. Schlitz TEI-EJ project planning 'mindmap'      Fig. 2. Screenshot of TEI-EJ website      This paper will introduce the TEI-EJ project, describing the why and how of the key theoretical, technological and editorial decisions that drove development as we advanced from theory into practice. In doing so, it aims to establish the project as a new model for academic publishing which is designed to harness emerging technologies, to leverage the fact that “Open access is changing the public and scholarly presence of the research article” (Willinsky), to promote learning objectives beside dissemination of scholarship, and to elevate the role of reader/end-user to the position of chief stakeholder.  ",
        "article_title": "The TEI's Extramural Journal Project: Exploring New Digital Environments and Defining a New Genre in Academic Publishing",
        "authors": [
            {
                "given": " Stephanie A.",
                "family": "Schlitz",
                "affiliation": [
                    {
                        "original_name": "Bloomsburg UniversityUSA",
                        "normalized_name": "Bloomsburg University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/007dga614",
                            "GRID": "grid.253165.6"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-03-18",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In 1997, at the ACH-ALLC'97 conference at Queen's University, there was a session presented by R. Harald Baayen, David I. Holmes, Joe Rudman, and Fiona J. Tweedie, \"The State of Authorship Attribution Studies: (1) The History and the Scope; (2) The Problems – Towards Credibility and Validity.\" Thirteen years have passed and well over 600 studies and papers dealing with non-traditional authorship attribution have been promulgated since that session.  This paper looks back at that session, a subsequent article published by Rudman in Computers and the Humanities, \"The State of Authorship Attribution Studies: Some Problems and Solutions,\" and the more than 600 new publications. There are still major problems in the “science” on non-traditional authorship attribution. This paper goes on to assess the present state of the field – its successes, failures, and prospects.   Successes It has been an exciting thirteen years with many advances. Each of the following (not a complete list) will be discussed:  Arguably, the most significant development in the field is the large contingent of computer scientists that have brought their perspectives to the table – led by Shlomo Argamon, Moshe Kopple, and a host of others.  The Dimacs Working Group on Developing Community. Sir Brian Vickers' London Authorship Forum. John Burrows' Busa Award. Forensic Linguistics.  Successful studies such as Foster's  Primary Colors work. The continuing advances of practitioners such as John Burrows, David Hoover, Matthew Jockers, David Holmes, and others.  John Nerbonne's reissue of Mosteller and Wallace's Applied Bayesian and Classical Inference: The Case of \"The Federalist Papers.\"  Patrick Juola's \"Ad Hoc Authorship Attribution Competition.\" and his NSF funded JGAAP project.  The PAN Workshops. Uncovering Plagiarism, Authorship, and Social Software Misuse.      Acceptance Contrary to what many practitioners of the non-traditional proclaim, there is not wide-spread acceptance of the field.  There have been many high profile problems with the concomitant negative publicity, e.g.:  Foster's misattribution of A Funerall Elegie Foster's misattribution of the Jon Benét ransom note Burrows' attribution then de-attribution of “A Vision” The continuing bashing of Morton's CUSUM   Burrows' shift is something that every good scientist should do – search for errors or improvements in their experimental methodology and self correct.   Failures and Shortcomings After thirteen years of increasing activity, there is still no consensus as to correct methodology or technique. Most authorship studies are still governed by expediency, e.g.:  The texts are not the correct ones but they were available The controls are not complete but it would have taken too long to obtain the correct ones    The “umbrella” problem remains – most non-traditional authorship practitioners do not understand what constitutes a valid study.  Problems in the following areas will be explicated and solutions proposed:  Knowledge of the Field (i.e. the Bibliography) – The fact that there have been so many authorship studies is good -- the fact that they have been published in over 90 different journals makes a complete literature search time consuming and difficult which is not good. To make things even more difficult, add to this the more than 14 books, 22 chapters in books, the 80 conference papers, the 10 reports, 22 dissertations, 9 newspaper articles, the 10 on-line self published papers, 4 encyclopedia entries.  Reproducibility – verification The Experimental Plan The Primary Data – This is a major problem that is almost universally side-stepped.  Style markers – Function words, n-grams, etc.  Cross Validation – necessary but not sufficient The Control Groups – Genre, gender, time frame, etc.  The Statistics – A range of techniques will be discussed – e.g. Neural Nets, SVM's, Sequence Kernals, Nave Bayes  The Presentation – visualization     Conclusion In conclusion, there is a discussion of our role as gatekeepers:  Rudman's caution that attribution studies on the Historia Augusta are an exercise in futility. Hoover and Argamon's modification and clarification of Burrows' Delta. Rudman's “Ripost” of Burrows' “History of Ophelia.” Should we oppose patents such as Chaski's? The Daubert triangle.    ",
        "article_title": "The State of Non-Traditional Authorship Attribution Studies – 2010: Some Problems and Solutions ",
        "authors": [
            {
                "given": " Joseph",
                "family": "Rudman",
                "affiliation": [
                    {
                        "original_name": "Carnegie Mellon University, USA",
                        "normalized_name": "Carnegie Mellon University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05x2bcf33",
                            "GRID": "grid.147455.6"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-05-16",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Digital humanities scholarship has expanded beyond its deep foundations in text analysis to find new meaning and knowledge through the creative reuse of historical photographs and other visual resources. Visual studies scholars in the humanities who wish to work primarily in the digital domain face a fundamental dilemma in the choice either to create “purpose-built” thematic collections tailored to specific studies (Palmer, 2008) or to make use of collections digitized for general purposes by an archives, a library, or other cultural heritage organization. “General-purpose” digital library collections are simultaneously mechanisms for delivering digital surrogates of archival holdings and new archival collections in their own right that reflect the decisions that digital curators make throughout the digitization process (Ross, 2007; Conway, 2008). The research issues associated with the actual use in humanities contexts of these large-scale general-purpose collections of digital images are profound and as yet largely unexplored (Saracevic, 2004). Concluding an important study establishing a typology of use in image retrieval, Graham (2004, p. 324) observes that “these uses do not tell us what was actually done with the images once they had been found.”  This paper reports on a multi-case study of the use of general purpose digitized photographic archives. The paper’s title is a play on John Berger’s somewhat forgotten pre-digital argument in Ways of Seeing that reproductions transform art (including of photographs and other graphical materials) into information, and in doing so expose original material objects to new uses not imagined by either the artist or, especially, the museums and archives that collect these artifacts. “It is not a question of reproduction failing to reproduce certain aspects of an image faithfully; it is a question of reproduction making it possible, even inevitable, that an image will be used for many different purposes and that the reproduced image, unlike the original work, can lend itself to them all” (Berger, 1972, p. 24). In suggesting that the post-modern critique has outlived its usefulness in the arena of visual studies, Mitchell calls for moving beyond Walter Benjamin’s skepticism of the reproduction by embracing digital image surrogacy as superior. “In a world where the very idea of the unique original seems a merely nominal or legal fiction, the copy has every chance of being an improvement or enhancement of whatever counts as the original” (Mitchell, 2003, p. 487). Efforts to extract evidence and meaning from the digitized photographic image extend well beyond the disciplines of art and art history to encompass history, a range of other social sciences, and increasingly the humanities. Humanists with a propensity toward visual studies run the gamut from skepticism to enthusiasm about the processes that digitally transform the material properties of original photographs and camera negatives. Koltun (1999, p. 124) claims that a digitized photograph “leaves behind another originating document whose disposal or retention can inspire other archival debates focused around original attributes and meanings not ‘translated’ into, even distorted by, the new medium.” Sassoon (2004, p. 199) largely sees diminished meaning (“an ephemeral ghost”) through digitization, whereas Cameron (2007, p. 67) projects archival properties onto the “historical digital object” that are distinctive and original. Skeptics and enthusiasts on both sides of this argument stake their claims with little regard for the actual uses of digitized historical photographs.  This paper exposes varying perspectives on “modes of seeing” by synthesizing case studies of seven deeply experienced researchers both within and outside the academy, ranging from scholars to serious avocational users to people whose livelihood depends on finding and using high quality representations of historical photographs. The group of study participants is broadly (but not statistically) representative of the variety of sophisticated humanities-oriented uses to which general purpose collections are put. The participants in the seven case studies vary widely in terms of demographic characteristics. Three are female; four are male. Their ages range from 30 to 67. The participants work and live east of the Mississippi River in five separate communities. Each case study revolves around a specific tangible product that was in some stage of completeness at the time of the interviews. The form of the products ranged from books and a dissertation, a complex and dynamic website, to a database for a membership organization. For their projects, participants made use of digitized photographs delivered from either the Library of Congress’s American Memory collection or the online catalog of the Prints and Photographs Division. Each of the five collections consulted is discrete within its particular delivery system. The Civil War Photographs collection is available through interfaces to both the American Memory and the PPD databases. A 1872 Turkestan photographic album and photographs from the National Child Labor Committee are available in digital form only through the online catalog. Portions of the extensive Farm Security Administration/Office of War Information collection are distributed through the American Memory interface, but the entire digitized collection is fully available only through the online catalog. Finally, the Bain photograph collection, including a sizable sub-collection on American baseball, is fully available digitally through the online catalog and selectively through the American Memory interface.  The paper frames the findings on the use of digitized photographs in digital humanities scholarship within new theoretical perspectives on visual literacy (Elkins, 2008) and remediation (Bolter and Grusin, 1996), and the practical aspects of imaging for humanities scholarship (Deegan and Tanner, 2002). The case studies are constructed using an innovative multi-method qualitative approach that encompasses archival research in photo archives combined with a two-stage qualitative investigation. Stage one gathers background information and an assessment of expertise from interview subjects. Stage two consists of in-depth, semi-structured, in-situ interviews and observations. A three-part “thinking out loud” protocol extracts extensive commentary on the nature of individual and community expertise, on macro decision making strategies for creating the research product, and the character of micro-decisions on the choice and use of individual photographs. The descriptive evidence of “modes of seeing” is derived from a “grounded theory” analysis of interview transcripts. Using extracted quotations and extensive visual examples, the paper presents an original typological model on the ways that perspectives of users on visual content, archival properties, and technical characteristics of digitized photographic archives combine to produce distinctive, but often intersecting “modes of seeing.” One mode, “Discovering,” takes maximum advantage of the visual detail discernable in high-resolution digital copies of camera negatives to find and contextualize new knowledge. A second mode, “Storytelling,” has a point of departure in the emotion evoked by wholly composed photographic images, seeking hidden narratives surrounding the subjects of the images, much in the way that textual archives yield their stories through the power of provenance. A third mode, “Landscaping,” finds meaning through the geospatial and temporal contexts of the images and the circumstances of their existence, sometimes providing a portal on technologically mediated power relations. All three modes carry either a “materialist” or “anti-materialist” stance that circumscribes the intimacy of original source and digital surrogate. The two stances have much to say about trust, integrity, and the archival nature of digital collections for humanities scholarship. The findings have at least three important implications for the use of general purpose collections of digitized photographs in a digital humanities context. First, the study demonstrates the relationship between the technical characteristics of digitally transformed photographs and the construction of visual narrative. Second, the study exposes how hidden archival properties embedded in the transformed archival photographic record create the context of use for scholars who privilege digital surrogacy over the material nature of original sources. Third, the study’s model of “modes of seeing” diversifies our understanding of how humanists interpret the visual order on, beneath, and beyond the visual plane of the photographic object. FundingThis work was supported by the U. S. National Science Foundation [IIS-0733279]. Ricardo Punzalan provided valuable assistance in conducting the phase-one interviews. ",
        "article_title": "Modes of Seeing: Case Studies on the Use of Digitized Photographic Archives",
        "authors": [
            {
                "given": " Paul",
                "family": "Conway",
                "affiliation": [
                    {
                        "original_name": "University of MichiganUSA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-22",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The application of statistical methods to style is now well accepted in author attribution. It has found less favour in broader stylistic description. Louis Milic’s pioneering quantitative work from the 1960s on the style of Jonathan Swift was vigorously contested by Stanley Fish, an attack which may well have had the effect of curbing enthusiasm for this kind of work. The other important exemplar is John Burrows’ book on Jane Austen from 1987. I am not aware of any subsequent books of this kind.  In the proposed paper I aim to demonstrate the usefulness of two measures from Information Theory in the broad comparative analysis of text. One is entropy, which calculates the greatest possible compression of the information provided by a set of items considered as members of distinct classes (Rosso, Craig and Moscato). A large entropy value indicates that the items fall into a large number of classes, and thus must be represented by listing the counts of a large number of these classes. In an ecosystem, this would correspond to the presence of a large number of species each with relatively few members. The maximum entropy value occurs where each item represents a distinct class. Minimum entropy occurs where all items belong to a single class. In terms of language, word tokens are the items and word types the classes. A high-entropy text contains a large number of word types, many with a single token. A good example would be a technical manual for a complex machine which specifies numerous distinct small parts. A low-entropy text contains few word types, each with many occurrences, such as a legal document where terms are repeated in each clause to avoid ambiguity. Entropy is a measure of a sparse and diverse distribution versus a dense and concentrated one.  A second information-theory quantity which can serve for generalising about a set of texts is Jensen-Shannon Divergence (JSD). This gives a value to each set of items for the distance from a reference point, generally the mean for the whole grouping. This distance is calculated as the sum of divergences between the specimen and the mean for each of the classes represented in the set (Rosso, Craig and Moscato). In language terms the divergence value of a given text is the sum of the differences between the counts for the text for each word type used in a corpus and the corpus mean count for that word type. Some texts use language in a way that closely corresponds to the norm of a larger set, others use some words more heavily, and others more lightly, than the run of a comparable corpus. JSD is a measure of normality in this specialised sense.  There are important caveats for interpreting these two measures of the properties of a text. Both are sensitive to text length, if for different reasons. Given a finite number of word types available to a given user of a given language, as a text sample grows, more of the pool is exhausted, and there is a greater tendency to recur to already-used word types. Thus in a novel the word tokens of a single sentence may well be all different word types, and have maximum entropy, but this is unlikely to be true of a paragraph, and still less so of a chapter. In the case of divergence from a mean, the law of averages means that for longer texts local idiosyncrasies tend to be balanced out by a larger body of less unusual writing and indeed by contrasting idiosyncrasies.  It is also important to rule out the idea that entropy and divergence values relate directly to quality. Entropy is related to a simpler measure, type-token ratio, sometimes called ‘vocabulary richness’. Yet ‘richness’ could scarcely be applied to a fighter plane manual, to revert to the example used above. One might associate divergence from the mean with originality or creativity, but it could just as well be the result of incompetence.  It is interesting that researchers have found genre to be a problem both with entropy work and with studies of intertextual distance when they are directed at authorship problems (Hoover, Labbé and Labbé). From a different point of view, this sensitivity to genre is part of what makes the methods valuable for a more general assessment of the style of texts.  The corpus for the study in the paper consists of 377 fiction texts, being the first 25,000 words of all the texts with 25,000 words or more in the British National Corpus ‘Imaginative Fiction’ section. This amounts to 15,421,915 words in all. The texts are predominantly prose fiction published in the United Kingdom in the 1990s, taken from a wide variety of sources, short stories as well as novels, intended for young and young adult audiences as well as for a general readership. The usefulness of JSD results depend on the validity of the point of reference chosen. In the present study the mean of this large collection of texts of very varied authorship and genre, within the larger text type ‘imaginative fiction’, should be a good approximation of the mean for contemporary fiction in general.  At the time of writing this proposal work on this corpus with these methods is at an early stage, but there are some preliminary findings. The first is that entropy and divergence are positively correlated in this corpus. As density decreases and a wider range of word types are used for the same extent of text, samples diverge more from the mean. The individual exceptions to these broad tendencies are instructive and some individual examples will be discussed.  It is also possible to see at this early stage that within the universe of prose fiction these two quantities align with more impressionist views of style. High entropy fiction texts follow a traditional ‘high style’. Their progression is linear, continuing to move on to new vocabulary, while low entropy texts retrace their steps and return to already used words. High entropy texts are demanding of the reader and dense in information. They constantly move to new mental territories; they are taxing and impressive. Low entropy texts are reassuring and familiar. They are implicit in their signification, assuming common knowledge, while high-entropy texts specify and create contexts for themselves. High-entropy texts contain more description and narrative, while low-entropy texts contain more dialogue.  The challenge for computational approaches to style is to use the power of statistics working on the abundant data available from texts to reveal tendencies which are important, yet would otherwise be invisible, or remain in the realm of the impressionistic. The argument of the proposed paper is that the entropy and divergence of words provide two useful ways of understanding fundamental properties of texts. Entropy and divergence are soundly based in statistical theory and informative on two fronts. They open the way to density and normality as fundamental ways of thinking about style; and they serve to place particular texts in relation to sets of comparison texts and thus to map them in a conceptual space. Short stories and novels may be virtual worlds, intensely personal meditations, and human dramas of love and conflict, but they are also sets of vocabulary items used with a given frequency, and it is surprising how much an analysis of that base level of their existence can reveal about them.   ",
        "article_title": "Entropy and Divergence in a Modern Fiction Corpus",
        "authors": [
            {
                "given": " Hugh",
                "family": "Craig",
                "affiliation": [
                    {
                        "original_name": "School of Humanities and Social Science, University of Newcastle, Australia",
                        "normalized_name": "University of Newcastle Australia",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/00eae9z71",
                            "GRID": "grid.266842.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-05-12",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  In 1879 the North American Review published in four separate monthly installments excerpts from \"The Diary of a Public Man\" in which the name of the diarist was withheld. It was, or purported to be, a diary kept during the \"secession winter\" of 1860-61. It appeared to offer verbatim accounts of behind-the-scenes discussions at the very highest levels during the greatest crisis the US had ever faced.  Interest in this real or purported diary was considerable. The diarist had access to a wide spectrum of key officials, from the South as well as the North, gave a number of striking anecdotes about Abraham Lincoln, and provided an important account of events at Washington during the critical days just before the Civil War. A detailed study of the Diary was conducted by Frank Anderson in 1948 in his book The Mystery of \"A Public Man\". Anderson argues that the Diary is part genuine and part fictitious with two of the three striking Lincoln incidents appearing to be inventions, along with other so-called \"interviews\" with prominent figures. He believes that, as a core, there is a genuine diary kept by Samuel Ward (1814-1884) at Washington during that winter, and that it is possible that the editor of North American Review, Allen Thorndike Rice, may have assisted in the process of embellishment. William Hurlbert (1827-1895), he argues, may also be involved, since the style of the Diary has a good deal of Hurlbert's pungency. Others have suggested that the diarist might be Henry Adams (1838-1918), who enjoyed close access to William Henry Seward who became Lincoln's Secretary of State and was a central figure in the Diary. Certainly the fact that, over a century after its publication, the authorship has remained undetermined is proof that the work of all those who may have shared in its preparation and publication was cleverly done.   Traditional Attribution This paper argues that the diarist was not Samuel Ward; it was, instead, William Hurlbert.  The preponderance of the evidence also suggests that the Diary may well be a legitimate historical document. The diarist was not simply an observer but very much a participant-observer. One key circumstance would have impeded Ward. At the precise time the Diary was being penned, he was busily engaged in writing a memoir of his experiences in the California gold fields in 1851-52. His recollections were published in a New York weekly, starting on January 22nd 1861, and concluding abruptly on April 23rd 1861. A great deal of internal evidence suggests that the Diary and the Gold Rush memoir could not have been written by the same person, even allowing for their radically different subject matter. Ward's sentences sometimes meander in a Baroque manner, he often alliterates, peppers his narrative with Spanish and French expressions, and has a habit of encasing unusual words or phrases within quotation marks. By contrast the Diary is fast paced and immediate, with a style running towards active verbs accompanied by adverbs of a certain type. In William Hurlbert, however, we find a newspaper writer whose style had sweep and dash. At the very moment when the biggest story he had ever witnessed burst to attention, he had no job, but nonetheless, had access to a remarkably wide range of prominent people. The Southern-born Hurlbert also had more basis than Ward to have developed close ties with leading Southerners. A comparison of the Diary with things known to have been written by Hurlbert yields some demonstrable parallels, not least in the number of signature words used in the Diary. Some specific features of the Diary also point to Hurlbert rather than Ward, for example twice the diarist mentions Josiah Quincy (1772-1864), the retired President of Harvard, who had been an important influence in Hurlbert's young life. There are circumstances, too, that suggest why Lincoln might initially have encountered Hurlbert and why he might have welcomed a repeat visit. Concerning its legitimacy, in a number of crucial particulars the Diary conveys an on-the-spot immediacy that would have been almost impossible to recreate even months after the fact, let alone years; for example the unfolding story of Lincoln's secret and circuitous trip to Washington in late February and the diarist's delayed realization that Seward warned Lincoln to undertake it. The diarist expresses repeated concerns about the potential economic effects of secession, concerns which were quickly subordinated once the war started. The diarist also demonstrates an excellent ear in his accounts of his interviews with others, in particular their personal mannerisms. In all its particulars, the Diary synchronizes perfectly with the way events unfolded at that time.      Non-Traditional Attribution For testing and validating the stylometric techniques involved in this phase of the study, preliminary textual samples were taken from prominent diarists of that era, George Templeton Strong, Gideon Welles, and Salmon Chase. Analysis of the top 50 frequently occurring function words using what is now known as the \"Burrows\" approach involving principal components analysis and cluster analysis showed clear discrimination between writers and internal consistency within writers. Textual samples were then taken from three candidate authors of the Diary, namely Samuel Ward, Henry Adams and James Harvey, with the \"Burrows\" approach once again indicating remarkable internal consistency and clear between–writer discrimination. Four textual samples each of approximately 3,000 words, representing in total about 2/5 of the work, were taken at various places throughout the Diary, being sufficiently spaced to enable a valid check to be made on internal consistency.  The Diary samples showed excellent internal consistency, suggesting single authorship which would refute Anderson's \"cut and paste\" theory.  They appeared to be quite distinct from the samples of the writings of Adams, Harvey and Ward. Focus then moved to the two main contenders for authorship, Ward and Hurlbert. Carefully controlling for genre in the selection of the textual samples from Hurlbert and Ward, subsequent multivariate analyses on high-frequency function words showed discrimination between these two writers, along with internal consistency. For the attributional stage of the research discriminant analysis was employed. The samples from the Diary, Hurlbert and Ward were divided into smaller sizes in order that as many high-frequency function words as possible could be used without violating the assumptions underlying the technique. All 12 Diary samples were placed into the Hurlbert group. Finally, the \"Delta\" technique, proposed by Burrows and refined by Hoover, was employed using the 100 most frequently occurring words in the pooled corpus and on four potential authors of the Diary: Ward, Adams, Harvey and Hurlbert.  The closest \"match\" to the Diary using Delta and its variants was indeed Hurlbert.     Conclusion The non-traditional stylometric analysis has supplied objective evidence that supports traditional scholarship regarding the problem of the authorship of the Diary. The likelihood that the entire document was written by one person is very strong. William Hurlbert has been pinpointed, to the exclusion of all others, as the Diary's author. Much of the Diary could never have been concocted after the fact; the chances are that the entire document is authentic.  ",
        "article_title": "The Diary of a Public Man: A Case Study in Traditional and Non-Traditional Authorship Attribution ",
        "authors": [
            {
                "given": " David I.",
                "family": "Holmes",
                "affiliation": [
                    {
                        "original_name": "The College of New Jersey, USA",
                        "normalized_name": "Princeton University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00hx57361",
                            "GRID": "grid.16750.35"
                        }
                    }
                ]
            },
            {
                "given": " Daniel W.",
                "family": "Crofts",
                "affiliation": [
                    {
                        "original_name": "The College of New Jersey, USA",
                        "normalized_name": "Princeton University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00hx57361",
                            "GRID": "grid.16750.35"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-03-17",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The need for text encoding standards for language resources (LRs) is widely acknowledged: within the International Standards Organization (ISO) Technical Committee 37 / Subcommittee 4 (TC 37 / SC 4), work in this area has been going on since the early 2000s, and working groups devoted to this issue have been set up in two current pan-European projects, CLARIN () and FLaReNet (). It is obvious that standards are necessary for the interoperability of tools and for the facilitation of data exchange between projects, but they are also needed within projects, especially where multiple partners and multiple levels of linguistic data are involved. One such project is the National Corpus of Polish (Pol. Narodowy Korpus Języka Polskiego; NKJP; ; Przepiórkowski et al. 2008, 2009) involving 4 Polish institutions and carried out in 2008–2010. The project aims at the creation of a 1-billion-word automatically annotated corpus of Polish, with a 1-million-word subcorpus annotated manually. The following levels of linguistic annotation are distinguished in the project: 1) segmentation into sentences, 2) segmentation into fine-grained word-level tokens, 3) morphosyntactic analysis, 4) coarse-grained syntactic words (e.g., analytical forms, constructions involving bound words, etc.), 5) named entities, 6) syntactic groups, 7) word senses (for a limited number of ambiguous lexemes). Any standards adopted for these levels should allow for stand-off annotation, as is now common practice and as is virtually indispensable in the case of many levels of annotation, possibly involving conflicting hierarchies. Two additional, non-linguistic levels of annotation required for each document are text structure (e.g., division into chapters, sections and paragraphs, appropriate marking of front matter, etc.) and metadata. The standard adopted for these levels should be sufficiently flexible to allow for representing diverse types of texts, including books, articles, blogs and transcripts of spoken data. NKJP is committed to following current standards and best practices in corpus development and text encoding. However, because of the current proliferation of official, de facto and purported standards, it is far from clear what standards a new corpus project should adopt. The aim of this paper is to attempt to answer this question.  Standards and best practices The three text encoding standards and best practices listed in a recent CLARIN short guide (CLARIN:STE, 2009)See also Bel et al. 2009. are: standards developed within ISO TC 37 SC 4, the Text Encoding Initiative (TEI; Burnard and Bauman 2008) guidelines and the XML version of the Corpus Encoding Standard (XCES; Ide et al. 2000). Apart from these, there are other de facto standards and best practices, e.g., TIGER-XML (Mengel and Lezius, 2000) for the encoding of syntactic information, or the more general PAULA (Dipper, 2005) encoding schema used in various projects in Germany.  XCES The original version of XCES inherits from TEI an exhaustive approach to metadata representation. It makes specific recommendations for the representation of morphosyntactic information and for the alignment of parallel corpora. In early the 2000s, it was probably the most popular corpus encoding standard. Currently, the claim of XCES to being such a standard is much weaker. A new — more abstract — version of XCES was introduced around 2003, where concrete morphosyntactic schema was replaced by a general feature structure mechanism, different from the ISO Feature Structure Representation (FSR) standard (ISO 24610-1). In our view, this is a step back, as adopting a more abstract representation requires more work on the part of corpus developers. Moreover, XCES has no specific recommendations for other levels of linguistic knowledge, and no mechanisms for representing discontinuity and alternatives, all of which need to be represented in NKJP. Taking also into account the lack of documentation and the potential confusion concerning its versioning,Two different sets of schemata have co-existed on XCES WWW pages since 2003, one given as DTD, another as XML Schema, without any clear indication that they specify different structures. XCES turns out to be unsuitable for the purposes of NKJP.   ISO TC37 SC 4 There is a family of ISO standards developed by ISO TC 37 SC 4 for modelling and representing different types of linguistic information. The two published standards concern the representation of feature structures (ISO 24610-1) and the encoding of dictionaries (ISO 24613). Other proposed standards are at varying levels of maturity and abstractness. While eventually these standards may reach stability and specificity required by practical applications, this is currently not the case. A tendency may be observed of increasing abstractness and generality of proposed standards, esp., SynAF (ISO 24615) and LAF (ISO 24612)\". This leads to their greater formal elegance, at the cost of their actual usefulness.   TIGER-XML and PAULA TIGER-XML and a schema which may be consider as its generalisation, PAULA, are specific, relatively well-documented and widely employed best practices for describing linguistic objects occurring in texts (so-called \"markables\") and relations between them (in the case of TIGER-XML, the constituency relation). They do not contain specifications for metadata or structural annotation.    TEI P5 For metadata and structural annotation levels there is no real alternative to TEI. Moreover, TEI P5 implements the FSR standard ISO 24610-1, which can be used for the representation of any linguistic content, along the lines of XCES (although the feature structure representations used in XCES do not comply with this standard), PAULA and the proposed ISO standard, Linguistic Annotation Framework (ISO 24612). TEI P5 is stable, has rich documentation and an active user base, and for these reasons alone it should be preferred to XCES and (the current versions of) the ISO standards. Moreover, any TIGER-XML and PAULA annotation may be expressed in TEI in an isomorphic way, thanks to the linking mechanisms of TEI P5. However, TEI is a very rich toolbox, proposing multitudinous mechanisms for representing multifarious aspects of text encoding, and this richness, as well as the sheer size of TEI P5 documentation (1350–1400 pages), are often perceived by corpus developers as prohibitive. For this reason, within NKJP, a specific set of recommendations for particular levels of annotation has been developed, aiming at achieving a maximal compatibility (understood as the easiness to translate between formats) with other proposed and de facto standards. For example, TEI P5 offers, among others, the following ways to represent syntactic constituency:  XML tree, built with elements such as <s>(entence), <phr>(ase), <cl>(ause) and <w>(ord), may directly mirror constituency tree;  all information, including constituency, may be encoded as a feature structure (Witt et al., 2009); each syntactic group is a <seg>(ment) of type group, containing a feature structure description and <ptr> pointers to other constituents, defined in the same file (for non-terminal syntactic groups) or in a stand-off way elsewhere (for terminal words).    While the first of these representations is the most direct, and the second most general, it is the third representation that directly mirrors TIGER-XML, PAULA and SynAF, and for this reason, it has been adopted in NKJP.  ",
        "article_title": "TEI P5 as a Text Encoding Standard for Multilevel Corpus Annotation",
        "authors": [
            {
                "given": " Piotr",
                "family": "Bański",
                "affiliation": [
                    {
                        "original_name": "University of Warsaw Poland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Przepiórkowski",
                "family": "Adam",
                "affiliation": [
                    {
                        "original_name": "Institute of Computer Science Polish Academy of Sciences Poland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2009-03-29",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Microblogging, a variant of a blogging which allows users to quickly post short updates to websites such as twitter.com, has recently emerged as a dominant form of information interchange and interaction for academic communities. To date, few studies have been undertaken to make explicit how such technologies are used by and can benefit scholars. This paper considers the use of Twitter as a digital backchannel by the Digital Humanities community, taking as its focus postings to Twitter during three different international 2009 conferences. This paper poses the following question: does the use of a Twitter enabled backchannel enhance the conference experience, collaboration and the co-construction of knowledge, or is it a disruptive, disparaging and a inconsequential tool full of ‘pointless babble’? Microblogging, with special emphasis on Twitter.com,Twitter was created by a San Francisco based privately funded startup and launched publicly in August 2006. http//:twitter.com/about the most well known microblogging service, is increasingly used as a means of extending commentary and discussion during academic conferences. This digital “backchannel” communication (non-verbal, real-time, communication which does not interrupt a presenter or event, Ynge 1970) is becoming more prevalent at academic conferences, in educational use, and in organizational settings. Frameworks for understanding the role and use of digital backchannel communication, such as that provided by Twitter, in enabling a participatory conference culture are therefore required. Formal conference presentations still mainly occur in a traditional setting; a divided space with a ‘front’ area for the speaker and a larger ‘back’ area for the audience. Implying a single focus of attention. There is a growing body of literature describing problems with a traditional conference setting; lack of feedback, nervousness about asking questions and a single speaker paradigm (Anderson et al 2003, Reinhardt et al 2009). The use of a digital backchannel such as Twitter, positioned in contrast with the formal or official conference programme, can address this, providing an irregular, or unofficial means of communication (McCarthy & Boyd, 2005). Backchannel benefits include being able to ask questions, or provide resources and references, changing the dynamics of the room from a one to many transmission to a many to many interaction, without disrupting the main channel communication. Negatives include a cause of distraction, disrespectful content and creating cliques amongst participants (Jacobs & Mcfarlane 2005, McCarthy and Boyd 2005).  Nevertheless research consistently shows the digital backchannel as a valuable way for active participation (Kelly 2009) and that it is highly appropriate for use in learning based environments (Reinhardt et al. 2009). Recently microblogging has been adopted by conferences such as DH2009 to act as a backchannel as it allows for the ‘spontaneous co-construction of digital artefacts’ (Costa et al 2008). Such communication usually involves note taking, sharing resources and individuals real time reactions to events. This paper presents a study that analyses the use of Twitter as a backchannel for academic conferences, focusing on the Digital Humanities community in three different physical conference settings held from June to September 2009. During three key conferences in the academic field (Digital Humanities 2009, That Camp 2009 and Digital Resources in the Arts and Humanities 2009), unofficial Twitter backchannels were established using conference specific hashtags (#dh09, #thatcamp and #drha09, #drha20091The community aspect of Twitter means that participants self organize, instigating tags themselves, hence the participants of Digital Resources in the Arts and Humanities used two different hashtags to discuss the conference depending on the twitter user.) to enable visible commentary and discussion. The resulting corpus of individual “tweets” provides a rich dataset, allowing analysis of the use of Twitter in an academic setting, and specifically presenting how the Digital Humanities community has embraced this microblogging tool.  Method Data from the three conferences was collected by archiving tweets which used the four distinct conference hashtags.  (These hashtags were used prior to and after the conferences, and have been reused by other conferences, therefore the corpus was limited to tweets posted during the span of each conference). This provided a data set of 4574 tweets from 326 distinct Twitter users, resulting in a corpus of 77308 tokens, which were analysed using various quantitative and qualitative methods which allowed us to understand and categorize the resulting corpus effectively.  Quantitative measures were used such as identifying prominent tweeters, analysing the frequency of conversations between users and the frequency of reposting messages (“retweeting”), and the differing use of Twitter at the three separate events. Text analysis tools were also used to interrogate the corpus.   Tweets were then categorized qualitatively using open coded content analysis where each post was read and categorized, determining the apparent intention of each twitter post.  It was necessary to develop our own categories: although Java et al (2007) present a brief taxonomy of Twitter user intentions (daily chatter, conversations, sharing information and reporting news)  they are based on general Twitter use and were too imprecise for our needs. Ebner (2008) discovered four major categories in his study of the use of Twitter during the keynote presentation at the Ed-Media 2008 conference, but this is a small study limited to 54 posts made by ten distinct users, whereas the DH conferences involved a much larger user population. Through our analysis, Tweets were divided into seven categories: comments on presentations; sharing resources; discussions and conversations; jotting down notes; establishing an online presence; and asking organizational questions.  Tweets which were highly ambiguous were placed in an Unknown category.    Findings Conference hashtagged Twitter activity does not constitute a single distributed conversation but, rather multiple monologues and a few intermittent, loosely joined dialogues between users. The majority of the activity was original tweeting (93%): only 6.7% were re-tweets (RT) of others’ ideas or comments. The real time interchange and speed of review of shared ideas appears to create a context of users offering ideas and summaries and not spreading the ideas of others verbatim. 45% of the tweets during the conference proceedings included direct references to others’ Twitter IDs, using the ‘@’ sign, as the source of a quote, object of a reply or debate. This implies a form of collaborative writing activity, driving a conference community of practice (Wenger 1998) who are involved in shared meaning making and the co-construction of knowledge (McNely 2009). However, the content of the tweets indicate that the discussion was between a few Twitter users rather than mass collaboration and was not necessarily focused on conference content. Jacob and Mcfarlane (2005) discuss polarization in digital backchannels, highlighting a conflict between an inclusive and participatory conference culture and a fragmentation of conference participants into cliques only intermittently engaged with the main presentations. This, in some instances seems to be the case during the Digital Humanities conferences, suggesting that newer members of the discipline or newer uses to Twitter may be excluded from the discussion. This also raises the question about official and unofficial backchannels. When communication is digitally mediated, backchannels may not be obvious. That is, even if participants know who else is participating in an interaction, this doesn’t guarantee (as it does in the front channel) that it is an accessible backchannel. Therefore by its nature an unofficial backchannel does not enable active participation. Further research is currently being undertaken on this corpus, which will be presented fully in the paper.  Most tweets in the corpus fell into the category of jotting down notes, triggered predominately by the front channel presentation, suggesting that participants are sharing experiences and to a degree co-constructing knowledge. What is surprising is the lack of direct commentary on presentations. Although Reinhardt et al (2009) argue that Twitter enables thematic debates and offers a digital backchannel for further discussion and commentary, the tweet data suggests that this does not appear to have happened to a significant extent at the digital humanities conferences. This raises the question of whether a Twitter enabled backchannel promotes more of an opportunity for users to establish an online presence and enhance their digital identity rather than encouraging a participatory conference culture.    Conclusion This study of digital humanities conference tweets provides an insight into the Digital Humanities community of practice. The Twitter enabled backchannel constitutes a complex multidirectional discursive space in which the conference participants make notes, share resources, hold discussions and ask questions as well as establishing a clear individual online presence. While determining individual user intentions in Twitter in a conference setting is challenging, it is possible to describe broad behavioral trends of tweeting during Digital Humanities conferences. The predominance of note taking suggests that the DH community could be classed as social reporters, commenting on the conference presentations for outsiders, rather than collaborating during the conference. There was also a tendency for a small group of users to produce the majority of tweets, interacting with each other about other matters. This suggests the small friendly nature of the DH researcher community, but may also be somewhat intimidating for those new to the field or conference. The Twitter enabled backchannel thus raises some interesting questions about the nature of conference participation and whether or not it is helped or hindered by a digital backchannel. Rather than pointless babble, the twitter record produced at each conference provides important evidence regarding how Digital Humanities, as a community of practice, functions and interacts.   ",
        "article_title": "Pointless Babble or Enabled Backchannel: Conference Use of Twitter by Digital Humanists",
        "authors": [
            {
                "given": " Claire",
                "family": "Ross",
                "affiliation": [
                    {
                        "original_name": "Department of Information Studies, University College London UK",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": " Melissa",
                "family": "Terras",
                "affiliation": [
                    {
                        "original_name": "Department of Information Studies, University College London UK",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": " Claire",
                "family": "Warwick",
                "affiliation": [
                    {
                        "original_name": "Department of Information Studies, University College London UK",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": " Anne",
                "family": "Welsh",
                "affiliation": [
                    {
                        "original_name": "Department of Information Studies, University College London UK",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-14",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Besides drawing on content experts, librarians, archivists, developers, programmers, managers, and others, many emerging digital projects also pull in disciplinary expertise from areas that do not typically work in team environments. To be effective, these teams must find processes – some of which are counter to natural individually-oriented work habits – that support the larger goals and group-oriented work of these digital projects. This paper will explore the similarities and differences in approaches within and between members of the Digital Libraries (DL) and Digital Humanities (DH) communities by formally documenting the nature of collaboration in these teams. The objective is to identify exemplary work patterns and larger models of research collaboration that have the potential to strengthen this positive aspect of these communities even further, while exploring the key differences between them which may limit digital project teams’ efforts. Our work is therefore designed to enable those who work in such teams to recognise factors that tend to predispose them to success, and perhaps more importantly, to avoid those that may lead to problematic interactions, and thus make the project less successful than it might otherwise have been.  Context Traditionally, research contributions in the humanities field have been felt to be, and documented to be, predominantly solo efforts by academics involving little direct collaboration with others, a model reinforced through doctoral studies and beyond (See, for example, Cuneo 2003; Newell and Swan 2000). However, DL and DH communities are exceptions to this. Given that the nature of digital projects involves computers and a variety of skills and expertise, collaborations in these fields involve individuals within their institutions and with others nationally and internationally. Such collaboration typically must coordinate efforts between academics, undergraduate and graduate students, research assistants, computer programmers and developers, librarians, and other individuals as well as financial and other resources. Further, as more digital projects explore issues of long term sustainability, academics and librarians are likely to enter into more collaborations to ensure this objective (Kretzschmar Jr. and Potter 2009). Given this context, some research has been done on the DL and DH (See, for example Liu and Smith 2007; Ruecker and Radzikowska 2008; Siemens 2009) communities as separate entities (See, for example Johnson 2009; Liu, Tseng and Huang 2005; Johnson 2005; Siemens et al. 2009b), but little has been done on the interaction between these two communities when in collaboration. Tensions can exist in academic research teams when the members represent different disciplines and approaches to team work (Birnbaum 1979; Fennel and Sandefur 1983; Hara et al. 2003). Collaborations can be further complicated when some team members have more experience and training in collaboration than other members, a case which may exist with digital projects involving librarians and archivists, who tend to have more experience, and academics, who have tend to have less. Ultimately, too little is known about how these teams involving DL and DH members collaborate and the types of support needed to ensure project success.   Methods This paper is part of a larger project examining research teams within the DH and DL communities, led by a team based in Canada and England (For more details, see Siemens et al. 2009a; Siemens et al. 2009b). It draws upon results from interviews and two surveys of the communities exploring the individuals’ experiences in digital project teams. The findings include a description of the communities’ work patterns and relationships and the identification of supports and research preparation required to sustain research teams (as per Marshall and Rossman 1999; McCracken 1988). A total of seven individuals were interviewed and another 69 responded to the two surveys.   Preliminary Findings At the time of writing this proposal, final data analysis of the surveys and interviews is being completed. However, some preliminary comparisons between the two communities can be reported. As a starting point, similarities exist among DL and DH projects. First, digital projects are being accomplished within teams, albeit relatively small ones, as defined by budget and number of individuals involved. Both communities report that the scale and scope of digital projects require individuals with a variety of skills and expertise. Further, these collaborations tend to operate without formal documentation that outline roles, responsibilities, decision making methods, and conflict resolution mechanisms. The survey and interview respondents from both communities report similar benefits and challenges within their collaborations. Finally, these teams rely heavily on email and face-to-face interaction for their project communications. Some interesting differences between DL- and DH-based teams exist and may influence a digital project team’s effectiveness. First, the DL respondents seem to have a greater reliance on email as opposed to face-to-face communications and tend to rate the relative effectiveness of email higher than the DH respondents. Several explanations may be offered for this. According to survey results, DL teams appear more likely to be located within the same institution, which means that casual interpersonal interaction may be more likely to occur between team members than with groups that are geographically dispersed, as many DH teams are. For dispersed teams, meetings need to be more deliberately planned, which may mean a higher consciousness about the importance of this kind of interaction and the necessity to build this into project plans. Also, given that many of the DL teams are within the same organization, team members may be more familiar with each other in advance of a project start, meaning that more communication can be done by email. Less time may need to be spent in formal meetings developing work processes as is the case with those teams whose members may not have worked together on previous projects. Second, a greater percentage of respondents (42%) within the DH community indicated that they “enjoyed collaboration” than the DL respondents (18%). Comprising of more academics, the DH community tends to undertake more solitary work, and therefore collaboration may be seen as a welcomed change and may be a deliberate choice that they have made to undertake this type of work. In contrast, team work is more the norm for librarians and archivists, and thus they may feel it is an expected part of their jobs, rather than a choice and welcomed activity. As a result, members of these two communities approach collaboration from two fundamentally different positions, which must be understood from the outset of a digital project in order to reduce challenges and ensure success. Further, differences in roles and perceived status may complicate collaboration. Often, tensions may exist between service departments, such as libraries and computer support, and the researcher, who is perceived to have higher status (Warwick 2004). These differences in perceived status can complicate work process as those with lower status may have difficultly directing those with perceived higher status (Hagstrom 1964; Ramsay 2008; Newell and Swan 2000). The benefits to the DL and DH communities will be several. First, the study contributes to an explicit description of these communities’ work patterns and inter-relationships. Second, it designed to enable those who work in such teams to recognise factors that tend to predispose them to success, and perhaps more importantly, to avoid those that may lead to problematic interactions, and thus make the project less successful than it might otherwise have been.  ",
        "article_title": "A Tale of Two Cities: Implications of the Similarities and Differences in Collaborative Approaches within the Digital Libraries and Digital Humanities Communities",
        "authors": [
            {
                "given": " Lynne",
                "family": "Siemens",
                "affiliation": [
                    {
                        "original_name": "Faculty of Business/School of Public Administration, University of Victoria Canada",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Richard",
                "family": "Cunningham",
                "affiliation": [
                    {
                        "original_name": "Acadia Digital Culture Observatory, Acadia University Canada",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Wendy",
                "family": "Duff",
                "affiliation": [
                    {
                        "original_name": "Faculty of Information, University of Toronto Canada",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Claire",
                "family": "Warwick",
                "affiliation": [
                    {
                        "original_name": "Department of Information Studies, University College London UK",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-03-30",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This paper will discuss the potential impact upon historical research of British History Online's annotation tool. British History Online (BHO)   is a digital library containing some of the core printed primary and secondary sources for the medieval and modern history of the British Isles. Created by the Institute of Historical Research (IHR), which is part of the University of London's School of Advanced Study, BHO  is a rigorous academic resource used by researchers at postgraduate level and above. The IHR is centrally placed within academic history in the UK, and as such it is highly regarded within the profession. Two years ago, as part of an Arts and Humanities Research Council (AHRC) grant, BHO undertook to provide an annotation tool alongside the digitisation of some 500 key historical sources for early modern history: the Calendars of State Papers. These calendars summarise the manuscript heritage of the working of the state in the early modern period. Essential research tools though these calendars are, they were mainly compiled in the Victorian period, and are known to be inadequate and erroneous in some cases. Furthermore, changes in perspective on history and subsequent research means that the calendars are badly in need of updating: papers which modern editors would think worthy of close attention are sometimes treated very cursorily.  BHO's annotation tool encourages the community of scholars to update, enlarge upon, and correct the calendars, and even to supply fuller transcriptions of documents.  At the planning stage of the tool's development the team looked at other online tools for user commenting, ranging  from scholarly collections such as the Digital Image Archive of Medieval Music to non-academic sites such as the blog-style Diary of Samuel Pepys. This paper will describe the reasons why we decided that an annotation tool was preferable to a  wiki, the design process for the tool, and BHO's subsequent attempts to engage the academic community in online collaborative work: now that the AHRC project has finished (but the annotation tool remains active) DH 2010 will be a good time to assess the successes and failures this aspect of the project and the lessons that might be applicable to other digital resources for historians. The IHR is now involved in a collaborative project, Connected Histories, and the lessons of BHO's annotation tool will have a direct bearing on how the front end for Connected Histories is designed. The paper will also touch upon the issues of moderation of academic work, the role of citation within web 2.0, and the constraints we imposed on annotators in this regard, intellectual copyright and the RAE, and the broader question of how humanities research culture might change as web-based collaboration becomes the norm. The IHR is currently addressing the question of how the research community within history might be mobilised to work together online, in more general ways, with European collaboration, semantic web research tools, and VREs. The paper will conclude by briefly placing these in the context of work already done on the annotation tool.  ",
        "article_title": "Developing a Collaborative Online Environment for History – the Experience of British History Online",
        "authors": [
            {
                "given": " Jonathan",
                "family": "Blaney",
                "affiliation": [
                    {
                        "original_name": "Institute of Historical Research, UK",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-02",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In a recent article entitled \"Innkeeper at the Roach Motel\", Dorothea Salo worries that focusing exclusively on preservation when designing institutional repositories leads to a situation in which documents are placed into repositories but never come out (Salo 2009). In this conceit, a \"live\" project gets placed in a repository and \"dies\" from lack of use. However, when attempting to preserve distributed, dynamic electronic textual editions, a somewhat different metaphor is needed. Like items in a specimen case, \"live\" digital projects must be \"killed\" before they are added to a conventional institutional repository such as DSpace. In such applications, they must be removed from the dynamic ecology of their production environments (the garden of our title) and frozen in a snapshot form that is substantially different in appearance and functionality. In our presentation, we will outline our own efforts to construct a preservation and sustainability plan for a multi-format, distributed, dynamic electronic textual edition that involves both the creation of a preservation \"specimen\" and the careful tending of the edition in the \"garden\". We will also share the general tools and workflows developed by the project that can help others tackle the same challenges. Due to their innovative nature and the environments in which they are created, Digital Humanities projects are often dynamic and distributed. In other words, they often exist in multiple parts maintained on distributed hardware (which itself is supported by multiple organizations), and are often compiled for viewing on the fly in response to readers' actions. The typical preservation strategy of frequent offsite backups is inadequate for these projects. The user or manager must to be able to find the backups (including all of the constituent parts of a complex project, wherever they reside) and recognize what they are; the backups must be in usable condition; and their contents need to be understandable to the people who want to use them. Moreover, if backup files are to be used in any way similar to their original use, the files must be (1) compatible with current hardware and software, (2) translated into formats that are compatible with current hardware and software, or (3) used on reconstructed or emulated hardware and software that match the environment in which the project was originally developed. Digital Humanities projects created by faculty often have the added vulnerability of relying on the creator's university computing accounts. Absent special accommodations, these accounts usually expire on a set schedule once the individual has moved on, taking with them information that often exists nowhere else.   Cultures of Preservation In short, digital materials require a culture of description, preservation, and access every bit as robust as the practices and institutions that allow us to preserve manuscript and print materials. The devil of preservation — whether of print, digital, or other material artifacts — lies in the details of production, use, description, storage, conservation, and access. This holds true whether we are talking about acidic paper disintegrating on library shelves, digital files in obsolete formats, or media spread across computer systems whose links to one another have been broken. Preservation is further complicated by the distinction between preserving physical artifacts (books, manuscripts, floppy disks, flash drives) and preserving the information contained on those media in a useful format.  \"The Specimen Case and the Garden: Preserving Complex Digital Objects, Sustaining Digital Projects\" focuses on the preservation challenges posed by complex digital humanities projects, which present unique challenges to libraries and repositories charged with accessioning, describing, and preserving the scholarly record. Our work, funded by the U. S. National Endowment for the Humanities, takes a two-pronged approach to the problem, developing technologies for preserving digital objects — and the relationships among them — that constitute complex projects, and establishing institutional structures for sustaining digital humanities projects after their creators are no longer actively involved in their development and maintenance. Over the course of more than a year, we have interviewed faculty involved in digital humanities projects, library professionals, and information technology professionals; assessed the need for new practices adapted to digital preservation at our institution; and documented the resources and workflows currently available for, or adaptable to, long-term preservation of digital objects. We have also begun to develop tools, institutional structures, and workflows for describing and archiving complex digital objects, as well as sustaining distributed digital production environments.   Preservation and Sustainability Tools and Workflows Our presentation will outline the problems associated with preserving and sustaining complex digital projects, review the data we collected during our interviews, literature review, and environment scan, and share the tools that we have developed, including the following:  a lifecycle map of complex digital projects that represents development and preservation milestones as interactions among scholars, library professionals, and IT professionals; a visual content manifest for complex digital projects that represents assets, the hardware on which those assets rely, and entities that enable the collaborative work of developing and preserving digital humanities projects; a Metadata Encoding and Transmission Standard (METS) profile for creating archival packages of complex digital projects; a visual representation of the roles of the scholars, library professionals, and IT professionals on our campus in the long-term preservation of digital humanities projects; a proposal for a Digital Humanities Network that could sustain selected distributed digital projects, without requiring that they sacrifice functionality for centralization.   Some of these tools and workflows will be easier to adapt to different projects, institutions, and cultural settings than others: for example, any library system should be able to adapt the METS profile to their needs, while our proposed Digital Humanities Network will serve mostly as a heuristic. Indeed, we hope to initiate a fruitful conversation about how to build cultures of preservation for complex digital projects among scholars, librarians, and IT professionals in a variety of institutional settings.  ",
        "article_title": "The Specimen Case and the Garden: Preserving Complex Digital Objects, Sustaining Digital Projects ",
        "authors": [
            {
                "given": " Melanie",
                "family": "Schlosser",
                "affiliation": [
                    {
                        "original_name": "University Libraries, The Ohio State University USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " H. Lewis",
                "family": "Ulman",
                "affiliation": [
                    {
                        "original_name": "Department of English, The Ohio State University USA",
                        "normalized_name": "The Ohio State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00rs6vg23",
                            "GRID": "grid.261331.4"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-03-19",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " We present research showing the possibility of finding stories in a digital text archive through computational methods. Referring to the concept of \"archival bond\", we define stories as formed by documents that relate to a target activity. We developed a method called paragraph alignment to find such documents and an interactive visualization to discover connected stories in context with provenance.  Our method was applied to the challenges presented by the digital archive of a multinational philanthropic organization who awarded grants to cultural, scientific, and social welfare activities (1985-2005). Over fifteen years, the staff members deposited their work documents in individual directories on a shared server without following any record-keeping rule. These documents reflect the organization's activities in the areas of Science and Education, Art and Humanities, and Social Welfare. They also reflect the staff members' records creation practices, afforded by the cut and paste function of the word processor and the possibility to collaborate through the network. These digital aggregations are sometimes perceived as chaotic, defined as ROT (redundant, outdated and trivial,) and deemed disposable (Henry, 2003; AIIM, 2009; Public Records Office, 2000). Yet they are ubiquitous in the networked servers of many organizations, so our goal was to find a method to make sense of the text records within.  Archival Bond A fundamental concept in archival theory, known as archival bond, describes the relationships between documents in an archive as essential properties of the documents (Duranti, 1997). While all the documents in a collection are bonded through the collection's structure (McNeil, 2000), there are stronger relationships between sub-groups of documents that belong to the same function and/or activity. In the case of disorganized electronic text archives in which the structure is nonexistent or loose, we suggest that the relationships among documents be defined based on their content referring to a target activity. By finding trails of documents that narrate stories about activities in context with provenance, we aim to establish order, identify structure, and learn about the archive's creators.   Paragraph Alignment (PA) We observed that in our archive, similar paragraphs about an activity are repeated across short - memos and press releases - and long documents - annual reports and board meeting minutes. As a group, these documents tell the story of an activity. We also observed that in many documents the same personal names, places, and institutions are mentioned in relation to different activities, and that documents that use similar terms may not be associated with the same activity. The traditional cosine similarity method measures global similarity between documents. Given the characteristics noted in this archive, we considered that calculating global similarity was not efficient to identify all the documents about a target activity. Instead, we draw from local alignment, a method used in bioinformatics to evaluate local similarity between sequences (Gusfield, 1997). While biological sequences evolve throughout time owing to constant mutation events, the parts of the sequences that directly participate in cellular activities remain relatively stable. Therefore, global similarity between two sequences is often less important than the local similarity, which is defined by the highest similarity between any two substrings from two sequences. Efficient methods for computing sequence similarities often follow a framework in which sequences are broken into n-gram for similarity computations and then assembled to derive an overall similarity (Wu et al., 1990). Here we adapt a similar approach that we call paragraph alignment to determine archival bond between documents.  Our method contrasts with previous work on document segmentation (Hearst, 1994). Rather than measuring inter-paragraph similarity within one document to identify subtopic structure, our approach focuses on comparing the similarity between document segments to identify topics across a collection. Hence the primary goal of document segmentation is to minimize the variation of length between documents for subsequent similarity comparison.   Methodology Figure 1 shows the workflow of our approach.   Figure 1  Workflow to compute document similarity using paragraph alignment.   Each document in a set is broken into one or more ordered segments based on the paragraphs in the document. If the length of a segment (including spaces) is less than a pre-defined minimum number of characters threshold (MNCT), the segment is merged with the following segment. We used MNCT of 1000, 750, and 500 characters. For each set of document segments we create a matrix of TFIDF weighted term frequencies after stop-words removal (McCallum, 1996), and then calculate the cosine similarity between every other segment (Salton, 1988). We then process the resultant matrix to derive similarity scores between document pairs, which are defined as the maximum similarity score between their segments. For evaluation, we compare the results of the different MNCT with those obtained by calculating cosine similarity as a measure of global similarity between the documents.  We tested the method in a set of 714 documents from the year 1997 with eight authors. Date and authorship were preserved in the documents' file name. The evaluation was based on assessing seven document test-groups. A team member familiar with the archive selected five query documents, each corresponding to a different activity (test-groups 1, 2, 4, 5, 6) and two containing summaries of various activities (test-groups 3 and 7). For each query document, the team member also identified a set of related documents. For each test-group, both the cosine similarity and the paragraph alignment methods returned a list of documents ranked from more similar to less similar. The team member checked the results against the content of the corresponding document labeling the ranked document as a \"true positive\" if it was related to the query document; otherwise the document was labeled as \"false positive\". Results were checked until the last true positive was found.   Results   Table1  Results of test-group one   The results show that the PA method with a MNCT of 750 characters returned better results five out of seven times (test-groups 1, 2, 4, 5, 6 and 7). For test-group 7, the best results were obtained with a MNCT of 500 characters. In this case the query document contained summaries of five different projects accomplished during 1997, each mentioned in other documents in the set. This suggests that although related documents in the set may not share similar global word distributions, they share similar word distributions in some of their segments. While the efficiency of the different MNCT depends on the particular word distribution of the documents that are being compared, in general, the smaller the MNCT used the higher the documents with less global similarity are ranked by the PA method. The PA method did not work for test-group 3 which contains sentences about activities most of which are not mentioned in other documents in the set. Figure 2 shows a plot of the results of test-group 1 in which the PA method with a MNCT of 750 characters performed the best.    Figure 2  Results of test-group one   The test-group one (Figure 2) contains documents about a program to train young orchestra directors. The query document is a memo including a brief description of the project and estimated costs for lodging and travel. Returned true positive documents of five authors include: other planning documentation, correspondence with potential contributors, the call for applications, a press release, a list of participants, the musical program, and various reports.    Visualization Through an interactive visualization (Figure 3) users can follow the connections between documents to identify stories (PREFUSE). Each document is labeled with a color corresponding to its author. The connectivity between the documents allows the identification of a) stories, b) the authors involved in a given activity, and c) connected stories. As the user interacts with the visualization, the structure of the archive takes shape. Below is a snapshot of the visualization interface showing stronger connections between a group of documents.   Figure 3  Network visualization of documents similarity and authorship.     Conclusions This research has implications for the retention of digital archives. Using the concept of archival bond as a framework we aim to make sense of ROT archives and to unveil their stories. The results show that for documents that share similar paragraphs, local similarity matters to identify an archival bond. The same characteristic is observed in the biological sequence analysis that inspired our method.    Acknowledgments This work was supported through a National Archives and Records Administration (NARA) supplement to the National Science Foundation Cooperative Agreement (NSF) TERAGRID: Resource Partners, OCI-0504077.  ",
        "article_title": "Finding Stories in the Archive through Paragraph Alignment",
        "authors": [
            {
                "given": " Maria",
                "family": "Esteva",
                "affiliation": [
                    {
                        "original_name": "Texas Advanced Computing Center (TACC), University of Texas at Austin, USA",
                        "normalized_name": "The University of Texas at Austin",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00hj54h04",
                            "GRID": "grid.89336.37"
                        }
                    }
                ]
            },
            {
                "given": " Weijia",
                "family": "Xu",
                "affiliation": [
                    {
                        "original_name": "Texas Advanced Computing Center (TACC), University of Texas at Austin, USA",
                        "normalized_name": "The University of Texas at Austin",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00hj54h04",
                            "GRID": "grid.89336.37"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-02",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "   Ensuring evidence of women’s experiences and contributions to our world are kept for the public record and adequately represented in memory institutions has been a key challenge for many inside and outside of the academy over the last half century. This material is vital in order to continue the work of retrieving women’s history from ‘the shrouds of silence and obscurity’ and ‘fill in the blank half of a huge canvas’.The title of this paper owes much to the wonderful words of Australia’s first female Governor General, Quentin Bryce, when re-launching the Australian Women’s Register on the 13 October 2009. In her speech she highlighted the words of Adrienne Rich, American poet and feminist, ‘Whatever is unnamed, undepicted in images, whatever is omitted from biography, censored in collections of letters, whatever is misnamed as something else, made difficult-to-come-by, whatever is buried in the memory by the collapse of meaning under an inadequate or lying language – this will become, not merely unspoken, but unspeakable.’ See http://www.gg.gov.au/governorgeneral/speech.php?id=625. The wordes in quotes come from the same source. Over the past decade, the Australian Women’s Archives Project (AWAP) has been developing the Australian Women's Register (http://www.womenaustralia.info/) as a central part of its strategy to encourage the preservation of women’s archival heritage and to make it more accessible to researchers. The Register is a specialist central access point to information about Australian women and their achievements and the multifarious resources in which varying aspects of their lives are documented. It provides a gateway to archival and published material relating to women held in Australian cultural institutions as well as in private hands. A series of small and large grants have contributed to the development of the content of the Register and the technology in which it is captured, managed and made available to as wide an audience as possible via the Web. The National Foundation for Australian Women,Information about the aims of the National Foundation for Australian Women can be found at http://www.nfaw.org/. the community organisation behind the AWAP, plays a significant role in securing project funding, along with driving innovation in its coverage and content.  The latest of these grants, an Australian Research Council Linkage Infrastructure Equipment and Facilities Grant (ARC LIEF) awarded in 2008, allowed the exploration of the Register as part of a federated information architecture to support historical scholarship in digital and networked environments. It involved the investigation of community based methods for populating the Register, as well as enabling the harvesting of its content into emerging national discovery services. With the National Library of Australia (NLA) as a key industry partner, a mechanism for harvesting Encoded Archival Context (EAC)Encoded Archival Context – Corporate bodies, Persons, and Families (EAC-CPF) is a metadata standard for the description of individuals, families and corporate bodies which create, preserve, use, are responsible for, or are otherwise associated with records. Its purpose is to standardize the encoding of descriptions of agents and their relationships to resources and to one another, to enable the sharing, discovery and display of this information. See http://eac.staatsbibliothek-berlin.de/  records from the Register was established for incorporation into their exciting new Trove discovery service,Trove is the National Library of Australia’s new discovery service, providing a single point of access to resources held in Australia’s memory institutions and incorporating rich contextual metadata from a variety of sources. See http://trove.nla.gov.au/. using the Open Archives Initiative Protocol for Metadata Harvesting (OAI-PMH).OAI Protocol for Metadata Harvesting (OAI-PMH) is a lightweight harvesting protocol for sharing metadata between services developed by the Open Archives Initiative. It defines a mechanism for harvesting metadata records from repositories based on the open standards HTTP (Hypertext Transport Protocol) and XML (Extensible Markup Language) in support of new patterns for scholarly communication. See http://www.openarchives.org/pmh/. The federated information architecture which such harvesting services make possible is aimed at increasing the productivity of all those associated with the creation, management and use of source material for historical research. As well as fostering the development of complicit systems, it is also about allowing a rich multiplicity and variety of voices to contribute their knowledge to resource discovery systems. It involves scholars' direct participation in resource description frameworks allowing their extensive, intimate and fine grained knowledge of sources and their relationships to areas of study to become part of networked information infrastructure. It also aims to provide a mechanism by which the flow of information about resources in and out of cultural institutions is improved, allowing researchers to discover, explore and make connections between materials held in disparate locations efficiently and effectively, and in turn to feed that knowledge back into the network.   As a pioneering e-Research initiative, the story of AWAP and the Australian Women’s Register (AWR) offers much insight into the establishment, evolution and sustainability of advanced scholarly information infrastructure to facilitate information intensive collaborative research in the humanities.Christine Borgman, Scholarship in the Digital Age: Information, Infrastructure and the Internet, MIT Press, Cambridge Massachusetts, 2007. It is illustrative of how digital and networking technologies change the roles and relationships of scholars, information professionals, universities and the wider community in order to build greater capabilities, connectedness, robustness and resilience into historical/archival/humanities information systems. Above all it asserts the value of scholarly principles, re-visioned, re-imagined and re-distributed for the digital and networked age, and it places women’s history firmly in the mainstream rather than being consigned to the margins. What began ten years ago as a small, community initiative aimed at securing the uncertain future of women’s archival records has developed into a project of national significance. The fact that it is a feminist project is entirely relevant to the story as well, given the distributed and partial nature of women’s archival collections and the historical circumstances of their production.   This paper will outline and review the development of the Australian Women’s Register, by discussing the problem of female under-representation in the archival record, explaining the implications of this for historical researchers and describing how the AWR works to harness information about existing records while it creates a new ‘community’ archive in cyberspace. There will be an emphasis on how it has and has not been able to address emerging requirements for e-Humanities infrastructure as articulated in reports such as Our Cultural Commonwealth; however, the focus will be on explaining how the successful development of any e-Humanities infrastructure is shaped by the strength of the collaboration between users and developers.American Council of Learned Societies, Our Cultural Commonwealth: The Final Report of the American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities & Social Sciences, 13 December 2006, 43 pp, http://www.acls.org/cyberinfrastructure/OurCulturalCommonwealth.pdf. It will discuss the content and technological developments undertaken as part of the ARC LIEF project, and reflect on the readiness of various stakeholders of the Register to take advantage of these capabilities and participate in the design and development of future ones.  ",
        "article_title": "Naming the unnamed, speaking the unspoken, depicting the undepicted: The Australian Women’s Register story",
        "authors": [
            {
                "given": " Joanne",
                "family": "Evans",
                "affiliation": [
                    {
                        "original_name": "The University of Melbourne, Australia",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            },
            {
                "given": " Helen",
                "family": "Morgan",
                "affiliation": [
                    {
                        "original_name": "The University of Melbourne, Australia",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            },
            {
                "given": " Nikki",
                "family": "Henningham",
                "affiliation": [
                    {
                        "original_name": "The University of Melbourne, Australia",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-16",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  \"Users of this or any edition are warned that the textual variants presented by citations from Plato in later literature have not yet been as fully investigated as is desirable\". This shortcoming, characterized by Kenneth Dover (Dover, 1980) is still existent and is unlikely to be corrected quickly by traditional research techniques. Textual reuse plays an important role in Classical Studies research. Similar to modern publications, classical authors used the texts of others as sources for their own work. In ancient texts, however, a less stronger form of word by word citation can be observed. Additionally, the complexity of ancient resources disallows fully manual research. From a bird's eye view there are different points of view to the problem of textual reuse implying different research interests (Büchler and Geßner, 2009):  A Computer Science perspective focuses on algorithms (technical view): Which algorithm is better than others? The scope of this research is wide ranging and also relates to plagiarism detection in modern texts like theses at universities (Potthast et al., 2009). A Historian is interested in more complex correlations (macro view). For this kind of work a dedicated user interface is necessary to figure out relations between e.g. chapters of a book and their citation usage on a timeline. The research interests of a Classical Philologist focus on the textual differences between the original text and its variants in citations (micro view). These varying requirements necessitate designing different user interfaces for these three kinds of researchers.   Within the eAQUA project we are investigating the reception of Plato as a case study of textual reuse in ancient Greek texts. Our research is carried out in two steps. On the technical level, we firstly extract word by word citations. This is achieved by combining syntactical ngram overlappings (Hose, 2009 and Büchler, 2008) and significant terms for several of Plato's works. In the second step the constraints on syntactic word order are relaxed. This is done by combining text mining and information retrieval techniques. A graph based approach is then introduced that can deal with free word order citations. The key concept is not syntactically based, but focuses on the semantic level to extract the relevant core information of a used citation. Then the information is represented as a formal graph that is similar to the Lexical Chaining approach (Waltinger et al. 2008) that is often used for text summarisation (Yu et al. 2007). On the one hand syntactical and semantic approaches are only used to select reuse candidates with a small set of uncommon matching words within a citation. On the other hand, a complete pairwise comparison of all of the nearly 5.5 million sentences in the TLG corpus would require approximately 1000 years due to the squared complexity of O(n2) that was used for example to compare the Dead Sea Scrolls with the Hebrew Bible (Hose, 2009). For this reason, an intelligent pre-clustering of relevant reuse candidates is needed. Such a divide and conquer strategy reduces the complexity dramatically. Whilst the second step only increases the degree of free word order, in the third step the algorithm is expanded by similarly used words like go and walk. Those candidates are computed by similar cooccurrence profiles. The three levels briefly described above are only one dimension of reuse exploration. Other relevant dimensions that will be discussed are the degree of preprocessing as well as the visualisation of textual reuse in terms of citations.  In the field of preprocessing the main focus lies on tokenisation (more active tokenisation is needed with ancient texts than on modern languages), normalisation (reducing all words internally to a lower-case representation without diacritics) and lemmatisation (reducing all words internally to a word's base form). This dimension can speed up the algorithm and also improves the results for strongly inflected languages like Ancient Greek.  Leaving the technical point of view of computer scientists, the research of Classicists includes both an application of a macro view for Historians as well as one for the micro view of Classical Philologists. The visualisation dimension of textual reuse is important since text mining approaches typically generate a huge amount of data that can't be explored manually. This is shown in Fig. 1. Whilst the light grey area marks Neoplatonism (about 5. AC) the grey ranges highlight Middle Platonism (about 2. AC). Taking Plato's Timaeus, one can clearly identify that both phases of Plato's reception (see Fig. 1 – top) are based on different \"chapters\" of Timaeus (bottom).     Fig. 1. Macro view: Screen of an interactive visualisation for citation usage. Citation distribution by Stephanus pages of Plato's Timaeus. The highest peak of the first picture is strongly correlated with the citation usage of the pages 27 to 42 of the second picture: Neo Platonism.    As Fig. 1 is of stronger interest for Historians, there is also a requirement for a visualisation for researchers from the field of Classical Greek Philology. As shown in Fig. 2, a visualisation highlighting the differences in citation usage is necessary. This is especially important if longer citations are investigated.      Fig. 2. Micro view: Highlighted differences of citations (green, orange) in relation to original text of Plato (blue). Top: The orange word highlights the same word but including a language evolution of about 10 centuries. Bottom: An included word (orange) in the citation is shown.    Additionally, it will be demonstrated how to detect different editions of the same original text. Such completely unsupervised approaches are important to investigate the scientific landscape of text digitisation. Furthermore, the relation to modern plagiarism detection will be given as well as the importance of building modern representative corpora since especially web corpora typically contain several duplicates of the same text.  In the evaluation section different results related to the comparison of various approaches on several text genres will be shown. An example of those results is given by contrasting citations of Plato's work with the textual reuse of the Atthidographers. Whilst citations of Plato can be extracted quite well by the syntactical approach even with very low similarity thresholds, the same approach works with an accuracy smaller than 20% for textual reuse of the Atthidographers.  Finally, results of a still in progress manual evaluation will be presented relating to the question of how and why a passage was cited.  ",
        "article_title": "Detection of Citations and Textual Reuse on Ancient Greek Texts and its Applications in the Classical Studies: eAQUA Project",
        "authors": [
            {
                "given": " Marco",
                "family": "Büchler",
                "affiliation": [
                    {
                        "original_name": "Natural Language Processing Group, Institute of Mathematics and Computer Science, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": " Annette",
                "family": "Geßner",
                "affiliation": [
                    {
                        "original_name": "Ancient Greek Philology Group, Institute of Classical Philology and Comparative Studies, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": " Gerhard",
                "family": "Heyer",
                "affiliation": [
                    {
                        "original_name": "Natural Language Processing Group, Institute of Mathematics and Computer Science, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": " Thomas",
                "family": "Eckart",
                "affiliation": [
                    {
                        "original_name": "Natural Language Processing Group, Institute of Mathematics and Computer Science, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-03",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " A virtual museum is a cyberspace in persistent virtual worlds, such as Second Life, for displaying digitalized heritage documents. Urban et. al. (2007) reported that over 150 sites in Second Life (SL) were developed for education and museum activities. Virtual museums in SL offer visitors opportunities to engage in opening art exhibitions, discuss with specialists, and enjoy exploring collections of the wide range of artifacts. Those artifacts displayed in the virtual museums vary from 3D documents of the world heritages to fictional creations (Rothfarb, R. and Doherty, P., 2007).  This paper aims at visualization and analysis of visitor behaviors in 3D virtual museums. Without loss of generality, we focused on a museum in Second Life, named Ritsumeikan Digital Archive Pavilion (RDAP) as shown in Figures 1-2. The museum was used in this paper for developing a prototype of our visualization and analysis tool. Efficient visualization of the user movement is very useful for analyzing his/her behaviors, in an implicit manner, in order to extract the disclosed information of individuals in the cyberspace. Applications of the proposed visualization method include the following:  The curators can design the exhibit space based on the majority of visitors as illustrated in Section 3.  The storytelling of an individual visitor can be expressed as a sequence of screenshots capturing the most favored exhibits (as described in Fujita, H. et al., 2008).  A guide system can be applied so as to achieve a satisfactory museum tour as introduced by Sookhanaphibarn and Thawonmas.      Figure 1. Kimono exhibition in Ritsumeikan Digital Archive Pavilion (RDAP)      Figure 2. Floor plan of RDAP with the locations of 19 Kimono objects denoted by a small solid square    Visualization and analysis of visiting patterns To validate our visualization approach, 36 avatars’ movements in RDAP were synthesized for obtaining four visiting styles. These styles, based on the metaphor of animal behavior, are ant, fish, grasshopper, and butterfly styles as follows (Veron, E. & Levasseur, M., 1983; Chittaro, L., 2004):  The ant visitors spend quite a long time to observe all exhibits and walk close to exhibits, but avoid empty spaces.  The fish visitors prefer to move to and stop over at empty spaces, but avoid areas near exhibits.  The grasshopper visitors spend a long time to see selected exhibits, but ignore the rest of exhibits.  The butterfly visitors observe almost exhibits, but spend varied times to observe each exhibit.      Local Visualization Tracing the user movement in Second Life was achieved by using the provided Linden Second Life script, named sensor function. The sensor function detects and reports the user position (x,y) within the particular range. It repeats every particular time interval. In this paper, the considered data consist of the three dimensional positions of an individual visitor and their corresponding time spent. Figure 3 shows four visiting paths in RDAP. The visualization consists of line segments and white dots. A colored line segment is a part of the avatar movement. A white dot represents the location of a Kimono object. The avatar’s path is displayed with the spectrum colors containing red, orange, yellow, green, cyan, blue, and violet. A path is in the form of connecting segments from red to violet. The session is equally divided in time into 7 periods in the ascending order from red to violet. The color of a particular segment indicates the passage of time. The length of a segment inversely represents the time spent. For example, the avatar as shown in Figure 3 (a) spent the longest time in the first period recognizable from the shortest red segment. The avatar started moving faster during the last two periods as denoted by longer blue and violet. In our visualization, the shorter a segment is, the longer an avatar spends time in that particular area.  Absent colors show that a longer time was spent than one period. For example, no red and orange segments are shown in Figure 3 (b) because the avatar spent time from the first period to the third near the entrance denoted by the yellow area. If an avatar spends too much time at a particular position exceeding the period length, then the color corresponding to that period will be skipped.    (a) Ant visiting style    (b) Fish visiting style      (c) Grasshopper visiting style    (d) Butterfly visiting style Figure 3: Visualization of four visiting styles in RDAP. These color figures will be provided in URL below. http://www.ice.ci.ritsumei.ac.jp/~kingkarn/     Analysis Our visualization approach can describe the aforementioned visiting styles. Ant, Fish, Grasshopper, and Butterfly visiting styles are displayed on totally different vivid graphic graphs. Hence, our tool is useful for distinguishing the visiting types.  The Ant visiting style is shown in Figure 3 (a). The avatar’s path was along the white dots. It means that the visitor walked close to Kimono objects in order to look at them in detail. The path contained its segments of nearly equal length, indicating that the visitor spent his/her time with the exhibits almost equally. The Fish visiting style is shown in Figure 3 (b). The avatar’s path was limited to the empty space between two exhibit rows. It means that this visitor preferred to stroll to take the atmosphere of the gallery. Most segments far from the white dots depict that the avatar did not pay attention to the Japanese art imposed on Kimonos. The Grasshopper visiting style is shown in Figure 3 (c). The avatar’s path was drawn as a triangle polygon having the smaller area than that of the Ant style. Its segments represent a kind of diagonally walking across the gallery to the interesting exhibits. The Butterfly visiting style is shown in Figure 3 (d). The avatar’s path was the longest path of the four styles. Its segments show plenty of diagonally walking across the gallery to most exhibits. The diagonal segment does not imply his/her preference, but this unorganized visit does not follow a well structured sequence like the Ant.   Global Visualization The global visualization of all visitors in each category is displayed in Figure 4. The methodology of this global visualization consists of  visiting style identification, trace accumulation of all users belonging to the same visiting style, and contrast enhancement in order to highlight the most popular route.   Using the synthesized data of 36 visitors, the global visualization of each visiting style can guide curators to rotate the museum items and arrange the sequence of items. Table 1 summarizes the interesting or skipped items associated with the visiting styles. An interesting item (I) can be determined if its observation area is darkened; otherwise, the item (S) is considerably ignored. The item numbers are those assigned in Figure 2. Curators can design an efficient exhibition based on the majority of visitors. It is assumed that our museums consist of four rooms each of which the majority of visitors are ant, fish, grasshopper and butterfly, respectively.  The ant room: the 13th item is possibly not related to others; therefore, a new one should be substituted. The fish room: the visitors prefer to pass slowly through the room. Therefore, all exhibits should be placed along both sides of the main path. The exit should be on either side of the entrance to prevent congestion.  The grasshopper room: half of the items are possibly not visitor attraction for busy people; on the other hand, they are perhaps varying and unrelated. Therefore, the curators should be re-design the exhibition room; in addition, the skipped items should be replaced with others more related to those visited. The butterfly room: the visit routes should be unorganized; therefore, the sequence of exhibits should be rearranged and some skipped items should be replaced/removed, accordingly.      (a) Common paths of ant visitors    (b) Common paths of fish visitors      (c) Common paths of grasshopper visitors    (d) Common paths of butterfly visitors Figure 4. Visualization of common paths based on four visiting styles, each of a group of seven visitors. Kimono exhibits and the user’s paths are denoted by red dots and blue lines. The color intensity indicates the frequency of visits. The darker blue the visualization shows, the more visitors spend time at that particular area.     Table 1. Example of visual analytics from Figure 4 showing the interesting and skipped items in RDAP are as denoted by “I” and “S”, respectively  Items Ant Grasshopper Butterfly   1 I S I   2 I S I   3 I I I   4 I I I   5 I I I   6 I S I   7 I I I   8 I S I   9 I S I   10 I I S   11 I S I   12 I I I   13 S I S   14 I S I   15 I S I   16 I I I   17 I I I   18 I S S   19 I I I     ",
        "article_title": "Visualization and Analysis of Visiting Styles in 3D Virtual Museums",
        "authors": [
            {
                "given": " Kingkarn",
                "family": "Sookhanaphibarn",
                "affiliation": [
                    {
                        "original_name": "Intelligent Computer Entertainment Laboratory Global COE Program in Digital Humanities for Japanese Arts and Cultures Ritsumeikan University Japan",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Ruck",
                "family": "Thawonmas",
                "affiliation": [
                    {
                        "original_name": "Intelligent Computer Entertainment Laboratory Global COE Program in Digital Humanities for Japanese Arts and Cultures Ritsumeikan University Japan",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-05-01",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Most computational stylistics methods were developed for authorship attribution, but many have also been applied to the study of style. Investigating Wilkie Collin's Blind Love (1890), left unfinished at his death and completed by Walter Besant from a long synopsis and notes provided by Collins, requires both authorship attribution and stylistics. External evidence indicates that Besant took over after chapter 48 (Collins 2003), which provides an opportunity to test whether Besant was successful in matching Collins's style and to investigate the styles of Collins and Besant. This divided novel also facilitates the comparison of two computational methods: the T-test and Burrows's Zeta. The t-test is a well-studied method for determining the probability of a difference between two groups arising by chance (a classic use in authorship and stylistics is Burrows 1992.) Here I use t-tests to identify words used very differently by Collins and Besant. After showing that those word frequencies accurately identify the change of authorship, I examine the words themselves for stylistically interesting characteristics. I created a combined word frequency list for four novels by Besant and three by Collins, then deleted words occurring only once or twice, personal pronouns (too closely related to the number and gender of characters), all words with more than 90% of their occurrences in one text (almost exclusively proper names), and words limited to one author (required for t-testing). I divided the novels into 167 4,000-word sections, and performed t-tests for the remaining 6,600 words (using a Minitab macro). I cleaned up the results and sorted them on the p value in Excel (with another macro), and retained only the 1719 words with p < .05, about 1,000 for Collins and 700 for Besant (see  for detailed instructions and the macros). I tested these words on six new texts for each author, a novel and five stories for Besant and six novels for Collins. Beginning with the 500 most distinctive words for each author, I deleted a few words that were absent from these texts and used the remaining 993 words to perform a cluster analysis (Fig. 1). (To keep the graph readable, I divided the novels into 10,000- word sections, retaining only half the sections.) Obviously, these marker words are quite characteristic of the authors.   Fig. 1. Besant versus Collins: Cluster Analysis    When sections of Blind Love are tested along with the texts above, the authorship change after chapter forty-eight is starkly apparent (Fig. 2). This graph is based on the sums of the frequencies of the 500 most distinctive words for each author in each section. (The texts are divided into 1,000-word sections; only a few sections of the novels are shown; the frequencies of Collins's marker words are multiplied by -1 for clarity.) Although Besant was working from extensive notes, his style is distinctly different. Had we not known which was Besant's first chapter, these t-tested marker words would have easily located it.   Fig. 2. Besant, Collins, Blind Love: T-tested Marker Words    Because the styles of Collins and Besant are so distinct, these marker words should also characterize them. Consider the twenty most distinctive words for each author: Besant: upon, all, but, then, and, not, or, very, so, because, great, thing, things, much, every, there, man, everything, is, well Collins: answered, to, had, Mrs, on, asked, in, Miss, mind, suggested, person, resumed, excuse, left, at, reminded, creature, inquired, reply, when  Obviously, more of Besant's words are high frequency function words, and many Collins words are related to speech presentation (answered, asked, inquired, resumed, suggested, reply, and reminded). The presence of added, begged, declared, exclaimed, explained, expressed, muttered, rejoined, and said as likely speech markers among the other Collins marker words, but only gasped, groaned, murmured, replied, and stammered for Besant, suggests they have different ways of presenting speech. Sorting all of each author's marker words alphabetically immediately reveals word families that each author favors, as thing, things, and everything among the twenty most distinctive Besant words already suggests (anything and nothing are also Besant markers). His every and everything are joined by everybody and everywhere; anything by any and anywhere; nothing and not by never, no, nobody, none, and nor; and much by more, moreover, most, and mostly among his markers. Collins's answered is joined by answer, answering, and unanswerable; and five of his twenty words are joined by two others: ask, asked, asks; inquired, inquiries, inquiry; leave, leaving, left; person, personally, persons; suggest, suggested, suggestion.  About 600 of the 1,700 distinctive words form groups favored by one author, but only about 175 form split groups, many of which fall into intriguing patterns. Collins uses more contractions, so didn't, doesn't, and don't are Collins words, but did and does are Besant words, and similarly for must, need, should, and would and their negative contractions. The singular and possessive forms of brother, friend, sister, and son are Collins's words and the plural forms are Besant's; the singular vs. plural pattern continues almost without exception in split noun groups. Verbs in -ing are Collins words and 3rd singular present forms Besant's. Finally, all nineteen cardinal number marker words are Besant's, including the numbers one to ten (note that Besant's preferred plural nouns often follow numbers). This extraordinary patterning may not seem particularly surprising, but, so far as I know, it has never been noticed before, and cries out for investigation. Two problems with t-testing are its privileging of relatively uninteresting high-frequency words and its inability to cope with words absent from one author. John Burrows's Zeta addresses both of these problems (Burrows 2006). (The specific form used here was developed by Hugh Craig (Craig and Kinney, 2009); for an automated spreadsheet and instructions for performing Zeta analysis see ). Zeta's simple calculation begins with the same novels and the same word frequency list used for the t-test, except that personal pronouns and words present in only one author are now included. Zeta is simply the sum of the proportions of Collins sections in which each word occurs and Besant sections in which it does not. Here answered, the most distinctive Collins word (as in the t-tests), has a Zeta score of 1.8, and is present in 89 of 90 Collins sections and absent from 65 of 77 Besant sections. The most distinctive Besant word is again upon, present in all 77 Besant sections and absent from 25 of 90 Collins sections, with a Zeta of 0.28. Below are the twenty most distinctive Zeta words (those also identified by t-testing in bold): Besant: upon, fact, presently, therefore, however, everything, real, whole, cannot, though, rich, none, thousand, except, fifty, ago, because, papers, also, twenty Collins: answered, Mrs, Miss, excuse, suggested, resumed, reminded, doctor, inquired, creature, notice, circumstances, tone, idea, temper, object, sense, feeling, governess, impression As noted above, Zeta marker words are less frequent than t-tested words. Only two Zeta marker words rank in the top 100 in the novels, compared to 20 of the t-tested words. About 3/4 of the 1000 t-tested marker words are also among the 1000 Zeta markers. Among the 2000 Zeta words are 275 words occurring in only one author; 59 form new single-author families, 27 join existing single-author families, and only 21 form split families. The Zeta words also effectively detect the change of authorship in Blind Love. In the scatter graph in Fig. 3, the axes show the percentages of all the word types (unique words) in each section that are Besant or Collins marker words (longer texts are divided into 4000-words sections; the labels for even-numbered Collins sections of Blind Love are removed; only a few sections of other novels are included). Note how distinct Besant's chapters of Blind Love are from Collins’s, though many of them are pulled toward Collins. This graph also includes The Case of Mr. Lucraft (Case in bold), jointly written by Besant and James Rice; it suggests, as has been argued (Boege 1956: 251-65), that Besant did most of the actual writing.   Fig. 3. Besant, Collins, Blind Love: Zeta Analysis    T-tests and Zeta analysis are both effective authorship attribution methods that produce lists of characteristic vocabulary for the authors being compared. Both identify morphological and semantic families of words and uncover extraordinarily consistent patterns and puzzling inconsistencies that suggest new directions for literary and stylistic analysis.  ",
        "article_title": "Teasing Out Authorship and Style with T-tests and Zeta",
        "authors": [
            {
                "given": " David L.",
                "family": "Hoover",
                "affiliation": [
                    {
                        "original_name": "New York University, USA",
                        "normalized_name": "New York University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0190ak572",
                            "GRID": "grid.137628.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2009-04-16",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Markup languages based on SGML and XML provide reasonably fine control over the syntax of markup used in documents. Schema languages (DTDs, Relax NG, XSD, etc.) provide mature, well understood mechanisms for specifying markup syntax which support validation, syntax-directed editing, and in some cases query optimization. We possess a much poorer set of tools for specifying the meaning of the markup in a vocabulary, and virtually no tools which could systematically exploit any semantic specification. Some observers claim, indeed, that XML and SGML are “just syntax”, and that SGML/XML markup has no systematic semantics at all. Drawing on earlier work (Marcoux et al., 2009), this paper presents two alternative and complementary approaches to the formal representation of the semantics of TEI Lite: Intertextual semantics (IS) and Formal tag-set descriptions (FTSD). RDF and Topic Maps may appear to address this problem (they are after all specifications for expressing “semantic relations,” and they both have XML transfer syntaxes), but in reality their focus is on generic semantics — propositions about the real world — and not the semantics of markup languages. In practice, the semantics of markup is most of the time specified only through human-readable documentation. Most existing colloquial markup languages are documented in prose, sometimes systematically and in detail, sometimes very sketchily. Often, written documentation is supplemented or replaced in practice by executable code: users will understand a given vocabulary (e.g., HTML, RSS, or the Atom syndication format) in terms of the behavior of software which supports or uses that vocabulary; the documentation for Docbook elevates this almost to a principle, consistently speaking not of the meaning of particular constructs, but of the “processing expectations” licensed by those constructs. Yet a formal description of the semantics of a markup language can bring several benefits. One of them is the ability to develop provably correct mappings (conversions, translations) from one markup language to another. A second one is the possibility of automatically deriving facts from documents, and feeding them into various inferencing or reasoning systems. A third one is the possibility of automatically computing the semantics of part or whole of a document and presenting it to humans in an appropriate form to make the meaning of the document (or passage) precise and explicit. There have been a few proposals for formal approaches to the specification of markup semantics. Two of them are Intertextual Semantic Specifications, and Formal Tagset Descriptions. Intertextual semantics (IS) (Marcoux, 2006; Marcoux & Rizkallah, 2009) is a proposal to describe the meaning of markup constructs in natural language, by supplying an IS specification (ISS), which consists in a pre-text (or text-before) and a post-text (or text-after) for each element type in the vocabulary. When the vocabulary is used correctly, the contents of each element combine with the pre- and post-text to form a coherent natural-language text representing, to the desired level of detail, the information conveyed by the document. Although based on natural language, IS differs from the usual prose-documentation approach by the fact that the meaning of a construct is dynamically assembled and can be read sequentially, without the need to go back and forth between the documentation and the actual document. Formal tag-set descriptions (FTSD) (Sperberg-McQueen et al., 2000) (Sperberg-McQueen & Miller, 2004) attempt to capture the meaning of markup constructs by means of “skeleton sentences”: expressions in an arbitrary notation into which values from the document are inserted at locations indicated by blanks. FTSDs can, like ISSs, formulate the skeleton sentences in natural language prose. In that case, the main difference between FTSD and ISS is that an IS specification for an element is equivalent to a skeleton sentence with a single blank, to be filled in with the content of the element. In the general case, skeleton sentences in an FTSD can have multiple blanks, to be filled in with data selected from arbitrary locations in the document (Marcoux et al., 2009). It is more usual, however, for FTSDs to formulate their skeleton sentences in some logic notation: e.g., first-order predicate calculus or some subset of it. Three other approaches, though not directly aimed at specifying markup semantics, use RDF to express document structure or some document semantics, and could probably be adapted or extended to serve as markup semantics specification formalisms. They are RDF Textual Encoding Framework (RDFTef) (Tummarello et al., 2005) (Tummarello et al., 2006), EARMARK (Extreme Annotational RDF Markup) (Di Iorio et al., 2009), and GRDDL (Gleaning Resource Descriptions from Dialects of Languages) (Connolly, 2007). RDFTef and EARMARK both use RDF to represent complex text encoding. One of their key features is the ability to deal with non-hierarchical, overlapping structures. GRDDL is a method for trying to make parts of the meaning of documents explicit by means of an XSLT translation which transforms the document in question into a set of RDF triples. GRDDL is typically thought of as a method of extracting meaning from the markup and/or content in a particular document or set of documents, rather than as a method of specifying the meaning of a vocabulary; it is often deployed for HTML documents, where the information of most immediate concern is not the semantics of the HTML vocabulary in general, but the implications of the particular conventions used in a single document. However, there is no reason in principle that GRDDL could not be used to specify the meaning of a markup vocabulary apart from any additional conventions adopted in the use of that vocabulary by a given project or in a given document. If proposals for formal semantics of markup are scarce, their application to colloquial markup vocabularies are even scarcer. Most examples found in the literature are toy examples. A larger-scale implementation of RDFTef for a subset of the TEI has been realized by Kepler (Kepler, 2005). However, as far as we know, no complete formal semantics has ever been defined for a real-life and commonly used colloquial vocabulary. This paper reports on experiments in applying ISSs and FTSDs to an existing and widely-used colloquial markup vocabulary: TEI Lite. Developing an ISS and an FTSD in parallel for the same vocabulary is interesting for at least two reasons. First, it is an opportunity to verify the intuition expressed in Marcoux et al., 2009 that working out ISSs and FTSDs involves much the same type of intellectual effort. Second, it can give insight into the relative merits and challenges of natural-language vs logic-based approaches to semantics specification. The full paper will focus on the technical and substantive challenges encountered along the way and will describe the solutions adopted. An example of a challenge is the fact that TEI Lite documents can be either autonomous or transcriptions of existing exemplars. Both cases are treated with the same markup vocabulary, but ultimately, the meaning of the markup is quite different: in one case, it licences inferences about the marked-up document itself, while in the other, it licences inferences about the exemplar. The work reported in Sperberg-McQueen et al., 2009 on the formal nature of transcription is useful here to decide how to represent statements about the exemplar, when it exists. However, the problems of determining whether any particular document is a transcription or not, and of putting that fact into action in the generation of the semantics remain. One possible solution is to consider as external knowledge the fact that the document is a transcription. In the FTSD case, that external knowledge would be represented as a formal statement that could then trigger inferences about an exemplar. In the ISS case, it would show up as a preamble in the pre-text of the document element. Another solution is to consider the transcription and autonomous cases as two different application contexts of the vocabulary, and define two different specifications. The benefits and disadvantages of the two solutions will be discussed. Follow-on work will include developing a GRDDL specification of TEI Lite, and comparing it to the ISS and FTSD. It will also include the elaboration of tools to read TEI Lite-encoded documents and generate from them either a prose representation of the meaning of the markup (from the ISS) or a set of sentences in a formal symbolic logic (from the FTSD). We also expect to induce a formal ontology of the basic concepts appealed to by the three formalisms and attempt to make explicit some of the essential relations among the concepts in the ontology: What kinds of things exist in the world described by TEI Lite markup? How are they related to each other?  ",
        "article_title": "Two representations of the semantics of TEI Lite",
        "authors": [
            {
                "given": " C. M.",
                "family": "Sperberg-McQueen",
                "affiliation": [
                    {
                        "original_name": "Black Mesa Technologies LLC, USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Yves",
                "family": "Marcoux",
                "affiliation": [
                    {
                        "original_name": "Université de Montréal, Canada",
                        "normalized_name": "University of Montreal",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0161xgx34",
                            "GRID": "grid.14848.31"
                        }
                    }
                ]
            },
            {
                "given": " Claus",
                "family": "Huitfeldt",
                "affiliation": [
                    {
                        "original_name": "Department of Philosophy, University of Bergen Norway",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-03-29",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This presentation is based on software interface development by a team at the University of California, Berkeley. The database which was used for this technology is the digital version of the Korean Buddhist canon written in Chinese characters. The tool shown was built with the help of a two year grant of support (2007-2009) from the National Science Foundation. International collaboration has included the Institute of Tripitaka Koreana in Seoul who provided scanned images of rubbings taken from the original printing blocks at Hae-in Monastery. The software metadata is based on the previous publication The Korean Buddhist Canon: A Descriptive Catalogue (Lancaster, 1979).Lewis Lancaster and Sungbae Park. The Korean Buddhist Canon: A Descriptive Catalogue. Berkeley: University Press. 1979. A digital version of this catalogue was made by Charles Muller of Tokyo University who has made it freely available on the internet (Muller, 2004).See  for the digital version of the The Korean Buddhist Canon: A Descriptive Catalogue on his server in Tokyo. (2004). The project has been a part of the Electronic Cultural Atlas Initiative (ECAI) and received support from that group’s Atlas of Chinese Religions research funded by the Luce Foundation. This atlas is being constructed in collaboration with the GIS Center at Academia Sinica in Taiwan and will provide references to the place names associated with the production of the translations and compilations included in the canon. Continued research on developing the software is being done in cooperation with the School of Creative Media and the Department of Chinese Translations Linguistics at City University of Hong Kong. It is important to understand that no project of this kind could possibly be undertaken without these multiple and widespread collaborations. In the example being described in this presentation, we use the software to focus on the digital version of the 13th century Korean printing block edition of the Buddhist canon (Lancaster, 1996).A description of this canon is found in my article \"The Buddhist Canon in the Koryo Period,\" Buddhism in Koryo: A Royal Religion, edited by Kikun Suh and Chai-shin Yu. Korea Research Monograph #21, Institute of East Asian Studies, University of California: Berkeley, 1996. The canon, represented on blocks, contains more than 52 million characters/glyphs carved onto 166,000 surfaces each producing a page of text when printed. The number of lines, containing up to 14 glyphs, on the plates number over three million. The entire set of the canon is divided into 1,514 different texts representing dated translations and compilations made over a period of seven centuries. The size of the data, the temporal span of its composition, and the history of acquisition in Korea of the hundreds of texts from China, provide us with a reasonable challenge for the interface design. The previous approach to the study of this canon was the traditional analytical one of close reading of specific examples of texts followed by a search through a defined corpus for additional examples. When confronted with 166,000 pages, such activity had to be limited. As a result, analysis was made without having a full picture of the use of target words throughout the entire collection of texts. That is to say, our scholarship was often determined and limited by externalities such as availability, access, and size of written material. In order to overcome these problems, scholars tended to seek for a reduced body of material that was deemed to be important by the weight of academic precedent. In the current digital age, however, the limits on “what can be considered” in the Korean Buddhist canon have been significantly removed. We can consider all of the texts, all of the words, and all of the metadata in every search. Consequently, the practices of traditional scholarship for the canon have begun to falter. When the entire canon had been digitized in the last decade of the 20th century, the process of search and retrieval of target words and phrases was transformed. Nonetheless, problems remain for Buddhist scholars using this digital version. In many cases, the menu which appears after a search of a term can contain thousands of references. The references presented as a display of each line where the word occurs can still occupy long hours of time to analyze and put into some form of presentation. We are in need of new ways to display search results that will allow scholars to quickly perceive such things as the patterns of occurrences, examples of clustering, view of target words with adjacent companion words, graphic models of profiles of sequence, computation of occurrences not only for the whole of the set but also broken down by text and date. The displays give the researcher aids in evaluating and analyzing the patterns for each word. As a first step, we look for the number of times that each of the characters appears in the canon. As each search is made, a report appears in visual form on a “ribbon” of blue dots, where each of the dots represents one of the 52 million glyphs. The “ribbon” is more than a picture for each “blue dot” has 35 fields of metadata behind it. It is marked for exact placement on the original printing block, date of translation, name of translator for the text in which it appears, UNICODE number, place of translation, name of text containing the example, etc. The dots are arranged by “panes” that correspond to the more than 160,000 pages of the version preserved in Korea. The dot is an abstract image that permits the user to see patterns of occurrence without the barrier of complex display of natural language glyph constructions (Lancaster, 2009).See examples of the interface and report of progress in my report to NSF: SGER: Text Analysis and Pattern Detection: 3-D and Virtual Reality Environments (2009). http://ecai.org/textpatternanalysis/ It is at this first step that we note the distinct shift in methodology. The initial move on the part of the scholar, who uses this interface, is to turn directly to the data itself rather than to reference works. This is accomplished because the software provides a process of searching through the entire set of data at once. There is no step of consulting a reference work such as a concordance before proceeding to the text itself. This visual becomes the first factor in the scholar’s “work flow” planning. It shows whether the glyph(s) are scattered throughout the canon, whether there are heavy concentrations in a few places, whether there are only a few examples that appear in widely separated examples in terms of texts, time, and translators. Securing this much information within a few seconds can be compared to the hours of effort it would take to construct such an analysis of patterning, even with an internet search for each example of the term. In every case, the “blue ribbon” and other graphics are displaying a large amount of data in the visual form. We can “see” the occurrences of our target search within the 52 million glyphs and immediately understand the nature of the patterning. Words in the canon have a history and we can begin to spot the ways in which they evolve. Since each dot has multiple metadata fields, they need not be seen only in the sequence of the canonic arrangement. They can be rearranged by time of translation, by translator, or place of translation. The visuals will quickly show that some words grow in number of occurrences over time and others fade into a more marginal role. There have been surprises from the use of the tool on the canonic words. The computation for an important expression can help us identify apocryphal texts, or texts that show the characteristics of translation rather than compilation. “Companion” words that become associated with a target word can be used to identify the primary meaning of an occurrence. Future plans call for the exploration of making this tool multi-lingual and provision for having it available as an open source and free add-on to datasets. Funding  This material is based upon work supported by the National Science Foundation under Grant No. 0840061. Support for the Atlas of Chinese Religions given by grants from the Henry Luce Foundation, Inc. New York. Support for Atlas of Chinese Religions provided in part by the GIS Center of Academia Sinica, Taiwan.   ",
        "article_title": "From Text to Image to Analysis: Visualization of Chinese Buddhist Canon",
        "authors": [
            {
                "given": " Lewis",
                "family": "Lancaster",
                "affiliation": [
                    {
                        "original_name": "University of California, Berkeley USA",
                        "normalized_name": "California Coast University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05t99sp05",
                            "GRID": "grid.468726.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-03-29",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The use of GIS in historical research, Historical GIS, is now a well established part of the discipline of history. The field has evolved to an extent where it can be shown to have made a significant impact in delivering high-quality research in books and peer reviewed journals including the British Medical Journal, Annals of the Association of American Geographers, the American Historical Review, Journal of Economic History, and the Agricultural History Review. Most of these studies are, however, largely concerned with quantitative, social science-based approaches to historical research. This paper explores how approaches based on other sources such as texts and images can be used to allow GIS to be applied across the disciplines of the humanities. Early research is already suggesting that it can and indeed a new field, spatial humanities, is increasingly being recognised. This paper will explore one example of this approach focusing on how we can use GIS techniques to integrate historical texts and modern 'born-digital' photographs to gain a better understanding of landscape appreciation in the Lake District.  The paper starts by looking at two early tours of the Lake District, Thomas Gray's proto-Picturesque tour of 1769 and Samuel Taylor Coleridge's 'circumcursion' of 1802. We are currently working to extend this to include a subset of William Wordsworth's work. This project extracted place-names from these texts and matched then to a gazetteer to turn them into GIS form. The advantage of this approach is that once the GIS database has been created the spatial information in the texts can be mapped, re-mapped, queried, integrated with other material, and manipulated in a wide range of ways. The project produced a range of maps including: simple dot-maps of places mentioned, density smoothed maps that use techniques pioneered in epidemiology and crime mapping to summarise complex point patterns, and maps of emotional response to the landscape. Some of these were of the individual texts, some compared and contrasted the different texts. Other forms of analysis integrated data from other sources such as a Digital Elevation Model of the Lake District, and contemporary population densities. From these we were able to show that Gray followed the main valleys of the Lake District and stayed in towns overnight. He rarely travelled to heights of more than a few hundred feet although the higher peaks, those of over 2,500 feet, attract considerable attention in his writing. Coleridge, by contrast, avoided the populous parts of the Lake District, staying in the Western Fells and climbing Sca Fell, the highest mountain in England, among other things. While his ascend (and hair-raising descent) of Sca Fell is well known, what is more interesting is that much of his account is also concerned with time spent in low places, similar to Gray, and also that he names places of all heights, especially those between 1,000 and 2,000 feet which Gray almost completely ignores. The two tours barely overlap, the only place where they do significantly is Keswick, where Coleridge lived and Gray spent several nights, and the road over Dunmail Raise between Grasmere and Keswick although neither account has much to say about this part of their journey.  This approach takes us into what F. Moretti (2005) has termed 'distant reading,' a methodology that stresses summarising large bodies of text rather than focusing on a few texts in detail. We also wanted to explore whether GIS could help with more traditional approaches to reading. To this end we created a KML version of the GIS implemented in Google Earth. This placed a text on the bottom half of the screen with a Google Earth map on the top-half. Super-imposed on the map were the locations mentioned in the texts, which can be switched on and off in various ways, and a contemporary map showing the Lakes in 1815. This allows the reader to read the text while following the locations named using either Google Earth's modern arial photographs or the historical map as a backdrop. This therefore enriches the experience of close reading of the text by visualising and contextualising the places mentioned. Given the numbers of places mentioned by both authors even someone highly familiar with the Lake District is unlikely to be able to accurately locate all of these mentally. An alternative approach that this framework provides is for the user to click on a location and ask \"what have the different writers said about this place?\" To enrich this further we allow users to link from the site to the photographic website Flickr. Flickr allows people to upload and share their digital photographs. Users can tag these with metadata such as 'landscape' or 'mountain' and can also add 'geo-tags' a latitude and longitude that give the photo a location. Using these allows us to link from our texts to allow the user to see what people have photographed nearby.  The initial idea behind linking to Flickr was simply to demonstrate what the different areas of the Lake District looked like to an audience who might be unfamiliar with it, and thus to assist the in-depth reading. It became apparent however that there are pronounced geographies within Flickr – some areas are extensive photographed and some ignored, while the different tags that people place on their images also have pronounced geographies. As Wordsworth is claimed to have extensively influenced the way people today view the landscape, particularly in the Lakes, which poses the question \"is there are relationship between the geographies of Wordsworth's writing and the geographies of Flickr.\" Using the Flickr API we were able to extract the number of photographs geo-tagged to locations in cells of approximately 1km square across the whole of England. This could be done for all photos or those with specific tags such as 'mountain(s)' or 'tree(s).'  Mapping all photographs produces some interesting geographies, in particular, most photos seem to be taken in the urban centres or the main valleys. Minor roads such as that over the passes of Wrynose and Hardknott, also seem to encourage photography. It may be therefore that modern visitors to the Lake District, at least as represented by people who upload geo-tagged photographs to Flickr, follow a tour that is more like the Picturesque tours of Gray than the Romantic experiences of Coleridge or Wordsworth. In this way we are able to return to distant reading and to integrate two apparently incompatible sources: historical writings and modern digital photographs.  This paper thus demonstrates the potential of using geo-spatial approaches to integrate disparate and apparently incompatible sources. In it we have integrated historical texts, historical maps, modern environmental information giving information on the topography, statistical data from the census giving population densities, and born digital images from Flickr. By bringing them together we have been able to shed new light on a specific topic, landscape appreciation in the Lake District. The implications, however, are far broader. The amount of geo-referenced data available from multiple sources is increasing exponentially. This can be expected to continue particularly given the growth of user-generated content and the availability of techniques to automatically geo-reference texts. The challenge is to use these sources in innovative ways to shed new insights into research questions in the humanities. If this can be done successfully it will lead to a re-awakening of the importance of geography to the humanities.   ",
        "article_title": "GIS, Texts and Images: New approaches to landscape appreciation in the Lake District",
        "authors": [
            {
                "given": " Ian",
                "family": "Gregory",
                "affiliation": [
                    {
                        "original_name": "Lancaster University UK",
                        "normalized_name": "Lancaster University",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/04f2nsd36",
                            "GRID": "grid.9835.7"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-28",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This paper details work that has been conducted through a collaborative project between JSTOR, the Frick Collection and the Metropolitan Museum of Art. This work, funded by the Andrew W. Mellon Foundation, was to understand how auction catalogs can be best preserved for the long term and made most easily accessible for scholarly use. Auction catalogs are vital for provenance research as well as for the study of art markets and the history of collecting. Initially a set of 1604 auction catalogs, over 100,000 catalog pages, was digitised – these catalogs date from the 18th through the early 20th century. An auction catalog is a structured set of records describing items or lots offered for sale at an auction. The lots are grouped into sections – such as works by a particular artist, each of the sections are then grouped into a particular sale – this is the actual event that happened in the sale room, and then these sales are grouped together in the auction catalog. The auction catalog document also generally includes handwritten marginalia added to record other details about the actual transaction such as the sale price and the buyer. A repository was constructed – this holds and provides access to page images, optical character recognition (OCR) text and database records from the digitised auction catalogs. In addition a website was created that provides public access to the catalogs and automatically generated links to other collections. This site offers the ability to search and browse the collection and allows users to add their own content to the site. When searching a user may only be interested in a single item within a larger catalog, therefore, to facilitate searching the logical structure of the catalog needs to be determined in order to segment the catalog into items. The catalogs are extremely variable in structure, format and language, and there are no standard rules that can divide the catalog into the lots, sections and sales. Therefore, machine-learning techniques are used to generate the segmentation rules from a number of catalogs that have been marked up by hand. These rules are then applied to classify and segment the remaining catalogs. The focus of this paper is the research and creation of a system to automatically process digitised auction catalog documents, with the aim to automatically segment and label entities within and create a logical structure for each document. The catalogs processed are in an XML format produced from physical documents via an OCR process. The segmentation and assignment of entity types will facilitate, deep searching, browsing, annotation and manipulation activities over the collection. The ability to automatically label previously unseen documents will enable the production of other large scale collections where the hand labelling of the collection content is highly expensive or unfeasible. The catalog, sale, section, lot model requires that the content of the document be distributed between these entities, which are themselves distributed over the pages of the document. Each line of text is assigned to a single entity, whole entities may be contained within other entities (a logical hierarchy), and a parent entity may generate content both before and after its child entities in the text sequence. This hierarchical organisation differentiates the problem of automatically labelling auction catalog document content from other semantic labelling tasks such as Part of Speech labelling (Lafferty et al., 2001) or Named Entity Recognition (McCallum and Li, 2003). In these tasks the classes or states can be thought of as siblings in the text sequence, rather than as having hierarchical relationships. Hence, the digitisation of auction catalog documents may require a different set of procedures to that applied to, for example, the digitisation of Magazine collections (Yacoub et al., 2005) or scholarly articles (Lawrence et al., 1999). Although a particular document model is assumed throughout this work, the theory and tools detailed can be applied to arbitrary document models that incorporate hierarchical organisation of entities. Techniques that are successfully applied to other Natural Language Processing and document digitisation tasks may be applied to this problem. Specifically, we have developed task appropriate feature extraction and normalisation procedures to produce parameterisations of catalog document content suitable for use with statistical modelling techniques. The statistical modelling technique applied to these features, Conditional Random Fields (CRFs) (Sutton and McCallum, 2007), models the dependence structure between the different states (which relate to the logical entities in the document) graphically. A diagrammatic representation of the auction catalogue document model is given in figure 1a and an example of a model topology that might be derived from it is given in figure 1b. It should be noted that CRFs are discriminative models, rather than generative models like HMMs, a property that may be advantageous when such models are applied to NLP tasks (Lafferty et al., 2001).   (a) Document data    (b) FST transition grammar  Figure 1: Document model for an auction catalog and a Simple Finite State Transducer topology derived from it    Figure 2: FST transition grammar extended to incorporate start and continue states for each entity type   The application of such techniques to hierarchically structured documents requires the logical structure of a document to be recoverable from a simple sequence of state labels. The basic transition grammar shown in figure 1b is not appropriate for this task as it is impossible to differentiate concurrent lines of a single entity from concurrent lines from two entities of the same type and relationships between a single ‘parent’ entity and multiple ‘child’ entities. These issues may be addressed by using two states, a start and a continuation state, in the model, to represent content relating to each entity, as shown in figure 2. This modification allows the full logical structure to be recovered by post-processing the sequence of state labels. In order to train any automated statistical learning procedure, a suitable sample of the data set must be labeled, usually by hand, to provide a target for learning or a ground-truth for evaluation. Hand labelling an XML-based representation of a document, produced via an OCR process, can be a difficult and frustrating task. To facilitate the fast labelling of documents, and thereby maximise the quantity of ground-truth data that could be produced, a cross-platform document segmentation user application was developed, shown in figure 3, that is able to format the OCR data for display and allow the user to simply and rapidly mark-up each document. This application could easily be adapted for use with other data sets and is a useful output of this work that can be used independently. Initial experiments with the segmentation and labelling system were conducted on a hand labeled collection of 266 auction catalogs, comprising just less than 400,000 lines of text content. A detailed breakdown of the content of these documents  Figure 3: Document segmentation user application   is provided in table 1. These experiments compare a number of variants of the system and analyses are provided to aid in the selection of parameters for the system. The experiments are evaluated using both the retrieval statistic F-measure (Baeza-Yates and Ribeiro-Neto, 1999) and an adaption of the WindowDiff metric (Pevzner and Hearst, 2002). The system has been found to produce a satisfactory level of performance to facilitate the automated processing of auction catalog content. A breakdown of the results achieved by the system is given in table 2.   Table 1: Number of lines corresponding to each entity type  Entity Number of lines   start-Catalog 266   cont-Catalog 41,275   Catalog 41,541   start-Sale 556   cont-Sale 3,469   Sale 4,025   start-Section 4,109   cont-Section 19,415   Section 23,524   start-Lot 91,240   cont-Lot 229,051   Lot 320,291   Total 389,381   Analysis of the errors produced by the system show that, owing to the hierarchical nature of the document model, a small number of errors at critical points in the content can lead to a large number of subsequent, concurrent errors. Unfortunately, these critical points in the ground-truth sequences are represented by the smallest quantities of content in the collection and therefore may form the weakest parts of the model. However, this also means that a small number of corrections at these key points could enable the correction of a much larger number of errors in the output. This is a useful property, as one of the design goals of this system is to facilitate the integration of feedback from users of the digitised documents into the statistical model and thereby allow the segmentations produced to improve over time. The feedback is used to improve the model by preserving any confirmations or corrections, made by users, of the segmentation output from CRF model. This preservation is achieved by reapplying the model to the document, which has been partially labeled by users, and forcing it to pass through the user indicated states as it determines a new sequence of state labels. This allows a user to supply corrections of major segmentation errors, such as a missing high-level entity (e.g. a sale), or minor errors, such as a single mislabeled line belonging to a lot. A user supplied correction to a major segmentation error could correct the labelling of many lines, for example by opening the Sale at the correct point and allowing the model to  estimate weights for Sections and Lots thereafter, whereas minor corrections may be simply preserved so that they don't need to be reapplied to the re-segmented document.   Table 2: Results achieved by the CRF-based system calculated over 5-fold cross-validation of the hand-labelled dataset  Metric Score   WindowDiff 0.103   F1 cont-Catalog 0.709   F1 start-Lot 0.870   F1 cont-Lot 0.934   F1 start-Sale 0.444   F1 cont-Sale 0.347   F1 start-Section 0.483   F1 cont-Section 0.548    ",
        "article_title": "\"Any more Bids?\": Automatic Processing and Segmentation of Auction Catalogs",
        "authors": [
            {
                "given": " Kris",
                "family": "West",
                "affiliation": [
                    {
                        "original_name": "JSTOR, USA",
                        "normalized_name": "JSTOR",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02ktbzt22",
                            "GRID": "grid.431174.4"
                        }
                    }
                ]
            },
            {
                "given": " Clare",
                "family": "Llewellyn",
                "affiliation": [
                    {
                        "original_name": "JSTOR, USA",
                        "normalized_name": "JSTOR",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02ktbzt22",
                            "GRID": "grid.431174.4"
                        }
                    }
                ]
            },
            {
                "given": " John",
                "family": "Burns",
                "affiliation": [
                    {
                        "original_name": "JSTOR, USA",
                        "normalized_name": "JSTOR",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02ktbzt22",
                            "GRID": "grid.431174.4"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-26",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Software and data can be considered as digital organisms that function in a \"digital ecosystem\" of computers. The concept of ecology has been borrowed from biology by other disciplines for explaining or describing a variety of phenomena. In some cases ecology and other concepts from evolutionary theory are only used as metaphors; in other cases attempts have been made to apply an adapted version of the theory to the evolution of non-biological phenomena.  We think it makes sense to borrow the notions from evolutionary theory in thinking about digital longevity. In this paper we will explore the potential of Darwin's theory as an explanatory framework for digital survival.  The construction of a theoretical foundation serves to answer questions of why some criteria or characteristics will guarantee the \"survival\" of digital objects better than other ones. Taking an evolutionary view will also make clear that there is no such thing as digital permanence for eternity: some objects only have better chances to survive than other ones. In computing technology, there is a struggle of survival of the fittest going on. In this struggle, new technologies arise as modifications or adaptations of earlier technologies and the older ones die out when newer technologies are stronger or better suited for their tasks. The digital objects that are already in existence have to be adapted to the new technological surroundings, otherwise they become extinct.  Spencer originally coined the phrase \"survival of the fittest\" in 1864, drawing parallels between his ideas of economics and Darwin's theory of evolution, which is driven by \"natural selection\". With respect to digital objects, it is people who are actively or passively involved in making the selections, thus deciding which digital objects survive and which do not.  As electronic digital data can only be understood using computers and software to \"translate\" them into visible or audible form, the media and data formats that are specific for hard- and software change according to the same evolutionary principles. If we accept this view, we can start to ask ourselves: which characteristics (comparable to the \"genetic properties\" of living organisms) may influence the chances of digital survival of data? This is however not unproblematic, as it is typically with hindsight that we see which (traits in) a biological species have survived and how the evolutionary process took place. The explanatory power of evolution theory is a posteriori, not a priori. It may therefore be difficult to predict which traits are good for survival.  We will explore the possible use of the concept of evolvability, which is usually defined as the ability of a population of organisms to generate genetic diversity, hence giving a measure of an organism's ability to evolve. Maybe there is a parallel here with respect to digital objects. For instance, if we look at several formats for Microsoft Word (.doc, .rtf, .html, .xml (2003), .ooxml) then we see an increase in usability/interchangeability, and hence probably: evolvability.  We can break down the survival problem to questions concerning:  The physical attributes of the media (tape, disk, etc.) The media format (density, size, etc.) The data content (integrity of the bits and bytes) The data format (the structure of the bits and bytes) The metadata content (the substantial description of the data) The metadata format (the format in which the metadata is described) The interlinking (the degree to which data is linked both internally and externally); a web of interlinked information is an ecosystem of its own.   We will demonstrate how digital preservation strategies such as technology preservation, software emulation, and data migration fit in an overarching evolutionary framework. The ecological approach also shows that it makes no sense to try to express the time horizon for the preservation of digital objects as a specific or indefinite period of time, but that we can better think in terms of \"chances of survival\".  The evolutionary framework can be used to argue why certain attributes and formats are more likely to survive than others. Also, analogous to natural selection, we will make clear that there is no single \"best\" strategy for survival of digital data. Some factors simply increase the chances of digital longevity, whereas other factors reduce these chances. Good factors for longevity may be bad for other desired characteristics. For example: stripping executable information from data improves its longevity, but hinders its functionality. It may also be so that some factors are intensely ambiguous for longevity. We may now think that \"wrapping\" text in WordPerfect in the 1990s was (with hindsight) not so good for survival, and that packaging it in Microsoft Word seems acceptable. This is probably related to the status (or market dominance) of the software packages. Similarly, packaging data in SGML in 1990 might have been not so good, while packaging it in XML in 2009 seems excellent. In the end, the environment determines what was good and what was bad for longevity.  So, when the whole \"technological ecosystem\" changes, what was well adapted before the change may appear to be ill suited in the next technological phase. Digital preservation strategies can use the principle of digital selection in order to maximize the adaptation of digital objects to their environment, thus increasing their chances of digital longevity.  Whether it makes sense to apply evolution theory to digital curation can be studied by looking at a number of parallels in other scientific domains. We will deal briefly with attempts to use Darwin's ideas in the social sciences and in technology. In the social sciences the idea of a \"social ecology\" was already applied and empirically tested in the 1920s by, among others, Robert Park and Ernest Burgess of the \"Chicago School\" of urban ecology. With respect to man-created systems it is probably better to use the ideas on evolution by Lamarck. Lamarckism is the idea that an organism can pass on characteristics that it acquired during its lifetime to its offspring (also known as heritability of acquired characteristics or soft inheritance).  Several researchers have proposed that Lamarckian evolution may be accurately applied to cultural evolution. Human culture can be looked upon as an ecological niche-like phenomenon, where the effects of cultural niche construction are transmissible from one generation to the next. Ecological notions on the evolution of software, in which ideas and characteristics of programming languages compete with each other, have been formulated in information science. Inheritance is an important concept with an evolutionary basis.  The development of open source software has also been described as evolving in a Lamarckian fashion. Ensuring free access and enabling modification at each stage in the process means that the evolution of software occurs in the fast Lamarckian mode: each favourable acquired characteristic of others' work can be directly inherited.  Kauffman and Dennett point out the parallels between biological evolution and technological evolution. They distinguish two stages: (i) explosion of the number of greatly different designs when there are still many unoccupied niches; (ii) microevolution where the existing designs are optimised for competition in existing niches.  It is also useful to compare the selection and survival of digital information with that of analogue information. In both cases there is \"information selection\" and evolution. What makes the digital world so different from the analogue world?  Next we will treat a few examples of the evolution of computing technologies, software, file formats and data sets, which will illustrate how well evolutionary theory is suited for explaining what has happened empirically. It makes sense to look backwards and use evidence from the – still very short – historical evolution of computing technology since the 1940s and '50s. Can we still read the first image, the first word processor file, the first database, the first web page, email or pdf-file? If yes, how come this is still possible? If not, why? And how did the formats of those data types (probably equivalent to the taxonomical rank of the genus in biology) evolve, and which abandoned file formats (\"species\") can still be read today?  Maybe there is a manifestation of genotype/phenotype here, where the application can be considered as the phenotype. Applications struggle with each other in the \"econosphere\" of consumers. The surviving application dictates the data format. If there are two strong surviving applications in a domain, you get peer-to-peer data convertors. If there are many, weaker survivors, you get interchange formats, which are better for preservation. A familiar lesson is not to rely on monopolists.  Finally, there is a marked tendency among data curators to set criteria for \"trusted digital repositories\", the nature reserves of the digital world. On the basis of the evolutionary ideas expressed in this paper, it makes sense not to make such criteria too narrow. They should be chosen in such a way that they make use of the \"natural\" evolutionary processes of technology and digital objects, making sure that what is threatened by extinction can be rescued in an effective way.   ",
        "article_title": "The ecology of longevity: the relevance of evolutionary theory for digital preservation",
        "authors": [
            {
                "given": " Peter",
                "family": "Doorn",
                "affiliation": [
                    {
                        "original_name": "Data Archiving and Networked Services, Netherlands",
                        "normalized_name": "Data Archiving and Networked Services",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/008pnp284",
                            "GRID": "grid.500519.8"
                        }
                    }
                ]
            },
            {
                "given": " Dirk",
                "family": "Roorda",
                "affiliation": [
                    {
                        "original_name": "Data Archiving and Networked Services, Netherlands",
                        "normalized_name": "Data Archiving and Networked Services",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/008pnp284",
                            "GRID": "grid.500519.8"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-28",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The metre of the Roman comic poet Plautus (flourished ca. 200 B.C.) still leaves one mystified. Although the scientific work of the 19th and early 20th century has established a number of important rules and licences, the exact range of these laws and licences remains a matter of debate. Taking into account these many open questions it is not surprising that metrical studies (as well as the important editions) of Plautus still display a huge amount of discrepancy in their handling of Plautine metre. The specific problem consists of the large number of transmitted verses in the Plautine corpus and the great complexity and diversity of competing explanations of remarkable metrical phenomena. Therefore, until now the results of scholarship often fail to convince, since they are based either on a limited textual basis or deal with a specific metrical phenomenon from the perspective of a single law or licence without taking into account competing explanations. This paper will cover a wide range of previous research from both Classics (Lotman 2000) and literature (Garzonio, 2006) to several techniques in the field of Computer Science (Heyer et al., 2008 and Volk, 2007). Metric analyses can be already be computed on German poems with only a small set of rules (Bobenhausen, 2009). Results imply, however, that foreign words are especially difficult to handle. In contrast to this, ancient texts pose a different problem as lots of variations of an original often exist. For this reason metric analysis can be divided into three different tasks:  Task 1: Dealing with different variations and variations of variations (Andreev, 2009 and Rehbein, 2009). Within this paper, a primary version of a verse is defined by researchers from Classics. Differences of variants in relation to the primary version are highlighted as described in (Büchler et al., 2009In this paper the same visualisation is used to highlight differences of \"literal\" citations in a historic context. and Rehbein, 2009). The variance caused by transmissions of several authors is also important to consider, however, when working with fuzziness. Consequently a set of possible metric analysis annotations are suggested rather than just one result. Task 2: Applying a metric rule-set to a text corpus (Bobenhausen, 2009 and Fusi 2008). Within this research - similar to part of speech tagging (Heyer et al., 2003) – a set of rules is applied to text. However, only the most probable metric candidate is selected. In contrast to that research (Bobenhausen, 2009 and Fusi, 2008), the approach in this paper scores several possible metric analyses. Task 3: Training of a metric ruleset based on manually annotated data from researchers. Typically, a fixed set of rules is taken as presumed, however, new rules need to be added manually. This paper thus also focuses on the computation of new rules. The importance of this step is motivated by the Theory of Selective Perception. Based on this, new and uncommon rules are determined by a computer model that is both objective and independent rather than selective like a human being.   In the field of natural language processing the task of tagging text is quite similar to part of speech tagging (POS). Typically, for such a tagger a Hidden Markov Model (Heyer et al., 2008) is trained and is traversed by dedicated algorithms like the Viterbi algorithm (Heyer et al., 2008). However, the already mentioned fuzziness of text variants makes both the training and traversing steps difficult. Furthermore, in the training step it is necessary to observe data on a larger window than the typical memory of 2 or 3. This would increase the complexity drastically during the trainings phase. Within the applying phase the Viterbi algorithm is typically used (Heyer et al., 2003). This algorithm reduces all paths locally except the most probable one. In metric analysis however this assumption is quite critical since due to syllable fusion an senarius is not required to have 12 but can also consist of 17 or 18 syllables. Motivated by the aforementioned problems of existing approaches this paper describes a three step approach. In a first step possible syllables are computed. This is simply done by using training data. In contrast to German poems (Bobenhausen, 2009) the approach is aware of possible fusions of syllables. In the second step all possible combinations are computed instantly removing candidates that do not fulfil the metric requirements. The training itself is done by distance-based co-occurrences (Büchler, 2008) on metric tags. In the last step metric candidates are scored based on both the training data as well as the variance of the alternatively transmitted variances. All relevant candidates are selected by researchers of Classics. The remaining metric analysis is represented in a dedicated visualisation highlighting the differences of several variants to the primary version (Büchler, 2009 and Rehbein, 2009). As an outcome of this paper several results will be shown. Besides the difference visualisation both results and experiences in training and application of a metric model will be provided. Both the expected results and the developed software, which can be easily adapted to other ancient poets, will give an original input to the research community and motivate and enable further investigations in the same spirit. ",
        "article_title": "Objective Detection of Plautus' Rules by Computer Support",
        "authors": [
            {
                "given": " Marcus",
                "family": "Deufert",
                "affiliation": [
                    {
                        "original_name": "Department of Classics, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": " Judith",
                "family": "Blumenstein",
                "affiliation": [
                    {
                        "original_name": "Department of Classics, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": " Andreas",
                "family": "Trebesius",
                "affiliation": [
                    {
                        "original_name": "Department of Classics, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": " Stefan",
                "family": "Beyer",
                "affiliation": [
                    {
                        "original_name": "Natural Language Processing Group, Institute of Mathematics and Computer Science, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": " Marco",
                "family": "Büchler",
                "affiliation": [
                    {
                        "original_name": "Natural Language Processing Group, Institute of Mathematics and Computer Science, University of Leipzig, Germany",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-15",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In 2007, John Burrows identified three regions in word frequency lists of corpora in authorship attribution and stylometry. The first of these regions consists of the most frequent words, for which his Delta has become the best-known method of study. This is evidenced by a varied body of research with interesting modifications of the method (e.g. Argamon 2008; Hoover 2004, 2004a). At the other end of the frequency list, Iota deals with the lowest-frequency words, while \"the large area between the extremes of ubiquity and rarity\" (Burrows, 2007) is now the target of many studies employing Zeta (e.g. Craig, Kinney, 2009; Hoover, 2007).  Due to the popularity of the three methods it was only a matter of time before Delta (and, to a lesser extent, Zeta and Iota) were applied to texts in languages other than Modern English: Middle Dutch (Dalen-Oskam, Zundert, 2007), Old English (Garcìa, Martìn 2007) and Polish (Eder, Rybicki 2009). Delta has also been used in translation-oriented papers, including Burrowsís own work on Juvenal (Burrows, 2002) and Rybicki's attempts at translator attribution (2009).  It has been generally - and mainly empirically - assumed that the use of methods relying on the most frequent words in a corpus should work just as well in other languages as it did in English; this question was approached in any detail only very recently (Juola, 2009). We could not fail to observe that its success rates in Polish, although still high, fell somewhat short of its guessing rate in English (Rybicki 2009a). Also, the already-quoted study by Rybicki (2009) seemed to suggest that, in a corpus of translated literary texts, Delta was much better at recognising the author of the original than the translator. This justified a more in-depth look at the workings of Burrowsís method both in its \"original\" English and in a variety of other languages.  Methods In this study, a single major modification has been applied to the usual Delta process. Each analysis was made for the top 50-5000 most frequent words in the corpus - but then the 50 most frequent words would be omitted and the next 50-5000 words taken for analysis; then the first 100 most frequent words would be omitted, and so on. This was done with a single R script written by Eder; the script produced word frequency tables, calculated Delta and produced \"heatmap\" graphs of Delta's success rate for each of the frequency list intervals, showing the best combinations of initial word position in wordlist and size of window, including variations of pronoun deletion and culling parameters. Thus, in the resulting heatmap graphs, the horizontal axis presents the size of each wordlist used for one set of Delta calculations; the vertical axis shows how many of the most frequent words were omitted. Each of the runs of the script produced an average of ca. 3000 Delta iterations.   Material The project included the following corpora (used separately); each contained a similar number of texts to be attributed.   Code Language Texts Attribution   E1 English 65 novels from Swift to Conrad Author   E2 English 32 epic poems from Milton to Tennyson Author   E3 English 35 translations of Sienkiewicz's novels Translator   P1 Polish 69 19th- and early 20th-century novels from Kraszewski to Øeromski Author   P2 Polish 95 translations of 19th- and 20th-century novels from Balzac to Eco Author   P3 Polish 95 translations of 19th- and 20th-century novels from Balzac to Eco Translator   F1 French 71 19th- and 20th-century novels from Voltaire to Gide Author   L1 Latin 94 prose texts from Cicero to Gellius  Author   L2 Latin 28 hexameter poems from Lucretius to Jacopo Sannazaro Author   G1 German 66 literary texts from Goethe to Thomas Mann Author   H1 Hungarian 64 novels from Kemèny to Bródy Author   I1 Italian 77 novels from Manzoni to D'Annunzio Author     Results The English novel corpus (E1, Fig. 1) was the one with the best attributions for all available sample sizes starting at the top of the reference corpus word frequency list; it was equally easy to attribute even if the first 2000 most frequent words were omitted in the analysis - or even the first 3000 for longer samples. The English epic poems (E2, Fig. 2) had their area of best attributive success removed away from the top of the word frequency list, into the 1000th-2000th most-frequent-word region. Some successful attributions could also be made with a variety of wordlists around the 2000 mark, starting at the 1st most frequent word.   Figure 1. Heatmap of 65 English novels (percentage of correct attributions). Colour coding is from low (white) to high (black)    Figure 2. Heatmap of 32 English epic poems    The final \"specialist\" corpus in the English section of the project - 32 works by Polish novelist Henryk Sienkiewicz, translated into English by a number of translators (Fig. 3) - showed Delta's expected problems in translator attribution; however, for a variety of culling/pronoun deletion parameters, a small yet fairly consistent hotspot would appear for small samples if the first 2000-3000 words were deleted from the frequency wordlist. The first Polish corpus, that of 69 19th- and early 20th-century classic Polish novels (P1, Fig. 4), showed marked improvement in Delta success rate when the wordlist taken for attribution started at some 450 words down the frequency list; the most successful sample sizes were relatively small: no more than 1200 words long.  When the corpus of Polish translations was studied for original authorship (P2, Fig. 5), the results were quite accurate for many sample sizes up to 1800 from the very top of the frequency list. Delta was equally successful for samples of up to 1400 words beyond the 800th most-frequent-word mark. The same corpus yielded lower attribution success when studied for translator recognition (P3, Fig. 6). In fact, it resembled somewhat the graph for Polish classics: a small range of passable attributions, usually for samples below 1000, and usually better when starting a hundred or so words down the frequency rank list.   Figure 3. Heatmap of 35 English translations of Sienkiewicz's works    Figure 4. Heatmap of 69 Polish novel classics      Figure 5. Heatmap of 95 Polish translations from Balzac to Eco (autorship attribution)    Figure 6. Heatmap of 95 Polish translations from Balzac to Eco (translator attribution)    The French corpus proved almost equally difficult (F1, Fig. 7): Delta was very successful mainly for small-sized samples from the top of the overall frequency wordlist. In contrast, the graph for the German corpus (G1, Fig. 8) presented a success rate akin to that for the English novels, with a consistently high correct attribution in most of the studied spectrum of sample size and for samples beginning anywhere between the 1st and the 1000th word in the corpus frequency list.   Figure 7. Heatmap of 71 French novels    Figure 8. Heatmap of 66 German texts    Of the two Latin corpora, the prose texts (L1, Fig. 9) could serve as excellent evidence for a minimalist approach in authorship attribution based on most frequent words, as the best (if not perfect) results were obtained by staying close to the axis intersection point: no more than 750 words, taken no further than from the 50th place on the frequency rank list.   Figure 9. Heatmap of 94 Latin prose texts    Figure 10. Heatmap of 28 Latin hexameter poems    The other Latin corpus, that of hexameter poetry (L2, Fig. 10), paints a much more heterogeneous picture: Delta was only successful for top words from the frequency list at rare small (150), medium (700) and large (1700) window sizes, and for a few isolated places around the 1000/1000 intersection point in the graph. The corpus of 19th-century Hungarian novels (H1, Fig. 11) exhibited good success for much of the studied spectrum and an interesting hotspot of short samples at ca. 4000 words from the top of the word frequency list.   Figure 11. Heatmap of 64 Hungarian novels    Figure 12. Heatmap of 77 Italian novels    With the Italian novels (I, Fig. 12), Delta was at its best for a broad variety of sample sizes, but only when some 1000 most frequent words were eliminated from the reference corpus.   Conclusions    Standard Delta (i.e. applied to the most frequent words) provides the best results for authorial attribution in English and German prose.  The same procedures still yield acceptable results in other languages and in translator attribution. The success here can be improved by manipulating the number of words taken for analysis and by selecting the reference wordlists at various distances from the top of the overall frequency rank list. The differences in attributive success could be partially explained by the differences in the degree of inflection/agglutination of the languages studied, the strongest evidence of this being the relatively highest success rate in English and German.    ",
        "article_title": "Deeper Delta Across Genres and Languages: Do We Really Need the Most Frequent Words?",
        "authors": [
            {
                "given": " Jan",
                "family": "Rybicki",
                "affiliation": [
                    {
                        "original_name": "Pedagogical University, Krakow, Pedagogical University of Krakow Poland",
                        "normalized_name": "Pedagogical University",
                        "country": "Mozambique",
                        "identifiers": {
                            "ror": "https://ror.org/0331kj160",
                            "GRID": "grid.442441.3"
                        }
                    }
                ]
            },
            {
                "given": " Maciej",
                "family": "Eder",
                "affiliation": [
                    {
                        "original_name": "Pedagogical University, Krakow, Pedagogical University of Krakow Poland",
                        "normalized_name": "Pedagogical University",
                        "country": "Mozambique",
                        "identifiers": {
                            "ror": "https://ror.org/0331kj160",
                            "GRID": "grid.442441.3"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-07",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Thomas Sonnet de Courval's satirical work, the Satyre Menippée du mariage, was initially published in 1608. In 1609, an expanded version appeared, with the addition of a second satire, the Timethélie, ou Censure des Femmes. In 1621, the first satire appeared in a new edition titled Satyre sur les Traverses du Mariage, then in the following year, Sonnet de Courval published his Œvres Satyriques. The Œvres includes 12 satires, with the final six consisting of fragmented, re-organized and re-edited versions of the Satyre Menippée and the Timethélie. A new edition of the 1609 text was published in 1623, edited by the publisher and probably without Courval's consent. In 1627, two more editions of the Œvres Satyriques appeared. (Coste 340-341).  The Mariage sous L'ancien Régime project has already digitized the 1609 text and most of the 1621, and will be working on other editions in the future. Our objective is to produce a genetic edition of those parts of Courval's work which bear on marriage; in particular, we would like to map the process by which the original two satires (Menippée and Timethélie) were re-constituted as six satires in the Œvres Satyriques. Since the texts are lengthy (the 1609 text runs to nearly 3,000 lines), we have begun to investigate ways to automate this mapping to some degree, and in particular, methods of measuring similarity between two pieces of text. In particular, we needed to find a way to detect corresponding lines between two witnesses, even when those lines might have been both relocated and altered. The Universal Similarity Metric is a method of measuring the similarity of two sets of data based on Kolmogorov complexity. It is described in Vitanyi (2005) as \"so general that it works in every domain: music, text, literature, programs, genomes, executables, natural language determination, equally and simultaneously\" (1). A practical implementation of this metric can be achieved using data compression, according to the following formula, where x and y are the two pieces of data being compared:  NCD(x,y) = C(xy) - min(C(x), C(y))  ___________________ max(C(x), C(y))  (Vitanyi 2)    NCD is \"Normalized Compression Distance\", an expression of the similarity of x and y; C(x) and C(y) are the respective lengths of the two compressed inputs; and C(xy) is the length of the compressed concatenation of x and y. The resulting NCD is a value between 0 and 1, where proximity to zero indicates greater similarity. This metric has been widely used in the sciences; for instance, Krasnogor and Pelta (2004) describes its use to measure the similarity of protein structures, and Li et al. (2004) apply it to evolutionary trees and to building language trees based on text corpora. Its universality and simplicity suggested that it might be the ideal tool to discover correspondences between lines and line-groups at different points in two of Courval's texts. To test it, I have created a prototype application using Borland Delphi. These are some example values generated with the prototype (Figure 1):  Figure 1: Example comparisons showing NCD raw score using Borland's Pascal ZLib library (zlib 1.2.3). (Shakespearean lines taken from Quarto 1 and Folio 1, Internet Shakespeare Editions transcriptions.)  Text 1 Text 2 NCD Score   These two lines are absolutely identical. These two lines are absolutely identical. 0,0000000   The vndiscouered country, at whose sight The vndiscouered Countrey, from whose Borne 0,3673469   To be, or not to be, I there's the point, To be, or not to be, that is the Question: 0,4545455   To Die, to sleepe, is that all? I all: Whether 'tis Nobler in the minde to suffer 0,7234043    これは日本語です。  To sleepe, perchance to Dreame; I, there's the rub, 0,8392857    Scores below 0.5 appear to be strongly indicative of similarity, while those over 0.6 usually signify disparity; it is actually quite difficult to generate any score above 0.84, as shown by the final example, in which there are no points of similarity at all. The prototype application takes two XML files as input, and performs the following steps:  Identifies the target elements to be compared (this is currently hard-coded, but would ideally be based on user-specified XPath). Adds @id attributes to any of those elements which don't yet have them. Extracts the text of all the target elements, and normalizes it in a variety of ways (specified by the user). Compares each single text item with each other text item, and generates a comparison score for it. (Optional) Runs an additional contextualizing algorithm which modifies the original scores based on the scores of surrounding elements in the document (see below). Sorts the matches in ascending order of similarity score (best matches first). Presents each of these matches to the user for categorization as one of:  Corresponding (equivalent) items Not corresponding, but an interesting relationship No relationship at all   Saves an XML file containing the scores for all matches, along with any categorization values chosen by the user. Saves copies of the input files with the added @id attributes, and also a CSV version of the score data for use in a spreadsheet program.   This correspondence data can be used to provide a component of a critical apparatus attached to a witness, linking it to corresponding lines in other witnesses. Step 5 is an attempt to detect correspondences between lines in situations where a line has changed significantly, or perhaps even been replaced completely, but the lines around it still match closely. Normally, where lines differ, a high score will be generated; if the user chooses only to examine the lower scores in the search for correspondences, the link between these two lines may not be detected. The contextualizing algorithm massages the score of each match such that it is affected by the score of the lines around it; if the preceding and following lines have low scores, the score of the line is lowered so that it too may be detected more easily as \"corresponding\", even though it has been substantially changed. The contextualizing algorithm can be run many times if required, massaging the scores each time. We are still investigating the outcomes and value of this process. These screenshots (Figures 2 and 3) show the prototype in use.  Figure 2: The main window of the prototype, showing the two input texts, and the Pre-comparison processing settings.     Figure 3: Reviewing matches between lines based on score.    This application is in many ways similar to the TEI-Comparator project created as part of the Holinshed Project. This is described in some detail in Cummings (2009). When writing our Mariage prototype at the beginning of 2009, I was unaware of the TEI-Comparator project; in addressing similar problems, we have arrived at remarkably similar solutions, especially in terms of process. The comparison algorithm used by TEI-Comparator, which is called Shingle Cloud, was developed by Arno Mittelbach, and uses a completely different process of comparison based on n-grams. Documentation for TEI-Comparator will be available soon; when it appears, I am looking forward to running tests to compare results between Shingle Cloud and the Universal Similarity Metric, and I will report the results in this paper. The prototype application will probably now be ported to C++, to create a cross-platform application, and I also intend to create a standalone Java library that can be called from the command line or from another Java application; perhaps it might be integrated into TEI-Comparator as an alternative comparison metric.  ",
        "article_title": "Using the Universal Similarity Metric to Map Correspondences between Witnesses",
        "authors": [
            {
                "given": " Martin",
                "family": "Holmes",
                "affiliation": [
                    {
                        "original_name": "University of Victoria Canada",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-03-23",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper will demonstrate an advanced work in progress, the digitised manuscript and transcription of Samuel Beckett’s novel Watt (composed in 1941-45 and first published in 1953). Discussion of the project will centre upon the digital resources buttressing the presentation of manuscript material and a range of related analytic features, and will outline some of the more significant ways in which specifically digital treatment of the material opens up new lines of literary and textual analysis. Indeed, some foundational concepts of textuality come into sharp focus by virtue of digital treatment of textual materials. Some of these concerns will be illustrated by way of examples taken from the Watt project, and by a fuller view of the complex relationship between text and manuscript arising from the project.  SBDMP and the Digitised Edition of the Watt Notebooks The digital and scholarly resources required to produce a digitised literary transcription are not trivial. Two related questions must frame any such project: what scholarly need is being met by the production of such an edition? What specific innovations are made available by virtue of its digital delivery? The digital transcription of Beckett’s Watt is an instalment of a larger international project – the Samuel Beckett Digital Manuscript Project – which aims to have all of Beckett’s literary manuscripts transcribed and represented in digital form. This initiative responds to a profound deepening of scholarly interest in modernist manuscripts as potential sources of literary hermeneutic attention, and in concert with this focal shift, a renewed interest in theories of textuality and textual criticism. The specific (and heightened) relevance to this particular text in Beckett’s oeuvre is immediately apparent on viewing the complex series of heavily revised and illustrated notebooks that constitute the manuscript of Watt. The primary materials do not lend themselves easily to conventional print publication, and indeed several dominant textual features would be lost or deeply submerged within any codex structure. For example, the relationships between dispersed narrative episodes and fragments within the manuscripts cannot be represented adequately in the linear structure of the codex, nor the complex patterns of transmitted, dispersed and submerged material between the manuscript and the published editions of Watt. Of all of Beckett’s major texts, Watt has received the least critical attention, despite significant scholarly curiosity regarding the deep ambiguity of the published narrative and the baroque nature of its manuscript archive.J. M. Coetzee described the Watt manuscript material and hypothesised its stages of composition in his PhD dissertation over thirty years ago at the University of Texas at Austin. An epitome of this description and analysis was published in his essay, \"The Manuscript Revisions of Beckett's Watt,\" JML 2.4 (1972): 472-480. Other discussions include: Sighle Kennedy, \"‘Astride of the Grave and a Difficult Birth’: Samuel Beckett’s Watt Struggles to Life,\" Dalhousie French Studies 42 (1998): 115-147; David Hayman, \"Beckett's Watt – the Graphic Accompaniment: Marginalia in the Manuscripts,\" Word & Image 13.2 (1997): 172-182 and \"Nor Do My Doodles More Sagaciously: Beckett Illustrating Watt,\" in Lois Oppenheim, ed., Samuel Beckett and the Arts: Music, Visual Arts, and Non-Print Media (New York and London: Garland, 1999), pp. 199-215; and John Pilling, \"Beckett’s English Fiction,\" in Pilling, ed., The Cambridge Companion to Beckett (Cambridge: Cambridge UP, 1994), pp. 17-42. One reason for this oversight pertains directly to the digitised manuscript project: the materials extend to nearly a thousand pages of autograph manuscript in Beckett’s notoriously difficult hand. Few scholars have read any of the primary materials, and only a very few have read them completely. The well-known hermeneutic difficulties presented by the published narrative are thus in no way adequately understood in relation to the primary materials, because they themselves constitute a kind of terra nullius. By representing and transcribing the manuscript archive of this pivotal text in digital form, such relations between the archive and publication can begin to proceed in an informed way, and more adequate editorial and hermeneutic strategies can be brought to bear on this most inscrutable of Beckett’s texts. The difficulties of reading Beckett’s manuscript and text are, in part, aesthetic. The manuscript was composed during the Second World War, when its author was displaced in the south of France, at a time when reflections upon the efficacy of literary expression were most acute. In addition, the fragments, riddles, and non sequiturs in the published novel (first published in 1953) strongly imply a process of archivisation of fuller manuscript material, or more accurately, providing keys by which to unlock abundant manuscript contents. By providing coherent and searchable access to such a large and complex document, the digitised manuscript project provides the grounds for extensive investigation into hitherto inscrutable textual features in the published text, and provides space for reflection on variant narrative structures and the evolution of literary works more generally.   Digital Technology and Editorial Practice The presence of digital technology in scholarship has become increasingly prominent in recent years. Digital aides to scholarship (online library catalogues, concordances, etc.) provide extensions to existing scholarly tools and practices, facilitating certain kinds of scholarship. Primary sources can be identified by means of web-based archive catalogues, and online digital representations of manuscripts allow scholars to conduct particular kinds of work at geographical distance. Whilst access to the physical document may be desirable or even critical in the final event, several stages of a research project can be accomplished prior to such access. Digital extensions of traditional analogue research tools are perfectly commonplace in most disciplines, and (in theory) are not particularly difficult to integrate into a disciplinary mentality. Recent innovative approaches to scholarly editing tend to imply or assert the relevance of a wider array of documentary sources: genetic editions seek to incorporate all manuscript material and published versions of a text, as well as a rationale of any stemmatic relationship between them, in an attempt to provide a \"total\" text; social text methods seek to integrate erstwhile secondary documents and materials into the very conceptual fabric of a text, as constituent parts of a text’s identity. These more aggregative models of text identity, and more specifically the texts to which they pertain, are clearly conducive to presentation as digital scholarly editions. Conversely, digital modes of representing literary texts can bring questions of a text’s identity into sharp focus. For example, the representation of multiple textual witnesses in collation software such as JuxtaJuxta was originally developed as a collation tool for Jerome J. McGann’s digital Rossetti Archive <www.rossettiarchive.org> and is now housed under the auspices of the Institute for Advanced Technology in the Humanities and NINES (a digital research environment for nineteenth century studies), Alderman Library, University of Virginia. or Versioning MachineSusan Schreibman began developing Versioning Machine <v-machine.org> in 2000. It is housed at the University of Maryland Libraries and the Maryland Institute for Technology in the Humanities. alters rather profoundly the reader’s apprehension of the textual matter at hand. The text is digitally mediated and may be represented by transposed digital reproductions and transcriptions suitably marked up for digital display. But this mediation can go to the very heart of what is considered to be the text. Digital scholarly editions can do two things that seem fundamentally new: firstly, a potentially large corpus of material can be represented in one space, and manipulated in ways simply not possible in the world of physical manuscripts and codex editions: a basic premise of the digitised manuscript of Watt. Secondly, digital collations allow for manipulations of the text material that are visually straightforward and intuitively intelligible, whilst bearing profound implications for the text’s identity and the authority of textual evidence. The digital manuscript of Watt deploys an interface powered by the Apache Tomcat servlet container, which represents files marked up in XML, in a streamlined version of the TEI5 protocols. A high-resolution digital image of the manuscript page appears alongside the marked-up transcription and attendant tools for analysing the transcribed document. In the case of this particular project, the use of Juxta collation software is not a straightforward choice, given that the manuscripts accord very closely to the published text in many places but diverge almost absolutely in many others. The relationship between text and archive is by no means self-evident, or even chronologically linear, witnessed by the density of cryptic allusions and riddles in the published narrative: many of these may only be understood following a close reading of the more expansive manuscripts episodes from which they are sedimented. The application of the Juxta software to such an editorial project as the digital variorum edition of Ezra Pound’s Cantos offers an instructive counterpoint, providing a view of the way in which a well-developed and intuitively graspable digital aid offers new opportunities for new documentary and analytic research. These two examples provide one aspect by which to view the question of digital tools for literary research: does each project in the field of digital humanities require custom digital tools, or are there ways to engineer convergences that continue to provide each project with the specific resources it needs? This remains an open question, inspiring in equal parts an anxiety of resource-intense customisation, on the one hand, and the very exciting prospect of powerful convergences of digital tools in literary research on the other. One critical implication for literary studies is that wherever this question may lead, the nature and status of text identity will demand radical investigation.   The Digital-Textual-Literary Future Recent advances in digital scholarly tools present exciting possibilities for scholarship and for reconsiderations of the paradigms of scholarship. They also present a basic challenge to the work undertaken in literary studies, by calling into question some of the most fundamental conceptual paradigms. The opportunity exists for significant developments in the theory of textual criticism. Whilst it is critical not to overstate the kinds of change made possible by digital scholarship – some apparent paradigm shifts are simply incremental changes to concepts and methods that remain integral to literary scholarship – it seems clear that we are only beginning to understand just what may be possible in the digital domain.  ",
        "article_title": "Digital Mediation of Modernist Literary Texts and their Documents",
        "authors": [
            {
                "given": " Mark",
                "family": "Byron",
                "affiliation": [
                    {
                        "original_name": "University of Sydney USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-03-22",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Circulation of Knowledge and Letters The scientific revolution of the 17th century was driven by countless discoveries in Europe and overseas in the observatory, in the library, in the workshop and in society at large. There was a dramatic increase in the amount of information, giving rise to new knowledge, theories and world images. But how were new elements of knowledge picked up, processed, disseminated and – ultimately – accepted in broad circles of the educated community? A consortium of universities, research institutes and cultural heritage institutions has started a project called CKCCCirculation of Knowledge and Learned Practices in the 17th-century Dutch Republic. A Web-based Humanities’ Collaboratory on Correspondences. Participants: Descartes Centre for the History and Philosophy of the Sciences and the Humanities, Utrecht University; National Library of the Netherlands; Huygens Institute,  (accessed 2010-03-19); DANS; Virtual Knowledge Studio. Start date: November 2008. Duration: 4 years. Budget: 1 M€.  to meet this research question, building a multidisciplinary collaboratory to analyze a machine-readable and growing corpus of letters of scholars who lived in the 17th-century Dutch Republic. Until the publication of the first scientific journals in the 1660s, letters were by far the most direct and important means of communication between intellectuals. Therefore the 17th-century Republic of Letters offers an ideal case for exploring the answers to this question.  Researchers want to uncover patterns in letters that are indicative for the circulation of knowledge, patterns that reveal the emergence of complex, collective phenomena in modern science. However, they face some fundamental problems with finding such patterns in letters. One cannot know in advance the nature of these patterns, and only few categorical hypotheses can be tested by simply data mining the letters. Purported patterns cannot be tested against the letters, because the heterogeneous information on which these patterns are based cannot be gleaned from the texts, but need considerable interpretation and contextualization. Here is a short list of the problems: (i) the letters are not uniformly available; (ii) the 17th century language varieties are not standardised and pose a challenge for language technology; (iii) much interpretation is needed to resolve references to people, places, dates, ideas and instruments; interpretations are complicated by the heterogeneity of annotations; (iv) it is not clear how to set up visualisations of patterns that are really informative to the historian of science. These four types of problems will be used to report on the methodology of the project and on its results so far.    Information technology as a humanities’ observatory  Availability of the Letters CKCC limits itself to the ca. 20,000 letters written by scholars that were active in the Netherlands: René Descartes, Hugo Grotius, Constantijn Huygens, Christiaan Huygens, Caspar Barlaeus, Jan Swammerdam and Anthony van Leeuwenhoek. Modern editions of these correspondences—already published or in an advanced state of production by members of CKCC—form the basis of the digitised texts. The letters, once converted to a minimal TEI format, will then be made available through e-Laborate,e-Laborate,  (accessed 2010-03-19), Huygens Instituut. a web-based philological annotation tool that will be transformed into a collaboratory for the history of science and the humanities in general. It serves three purposes: (a) providing scholarly access to the letters; (b) allowing researchers to enrich existing datasets and annotate the letters; (c) using the letters and the input of researchers to visualise patterns meaningful for the circulation of knowledge.   Use of other datasets We will incorporate a particular database of (meta)data, the Catalogus Epistularum Neerlandaricum (CEN), or the Catalogue of letters in Dutch repositories. It is a relatively old database, already available via Telnet in the early 1990s, before the world wide web came into being.  CEN is an exhaustive database of letters in the collections of five Dutch university libraries, the Royal Library, and four other important libraries. It contains more than 265,000 descriptions of approximately 1,000,000 letters, dating from 1600 until the present day (of which ca. 100,000 from the 17th century). It supplies the following metadata: sender, recipient, place of sending, year, language, repository and shelf mark.  The format in which this database will be made available to the project is to be negotiated with the owner, OCLC.Online Computer Library Center,  (accessed 2010-03-19). Usage of this database will enable us to make assertions about the fraction of the selected letters with respect to the total body of letters. Moreover, it allows us to increase the density of the networks we are interested in, leading to unprecedented research opportunities.   Language technology In order to find meaningful patterns in social networks of scholars and in circulation of knowledge language technology is needed. For this, CKCC is cooperating with CLARIN.Common Language Resources and Technology Infrastructure,  (accessed 2010-03-19). CKCC is on the list of supported projects by which CLARIN is reaching out to the humanities,  (accessed 2010-03-19). The mission of CLARIN is to make language technology interoperable and to make linguistic resources accessible on a European infrastructure, so that all the arts and humanities can make use of it. The Netherlands pillar of CLARIN, CLARIN-NL,CLARIN-NL (Netherlands),  (accessed 2010-03-19). has already obtained funding for constructing such infrastructure, and has issued a call for proposals for adding existing resources to this infrastructure and writing demonstrator services. Aided by expertise provided by CLARIN members, in particular by the University of Lancaster,University Centre for Computer Corpus Research on Language,  (accessed 2010-03-19), in particular Paul Rayson,  (accessed 2010-03-19). CKCC is developing such a demonstrator. A proposal to this end has been accepted by CLARIN-NL. The demonstrator, comprising the correspondences of Grotius, Const. Huygens and Descartes (ca. 15,000 letters in all), is planned to be completed by October 2010. It will perform a time-sensitive keyword extraction, which can be visualised by means of a dynamic word cloud. As the source languages are 17th century Dutch, French and Latin, one needs at least spelling normalisation and harmonisation of keywords across languages.   Interpretation and Enrichment References to people, places and times are often implicit and can only be retrieved by studying contextual material or by using secondary sources. Named Entity Recognisers are helpful, but it is not possible to rely on technology alone. In order to get an accurate picture in sufficient resolution, interplay between manual work and automatic tools is needed. The collaboratory based on e-Laborate gives researchers the opportunity to collect their interpretations of the texts, compare them to others and to annotate them with their insights. Over time, the results of this hand/mind work might be automatically gathered and incorporated in enriched transcriptions of the texts.   Visualisation By offering meaningful visualizations of the data, the CKCC will enable humanities researchers in a wider context to use the tools and the results yielded. Not only the relationships between corresponding authors will be made visible in time and space, but CKCC also aims at visualizing the dynamics of knowledge production by focusing on the emergence of themes in scholarly debates and social networks of 17th century natural philosophers. The dynamic word clouds based on keyword extraction is just a first step. CKCC will subsequently explore several approaches of gathering and visualising meaningful patterns, which are deliberately different in nature. The first approach (a) is a sophistication of keyword extraction, and the second one (b) is based on associations in text. Both methods can be used to evaluate the results of each other. (a) Concept analysis. This requires considerable more analysis than keyword extraction. For example: the many surface expressions of a concept must be linked into one entity, preferably part of an ontology or thesaurus. Existing subject indices and reference corpora will be used. Visualising the behaviour of concepts over time will yield a good approximation of knowledge circulation. (b) Associative neural network technology. This is an application of a recent effort, ANNH,Associative Neural Networks for the Humanities. An application developed by the Department of Philosophy (P. Ziche, E.-O. Onnasch, E.-J. Bos), Utrecht University, and Dr. Holthausen GmbH, Bocholt. See (Holthausen et al., 2007). to apply the idea of neural networks to the humanities. This approach enables the automatic comparison of texts, based on the degree of associative similarity. Concepts or themes occurring in the letters could thus be made visible, either by focussing on single terms (e.g. ‘observations’) or word pairs (e.g. ‘soul’ and ‘matter’). Moreover, it is possible to query for letters associated to a given one, and rank the results by the degree of association. The facility to track the circulation of knowledge will then be within reach.     An infrastructure for the digital humanities Infrastructure is of particular interest in view of current developments in Europe as testified by the ESFRI roadmap.European Roadmap for Research Infrastructures,  (accessed 2010-03-19). The roadmap funds the preparation for several research infrastructures in the humanities, among which CLARIN, as demonstrated above, is most relevant for the CKCC project. CKCC will take care that the materials residing in the collaboratory can be exported in such a way that it is available on the CLARIN infrastructure, thus contributing to the much grander vision to have all scholarly letters of the 17th century uniformly available for research, including the results of related work.Mapping the Republic of letters,  (accessed 2010-03-19). Cultures of Knowledge,  (accessed 2010-03-19). In due course, CKCC will not only contribute to the understanding of the circulation of knowledge in the 17th century, but also generate useful technologies for cross-disciplinary collaborations involving data-sharing and data-enrichment in the Humanities. As such, this web-based humanities collaboratory on correspondences is a valuable prototype for possible future research collaborations focusing on large, heterogeneous datasets in the Humanities.  ",
        "article_title": "Letters, Ideas and Information Technology: Using digital corpora of letters to disclose the circulation of knowledge in the 17th century",
        "authors": [
            {
                "given": " Dirk",
                "family": "Roorda",
                "affiliation": [
                    {
                        "original_name": "Data Archiving and Networked Services,  Netherlands",
                        "normalized_name": "Data Archiving and Networked Services",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/008pnp284",
                            "GRID": "grid.500519.8"
                        }
                    }
                ]
            },
            {
                "given": " Erik-Jan",
                "family": "Bos",
                "affiliation": [
                    {
                        "original_name": "Department of Philosophy,  Utrecht University, Netherlands ",
                        "normalized_name": "Utrecht University",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04pp8hn57",
                            "GRID": "grid.5477.1"
                        }
                    }
                ]
            },
            {
                "given": " Charles",
                "family": "van den Heuvel",
                "affiliation": [
                    {
                        "original_name": "Virtual Knowledge Studio, Netherlands",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-05-01",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Premodern Chinese texts pose problems that are difficult to accommodate with the current TEI text model, which bases the main hierarchy of a text on its structural content, rather than on a hierarchy that models the pages, lines and character positions. For the TEI, this is a sensible decision and has led to the abolishment of elements like <page> and <line> in the latest release of the Guidelines. For premodern Chinese texts however, especially texts that are transmitted as manuscripts or woodblock printings and have not yet seen a modern edition printed with movable type (let alone as, more recently, computerized typesetting), establishing the structural hierarchy of the text content is, together with the even more daunting question of establishing the proper characters of the text (on which see below), an important part of the research question that motivates the digitization of the text. Requiring an answer to this question before a proper electronic text can be created makes this intractable in the digital medium and glosses over an important leap of faith in the creation of a TEI encoded text. In this paper, I will try to trace some of the implications and propose an approach that allows different models of the text for different stages in the encoding process, thus closer modeling the process of the creation of an electronic text. To arrive at a text properly encoded according to the TEI Guidelines is not a straightforward process, but in the setup described here requires a detour through at least three stages:  Draft input of the text without any further markings; An incubator phase, in which the text is dealt with as a series of pages (or scrolls), lines and characters; The mature text, based on the structural model of a TEI <text>, which is available for further refinement;   Of these steps, the second one is at the center of attention in this paper, which will include the discussion of the following three aspects:  A text model according to these requirements; Mandoku, an application that allows manipulating the text; A transform that specifies how a text conforming to this specification can be turned into a TEI encoded text.    Different Models for the Same Text  The structural, content based hierarchy of the text has to be established as part of the research process. For this reason, the text at this stage uses the only hierarchy available, that is the one that is based on how the text is physically recorded on the writing surface in the edition used. During the process of working with the text, milestone-like elements are inserted at the starting points of elements of interest, using the incubator as described in the next section. Headings are numbered according to their nesting depth as in a HTML document; this forms the base for their transformation into regular TEI nested <div>s followed by <head> elements.    The Incubator: Mandoku  The tool used to manipulate a premodern Chinese text in the incubator phase has been called Mandoku. It makes it possible to display a digital facsimile of one or more editions and a transcribed text of these editions side by side on the same screen. From there, the texts can be proofread, compared and annotated. A special feature is the possibility to associate characters of the transcription with images cut from the text and a database of character properties and variants, which can be maintained while operating on the text. Interactive commands exist also to assist in identifying and record structural and semantic features of the texts.  One of the major obstacles to digitization of premodern Chinese texts is the use of character forms that are much more ideosyncratic than today's standard forms. Since in most cases they cannot be represented, they are exchanged during input for the standard forms. This is a potentially hazardous and error-prone process at best, and completely distorts the text in worse cases. To improve on this situation and to make the substitution process itself available to analysis, Mandoku uses the position of a character in a text as the main means of addressing, allowing the character encoding to become part of the modelling process, thus making it available to study and analysis, which in turn should make the process of encoding more tractable even for premodern texts. The current model is still experimental, but initial results have been encouraging.  Mandoku is work in progress and is developed as part of the Daozang jiyao project at the Institute for Research in Humanities, Kyoto University by Christian Wittern. In this paper, an emphasis will be placed on the different models of a text that are underlying the different stages of preparation of a text and the friction, but also benefits, that arise out of such a situation. The following is a screenshot of the main interface, displaying a facsimile and a transcribed version of the same text side by side.  Fig. 1. Mandoku in action     Transform to TEI <text>  Finally, as a proof of concept, a XSLT script has been developed that performs an algorithmic transformation from the text in the intermediate format to a text as it has to appear as content of the TEI <text> element.  This produces a new version of the text with an inverted hierarchy: The primary hierarchy now is the content hierarchy, whereas the hierarchy of the text bearing medium is demoted to a secondary one, represented by milestones. None of these hierarchies is a priori superior to the other, but in the context of the Daozang jiyao project the purpose of preparing the texts is to make it available for a study of the collection, so the emphasis during the later stages in the life of the text will lie on the content hierarchy. The problem of overlapping hierarchies, which is such a scratching itch for many text projects, poses itself thus in a slightly different incarnation: The different hierarchies occur in two different stages of preparation of the text, which require different viewpoints, but not simultaneous presentation, which makes it easier to accommodate the two in our workflow.  The preparation of a TEI encoded representation of the texts is however not the ultimate goal of the project. The next phase requires analytical interaction with the text for which again the TEI representation might not be the ideal format to work with, so there might be a number of different, purpose-specific derivative formats generated from the TEI texts. They will maintain the required information to refer additional information back to the master files kept in TEI, and to be able to participate in the ongoing evolving of the master text, to which transcriptions of more witnesses will be added, but will otherwise also contain additional commentary, translation and other information that will not belong to the original file. The details of this part of the system are under consideration now and will be the topic for another presentation.    Conclusions The current TEI text model does not allow the direct description of the document as it appears on a text bearing surface without also establishing a content hierarchy. For this reason, a temporary encoding strategy had to be developed, which is TEI conformant to the letter, but not to the spirit of the TEI Guidelines by wrapping all of the text content in one giant <p> (or possibly <ab>) element. Only after the structural hierarchy has been established is it possible to make a transformation to a truly conformant and satisfying TEI document. The slight feeling of uneasiness that this workaround causes might go away once the new <document> element proposed by the TEI working group on genetic editions has been adapted to the Guidelines and can be used for the phase of work in the incubator, thus making the text fully TEI conformant from the beginning. On the other hand, this project also clearly demonstrates the necessity of being able to represent the document in its own right in a TEI text, even if in the context of this project the documentary part is considered transitory.   ",
        "article_title": "Mandoku – An Incubator for Premodern Chinese Texts – or How to Get the Text We Want: An Inquiry into the Ideal Workflow ",
        "authors": [
            {
                "given": " Christian",
                "family": "Wittern",
                "affiliation": [
                    {
                        "original_name": "Kyoto University Japan",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-03-23",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Electronic literature poses several exciting challenges and questions to literary scholars:  How do we balance our interpretations of digitally born works against the specific modes of production that make such works possible? How do our conceptions of authorial intent shift in relation to works that solicit active participation from their readers? How do we account for readers’ participation in such works, as well as the way their experiences shape and re-shape the text? In this paper I offer one strategy of interpretation that cuts across some of these questions: tracing the path of direct address in works that are digitally born, a technique that both emerges and departs from conventional literary practice.  When I visit a certain website, I am greeted in a peculiar fashion: an animated avatar with a human form speaks to me, blinks at me, and follows my mouse movements on the screen with her eyes while I read. On another site, a string of text hails me and addresses me by name, purporting to welcome me to all the treasures contained within its digital domains. As startling as these salutations initially seemed and as commonplace as they have become, I remain intrigued by their overt and shameless invocation of the reader — in this case, me. Strictly speaking, this mode of address should not be possible, at least not according to the familiar conventions of literary tradition.  In Anatomy of Criticism, Northrop Frye states the matter unequivocally: “Criticism can talk, and all the arts are dumb…there is a most important sense in which poems are as silent as statues. Poetry is a disinterested use of words: it does not address a reader directly” (4). While the examples of address above are decidedly not the poetical specimens Frye has in mind, his stance nevertheless serves as a firm response to a larger problem, one that has endured since the time of Socrates and persists to this day, a problem that can be crudely summarized in the following terms: there has always been something of a gap between the written word and its reception. Each time I see my own name staring back at me, however, I question whether the gap between text and reader has been in some way bridged, or at least contracted. Each time the avatar speaks to me, I am unable to locate myself in relation to the text no matter which paradigm I might use to explicate our relationship. Within a spectrum bounded at one end by the New Critical emphasis on textual autonomy and at the other by the “virtual” text that emerges necessarily as a correspondence between author and audience in reader-response theory, I do not know where I stand. With the announcement of my own name, I am aware that I have been identified, and therefore can no longer even maintain the convenient illusion of being, as a reader, either ideal or implied.  I have been specified. The “text,” such as it is, has called me out. The spectrum I have identified here is, of course, absurdly streamlined and unequally weighted. The New Critics exclude the reader’s thoughts as a given principle, while reader-response theory alone has perhaps generated more ways of labeling its reading audience than the sum of other critical interventions combined — in addition to offering a strong and convincing counterpoint to Cleanth Brooks’ ideal reader, Wolfgang Iser’s implied reader is only one star in a constellation of terms that includes the mock reader, the actual reader, the fictionalized reader, the hypothetical reader, the narrative reader, the ideal narrative reader, and the “real” reader, not to mention Stanley Fish’s interpretive reading communities (Brooks, 24; Iser, The Implied Reader, xii; Rabinowitz, 125-128; Fish, 219). In all of these models of reception, the impulse to name the reader, to re-assert her importance in the construction of textual meaning, still participates in the tacit agreement that this reader, whoever she may be, is never fully concretized by the written text. How could she be?  Rather, a “virtual” text emerges as a sort of ghostly correspondence between the two, one that is nigh on impossible to trace.  In the words of Iser, “It’s difficult to describe this interaction…because…of course, the two partners in the communication process, namely, the text and the reader, are far easier to analyze than is the event that takes place between them” (“Interaction Between Text and Reader,” 107).   The hypothesis that I would like to test in this essay is that works of electronic literature push the issue of responsibility and specificity into uncharted readerly terrain. What if, in certain examples of electronic literature, direct address online were specific to you, the reading reader, and not an implied reader? Put more specifically, pressed even further, what if direct address online were to make traceable the ghostly correspondence between reader and text that Iser outlines? This is not as far-fetched as it seems. As we shall see, the reader’s participation in some examples of electronic literature is required for textual constitution in ways that are fundamentally different from even the most successful and extreme examples of non-linear narrative practices found in print.  In the case of electronic literature, direct address functions to bring the text into being, by signaling the reader and requiring a response of her. Even more remarkable, this response has the ability to become a part of the initial text, such that the text that emerges is literally constituted through the feedback that exists between the reader’s actions and the author’s words.  While many claims about interactivity and customization have been made about electronic literature, there has not yet been a sustained attempt to consider the more specific mode of address that occurs in such works in relation to overtly literary practice.  In the space that follows I attempt to remedy this by considering instances both subtle and overt that occur in select works of electronic literature — including Dan Waber and Jason Pimble’s “I, You, We,” Mary Flanagan’s [theHouse], and Emily Short’s “Galatea” — that signal, cue, or otherwise point outside themselves to the reader as she progresses through the text. If the use of the vocative in conventional literary texts has the ability to point not only to characters within their narrative confines, but to an entire social, political and cultural discourse that lies tantalizingly close, yet perhaps ultimately outside the textual boundaries, I explore whether modes of address in online works allow us to exceed these boundaries altogether.  ",
        "article_title": "e-Vocative Cases: Digitality and Direct Address",
        "authors": [
            {
                "given": " Lisa",
                "family": "Swanstrom",
                "affiliation": [
                    {
                        "original_name": "HUMlab, Umeå University, Sweden",
                        "normalized_name": "Umeå University",
                        "country": "Sweden",
                        "identifiers": {
                            "ror": "https://ror.org/05kb8h459",
                            "GRID": "grid.12650.30"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-30",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The Courtauld Gallery occupies a unique position in central London as a university art museum and key facilitator for collaborative opportunities between research academics and curatorial practitioners. Given the context of the gallery as an institution housed within a building not purpose-built for the function of exhibition, the ways in which the gallery space acts as a directive on visitors’ viewing patterns is of particular interest. This paper introduces a methodology of documenting and visualizing those patterns.  The research is approached from the perspective of a performance specialist (Guy), investigating the curated gallery as a place for performance and the visitors’ role within such exhibition spaces. This has led to the generation of questions vital to new methodological approaches for exploring data only existent in the form of events, and to future processes of gallery operation and evaluation.  This paper focuses on the empirical aspects of this work, reporting on a project to create digital objects, displayable in Virtual World platforms such as Google Earth, from the experience of visitors to the Courtauld Gallery, London. The digital objects consist of visualisations in virtual space and time of visitor experiences, documented in KML, and represented using the Google Earth platform.  Studying Visitor Experiences To date, the evaluation of exhibitions has largely been based on attendance numbers, with very little attention given to actual visitor behaviours within gallery environments. Traditionally, where visitor behaviours have been observed, methods for achieving this have been based on pen and paper recording using methods such as those developed by Space Syntax (Space Syntax, 2010). Capturing this data in digital form will allow a more thorough and formal analysis of the ‘success’ of exhibitions by permitting the replay in both time and virtual space of visitors’ behaviours and their interaction with staff, other visitors, and gallery exhibition materials. More specifically, we are concerned with:  patterns of movement within gallery spaces specific pathways constructed by individual visitors through the museum duration of engagement with individual exhibits actions and interactions of gallery visitors  This information makes possible an analysis of how visitors explore exhibitions that is not preconceived but observed. Anonymous representations of visitor behaviours can be offered back to the gallery prompting curatorial assumptions to be validated or, where appropriate, reconsidered in light of evidential data about real patterns of visitor behaviour. This, in turn, can have important implications for maximizing public engagement within exhibition contexts and ensuring efficiency of interaction between staff and visitors, as well as other aspects of social and economic exchange. Using geovisualization techniques to generate the interactive maps based on individual experience raises questions about how possible it is to produce and manage the documentation of human behaviour.   Technological and Methodological Issues Ideally, visitors would be totally oblivious to the data capture process or at least, such technological means as are necessary for capture would be non-intrusive. The demands of the research problem require high fidelity of location, orientation, and behaviour making the technological issues more complex than simply determining approximate location. Borriello et al. (2005) report that GPS systems do not work reliably inside buildings, wi-fi systems require calibration and achieve accuracy of only about 3 metres, and others require infrastructure installation. This is rarely possible in protected buildings such as the Courtauld, especially on a temporary basis. Consequently, the method described below was developed (related approaches will be discussed in presentation).  A pilot study was carried out prior to the main observation period in the Courtauld’s Frank Auerbach exhibition (Courtauld Gallery, 2009).   Data Capture Method The method used is predominantly based on field observation in two forms. These approaches are influenced by the observation methods used by Space Syntax, a company with an established history of evidence-based design and evaluation for buildings and urban spaces. Space Syntax observations are undertaken using pen and paper.  The first method involves a human observer tracking the movement pattern of visitors around parts of the gallery in order to observe particularized routes specific to individual visitors. This is facilitated by the use of Tablet PCs and custom-developed software which displays an editable floor plan of the gallery, divided into map tiles based on the gallery rooms. In the case of the Courtauld, the map tiles are 610x365 pixels. The actual room size in the pilot is approximately 1290.5cm x 772cm = 12.9x7.7m. By moving the digital pen around the image the pathway of an individual can be recorded on the map and the movement documented with an x,y pixel reference and a timestamp. Duration and location are recorded both when the visitor is moving and stationary. Additional coding concerning, for example, the activity undertaken at any given moment, such looking at a map or signage or taking a photograph, can also be coded against points on each pathway.   The second method involves participants being asked to complete an exit questionnaire detailing their familiarity with the specific gallery and exhibits as well as more general experience of exhibition environments. This information is cross-referenced to the trace of each visitor’s pathways. A screenshot from the software can be seen in Figure 1, showing a gallery floorplan with a partial trace. Figure 2 shows the simple comma separated values (CSV) data produced by the trace activity in realtime (the fields being X, Y, timestamp, visitor activity, and map file used). The system defaults to tracking movement as the pen is drawn across the screen, however, the observer can simply switch the “mode” of the current point by selecting one of the buttons to the right. Where data about interaction with an exhibit is necessary, the point of the observee’s interest can be noted by simply pointing at it on the diagram after selecting either “Sign” or “Exhibit” as the object of interest. The final system was developed in Borland Turbo Delphi with early prototyping undertaken in Processing. For the Courtauld case study, the maps are scaled such that 1 pixel is approximately 2cm2 of real gallery space. The maps are converted from architectural plans and the location and size of furniture in the gallery included from measurement in the galleries themselves. Visitor location is recorded as a relative pixel position from the top left corner of the image. After capture this data can be transformed to generate real visitor positions in the room for subsequent visualisation.   Figure 1: Screenshot from Data Capture Application       Figure 2: Extract from CSV data file      Visualisation of Results The relative positional information in the CSV file(s) is converted to absolute positions and stored in KML. The timestamp stream that indicates where a visitor was at a given time can be used to generate a trace of movement (and speed). For the purposes of the visualization, the floorspace of the Auerbach, as defined by the map tiles in the data capture software, was rotated from its SE-NW alignment (see Figure 3). This was necessary to facilitate the georeferencing, or conversion of the x,y pixel data points into decimal degree coordinates. It was therefore possible to make the observation that the gallery room equated to 0.000118 x 0.000027 decimal degrees. The pixel readings (r)_ can therefore be converted to decimal degrees (d) using the simple conversion formula d=W. longitude – (0.000118/610)*r. For the latitude readings, this is repeated, but substituting (0.000027/325). An arbitrary altitude value, starting at 0.001 at the first reading, and increasing sequentially by 0.001 throughout the CSV dataset was added to the datapoints. In the visualization (see Figure 4), this gives the impression of the pathway rising as the visitor progresses through the gallery and through time: in this, it follows the principle of the ‘space time cube’, developed elsewhere (Kraak, 2008). As more data is added, this will allow us to build complex structured visualizations, which will add significant support to interpreting the visitors’ uses of, and interactions with, the space. These will be reported in full in the full presentation of this paper.   Conclusion At the time of writing, the technology platforms are fully developed, ready for use and have been tested (with subsequent enhancement) in a pilot session in the gallery. The main observations will take place in November and December 2009 during the Courtauld Gallery’s exhibition of Frank Auerbach: London Building Sites 1952–62. This paper has presented an approach to capturing visitor experience and interactions in a gallery using methods based on tried and tested approaches, but augmented by digital tools. The data generated can subsequently be visualised in virtual space and time to allow questions of performance practice to be addressed.   Figure 3: The Auerbach gallery in Google Earth, and the area of the map tile on which the trace was taken, aligned on a N-S bearing. (© 2009 Google, Image © 2009 Bluesky)       Figure 4. Representation of visitor pathway from the pilot study in ‘space time cube’ format, represented in Google Earth. (© 2008 Google, Map Data © 2009 Tele Atlas, Image © 2009 Bluesky)       ",
        "article_title": "Capturing Visitor Experiences for Study and Preservation ",
        "authors": [
            {
                "given": " Georgina",
                "family": " Guy",
                "affiliation": [
                    {
                        "original_name": "King's College London UK",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            },
            {
                "given": " Stuart",
                "family": " Dunn",
                "affiliation": [
                    {
                        "original_name": "King's College London UK",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            },
            {
                "given": " Nicolas",
                "family": " Gold",
                "affiliation": [
                    {
                        "original_name": "King's College London UK",
                        "normalized_name": "King's College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0220mzb33",
                            "GRID": "grid.13097.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-30",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Transition and decline are pressing issues for scholars in the digital humanities, as our projects tend to be both collaborative and open-ended. Project staff relocate, reestablish themselves in new areas, or retire, even as funding and institutional support comes and goes. How are projects to be designed so that they can be maintained, or maintain themselves, through periods of change? How might projects be designed in a way that takes periods of transition and possible decline into account from the very start? These are some of the issues we sought to explore in undertaking \"Graceful Degradation: Managing Digital Projects in Times of Transition and Decline,\" a wide-ranging survey of the digital humanities community, in the summer of 2009. Our intent was to investigate how the community currently deals with these problems and, using our survey data – which also included some demographic information and measures of perceived levels of support and impact of various kinds of change – to make recommendations on how we, as a community, might improve the current approach.  This presentation will provide a detailed look at the outcomes of the \"Graceful Degradation\" survey, and propose some initial recommendations. (Full recommendations will be published in a separate article.)  The survey was designed in consultation with statistical analysis staff at the Scholars' Lab, University of Virginia Library and unveiled at Digital Humanities 2009 in College Park, Maryland and at Digital Resources for the Humanities and Arts (DRHA 2009) in Belfast, Northern Ireland. It was conducted online between July and September 2009. There were 102 completed surveys, representing 114 discrete projects. Some of our findings are presented below.  The vast majority (76%) of Graceful Degradation respondents come from \"large universities with a research emphasis,\" but teaching colleges, cultural heritage institutions, and commercial ventures were also represented. Most respondents have worked in project management or digital research and development efforts in the humanities for 2-10 years, but 35% of respondents have been engaged in this activity for more than a decade.  Respondents were asked to rate perceived levels of support for the digital humanities at their home institutions, including (as separate queries) general support, support for collaborative activities, local funding and cost-share opportunities, support by higher administration, department-level or local support, and support for project management and grant-writing.  64% of respondents had experienced the decline of a project or had weathered a period of difficult transition. 29% of respondents indicated a sense that digital humanities projects are more likely to decline or suffer these difficult transitions at their institutions than at others.  Participants were asked to respond in detail regarding their experiences with a particular project that suffered decline or a difficult transition. The following percentages apply to the primary or to the single project which survey participants addressed. 37% of respondents identified themselves as project lead or principal investigator (PI) for the project they discussed in depth. 29% of respondents self-identified as project managers, and other respondents fell into categories such as \"dedicated, project-specific support staff,\" \"support staff on loan from other units,\" \"graduate or undergraduate research staff,\" \"post-docs or faculty collaborators.\"  38% of projects discussed fell into the category of \"content creation, digitization, and archive-building,\" but other categories (including software development, online community-building activities, online journals and other publications, and creation of support infrastructure for digital scholarship) were also represented. Predominant disciplines and time periods addressed were literary and textual studies and digital history, from the modern or early modern era. More projects (31% and 24% respectively) identified an academic department and a library or museum as their primary institutional home, with 23% primarily housed in a digital humanities center. Of projects that had experienced decline or difficult transition, most were identified as still \"ongoing and active\" (51%), with 26% abandoned or dormant, and 15% and 8% either complete or \"just getting started,\" respectively.  Participants were asked about funding sources for these projects (generally via institutional support or \"external public funding\") and understood length of funding or support. Projects treated were generally funded for 2-3 years, with no possibility of renewal, but often (in 21% of cases) the length of funding or support was \"unclear.\" That said, 75% of respondents considered their project's funding to be \"reliable and clear in scope.\"  Most respondents undertook the treated project with clear plans for supporting it beyond an initial funding period, but most projects also ultimately \"differed in scope or definition from early plans.\" In 68% of cases, participants had identified both short-term and long-term goals for their projects, but conscious use of \"specific project management techniques or tools\" and \"risk management strategies\" was a rarity. Anecdotal responses treated the impact of varying levels of planning or lack of planning on digital projects.  The majority of projects (55%) experienced no negative impact due to staff overturn whatsoever. For projects that did, we asked participants to rate the negative impact of overturn of six different categories of staff members and collaborators. Survey participants also rated the broad impact of their projects in a dozen areas, such as \"scholarly inquiry in a particular field,\" \"my own pedagogical practice,\" and \"the professional advancement of my collaborators.\"  Participants were additionally given the opportunity to respond to several prose prompts, and to add more contextual information to many of the questions for which we had devised statistical measures. They summarized the reasons for the project decline or difficult transitions they experienced, and offered formulae for their successes. Some respondents identified nuanced issues with intellectual property and open source as contributing factors. We plan to summarize these rich responses and reveal the results of qualitative data analysis at the conference.  67% of respondents indicated that their personal views and practices have evolved as a result of experiencing a period of difficult transition or the decline of digital humanities project, but in only 32% of cases did they feel that the views or practices of their local institutions or the larger academic community have evolved in response to such experiences like these.  67% of respondents also indicated that they had experienced what they would consider a \"phase of successful transition\" in their digital projects, and offered anecdotal advice as to what made that possible.  At Digital Humanities 2010, we will summarize and offer some visualizations and analysis of these findings and others, and we will address the extensive qualitative data that were collected from participants in free-form text responses. (Several participants granted us permission to quote their responses directly. We will anonymize and summarize responses from others.) We will also draw conclusions about avenues for future research and – more importantly – identify areas for future action on the part of institutions supporting digital humanities projects and professional societies representing the digital humanities community. ",
        "article_title": "The Graceful Degradation Survey: Managing Digital Humanities Projects Through Times of Transition and Decline",
        "authors": [
            {
                "given": " Bethany",
                "family": "Nowviskie",
                "affiliation": [
                    {
                        "original_name": "Scholars' Lab, University of Virginia Library USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Dot",
                "family": "Porter",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Observatory, Royal Irish Academy Ireland",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2009-04-23",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper will report on a corpus-based study of the cultural keywords (in the Raymond Williams’ sense) via the analysis of key-words (in the corpus/statistical sense) of newspaper reporting in the years since Labour came to power. The project demonstrates that certain lexemes (or lexical strings) gain currency in relatively short historical periods and may take on political importance. This project assesses the ideological landscape during the years of the New Labour project by extracting the cultural keywords of the time, and demonstrating their evolving meanings in the commentary provided by the print media. The project takes inspiration from Raymond Williams’ book ([1975] 1983) Keywords which attempted to sum up the ideology of the post-war years. Williams chose a set of words which he thought had taken on particular meanings in that period, and wrote an informed but ultimately anecdotal commentary on each one. Like Williams, we begin with a hypothesis that some words (such as, for example, radicalisation, choice) have both increased in usage and polarised in their meaning since 1998. Unlike Williams, this project pursues a rigorous approach to the discovery of which words characterise the period under investigation, using two corpora of newspaper data and computer tools. This enables us to make a comprehensive investigation and an objective assessment, including use and meaning, of the cultural keywords of the Blair years The project is primarily corpus-based, but with a strong qualitative focus, using an approach to studying textually-constructed meanings of words and other linguistic items which recognises both their place in a relatively stable system of language, and their capacity to take on additional meaning in specific contexts of time and place.  Background Our project links the corpus linguistic notion of key-words to earlier work into the ‘emergent meaning’ of individual lexical items (see Jeffries 2003, 2006 and 2007).  Jeffries (2003) investigated the meaning of water, in the context of the Yorkshire water crisis of 1995. Jeffries (2006) investigated the speech act of apology, in particular news commentators’ view of Blair’s putative apology for the Iraq war. Jeffries (2007) was a much more extensive consideration of the way in which the female body was constructed by women’s magazines in 2004. This larger study developed a system of describing textual meaning which draws on Hallidayan approaches to systems of linguistic form and meaning applying his combined semantic and syntactic view of textual meaning to other functions such as the construction of opposites. The current project was designed in the spirit of Critical Discourse Analysis, in particular the work of Fairclough whose work on ideology in language, and specifically the language of New Labour (Fairclough 2000) influences the approach taken here. However, the methods used in this project are closer to corpus stylistics in that they are text-analytic and at least in some of the stages, computer-assisted and corpus-driven. Work already carried out in this area (see for example McIntyre and Walker 2010) showed that corpus approaches and tools, in particular Wmatrix, can successfully be applied to textual analysis. Baker and McEnery (2005) and Baker and Gabrielatos (2008) are also influential on this project because this work has paralleled Jeffries’ work in looking at sets of texts from a particular time period to demonstrate political ideologies in news texts. The project also reflects renewed interest in cultural keywords in the Williams sense, with a recent special issue of Critical Quarterly (2007) devoted to the subject, and Durant’s (2006) related article which suggests that “[…] the development of electronic search capabilities applied to large corpora of language use […] encourages renewed attention to cultural keywords.” (Durant 2006). This project effectively takes up that suggestion.   Research questions   What are the key-words for the years 1998–2007, as evidenced in the British press and can they be identified as cultural keywords? Have they developed meanings specific to this period and have these meanings evolved within the period?     Methodology The project focuses on news texts from 1998 to the end of 2007. A corpus of comparable data from three national daily newspapers (The Guardian, The Independent, and The Times) was assembled from a large, on-line newspaper database. This database represents a very rich and potentially overwhelming amount of data (100s of millions of words). However, our project had very limited timescales and we found it necessary to carefully control the amount of data that we collected. This was because: (i) downloading selected articles from the database is largely a manual and fairly time consuming process; and (ii) in its raw form each downloaded article contained structured extra-textual details (headers containing titles, dates and so forth), random intra-textual information (such as journalist’s email addresses) and corruptions. This extra text and corrupted data had to be removed from and amended in each of the downloaded files: a laborious process which consumed a lot of project time. We also found that the corpus tools we used for data manipulation struggled when presented with files of more than one million words. Consequently, we took a structured sampling approach, choosing a week from the politically ‘busy’ month of September (party conferences), and collected selected news-related items from these weeks. The resulting corpus was approximately 2.3 million words, which we anticipated would be sufficient to answer our research questions. A comparison corpus was built along similar lines using newspaper data from the five year period prior to 1997 (the Major years). The corpus was automatically analysed, in the first instance, using Wmatrix (Rayson 2008), which is a relatively new corpus tool that can calculate keyness (using Log-likelihood) at the word level (key-words), at the grammatical level (key-POS), and the semantic level (key-concepts). The present study uses just the key-word output. To address the qualitative aspect of the research questions, this investigation included the following considerations:  Do the collocations of the key-word demonstrate particular nuances of meaning? How does the semantico-syntactic behaviour of the key-word demonstrate meaning specific to the context? Does the key-word enter into any unconventional lexical relations (e.g. of opposition)? Is the key-word associated with any modal or negated text worlds?    Results The key-words, generated from the comparison of our corpora, that we consider to be the important cultural keywords from the Blair years are as follows:    No.  Keyword Associated key-words   1  Terror  Terrorism, terrorist(s), attacks, atrocities, threat    2  Global  Globalisation, world, international    3  Spin  spun    4  Reform  progressive, radical, modernise(d) / er(s) / ation    5  Choice    6  Respect    Items in the ‘keyword’ column are the main items used in our investigation and the terms that we consider to be culturally significant. The items in the third column are key-words resulting from our corpus comparison which are related to individual (cultural) keywords and which, we hypothesise, form a network of meaning. These are still to be fully investigated and we do not report on them in this paper. For each keyword we provide a more detailed quantitative commentary using concordance and collocation data. Our major findings, though rigorous and replicable, are qualitative, and provide the basis of both detailed linguistic commentaries on each key-word and could also provide the foundation for more general popular essays not dissimilar to those provided by Williams, but with more clarity about their provenance. There will not be time to discuss all our findings, but our paper will report on some of the quantitative data and focus qualitatively on ‘spin’.  ",
        "article_title": "A corpus approach to cultural keywords: a critical corpus-based analysis of ideology in the Blair years (1998-2007) through print news reporting",
        "authors": [
            {
                "given": " Lesley",
                "family": " Jeffries",
                "affiliation": [
                    {
                        "original_name": " The University of Huddersfield, UK",
                        "normalized_name": "University of Huddersfield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05t1h8f27",
                            "GRID": "grid.15751.37"
                        }
                    }
                ]
            },
            {
                "given": " Brian David",
                "family": " Walker",
                "affiliation": [
                    {
                        "original_name": " The University of Huddersfield, UK",
                        "normalized_name": "University of Huddersfield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05t1h8f27",
                            "GRID": "grid.15751.37"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-29",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  By \"hermeneutic\" markup I mean markup that is deliberately interpretive. It is not limited to describing aspects or features of a text that can be formally defined and objectively verified. Instead, it is devoted to recording a scholar's or analyst's observations and conjectures in an open-ended way. As markup, it is capable of automated and semi-automated processing, so that it can be processed at scale and transformed into different representations. By means of a markup regimen perhaps peculiar to itself, a text would be exposed to further processing such as text analysis, visualization or rendition. Texts subjected to consistent interpretive methodologies, or different interpretive methodologies applied to the same text, can be compared. Rather than being devoted primarily to supporting data interchange and reuse – although these benefits would not be excluded – hermeneutic markup is focused on the presentation and explication of the interpretation it expresses.  Hermeneutic markup in its full form does not yet exist. XML, and especially TEI XML, provides a foundation for this work. But due to limitations both in currently dominant approaches to XML, and in XML itself, a number of important desiderata remain before truly sophisticated means can be made available for scholars to exploit the full potentials of markup for literary study, as implied, for example, by ideas such as Steven Ramsay's Algorithmic Criticism or what I described in 2001 (following Rockwell and Bradley) as \"exploratory markup\" (Piez 2001. See also especially Buzzetti, 2002 and McGann, 2004).  Prototype user interfaces designed to enable one or another kind of ad hoc textual annotation or markup have been developed, for the most part independently of one another (several are cited.). This shows that the idea of hermeneutic markup, or something like it, is not new; but none of these have yet made the breakthrough. An important reason is that hermeneutic markup in its full sense will not be possible on the basis simply of a standard tag set or capable user interface, because it will mean not just that we can describe a data set using markup (we can already do that), but that we can actively develop, for a particular text or family of texts, an appropriate, and possibly highly customized, means and methodology for doing so.  A demonstration of a prototype markup application helps to show the potentials and challenges, in a very rudimentary form [screenshots appear in Figure 1.] This graphical and interactive rendering of markup in the source files presents an interpretation of the grammatical/rhetorical structure (sentences and phrases) as well as verse structure (lines and stanzas) in the text. Unfortunately, while the encoding for the sonnets here is not inordinately difficult – \"milestones\" are used, in a conventional manner, to denote the presence of structures that overlap the primary structure of the encoded document – the code that renders it (not included in the package) incurs significant extra overhead to run, because XML technologies are ill-fitted to manage the kind of information we are interested in here, namely the overlapping of these structures that characterizes the sonnet form. XML doesn't do overlap. As long as a sentence or phrase overlaps a line – a very common occurrence and important poetic device – the normative XML data model, a \"tree\", cannot capture both together. In order to do processing like what happens here, one or another workaround has to be resorted to. So while XML is being used here, it is a clumsy means to this end.           Figure 1: Screenshots of three sonnets with rendition of overlapping (verse and sentence/phrase) structures. The interface (implemented in W3C-standard SVG) is dynamic and responds to user input to highlight overlapping ranges of text.   But overlap is only part of the problem. Consider Alfred Lord Tennyson's Now Sleeps the Crimson Petal, Now the White. This too is a sonnet, after a fashion, although it does not have a conventional sonnet's octave/sestet structure. Since this application does not work with a schema, this is not a problem here. Yet as texts or collections grow in scale and complexity, having a schema is essential to enforcing consistency and helping to ensure that like things are marked up alike. A framework for this application must not only find a way to work around the overlap; it must also deploy a schema (or at any rate some sort of validation technology) flexible enough – at least if this instance is to be valid to it – that such outliers from regular form are permissible, even while attention is drawn to them (see Birnbaum 1997). Currently, XML developers generally (setting aside the problem of overlap) do not consider this to be problematic in itself; indeed, part of the fun and interest of markup is in devising and applying a schema that fits the data, however strange and interesting it may be. What is not so fun is to have to repeat this process endlessly, being caught in a cycle of amending and adjusting a schema constantly (and sooner or later, scripts and stylesheets) in order to account for newly discovered anomalies. Sooner or later, when exhaustion sets in or the limits of technical knowhow are reached, one ends up either faking it with tags meant for other purposes (thereby diluting markup semantics in order to pretend to represent the data), or just giving up. Extending a schema is found to be a problem not only because validating and processing any model more complex than a single hierarchy is a headache even for technical experts, but also, more generally, because current practices assume a top-down schema development process. Despite XML's support for processing markup even without a schema, both XML tools and dominant development methodologies assume that schema design and development occurs prior to the markup and processing of actual texts. This priority is both temporal and logical, reflecting a conception of the schema as a declaration of constraints over a set of instances (a “type”), appropriate to publishing systems built to work with hundreds or thousands of documents, with a requirement for backwards compatibility (documents encoded earlier cannot be revised easily or at all) and limited flexibility to adapt to new and interesting discoveries. The centrality of the schema within this kind of system inhibits, when it does not altogether frustrate, the flexible development of a markup practice that is sensitive, primarily, to a text under study, and this conception of a schema's authority works poorly when considering a single text sui generis – the starting point for hermeneutic markup. In hermeneutic markup, a schema should be, first and last, an apparatus and a support, not a Procrustean bed. All these problems together indicate the outline of a general solution:  A data model supporting arbitrary overlap. Interfaces, including a markup syntax, that facilitate the creation, editing and analysis of texts using this data model, with the capability of defining ad hoc elements and properties (attributes) on the fly. A transformation technology supporting (in addition to data transformations) analytical tools applicable to the markup as such (not just the raw text), with the capability of managing elements and their properties in sets, locating them, listing them by type, sorting, visualizing and comparing them. Schema-inferencing capabilities for describing the structural relations within either an entire marked-up corpus, or within identifiable segments, sections or profiles of it. In connection this, a schema technology that supports partial and modular validation.   A system with all these features would support an iterative and \"agile\" approach to markup development. We would start by tagging. (In a radical version of this approach we might start by tagging for presentation, perhaps using just a lightweight HTML or TEI variant for our first cut.) Then we introduce a provisional schema or schemas capable of validating the tagging we have used. This requires assessing which cases of overlap in the text are essential to our document analysis, and which are incidental and subject to normalization within hierarchies. Having refined the schema, we return to the tagged text, to consider both how its tagging falls short (with respect to whatever requirements we have for either data description or processing), and how it may be enhanced, better structured and regularized. During this process we also begin to develop and deploy applications of the markup. We then revise, refactor and extend both tagging and schema, applying data transformations as needed, in order to better address the triple goals of adequate description, processing, and economy of design. Such a system would not only be an interesting and potentially ground-breaking approach to collaborative literary study; it would also be a platform for learning about markup technologies, an increasingly important topic in itself. Moreover, hermeneutic markup represents an opportunity to capitalize on investments already made, as texts encoded in well-understood formats like TEI are readily adaptable for this kind of work. Many of these capabilities have already been demonstrated or sketched in different applications or proposals for applications, including W3C XML Schema (partial validation); James Clark's Trang (schema inferencing for XML); LMNL/CREOLE (overlap, structured annotations, validation of overlap); JITTs (XML \"profiles\" of concurrent overlapping structures); and TexMECS (overlap, \"virtual\" and discontinuous elements). The presentation will conclude with a demonstration of various outputs from the data sources used in the demo, which provide building blocks towards the kind of system sketched here. A range-analysis transformation can show which types of structures in a markup instance overlap with other structures, and conversely which structures nest cleanly. Complementary to this, an \"XML induction processor\" is capable of deriving well-formed XML representations of texts marked up with overlapping structures – from which, in turn, XML schemas can be derived.   Figure 2: A workflow diagram showing the architecture of present (XML-based) markup systems. Both schema and processing logic are considered to be static; modifying them is an activity extraneous to document markup and production.     Figure 3: An architecture capable of supporting hermeneutic markup would account directly for document analysis and for the design of schema, queries and processing. While in fact this is often done even today, one has to work against the current tool set to do it, questioning its assumptions regarding the purposes, roles and relations of source text, markup and schema.    A final version of this paper, with the demonstrations, is available at    ",
        "article_title": "Towards Hermeneutic Markup: An architectural outline",
        "authors": [
            {
                "given": " Wendell",
                "family": "Piez",
                "affiliation": [
                    {
                        "original_name": "Mulberry Technologies, Inc., USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-29",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The aim of this study is to find a minimal size of text samples for authorship attribution that would provide stable results independent of random noise. A few controlled tests for different sample lengths, languages and genres are discussed and compared. Although I focus on Delta methodology, the results are valid for many other multidimensional methods relying on word frequencies and \"nearest neighbor\" classifications. In the field of stylometry, and especially in authorship attribution, the reliability of the obtained results becomes even more essential than the results themselves: failed attribution is much better than false attribution (cf. Love, 2002). However, while dozens of outstanding papers deal with increasing the effectiveness of current stylometric methods, the problem of their reliability remains somehow underestimated. Especially, the simple yet fundamental question of the shortest acceptable sample length for reliable attribution has not been discussed convincingly.  In many attribution studies based on short samples, despite their well-established hypotheses, convincing choice of style-markers, advanced statistics applied and brilliant results presented, one cannot avoid a very simple yet uneasy question: whether those impressive results could be obtained by chance, or at least positively affected by randomness? This question can be also formulated in a different way: if a cross-checking experiment with numerous short samples were available, would the results be just as satisfying?  Hypothesis It is commonly known that word frequencies in a corpus are random variables; the same can be said about any written authorial text, like a novel or poem. Being a probabilistic phenomenon, word frequency strongly depends on the size of the population (i.e. the size of the text used in the study). Now, if the observed frequency of a single word exhibits too much variation for establishing an index of vocabulary richness resistant to sample length (cf. Tweedie and Baayen, 1998), a multidimensional approach – based on several probabilistic word frequencies – should be even more questionable.  On theoretical grounds, we can intuitively assume that the smallest acceptable sample length would be hundreds rather than dozens of words. Next, we can expect that, in a series of controlled authorship experiments with longer and longer samples tested, the probability of attribution success would at first increase very quickly, indicating a strong correlation with the current text size; but then, above a certain value, further increase of input sample size would not affect the effectiveness of the attribution. In any attempt to find this critical point in terms of statistical investigation, one should be aware, however, that this point might depend – to some extent – on the language, genre, or even the text analyzed.   Experiment I: Words A few corpora of known authorship were prepared for different languages and genres: for English, Polish, German, Hungarian, and French novels, for English epic poetry, Latin poetry (Ancient and Modern), Latin prose (non-fiction), and for Ancient Greek epic poetry; each contained a similar number of texts to be attributed. The research procedure was as follows. For each text in a given corpus, 500 randomly chosen single words were concatenated into a new sample. These new samples were analyzed using the classical Delta method as developed by Burrows (2002); the percentage of attributive success was regarded as a measure of effectiveness of the current sample length. The same steps of excerpting new samples from the original texts, followed by the stage of \"guessing\" the correct authors, were repeated for the length of 600, 700, 800, ..., 20000 words per sample.  The results for a corpus of 63 English novels are shown on Fig. 1. The observed scores (black points on the graph; grey points will be discussed below) clearly indicate the existence of a trend (solid line): the curve, climbing up very quickly, tends to stabilize at a certain point, which indicates the minimal sample size for the best attributing rate. It becomes quite obvious that samples shorter than 5000 words provide a poor \"guessing\", because they can be immensely affected by random noise. Below the size of 3000 words, the obtained results are simply disastrous. Other analyzed corpora showed that the critical point of attributive success could be found between 5000 and 10000 words per sample (and there was no significant difference between inflected and non-inflected languages). Better scores were obtained for the two poetic corpora: English and Latin (3500 words per sample were enough for good results), and, surprisingly, the corpus of Latin prose (its minimal effective sample size was of some 2500 words; cf. Fig. 2, black points).   Experiment II: Passages The way of preparing samples by extracting a mass of single words from the original texts seems to be an obvious solution for the problem of statistical representativeness. In most attribution studies, however, shorter or longer passages of disputed works are usually analyzed (either randomly chosen from the entire text, or simply truncated to the desired size). The purpose of the current experiment was to test the attribution effectiveness of this typical sampling. The whole procedure was repeated step by step as in the previous test, but now, instead of collecting individual words, sequences of 500 words (then 600, 700, ..., 20000) were excerpted randomly from the original texts.  Three main observations could be made here: 1. For each corpus analyzed, the effectiveness of such samples (excerpted passages) was always worse than the scores described in the former experiment, relying on the \"bag-of-words\" type of sample (cf. Fig. 1 and 2, grey points). 2. The more inflected the language, the smaller the difference in correct attribution between both types of samples, the \"passages\" and the \"words\": the greatest in the English novels (cf. Fig. 1, grey points vs. black), the smallest in the Hungarian corpus. 3. For \"passages\", the dispersion of the observed scores was always wider than for \"words\", indicating the possible significance of the influence of random noise. This effect might be due to the obvious differences in word distribution between narrative and dialogue parts in novels (cf. Hoover, 2001); however, the same effect was equally strong for poetry (Latin and English) and non-literary prose (Latin).   Experiment III: Chunks At times we encounter an attribution problem where extant works by a disputed author are doubtless too short for being analyzed in separate samples. The question is, then, if a concatenated collection of short poems, epigrams, sonnets, etc. in one sample (cf. Eder and Rybicki, 2009) would reach the effectiveness comparable to that presented above? And, if concatenated samples are suitable for attribution tests, do we need to worry about the size of the original texts constituting the joint sample?  The third experiment, then, was designed as follows. In 12 iterations, several word-chunks were randomly selected from each text into 8192-word samples: 4096 bi-grams, 2048 tetra-grams, 1024 chunks of 8 words in length, 512 of 16 words, and so on, up to 2 chunks of 4096 words. Thus, all the samples in question were 8192 words long. The obtained results were very similar for all the languages and genres tested. As shown in Fig. 3 (for the corpus of Polish novels), the effectiveness of \"guessing\" depends to some extent on the word-chunk size used. Although the attributive scores are slightly worse for long chunks within a sample (4096 words or so) than for bi-grams, 4-word chunks etc., every chunk size could be acceptable to constitute a concatenated sample.  However, although this seems to be an optimistic result, we should remember that this test would not be feasible on really short poems. Epigrams, sonnets etc. are often masterpieces of concise language, with a domination of verbs over adjectives and so on, and with a strong tendency to compression of content. For that reason, further investigation is needed here.   Conclusions The scores presented in this study, as obtained with classical Delta procedure, would be slightly better when solved with Delta Prime, and worse if either Cluster Analysis or Multidimensional Scaling is used (a few tests have been done). However, the shape of all the curves, as well as the point where the attributive success rate becomes stable, are quite identical for each of these methods. The same refers to different combinations of style-markers' settings, like \"culling\", the number of the Most Frequent Words analyzed, deleting/non-deleting pronouns, etc. – although different settings provide different \"guessing\" (up to 100% for the most efficient), they never affect the shape of the curves. Thus, since the obtained results are method-independent, this leads us to a conclusion about the smallest acceptable sample size for future attribution experiments and other investigations in the field of stylometry. It also means that some of the recent attribution studies should be at least re-considered. Until we develop style-markers more precise than word frequencies, we should be aware of some limits in our current approaches. As I tried to show, using 2500-word samples will hardly provide a reliable result, to say nothing of shorter texts.     Figure 1: English novels  English novels, attribution effectiveness. Single-word (red points) and text-passage (green points) samples     Figure 2: Latin prose  Latin prose, attribution effectiveness. Single-word (red points) and text-passage (green points) samples     Figure 3: Polish novels  Polish novels, attribution effectiveness. Samples of 8192 words, concatenated in chunks of 2, 4, 8, 16, 32, ..., 8192 words    ",
        "article_title": "Does Size Matter? Authorship Attribution, Small Samples, Big Problem",
        "authors": [
            {
                "given": " Maciej",
                "family": "Eder",
                "affiliation": [
                    {
                        "original_name": "Pedagogical University, Krakow, Pedagogical University of Krakow Poland",
                        "normalized_name": "Pedagogical University",
                        "country": "Mozambique",
                        "identifiers": {
                            "ror": "https://ror.org/0331kj160",
                            "GRID": "grid.442441.3"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-05-01",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " \"Records are no longer fixed, but dynamic. The record is no longer a passive object, a 'record' of evidence, but an active agent playing an on-going role in lives of individuals, organizations, and society.\"Terry Cook. \"Archival science and postmodernism: new formulations for old concepts.\" Archival Science. 1, 2001. 22.  Advances in digital representation and preservation have ushered in new perspectives for defining the record. Terry Cook, speaking on behalf of a growing cohort interested in reshaping the disciplinary boundaries of archival studies, argues that records no longer possess the aura of absolute authority that they once held. Practitioners and theorists working within a postmodernist framework have broken open the once sacred bond between the historical craft and the archive, in the process challenging notions of evidence, truth, and narrative.  One of the great beneficiaries and active participants of this re-evaluation of the archive and the record, of course, is digital history. Digital tools, from Omeka to ArcGIS, have empowered a growing community of professional and amateur historians, museums, and libraries to provide unprecedented access to collections of primary materials and historical data. Such tools also demonstrate the ease with which archival and historical practices have come into contact with one another, thereby disrupting conventional understanding of the record in both fields.  In this paper, I will address the cross-disciplinary relationship between history and archival theory as one component in the broader development of a much-needed digital historiography.The term \"digital historiography\" has lingered in the background of the field throughout the last decade, most notably in a series of reviews by David Staley in the Journal of the Association for History and Computing between 2001-2003. I will argue that principles of archival theory and historiography together may guide the evaluation of digital and new media historical representations, especially with regards to the contextualization of historical evidence. Whether considering an online archive, database, or GIS visualization, the aggregation of large data sets necessitates proper archival management of the data. Besides enhancing the long-term sustainability and preservation of the representation – itself a worthy and often overlooked objective –, the application of archival standards to data collection, organization, and presentation influences the type and quality of conclusions users may generate. Within this complex association among history, archival theory, and digital technology this paper will examine two interrelated \"building blocks\" – search and metadata – that work hand-in-hand to form the foundation for a sound digital historical representation. How a user queries a representation, even one that is non-textual, governs the quality of historical knowledge at the user's disposal, whereas a representation's content metadata governs the conclusions the user may draw with that knowledge.   A Call for Digital Historiography New techniques to query, sort, catalogue, and visualize historical data have brought renewed interest to understanding the past on every scale, from the personal to the global. As scholars and teachers, we have encouraged digital exploration, whether in gathering local data with the support of a historical society or archive, or repurposing historical materials through museum installations, websites, documentaries, and multimedia mashups.  Despite promising possibilities in historical computing, the emergence of digital history has also created a distinct fissure in the wider field of history. Practicing digital history challenges methodological preconceptions. Conducting search queries across vast digital collections seems antithetical to visiting an archive. Similarly, navigating through a three-dimensional environment enhances interactivity and engagement with the historical representation, and in the process confronts, or at times abandons altogether, the core activities of reading and writing historical texts. In short, history in the digital age has upended notions of representation, context, inquiry, narrative, linearity, temporal and spatial orientation, and experience.For a recent survey of the digital history field and its variant representational forms see Paul Arthur. \"Exhibiting history: The digital future.\" reCollections: The National Museum of Australia. Vol. 3, number 1. http://recollections.nma.gov.au/issues/vol_3_no_1/papers/exhibiting_history/. Accessed March 12, 2010. This undeniable shift in the landscape demands that we harness the potential of digital history while not altogether abandoning established theoretical and methodological practices. A rush to embrace new digital modes of doing history, unfortunately, has overwhelmed a parallel critical examination of changes to these fundamentals. The same techniques and technologies that are laudably tearing down institutional barriers, challenging entrenched theories, and introducing new voices and democratic perspectives, can also advance specious information and theories; distort or obscure the historical record; or worse – eliminate it altogether. The role of the historian, therefore, has shifted from that of exclusive authority to the equally critical role of mediator of historical knowledge. If active participation and exploration have become the benchmarks of digital historical representations, then the (digital) historian must ensure that the manner of user participation is conducted equitably and responsibly insofar as the knowledge produced through the representation is predicated upon rigorous logic and concentrated historical data.  What principles should a new digital historiography advocate and why is its cultivation imperative? A working digital historiography will enable critical engagement with digital and new media representations, a challenging endeavor considering the spectrum of possible forms that a representation may take. We may justifiably question whether an online collection, for example, shares traits with a GIS-based visualization. While each representational genre warrants a unique set of evaluative criteria, commonalities across formats and historical content do exist and warrant further attention. We may begin with the notion that all representations possess some form of a user interface. Interrogating the user interface can lead one to assess the transparency with which the representation has selected and organized its content. We may also ask whether its formal design complements and provides sufficient access to the content. With a scholarly text, answers to such questions are readily apparent by poring over indicators such as footnotes, bibliography, and the table of contents. Many digital representations, however, collate information within multi-dimensional, non-linear structures, thereby subverting or eliminating such identifiable cues. As Edward Ayers remarks, \"We cannot judge a Web site by its cover – or its heft, its publisher's imprint, or the blurbs it wears.\"Edward L. Ayers. \"Technological Revolutions I Have Known.' In Computing in the Social Sciences and Humanities. Ed. Orville Vernon Burton. University of Illinois Press, 2002. 27. My call for a rigorous digital historiography coincides with Ayers' own remarks, when he writes: \"Whatever a project's scale and level of complexity, new media should meet several standards to justify the extra effort they take to create, disseminate, and use.\"    The Building Blocks of Digital Historiography: Search and Metadata In developing a set of evaluative criteria, we must consider the association between a representation's form and content, which together comprise the representation's overall historical argument. While there are numerous components worthy of consideration, two in particular – search and metadata – determine to a large extent how a representation organizes its historical information. Without a robust search engine the user cannot access historical data; similarly, without quality metadata, a strong search engine is rendered ineffective. While this may seem self-evident, the integration of search and metadata in a representation runs much deeper; it affects, and is affected by, nearly every aspect of the representation, including its interface, aesthetic, design, structure, and functionality. Search and metadata together govern the transformative process by which historical information becomes historical evidence.  This paper will use examples of current digital collections and visualizations to illustrate how search and metadata contributes to the overall value of the representation. I will argue that an assessment of these two building blocks, when considered from both an historical and archival perspective, can shed light on the argument put forth by the representation. In the case of an online collection, for example, the creator must weigh the benefits of generating metadata according to standardized thesauri, scholarly input, or folksonomy. These very different approaches, if applied to the same archival collection, would not only influence the type of audience that may use the archive, but also steer users towards divergent search results, which could ultimately determine how the content is recombined.For further discussion on how archival description can shape the narrative embedded within archival records, see Wendy Duff and Verne Harris. \"Stories and Names: Archival Description as Narrating Records and Constructing Meanings.\" Archival Science. 2: 263-285, 2002.  A reconstruction of an historic building, meanwhile, invites a \"search\" process of a different sort. Searching occurs while the user navigates through the environment. Is the user invited to discover new sightlines or gauge the distance between structures? If so, does the user have access to previous theories with which to compare a new finding? Even small questions, such as why a virtual archway was set at eight feet instead of six when there may be inconclusive evidence for both, can unearth rich discoveries. The reconstruction thus must make architectural or GIS metadata discoverable, to the extent that this is feasible, in order to foster further investigation.  It is critical that we do not lose sight of the underlying question that should guide the creation and evaluation of all digital historical representations: does the representation invite the user to conduct humanistic inquiry? What are the historical problems encompassed by the representation, and does the evidence compel the user towards addressing those questions or asking new ones? The more the user is made aware of a representation's construction, the greater the potential for productive engagement. Search and metadata thus function as the bridge linking a representation's formal structure and content. Evaluating these two areas along archival and historiographical lines can lead to an assessment of its trustworthiness as a source for generating historical knowledge. In other words, interrogating a representation's search and metadata provides a window to explore a representation's construction of historical context.  This paper will not advocate a single approach or methodology for applying and evaluating search and metadata to a digital representation; rather, it will argue that digital historians should think archivally when considering how these components contribute to a representation's historical contextualization. Refinement of this mindset through rigorous, systematic, and interdisciplinary theoretical and practical experimentation could benefit scholarship, peer review, pedagogy, public history, and cultural heritage.Among the possible applications could be the development of higher education curriculum constructed around a hybrid digital history-archival studies model. NYU's Archives and Public History is one of the leading programs that have taken up the call to teach archival theory alongside digital history theory and practice. It recently unveiled a new website showcasing its revamped academic program: http://aphdigital.org/. Accessed March 12, 2010.The thoughts and ideas expressed in this abstract and the conference presentation are entirely my own and do not necessarily reflect those of NEH or any other federal agency.      ",
        "article_title": "Thinking Archivally: Search and Metadata as Building Blocks for a New Digital Historiography ",
        "authors": [
            {
                "given": " Joshua",
                "family": "Sternfeld",
                "affiliation": [
                    {
                        "original_name": "National Endowment for the Humanities, USA",
                        "normalized_name": "National Endowment for the Humanities",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02vdm1p28",
                            "GRID": "grid.422239.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-16",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The goal of automatic phrase break prediction is to emulate human performance in terms of naturalness and intelligibility when assigning prosodic-syntactic boundaries to input text. Techniques can be deterministic or probabilistic; in either case, the problem is treated as a classification task and outputs from the model are evaluated against 'gold standard' phrase break annotations in the reference dataset or corpus. These annotations may represent intentions (of the speaker or writer) or perceptions (of the listener or reader) about alternating chunks and boundaries in the speech stream or in text, where the chunking bears some relationship to syntactic phrase structure but is thought to be simpler, shallower and flatter.  In this paper, we begin by reviewing methodologies and feature sets used in phrase break prediction. For example, a tried and tested rule-based method is to employ some form of 'chink-chunk' algorithm (Liberman and Church, 1992) which inserts a boundary after punctuation and whenever the input string matches the sequence: open-class or content word (chunk) immediately followed by closed-class or function word (chink), based on the principle that chinks initiate new prosodic phrases.  We discuss the limitations of using traditional features in the form of syntactic and text-based cues as boundary correlates, with illustrative experimental predictions from a shallow parser and evidence from the corpus. We then discuss the limitations of evaluating any phrase break model against a \"gold standard\" which itself only represents one phrasing variant for an utterance or text.  There is an emerging trend of leveraging real-world knowledge to improve performance in machine learning, including speech and language applications. Nevertheless, we have diagnosed a deficiency of a priori knowledge of prosody in the feature sets used for the phrase break prediction task. In contrast, a competent human reader is able to project holistic linguistic insights, including projected prosody, onto text and to treat them as part of the input (Fodor, 2002). In this respect, multiple prosodic annotation tiers in the Aix-MARSEC corpus (Auran et al., 2004) have been revelatory, since they capture the prosody implicit in text and currently absent in learning paradigms for phrase break models.  Insights such as: (i) the transferability of the chinks and chunks rule; plus (ii) the possibility of encoding a variety of prosodic phenomena (including rhythm and beats) in categorical labels (cf. the Aix-MARSEC corpus); plus (iii) an appreciation of prosodic variance gleaned from corpus evidence of alternative parsing and phrasing strategies, have informed the creation of ProPOSEL (Brierley and Atwell, 2008a; 2008b), a domain-independent prosodic annotation tool.  ProPOSEL is a prosody and part-of-speech English lexicon of 104,049 entry groups, which merges information from several widely-used lexical resources for corpus-based research in speech synthesis and speech recognition. Its record structure supplements word-form entries with syntactic annotations from four rival POS-tagging schemes, mapped to fields for: default open and closed-class word categories; syllable counts; two different phonetic transcription schemes; and lexical stress patterns, namely abstract representations of rhythmic structure (as in 201 for disappear, with secondary stress on the first syllable and primary stress on the final syllable).  We then contend that native English speakers may use certain sound patterns as linguistic signs for phrase breaks, having observed these same patterns at rhythmic junctures in poetry. We also contend that such signs can be extracted from canonical forms in the lexicon and presented as input features for the phrase break classifier in the same way that real-world knowledge of syntax is represented in POS tags; and that like content-function word status or punctuation, such features are domain-independent and can be projected onto any corpus. One such sound pattern is the subset of complex vowels, which we define as the eight diphthongs, plus the triphthongs, of Received Pronunciation (Roach, 2000: 21-24).  Finally, we test the correlation between pre-boundary lexical items bearing complex vowels and gold-standard phrase break annotations on different kinds of speech via the chi-squared statistic, to determine whether the perceived association is statistically significant or not. Our findings indicate that this correlation is extremely statistically significant: it is present in contemporary, formal, British English speech (Brierley and Atwell, 2009) and seventeenth century English verse (Brierley and Atwell, 2010a); and it holds for spontaneous as well as read speech, and for multiple speakers (Brierley and Atwell, 2010b). We hypothesise that while complex vowels seem to constitute phrase break signifiers in English, this may translate to a subset of the vowel system in other languages.   ",
        "article_title": "Non-traditional Prosodic Features for Automated Phrase-Break Prediction",
        "authors": [
            {
                "given": " Claire",
                "family": "Brierley",
                "affiliation": [
                    {
                        "original_name": "University of Bolton, UK",
                        "normalized_name": "University of Bolton",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01t884y44",
                            "GRID": "grid.36076.34"
                        }
                    }
                ]
            },
            {
                "given": " Eric",
                "family": "Atwell",
                "affiliation": [
                    {
                        "original_name": "University of Leeds, UK",
                        "normalized_name": "University of Leeds",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/024mrxd33",
                            "GRID": "grid.9909.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-18",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The philosophy of \"literate programming\" (Knuth 1984), on which the TEI ODD is founded, proposes that code and documentation be written and maintained as a single integrated resource, from which both working programs and readable documentation can be generated. As currently designed, the TEI ODD system supports these goals, and there exist several good examples of extended project documentation written using the ODD customization file (see (Trolard 2009), (Burnard and Sperberg-McQueen 2006), (Burnard et al. 2010) and also the TEI Guidelines themselves). However, at present these examples only assume and demonstrate the ability to generate two types of documentation: a prose narrative and a set of reference documentation. As text encoding projects develop and mature they generate a variety of documentation that may include training tutorials and reference documentation, public documentation of editorial and encoding practices, documentation of their TEI customization, and internal documentation of the encoding decisions that have resulted in their current encoding rationale. Many of these other forms of output (such as training tutorials) have not been tried in practice and the current ODD processor, Roma, does not explicitly support them. In this paper we explore the possibility of generating more complex and varied forms of documentation using the TEI ODD customization file.  As background for this discussion we should begin by describing the nature and role of this customization file. Underlying any TEI-encoded document is a schema defining the terms of its validity, and underlying that schema is a further specification: the ODD customization file, which is the source file from which the schema is generated (documented in detail in chapters 22 and 23 of the TEI Guidelines (TEI 2007)). The ODD file serves several important functions:  To express the specific choices that are being made with respect to the TEI system as a whole: which TEI modules are to be included in the generated schema, which elements and attributes from these modules are to be included or omitted, changes to content models, etc. To document those choices: for instance, to explain the meaning of controlled vocabularies for attribute values, or to express the rationale for applying an element only in specific contexts or with a slightly broader or narrower significance than described in the Guidelines. To permit the generation (using these two kinds of information) not only of a custom TEI schema but also of custom documentation.   This custom documentation includes a re-expression of the TEI reference documentation: that is, the second volume of the printed TEI Guidelines, the portion containing separate entries for each element, attribute, class, and macro. This custom reference documentation includes only references to elements, attributes, and classes that are actually present in the custom schema, and includes as part of these entries some of the additional documentation expressed as part of point 2 above (e.g. glosses of specific attribute values, etc.).  The goal of the TEI customization mechanism as a whole, then, is two-fold. First, it aims to make TEI schemas self-documenting, by encapsulating all of the choices made in a separate document (the customization ODD file) that can be stored, maintained, exchanged. And second, to a certain extent the mechanism is intended to permit encoding systems to be self-documenting, in the sense that the documentation of the encoding practice can be bundled together with the raw materials for creating and maintaining the custom schema. This is true in a very straightforward manner to the extent that information about encoding practice can be directly associated with specific schema modifications. But more complex documentation is also possible: since the ODD file is a TEI document, one can also include more detailed documentation that is not directly associated with a schema modification, simply by adding prose to that TEI document. When the ODD file is processed, the resulting documentation will include that additional prose. This more detailed approach is currently uncommon, but this is primarily because the Roma web interface does not currently support the authoring of such documentation; users need to author the ODD directly in order to create more complex documentation forms. Examples of this more detailed approach include the TEI in Libraries best practices documentation (Hawkins and Bauman 2009), and also the documentation for TEI Lite (Burnard and Sperberg-McQueen 2006).  With this in mind, however, it is tempting to extend this process even a step further: to use the ODD file as a way of writing documentation of other sorts that are even less closely attached (in their methods of organization) to the schema specifications. For example, for many purposes a project may need to maintain both reference-style documentation for each element or encoding concept, and also tutorial-style documentation whose emphasis is on leading the reader through a pedagogically structured narrative. Encoders learning to transcribe manuscript diaries might need to learn first the specific set of structural elements that will be used to encode the overall structure of the document (<div>, <opener>, <dateLine>, etc.) and then the set of elements having to do with transcriptional difficulties (<gap>, <unclear>, <add>, <del>). At the same time, in other areas of the project it might be essential to have documentation of the underlying rationale for the encoding approach, or a high-level narrative with links to specific entries. More importantly, different tutorials or forms of documentation might need to use particular portions of the specification in different orders: the tutorial format might take specific sets of elements and treat them as groups, while a more comprehensive narrative documentation might treat the same encoding concepts in alphabetical order, or by conceptual grouping, or by TEI module (to take just a few examples).  To support the generation of multiple documentation narratives from a single ODD requires two changes to the way the ODD is written. First, additional prose (from which the various narratives will be generated) will need to be included in the ODD, and the ODD itself may need to be organized somewhat differently to accommodate this prose. Second, and more challengingly, some mechanism is required by which the different narrative orderings can be expressed in the ODD and then processed to produce the various appropriate forms of output. These different forms of documentation could of course be maintained separately (as is currently the case) but the value of the ODD system lies in its philosophy (expressed somewhat obscurely in the word \"ODD\", or \"one document does-it-all\") of having a single document that expresses and documents all aspects of a TEI encoding scheme. Rather than creating separate prose documentation for these additional forms, there is clear value in being able to generate multiple forms of documentation serving different purposes, from a single ODD.  We are not aware of any examples of this latter type, but the utility of this approach is clear and the ODD language – because it is part of the TEI and thus can use the full expressive resources of the TEI language as a whole – contains the features necessary to support it. In this paper we present an initial implementation in which we construct a set of tutorials on specific encoding topics in areas where the WWP has customized the TEI (for instance verse, title pages, and notes), using the ODD mechanism. The approach we explore entails encoding the components of the various narratives using standard TEI prose and documentation elements, and constituting each individual narrative using stand-off markup to identify and assemble the pieces in the appropriate order for the documentary form in question. We present the proposed ODD encoding for these tutorials, using the Women Writers Project internal documentation as a testbed. We also present prototype stylesheets that will generate multiple documentation narratives from the single ODD source.   ",
        "article_title": "Using ODD for Multi-purpose TEI Documentation",
        "authors": [
            {
                "given": " Julia",
                "family": "Flanders",
                "affiliation": [
                    {
                        "original_name": "Women Writers Project, Brown University, USA",
                        "normalized_name": "Brown University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05gq02987",
                            "GRID": "grid.40263.33"
                        }
                    }
                ]
            },
            {
                "given": " Syd",
                "family": "Bauman",
                "affiliation": [
                    {
                        "original_name": "Women Writers Project, Brown University, USA",
                        "normalized_name": "Brown University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05gq02987",
                            "GRID": "grid.40263.33"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-20",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Creative partnership between computer science and the humanities – what we now call \"digital humanities\" – is the cornerstone of the digital revolution. Cathy Davidson writes (2008) that \"perhaps we need to see technology and the humanities not as a binary but as two sides of a necessarily interdependent, conjoined, and mutually constitutive set of intellectual, educational, social, political, and economic practices.\" Significant educational challenges exist, however, in creating a cadre of professionals who understand the intellectual context of digital humanities research and who are also capable of building the supporting infrastructure of digital collections, tools, and services (de Smedt 2002). The American Council of Learned Societies' groundbreaking report – Our Cultural Commonwealth: Report of the Commission on Cyberinfrastructure for the Humanities and Social Sciences – focuses attention on the need to \"cultivate leadership in support of cyberinfrastructure from within the humanities and social sciences, encourage digital scholarship, develop and maintain open standards and robust tools, and create extensive and reusable digital collections\" (ACLS 2006, p. 4).  An international network of digital humanities centers creates and develops access to the digital documents, images, languages, sound, and film that constitute the human record and facilitate its understanding. In a quite separate but potentially symbiotic movement, graduate schools of information in the United States and elsewhere are producing technologically sophisticated professionals with deep backgrounds in and commitments to the humanities. Schools of Information, or \"iSchools\" have emerged from a two-decade long era of consolidation and reform, during which traditional schools of library science struggled with irrelevancy, diminished scale, and a fundamental societal transformation in the use of new and emerging technologies (Sawyer 2008). In North America, twenty-four iSchools have formed a caucus  (http://www.ischools.org/) to advance a common agenda regarding the future of information studies. John Unsworth (2007) notes that digital humanities centers can establish new working relationships between humanities faculty and iSchool programs. iSchool faculty \"are about half from other disciplines, and humanities computing is very much about information organization, ontologies, taxonomies, schema, preservation, interface design, and other issues that are studied and taught in [iSchool] programs. The [iSchool] connection also would help to activate the NEH/IMLS connection, as well as the NSF cyberinfrastructure connection.\"  While the move to develop digital humanities centers has demonstrated great successes, it has also meant the development of a number of unique but remote archives that are in danger of being lost. Universities in this digital age need to produce research and graduates that transcend traditional barriers and ways of working. The most influential origins of change wrought by information technology might well emerge from the humanities and information sciences, which consider most deeply the heritage and future of the human experience. The progress of this interdisciplinary field, however, requires new models of collaboration among the information sciences and the humanities disciplines. In this context it is worth noting that all of the iSchools involved in the digital humanities internship program (described below) have well established archives programs, from which they recruit graduate students to send to the participating DH Centers.  This paper for DH2010 presents a new model partnership initiative to help build curricular and scholarly institutional infrastructures that leverage the existing and emerging capabilities of iSchools and digital humanities centers. With generous three-year support from the U.S. Institute of Museum and Library Services (IMLS), three iSchools and three digital humanities centers are placing graduate student interns for extended summer work experiences in digital humanities centers. The collaborators include the iSchools at the University of Maryland, the University of Texas and the University of Michigan; and the DH Centers at the University of Maryland (Maryland Institute for Technology in the Humanities), the University of Nebraska (Center for Digital Research in the Humanities), and Michigan State University (MATRIX). The partners are also developing a collaborative research program that draws on complementary areas of expertise and interest in the digital humanities and information studies. The project is in its second year, preparing to place a second set of interns in summer 2010, with a total of 18 internships offered over the duration of the project.  The DH2010 paper contextualizes the model internship program within the broader academic framework of the mission and activities of iSchools, including the humanities-oriented profiles of students, a curriculum that meaningfully combines--in holistic fashion--computational, legal, informational, cultural, social, and managerial content; and faculty research that crosses the two cultures of the humanities and sciences.   Student Profiles The DH2010 paper presents a demographic analysis of the students enrolled at the three collaborating iSchools, demonstrating the affinity (and enriching the alliance) between iSchools and DH Centers. A majority of students enter into iSchool programs with undergraduate and/or graduate degrees in the arts and humanities, frequently outnumbering their more science-oriented peers by statistically significant margins. At Maryland's iSchool, for example, approximately 62 percent of current masters students (or 212 out of 343) obtained undergraduate degrees in English, History, Art History, Religion, Classics, and Philosophy. At Texas, well over half of the students have solid humanities backgrounds in literature, the arts, and especially in history; and show an impressive understanding of the values, styles, and methods of humanities researchers. At Michigan the humanities subject expertise of fully one-third of entering graduate students is integrated into a broader framework that incorporates techniques for systematically creating, managing, preserving and otherwise enhancing the value of cultural heritage information.    iSchool Curriculum The DH2010 paper exposes how iSchools have implemented curricula of relevance to digital humanities centers, particularly in the area of cyberinfrastructure. At Michigan, for example, a suite of technology/systems-oriented courses teach students how to build and evaluate dynamic complex websites and databases; undertake preservation reformatting of books, graphical, and audiovisual resources; produce EAD finding aids and other access tools; and create and maintain online communities. At Maryland, students are exposed to the legal issues in managing information and the corpus of documents – such as donor agreements – that codify them. Copyright, privacy, freedom of information, and other topics pertinent to archives and digital libraries are also covered. At Texas, a series of courses in digitization for preservation and access is paired with courses in digital libraries and a sequence developing digital archiving practices to provide students with a range of skills and knowledge pertinent to preserving and providing access to humanities content.   Faculty Research The DH2010 paper shows that iSchool faculty – an increasing number of whom have PhDs in arts and humanities disciplines – often conduct research with the potential to leverage and support the work of DH Centers. At Michigan, for example, Paul Resnick is pioneering work on recommender systems and the incentives that motivate eCommunities; Paul Conway is discovering how image quality issues in the large-scale digitization of cultural heritage resources impact innovative scholarship and use. At Texas, Gary Geisler is delving into improved interfaces for digital library presentations of materials from collections of the Harry Ransom Humanities Research Center; Patricia Galloway is investigating the use of tools from information retrieval and computational linguistics to frame digital corpora for the purposes of management and presentation. At Maryland, Derek Hansen and Kari Kraus recently received an NSF grant to understand and tailor alternate reality games for purposes of education and scholarly collaboration.  The DH2010 paper demonstrates how the first round of internships from the model program is establishing rich connections between iSchools and DH Centers. The model is sufficiently well articulated that it could further close working relationships between arts and humanities departments and DH Centers that wish to develop their own signature majors, minors, or concentrations in digital humanities. The paper concludes with a summary of successes, progress, and road-blocks in implementing the new internship model.   Funding This work was supported by The Institute of Museum and Library Services [grant number RE-05-08-0063-08].   ",
        "article_title": "Digital Humanities Internships: Creating a Model iSchool-Digital Humanities Center Partnership ",
        "authors": [
            {
                "given": " Paul",
                "family": "Conway",
                "affiliation": [
                    {
                        "original_name": "School of Information, University of Michigan, USA",
                        "normalized_name": "University of Michigan–Ann Arbor",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00jmfr291",
                            "GRID": "grid.214458.e"
                        }
                    }
                ]
            },
            {
                "given": " Neil",
                "family": "Fraistat",
                "affiliation": [
                    {
                        "original_name": "Maryland Institute for Technology in the Humanities, University of Maryland, USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Patricia",
                "family": "Galloway",
                "affiliation": [
                    {
                        "original_name": "School of Information, University of Texas, Austin University of Texas at Austin, USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Kari",
                "family": "Kraus",
                "affiliation": [
                    {
                        "original_name": "College of Information Studies, University of Maryland, USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Dean",
                "family": "Rehberger",
                "affiliation": [
                    {
                        "original_name": "MATRIX, Michigan State University, USA",
                        "normalized_name": "Michigan State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05hs6h993",
                            "GRID": "grid.17088.36"
                        }
                    }
                ]
            },
            {
                "given": " Katherine",
                "family": "Walter",
                "affiliation": [
                    {
                        "original_name": "Center for Digital Research in the Humanities, University of Nebraska-Lincoln, USA",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-20",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Overview The Modern Art Iraq Archive (MAIA) project is a participatory content-management system to share, trace and enable community enrichment of the modern art heritage of Iraq. The focus of the project is thousands of works of art, many of them now lost, from the Iraqi Museum of Modern Art in Baghdad. MAIA is unique in that it not only documents the lost artworks, but also provides tools for community enhancement of those works, allowing contribution of stories, knowledge and documentation to the system, as well as syndication of the content elsewhere on the web.  For the past eight months, participants in this project have been building a comprehensive virtual archive of the works in the Museum's various galleries, including a database of images and information about the objects (artist name, title, date, dimensions, subject matter, medium, condition, current location, related works, etc). These significant national treasures are displayed in an open format that invites participation from users worldwide, including the Iraqi national and expatriate communities, and users will be encouraged to help identify and understand individual pieces. The MAIA system, which integrates two extant content management systems, Open Context and Omeka, will provide a valuable research tool for scholars, students, as well as the general public, but most importantly for Iraqis: these works of art form an important expression of the Iraqi national experience.    Project History The Iraqi Museum of Modern Art, formerly the Saddam Center for the Arts (Markaz Saddam lil Funun), was established in 1986 as Iraq's museum of modern and contemporary art. During the invasion of Baghdad in April 2003, the structure was severely damaged by fire and looters. Without security and protection from the occupying powers after the collapse of the Baath regime, its collections of approximately 8,000 modern and contemporary Iraqi paintings, sculptures, drawings and photography, dating from late 19th century until April 2003, were entirely looted. Prof. Nada Shabout's research based on sources inside of Iraq indicated that while some works were smuggled outside of the country, most works were still on the market for sale in Baghdad. At an early stage after the invasion, about 1,300 works were found at the National Gallery's basement. They have since been are stored at a facility administered by the Ministry of Culture, without restoration, authentication or archiving.  While the fate of the collection is tragic enough, what exasperated the situation further is that the Museum's inventory and documentation disappeared with the works as well, meaning that missing works cannot be traced or repatriated. Nada Shabout has spent the last three years collecting and digitizing all available information about the lost works through meetings with artists, gallery owners, and art educators. In this time, she has found that the situation is dire, with improper documentation and accessioning procedures, scant publication and recording in catalogs, and a lack of inventory for the two decades before the invasion. In the end, the richest available information is in fact in the recollections of individual people; hence, the imperative to develop ways for people to share their knowledge of these works.    Approach The MAIA prototype integrates two existing, open source, content delivery systems, Omeka and Open Context. Omeka (http://omeka.org/) is an open source, collections-based publishing platform that allows individuals and organizations to share collections, structure content into exhibits and write essays. It offers customizable themes and a suite of easy-to-install add-ons for customizing site appearance and functionality. Omeka brings Web 2.0 technologies and approaches to academic and cultural websites to foster user interaction and participation, while making design easy with a simple and flexible templating system. Robust open-source developer and user communities underwrite Omeka's stability and sustainability.This section has been adapted from: http://omeka.org/about/  Open Context (http://www.opencontext.org) is a web-based, open access data publication system that supports enhanced sharing of museum collections and field research data by enabling researchers and cultural heritage collections managers to publish their primary field data, notes and media (images, maps, drawings, videos) on the web. It is free and uses open source software built on common open source technologies (Apache-Solr, MySQL, PHP, and Dojo Ajax) widely accessible and supported by a vast global developer community. Open Context uses Apache-Solr to power a \"faceted browse\" tool, which allows for much more informed navigation and understanding of collections than the \"type and hope\" approach of simple key-word searches. This component also delivers web-services, enabling a feed-based approach to syndicating content and integrating collections distributed across the web. These web-services represent a powerful, scalable, and elegantly simple way to facilitate aggregation across multiple collections.  MAIA's approach integrates features of these two systems to maximize the collection's reach, discoverability and creative reuse. Omeka offers a user-friendly platform for building, customizing and organizing the MAIA collection, as well as allowing options for contributions and comments from the community. However, search functionality is limited and Omeka content is largely confined within each Omeka instance, despite its support of OAI/PMH and some feed capabilities. Finally, while Omeka offers stable URLs for every item, Omeka users need to make additional arrangements for archiving their collections. Open Context complements Omeka's capabilities with a faceted browse \"plugin,\" offering powerful web-service capabilities that enable distributed search and syndication of content. These capabilities make Open Context a more powerful platform for supporting aggregation and mashups. Open Context's faceted browse tool provides a much more informed overview of a collection, showing fields associated with content even for custom metadata, allowing exploration beyond simple searches. The Open Context plug in will also greatly expand Omeka's feed capabilities, allowing users to draw custom feeds tailored to their specific interests (such as a particular artist, time period and/or region). Any Omeka site implementing the Open Context faceted browse plug-in will be indexed by Open Context, opening up Omeka content to dynamic searching across multiple collections. Finally, Omeka collections using the Open Context plug-in will benefit from accessioning by the California Digital Library. Thus, Omeka's user-friendly collections management and publishing functions are joined with Open Context's powerful web services to increase the reach and potential for reuse of MAIA content.  Taking advantage of the flexibility in content-management and sharing provided by the integration of these two powerful, open source systems, the MAIA platform is available for free on the web and offers the following additional features:  Localization: All static content in the system is translated into Arabic. Participatory tagging and commentary features are also available in multiple languages. Community input: A tagging system allows users to comment on any item in the database, thus enriching the content and helping build a memory of the lost works. Users can also link to external content related to the works. Citation: Easy citation retrieval makes MAIA a useful research tool. Unique citations are generated for every single item in the system, and Omeka expresses bibliographic metadata in a format that the popular Zotero citation management tool can recognize. Copyright Pragmatism: The creators of many of the works included in the website are unknown, and it is not possible to get permissions from them to disseminate their work online. However, such dissemination is essential if these creators are ever to be identified. Therefore, this project hosts works even in cases where the creator is not known. A clear \"take-down\" policy ensures that artists can request that their work be removed from the website. In cases where copyright permissions can be obtained, a Creative Commons Attribution, Noncommercial, Share-alike license http://creativecommons.org/licenses/by-nc-sa/3.0/ is used.      Outcomes By archiving and documenting the known modern artistic works from the Iraqi Modern Art Museum, this project contributes to the preservation of Iraq's cultural heritage. However, the project's greater success lies in the amount of exposure and the quantity and quality of community participation that the project garners. That is, free global exposure of the known content is helpful and informative, but active community input that enriches the works, and perhaps locates some of the lost works, is ideal. We are currently working on maximizing the dissemination of MAIA content, through the use of blogs, interviews, publications and presentations. Our long-term vision for MAIA is that it will become a virtual museum, where visitors will navigate through a map-based interface, exploring galleries and viewing individual works of art, ideally in the place they stood before the museum was damaged and many of the works lost. In this way, the public will have a visual understanding of the number of works still missing or for which no documentation exists. The emotional impact of seeing blank sections of gallery walls is far greater than reading a number or percentage, and will give the public a more profound understanding of the loss of these works of modern Iraqi heritage. On a more positive note, a visualization of the rich, related content around many of these works will enrich the visitor's experience and understanding of any single work.   ",
        "article_title": "The Modern Art Iraq Archive (MAIA): Web tools for Documenting, Sharing and Enriching Iraqi Artistic Expressions",
        "authors": [
            {
                "given": " Sarah Whitcher",
                "family": "Kansa",
                "affiliation": [
                    {
                        "original_name": "The Alexandria Archive Institute USA",
                        "normalized_name": "Alexandria Archive Institute",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05691ev83",
                            "GRID": "grid.446377.5"
                        }
                    }
                ]
            },
            {
                "given": " Nada",
                "family": "Shabout",
                "affiliation": [
                    {
                        "original_name": "The University of North Texas USA",
                        "normalized_name": "University of North Texas",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00v97ad02",
                            "GRID": "grid.266869.5"
                        }
                    }
                ]
            },
            {
                "given": " Saleem",
                "family": "Al-Bahloly",
                "affiliation": [
                    {
                        "original_name": "University of California, Berkeley USA",
                        "normalized_name": "California Coast University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05t99sp05",
                            "GRID": "grid.468726.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-20",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In the literary world, authorship of great novels is like writing a great piece of music; while there may never be a perfect way to determine if someone wrote a particular work or not, equations and algorithms have been developed in information theory and statistics to help those trying to discover the true authorship of contested written works. Because no method is perfect, using a set of methods on the same works can be used to give high probabilities of authorship. The JGAAP system houses a collection of methods such as Histogram Distance and Manhattan Distance and event sets such as word bigrams and character trigrams to allow users to perform multiple tests on contested works to see if the supposed author is actually the author by comparing samples from both the contested author and other possible authors.  We apply this framework and show a notable discontinuity in the authorial style of the novel El Ingenioso Hidalgo don Quijote de la Mancha, better known as Don Quijote (or Don Quixote).  Background While there have been skeptics and scholars alike that have doubted Miguel de Cervantes Saavadre's true authorship of the entirety of Don Quijote, no one had tested whether or not Cervantes was in fact the true author of the whole of Don Quijote. The purpose of using the JGAAP system was to either give merit to or disprove this theory. By comparing to other authors who wrote works at about the same time Don Quijote was written, the JGAAP system would test to see if the text that Cervantes supposedly wrote was closer to the first volume of Don Quijote or closer to other authors of the same time period. If a definitive break could be established between where the program attributed Cervantes as the author and where it did not, that would suggest either a major style shift or the presence of another author different from Cervantes, while no break at all would suggest that Cervantes was in fact the true author of the second volume of Don Quijote, assuming that he was also the author of the first volume.    Methods and Materials For this authorship attribution, the program JGAAP 4.0 was downloaded from http://www.jgaap.com, developed by Patrick Juola at Duquesne University. The Don Quijote text used was acquired from Project Gutenburg at http://www.projectgutenburg.org. The full text of Don Quijote was then stripped of the introductions and separated into chapters by volume. The first volume was then set as the basis for Miguel de Cervantes' original authorship. Every third chapter, starting with chapter three, was used as the base case for Cervantes' work. Two other authors used for comparison, Fransisco de Quevedo and Mateo Alemán, were also used. Quevedo's, Historia de la vida del Buscón, llamado Don Pablos, ejemplo de vagamundos y espejo de tacaños and Alemán's Guzmán del Alfarache were also taken from Project Gutenburg and broken into roughly the same number of chapter-type sections as the number of chapters used for Cervantes' Don Quijote. In order to make sure that Cervantes' was actually the author of volume one of Don Quijote, every chapter not used in the base case was compared to the base chapters, Quevedo's work, and Alemán's work. Each test used JGAAP's Normalize Whitespace, Strip Punctuation, and Unify Case canonicizers on all of the documents. Five event sets - Word, WordBiGram, WordTriGram, WordTetraGram, and Word Length - were all paired with nine analysis methods - Camberra Distance, Cosine Distance, RN Cross Entropy, Histogram Distance, Kullback Leibler Divergence, Levenshtein Distance, Manhattan Distance, KS Distance, and Naive Bayes Classifier, for a total of 45 unique event set-analysis methods. Once the first volume of Cervantes' work was confirmed to be uniformly Cervantes', volume two of Don Quijote was tested in the same manner as the first volume in order to provide an accurate analysis.  We apply a mixture-of-experts approach to the evaluation of authorship. Each different method is treated as a single \"expert\" in different aspects of authorial style, and permitted to vote on who (among the candidates) is the author of any specific fragment. If all 45 test \"experts\" vote on Cervantes, we consider this to be strong evidence supporting his authorship, while if only 5 or so of the 45 consider Cervantes to be the most likely author, we consider this to be evidence against.    Results As a result of the analysis on the second volume of Don Quijote, the JGAAP program indicated that starting at chapter 6 Cervantes was not the author. Out of the 45 tests run on each chapter, the chapters in the first volume had a mean of 37.54 occurrences of Cervantes as the author with a standard deviation of .852. The first five chapters of the second volume had a mean of 36.50 occurrences of Cervantes as the author with a standard deviation of 1.517. Chapters 6-74 of the second volume, however, had a mean of 4.90 occurrences of Cervantes as the author with a standard deviation of 1.436. This radical shift in authorship means either Cervantes completely shifted his writing technique or he did not write the latter 69 chapters of the second volume of Don Quijote.    Discussion While there are people who are skeptic about the authorship of Don Quijote, nothing up until now has given those claims any grounds other than speculations based on inconsistencies in the text. Although this analysis does not guarantee that Cervantes did not write the last 69 chapters of the second volume, it does make the probability of that claim much greater. This, in part, is due to the fact that none of the tests in JGAAP has been tested enough to show that it will work for all documents. As further analysis of the methods continues, the results of the tests used in this authorship attribution will most likely validate these results. As tests and methods prove to not work, the analysis will be redone with these tests omitted from the analysis giving a more accurate result.   ",
        "article_title": "Authorship Discontinuities of El Ingenioso Hidalgo don Quijote de la Mancha as detected by Mixture-of-Experts",
        "authors": [
            {
                "given": " Christopher",
                "family": "Coufal",
                "affiliation": [
                    {
                        "original_name": "Duquesne University USA",
                        "normalized_name": "Duquesne University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02336z538",
                            "GRID": "grid.255272.5"
                        }
                    }
                ]
            },
            {
                "given": " Patrick",
                "family": "Juola",
                "affiliation": [
                    {
                        "original_name": "Duquesne University USA",
                        "normalized_name": "Duquesne University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02336z538",
                            "GRID": "grid.255272.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-29",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The Library of Congress has expanded its digital spectral imaging research of humanities artefacts that reflect the history of the United States, with the development of advanced imaging techniques that provide data for the studies of manuscripts that span the centuries: Portolan Charts – from 1320-1633, Jefferson’s handwritten draft of the Declaration of Independence from 1776, and Herblock’s political cartoons from 1929-2001. Using standardized digital imaging techniques, the Library of Congress Preservation Directorate is providing preservation scientists, conservators and humanities scholars with access to digital information on historic and fragile documents with conservation-safe, non-destructive technologies. This provides data for greater understanding of the original object, including revealing creation techniques, and identifying the origin of the substrate (paper, parchment) and media. The Library plans to host this data in standardized format for access as part of a broader preservation database of scientific reference materials of naturally-aged substrate, media (inks, colorants and pigments), treatment effects, environmental data and other document production and creation information. These recent advances in technology and digital access have paved the way for the improved utilization and interpretation of scientific analyses to contribute to scholarly interpretations of heritage materials.  Integration of Imaging and Data Management for Discovery The Library of Congress has implemented digital spectral imaging of the following objects to collect preservation data, scholarly information and a cross-section of data on cultural heritage old and new:  Portolan Navigational Charts:Portolan charts are early nautical navigational maps based on realistic descriptions of harbours and coasts. These were first made in the 1300s in Italy, Portugal and Spain (portolan comes from Italian, portolano, meaning \"related to ports or harbours\"). The charts cover the period from 1320 to 1633, and were created on vellum or parchment. Imaging is being used to non-destructively characterize a range of pigments, details of compass points, creation techniques and tools, and potential palimpsest information.  The handwritten draft of the Declaration of Independence:The draft of the Declaration of Independence was handwritten in 1776 by Thomas Jefferson in iron gall ink on laid paper, with corrections and changes by Benjamin Franklin and John Adams. Hyperspectral imaging revealed layers of changes and different inks, and offered new insights into the original Jefferson text that was crossed out and overwritten.  Herblock Political Cartoons:Created by Herbert Block from 1929 to 2001, these cartoons represented the penmanship of a man who influenced public opinion in America throughout his 72-year career. A selection of the large original drawings were spectrally imaged to assess the condition of light sensitive inks, also revealing details of the drawings not previously discovered.   The application of digital hyperspectral imaging and associated non-destructive technical analyses to key cultural objects at the Library of Congress required the integration of complementary data to address a wide range of preservation and scholarly challenges. The advanced imaging data is incorporated into ongoing humanities studies of objects, generating digitally linked data sets in standardized format for Internet access. The non-destructive imaging capabilities allow researchers to characterize pigments and media on the artefact, retrieve hidden and lost text, and illuminate production methods of a range of cultural objects. Characterization of a range of materials has been enhanced through the development of a standardized spectral reference sample set, virtually eliminating the need for any sampling. Integration of the data from these technological advances with information from other preservation studies, allows greater scholarly access to the information available from fragile historic documents on parchment and paper.  Application of non-destructive imaging techniques allows the equivalent of optical archaeology of these manuscripts and documents. The profound advantage of this technique is to provide a wealth of information and data – while minimizing and preventing further deterioration of fragile heritage items through handling and invasive analytical techniques. Integrating the range and volume of data collected from any one of these objects in a cohesive data set requires the development of a spatial \"map\" of the object with layers of information that relate to specific points and details on the document. This range of data can include materials characterization of pigments, colorants and other components, scholarly interpretations of text, organic and inorganic compound information, topographical layering of additions to the document, and evidence about equipment and tools used in the creation of the document. All this information adds to the interpretation of cultural objects and advances the role of non-destructive heritage science techniques in humanities research.    Imaging  The Preservation Research and Testing Division at the Library has developed its MegaVision-Equipoise Spectral Imaging System for preservation research into a range of United States’ Top Treasures and international objects of cultural import from across the vast Library collection of nearly 145 million items, including: More than 32 million cataloged books and other print materials in 470 languages; over 62 million manuscripts; the largest rare book collection in North America; and the world's largest collection of legal materials, films, maps, sheet music and sound recordings. Collecting accurate standardized imaging data with this or any other imaging system requires integration and management of three critical factors: 1) the imaging system, 2) standardized metadata capture, and 3) efficient work processes. The Megavision-Equipoise imaging system has been customized for cultural studies, including a focus on conservation safe lighting that minimizes light on the document while generating a high resolution image. The system collects a \"cube\" of standard registered images from the ultraviolet through the visible spectrum to the infrared (approximately 365nm – 1050nm) with a MegaVision 39 Megapixel monochrome camera and Equipoise LED EurekaLights. Integrated side-light panels add to the previous lighting system. The inclusion of a hyperspectral scientific reference sample collection has greatly aided the characterization of a range of materials. Standardized metadata with imaging and illumination information, as well as information about the document and its content is captured with the PhotoShoot™ software, which embeds metadata in the header of the image files, based upon EXIF and IPTC standards. The quality of the spectral imaging data relies on the adaptation of proven work processes to the requirements of each object. New imaging applications can then focus on acquiring scholarly information.As new imaging applications are integrated into the process, rigorous attention to imaging details allows an efficient capture of data with careful document handling, standardized imaging procedures (including lighting, relative humidity and temperature control) and data management. These factors are supported with environmental management, security, contingency planning and IT infrastructure. The process flow continues beyond the actual imaging, to include image analysis, potential post processing and validation of the data – an iterative loop requiring input from a range of personnel and expertise – requiring additional time, resources, and management.    Data Management For the collection, coordination, access and presentation of this data, the Preservation Directorate is developing a comprehensive approach for access to digital files in a universally accessible format. This format will utilize an RFD framework for international collaboration and ease of access, a critical component being standardization of file formats from a range of instrumentation. The approach is to integrate scientific and intellectual scholarly information, including the interpretive data required for humanities researchers to utilize the information effectively. For example, this interpretive data could involve identification of a pigment that was not discovered until after the time period attributed to the document, or lost text that changed the interpretation of the document, and provided greater insight into the thought process of the creators. The range of data is being structured in a comprehensive format with the utilization of \"scriptospatial\" digital objects – essentially a geospatial information system (GIS) for documents. This data organization includes addressing the challenge of presented and linking data across a collection of items to show the scientific visualization and representation of changes across geographical locations, chronological time periods, changing use of materials, and the development of new production techniques.    Conclusion These hyperspectral imaging studies from the 1400s to the twenty-first century are revealing a range of scholarly and preservation information about seminal objects that represent specific aspects of their era. Linking non-destructive scientific imaging and digital technologies with humanities research augments the preservation of these often fragile cultural artefacts, while improving and increasing access for researchers, with extensive intellectual implications. This has created a powerful tool for probing the past; revealing levels of data and raising and answering further questions about the documents, questions that could not previously be contemplated due to missing and incomplete information. The combination of scientific and humanities research allows an enhanced dialogue between researchers, harnessing the strengths of researchers and scholars in each field, and creating a more effective interpretation of data. Often the focus and import of this exchange of information and newly acquired data is on the movement of information from scientific analyses to the humanities. However it should be noted that the flow of data is not one-way. Humanities researchers and scholars provide knowledge of past eras and culturally related information that can prove of great benefit in the interpretation of scientific analyses, such as the knowledge of local and or cultural practices, and treatises on commonly used materials and practices. The comprehensive presentation of this data in a form that allows these two complementary streams of research to be linked and integrated greatly enhances this dialogue. This ongoing and iterative dialogue is a critical component for advancing and preserving cultural knowledge throughout the centuries – not only from the past, but also through preservation research into modern fugitive materials and media. This collaborative research can only be accomplished with the ability to capture standardized images, data and metadata from scientists and scholars and integrate them into a common data set, advancing the integration of heritage science and humanities research. The combination of technological advances and structured data access enhances accessibility to original scientific data files and images. Interpretation of this data is enhanced by ease of access to integrated data files. For humanities research, this provides access to linked data files, increasing the intellectual capacity to harness and share knowledge internationally through digital and technology advances.  ",
        "article_title": "Challenges of Linking Digital Heritage Scientific Data with Scholarly Research: From Navigation to Politics",
        "authors": [
            {
                "given": " Fenella G. ",
                "family": "France",
                "affiliation": [
                    {
                        "original_name": "FAIC, Library of Congress, USA",
                        "normalized_name": "Library of Congress",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04p405e02",
                            "GRID": "grid.241525.5"
                        }
                    }
                ]
            },
            {
                "given": " Michael B. ",
                "family": "Toth",
                "affiliation": [
                    {
                        "original_name": "R. B. Toth Associates, USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Eric F.",
                "family": "Hansen",
                "affiliation": [
                    {
                        "original_name": "FAIC, Library of Congress, USA",
                        "normalized_name": "Library of Congress",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04p405e02",
                            "GRID": "grid.241525.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-30",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Texts encoded using the Text Encoding Initiative Guidelines (TEI) are ideally suited to close examination using a variety of digital methodologies and tools. However, because the TEI is a broad set of guidelines rather than a single schema, and because encoding practices and standards vary widely between collections, programmatic interchange among projects can be difficult. Software that operates effectively across TEI collections requires a mechanism for normalizing data - a mechanism that, ideally, preserves the essence of the source encoding. Brown University’s Center for Digital Scholarship will address this issue as one part of a project, funded by a National Endowment for the Humanities Digital Humanities Start-Up Grant, to develop tools for analyzing TEI-encoded texts using the SEASR environment (National Center for Supercomputing Applications). SEASR is a scholarly software framework that allows developers to create small software modules that perform discrete, often quite simple, analytical processing of data. These modules can be chained together and rearranged to create complex and nuanced analyses. Both the individual modules (or components, in SEASR’s parlance) and the chains (or flows) running within a SEASR server environment can be made available as web services, providing for a seamless link between text collections and the many visualizations, analysis tools, and APIs available on the web. For literary scholars using TEI, this approach offers innumerable possibilities for harnessing the semantic richness encoded in the digital text, provided that there is a technological mechanism for negotiating the variety of encoding standards and practices typical of TEI–based projects. The central thrust of Brown’s effort is to develop a set of SEASR components that exploit the semantic detail available in TEI-encoded texts. These components will allow users to submit texts to a SEASR service and get back a result derived from the analytical flow that they have constructed. Results could include a data set describing morphosyntactic features of a text, a visualization of personal relationships or geographic references, a simple breakdown of textual features as they change over time within a specific genre, etc. SEASR flows can also be used to transform parts of TEI documents into more generic formats in order to use data from digital texts with web APIs, such as Google Maps. Because these components deal with semantic concepts rather than raw, predictable data, they require a mechanism to map concepts to their representations in the encoded texts. Furthermore, in order for these modules to be applicable to a variety of research collections, they must be able to adapt to different manifestations of semantically similar information. Users need the freedom to tell the software about the ways in which data with semantic meaning, such as relationships or sentiment, are encoded in their texts. To do this, we will build a semantic map – a document that describes a number of relationships between 1) software (in this case, SEASR components), 2) ontologies, and 3) encoded texts (see Figure 1).    Figure 1   The semantic map is modeled on the approaches of previous projects working with semantically rich TEI collections, most notably the work of the Henry III Fine Rolls Project, based at King’s College London. The creators of that project developed a method for using RDF/OWL to model the complex interpersonal and spatial relationships represented in their TEI documents (Vieira and Ciula). They use semantic-web ontologies, such as CIDOC-CRM and SKOS, to define relationships between TEI nodes. Vieira and Ciula offer examples in which TEI nodes that describe individuals are related to other nodes that describe professions, using the CIDOC-CRM ontology to codify the relationship. (Vieira and Ciula, 5) This approach serves well as a model for mapping some of the more nebulous concepts of interest to the SEASR components to the variable encoding of those concepts in TEI. The links between the encoded texts and the SEASR components are forged using two techniques that exploit the Linked Data concepts around which SEASR - and many other emerging software tools - are designed (Berners-Lee). The first is a RDF/OWL dictionary that defines the ontology of the semantic concepts at work in the TEI component suite. RDF/OWL allows us to make assertions about the concepts associated with any resource identified by a URI (Uniform Resource Identifier). Within SEASR, any component or flow is addressable via a URI, making it the potential subject of a RDF/OWL expression. For example, a component that extracts personal name references from a text can be defined in an RDF/OWL expression as having an association with the concept of a personal name, as defined by any number of ontologies. The second part of the semantic map is an editable configuration document that uses the XPointer syntax to identify the fragments of a particular TEI collection that correspond to the semantic definitions expressed in the RDF/OWL dictionary. In this example, collections that encode names in several different ways can specify, via XPointer, the expected locations of relevant data. When the SEASR server begins an analysis, data is retrieved from the TEI collection using the parameters defined in the semantic map, and is then passed along to other components for further analysis and output. In addition to the semantic map and the set of related analytical components, the planned TEI suite for SEASR includes components to ease the retrieval and validation of locally defined data as they are pulled into analytical flows. One such component examines which of the TEI-specific components in a flow have definitions in the RDF/OWL dictionary. The result is passed to another component, which uses Schematron to verify that the locations and relationships expressed in the semantic map are indeed present in the data being received for analysis. A successful response signals SEASR to proceed with the analysis, while an unsuccessful one returns information to the user about which parts of the text failed to validate. Our approach is different from that of other projects, such as the MONK Project, which have also wrestled with the inevitable variability in encoded collections (MONK Project). For MONK, this issue was especially prominent as the goal of the project was to build tools that could combine data from diverse collections and analyze them as if they were a uniform corpus. This meant handling not only TEI of various flavors, but other types of XML and SGML documents as well. To solve this problem, MONK investigators developed TEI Analytics, a generalized subset of TEI P5 features designed \"to exploit common denominators in these texts while at the same time adding new markup for data structures useful in common analytical tasks\" (Pytlik-Zillig, 2009). TEI-A enables developers to combine different text collections to allow large-scale analysis by systems such as MONK. As a solution to handling centralized, large-scale data analysis, TEI-A is an invaluable achievement in light of the maturation of mass-digitization efforts such as Google Books and HathiTrust. However, our goal in creating SEASR components is fundamentally different than that of MONK, and thus warrants a different approach. Our SEASR tools are designed to be shared among institutions but to be used differently by each, as a part of a web interface for a particular collection. Furthermore, the granularity of the concepts of interest to the TEI components for SEASR makes it infeasible that we could easily map such encoding to a common format, such as TEI-A. Hence, the semantic map – an index that acts as a small-scale interpreter between local collections and the abstract semantic notions marked-up within them. In developing TEI tools for SEASR, we address several issues of immediate interest to scholars and tools-developers in the Digital Humanities. It is certainly useful to create text analysis tools for this new software environment. But of broader interest to scholars and users of digital text collections are the semantic mechanisms that permit interplay between community-based tools and their own collections. Several issues require close scrutiny as this model develops: with careful forethought, we need to ensure that our tools are viable outside of the SEASR framework; we need to consider whether the XPointer syntax has a future in the evolving Semantic Web ecology, and likewise consider how our curated scholarly collections can interact more seamlessly with that environment. Ultimately, the tools that we develop will be available in a public repository for institutions experimenting with SEASR. Funding: This work was supported by the National Endowment for the Humanities Digital Humanities Start Up Grants program.   ",
        "article_title": "Semantic Cartography: Using RDF/OWL to Build Adaptable Tools for Text Exploration",
        "authors": [
            {
                "given": " Andrew ",
                "family": "Ashton",
                "affiliation": [
                    {
                        "original_name": "Center for Digital Scholarship, Brown University, USA",
                        "normalized_name": "Brown University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05gq02987",
                            "GRID": "grid.40263.33"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-30",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Our paper will discuss conceptual networks present in Victorian poet Algernon Charles Swinburne's mid-career collection Songs of the Springtides (1880) and how those networks may be represented in TEI P5 XML markup and graphic visualizations driven by the encoded text. Swinburne’s work is full of familiar signposts and nodes, such as his trademark binary oppositions and pairings: pain/pleasure, life/death, love/hate, hope/fear, sleep/death. An incredibly learned poet with an extensive range of form and allusion, Swinburne’s poems are packed with often obscure references to the Bible, classical mythology, and Arthurian legend. He wrote a number of political poems addressing contemporary events. He wrote parodies of other contemporary poets, including Tennyson, Browning, and Rossetti. And as Jerome McGann has noted, “No English poet has composed more elegies than Swinburne” (McGann 293). These binary oppositions; the many biblical, mythical and legendary references; the historical and contemporary figures who are eulogized in the elegies and praised in the many tributes and dedications; the pervasive symbols of song and the sea: these elements of Swinburne’s verse all serve as familiar, easily identifiable nodes of information, laden with meaning acquired through strategic repetition and structural integration into the intellectual networks of Swinburne’s work. We will examine these nodes, structures and architectonic forms in one of Swinburne’s most artfully crafted and carefully designed collections, Songs of the Springtides. The mid-career Songs of the Springtides is a particularly interesting volume in the context of inter- and intra-textual networks. For Songs of the Springtides, Swinburne originally planned “a little volume containing three poems upwards of 500 lines each in length, all of them in a sense sea-studies” (Swinburne Uncollected Letters 2:181). The three poems are: “Thalassius,” “On the Cliffs,” and “The Garden of Cymodoce.” To this “triad of sea-studies” Swinburne added the “Birthday Ode” to Victor Hugo. Unannounced but also present in the volume are three short poems: the fifteen-line “Dedication” to Edward John Trelawny, Swinburne’s “old sea king” and a friend of Shelley’s (Swinburne Uncollected Letters 2:181); an untitled sonnet, with the first line “Between two seas the sea-bird’s wing makes halt;” and another sonnet, buried in the notes to the ode for Hugo, “On the proposed desecration of Westminster Abbey by the creation of a monument to the son of Napoleon III.” This small volume is an artful example of a deliberately fashioned and architected whole connected by complex discourse networks of key concepts that operate within, across, and beyond the individual poems. Familiarity with the poems of Songs of the Springtides reveals a few key concepts, figures, or images of particular import and penetration: Swinburne's pantheon of literary heroes; song and music; the natural world, especially the sea; the poet; the text. In many cases occurrences of these concepts may be identified algorithmically. However, one cannot rely on string pattern matching to find all words and phrases related to a particular concept. In the case of song, for instance, automated processes may be used to identify the many clear and obvious occurrences of this concept, phrases including words such as song, songs, sing, singer, music, etc. However, the poems also contain phrases such as the following: “lutes and lyres of milder and mightier strings,” which is obviously related to music, but less susceptible to automated identification. A combination of automated and manual markup then has been used to identify and encode words and phrases related to the concepts of interest in the texts. This notion of the text as a self-constituted network or as a part of a larger inter-textual network is found in influential writings of the major sages of poststructuralism and postmodernism. In S/Z, Roland Barthes writes about the text as an entrance into a network with a thousand entrances; to take this entrance is to aim, ultimately, not at a legal structure of norms and departures, a narrative or poetic Law, but at a perspective (of fragments, of voices from other texts, other codes), whose vanishing point is nonetheless ceaselessly pushed back, mysteriously opened; each (single) text is the very theory (and not the mere example) of this vanishing, of this difference which indefinitely returns, insubmissive. (12)  Michel Foucault, in The Archaeology of Knowledge writes, The frontiers of a book are never clear-cut: beyond the title, the first lines, and the last full stop, beyond its internal configuration and its autonomous form, it is caught up in a system of references to other books, other texts, other sentences: it is a node within a network. And this network of references is not the same in the case of a mathematical treatise, a textual commentary, a historical account, and an episode in a novel cycle; the unity of the book, even in the sense of a group of relations, cannot be regarded as identical in each case. The book is not simply the object that one holds in one's hands; and it cannot remain within the little parallelepiped that contains it: its unity is variable and relative. As soon as one questions that unity, it lows its self-evidence; it indicates itself, constructs itself, only on the basis of a complex field of discourse. (23)  More recently, Friedrich Kittler in Discourse Networks 1800/1900, building on and synthesizing the work of Barthes, Foucault, Derrida, and others, writes about literature as an information system supported and shaped by the available technologies of discourse: An elementary datum is the fact that literature (whatever else it may mean to readers) processes, stores, and transmits data, and that such operations in the age-old medium of the alphabet have the same technical positivity as they do in computers. (370)  These theories of the text as constituting and constituted by information networks have obvious relevance and resonance for digital humanities scholarship, much of which is engaged in explicitly identifying, encoding, and otherwise representing the information structures in texts of all kinds. By encoding the individual information nodes, one can generate new interfaces and mechanisms for reading and navigating the text and for visualizing the patterns and interactions of the information networks operating throughout Swinburne's volume. The authors have been working on a specific web-based visualization to represent graphically the conceptual networks at play across a series of literary texts, in this case Swinburne's Songs of the Springtides, and to allow users to view and browse these networks from a distance and to zoom in and focus on local clusters and concentrations of the conceptual nodes. Our presentation will include a detailed discussion of Songs of the Springtides as a carefully designed information system, supported by a framework of internal and external discourse networks. Following this more theoretical discussion of Swinburne's volume, we will review and illustrate the TEI P5 mechanisms used to encode the networks and demonstrate the web-based visualization of the text.  ",
        "article_title": "“Quivering Web of Living Thought”: Mapping the Conceptual Networks of Swinburne's Songs of the Springtides",
        "authors": [
            {
                "given": " John A.",
                "family": "Walsh",
                "affiliation": [
                    {
                        "original_name": "Indiana University USA",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            },
            {
                "given": " Pin Sym",
                "family": "Foong",
                "affiliation": [
                    {
                        "original_name": "Indiana University USA",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            },
            {
                "given": " Kshitiz",
                "family": "Anand",
                "affiliation": [
                    {
                        "original_name": "Indiana University USA",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            },
            {
                "given": " Vignesh",
                "family": "Ramesh",
                "affiliation": [
                    {
                        "original_name": "Indiana University USA",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-28",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  In the past quarter century, established methods of literary history have been severely contested. On the one hand, syncretic, single-author histories have become problematic as a result of a combination of the expanded literary canon and a range of theoretical challenges. On the other, a demand for historicized overviews that reflect the radical recent reshaping in all fields of literary study has produced large numbers of both collectively written histories and encyclopedias or companions. Literary history thus tends towards compilations in which specialists treat their particular fields, at the cost of integration or of coherence. Meanwhile, the primary materials are increasingly available in digital form, and literary historical scholarship itself is increasingly produced digitally, whether as versions of established forms such as journal articles, or in resources that invoke the potential for new kinds of analysis. Major digital initiatives over the past decades have focused almost exclusively on digital resource creation: the increasingly pressing question is how to use this expanding body of materials to its fullest potential. In this project, we investigate how literary historical analysis can be extended using various forms of visualization, using the experimental Orlando Project as our test bed. Orlando: Women's Writing in the British Isles from the Beginnings to the Present is recognized as the most extensive and detailed resource in its field and as a model for innovative scholarly resources. Composed of 1,200 critical biographies plus contextual and bibliographical materials, it is extensively encoded using an interpretive Extensible Markup Language (XML) tagset with more than 250 tags for everything from cultural influences, to relations with publishers, or use of genre or dialect. The content and the markup together provide a unique representation of a complex set of interrelations of people, texts, and contexts. These interrelations and their development through time are at the heart of literary inquiry, and having those relations embedded in the markup, and hence processable by computer, offers the opportunity to develop new forms of inquiry into, and representations of, literary history. Such new opportunities of scale are often invoked using Greg Crane’s seminal question, “What can you do with a million books?” (2006). We need to be able to ask big, complex questions while remaining grounded in particularities, and we need new ways of representing answers to those questions. This requires new tools for scholarly research that can access, investigate, and present new aspects of the human story and history. In this context, we contend that the scholarly interface requires not only experimentation but also careful assessment to see what works to make digital materials of real value to humanities scholars. As argued by Ramsay (2003), Unsworth (2006), and others, using computers to do literary research can contribute to hermeneutic or interpretive inquiry. Digital humanities research has inherited from computational science a leaning towards systematic knowledge representation. This has proved serviceable in some humanities activities, such as editing, but digital methods have far more to offer the humanities than this. As Drucker and Nowviskie have argued, “The computational processes that serve speculative inquiry must be dynamic and constitutive in their operation, not merely procedural and mechanistic” (431). The Orlando encoding system, devised for digital rather than print textuality, facilitates collaboratively-authored research structured according to consistent principles. The encoding creates a degree of cross-referencing and textual inter-relation impossible with print scholarship—not simply hyperlinking but relating separate sections of scholarly text in ways unforeseen even by the authors of the sections. It represents a new approach to the integration of scholarly discourse, one which allows the integrating components to operate in conjunction with, rather than in opposition to, historical specificity and detail (Brown et al. 2006c). However, the search-and-retrieval model of the current interface for Orlando, while user-friendly in that it resembles first-generation online research tools, cannot exploit this encoding to the fullest. Search interfaces only find what the user asks for, whereas visualization enables exploration and discovery of patterns and relationships that one might not be able to search for. For instance, the current interface permits users to search for authors by the number of children they had, and thus to explore the relationship between literary production and reproduction. The quantity of material in Orlando makes it difficult to see overall patterns amongst the results. Recent experiments with the Mandala browser have demonstrated that visualization permits one to see both interesting anomalies (e.g. in lives which have demanded the use both of the childlessness tag and the children tag), and larger patterns, such as the non-correlation between high literary productivity and childlessness or small family size (Brown et al. 2008). These preliminary investigations confirm Moretti’s argument (2005) that visual representations enable kinds of literary historical inquiry that are not supported by conventional search interfaces. Orlando has the added advantage of making it possible to dive back into the source material to see the specifics from which the representation is produced.  In addition to the Mandala experiments, we have also been working on a set of designs for visually summarizing relationships in a manner that allows interactive exploration (e.g. Fig. 1). Building on the large body of previous literature in network visualization (e.g. Barabási 2002; Watts 2003; Christakis and Fowler 2009), we are experimenting with new visual representations for networks of people.   Fig. 1: One of several concepts for summarizing relationships among authors in Orlando. Here the authors on the path of connection are shown as coloured circles, where size is frequency and a unique colour is assigned to each author.    Interviews and observations of users at a recent hackfest provided some excellent insights into the sorts of interfaces that are likely to appeal to users wanting to explore embedded relationships in a body of texts in an open-tended way. While point-to-point visualization was considered to have some value, more excitement was generated by open-ended interface sketches, such as a visualization that resembles a cityscape, even when these were much less representational than conventional interfaces for the humanities. At the same time, Brown and Bauer have been working on a visualization tool that illustrates the challenges facing the project of representing all of Orlando’s semantically interrelated data through a graphical representation based on nodes and edges. It highlights the difficulty of providing prospect when dealing with a large and complexly structured data set, since the full set of relationships even of a moderate subset of the 1200 writers becomes unreadable, with over 16 million edges in the graph. We have done some work to explore algorithms and interfaces to accommodate these large data spaces and multitudes of relationships and tags. The challenge is to provide the researcher with a means of perceiving or specifying subsets of data, extracting the relevant information, building the nodes and edges, and then providing means to navigate the vast number of nodes and edges, especially given the limited amount of space on a computer monitor. The figures below illustrate some of the aspects of the tool.   Fig. 2: several writers and their interconnections    Fig. 3: zoomed view of relationships    The nodes (at the centre of the starbursts) represent writers, while the blue dots show other individuals (Fig. 2). The edges are shown as differently colored lines indicating different kinds of relationships as determined by tags (identified in the colored boxes). A researcher can display names, hide certain edges by deselecting tags, and zoom in and move around a large graph of nodes and edges (Fig. 3). The tool is a starting point for evaluating existing computational approaches and graphical displays of relationships as a means of exploring literary questions. It raises exciting questions regarding the integration of data mining approaches with a graphical interface, particularly for scholars suspicious of abstractions. Computationally, the question of how to make such a tool accessible to remote users is a challenge. This paper will compare the various approaches to visualizing links that we have employed to date on this data, and reflect on them in relation to both the literature on visualization approaches and our user feedback as a means of advancing our thinking on the challenge of creating interfaces for exploring large numbers of interlinkages within or between humanities resources.  ",
        "article_title": "How Do You Visualize a Million Links?",
        "authors": [
            {
                "given": " Susan",
                "family": "Brown",
                "affiliation": [
                    {
                        "original_name": "University of Alberta and University of Guelph, English Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": " Jeffery",
                "family": "Antoniuk",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Orlando Project Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": " Michael",
                "family": "Bauer",
                "affiliation": [
                    {
                        "original_name": "University of Western Ontario, Computer Science Canada",
                        "normalized_name": "Western University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02grkyz14",
                            "GRID": "grid.39381.30"
                        }
                    }
                ]
            },
            {
                "given": " Jennifer",
                "family": "Berberich",
                "affiliation": [
                    {
                        "original_name": "University of Western Ontario, Computer Science Canada",
                        "normalized_name": "Western University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02grkyz14",
                            "GRID": "grid.39381.30"
                        }
                    }
                ]
            },
            {
                "given": " Milena",
                "family": "Radzikowska",
                "affiliation": [
                    {
                        "original_name": "Mount Royal College Mount Royal University, Communications Canada",
                        "normalized_name": "Mount Royal University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04evsam41",
                            "GRID": "grid.411852.b"
                        }
                    }
                ]
            },
            {
                "given": " Stan",
                "family": "Ruecker",
                "affiliation": [
                    {
                        "original_name": "University of Alberta, Humanities Computing Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": " Terence",
                "family": "Yung",
                "affiliation": [
                    {
                        "original_name": "Mount Royal College Mount Royal University, Communications Canada",
                        "normalized_name": "Mount Royal University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04evsam41",
                            "GRID": "grid.411852.b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-26",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Narrative, as it evolves with technological developments, constantly reinvents itself in order to better capture new social orders and individual experiences. As an emerging cultural expression, however, most computer-generated narrative works are still restricted to an action-based, goal-driven aesthetics, leaving little space for characters’ inner world. This paper proposes to expand the range of computer-generated narrative by addressing this imbalance between the “physical” and the “internal.” It presents our approach for algorithmically narrating characters’ inner world by leveraging the synergy between modernist stream of consciousness literature and contemporary research in artificial intelligence and cognitive science. The Riu system, a text-based computational narrative system inspired by Woolf’s novel Mrs. Dalloway, is provided as a case study towards this new direction.  Computer-Generated Narrative Contemporary forms of narrative have evolved rapidly as digital technologies continue to be integrated in modern society. New conventions at the levels of both content and discourse have been established to reflect the constantly changing relationship between human and technology. For instance, popular science fictions of the 1980s (e.g., The Terminator) embodied the prevailing cyborg discourse and confusions of human identity within the Cold War context (Edwards, 1996). Similarly, hypertext fictions in the 1990s instantiated the postmodernist mentality of its time by turning everything – writer, reader, and society – into fragments (Johnson-Eilola, 1997). In this regard, the emerging form of computer-generated narrative,Although this paper focuses on text-based narratives generated by computer algorithms, the core of the discussion can be extended to other computational narrative forms, such as video games. that is, stories produced by computer algorithms, may offer an important cultural expression to portray our increasingly technology-dependent modern life. Compared to other forms of electronic literature (e.g., hypertext fictions), the strict technological requirements for computer-generated narrative have confined its development largely to the computer science community, particularly artificial intelligence (AI). Over the past decades, serious attempts have been made to integrate narratology theory into existing AI framework for story generation (Bringsjord & Ferrucci, 2000; Cavazza & Pizzi, 2006; Mateas, 2002; Meehan, 1976). Despite the considerable progress the community has made, the expressive power of computer-generated narrative is still limited compared to its non-digital antecedents. In particular, this paper is concerned with the prominent goal-driven, problem-solving aesthetics that dominate many story generation systems. A salient example is the Tale-Spin system (Meehan, 1976), which generates stories in the spirit of: Joe Bear was hungry; Joe couldn’t reach his food because of certain obstacles; Joe resolved the issues; Joe got his food.  It is true that recent narrative systems have evolved in numerous aspects since then. Nevertheless, this ultra-rational, \"behaviorist\" narrative style, afforded by Meehan’s now-widely-adopted planning-based framework, has remained and been taken for granted by many practitioners. As we become more aware of digital media’s capability of constructing subjective mental imagery and evoking users’ imagination and awareness (Harrell, 2009), it is crucial to revisit some of these early assumptions of computer-generated narrative and critically understand the expressive affordances as well as restrictions of the computational techniques that we use.  This paper proposes to expand the spectrum of computer-generated narratives by focusing on the richness of characters’ inner world, hidden behind the external world of actions. This approach aligns with modernist writers’ concerns of depicting \"hidden life at its source\" (Woolf, 1957 [1925]). Notice this is not a strong AI attempt to model human (semi-)consciousness. Instead, the goal is to explore new ways of conveying human subjectivity and life stories by algorithmically generating narratives that are reminiscent of similar phenomena. Informed by modernist literary techniques (particularly Virginia Woolf’s work), cognitive science discoveries and AI, this paper proposes a new approach for generating inner narratives and presents initial results from our on-going narrative project Riu.   Synergy of the Old and New As argued elsewhere (Zhu & Harrell, 2010 (forthcoming)), the overlooked synergy between stream of consciousness literature, artificial intelligence (AI), and cognitive science provides valuable insights to generating stories about characters’ inner world. In their respective historical contexts, both stream of consciousness literature and AI challenged the domination of behaviorism by turning internally to the human psyche. Rejecting the literary representation of characters as the \"external man\", modernist writers such as Virginia Woolf and James Joyce invented techniques to depict the moment-by-moment psychic existence and functioning of the \"internal man\" (Humphrey, 1954). Similarly, AI broke away from the behaviorism-dominated scientific community in the 1950s and legitimated human mental constructs, such as knowledge and reasoning, as crucial subjects of scientific inquiries. The differences between AI and modernist literature further dissolve when we take account of recent cognitive science theory, a sister field of AI. Stream of consciousness literature’s key concern with pre-speech level of consciousness, minimally mediated by rationality and language, is echoed by new discoveries in cognitive linguistics. Recent research (Fauconnier & Turner, 2002) has confirmed that the vast cognitive resources of \"backstage cognition\" are called up unconsciously when we engage in any language activity.   Generating Inner Narratives Generating narratives about characters’ inner world requires innovation at the story content, discourse and algorithmic levels. The techniques in stream of consciousness literature offer invaluable insights into literary representations of inner life, such as Woolf’s loosely structured plot, the \"caves\" of characters’ past (Woolf, 1957 [1925]), and various modes of interior monologues (Cohn, 1978).  The insights from planning-generated stories illustrate the impact of underlying computational techniques. Substantial changes at the algorithmic level therefore are needed to incorporate the new content and aesthetic requirements. As we have argued (Zhu & Ontañón, 2010), computational analogy, influenced by related cognitive science studies, is one of the promising directions towards our goal. Its emphasis on similarities and associations between different constructs is particularly useful to establish connections between external events and inner thoughts (e.g., the action of \"buying flowers\" and flower-related memories). Computational analogy may also be used to depict \"the train of thoughts\" by connecting a sequence of related events, one after another.    Case Study: Riu Our generative narrative project Riu is an on-going attempt to computationally generated stories about characters’ inner world. Inspired by Woolf’s novel Mrs. Dalloway (Woolf, 2002 (1925)), this project harnesses computational analogy at different levels of story generation for various narrative effects (Zhu & Ontañón, 2010). Similar to our earlier conceptual-blending-based (Fauconnier & Turner, 2002) project Memory, Reverie Machine (Zhu & Harrell, 2010 (forthcoming)), Riu is explicitly geared towards algorithmically narrating characters’ inner world, through the depiction of characters’ unrolling thoughts and subjective variations of such thoughts based on user interaction. An excerpt of system output at the current stage of development can be found in Fig 1.   Figure 1. Sample output of Riu (including user input)   Rather than ordering events in ways that lead to a desired goal, as in many planning-based systems, Riu adopts the computational analogy algorithm of structural mapping (Falkenhainer, Forbus & Gentner, 1989; Gentner, 1983) to connect external events with inner thoughts. The overall plot is loosely structured with potential inner thoughts between various external events. The overall story intends to invoke what Woolf described as \"no plot, no comedy, no tragedy, no love interest or catastrophe\" (Woolf, 1957 [1925]) and thus focuses primarily on characters’ psyche.  In Fig 1, the protagonist’s memory of a pet bird (italicized) is triggered because of its similarity to his encounter of another animal in the external story world. Importantly, inner events such as memories in Riu also have a significant impact on the protagonist’s external actions. In the case of Fig 1, when the user chooses to play with the cat on the street, the system infers that this action may, again, lead to the death of the cat and hence the protagonist’s sadness. In order to prevent that from happening, the system ignores this user command and removes \"play\" from the next user input options.  In conclusion, computer-generated narrative is an important frontier in digital humanities as a potential cultural expression innate to our contemporary society. Drawn on the synergy between stream of consciousness literature, artificial intelligence, and cognitive science, this paper proposed a new approach for expanding its narrative spectrum and focusing on characters’ inner world with its connection to external behaviors and environments. The Riu system was used to illustrate the initial results of our work, which suggests new expressive possibilities afforded by our approach.  ",
        "article_title": "Towards a Computational Narration of Inner World",
        "authors": [
            {
                "given": " Jichen",
                "family": "Zhu",
                "affiliation": [
                    {
                        "original_name": "Department of Digital Media, University of Central Florida USA",
                        "normalized_name": "University of Central Florida",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/036nfer12",
                            "GRID": "grid.170430.1"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-30",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Visualization tools for the discipline of history are being used increasingly. Historians can explore their efficacy as both educational instruments and platforms for displaying research findings. In order to obtain patterns of significance, historian’s work involves manipulating, memorizing, and analyzing substantial quantities of information from the series of documents at their disposal. In so doing, a central purpose emerges: the construction of a narrative that best fits into the representation of the past. Visualization tools have proved to be effective for facilitating users’ analytical tasks (Heer et al., 2008; Heer et al., 2009; Weizhong and Chen, 2007, James and Cook, 2005). Moreover, they have proved to be fruitful in the context of the digital humanities (Bonnet, 2004; Dalen-Oskam and Zundert, 2004). Recently, we proposed a visualization system for analyzing aristocrat names in a Japanese historical diary called Hyohanki (Toledo et al., 2009). In our system, the stacked graph is utilized to analyze the time series of those names. Stacked graphs, stacking time series on top of each other, are a useful method to time series visualization, resulting in a visual summation of time series values that provides an aggregate view stratified by individual series. Projects such as NameVoyager (Wattenberg et al., 2005) and sense.us (Heer et al., 2009) used animated stacked graphs to explore demographic data. In this paper, to preserve contexts through the stacked graph usage, we propose an extension of our previous work. Our system provides two functionalities: an interaction control for saving, querying, and deleting views, as well as a dynamic repository of views representing the context of the stacked graph usage. Keeping useful contexts facilitates coherent narratives (Bonnet, 2004). To support historians’ efforts in keeping contexts through interactive visual analysis of time series, both functionalities aim at recording how users receive units of information appropriated to construct narratives. To analyze the end-user perception of our system, we conducted a heuristic evaluation (Nielsen, 1992) with a domain expert who explored the data using the tool. This heuristic evaluation was a form of user study in which the expert reviewed the system to suggest advantages and disadvantages against the new functionalities. This approach helps to further elucidate the requirements and how the system meets experts’ needs. The results provide useful guidance for highlighting known historical facts as well as hints to unknown historical facts.  Methodology Our visualization system is based on a time series set containing the quantitative analysis of name occurrences in the Hyohanki diary. The set contains 121 time series in a timescale spanning from 1132 to 1171. Some parts of the diary suffer from missing data; for that reason, that period includes only the years 1132, 1139, 1149, 1152-1158 and 1166-1171. Likewise, the data has been normalized in order to measure the percentage relative value of the number of occurrences of a given name, in a given year, with the total percentage of that year. Additionally, using the Euclidean distance metric, we calculated the trends' similarity between time series. For each name, we recorded their five most similar trends. The method used to visualize the data is straightforward. Given a set of aristocrat names’ time series, a stacked graph is produced (Fig. 1a). The x axis corresponds to year and the y axis to the occurrence ratio — in percentage, as the data has been normalized —, for all names currently in view. The stacked graph contains a set of stripes, each one representing a name. The width of each stripe is proportional to the ratio of that name mentioned in a given year. The stripes are colored blue and the brightness of each stripe varies according to the number of occurrences, so that the most mentioned names during the whole period are darkest and stand out the most. Likewise, the name’s font size follows a similar encoding; the higher the occurrence frequency of a name, the larger its font size.    a) Initial view    b) Filtered view with occurrence frequency above 200 Fig. 1 Screenshots of the stacked graph Stacked Graph URL :    The interaction capability provided by our visualization system comes in two flavors. First, a stripe filtering mechanism allows users to filter the stacked graph in three ways: prefix typing, occurrence frequency, and stripe similarity. The second interaction control, which we call contextualization keeping, aims at keeping the views produced by users as they interact with the visualization system. Because stripe filtering acts as a view producer, and contextualization keeping as a dynamic repository of those views, we believe that both types of interaction controls embody a useful tool for visual exploratory analysis. Stripe filtering forms the base for the source of views. Consider for example the interaction control for filtering by string prefixes (Fig. 2a). Users are allowed to type either a complete name or only a prefix of that name. The figure shows a view corresponding to the prefix 藤原 (family name Fujiwara). The number of possible views that this interaction control can produce is equivalent to at least the size of the time series set. The Hyohanki diary produces about 150 views from a total of 121 names.              Fig. 2 Stripe filtering interaction controls   In addition to the stripe filtering, our system provides filtering by occurrence frequency (Fig. 2b), which uses a set of discrete values leading to seven possible views. Users, however, will likely want to combine the effects of more than one interaction control; for which case, the amount of possible views might increase enormously. An extreme scenario may occur when considering the last type of filtering, similarity of trends (Fig. 2c), which allows users to reveal those stripes that are similar to others. To cope with this issue, we propose in this paper a preliminary implementation for keeping the contextualization of the stacked graph usage. Our proposal for keeping contextualization in the stacked graph is composed of two parts: a toolbar which users can use to save and delete views and a repository of the resulting views from the effects of the toolbar usage (Fig. 3). For each saved view, the system records contextual information of the current visualization state. This information is saved in terms of the stripe filtering interaction controls used to produce those views. In order to support their sense making tasks, users can operate on the toolbar, especially on the saving button, whose effect suggests a mapping between the intrinsic facts of the view with the user mental model created from that view. The repository, on the other hand, provides the stacked graph with the ability to serve as a cognitive tool. This is because users can recover views from the repository, reason on them depending on their own criteria, and decide whether to keep the view or delete it. The main attraction of our proposal is its potential provision of stacked graph’s usage paths as the results of a visual exploratory analysis.    Fig. 3 Interaction control for keeping contextualization in the stacked graph     Results and Discussions This work attempts to provide a stacked graph module for gathering views to construct narratives. A concrete prototype has been developed and evaluated with an expert. In general, our user had a favorable experience with the system, though he commented that the similarity filtering was confusing and should provide a more effective approach for this. Below we have an observation the expert was able to make using the tool. Fig. 3 shows a use case of our visualization system. Six views were saved into the repository using a deductive reasoning approach, i.e., going from the stacked graph as a whole to a view containing only one name. From those views, only three of them gained relevance to construct the narrative, i.e., views 2-4. As shown in Fig. 4, or View 2 of Fig. 3, there are no significant variations in the trends with prefix 藤原 (family Fujiwara) — to some extent they were mentioned over the same period —. However, the similarity view of 藤原忠実 (Fujiwara no Tadazane) in View 3 of Fig. 3 (see also Fig. 2c) excludes his eldest son 藤原忠通 (Fujiwara no Tadamichi). In contrast, Fujiwarano Yorinaga (藤原頼長) not only is a member of that group, but he has the most similar trend to his father (Tadazane). This situation was used to construct the following narrative: \"Tadazane was possibly closer to his second son\", which actually confirms a known historical fact.    Fig. 4 Trends of family name 藤原 (Fujiwara) co-occurring over the same periods, View 2 from the repository in Fig. 3    ",
        "article_title": "Contexts, Narratives, and Interactive Visual Analysis of Names in the Japanese Hyohanki Diary",
        "authors": [
            {
                "given": " Alejandro",
                "family": "Toledo",
                "affiliation": [
                    {
                        "original_name": "Intelligent Computer Entertainment Laboratory Global COE Program in Digital Humanities for Japanese Arts and Cultures Ritsumeikan University Japan",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Ruck",
                "family": "Thawonmas",
                "affiliation": [
                    {
                        "original_name": "Intelligent Computer Entertainment Laboratory Global COE Program in Digital Humanities for Japanese Arts and Cultures Ritsumeikan University Japan",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-05-01",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Whereas Darwin is nowadays considered the founder of the modern theory of evolution, he wasn't the first to use this word in a biological context: indeed, the word \"evolution\" already had two distinct biological uses at the time the Origin of Species was first published (Bowler, 2003; Huxley, 1897): \"initially, to refer to the particular embryological theory of preformationism; and later, to characterize the general belief that species have descended from one another over time\" (Richards, 1998: 4). Deriving from the Latin evolutio, which refers to the scroll-like act of unfolding or unrolling, the word «evolution» was first used in biology to refer to the development of the embryo, mainly through the formulation, promulgation, and justification of preformationist and epigenetical theories. Embryological evolution would receive its fullest, most modern experimental and theoretical account in the works of Karl Ernst von Baer: characterizing embryological development as a gradual differentiation process leading from homogeneous matter to the production of heterogeneity and complexity of structure, von Baer would usually use the word Entwickelung to refer to this dynamic phenomenon, often followed by the Latin evolutio in parentheses. The ground-breaking importance of von Baer’s work, as well as its diffusion in the scientific community through numerous translations, commentaries, and appropriations, significantly contributed to consecrate the embryological use of the word \"evolution\". As for the use of the word evolution to describe specific development, its emergence is closely tied to Lamarckism: even though Lamarck never used the word ‘evolution’ himself to refer to the transformation of species over time and generations, his commentators, detractors, readers and followers often did however, thus contributing to the semantic alteration of the term. Indeed, \"by the 1830s, the word \"evolution\" had shifted 180 degrees from its original employment and was used to refer indifferently to both embryological and species progression\" (Richards, 1992: 15): Étienne Renaud Serres used the expression théorie des evolutions in his 1827 article Théories des formations organiques to refer both \"to the recapitulational métamorphoses of organic parts in the individual and the parallel changes one sees in moving (intellectually) from one family of animals to another and from one class to another\" (Richards, 1992: 69); von Baer, in rejecting the possibility of transmutation and the popular idea that embryological development recapitulates the progression of the species, used the word «evolution» to refer to both processes; in England, naturalists such as Charles Lyell, Joseph Henry Green, Robert Grant and Richard Owen also used the word \"evolution\" to both comment and reject Lamarckism (Bowler, 2003; Richards, 1993). While this dual usage of the word and its most common synonyms at the time (transformation, development, transmutation…) has been confirmed in the works of the most important biologists and naturalists of the first half of the 19th century, little is known about Darwin’s own stance on this matter: did he or not use the word ‘evolution’ or any other word to refer both to embryological and specific development? This question, however crucial it may appear, proves very difficult to answer: while the Origin of Species is generally considered as the birth document of the theory of evolution, studies on and around this book often overlook the fact that the word itself is rarely used by Darwin, the sole and slight exception being the sixth and last edition (1872) of the work.  Occurrences of \"evolution\", \"evolve\", and \"evolved\" in the Origin of Species  1st Edition (1) evolved: XV (490)   2nd Edition (1) evolved: XV (490)   3rd Edition (1) evolved: XV (525)   4th Edition (1) evolved: XV (577)   5th Edition (2) evolved: XV (573), XV (579)   6th Edition (14)  evolution: (VII:201(2), 202), VIII (215), X (282), XV (424 (3)) evolve: VII (191) evolved: VII (191, 202(2)), XV (425, 429)    This lexical scarcity doesn’t necessarily mean however that the concept of evolution isn’t present elsewhere in the text, where the words ‘evolution’, ‘evolved’, and ‘evolve’ don’t appear. According to distributional semantics theory, meaning can be more easily stated as a property of word combinations than of words per se: in every sentence and paragraph, each word brings its own constraints to the whole, reduces the sets of possible words that could fit with it, therefore increasing the total information conveyed and structuring the semantic dimension of each word thus combined. In short, this theory holds that \"similarities and patternings among the co-occurrence likelihoods of various words correlate with similarities and patternings in their types of meaning\" (Harris, 1991: 341). In this sense, if concepts are thought of as networks of such meaning-bearing word combinations, then, conceptual structures can determine the semantic dimension of a text without being properly lexicalized; in other words, such considerations, while emphasizing the distinction between the semantic associations of specific concepts and their embodiment in natural language, also seem to imply the possibility of \"reading between the lines\", that is, of identifying and analyzing concepts on the sole basis of their relations with other words and concepts and independently of any proper designation. In view of this, the fact that the word \"evolution\" itself is rarely found in the sixth edition of the Origin of Species doesn't necessarily imply that the lexical and inferential network it refers to and that constitutes its conceptual dimension isn’t present elsewhere in the text and can't be studied in its stead. In this sense, taking into account word combinations similar to those where the word 'evolution' occurs instead of focusing solely on the latter might be the most reliable way to determine whether or not Darwin’s concept of evolution in the Origin of Species refers to both embryological and specific development, like most biological theories of the same period. However, dealing with word combinations manually might prove difficult, if not impossible. In light of this, a new computer-assisted conceptual analysis tool has been developed by the LANCI laboratory, one which aims to \"read Darwin between the lines\", that is to identify where the author \"conceptually\" refers to evolution, regardless of the presence or the absence of the word itself. Theoretically speaking, this new approach is based on two fundamental assumptions: 1) The inferential nature and dimension of a concept are linguistically expressed in a differentiated, contextualized and regularized manner; 2) these regularities and patterns can be identified or distinguished using algorithmic, iterative and automatic clustering methods. Concretely, the algorithm aims at \"digging deeper into data\" by means of an iterative clustering process. Following an initial clustering of the analyzed corpus (in this case, the 974 paragraphs of the sixth edition of the Origin of Species), the iterative concordance clustering process starts by retrieving the most characteristic word of each cluster containing the word(s) to be analyzed, that is, the word that has the higest TF.IDF rating (Term Frequency – Inverted Document Frequency) for each of these clusters. Then, the concordance of each of these characteristic words is extracted from the corpus, and the same process of clustering, cluster selection, TF.IDF rating and ranking, word selection and concordance extraction is performed on each of those new concordances, until no new characteristic word is found or no more clusters containing the word(s) to be analyzed are found.  Iterative Concordance Clustering Algorithm  1. Concordance extraction: For each cluster containing the word(s) to be analyzed, extract the concordance of the highest-TF.IDF-ranked word.   2. Concordance clustering: For each previously unselected word, proceed to the clustering of its concordance.   3. Iteration: Return to step 1, unless 1) no new highest-TF.IDF-ranked word is found, or 2) no clusters containing the word(s) to be analyzed are found.   In order to identify the principal lexical constituents of the concept of evolution and determine whether or not this underlying conceptual structure includes references to both embryological and specific processes,two different extraction procedures were made: the first one only aimed at the word \"evolution\", while the second one also added \"evolve\" and \"evolved\". Figures 1 and 2 show the results of the two analyses.     Figure 1: Conceptual analysis of \"evolution\"      Figure 2: Conceptual analysis of \"evolution\", \"evolve\", and \"evolved\"   In addition to new and unforeseen methodological discoveries, interpretation of both conceptual analyses seems to bring the sixth edition of the Origin of Species closer to the contemporary works of the more mature Herbert Spencer, who began to de-emphasize the connection between embryology and the general process of \"evolution\" and thus contributed to forge the present, strictly specific and most commonly known biological use of the word \"evolution\". These results, along with the method that made them possible, are not in any way definitive, and further improvements and modifications of the iterative concordance clustering process are to be expected. Upon completion, this rather new and original approach, while hoping to bring new insights in the understanding of the Origin of Species, also aims at underlining the pertinence and usefulness of text mining methods and applications for expert and specialized text reading and analysis, as well as their importance for the future development of philology, hermeneutics, social sciences and humanities in general.  ",
        "article_title": "Reading Darwin Between the Lines: A Computer-Assisted Analysis of the Concept of Evolution in The Origin of Species",
        "authors": [
            {
                "given": " Maxime B. ",
                "family": "Sainte-Marie",
                "affiliation": [
                    {
                        "original_name": "Université du Québec à Montréal, Canada",
                        "normalized_name": "University of Quebec at Montreal",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/002rjbv21",
                            "GRID": "grid.38678.32"
                        }
                    }
                ]
            },
            {
                "given": " Jean-Guy",
                "family": "Meunier",
                "affiliation": [
                    {
                        "original_name": "Université du Québec à Montréal, Canada",
                        "normalized_name": "University of Quebec at Montreal",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/002rjbv21",
                            "GRID": "grid.38678.32"
                        }
                    }
                ]
            },
            {
                "given": " Nicolas ",
                "family": "Payette",
                "affiliation": [
                    {
                        "original_name": "Université du Québec à Montréal, Canada",
                        "normalized_name": "University of Quebec at Montreal",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/002rjbv21",
                            "GRID": "grid.38678.32"
                        }
                    }
                ]
            },
            {
                "given": " Jean-François ",
                "family": "Chartier",
                "affiliation": [
                    {
                        "original_name": "Université du Québec à Montréal, Canada",
                        "normalized_name": "University of Quebec at Montreal",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/002rjbv21",
                            "GRID": "grid.38678.32"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-05-01",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The Linguistic Atlas team at the University of Georgia (LAP) and the LICHEN research team at the University of Oulu have been investigating the application of advances in information engineering to humanities scholarship, in particular methods for managing and mining large-scale linguistic databases. Our aim has been to bring together the linguistic and technological expertise from Oulu and Georgia in order to develop practical solutions for common problems. In this paper we will show the results of this cooperation--including the Digital Archive of Southern Speech (DASS), the pilot product for LAP-LICHEN released in 2009--and discuss our experiences and lessons learned. The LAP audio archive, amounting to 7000 hours of interviews, is an unparalleled resource for study not only of the common language of the US but for its culture more generally, stories of daily life in America. Study of LAP interviews so far has taken advantage only of small bits of transcribed data extracted from the full interview. The large, untranscribed bulk of LAP interviews consists of the speakers’ accounts of their lives and their families, their occupations and their diversions, their houses and their land. Along with direct questioning and conversational passages, in a quarter of the thousands of audio files so far processed these stories take the form of narratives of at least one minute of continuous speech. To preserve and to make this audio archive available puts the people back into what has seemed to some scholars to be a dry academic exercise of collecting words. The LAP team has always appreciated the personal, individual nature of each interview, as well as the way that the interviews can represent American culture; with DASS, we can now share that appreciation much more broadly with both the academic community and with the public. DASS is a collection of 64 interviews from the Linguistic Atlas of the Gulf States (LAGS) selected by LAGS Director Lee Pederson. Four interviews come from each of the sixteen regional LAGS sectors. Within each sector there is one speaker from each Atlas Type: folk (largely uneducated and insular), common (moderate education and experience), cultivated (higher education and/or participation in high culture). One African American speaker was selected from each sector, and folk, common, and cultivated African American speakers are distributed across the sectors. Speakers cover a wide range of ages and social circumstances. Over 400 hours of audio files are provided both as large uncompressed .wav files (useful for acoustic phonetic processing) and as thousands of small .mp3 files for general listening. Files are indexed according to subject matter and speaker, according to a set list of 40 topics. Metadata and finding aids for particular topics and kinds of speakers are provided, including search tools and a GIS function. Together the DASS data and the LICHEN tools comprise about 200 GB of data, provided on a portable USB drive. The interviews were digitized and processed by the LAP team at the University of Georgia with assistance from a grant from the National Endowment for the Humanities (PW-50007, “Digitization of Atlas Audio Recordings”, with Opas-Hänninen as partner). The first phase of the LICHEN project was lead jointly by Opas-Hänninen and Seppänen and funded by a grant from the Emil Aaltonen Foundation (2006-2008, with Kretzschmar as an international collaborator). The University of Oulu and the University of Georgia drew up legal agreements regarding copyrights and the distribution of the software with the data. The DASS/LICHEN package is distributed by the LAP. DASS is only the beginning, however. The research team at the University of Oulu has developed LICHEN as an electronic framework, i.e. a type of toolbox, which handles multimodal data. The toolbox has been developed using two sets of data as testbeds, namely the Oulu Archive of Minority Languages in the North containing samples from the Kven, Meänkieli, Veps and Karelian languages, as well as DASS. Some of the data from minority languages exists as video, which the toolbox handles along with audio and text. We are now working on a transcription tool, so that audio and video materials can be provided with textual representations aligned with the sound and video. Finally, we are rebuilding LICHEN as a Web-enabled framework, so that users can access our language and cultural materials remotely in line with the movement for the creation of public corpora (see, e.g., Kretzschmar, Anderson, Beal, Corrigan, Opas-Hänninen, and Plichta 2006; Kretzschmar, Childs, and Dollinger 2009). LAP-LICHEN cooperation began in 2004, when Kretzschmar, Opas-Hänninen, Anderson, Beal, and Corrigan met in Newcastle, to follow up on conversations about public corpora begun earlier. The group found that there were common problems, standards, best practices for the corpora managed by those attending, and agreed to prepare a presentation at ICAME in 2005 to highlight the possibility for shared methods and joint actions (later published as the 2006 programmatic article). The LAP-LICHEN collaboration bloomed as a result. Grants were obtained for cooperation: the LICHEN model was included in a large NEH proposal for archival digital audio processing for LAP, and conversely LAP was adopted as the large-scale test for the enhancement of LICHEN at Oulu. An operational version of LICHEN that might be used for LAP was available and demonstrated at DH2007 (Urbana). Further development occurred in conjunction with DH2008 (Oulu), leading to testing of LICHEN on LAP materials in Georgia in Fall 2008. As a result of these steps, the collaborators rewrote the specifications for what the program needed to do in February 2009, as substantial bodies of archivally-processed LAP sound files became available for LICHEN development and testing. The collaborators decided that the key requirements for the specification arose from not only the characteristics of the data (e.g. the structure consisting of interviews, reels and clips with metadata and multimedia on each level) and the desired uses of the framework (e.g. search, browse, and view possible audio selections on a map with GIS), but also from the fact that both tools development and the final stages of data preparation were taking place simultaneously. The data had to be available as flat files usable through any regular file browser, and the sheer scale of the data ruled out the possibility of creating duplicate files inside the program structure as originally designed. The software needed to make use of the existing files and file structure, and so the task of combining the data and the tools became an exercise in conforming tools to data with as little effect on the data itself as possible. To this end, the collaborators developed two methods for bringing in the data and its associated metadata: 1) a general-purpose parser that could traverse file and folder structures and evaluate regular expression patterns to parse metadata from the file and folder names, and 2) a mechanism for re-formatting standard spreadsheet documents into XML documents for use by the tools. The existence of the tools affected the preparation of the data in two ways: 1) some incorrectly named files and folders had to be renamed to conform to the agreed format, and 2) all text files had to be converted from the Microsoft Word format into a plain text format for viewing from within the developed tools. Both changes were also beneficial to the data collection itself, improving consistency within the data and file support across different platforms. In turn, the developed tools provide access into the database through browsing of the data by natural entities such as interviews, topics and geographic location (as opposed to just files and folders) and queries leveraging the full potential of all the metadata fields. The DASS product was launched in April 2009 at the SECOL conference (New Orleans), and public distribution began in the summer of 2009 after resolution of the legal issues of the collaboration with university authorities at Georgia and Oulu. Even given the close collaboration between developers, arriving at legal language that suited the university authorities proved to be quite difficult: the Georgia lawyers thought they could be more laissez-faire because the product was unlikely to generate substantial monetary returns, while the Oulu authorities were more focused on retaining the university's rights of ownership. In the end, separate rights language had to be included for materials developed at Georgia and for LICHEN development at Oulu. As might be expected, integration of a large-scale database with multimedia display functions, while still maintaining the high degree of usability necessary for public access, has turned out to be more difficult than accomplishing any of the separate tasks. And we are not finished. We are currently building a transcription tool to incorporate into the LICHEN framework, so that the public (as well as professional researchers) can contribute transcriptions of audio files. We also want to increase the Geographic Information System (GIS) functionality into the LICHEN toolbox in order to support map-based selection and visualization schemes, surpassing those implemented on the existing Atlas Web site. Finally, we want incorporate the CATMA concordancing tools, currently being built by a collaborating research team at the University of Hamburg. All of these goals, we trust, will make LAP and other data accessible, not just on portable media, but on the web with LICHEN.   ",
        "article_title": "LAP, LICHEN, and DASS – Experiences combining data and tools",
        "authors": [
            {
                "given": " Lisa Lena",
                "family": "Opas-Hänninen",
                "affiliation": [
                    {
                        "original_name": "University of Oulu, Finland",
                        "normalized_name": "University of Oulu",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/03yj89h83",
                            "GRID": "grid.10858.34"
                        }
                    }
                ]
            },
            {
                "given": " Ilkka",
                "family": "Juuso",
                "affiliation": [
                    {
                        "original_name": "University of Oulu, Finland",
                        "normalized_name": "University of Oulu",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/03yj89h83",
                            "GRID": "grid.10858.34"
                        }
                    }
                ]
            },
            {
                "given": " William A. Jr.",
                "family": "Kretzschmar",
                "affiliation": [
                    {
                        "original_name": "University of Georgia, USA",
                        "normalized_name": "University of Georgia",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00te3t702",
                            "GRID": "grid.213876.9"
                        }
                    }
                ]
            },
            {
                "given": " Tapio",
                "family": "Seppänen",
                "affiliation": [
                    {
                        "original_name": "University of Oulu, Finland",
                        "normalized_name": "University of Oulu",
                        "country": "Finland",
                        "identifiers": {
                            "ror": "https://ror.org/03yj89h83",
                            "GRID": "grid.10858.34"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-29",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " More than ten years ago what was called a meta-dictionary was proposed as a central part of the framework for a dictionary laboratory at the University of Oslo (Ore 2001). The framework has since functioned as a pivot in the combined lexical database, text corpus and manuscript editing system for Norsk Ordbok (Norwegian Dictionary). Norsk Ordbok is published in twelve volumes (to be completed in 2014) and provides a scholarly and exhaustive account of the vocabulary of Norwegian dialects and the written language Nynorsk, one of the two official written forms of Norwegian.  The architecture of the dictionary framework described in this paper was based upon both explicit and implicit assumptions - and some of the latter were not only not consciously considered in the construction phase, they have also led to features or lacks of features in the system where we now see the need for change. In this paper we look at problems related to links between the meta-dictionary and the sources and show how some of the problems are solved.   The meta-dictionary? In the 1990s a huge amount of lexicographical source material (dictionaries, slip archives and texts) was made electronically available by a national digitization project. By then Norsk Ordbok had produced three volumes out of twelve. Being a project started in 1930 the future of the project was highly uncertain. Thus the original motivation behind the meta-dictionary was to create a common web based interface to the background material by inter-linking the material to a common headword list as a meager substitute for the edited dictionary. A similar approach has later been taken by the Dictionary of Old Norse Prose in Denmark (ONP). Fortunately, the Norsk Ordbok project was refunded and revitalized in 2001. It was decided that the new project should be completely digital. As a result a new version of the meta-dictionary was designed.  An entry in the meta-dictionary can be seen as a folder containing (pointers to) possibly commented samples of word usage and word descriptions taken from the linked databases etc. Each entry is labeled by normalized headword(s), word class information and the actual orthographical standard used. The working lexicographers view the meta-dictionary as an easy access to systematized source material. The chief editors use it as tool for headword selection and in dimensioning the printed dictionary. The database used in the Cobuild project for lemma selection from the corpus, is an early example of such a database (Sinclair 1987). The Norsk Ordbok is a historically oriented dictionary covering the period 1600 to the present. The time span and the focus on dialects make the background material heterogeneous. The oldest sources are glossaries compiled in the 17th/18th centuries, mainly the results of work done by vicars collecting information about their parishioners' language on request from the government in Copenhagen. For the description of the word inventory of the current dialects surveys and especially local dictionaries form valuable sources. The meta-dictionary constitutes a bidirectional network. Thus the historical or dialectal dictionary linked to the system can be used as an entry point to the entire set of information.    Building a dictionary net The traditional systematic overviews of the use of words in context have been alphabetically ordered paper slips with each word in a small context and the source information. The slip collections have gradually been replaced by text corpora. In the Norsk Ordbok project the slip collections are digitized and linked to the meta-dictionary, and a new annotating tool for singular language observations has been developed. A standard TEI-encoded text corpus spanning the period 1850 to present is gradually constructed. The results from corpus queries can be stored and linked to the meta-dictionary.  The old and the local dictionaries and glossaries constitute an important source for historical and dialectal word usage respectively. Traditionally such dictionaries and glossaries have been transcribed to paper slips and stored in the slip collection. In the new system the dictionaries could have been included in the corpus. This may be done with the newer dialect dictionaries. The old dictionaries are written in Danish or Latin and would have introduced a lot of linguistic noise in the corpus. As these dictionaries are important documents in themselves it was decided to treat them as individual works documenting the language view of their time. The modern dialect dictionaries are given an XML-encoding according to TEI’s printed dictionary format. The 17th /18th centuries' dictionaries are represented by printed, annotated text editions of the original manuscripts. These editions have been transcribed and given a TEI markup reflecting their structure, generally not compatible with TEI’s printed dictionary format. Due to their systematic character, <div>-elements can be used to organize the text into chunks describing words and thematic sets of words. The “headwords” are clearly identifiable and are marked as <w>-elements. The loose structure implies that there may be more than one “headword” in each text chunk. The TEI-texts are stored as blobs in a relational database. The TEI-texts are chopped up according to the entries (dictionaries) and the text chunks (glossaries) and stored together with the headwords (slightly normalized) in a separate table structure.   In the early version of the system, the linking between these sources and the meta-dictionary were on the <entry> level for the local and on the <div> level for the old dictionaries. There was no information about the keyword in the selected dictionary that was used to create a link. In some cases when a <w>-element was removed an invalid link from the meta-dictionary to the external text sets was left. Today the link is annotated with the actual headword and the person responsible for the link.  The process is automatic with a manual check: a daily job runs through registered dictionaries and looks for keywords in a special metadata field in the database. If the word is found but there is no existing link between the meta-dictionary and this text unit, a link is created, and the record in the meta-dictionary is marked as changed and will be forwarded to an editor for approval. If the word is not found in the meta-dictionary, a new entry is created and linked with the text unit, and this will be sent to the editor for approval (see also Fournier 2001 and Gärner 2008 for interlinking of dictionaries).    The meta-dictionary and other dictionary nets What constitutes a word is an unresolved linguistic question. A traditional monolingual dictionary is a word form oriented index to a set of concepts and meetings where each entry is indexed by a headword and contains a possible meaning hierarchy with samples. Word forms denoting related concepts are connected by cross references. The Wordnet approach is to focus on the concepts and collect the word forms denoting the same concepts in sets of synonyms (synsets). The synsets can then be organized according to a predefined ontology such as in the Global Wordnet Grid (Vossen 2009). The two ways of organizing word information is fully compatible. A word net can be converted to a traditional dictionary and a well organized dictionary rich on semantic references can be converted into a Wordnet.  The current meta-dictionary was pragmatically designed 8 year ago. It has in itself become a valuable lexicographical documentation system. The source material spans both in time and space. Due to the practical purpose, that is, editing a traditional dictionary, the word forms are linked mostly etymologically. Thus an entry covers many concepts as does an entry in a traditional dictionary. However, the meta-dictionary also groups how different scholars have described the meaning of a word from 1600 to the present. The resources comprise the old digitized paper slip collections, the dictionaries and glossaries and stored results from querying the corpus. They all represent collections of systematized language documentation in their own right and premises. The entries in the Norsk Ordbok are in fact just yet another source of (scientifically) systematized language information linked to the others. However, the Norsk Ordbok system groups the information according to meaning and the dictionary is rich in synonym relations. A future research project is to use the information from the dictionary to create a second set of articles in the meta-dictionary in a Wordnet fashion with semantic relations.  ",
        "article_title": "Re-linking a Dictionary Universe or the Meta-dictionary Ten Years Later",
        "authors": [
            {
                "given": " Christian-Emil",
                "family": "Ore",
                "affiliation": [
                    {
                        "original_name": "University of Oslo, Norway",
                        "normalized_name": "University of Oslo",
                        "country": "Norway",
                        "identifiers": {
                            "ror": "https://ror.org/01xtthb56",
                            "GRID": "grid.5510.1"
                        }
                    }
                ]
            },
            {
                "given": " Espen S.",
                "family": "Ore",
                "affiliation": [
                    {
                        "original_name": "University of Oslo, Norway",
                        "normalized_name": "University of Oslo",
                        "country": "Norway",
                        "identifiers": {
                            "ror": "https://ror.org/01xtthb56",
                            "GRID": "grid.5510.1"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-20",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  HESTIA (the Herodotus Encoded Space-Text-Imaging Archive) is an interdisciplinary project, sponsored by the AHRC and involving the collaboration of academics from Classics, Geography and Archaeological Computing, that aims to enrich contemporary discussions of space by developing an innovative methodology to the study of an ancient narrative, Herodotus’ Histories. Using the latest ICT, it investigates the ways in which space is represented in the Histories, and develops visual tools to capture the ‘deep’ topological structures of the text, extending beyond the usual two-dimensional Cartesian maps of the ancient world. In addition to exploring the network culture that Herodotus represents, one of its stated outcomes is to introduce Herodotus’ world to new audiences via the internet. This paper will set out in more detail that methodology, paying particular attention to the decisions that we have made and the problems that we have encountered, in the hope that our project can contribute not only to offering a more complex picture of space in Herodotus but also to establishing a basis for future digital projects across the humanities which deal with large text-based corpora. For the purposes of a twenty minute presentation, we address three key areas of interest:  To provide the background to the data capture and digital mark-up of the Histories. Our project differs from many by utilizing a digital resource already in the public domain: the text of Herodotus freely available from Perseus (http://www.perseus.tufts.edu/hopper/). Though the capture of the digital text from Perseus (version P4) gave our project a welcome initial boost, a number of issues have had to be overcome including procedural conversion, which involves handing back a P5 text to Perseus. To sketch out the the type, structure and categorization of our spatial database. A PostgreSQL database was chosen because its PostGIS extension provides excellent functionality for spatial data and is widely supported by other applications: one key principle of HESTIA has been to use open source software in order to maximize its potential dissemination and reusability of its data. By storing information about references, locations and the text in the database, it has been possible to provide it to both a Desktop GIS system and Webmapping server simultaneously (see figure 1).  To present a sample set of results of the maps that we have been able to generate using the geo-referenced database. While sections 1 and 2 will be of particular concern to anyone wishing to understand how one may interrogate spatial data using the digital resources available, this last stage holds the greatest interest for the non ICT expert since it demonstrates the use to which data in this form can be employed: hence the main focus of this paper will be on explaining the five kinds of map that we have been able to generate:  Geographical Information System (GIS) maps. The most basic maps that are generated simply represent a ‘flat’ image of the spatial data: that is to say, they mark all the places that Herodotus mentions over the course of his work with a single point, thereby providing a snapshot of the huge scope of his enquiry. In this way one is able to gain an overview of the places mentioned in Herodotus and divide them according to three different kinds of spatial category: settlement, territory and physical feature (see figure 2). A variation on this basic model depicts places according to the number of times they are mentioned (see figure 3). GoogleEarth. In order to start experimenting with public dissemination it was decided to expose the PostGIS data as KML: a mark-up format that can be read by a variety of mapping applications including GoogleEarth. With this ‘Herodotus Earth’ application, users will be able to construct ‘mashups’ of visual and textual data. So, for example, since all places are linked to entries in the database, when one clicks on a particular location in GoogleEarth, it will be possible to bring up a dialog box containing Herodotus’ text (in both English and Greek) for that particular location for every occasion when it is mentioned in the narrative (see figure 4). TimeMap. Whilst it is possible to visualise narrative change using graphs, and static differences using GIS, it is more difficult to visualize spatial changes throughout the narrative; GIS does not have useful functionality in this regard except for the ability to turn layers on and off, a process which becomes impractical beyond book level. The most likely candidate to provide this kind of functionality is an Open Source JavaScript project called TimeMap, developed by Nick Rabinowitz, which draws on several other technologies in order to allow data plotted on GoogleMaps to appear and disappear as a timeline is moved. In collaboration with the project’s IT consultant Leif Isaksen, Nick Rabinowitz has adapted his schema in order to represent the book structure of Herodotus’ narrative in a similar way (see figure 5). Database-generated network maps. Since the GIS maps outlined in i. have little to say per se regarding Herodotus’ organization of space, a key next step has been to explore rapidly-generated networks based on the simple co-presence of terms within sections of the text. The purpose of producing networks of this kind is to start exploring the connections that Herodotus himself makes between places, seeing how strongly the narrative is bound to geographical regions, and flagging up potential links between particular locations (see figure 6). Figure 7 illustrates one such simple network, that for “territories” across the entire Histories. It shows a series of links connecting Greece to other areas within the Mediterranean world: but the territory that has the strongest connections in this basic network culture is Egypt. While surprising, it does make sense on reflection, since for a better part of one book Herodotus uses Egypt as the touchstone against which other cultures, including Persia and his own, Greece, are compared. It is as a tool of comparison, then, that Egypt appears to be the centre of Herodotus’ network picture of the Mediterranean. Figure 8 complements this picture by presenting the networks of physical features, which envelop the comparison between Greece and Egypt. Manual network maps. The automated maps outlined in iv. rely on ‘counting’ the number of times two or more places are connected to each other: they have little to say about the kind of connection being drawn. We end our presentation, then, with a brief comparison to text-based qualitative analysis, which attempts to categorize relationships according to fundamental geographical concepts of movement or transformation, based on the close reading of one book (5). Our different approaches are intended to complement, challenge and inform each other with a view also to suggest ways by which the automated process may be extended, such as by adopting text-mining procedures.     In sum, this paper aims to meet three outcomes:  To outline a methodology for dealing with digital data that may be transferred, adapted and improved upon in other fields of digital humanities. To demonstrate the value of digital projects within the humanities for helping to achieve ‘impact’ by bringing the world of a fifth-century BC Greek historian into everyone’s home. To show the potential for the digital manipulation of data in posing new kinds of research questions.                 Fig. 1    Fig. 2    Fig. 3    Fig. 4    Fig. 5    Fig. 6    Fig. 7    Fig. 8    ",
        "article_title": "Mapping the World of an Ancient Greek Historian: The HESTIA Project",
        "authors": [
            {
                "given": " Elton",
                "family": "Barker",
                "affiliation": [
                    {
                        "original_name": "Open University, UK",
                        "normalized_name": "Universidade Aberta",
                        "country": "Portugal",
                        "identifiers": {
                            "ror": "https://ror.org/02rv3w387",
                            "GRID": "grid.26693.38"
                        }
                    }
                ]
            },
            {
                "given": " Chris",
                "family": "Pelling",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, UK",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": " Stefan",
                "family": "Bouzarovski",
                "affiliation": [
                    {
                        "original_name": "University of Birmingham, UK",
                        "normalized_name": "University of Birmingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/03angcq70",
                            "GRID": "grid.6572.6"
                        }
                    }
                ]
            },
            {
                "given": " Leif",
                "family": "Isaksen",
                "affiliation": [
                    {
                        "original_name": "University of Southampton, UK",
                        "normalized_name": "University of Southampton",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01ryk1543",
                            "GRID": "grid.5491.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-20",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Carolina Digital Library and Archives (CDLA) and Documenting the American South (DocSouth) are a digital library laboratory that creates, develops, and maintains online collections regarding the history of the American South with materials drawn primarily from the outstanding archival holdings of the UNC library. In this presentation, we plan to demonstrate how the close partnership between UNC librarians and faculty forges its path in the frontier of digital humanities. Our experience clearly demonstrates that digital historical scholarship cannot be done on the old model of the scholar laboring alone, “the solitary scholar who churns out articles and books behind the closed door of his office” (see Kenneth Price, 2008). By bringing together faculty and librarians’ expertise, collaborators endeavor to use digital technologies in a variety of innovative ways to collect, organize, and display data and materials that illuminate the temporal and spatial unfolding of historic events. Recent experimental work with GIS helps us to better understand how the use of digital technologies changes the way we do research in humanities and how it facilitates learning in the classroom. Indeed, “GIS, in combination with other branches of scholarship, has the potential to provide a more integrated understanding of history” (see Ian N. Gregory, 2003). At the same time, the wide array of issues (digitizing and geo-referencing of Sanborn and other historic maps, use of JavaScript mapping APIs, such as Google Maps and the open-source Open Layers, for zooming and hotspot addition, layering and geo-tagging scholarly content) will be presented based on several completed and in progress digital history collections built in close collaboration of UNC librarians working with UNC scholars.  1. “Going to the Show” (www.docsouth.unc.edu/gtts/) is the first digital archive devoted to the early experience of cinema across an entire state. In a research project, Prof. Allen collaborated with digital publishing experts and special collections librarians at UNC to create an online, interactive digital collection of maps, photos, postcards, newspaper clippings, architectural drawings, city directories and historical commentary that illuminate and reconstruct cultural and social life during the first three decades of the 20th century in North Carolina. Supported by a grant from the N.C. State Library and a National Endowment for the Humanities Digital Humanities Fellowship, “Going to the Show” (GttS) developed the innovative system for layering content on electronically stitched and geo-referenced Sanborn Fire Insurance Maps. Especially in its highly detailed case study of early moviegoing in Wilmington, N.C., GttS demonstrates the extraordinary potential for illuminating community history through the interaction of documentary material and Sanborn maps (see Figure 1).   Figure 1. Google Maps AP1 used to present 1915 SAnborn map with layered historic materials to document the moviegoing in North Carolina.    2. Building on the digital history project “Going to the Show”, the project team decided to expand the reach of their expertise by creating a web-based toolkit that will allow libraries, schools, museums, local history societies, and other community organizations to preserve, document, interpret, display, and share the history of their downtowns. Called “Main Street, Carolina: Recovering the History of Downtown Across North Carolina,” the toolkit will provide users with a flexible, user-friendly digital platform on which they can add a wide variety of “local” data: historical and contemporary photographs, postcards, newspaper ads and articles, architectural drawings, historical commentary, family papers, and excerpts from oral history interviews—all keyed to and layered on top of digitized Sanborn Fire Insurance Maps. The toolkit will consist of a PHP-based web application and a JavaScript API. The web application will be compact in size, resource-light, and easy to install on the local organization’s own web server or that of a third-party web-hosting service. It will provide administrative tools for configuring the site, creating place markers, creating simple web pages for content, and customizing the look and feel (see Figure 2). These place markers can then be associated with images, stories, or other content, providing a visual link between the content and related physical locations. The map interface is the focal point of the software and will allow users to explore content associated with specific geographic locations by interacting with place markers, or \"push-pins,\" overlaid on top of historic maps. Clicking on a place marker's icon will display an information bubble which can contain text, images, and links to additional content. The user will be able to view and effortlessly pan across entire downtowns as a seamless integration of multiple high-resolution map pages; zoom from a bird-eye view to the smallest cartographic feature; compare successive map iterations showing the same building, block, or neighborhood; and overlay any of these views with contemporary satellite and map images at the same scale. The JavaScript API will allow users to include digitized maps created for other CDLA projects as layers in their own websites or mash-ups which use the Google Maps or Open Layers mapping APIs. For example, a user could embed an historic map in a blog post or add a Sanborn Map as a layer to their existing website which uses Google Maps to show the location of homes that are listed on the National Register of Historical Places. MSC is funded by a private funding and an NEH Start-up Grant. Development for this project began in October 2009. We plan to release the toolkit and pilot projects developed in collaboration with external partners in summer 2010, prior to the start of the conference.  Figure 2. “Main Street, Carolina: Recovering the History of Downtown Across North Carolina” tool kit. Administrative form for entering historical documents.    3. “Driving through Time: The Digital Blue Ridge Parkway in North Carolina” will present an innovative visually and spatially based model for illustrating North Carolina’s key role in creating the Parkway, representing the twentieth-century history of a seventeen-county section of the North Carolina mountains, and for understanding crucial elements of the development of the American National Park system. The project will feature historic maps, photographs, postcards, government documents, oral history interviews, and newspaper clippings. Each historic document will be assigned geographic coordinates so that it can be viewed on a map, enabling users to visualize and analyze the impact of the Blue Ridge Parkway on the people and landscape in western North Carolina over both space and time (See Figure 3). Primary sources will be drawn from the collections of the UNC-Chapel Hill University Library, the Blue Ridge Parkway Headquarters, and the North Carolina State Archives. These materials are especially significant in that they document one of North Carolina’s most popular tourist attractions, but also in the way that they help illuminate the way that the Blue Ridge Parkway transformed the communities through which it passed. In addition to the digitized primary sources, the project will include scholarly analyses of aspects of the development of the Blue Ridge Parkway.  A geospatial format is uniquely appropriate for considering the history of the Parkway and its region. As a narrow park corridor pushed through a long-populated southern Appalachian landscape, the Parkway rearranged spaces, repurposed lands, reorganized travel routes, and opened and closed economic opportunities through control of road routing, access, and use. The social conflicts it engendered, therefore, frequently entailed spatial components – should the road go here, or there; should it take more or less land; should this or that property be favored with direct Parkway access (or not)? Understanding these aspects of Parkway history without reference to spatial relationships on the land is challenging, as the project’s scholarly adviser, Dr. Anne Mitchell Whisnant, recognized when publishing her 2006 book, Super-Scenic Motorway: A Blue Ridge Parkway History (UNC Press). Her experience, both in writing the book and in delivering numerous public presentations since its appearance, is that narrative alone cannot provide the public with the tools to comprehend past controversies or present land protection challenges. Using digital and geospatial technologies to open a new window on the history of the Parkway and its region is especially timely considering the approach of the Parkway’s 75th anniversary in 2010 and the National Park Service’s 100th anniversary in 2016.  The collaboration between the library and Dr. Whisnant has been enhanced by Whisnant’s engagement in related field of “public history,” which is history practiced outside the walls of academia, with and for public audiences. Based in academia but designed for public benefit, “Driving through Time” has offered an exceptional opportunity for involving undergraduate and graduate students in Dr. Whisnant’s Introduction to Public History class in its creation. As a scholarly project being built through the expertise of a large team (as nearly all public history undertakings are), “Driving through Time” has been an ideal space for students to gain hands-on experience in doing public history collaboratively, in real-time, with their instructor. Students are doing original primary source research in the university’s special collections, identifying materials for inclusion in the online exhibit, developing their own historical narratives, working with new tools such as wikis and databases, and contributing to the creation of metadata. Because the instructor has not predetermined the final outcome, furthermore, students are being given ownership over both their process and their final products and practicing navigating the unexpected twists, turns, delights, and disappointments that historical research always entails.  Figure 3. \"Driving throug Time: The Digital Blue Ridge Parkway in North Carolina\"     ",
        "article_title": "Unfolding History with the Help of the GIS Technology: a Scholar-Librarian Quest for Creating Digital Collections",
        "authors": [
            {
                "given": " Natasha",
                "family": "Smith",
                "affiliation": [
                    {
                        "original_name": "University of North Carolina at Chapel Hill USA",
                        "normalized_name": "University of North Carolina at Chapel Hill",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0130frc33",
                            "GRID": "grid.10698.36"
                        }
                    }
                ]
            },
            {
                "given": " Robert",
                "family": "Allen",
                "affiliation": [
                    {
                        "original_name": "University of North Carolina at Chapel Hill USA",
                        "normalized_name": "University of North Carolina at Chapel Hill",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0130frc33",
                            "GRID": "grid.10698.36"
                        }
                    }
                ]
            },
            {
                "given": " Anne",
                "family": "Whisnant",
                "affiliation": [
                    {
                        "original_name": "University of North Carolina at Chapel Hill USA",
                        "normalized_name": "University of North Carolina at Chapel Hill",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0130frc33",
                            "GRID": "grid.10698.36"
                        }
                    }
                ]
            },
            {
                "given": " Kevin",
                "family": "Eckhardt",
                "affiliation": [
                    {
                        "original_name": "University of North Carolina at Chapel Hill USA",
                        "normalized_name": "University of North Carolina at Chapel Hill",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0130frc33",
                            "GRID": "grid.10698.36"
                        }
                    }
                ]
            },
            {
                "given": " Elise",
                "family": "Moore",
                "affiliation": [
                    {
                        "original_name": "University of North Carolina at Chapel Hill USA",
                        "normalized_name": "University of North Carolina at Chapel Hill",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0130frc33",
                            "GRID": "grid.10698.36"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-27",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Digitization is a sampling process The act of papyrological interpretation is a continuous thought process that unravels non-linearly (Youtie, 1963; Terras, 2006). Throughout this sense-making process, ancient and scarcely legible documents progress from the status of pure physical objects to that of meaningful historical artefacts. Within the e-Science and Ancient Documents project,Project website: http://esad.classics.ox.ac.uk/ we aim to make explicit some of the implicit mechanisms that contribute to the development of hypotheses of interpretation by designing and implementing a web-based software offering digital support for the hermeneutic task. This tool aims to record the intermediary hypotheses of interpretation, thus keeping track of the rationale and allowing easier and better revision when required. The model we have adopted (Roued Olsen et al., 2009) is that of a network of percepts, where a percept is defined as a minor interpretation that stems from perception and cognition (Tarte, 2010). An understanding of expert knowledge and of how it is mobilised is required to identify the crucial steps that allow us to reconstruct a rationale. The level of the granularity at which we choose to provide support also is essential to the usability of the software. Further, each percept, each intermediary interpretation, each piece of evidence used either to support or to invalidate a claim is potentially mutable. The implementation of an Interpretation Support System (ISS) taking these considerations into account poses the question of how to digitize or record a thought process; it is an epitome of the ‘continuous-to-discrete’ (or ‘analogue-to-digital’) problem. In the theoretical and life sciences, measurement devices are developed to sample the signals of interest. Then, based on the discrete sampled signal, on an underlying model of the behaviour of the signal, and on more general knowledge of signal processing and information theory (e.g. the Nyquist-Shannon sampling theorem), the continuous signal can be reconstructed with minimal deviation from the original signal. Similarly, the ambition of our ISS is, based on an appropriate model of the papyrological hermeneutic task, to allow the user to capture the information necessary to the reconstruction of the rationale that yielded a given interpretation. Two difficulties in sampling the interpretive thought process are: (1) to take advantage and to beware of the sense of scientific rigour that digitization conveys; and (2) to allow the digital expression of uncertainty and mutability of percepts. In this paper, we explain how, while attempting to digitize the papyrological interpretation act, we strive to avoid spurious exactitude and accommodate genuine uncertainty.    Choosing what to digitize and how The papyrological model of reading developed by Terras (Terras, 2006) identified ten levels of reading, corresponding to ten levels of granularity at which an interpretation in progress is discussed. Tools stemming from web-based technology (Bowman et al., 2010), image processing (Tarte et al., 2009) and artificial intelligence can help support the digitization of the hermeneutic task (see fig. 1). To illustrate how we negotiate between spurious exactitude and genuine uncertainty, we focus here on two specific stages of the digitization of the papyrological interpretation process: how artefact digitization is being performed; and how, by identifying the mechanisms that trigger the jumps between the ten levels of reading, we propose to address the representation of uncertainty.  Figure 1: Model of the act of interpretation detailing the various levels of reading based on (Terras, 2006) and the tools involved in the implementation of our ISS.  Note the deliberate choice of representation of the ten levels of reading as star-shaped around a stylus tablet to convey the recursive nature of the hermeneutic process. Although the levels of reading are defined according to the granularity of the subject discussed, oscillations between levels of reading constantly occur, and these oscillations do not necessarily take place between adjacent levels of reading.   Digitizing the text-bearing artefact The problem of spurious exactitude is most prevalent at the stage where the text-bearing artefact is digitized. For stylus tablets, for example, high-resolution pictures are not enough. The materiality of the artefact needs to be taken into account in a similar way as the experts exploit it in the real-world. The guiding principle we choose to follow in this context is mimesis. Indeed, when the papyrologists have physical access to such an incised tablet, in order to see better the incised text, they lay the tablet flat on their hand, lift it at eye level and expose it to raking light while applying pitch-and-yaw motions to it. This signal enhancement strategy exploits the shadow-stereo principle by which stronger and more mobile shadows and highlights occur at the incisions than they do on the bare wood; the text is thereby accentuated. Digitally imitating this process, we capture series of images of incised tablets with varying light positions (Brady et al., 2005), allowing users to reproduce digitally the visual phenomenon they naturally exploit in the real-world. Note that, similarly to signal measurement devices, we adopt a digitization process that is already part of the interpretation process. And the intention behind artefact digitization, as well as the intention behind signal measurement, is always an implicitly set variable that affects downstream results.   Digitizing the thought process When attempting to capture the milestones of the thought process that builds an interpretation of an ancient or damaged text, we need to capture the uncertainty of the intermediary percepts and their mutability. A numerical approach to uncertainty such as bayesian networks could have been adopted, but such a quantification of uncertainty usually presupposes that problems are complete, i.e. that all the alternatives to a given situation are known (Parsons & Hunter, 1998). Instead, we have decided to turn to argumentation theory (Parsons & Hunter, 1998) and theory of justification (Haack, 1993), and combine them to provide a formal, yet invisible, epistemological framework that will allow us to point out inconsistencies without forbidding them. Indeed, inconsistencies in an unravelling interpretation naturally occur and can be rooted either in the implicit expectations of the user or in the validity of the actual claims (see Tarte, 2010 for an account of an inconsistency due to an implicit assumption and its resolution). The balance to be found here (Shipman III & Marshall, 1999) is between on the one hand the usefulness of a formal system as a backbone to support reasoning under uncertainty and make implicit mechanisms explicit, and on the other the excess of formalism and explicit formulation that can become a hindrance by creating for the user an overhead disruptive to the interpretation process. Here again, our design choice is based on the observation of the experts at work. We allow both palaeographical and philological approaches in combination, through the possibility of tracing the letters and of handling the text as a crossword puzzle (Roued-Cunliffe, 2010); these are both approaches that we have identified as the main strategies experts develop when interpreting documents (Tarte, 2010). The expression of uncertainty is then made inherent to the mode of interaction with the artefact, and the transposition of the real-world tasks of drawing and crossword puzzle solving allows us to keep the interface intuitive while, in the background, more formal mechanisms can run routines such as consistency checks and consultation and constitution of knowledge bases. In her doctoral work, Roued-Cunliffe (Roued-Cunliffe, 2010) is currently concentrating on the crossword puzzle approach and combining it to consultation of knowledge bases through web-services.    Conclusion: digitization is also interpetation Digital technologies can easily trick the mind into thinking that their use confers an exactitude on the results obtained with their support. It is however worth noting that in the sciences too, digitization is always made with an intention. When looking to sample a continuous signal, be it a temperature as a function of time, or a thought process as a function of time, the sampling strategy is always adopted in the light of an intention. Digitization is actually also an act of interpretation. To record digitally the continuous papyrological interpretation process, we have to identify clearly our final aim, and to adapt our sampling strategy accordingly. Here, our aim is to enable to record, reconstruct, back-track if necessary, the interpretation process by making explicit (some of) the epistemological evidence substantiating the interpretation in progress; an added benefit to the software is that it will also enable easier production of an edition of a text, as the evidence will have been laid out clearly. Capturing uncertainty is vital to the recording process, and being conscious that its very capture is also part of the hermeneutic task is crucial to allow the software design to take on board the elements that are core to the whole interpretation process. AcknowledgementsThis work was supported by the joint AHRC-EPSRC-JISC Arts and Humanities e-Science Initiative UK [grant AH/E00654X/1]. The author wishes to thank Prof. Alan Bowman, Prof. Sir Michael Brady, Dr. Roger Tomlin, Dr. Melissa Terras, Dr. Charles Crowther and Henriette Roued-Cunliffe for their support, as well as the reviewers for their helpful comments. ",
        "article_title": "Digitizing the Act of Papyrological Interpretation: Negotiating Spurious Exactitude and Genuine Uncertainty",
        "authors": [
            {
                "given": " Ségolène M.",
                "family": "Tarte",
                "affiliation": [
                    {
                        "original_name": "University of Oxford, UK",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-20",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Digital libraries are a key technology for hosting large-scale collections of electronic literature. Since the first digital library (DL) systems in the early 1990s, the sophistication of DL software has continually developed. Today, systems such as DSpace and Greenstone are in use by institutions both large and small, providing thousands of collections of online material. However, there are limitations even to “state-of-the-art” DLs when considering digital humanities. Contemporaneously with the growth of DL technology, digital scholarly editions of significant texts and archival material have emerged. In contrast to digital libraries, where there are a number of readily available generic software systems, critical editions are largely reliant on bespoke systems, which emphasise internal connections between multiple versions. Whilst useful editions are online, and are increasingly used in scholarly endeavour, digital scholarly editions suffer from “siloing”: each work becoming an island in the ocean of the web. Like ‘physical’ libraries, digital libraries provide consistent support for discovering, reading and conserving documents in large collections. For scholarly editions, these features present a potential solution to “siloing”. Without trusted digital repositories, preservation and maintenance are endemic problems, and providing consistent experiences and unified workspaces across many sites (i.e. individual texts) is proving highly challenging. However, current DL systems lack critical features: they have too simple a model of documents, and lack scholarly apparatus. Digital library systems can readily contain electronic forms of “traditional” critical editions in static forms where each work is a separate and indivisible document. However, search facilities cannot then reliably distinguish between commentary and the primary content. Using XML formats such as TEI can permit this distinction to be made, but only with extensive configuration. Furthermore, the reading experience in such a DL is likely to fall far below scholars’ requirements of digital editions. European initiatives, such as DARIAH, focus on facilitating access to existing scholarly editions, with longer-term aims of fostering standards and interoperability of data. This approach presumes that each existing site (and hence, typically, edition) remains autonomous, and remains a discrete entity, which is then aggregated through a centralised service. It also admits the absence of standardised, highly functional storage and publication systems. Furthermore, this approach has been attempted in “federated” DLs, with only limited success. In federated DLs, unless every member uses the same software configured in the same manner, the appearance of each library differs and – worse – preservation remains in the hands of individual sites, and cross-site services (e.g. search) can only operate at a very rudimentary level. There have been projects to develop generic scholarly edition software, but success has been limited. Shillingsburg [Buchanan 2006] highlights a number of such systems up to the mid-2000s. Few of these initiatives engaged with computer science, and the software systems have proved hard to maintain. Digital library systems provide a potential route for providing collections of digital scholarly editions. However, they are not yet an answer. When a digital edition supports discussion between scholars, grounded on and linked to the text, the standard DL infrastructure requires extensive modification. Data structures are required to capture and store scholarly discourse, relate each item of discourse in detail to part of a complex document structure, and provide this through a seamless and consistent user interface. Multiple structures and complex document relationships fit uneasily within current DL software [Buchanan et al. 2007,Rimmer et al. 2008]. For instance, most DL software requires or assumes that any collection of documents is homogenous in terms of the interior structure of each document. This simply cannot be true of a collection including – say – diaries, journals, letters and novels. We need software that provides DL collection support with the ability to provide for complex document structure.  Current Work The goal of our research is to develop software that transcends the current limitations of DL systems in supporting digital scholarly editions for the humanities. Our intention is that in turn organisations and publishers who seek to provide series of critical material can build upon software that is scalable, systematically engineered and sustainable. This software will also support the necessary complexity of critical editions and possess a rich apparatus to support contemporary digital practices, not simply digitised forms of practice from the print era. Whilst no single system is likely to provide all the requirements of all possible circumstances, our aim is to create software that can provide the technical core of any collection of critical editions, with a minimum of effort. Adapting the system for a specific need may require extensive work, but only for more unusual circumstances. This would bring us to a point comparable to the support that current DLs give for simpler texts and scholarly practices. For users of scholarly editions – i.e. the research community – the presence of a common infrastructure and the increased ease of working across sites will very likely increase research activity across multiple ‘editions’.   Context and Motivation This project has identified Wales as presenting an interesting case study. It is a distinct cultural entity with an abundance of valuable written cultural material and a sizable scholarly community researching the cultural life and output of the nation. Reflecting the bilingual linguistic identity of the nation, there are extensive archives and printed matter in national, university, local government and private hands in both Welsh and English (as well as other languages). Wales suffers from a poor physical infrastructure, and this has motivated the provision of digital access to cultural material, from the early days of the National Library of Wales’s digitisation projects (e.g. ‘Campaign’ 1999) to the present. While the National Library of Wales has done outstanding pioneering work in digitisation of its collections and remains an asset in our selection of Wales as a ‘case study’, their remit does not extend to the interpretation of their collections. Despite considerable demand from scholars in Wales and beyond for digital critical editions of Welsh material (in both languages) no one project has access to the technical expertise to create software that embodies the requirements of the scholarly community. The motivation of our project is to build a common infrastructure that both enables each project to produce high-quality scholarly work, and provides for consistent access and preservation of that work.   Understanding User Needs To undertake this work requires not only technical expertise, but also a systematic study of the requirements of scholarly practice in the digital age. To date, we have reviewed the existing literature, and gained an initial set of requirements from a retrospective analysis of data from the recent User Centred Interactive Search (UCIS) project at University College London [Rimmer et al. 2008]. The UCIS project revealed that many technical difficulties emerged when configuring DL systems, even with relatively simple digital humanities material. Humanists do not necessarily search for material that directly corresponds to the “book” or “document” level of a particular library. Items may be sought that constitute part of a single document (e.g. a poem in a collection of poetry), and conversely larger works may be realised in several separate “documents”. Search and browse facilities typically work only at one level, typically consonant with either a book or article. However, collections are frequently heterogeneous and multi-layered. In the case of critical editions, the complexity of document structure and users’ tasks is even greater. A second problem is that humanists often require different variants of one work. Though library infrastructures can relate these together, using standard features alone is insufficient [Shillingsburg 2006]. Even the more developed features that of a few DL systems are simplistic when compared to the complex relationship between different renditions and editions of a work that critical scholarship requires. Current methods relate entire separate items together – e.g. a chapter to a chapter – but scholarly criticism and annotation do not neatly conform to the clean structural boundaries favoured in computer or library science. Thirdly, whilst some specific digital library installations do permit individual works to be linked to their author, or even specific words in a text to related material, this is not a standard part of DL software, and in contrast to the advanced facilities available in the best hypertext systems, the current technologies are primitive [Goose et al. 2000]. These shortcomings represent only a few of the problems already identified, and while we have developed partial solutions to parts of these, our technologies are not yet comprehensive, and other challenges have yet to be answered at all.   Summary This presentation will articulate the shortcomings and problems raised when collections of critical materials are hosted through current digital library systems, and the contrasting “siloing” problems faced in the field of digital critical editions. We will demonstrate the requirements that will have to be matched to provide a single software system that delivers the needs of critical editions whilst also providing methods to develop collections of critical works. We illustrate how some of these requirements can be met, and prioritise and elucidate the remaining challenges in creating a unified system.  ",
        "article_title": "Digital Libraries of Scholarly Editions",
        "authors": [
            {
                "given": " George",
                "family": "Buchanan",
                "affiliation": [
                    {
                        "original_name": "School of Informatics, City University London UK",
                        "normalized_name": "City, University of London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/04489at23",
                            "GRID": "grid.28577.3f"
                        }
                    }
                ]
            },
            {
                "given": " Kirsti",
                "family": "Bohata",
                "affiliation": [
                    {
                        "original_name": "Centre for Research into the English Literature and Language of Wales (CREW), Swansea University UK",
                        "normalized_name": "Swansea University",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/053fq8t95",
                            "GRID": "grid.4827.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-29",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  A common trope in discussions of scholarly editions in digital form is to praise, on the one hand, the extraordinary potential of electronic editions while, on the other hand, regretting that so few actual electronic editions come anywhere near realizing this potential (Robinson 2005). The potential is well-known: an explicit hyper-textual structure, publication in a distributed network environment, escape from the storage limit of the printed medium and possession of multiple layout possibilities (such as normalized and diplomatic transcriptions juxtaposed to facsimile images). The difficulties are also well-known: among them, the need for a formal, comprehensive and efficient encoding scheme to underpin scholarly editions in electronic form. The Text Encoding Initiative Guidelines provided a crucial element, by supplying namings, specifications and structure for key components of electronic editions: thus the specialized lower-level elements for manuscript description and critical apparatus, along with higher-level elements such as msDescription and facsimile. However, the TEI does not address two areas, crucial for the full encoding of scholarly editions in electronic form:  The naming of components of the editions: thus, of the works edited and their parts; the source manuscripts or print documents and their parts which carry the texts of the work edited; The relationships between the components: thus, between the documents, the texts they carry, and the works which those texts instance.   This paper reports on a scheme prepared by the authors, designed to provide a solution to the problems proposed in both areas. The provision of a shared epistemological framework for handling works, texts and text sources (cf. Buzetti 2009) will also facilitate the shift from stand alone publishing frameworks to shared distributed on-line environments, enabled by powerful and flexible underlying infrastructures,Such as the European initiative DARIAH <http://www.dariah.eu/> generally named Virtual Research Environments (Fraser 2005, Dunn et al. 2008). This framework will advance interoperability, long a problem area in electronic texts. Interoperability has been defined by IEEE as “The ability of two or more systems or components to exchange information and to use the information that has been exchanged”.<http://en.wikipedia.org/wiki/Interoperability> A recent briefing paper by Gradmann identifies four different levels of interoperability, one built on the top of the other. From the bottom these levels are technical/basic, syntactic, functional and semantic. While technologies such as TCP/IP, HTTP and XML already provide sound basis for interoperability at the lower levels, much work is still to be done at the top levels. The semantic frame for interoperability offered by this scheme speaks to this need. Semantic issues in networked publication systems are advanced by the work done in the last years on the ‘Semantic Web’ (Berners-Lee et al. 2001), which has recently evolved into the Linked Data initiative (Berners-Lee 2006). The Semantic Web seems to have survived its own hype, having finally entered the plateau of productivity phase, as happened for XML some years ago. The ontological level of the Semantic Web stack, represented by the OWL language, has presented a steep learning curve, due partly to its roots in Description Logic and First-Order Logic (Gruber 1993), but also presents at the same time the greatest potential. The relationship between textual scholarship in its electronic dimension and ontologies has not hitherto been much apparent, as textual scholars using digital methods have focussed rather on the related, but separated field of Library and Information Science (Vickery 1997). However, ontologies have much to offer the textual editing enterprise. Both ‘recensio’ and the construction of a stemmatic graph are implicit formalizations that would benefit from the adoption of an explicit modelling. Moreover, both Sperberg-McQueen and Peter Shillingsburg implicitly hints at the potentialities of an ontological approach in scholarly editions, the former when writing about the “infinite set of facts related to the work being edited” (Sperberg-McQueen 2002) and the latter about “electronic knowledge sites” (Shillingsburg 2006). In the world of digital humanities and electronic editions proficient uses of ontologies have already appeared, such as the Discovery<http://www.discovery-project.eu/> and the Nines<http://www.nines.org/> projects, also leveraging existing standards from related sectors such as IFLA’s FRBR<http://www.ifla.org/en/publications/functional-requirements-for-bibliographic-records> (Mimno 2005) or the cultural heritage oriented CIDOC-CRM<http://cidoc.ics.forth.gr/> (Ore et al. 2009). Substantial work is now being done on implementing an actual interchange and interoperability framework for electronic editions, and arbitrary portions of them, of the kind, in (for example) the COST Action Interedition.<http://www.interedition.eu/> A first proposal by Peter Robinson (Robinson 2009) was based on the Kahn/Wilensky Architecture (Kahn et al. 1995),Which constitutes also the basis for the Handle system <http://www.handle.net/> having therefore a naming authority together with a series of key/value pairs identifying portions of an electronic text, which therefore could be exchanged over the net thanks to a protocol such as the one established by the OAI-PMH standard.<http://www.openarchives.org/OAI/openarchivesprotocol.html> This addressed the first need stated above, for agreed conventions on naming. The second need, for formal expression of relationships, is addressed by the adoption of the Linked Data paradigm. While keeping the use of the Kahn/Wilensky Architecture for the labelling system, and using a URN-like syntax compatible with the Semantic Web requirements, an ontology representing the entities involved together with their relationships has been developed. The main entities of this ontology are:  ‘Work’: Canterbury Tales, and ‘WorkPart’, the first line of the Canterbury Tales; ‘Document’, the Hengwrt or the Ellesmere manuscripts, and ‘DocumentPart’, a page, folio or quire, which might carry an instance of the ‘Work’ ‘Text’: a single instance of a work, or work part, in a document or document part. Thus: the text of the work 'The Canterbury Tales' as it appears in the document, the Hengwrt manuscript;   The three-fold distinction between ‘Work’, ‘Document and ‘Text’ reflects the fundamental scholarly distinction between the ‘Work’, independent of its realization in any object; the ‘Document’ which might carry an instance of the ‘Work’; and the ‘Text’: the instance of the work in the document. Digital resources such as ‘Image’ or ‘Transcript’ are related to ‘Text’ and ‘Document’ and their parts, using relationship such a ‘hasImage’, ‘isTranscriptOf’, or ‘transcribedFrom’. Basic properties such as “isPartOf” or other properties from existing vocabularies, such as Dublin Core,<http://dublincore.org/> have also been used, so to guarantee compatibility with other schemes in the best possible way. The resulting RDF can be stored in a triplestore and made available on the web, so to allow further uses from third parties without the need to establish exclusive protocol verbs. This paper will present the methodological thinking behind the development of this ontology for the interchange of electronic editions of literary texts, starting from the first proposal until the more recent developments. The ontology will be contextualized with the existing related standards, particularly FRBR, CIDOC-CRM and the recent OAI-ORE<http://www.openarchives.org/ore/> (a gross-grained vocabulary for the reuse and exchange of digital objects developed by the Open Access Initiative) and with the similar initiative of the Canonical Text Service Protocol (CTS),<http://chs75.chs.harvard.edu/projects/diginc/techpub/cts> which recently also added an ontological dimension to its basic syntax (Romanello et al. 2009).  ",
        "article_title": "Works, Documents, Texts and Related Resources for Everyone",
        "authors": [
            {
                "given": " Peter",
                "family": "Robinson",
                "affiliation": [
                    {
                        "original_name": "Institute for Textual Scholarship, University of Birmingham UK",
                        "normalized_name": "University of Birmingham",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/03angcq70",
                            "GRID": "grid.6572.6"
                        }
                    }
                ]
            },
            {
                "given": " Federico",
                "family": "Meschini",
                "affiliation": [
                    {
                        "original_name": "Centre for Textual Studies, De Montfort University UK",
                        "normalized_name": "De Montfort University",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/0312pnr83",
                            "GRID": "grid.48815.30"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-05-04",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " During the last 10 years, XML has gained general acceptance as a data model in the Digital Humanities. Actually, it even leveraged the success of digital projects in the humanities. Meanwhile TEI is the unchallenged standard for all projects in the fields of literature studies, epigraphy, linguistics, history sciences and so on. Many thoughts were invested to bring TEI and other related formats like EpiDoc to a level that suffices general scholarly needs. At first sight, things went differently in the field of music encoding. Around the year 2000 a couple of XML-based encoding schemes for music notation emerged, and within just a few years MusicXML became the best-known and most widespread music encoding format. It was intended to serve as an interchange format between different music applications, and even today it is virtually indispensable for this very important task. At the same time, this orientation of MusicXML requires a certain \"simplicity\" that facilitates implementations in various applications. The Music Encoding Initiative (MEI) went a different way. Not aiming at application support in the first place, an encoding model for scholarly purposes was developed over years. Strongly influenced by the concepts of the TEI, Perry Roland as initiator of the format tried to transfer these concepts to the field of music encoding. For large parts, this is quite easy: music notation is a kind of text, and many unspecific modules of TEI can be reused for music encoding with only small changes. But then again, music notation itself offers a much higher complexity than other texts. It is multi-dimensional not only because of its layout of multiple vertically aligned staves, but also because of its simultaneity of harmonic and melodic progression. In music notation, the text itself consists of overlapping hierarchies and therefore demands a quite sophisticated data model. Most often, it is virtually impossible to preserve all possible meanings (or better: interpretations) of a musical text with reasonable effort. The reason is that the written text is only a part of the complete information. Every notation serves a certain purpose, and each composer or copyist uses only as many symbols as he needs to be explicit to his contemporaries. Besides this, the \"rules\" of music notation changed significantly over time, even though these developments often seem to be very subtle. All this leads to the problem that there is no absolutely fixed terminology in music notation. Some phenomena are still not completely understood or even defined, such as the problem of dots, strokes and hooks in scores from the classical period. The lack of a complete and well-defined terminology even for restricted repertoires makes the encoding of music notation on a scholarly level highly demanding, and, at the same time, the implementation and usage of such an encoding scheme is anything but trivial. The Music Encoding Initiative has chosen this way, and currently it stands on an important turning point: In a one-year project funded by the NEH and DFG the original model was revised and has proven to meet all essential scholarly requirements for such a format. In the next years, it needs to be disseminated in the fields of musicology, music information retrieval, music philology and digital humanities in general. A first step in this direction is the TEI's Special Interest Group on music encoding, whose members were actively involved in the recent developments on MEI, and who seek to find ways to bring MEI and TEI closer together. Due to the complexity of music notation – and thus music notation encoding too – application support for MEI is crucial to ensure its dissemination: Almost no traditional musicologist would be willing to work with a XML-editor like Oxygen. There are several projects currently working on such applications for MEI: The DiMusEd-Project, situated in Tübingen (Germany), uses SVG to render encodings of multiple sources of music notated with medieval neumes. Although this repertoire uses a limited set of symbols, this project already shows the benefits of a dynamic rendering from an encoding instead of engraved scores. The Edirom project, (Detmold, Germany) aims to establish workflows for digital scholarly editions of music. In the application for preparing such editions it is already using MEI to store all structural information about the musical text as well as the containing documents. For moving from basically facsimile-based editions to completely digital editions it is planned to offer complete encodings of all relevant sources including the rendering-facilities already demonstrated by the DiMusEd-project. In order to achieve this goal Edirom closely collaborates with the most ambitious of all ongoing MEI-related projects: TextGrid. A sub-project of this major German initiative, which is also located in Detmold, seeks to develop a limited scorewriter for MEI offering a graphical user interface for musicologists. In this case „limited“ means that the project neither intends to support MEI completely nor tries to keep up with the engraving quality of already existing scorewriters: the unambiguity of the output is more important than its beauty. All these German projects collaborate closely with the ongoing efforts in the US to further improve the format itself and to provide interchange to other relevant formats such as Humdrum and MusicXML. Depending on further funding by NEH and DFG respectively it is intended to provide reasonable collections of MEI encodings to facilitate further usage of the format. Although MEI will not find the wide acceptance MusicXML already has, all these components will help to disseminate MEI in the academic world, to promote interchange of high-quality data and to explore new methods for digital representations of written music. The talk will provide a short introduction to the current state of MEI – both the format itself and the projects and applications already working on and with it. ",
        "article_title": "A Data Model for Digital Musicology and its Current State – The Music Encoding Initiative",
        "authors": [
            {
                "given": " Johannes",
                "family": "Kepper",
                "affiliation": [
                    {
                        "original_name": "University of Paderborn Germany",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-27",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Overview This paper is a case study that is part of a larger Ph.D. dissertation project: an exploration of the networks of references and ideas that make up the social lives of books online. In a time of rapidly evolving ecologies of reading and writing, I argue that the Internet affords us massive amounts of new data on previously invisible cultural transactions. New architectures for reviewing, discussing and sharing books blur the lines separating readers, authors and critics, and these cultural structures capture thousands of conversations, mental connections and personal recommendations that previously went unrecorded. I call these webs of references, allusions and recommendations ideational networks. Using Toni Morrison’s career as a model, I will closely examine the ideational networks surrounding her work using the methodologies of social network analysis in order to define a new, statistically informed conception of cultural capital in the digital era.   Background My project is founded on the argument that as literary production evolves, new kinds of reading communities and collaborative cultural entities are emerging. Many of these communities are ephemeral and quite often they are fostered by commercial interests seeking to capitalize on their cultural production. Nevertheless, a handful of websites like Amazon continue to dominate the marketplace for books and attract millions of customer reviews, ratings and purchase decisions, and the literary ecologies of these book reviews have become valuable research resources. The ideational networks I explore are made up of first books, authors, characters and other literary entities (these are the nodes), and second the references linking them together as collocations in book reviews, suggestions from recommendation engines, and other architectures of connection. Advancing from my first case study, Thomas Pynchon, to Morrison’s ideational networks, I have discovered the utility of social network analysis methodologies to better analyze graphs of these literary references. With new data and new tools, I hope to trace the networks of influence and exchange that have contributed to making Toni Morrison arguably the most critically acclaimed and popularly successful author in the United States.   Proposal This paper will present my research on the ideational networks surrounding the works of Toni Morrison. Morrison makes an excellent subject for this kind of study for a number of reasons. As a Nobel laureate and widely read popular author, she has attracted millions of devotees. In her writing she often draws on the African American literary tradition of the talking book, and throughout her career she has explored the ontological power of narrative to create and destroy worlds. It is not surprising, then, that as an author she is deeply committed to expanding the act of reading not only to include millions of people, especially women, who never considered themselves readers before, but also to changing its definition to include conversation, community, and a kind of collaborative reflection. This element of Morrison’s authorial appeal is best exemplified by her long-running association with Oprah’s Book Club, an enterprise that has had a huge impact on the U.S. publishing industry and on conceptions of reading as a social act. My presentation will explore the communities of readership that have emerged around Morrison’s work and consider the literary company in which her readers and reviewers perceive her. Focusing on a limited set of professional book reviews, reader reviews and recommendations from a dataset of print media, Amazon and LibraryThing, I will map out connections that reviewers and consumers have made between Morrison’s works and other literary figures and texts (see Figure 1). I believe these connections will delineate Morrison’s position as an extremely popular author who nevertheless challenges her readers to grapple with unflinching, emotionally raw narratives. Her books have introduced millions to deeply troubled corners of American history, combining a modernist style with diverse literary traditions in a way that is both acutely culturally specific and universally compelling. I hypothesize that these factors have driven her remarkable ability to create togetherness and communities of readership even as she traces out the wounds and scars of division, inequality and bias latent in American culture. I also hope to contrast her ideational networks with those of Thomas Pynchon, who has pursued a radically different literary approach through his aversion to publicity and his recondite fiction.  Figure 1: Sample image from work in progress of a Morisson ideational network based on Amazon's recommendation engine. Here the nodes are books connected together by Amazon's \"Customers who bought this also bought\" feature, centered on Morisson's novels in the middle. Arrows indicate direction (i.e. Twain's Huckleberry Finn is recommended from O'Brien's The Things They Carried, but not vice versa). Note both the range of texts and the cultural vectors present, with syllabus classics like Salinger, Steinbeck, Miller and Howthorne moving down from the center, canonical Native American writers at the top left, etc. Visualization based on Prefuse Java Toolkit.     Methodology This argument will draw on results from several specific datasets of ideational networks.  I have collected professional book reviews from a set of major U.S. newspapers and magazines that consistently reviewed Morrison’s publications. These will be analyzed along with customer reviews from Amazon’s product pages for Morrison’s works, which have been accumulating reviews since 1996. Employing the MorphAdorner project’s Named Entity Recognition tool, I am assembling a dictionary of proper nouns that reviewers use as literary references in discussing Morrison’s work. Tagging those references in the reviews, I will then explore collocations of references to construct network graphs of the books, authors and other literary entities that reviewers link together. I have also assembled a database of book recommendations using Amazon’s “Customers who bought this also bought” engine and LibraryThing’s recommendation engine. These links provide a valuable counterpoint to those works that reviewers choose to mention, since these recommendations are generated by indirect user actions (i.e. when a user buys, reviews or catalogs multiple texts and thereby creates a statistical association among them). Recommendation engines attempt to mimic or track the sale and ownership of cultural products, creating a feedback loop of cultural consumer desire, while review analysis explores a more abstract realm of ideational exchange. Using methodologies of social network analysis, I will identify those works and authors with the most prestige (i.e. the books most frequently recommended) and centrality (i.e. the author who is best-connected to other authors) in these networks and consider the role of Morrison’s texts as centers of ideational networks and, potentially, as bridges between different genre or category groupings. I also hope to explore the role of clustering effects in these networks to see if they are based on predictable factors like genre. Depending on the speed of my progress with the objectives above and the cooperation of Oprah’s Book Club, I also hope to explore the networks of discussion and dialog that have emerged around Morrison’s long collaboration with Oprah Winfrey, which has inspired millions of people to take up or return to reading as a leisure activity in adult life. I hope to apply similar methodologies of literary reference to see how Book Club participants contextualized Morrison.   Conclusion As I continue to refine my understanding of ideational networks and improve the methodologies necessary to study them, I am beginning to develop techniques that can effectively be applied to very different authors and provide comparable data. This second case study will provide fertile ground for exploring Toni Morrison’s unique authorial fame and to map out the kinds of cultural production that large groups of committed readers can engage in online.   ",
        "article_title": "The Social Lives of Books: Mapping the Ideational Networks of Toni Morrison",
        "authors": [
            {
                "given": " Edward",
                "family": "Finn",
                "affiliation": [
                    {
                        "original_name": "Department of English, Stanford University, USA",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-20",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The world of character encoding in 2010 has changed significantly since TEI began in 1987, thanks to the development and adoption of Unicode (/ISO/IEC 10646) as the international character encoding standard for the electronic transmission of text. In December 2008, Unicode overtook all other encodings on the Web, surpassing ASCII and the Western European encodings (Davis 2009). As a result, Unicode’s position seems to be increasingly well-established, at least on the Web, and TEI was prescient to advocate its use. Over 100,000 characters are now defined with Unicode 5.2, including significant Latin additions for medievalists, a core set of Egyptian hieroglyphs, and characters for over 75 scripts. As such, Unicode presents a vast array of character choices for the digital humanist, so many that it can be difficult to figure out which character – if any – is the appropriate one to use. When working on a digital version of a Latin text that contains Roman numerals, should text encoder use U+2160 ROMAN NUMERAL ONE or U+0049 LATIN CAPITAL LETTER I? Should one use the duplicate ASCII characters that are located at U+FF01ff. (and why were they ever approved)? These types of questions can create confusion for text encoders.  The process of approving new characters by the Unicode Technical Committee and the relevant ISO committee is intended to be open, meaning that scholars, representatives of companies and national bodies, and other individuals may make proposals and, to a certain extent, participate in the process. Yet which characters get approved – and which don’t – can still be baffling. On the TEI-list, one member wrote on 1 August 2009: What is and isn't represented in unicode is largely a haphazard mishmash of bias, accident and brute-force normalisation. Unicode would be dreadful, if it weren't for the fact that all the alternatives are much worse. This paper addresses the question of which characters get approved and which don’t, by examining the forces at work behind the scenes, issues about which digital humanists may not be aware. This talk, by a member of the two standards committees on coded character sets, is meant to give greater insight into character encoding today, so that the character encoding standard doesn’t seem like a confusing set of decisions handed down from a faceless group of non-scholars. Specific examples of the issues will be given and the talk will end with suggestions so that digital humanists, armed with such information, will feel more confident in putting forward proposals for needed, eligible characters. The Unicode Technical Committee (UTC) is one of the two standards committees that must approve all new characters. Since it is composed primarily of industry representatives, technical discussion often predominates at meetings, including the question of whether given characters (or scripts) can be supported in current font technology and in software. For the academic, the question of whether a given character (or script) can be implemented in current fonts/software is not one commonly considered, and wouldn’t necessarily be known, unless they attended the UTC meetings in person. Also, the acceptance of characters can be based on current Unicode policy or precedence of earlier encoding decisions, which again is often not known to outsiders. How to handle historical ligatures, for example, has been discussed and debated within UTC meetings, but since the public minutes of the UTC do not include the discussion surrounding a character proposal, it may appear that the UTC is blind to scholars’ concerns, which is frequently not the case. In order to have a good chance at getting a proposal approved in the UTC, it is hence important for scholars to work with a current member of the UTC who can trouble-shoot proposals and act as an advocate in the meetings, as well as explain concerns of the committee and the reasoning behind their decisions.  The ISO/IEC JTC1/SC2 Working Group 2, a working group on coded character sets, is the second group that must approve characters. This group is composed of national standards body representatives. Unlike the UTC, the WG2 is not primarily technical in nature, as it is a forum where national standards bodies can weigh in on character encoding decisions. This group is more of a “United Nations” of character encoding, with politics playing a role. Discussion can, for example, involve the names of characters and scripts, which can vary by language and country, thus causing disagreement among member bodies. Like the other International Organization for Standardization groups, decisions are primarily done by consensus (International Organization for Standardization, \"My ISO Job: Guidance for delegates and experts\", 2009). This means that within WG2, disagreements amongst members can stall a particular character or script proposal from being approved. For example, a proposal for an early script used in Hungary is currently held up in WG2, primarily because there is disagreement between the representatives from Austria (and Ireland) and the representative from Hungary over the name. To the scholar, accommodating national standards body positions when making encoding decisions may seem like unnecessary interference from the political realm. Still, diplomatic concerns need to be taken into account in order for consensus to be reached so proposals can be approved. Again, having the support of one’s national body is a key to successfully getting a character proposal approved.  Since WG2 is a volunteer standards organization within ISO, it relies on its members to review proposals carefully, and submit feedback. Unfortunately, many scholars don’t participate in ISO, partly because it involves understanding the international standard development process, as well as a long-term commitment – the entire approval process can take at least two years. Another factor that may explain the lack of regular academic involvement is that scholars participating in standards work typically do not receive professional credit. Because there is not much expert involvement in WG2 to review documents (perhaps even fewer experts than in the UTC), errors can creep in. For many of the big historic East Asian script proposals, for example, only a small handful of people are reviewing the documents, which is worrisome. The recently addition of CJK characters (“Extension C”), which has 4,149 characters, could have benefited from more scholarly review. Clearly there remains a critical need for specialists to become involved in the ISO Working Group 2, so as to prevent the inclusion of errors in future versions of the character encoding standard.  Besides the activity within each separate standards group, there are developments affecting both standards groups that may not be known to digital humanists, but which influence character encoding. New rules have recently been proposed within ISO, for example, which will slow the pace at which new characters and scripts are approved by ISO and published in The Unicode Standard (ISO/IEC JTC1/SC2 meeting, 2009). The new rules will severely impact new character requests. Another example of activity affecting digital projects, particularly those using East Asian characters, was the announcement in October 2009 by the Japanese National Body that it has withdrawn its request for 2,621 rare ideographs (“gaiji” characters) (Japan [National Body], “Follow-up on N3530 (Compatibility Ideographs for Government Use)”, 2009), instead opting to register them in the Ideographic Variation Database, a Unicode Consortium-hosted registry of variation sequences that contain unified ideographs (Unicode Consortium, \"Ideographic Variation Databse\", 2009). The use of variation selectors is a different approach than that advocated in the TEI P5 for “gaiji” characters (TEI P5 Guidelines: \"5. Representation of Non-standard Characters and Glyphs\"), but is one that should be mentioned in future TEI Guidelines as an alternative.Variation selectors have also been mentioned as being used to handle variants in other scripts, such as the historic script Tangut. In order to keep apprised of developments within the standards groups, a liaison between TEI and the Unicode Consortium (and/or ISO/IEC JTC1/SC2) would be advisable, as the activities of Unicode (/ISO) can influence TEI recommendations. In sum, the process of character encoding is one that ultimately involves people making decisions. Being aware of the interests and backgrounds of each standard group and their members can help explain what appears to be a spotty set of characters in Unicode. Keeping up-to-date on developments within the committees can also provide insight into why a particular character is approved or not, or why its progression has been slowed. The talk will conclude with suggestions on how digital humanists can participate more actively and effectively in the standards process.  ",
        "article_title": "Character Encoding and Digital Humanities in 2010 – An Insider's View",
        "authors": [
            {
                "given": " Deborah",
                "family": "Anderson",
                "affiliation": [
                    {
                        "original_name": "UC Berkeley, University of California, Berkeley USA",
                        "normalized_name": "California Coast University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05t99sp05",
                            "GRID": "grid.468726.9"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-14",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Despite our differing research methodologies, subjects, and motives, the one thing scholars across the disciplines and around the world might agree upon is the significance of peer review. Peer review may be the sine qua non of academic work; we use it in almost everything we do, including grant and fellowship applications, hiring and promotion processes, and, of course, in vetting scholarly work for publication.  We all operate under the agreement that peer review is a good thing, by and large, both a means of helping scholars  improve their work and a system for filtering that work for the benefit of other scholars. However, as I argue in Planned Obsolescence, the means by which we conduct peer review demand careful reconsideration and revision as academic publishing moves increasingly online. Clay Shirky has argued that the structures of the internet demand a “publish, then filter” process, encouraging the open communication of all of the results of scholarly investigation, followed by a process that filters those results for quality (Shirky 2008). I explore the reasons such a transformation is desirable at length in Planned Obsolescence, primarily that it makes little sense to replicate a mode of review designed for print’s economics of scarcity within the internet’s economics of abundance (see Jensen 2007); if what is scarce in the age of the network is not the means of production but the time and attention available for consumption, the best use of peer review would be to help researchers find the right text, of the right authority, at the right time. A born-digital system of review would work with rather than against the strengths and values of the network by privileging the open over the closed, and by understanding the results of peer review as a form of metadata enabling scholars to find and engage with research in their fields. How to build and implement such a system, however, remains in question: how do we devise a networked review system that is open, honest, and thorough, that draws the best from the “wisdom of the crowds” (Surowiecki 2004; Anderson 2006) while upholding the standards that review is meant to serve? Several examples of online review processes already exist; within humanities journal publishing, the most significant may be that of electronic book review; articles submitted there are posted in a password-protected review space, where registered users of the site can read them, leave glosses, and recommend acceptance or rejection. However, though the editors use my term “peer-to-peer review” in describing their system, it falls a bit short of the truly open system I imagine; the review system is still kept behind the scenes, and while the reviews are crowd-sourced, the reviewers producing them aren’t asked to take responsibility for their opinions by expressing and defending them in public. This aspect of peer-to-peer review is key; just as the quality of the algorithm determines the quality of a computational filtering system, the quality of the reviewers will  determine the quality of a human filtering system. Online peer review must made open and public not just as a means of increasing communication but as a means of increasing reviewer accountability, providing for the ongoing review not just of texts but of reviewers. In order to experiment with the possibilities for an open review system, and with the consent of NYU Press and my editor, Eric Zinner, I placed the entirety of Planned Obsolescence online in late September 2009. The text was published in CommentPress, a WordPress plugin developed by the Institute for the Future of the Book, which enables the discussion of texts at a range of levels of granularity, from the paragraph to the page to the document as a whole. At the same time, NYU Press sent the project out for traditional peer review. Such experiments have been conducted before; in 2008, Noah Wardrip-Fruin published a draft of Expressive Processing through Grand Text Auto, while MIT Press sent it to outside readers.  Noah, however, wasn’t seeking to create a head-to-head contest between closed and open review; he was motivated by the desire for feedback from a community he trusted (see Wardrip-Fruin 2009b). My motives were a bit more complex; I wanted that same community-based feedback, but I also wanted to test open review against more traditional reviews, to gauge differences in the kind and quality of responses produced within an online system, and to project the kinds of changes to CommentPress that might help transform the plugin into a viable mechanism for peer-to-peer review. In slightly less than six months, Planned Obsolescence received 205 comments from 39 different readers (not counting my own 78 responses).  These comments are by and large excellent, and have been extremely helpful in thinking about the revision of the manuscript. Most of the comments, however, are locally oriented; CommentPress’s paragraph-level commenting strategy encourages a close attention to the particulars of a text rather than a more holistic discussion.  This focus on the text’s details in the comments wasn’t unexpected; we anticipated that the traditional reviews, being written after the entire manuscript had been read, would tend to focus a bit more on the big picture than would comments made in medias res. This assumption did largely bear out; the offline reviewers tended more toward an assessment of the overall argument. Our first tentative conclusion, then, was that a functional open review system would require clearer ways for online reviewers to leave broader comments. An update to the CommentPress plugin, released a couple of months into our experiment, helped provide that functionality by highlighting the “community blog” section of the site, which in theory would allow members of a community of readers to engage one another in discussion of their reviews and of the project as a whole.  In actual practice, however, that engagement did not occur, though it remains unclear whether this is due to the blog’s belated introduction or some other issue. Additionally, however, Zinner asked the offline reviewers whether they would be willing to participate in our process, allowing us to post their reviews for discussion and response; one, Lisa Spiro, agreed. Spiro’s willingness to participate, and the generosity of her review, revealed the importance of the social commitments involved in the peer review process. Those scholars who have long undertaken the often thankless work of peer review have largely done so out of a commitment to the advancement of knowledge in the field. But fostering participation in online discussion requires not just intellectual interest on the part of individuals but also a solid, committed social network. Reviewers participating in an open process must have a stake in that process beyond that of the disinterested reader; they must understand the text and its author to be part of a community in which they are invested and to which they are accountable. Beginning in March 2010, MediaCommons will conduct another open review experiment, publishing a small group of papers being considered for a special issue of Shakespeare Quarterly. Through this experiment, we hope to explore a number of variables:  the relative weights of commitment to subject matter and commitment to digital methodologies in determining participation in open review, the level of engagement in the review of article-length (as opposed to book-length) texts online, and the structures of participation in the review of work by multiple authors in one venue.  Both experiments involve the review of comparatively traditional forms of scholarship, the book and the journal article, which we have opted to begin with for two reasons: first, that transforming the processes of reviewing these forms of scholarship presents the broadest potential impact on academic publishing as it exists today, and second, that it confines the question under consideration to mode of review, rather than expanding into criteria for review. That last is extremely important; many, if not most, scholars working in new forms of multimodal scholarship have encountered the sense that the academy in general does not know how to review such work. We hope to experiment in the future with models for review of new forms of scholarship. This paper will, in the end, argue that a truly effective peer-to-peer review system will need to place its emphasis not just on developing the technological network but on developing the social network; it must be focused around clusters of scholars who are already in dialogue with one another. It must also be accompanied by a shift in values that encourages scholars to understand the business of reviewing as being a commitment not just to the advancement of intellectual thought but to the structure of that community and its dialogue. And, as participation in such review requires significant time and energy from a larger number of scholars than traditional review, the academy must recognize the importance of reviewing, acknowledging the significant labor involved, creating structures through which reviewers can receive “credit” for their work.  MediaCommons hopes to foster these developments in a genuinely peer-to-peer review process in the coming months by adding functionality allowing readers to rate and respond to the comments left by others, as well as by building links between these reviews and our social network (see Fitzpatrick 2009b), making a scholar’s reviews visible as a part of their portfolio of scholarly work. Together, we hope, these links will allow peer review to become an open, social process, one that will transform online peer review into a mechanism for collaborative post-publication filtering, helping authors to improve their work and enabling researchers to find and assess the authority of the texts they need, by working with rather than against network-based interactions.  ",
        "article_title": "Open vs. Closed: Changing the Culture of Peer Review",
        "authors": [
            {
                "given": " Kathleen",
                "family": "Fitzpatrick",
                "affiliation": [
                    {
                        "original_name": "Pomona College, USA",
                        "normalized_name": "Pomona College",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0074grg94",
                            "GRID": "grid.262007.1"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-15",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This paper will report on the first year of a 3-year grant funded by the Fonds québecois de la recherche sur la société et la culture led by playwright Patrick Leroux (overseeing the creative component) and Michael Eberle-Sinatra (overseeing the academic component). The specific nature of this group project is nestled in the promising dialogue to be established between Romantic literature scholars, a theatre practitioner, and a scholar preoccupied with the pedagogy of Romantic drama using hypermedia as a template and an engaging interface. When artists teach, they never quite relinquish their initial creative impulse. Historical works, while being taught for their intrinsic value and larger pertinence within a literary context, nevertheless solicit a resonant response. Classroom exercises in both academic and creative courses suggest that many students engage in a similar empathic manner when allowed to prod, question, and interact actively with a studied text. The “creative” component of this research-creation project with strong pedagogical intent is precisely linked to an artistic response to the source text, Joanna Baillie’s Witchcraft. In addition to the edited text, its scholarly annotations and commentary, and the filmed Finborough production of the play, we will create workshop situations with actors and students in which the play will be explored in rehearsal prompting us to investigate other manners of staging the work and illustrating, through filmed documentation, the process of reading a text for performance. Short video presentations of key creative and interpretive issues will be edited for inclusion in the hypermedia presentation. The actual process, whether filmed or not, will allow the actors and creative faculty to fully immerse themselves in Baillie’s world and work in order to fuel their resonant responses to them. This second creative component, the resonant response, will take the form of short theatrical pieces conceived for film. The nuance is essential as the pieces will not be short cinematic films but rather short-filmed theatrical pieces. The emphasis on speech, dramatic action, and relationship to a defined theatrical space will differentiate these pieces from more intimate, image-based cinematic pieces. The resonant responses could be as short as two minutes or as long as ten minutes, in order to fully explore very precise formal issues (a character’s speech, the subtext in a given dialogue, what we couldn’t stage during the 19th Century but feel we could now). These creative pieces will be developed with Theatre, Creative Writing, and English literature students and faculty. Existing TEI guidelines for scholarly encoding do not account for the unique relationship between a play script and performance practice and history. Scholarly encoding typically views the structures of texts in relation to the protocols that guide how readers interpret documents. But dramatic scripts require different kinds of reading and, thus, different kinds of encoding. Performance-informed inquiry into play texts depends on a reader’s ability to think about the range of possibilities—both historically distant and contemporary—for theatricalization of a line of dialogue, a bit of physical action, or a visual space. Additional historical materials on the theatre and culture of Baillie’s era will be provided by team members. For our hypermedia resource to organize multi-media materials in ways that will help students in literature classes to use the hypermedia edition of the play, we will need to develop innovative customizations of TEI encoding guidelines. Discovering how best to support a student reader’s work with a historically unfamiliar dramatic work provides an important test case for existing guidelines for XML encoding of drama. This project will take an innovative approach in several senses. It will use hypermedia to try to solve a classroom problem created by plays with little performance history or connection to familiar theatrical styles. It will also test the limitations of the TEI scholarly encoding guidelines by exploring how, in the case of play scripts, building hypermedia resources requires creative, user-oriented strategies of encoding. The research-creation program will illustrate how contemporary artists can engage with historical works, while shedding light onto the theatrical creative process. Finally, our Resonant Response to Joanna Baillie’s Drama will combine scholarly research on Romantic drama, practice-driven analysis, the creation of new work, and hypermedia expertise. This particular research-creation program is singular and innovative in its combination of academic close reading, dramaturgical analysis, dramatic writing and theatrical performance, filmed theatre, and a resolutely pedagogical preoccupation with a full and thorough exploration of the possibilities of hypermedia edition. In addition to creating a prototype hypermedia edition, the project seeks to find out:  what value performance annotations can add to a teacher’s work with students on a seldom-performed play; how the Text Encoding Initiative’s (TEI) scholarly encoding guidelines can best be customized to design hypermedia play editions; how the process of collaboration among faculty and students in humanities and communications disciplines can enrich understanding of technology’s interaction with interpretation.    ",
        "article_title": "Joanna Baillie’s Witchcraft: from Hypermedia Edition to Resonant Responses",
        "authors": [
            {
                "given": " Michael",
                "family": "Eberle-Sinatra",
                "affiliation": [
                    {
                        "original_name": "Université de Montréal, Canada",
                        "normalized_name": "University of Montreal",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0161xgx34",
                            "GRID": "grid.14848.31"
                        }
                    }
                ]
            },
            {
                "given": " Tom C.",
                "family": "Crochunis",
                "affiliation": [
                    {
                        "original_name": "Shippensburg University, USA",
                        "normalized_name": "Shippensburg University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/048arep70",
                            "GRID": "grid.263520.0"
                        }
                    }
                ]
            },
            {
                "given": " Jon",
                "family": "Sachs",
                "affiliation": [
                    {
                        "original_name": "Concordia University, Canada",
                        "normalized_name": "Concordia University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01qxhf360",
                            "GRID": "grid.448967.0"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-30",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper is situated within the debate between “specialist” and “generalist” methods of analysis in the study of world literature. It is argued that a systemic linguistic discourse analysis of appropriately encoded text passages can provide a methodology which can be utilised to interrogate the national and international demarcations of comparative literary analysis. A case study consisting of a textual analysis of the dialogical relationship between patient and therapist in a “factional”, i.e. works of fiction, which draw upon historical fact. Irish and English novel is provided. The benefits of the results yielded to current understandings of national literature and definitions of world literature are discussed.  In 1827, the German poet, Johann Wolfgang von Goethe, declared to his young disciple Johann Peter Eckermann that “National literature is now a rather unmeaning term; the epoch of world literature is at hand, and everyone must strive to hasten its approach” (Eckermann, cited in [Damrosch, 2003b, p. 1]). History informs us however that Goethe was premature in his heralding of a new age of “postnational” literature, as up until recently all literatures tended to have been studied along national lines [Dimock, 2006, pp. 2-5]. Yet in more recent decades, nations and, by extension, “national” literatures have become increasingly under threat in their sovereignty over all elements of human life due to the homogenising and heterogenising effects of globalisation. Globalisation is defined by Malcolm Waters as being “a social process in which the constraints of geography on social and cultural arrangements recede and in which people become increasingly aware that they are receding” [Waters, 1995, p. 3]. It is not surprising therefore, that in an age where national boundaries, both physical and mental, are become increasingly insignificant and blurred, we find a renewed interest in Goethe’s concept of Weltliteratur. As with all things new, the emerging discipline of world literature has evoked fear and reservations among literary scholars. According to David Damrosch, the possibility of recognizing the ongoing, vital presence of the national within the life of world literature poses enormous problems for the study of world literature [Damrosch, 2003a, p. 514]. Thus the field tends to be divided into “specialists”, those who are concerned with national literatures, and “generalists”, those who are interested in studying global patterns. But instead of this either/or method, Damrosch maintains that what is need is a method that can mediate between broad and often reductive overviews and intensive, but often atomistic close readings [p. 519]. As Franco Moretti argues, “we must find a way to combine the individual who reads a single work with great collective efforts and vision” [Sutherland, 2006, Monday 9 January, 2006]. This paper argues that a combination of Systemic Functional Linguistics and Digital Humanities offers one way whereby this may be achieved. The complexities involved in the interpreting of “literary language” for electronic media have perhaps posited the greatest deterrent for literary scholars in embracing digital humanities to date. The pioneering work of scholars such as Willard McCarty and Jerome McGann (among others) unfortunately remain the exception among their peers in the field of literary studies. The majority within the discipline retain the fear that computer based analysis of texts can only reveal “broad sweeping patterns” within literary works. Interestingly, this fear echoes the reservations that are held about the theoretical methods deployed by “generalists” in the study of world literature. Contrary to both these fears, this paper will argue that computer-based literary research can be utilized to provide both a means of analysis for the specificities of national literatures, while also serving as a tool for carrying out comparative textual analysis at an international scale. We wish to present to the following methodology to the community.  Case Study: The Patient-Therapist Relationship in Literature This project will consist of an analysis of a passage of dialogue between patient and therapist in Sebastian Barry’s The Secret Scripture (2008) and Pat Barker’s Regeneration (1995). For the purpose of this study, we have chosen a novel by an Irish writer and an English writer respectively. Given that this methodology is in its infancy, it is presumed best to begin with two texts written in the same language and which originate from countries of similar cultural systems. The Secret Scripture is set in present day Ireland but the narrative is made up of a double narrative; the personal recollections of Roseanne Clear, who was incarcerated in a mental institution during the mid twentieth century, and the account by the psychiatrist, Dr. Greene, of his own investigation into Roseanne’s admittance into the hospital. Regeneration is based on the real-life experiences of British army officers being treated for shell shock during World War I at Craiglockhart War Hospital in Edinburgh. Its narrative relays the treatment of soldiers suffering mental break down. It is shaped predominately around the discussions which the psychiatrist, Dr. Rivers, has with a number of patients within the asylum in which he works. Both novels are centered around events which have caused psychological distress to the individual characters, but which have also caused what is known as ‘cultural trauma’Neil J. Smelser defines cultural trauma as being a memory accepted and publicly given credence by a relevant membership group and evoking an event or situation which is a) laden with negative effect, b) represented as indelible, and c) regarded as threatening a society's existence or violating one or more of its fundamental cultural presuppositions (Smelser cited in [Alexander et al., 2004, p. 44]) to the nations in which they are set.   Methodology Our method of analysis is based on Systemic functional linguistics (SFL) which is a model of grammar that was developed by Michael Halliday in the 1960s [Halliday, 1976, Halliday, 2004]. It is part of a broad social semiotic approach to language called systemic linguistics. The term systemic refers to the view of language as “a network of systems, or interrelated sets of options for making meaning”. The term functional indicates that the approach is concerned with meaning, as opposed to formal grammar, which focuses on word classes such as nouns and verbs, typically without reference beyond the individual clause. Systemic-Functional Linguistics (SFL) is a theory of language centred around the notion of language function. SFL places the function of language as central (what language does, and how it does it), in preference to more structural approaches, which place the elements of language and their combinations as central. Specifically, it begins with a social context, and looks at how language both acts upon, and is constrained by, this social context. In the model, and methodology, particular aspects of a given social context (such as the topics discussed, the language users and the medium of communication) define the meanings likely to be expressed and the language likely to be used to express those meanings. Since language is viewed as semiotic potential, the description of language is a description of choice. Systemic linguists examine the choices language users can make in a given setting to realise a particular linguistic product (the available choices depend on aspects of the context in which the language is being used). By examining the different choices for the discourse between characters in similar social contexts in different texts, we believe that it may be possible to identify, or describe, the features associated with national literatures that address the problem described here. Our approach is essentially a discourse analysis rather than textual analysis, although it relies on encoded text for comparative analysis. Specifically, we draw on existing SFL models of sociolinguistic and cognitive approaches to doctor-patient discourse [Todd and Fisher, 1993, Togher, 2001]. These models (i) analyse the patterns of talk that are produced by the situational demands of the particular setting, (ii) provide a detailed examination of the interplay of language use in this organisational context of health care delivery, and (iii) examine the production of doctor-patient communication, and (iv) examine the relationship between social structure and social interaction, and explore the relationship between power and resistance. The discourse analyses will utilise custom developed software that analyses encoded passages of text from the novels. A number of encoding schemes exist and provide mechanisms for encoding linguistic data, for example, the Text Encoding Initiative (TEI) [Sperberg-McQueen and Burnard, 1994] or the more recent XCES, based on the Corpus Encoding Standard (CES) which is a part of the EAGLES Guidelines developed by the Expert Advisory Group on Language Engineering Standards (EAGLES). EAGLES provides a set of encoding standards for corpus-based work in natural language processing applications [Ide et al., 2000].   Discussion Through the investigation into past experiences in an attempt to divulge facts about the present condition, the patient-therapist relationship has enormous significance in “factional” novels, given their link to cultural memory and cul- tural trauma. For example, the revelation of the occurrence of nominalisation in the dialogue between patient and therapist provides textual evidence as to whether the patient successfully ‘moves on’ from the traumatic past experience. Nominalisation is an example of grammatical metaphor; the term given when one grammatical class is substituted for another, for example, replacing he departed with his departure. The lexical items are the same, but their place in the grammar has changed. In this example, the meaning expressed by an individual differs in that one form identifies a transient event while the other is more permanent. Attention is also paid to the gender of the characters. The case-study will provide (i) an example of a recurring literary structure of focused dialogue, (ii) one which has a general referent in therapeutic terms: literature of trauma etc., and (iii) but also a dialogue which gathers particular meaning from its specific national and cultural context of conflict (in the case of our chosen novels, an emerging independent Ireland and immediate post World War I Britain). We demonstrate how the findings of our study can be utilized to provide a specialist analysis of the texts within their respective national literatures of Ireland and Britain by applying them to answer a specific literary question about the texts in their national contexts. Having done so, we then illustrate that our findings also elucidate what can be achieved by generalist studies in the context of world literature by comparing the results to provide a commentary on variations in the artistic treatment of cultural trauma.   Conclusion Our case study tests a methodology, which is concerned with analyzing a ‘common- denominator’ (the patient-therapist relationship) between texts. This opens up the two novels under examination to a useful comparative reading as works of world literature, while also yielding useful results to study of the texts within their respective national literatures. In so doing it (i) introduces a new methodology to the academic community and (ii) demonstrates the application of the results produced by said methodology to the answering of a specific literary question. The ultimate aim of the model is to provide a digital linguistic tool that supports individual and collaborative projects in the study of world literature, thus assuaging the needs of both ‘specialists’ and ‘generalists’. In so doing, it offers a solution to one of the problems present in the study of world literature while simultaneously advancing the use of digital humanities in literary studies.  ",
        "article_title": "A New Digital Method for a New Literary Problem: A Proposed Methodology for Bridging the \"Generalist\" - \"Specialist\" Divide in the Study of World Literature",
        "authors": [
            {
                "given": " Sonia",
                "family": "Howell",
                "affiliation": [
                    {
                        "original_name": "School of English and Media Studies, National University of Ireland, Maynooth, Maynooth Co. Kildare, Ireland",
                        "normalized_name": "National University of Ireland",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/00shsf120",
                            "GRID": "grid.9344.a"
                        }
                    }
                ]
            },
            {
                "given": " John G.",
                "family": "Keating",
                "affiliation": [
                    {
                        "original_name": "An Foras Feasa:The Institute for Research in Irish Historical and Cultural Traditions, National University of Ireland, Maynooth, Maynooth Co. Kildare, Ireland",
                        "normalized_name": "National University of Ireland",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/00shsf120",
                            "GRID": "grid.9344.a"
                        }
                    }
                ]
            },
            {
                "given": " Margaret",
                "family": "Kelleher",
                "affiliation": [
                    {
                        "original_name": "An Foras Feasa:The Institute for Research in Irish Historical and Cultural Traditions, National University of Ireland, Maynooth, Maynooth Co. Kildare, Ireland",
                        "normalized_name": "National University of Ireland",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/00shsf120",
                            "GRID": "grid.9344.a"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-40-16",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  Digital humanities practitioners typically deal with polysemous terms by specifying the intended sense of a term in accompanying documentation (when it is one of the set of terms in a schema) or by giving a localized qualification (when the term is being used in a scholarly article). Granted, practitioners do interrogate their use of ubiquitous terms: 'theory,' 'model,' and 'text,' for example, have all been critically examined.As a representative selection from the existing literature, see Caton \"Theory\"; Caton \"Text Encoding\", passim; DeRose et al; Eggert; McCarty, passim; Robinson; Renear \"Out of Praxis\"; Renear \"Theory Restored\"; Renear, Durand, and Mylonas. These discussions, however, have not visibly affected the prevailing ad hoc, localized approach to sense disambiguation. In ordinary language use multiple senses are the norm: we might hope for greater precision in an academic field, but cannot assume it. \"After all,\" writes Allen Renear apropos of conflicting views on the essential characteristics of textuality, \"there is not even a univocal sense of 'text' within literary studies: Barthes's 'text' can hardly be Tanselle's 'text'\" (\"Out\" Note 1 124). The more finely senses are distinguished, though, the greater the need for documentation to point to, the greater the amount of documentation there must be, and the greater the requirement that digital resources make all the necessary pointers available. There is a case, then, for relieving the polysemous burden carried by terms like 'text'. This could be done either by shifting some senses onto different terms or by adding an agreed upon set of clearly defined qualifiers to the original term. One example of different terms being available is the FRBR Group One entity types (IFLA Study Group 3.2). It may not have been the intention of the IFLA Study Group to provide alternatives for 'text', but unquestionably each Group 1 entity type - work, expression, manifestation, and item - corresponds to an existing sense of 'text' and can therefore be used in place of it. However, while these types do capture some broad distinctions, the set is very small. More ambitious is the taxonomy of texts proposed by Shillingsburg as part of his overall concept of a 'script act.'See Resisting ch. 3. This is a revised version of his \"Text\" where the term originally used was 'write act.' Here the semantic burden is shifted to a qualifying phrase and 'text' has the constant sense of a sign sequence (in material or immaterial state), whose existence is established by at least one material instantiation, and which is intended as a unitary communication (whether actually finished or not). Extrapolating from this, we can say that--in relation to this taxonomy--'textuality' is the exhibiting of such properties, and 'text' as a general phenomenon (that is, as a mass noun rather than a count noun) is some quantity of that which exhibits 'textuality'. These definitions are ours and not Shillingsburg's, but derive from his definitions and are consistent with the principles upon which his taxonomy is based. Furthermore, they accord with common senses of those terms. We emphasize this both because it has methodological implications and because it helps us rethink a notion of 'text' that is well-known in the digital humanities community and to see its proper relation to the senses just described. The quote from Renear given earlier comes from his discussion of \"theory about the nature of text\" coming out of the electronic text processing and text encoding localities (\"Out\" 107). The view Renear himself espouses--\"Pluralism\"--developed as a refinement of the earlier view--\"Platonism\"--associated with the assertions made by de Rose et al in the paper \"What is Text, Really?\" This line of thinking has presented itself as definitional, offering a sense to associate with 'text.' Also, by emphasizing its origins in work on automated document processing, it presents this sense of 'text' as fundamental: that is, a more universal sense of 'text' than any sense coming from the traditional humanities localities, because it is as applicable to tax forms, memos, and technical manuals as to novels, plays, and poems. The third thing to note is that this approach has used 'text' in both mass noun and count noun senses interchangeably, and so whatever is said about one applies equally to the other. In the Pluralist view, what defines text is the presence of one or more structures of content objects. We believe this view actually has the opposite effect of what it originally intended because, despite its avowedly universal scope, it actually imposes a greater restriction on what qualifies as a text than Shillingsburg's taxonomy does. Shillingsburg's categories have the form QualifyingLabel+'text', where 'text' has the sense of a sign sequence as described earlier. The sentence \"Call me Ishmael.\" clearly counts as 'text' in Shillingsburg's sense, and equally clearly does not count as 'text' in the Pluralist sense - unless we dilute the sense of the phrase 'content object' until it includes standard linguistic structural units such as the clause, in which case the Pluralist sense simply becomes the same as Shillingsburg's sense. What that line of thinking about text, texts, and textuality that runs from \"What is Text, Really?\" through \"Out of Praxis\" actually describes is a property that many--indeed most--texts exhibit, but that is not an essential property of a text. In a footnote to the discussion in \"Out of Praxis\" Renear acknowledges that the various meanings 'text' has in the various disciplinary localities do share a common ground, namely that \"they all are efforts to understand textual communication.\" But he continues \"I think that taxonomies of sense are best deferred until after we have a better understanding of actual theory and practice\" (124). We think the conceptual help afforded by the clarity of Shillingsburg's distinctions shows the opposite is true: having taxonomies in place first betters our theoretical understanding. That last statement brings out the 'chicken and egg' nature of this problem with terminology, as many scholars would doubtless argue that specifying a taxonomy like Shillingsburg's presupposes one's holding to a particular theory of text/textuality. Debating that, however, would in turn be helped by having a taxonomy of 'theory' available, because what that term means in digital humanities is itself hotly contested. As helpful as we believe Shillingsburg's taxonomy to be, it only clarifies a few items of the \"essential vocabulary,\" and while we think his overall 'script act' framework a good place to start, it needs adding to--for example, in the area that Shillingsburg calls \"reception performance\" (Resisting 77-80). Though he emphasizes his debt to McGann he doesn't attempt a taxonomy of the bibliographic codes that McGann considers such an important feature of production texts (Textual passim). Nor does he really say what happens to the notion of illocutionary point when we move from speech act to script act.On illocutionary point see Searle 2. We find no treatment of it by Shillingsburg in either Resisting or From Gutenberg. This is work still to be done.  ",
        "article_title": "No Representation Without Taxonomies: Specifying Senses of Key Terms in Digital Humanities",
        "authors": [
            {
                "given": " Paul",
                "family": "Caton",
                "affiliation": [
                    {
                        "original_name": "INKE Project Canada",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-30",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  To date, the scholarship on social networking sites (SNSs) such as Facebook and MySpace has focused largely on areas other than pedagogy, with Boyd and Ellison (2007) observing that so far, most SNS research has centred on “impression management and friendship performance, networks and network structure, online/offline connections, and privacy issues”. Some work has been done on the effect of instructor presence on Facebook (Hewitt and Forte, 2006; Mazer et al., 2007; Szwelnik, 2008), the creation of MySpace pages in terms of the acquisition of new forms of digital literacy (Perkel, 2008), and some of the difficulties and benefits of SNSs for university students (Thelwall, 2008). Aside from this, however, there is little scholarship on the educational uses and potential of SNSs at university level. As Szwelnik (2008) comments, ‘Facebook has attracted a lot of attention from media and business but not yet a lot of attention from educational researchers’.  It is perhaps not surprising that it is the social aspects of these sites that have attracted the most critical attention, given that their central purpose is understood to be the management and navigation of (often pre-existing) relationships, rather than a means by which to share interests, complete tasks, or simply communicate with others (Boyd and Ellison, 2007; OFCOM, 2008). However, given that the social dimension of education is a fundamental to learning, it is worth exploring how SNSs may be used to pedagogical advantage. This is particularly the case given the large proportion of university students that access the sites: Ellison et al. (2007) found that 94% of the undergraduate population at Michigan State University were members, while a 2007 Ipsos Mori poll found that 95% of British undergraduates are regular users (Shepherd, 2008). SNSs, it would seem, are a resource not yet being used to their full potential for university teaching.  This paper reports on the findings of a research project designed to address the under-utilisation of SNSs at university level and the corresponding gap in the literature. Undertaken with students in the School of Languages, Linguistics and Film at Queen Mary, University of London, it investigates the educational potential of Facebook for facilitating informal learning with students on their year abroad, particularly in the domain of intercultural awareness and communication. Events held in previous years demonstrated the usefulness of bringing language students together in face-to-face contexts to reflect on their own and others’ diverse experiences of the year abroad. This project set up an online space on an SNS to facilitate this kind of learning through a peer mentoring framework, and to allow discussions of this sort to occur regularly during the students’ time abroad, rather than after it was over.  Undergraduate students in their second and final year of a language course were surveyed about their attitude towards the year abroad, their use of technology and SNSs, and, following Ellison et al. (2007), their affective investment in SNSs. Two different populations were surveyed: second year students organising their year abroad for the following year, and final year students who had returned from their year abroad. The results were used to develop focus group protocols to gauge students’ receptivity to the idea of using Facebook to carry out course-related discussions, to judge the extent to which Facebook was a hospitable environment for peer mentoring to occur, and to determine which Facebook applications would best assist with educational objectives. Students were also asked about which aspects of the year abroad they were already using Facebook to engage with, and which elements of their time away could be ameliorated through provision of a virtual meeting place for discussion. Several peer mentors were then chosen from the cohort who had already completed a year abroad, and these students were trained in online moderating and mentoring. A Facebook group was set up for the student mentors to use, and this was observed over a period of three months. Following this, four methods of evaluation were used to measure the effectiveness of the Facebook group: a) an online survey for third-year students currently on their year abroad; b) informal discussions with academic staff in modern languages; c) interviews with the peer mentors held at a computer; and d) close analysis and corpus analysis of the text of the online discussions. Paul and Brier (2001) and Cummings et al. (2006) have explored how students of university age use Facebook and other internet technologies to alleviate the “friendsickness” brought about by moving away from one’s friends. This project aimed to capitalise on the powerful ability of SNSs to address this relational need by drawing students into online conversations and collaborations that not only helped them to sustain relationships but also to use those relationships to learn from one another. However, existing research points to a strong resistance from university students to academics occupying “their” space on an SNS, something Szwelnik terms “crossing the boundary” (2008). Hewitt and Forte (2003) observe that identity management is a significant concern for SNS users when the roles they occupy cross perceived social boundaries and bring organizational power relationships into visibility, citing one student’s fears that Facebook could “unfairly skew a professor's perception of a student in a student environment”. Given that both social boundaries and uneven power relationships both come into play in the context of teacher-led discussions around course-related material, the project sought to find a way to build a learning community without infringing on a space perceived not to belong to academic staff, and to shift the discursive content from social to educational without forcing students to “cross the boundary”. In working with peer mentors, the project aimed not only to avoid these boundary-crossing problems but also to work intentionality into the fabric of the Facebook group. As Woods and Ebersole (2003) observe, transforming textual exchanges into a learning community with a positive social dynamic requires intentional decisions in the realm of both verbal and nonverbal communication, so student mentors needed to be made acquainted with the learning objectives for the educational context, and carefully trained in techniques of e-moderation to overcome the challenges a mediated environment can pose to productive discussions. This approach also had the advantage of being, to an extent, futureproofed: students were more likely than academic staff to know which technologies are most popular with their peers, and once a framework for online mentoring is established, this could be moved in future years to different sites or applications as students’ usage patterns change. A further advantage of this model is that it equips the student mentors with the digital literacy and communication skills to operate in the kinds of virtual environments that, as knowledge workers, they are likely to inhabit in their careers.  In summarising the results of this study, this paper will report on the benefits of using an SNS to support informal learning in the context of students on a year abroad, and set out approaches that universities can take to promote learning more generally through the use of SNSs. It uses tools from the Internet Community Text Analyzer (http://textanalytics.net/) to visualise the networks that developed between students on the site, and to identify productive models of SNS-mediated student mentoring behaviour.  ",
        "article_title": "Crossing the Boundary: Exploring the Educational Potential of Social Networking Sites",
        "authors": [
            {
                "given": " Anouk",
                "family": "Lang",
                "affiliation": [
                    {
                        "original_name": "Queen Mary, University of London UK",
                        "normalized_name": "University of London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/04cw6st05",
                            "GRID": "grid.4464.2"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-19",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  What does it mean to be digitally literate?  Obviously it entails a basic familiarity with commonly used technologies, so that one may navigate the technological life world that has permeated nearly every aspect of the human one. One aspect of this knowledge is the recognition of computer languages, communications protocols, syntactic forms, passages of program code, and command line arguments, even when they have been taken out of their operational context for use as literary and rhetorical devices.  In addition to the infiltration of the abbreviated language of email and text messaging into mainstream print media, it is now also commonplace to encounter programming keywords, symbols, operators, indentation, and pagination entwined with natural, non-technical, mother tongue expressions.  Codework is the term associated with the literary and rhetorical practice of mixing human and computer languages (Hayles, 2004; Raley, 2002; Cramer, 2008).  Types of codework span from intentionally arranged constructions intended for human consumption that do not execute on any real computer system, to valid expressions in bona fide programming languages that are meaningful to both human and machine readers.  Examples of the former include the work of Mez (Mary-Anne Breeze) and Talon Memmott, of the latter, the work of John Cayley and Grahan Harwood (Raley, 2002; Fuller, 2008).  Rita Raley notes, however, that of the popular electronic literature of the early twenty first century, there is “less code per se than the language of code.” In addition to its infusion for literary effect, program source code may be cited in scholarly texts like conventional citations to explain a point in an argument.  Although it is more common to encounter screen shots of user interfaces, examples of working source code appear on occasion in humanities scholarship.  This study will briefly consider why working code has been largely shunned in most academic discourse, and then identify the types and uses of bone fide code that do appear, or are beginning to appear, in humanities scholarship.  Its goal is to suggest ways in which working code – understood both as code that works, and as the practice of working code – plays a crucial role in facilitating digital literacy among social critics and humanities scholars, and demonstrate through a number of examples how this effect may be achieved. The first argument in favor of studying computer code in the context of humanities scholarship can be drawn from N. Katherine Hayles' methodological tool of Media-Specific Analysis (MSA).  Probing the differences between electronic and print media when considering the same term, such as hypertext, requires comprehension of the precise vocabulary of the electronic technologies involved.  A second, more obvious argument comes from the growing disciplines of Software Studies and Critical Code Studies.  If critical analysis of software systems is to reveal implicit social and cultural features, reading and writing program code must be a basic requirement of the discipline (Fuller, 2008; Mateas, 2005; Wardrip-Fruin, 2009).  As the media theorist Friedrich Kittler points out, the very concept of what code is has undergone radical transformations from its early use by Roman emperors as cipher to a generic tag for the languages of machines and technological systems in general; “technology puts code into the practice of realities, that is to say: it encodes the world” (Kittler, 2008).  Or, following the title of Lev Manovich's new, downloadable book, software takes command.  Yet both Kittler and Manovich express ambivalence towards actually examining program code in scholarly work.  A third argument, which will form the focus of this study, is reached by considering the phenomenon of technological concretization within computer systems and individual software applications.  According to Andrew Feenberg, this term, articulated by Gilbert Simondon, describes the way “technology evolves through such elegant condensations aimed at achieving functional compatibilities” by designing products so that each part serves multiple purposes simultaneously (Feenberg, 1999).  The problem is that, from the perspective of a mature technology, every design decision appears to have been made from neutral principles of efficiency and optimization, whereas historical studies reveal the interests and aspirations of multiple groups of actors intersecting in design decisions, so that the evolution of a product appears much more contingent and influenced by vested interests.  The long history of such concretizations can be viewed like the variegated sedimentation in geological formations, so that, with careful study, the outline of a technological unconscious can be recovered.  The hope is that, through discovering these concealed features of technological design, the the unequal distribution of power among social groups can be remedied.  Feenberg's project of democratic rationalization responds to the implicit oppression of excluded groups and values in technological systems by mobilizing workers, consumers, and volunteers to make small inroads into the bureaucratic, industrial, corporate decision making. For computer technology in particular, digital literacy is the critical skill for connecting humanities studies as an input to democratic rationalizations as an output.  Working code replaces the psychoanalytic session for probing the technological unconscious to offer tactics for freeing the convention-bound knowledge worker and high tech consumer alike.  Many theorists have already identified the free, open source software (FOSS) community as an active site for both in depth software studies and for rich examples of democratic rationalizations (Fuller, 2008; Yuill, 2008; Jesiek, 2003).  Simon Yuill in particular elaborates the importance of revision control software for capturing and cataloging the history of changes in software projects.  As as corollary to this point, it can be argued that concealed within these iterations of source code are the concretizations that make up the current, polished version of the program that is distributed for consumption by the end users, and from which the technological unconscious may be interpreted.  However, even when they are freely available to peruse in public, web-accessible repositories, these histories are only visible to those who can understand the programming languages in which they are written.  Therefore, it is imperative that humanities scholars who wish to critically examine computer technology for its social and cultural underpinnings include working code - as practicing programming - in their digital literacy curricula.  ",
        "article_title": "From Codework to Working Code: A Programmer's Approach to Digital Literacy",
        "authors": [
            {
                "given": " John",
                "family": "Bork",
                "affiliation": [
                    {
                        "original_name": "University of Central Florida USA",
                        "normalized_name": "University of Central Florida",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/036nfer12",
                            "GRID": "grid.170430.1"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-22",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " This paper presents the outcomes to date of the annotation interoperability component of the Open Annotation Collaboration (OAC) Project.http://www.openannotation.org/ The OAC project is a collaboration between the University of Illinois, the University of Queensland, Los Alamos National Laboratory Research Library, the George Mason University and the University of Maryland. OAC has received funding from the Andrew W. Mellon Foundation to develop a data model and framework to enable the sharing and interoperability of scholarly annotations across annotation clients, collections, media types, applications and architectures. The OAC approach is based on the assumption that clients publish annotations on the Web and that the target, content and the annotation itself are all URI-addressable Web resources. By basing the OAC model on Semantic Web and Linked Data practices, we hope to provide the optimum approach for the publishing, sharing and interoperability of annotations and annotation applications. In this paper, we describe the principles and components of the OAC data model, together with a number of scholarly use cases that demonstrate and evaluate the capabilities of the model in different scenarios.   Introduction and Objectives Annotating is both a core and pervasive practice for humanities scholarship. It is used to organize, create and share knowledge. Individual scholars use it when reading, as an aid to memory, to add commentary, and to classify documents. It can facilitate shared editing, scholarly collaboration, and pedagogy. Although there exists a plethora of annotation clients for humanities scholars to use (Hunter 2009) - many of these tools are designed for specific collection types, user requirements, disciplinary application or individual, desktop use. Scholars are also confronted with having to learn different annotation clients for different content repositories, have no easy way to integrate annotations made on different systems or created by colleagues using other tools, and are often limited to simplistic and constrained models of annotations. For example, many existing tools only support the simplistic model in which the annotation content comprises a brief unformatted piece of text. Many tools conflate the storage of the annotations and the target being annotated.  Frameworks for annotation reference are inconsistent, not coordinated, and frequently idiosyncratic, and the constituent elements of annotations are not exposed to the Web as discrete addressable resources, making annotations difficult to discover and re-use. The lack of robust interoperable tools for annotating across heterogeneous repositories of digital content and difficulties sharing or migrating annotation records between users and clients – are hindering the exploitation of digital resources by humanities scholars. Hence the goals of the Open Annotations Collaboration (OAC) are:  To facilitate the emergence of a Web and Resource-centric interoperable annotation environment that allows leveraging annotations across the boundaries of annotation clients, annotation servers, and content collections. To this end, annotation interoperability specification consisting of an Annotation Data Model will be developed. To demonstrate through implementations an interoperable annotation environment enabled by the interoperability specifications in settings characterized by a variety of annotation client/server environments, content collections, and scholarly use cases. To seed widespread adoption by deploying robust, production-quality applications conformant with the interoperable annotation environment in ubiquitous and specialized services and tools used by scholars (e.g., JSTOR, Zotero, and MONK).   In the remainder of this paper we describe related efforts that have informed the development of our Annotation Data Model. We then describe the data model itself that lays a foundation for follow-on work involving demonstrations and reference implementations that exploit real-world repositories such as JSTOR, Flickr Commons, and MONK and leverage existing scholarly annotation applications such Zotero, Pliny and Co-Annotea.   Related Work Despite the vast body of work regarding annotation practice, annotation models, and annotation systems, little attention has been paid to interoperable annotation environments. The few efforts in this realm to date comprise:  RDF-based Annotea developed by Kahan and Koivunen (Kahan et al., 2001); Agosti’s “A Formal Model of Annotations of Digital Content” (Agosti and Ferro, 2007); SANE Scholarly Annotation Exchange; OATS (The Open Annotation and Tagging System (Bateman et al., \"OATS: The Open Annotation and Tagging System\")).   An analysis of these existing models reveals that on the whole, they have not been designed as Web-centric and resource-centric, or that they have modeling shortcomings that prevent any existing resource from being the content or target of an annotation and from giving an annotation independent status as a resource itself. Further requirements that we have identified that these approaches fail to fully support include:  Resources of any media type can be Annotation Content or Targets; Annotation Targets or Content are frequently segments of Web resources; The Content of a single annotation may apply to multiple Targets or multiple annotation Contents may apply to a single Target;  Annotations can themselves be the Target of further Annotations.     The OAC Data Model By exploiting the Web- and Resource-centric approach to modelling annotations, we leverage existing standards and facilitate the interoperability of annotation applications. In the OAC model, an Annotation is an Event initiated at a date/time by an author (human or software agent). Other entities involved in the event are the Content of the Annotation (aka Source) and the Target of the Annotation. The model assumes that the core entities (Annotation, Content and Target) are independent Web resources that are URI-addressable. This approach simplifies and decouples implementation from the repository. An essential aspect of an annotation is the (implicit or explicit) expression of “annotates” relationship between the Content and the Target. The model allows for Content and Target of any media type and the Annotation, Content, and Target can all have different authorship. In situations where the annotation Content or Target is a segment or fragment of a resource (e.g., region of an image), we will draw on the work of the W3C Media Fragments Working Group to specify the fragment address. Figure 1 illustrates the alpha version of the OAC data model.   Fig. 1. The Alpha OAC Data Model     Use Cases In order to evaluate and demonstrate the feasibility of the OAC Data Model, an initial set of use cases has been developed that are representative of a range of common scholarly practices involving annotation. This preliminary set is available from the OAC Wiki as OAC User Narratives/Use Caseshttps://apps.lis.illinois.edu/wiki/display/openannotation/OAC+User+Narratives+-+Use+Cases and includes:  Citation of Non Printed Media Commentary on Remote Resources Shared Annotations Across Interfaces Harvesting, Aggregating, Ranking and Presenting Annotations from Multiple Sites Annotating Relationships Between Multiple Mixed-Media Resources Annotations which Capture Netchaining Practices Annotations with Compound Targets   For example, Figure 2 illustrates a scholarly annotation example involving multiple targets, in which a scholar is making a comment on the differences between segments in scholarly editions of the poem “The Creek of the Four Graves” by Charles Harpur.  Fig. 2. Annotating the differences between two scholarly editions in AustLit   Figure 3 below illustrates the corresponding OAC model for the use case in Figure 2 in which a single annotation Content applies to two Target resources.  Fig. 3. OAC Model for the example in Figure 2     Discussion and Conclusions The proposed OAC Data Model will enable the sharing and discovery of annotations beyond the boundaries of individual solutions or content collections, and hence will allow for the emergence of value-added cross-environment annotation services. It will also facilitate the implementation of advanced end-user annotation services targeted at humanities scholars that are capable of operating across a broad range of both scholarly and general collections. Furthermore, it will enable customization of annotation services for specific scholarly communities, without reducing interoperability. The proposed work will also enable more robust machine-to-machine interactions and automated analysis, aggregation and reasoning over distributed annotations and annotated resources. By grounding our work in a thorough understanding of Web-centric interoperability and embedded models implemented by existing digital annotation tools and services, we create an interoperable annotation environment that will allow scholars and tool-builders to leverage prior tool development work and traditional models of scholarly annotation, while simultaneously enabling the evolution of these models and tools to make the most of the potential offered by the Web environment.   AcknowledgmentsThe Open Annotations Collaboration (OAC) is funded by the Andrew W. Mellon Foundation. The authors would also like to acknowledge the valuable contributions to this work made by: Neil Fraistat, Doug Reside, Daniel Cohen, John Burns, Tom Habing, Clare Llewellyn, Carole Palmer, Allen Renear, Bernhard Haslhofer, Ray Larsen, Cliff Lynch and Michael Nelson. Figure 2 is courtesy of Anna Gerber, Senior Software Engineer on the Aus-e-Lit project.  ",
        "article_title": "The Open Annotation Collaboration: A Data Model to Support Sharing and Interoperability of Scholarly Annotations ",
        "authors": [
            {
                "given": " Jane",
                "family": "Hunter",
                "affiliation": [
                    {
                        "original_name": "University of Queensland Australia",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Tim",
                "family": "Cole",
                "affiliation": [
                    {
                        "original_name": "University of Illinois, Urbana-Champaign University of Illinois at Urbana-Champaign USA",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            },
            {
                "given": " Robert",
                "family": "Sanderson",
                "affiliation": [
                    {
                        "original_name": "Los Alamos National Laboratory USA",
                        "normalized_name": "Los Alamos National Laboratory",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01e41cf67",
                            "GRID": "grid.148313.c"
                        }
                    }
                ]
            },
            {
                "given": " Herbert",
                "family": "Van de Sompel",
                "affiliation": [
                    {
                        "original_name": "Los Alamos National Laboratory USA",
                        "normalized_name": "Los Alamos National Laboratory",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01e41cf67",
                            "GRID": "grid.148313.c"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-28",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  People often want to collect and utilize free, publicly available images on a given subject. Image sharing systems such as Flickr store billions of user-contributed images. While such systems are designed to encourage user contributions and sharing, they are not well-organized collections on any given subject. We propose an approach that systematically harvest images from Internet and organize the images into an evolving faceted classification. We implemented a prototype to continuously harvest the most popular images on Flickr related to African American history, and organize them into an evolving faceted classification collaboratively maintained by users. The same approach can be applied to other digital humanities resources on the Internet. The talk will elaborate the details of technical design and prototype implementation, and discuss evaluation results.   Introduction Flickr hosted over 4 billion images as of October 2009 and is growing by about 4 million pictures a day. Facebook hosted 15 billion photos and Imageshack hosted 20 billion images as of April 2009. Other photo sharing sites such as Picasa, Multiply and PhotoBucket also host billions of images [Schonfeld, 2009]. In contrast, organized public domain image collections are relatively scarce [Wikipedia: 'Public domain image resources']. The largest public domain image repository, Wikimedia Commons, reached 5 million images as of September 2009. It would greatly benefit the humanities community if images in those image sharing sites can be organized and utilized. We propose an approach that systematically searches and harvests images (actually the link to the image and metadata, but not image itself) from image sharing sites, and organizes the images into a multi-faceted classification. The data harvesting is performed on a continual basis, and the classification evolves over time. Besides automated programs, the approach utilizes collaborative human efforts to improve the quality of collection. We implemented a prototype that builds a dynamic image collection on African American History from the most popular images on this subject on Flickr.com. Our fundamental belief is that a large, diverse group of people (students, teachers, etc.) can do better than a small team of librarians or editors in constructing a multimedia collection at virtually no cost. Note that not all the images on those sharing sites are copyright-free or have a creative commons license. However, most of the sites allow other websites to directly link to their images if the images are marked as public access by their contributors, and if credits are properly given. Our approach displays images through embedded image URLs but does not download the images from their original sources.   Related Work Many are trying to utilize the images in fast-growing photo sharing and social networking sites. For example Getty Images, the leader in stock photography, hires image editors to select most popular Flickr images and obtain copyright from individual contributors, then sells the images for $5 per image (http://www.gettyimages.com/). Computer-graphics researchers at the University of Washington have utilized Web images to digitally reconstruct buildings in 3-D. For example, based on 150,000 publicly accessible Flickr pictures of Rome, the program automatically re-created the Colosseum, Trevi Fountain, and the outside and inside of St. Peter's Basilica, among other Roman icons. The technique can be used to make virtual-reality experiences for tourism, auto-build cities for video games and movies, or help digitally preserve and study historic cities that are being destroyed by human-caused or environmental factors [Jaggard 2009]. Researchers have argued for building an academic Flickr, or an academic photo sharing site in general: a net-based service that would enable faculty and researchers to post and share images with scholarly value, either with the general community, or pursuant to any associated rights, to restricted-use populations [P. Brantley's blog]. For example, a group at Lewis & Clark College in Portland is in the process of developing an educational collection of contemporary ceramics images using the photo sharing site Flickr [McWilliams 2008]. Our project attempts to build free, well-organized topical images collections from the images contributed by Internet users, to support education or research objectives. While most photo sharing systems support keyword-based search utilizing user-contributed metadata, none of them support browsable hierarchies that allow users to explore a given subject in depth. Using librarians or images editors to manually construct a topical collection is cost prohibitive, and unfeasible if the collection needs to keep up with rapidly growing data sources. Our collection-construction approach combines the collaborative concepts of wiki and social tagging systems with automated classification techniques. Our system allows users to collaboratively build a classification schema with multi-faceted categories, and to classify documents into this schema. Utilizing users’ manual efforts as training data, the system’s automated techniques build a faceted classification that evolves with the growing collection, the expanding user base, and the shifting user interests.   Architecture and Prototype Implementation Our collection construction approach is summarized in Figure 1. The system first collects images (links and metadata such as tags) on a given topic using keyword search, utilizing the APIs (Application Programming Interface) provided by image sharing sites or search engines. For the initial collection, a group of experts or administrators create the initial classification schema and classify a set of images into the initial schema. Utilizing experts’ classifications as training data, and also Wordnet and Wikipedia as knowledge bases, the system employs automated techniques (heuristic matching rules and support vector machine-based classifiers) to classify images into the classification schema. In a wiki fashion, users of the image collection can modify and improve the classification schema, and manually classify items into the schema. Users can also assign additional tag or annotations to image objects. Utilizing the additional metadata from users’ tagging and annotation efforts and by analyzing users’ classification/usage history, the system refines both the classification schema and the item-category associations. The system continues to collect and classify images to stay up-to-date with external image sources.   Figure 1. Systematic approach of constructing a topical collection using Internet images.    We built a prototype to construct an image collection on African American History from Flickr. By querying “African American History” in the search field, we extracted metadata for all the images in the result pages: title, url, description, tags, and the contributor. The initial collection contained about 11,000 Flickr images on African American History. Over 3 months the collection has grown to contain about 13,000 images. During the conference we will elaborate the details of technical design, prototype implementation, and the evaluation results. Figure 2 shows the browsing and classification interfaces of our prototype.   Figure 2. Browsing and classification interface of the prototype.       Discussion Evaluation of the prototype in a classroom environment shows promise. Measured by metrics such as precision, recall and image quality (popularity), the prototype is more effective than Flickr in supporting several image retrieval tasks. The evolution of classification shows improvements, based on user ratings of categories and category-item associations. We conducted interviews and usage observations, which help understand the level of efforts that users spend on tagging and classification. For future research, we are interested in whether social tagging and tag convergence [Muller et al. 2008] can be utilized to assist or substitute classification efforts. Our approach can be applied to other digital humanity resources. For example, we have developed another prototype to construct a dynamic collection of news items on a given topic based on Google News. We believe that a combination of collaborative and automated classification techniques can construct valuable digital humanities collections at low costs. As far as we know, no one has combined user efforts and automated techniques to build a faceted classification. Several research projects are related to social tagging and classification, however. Several projects attempt to construct tag hierarchies or ontologies, or otherwise harvest the intelligence stored in tags [Heymann and Garcai-Molina 2006, Schmitz and Patrick 2006, Harris et al. 2006]. Our earlier work [Arnaout et al. 2008] on faceted classification was presented in the Digital Humanities 2008 conference.   Acknowledgements This project is supported in part by the United States National Science Foundation, Award No. 0713290.  ",
        "article_title": "Building Dynamic Image Collections from Internet",
        "authors": [
            {
                "given": " Liuliu",
                "family": "Fu",
                "affiliation": [
                    {
                        "original_name": "Old Dominion University, USA",
                        "normalized_name": "Old Dominion University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04zjtrb98",
                            "GRID": "grid.261368.8"
                        }
                    }
                ]
            },
            {
                "given": " Kurt",
                "family": "Maly",
                "affiliation": [
                    {
                        "original_name": "Old Dominion University, USA",
                        "normalized_name": "Old Dominion University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04zjtrb98",
                            "GRID": "grid.261368.8"
                        }
                    }
                ]
            },
            {
                "given": " Harris",
                "family": "Wu",
                "affiliation": [
                    {
                        "original_name": "Old Dominion University, USA",
                        "normalized_name": "Old Dominion University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04zjtrb98",
                            "GRID": "grid.261368.8"
                        }
                    }
                ]
            },
            {
                "given": " Mohammad",
                "family": "Zubair",
                "affiliation": [
                    {
                        "original_name": "Old Dominion University, USA",
                        "normalized_name": "Old Dominion University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04zjtrb98",
                            "GRID": "grid.261368.8"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-05-04",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The 70th anniversary of WW2 generated a new discussions about Poles’ attitudes to the country’s important historical events. The general aim of my present study is to investigate how contemporary Poles’ attitudes to the two world wars are expressed in electronic discourse, using the example of a specialist Internet forum. The research data comes from an active Polish forum devoted to the history of the two world wars: Your Forum About the World Wars retrieved from www.dws.org.pl. The quantitative-qualitative analysis of the compiled corpus is conducted by means of the UAM Corpus Tool, a multiple-level environment for annotation and analysis of text corpora (http://www.wagsoft.com/CorpusTool/). The forum under study provides a challenging research context for the study of evaluative language in the electronic medium by means of corpus tools. The forum gathers both professionals and amateurs interested in an academic debate on the issues related to their interests in the history of the two world wars. Interaction on the forum follows the rules of scholarly exchange, where careful use of language and factual expression are encouraged, whereas emotionality is rather disfavoured. This makes textual realizations of evaluation less overt: thus, how attitudinal language is transmitted through the text can be studied effectively by means of corpus tools. The theoretical basis of my study is a functional approach to the analysis of evaluative language, where evaluation encompasses both attitudinal and affective aspects of language use (e.g. Hunston and Thompson, 1999). For the corpus-based study of evaluative language, I adopt Martin and White’s Appraisal Framework (Martin and White, 2005), selecting their systems of Attitude and Engagement. More specifically, I analyse linguistic realisations of affect (within the system of Attitude) and modality and evidentiality (within the system of Engagement) as elements of interactional aspects of language use. In this way I aim to investigate evaluation as 1) expression of both individual and communal value systems of language users, and 2) expression of the speaker-audience relations (especially with the focus on the rhetorical effects of evaluation and its role in the social construction of knowledge). With the present study I also hope to contribute to the previous research that shows how text analyses within the framework of systemic-functional linguistics can profit from the use of corpus-linguistic methods (Bednarek, 2008; Thompson and Hunston, 2006), at the same time attempting to investigate how the combination of SFL and CL works for the Polish language.  The preliminary results of the study in progress prove the presence of evaluative language across the corpus, with the prevalence of engagement over attitude. This demonstrates that the members of the forum under study are primarily involved in objective and to-the-point discussions on the issues related to their interests. Evaluative language is mainly used as a persuasive tool to enhance the power of knowledge claims, and to manage interpersonal relations. As regards the general attitudes of the forum members, the study proves that they are actively engaged in constructing and preserving the collective memory of the two world wars. The wars are discussed with professional commitment and without overly sentimental pathos, which makes the forum and history of the two world wars attractive also for young people. The application of the UAM Corpus Tool in the analyses of evaluative language proves advantageous first of all thanks to the tool’s possibility of multi-level annotation and cross-classification of features. These features allow me to study the distribution of evaluative language relative to the sub-generic status of postings (i.e. whether a posting is identified as a voice in a discussion, initiation of a discussion, request, query, etc.). In addition, the tool allows to manage the problem of evaluation “hidden” behind stretches of discourse, as well as to analyse the distribution patterns of evaluative lexis. Finally, as the UAM Corpus Tool allows annotation of both text and images, it can be used for the analysis of multimodal genres, an example of which is an e-forum. The UAM Corpus Tool can be useful and flexible in corpus-based analyses of evaluative language, however its application invariably demonstrates the importance of human choice in working with corpora in discourse analysis (cf. also Baker, 2006).  ",
        "article_title": "WW1 and WW2 on a Specialist E-forum. Applying Corpus Tools to the Study of Evaluative Language",
        "authors": [
            {
                "given": " Małgorzata",
                "family": "Sokół",
                "affiliation": [
                    {
                        "original_name": "Szczecin University, Poland",
                        "normalized_name": "University of Szczecin",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/05vmz5070",
                            "GRID": "grid.79757.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-21",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " At the 2009 Digital Humanities conference I presented my paper on the WikiPhiloSofia (http://research.cis.drexel.edu:8080/sofia/WPS/) project (Athenikos and Lin, 2009), which was concerned with extraction and visualization of facts, relations, and networks concerning philosophers using Wikipedia (http://www.wikipedia.org/) as the data source. In this proposal, I present a related, extended project in progress, entitled PanAnthropon, which incorporates the problems of retrieving entities in response to a query and retrieving entities related to a given entity and which extends the scope of application to domains other than philosophy.   Background Traditional information retrieval is concerned with retrieving documents that are potentially relevant to a user’s query. The relevance of a document to a given query is usually measured by lexico-syntactic matching between the terms in the query and those in the document (title). Familiar Web search engines, such as Google and Yahoo, for example, return a ranked list of Web pages that contain all or some of the keywords in the query entered by a user. The Semantic Web (Berners-Lee et al., 2001) initiative aims at transforming the Web of pages (documents) into the Web of entities (things in the broadest sense) (cf. OKKAM project (http://www.okkam.org/) (Bouquet et al., 2007)). Information retrieval on the Semantic Web is no longer a matter of retrieving documents via semantics-unaware keyword matching but a matter of retrieving entities that satisfy the semantic constraints imposed by the query, i.e. those that are of specific semantic type and that satisfy the given semantic conditions. Wikipedia has become an important semantic knowledge resource (cf. Zesch et al., 2007) thanks to its unique set of semi-structured semantic features and the huge amount of content covering a wide range of topics. What renders Wikipedia more interesting is the fact that it can be considered as a self-contained web of entities. Each Wikipedia article is concerned with one entity, and the given entity is connected to other entities via explicit semantic relations as in infoboxes and wikitables or via implicit semantic relations as in hyperlinks.    Motivation Through the WikiPhiloSofia project I demonstrated extracting, retrieving, and visualizing specific facets of information, not documents, concerning entities of a selected type, namely, philosophers, by exploiting the hyperlinks, categories, infoboxes, and wikitables contained in Wikipedia articles. The interface that I created enables the users to select a focus of query in the form of an entity (philosopher) or a pair of entities (philosophers) and then to retrieve entities that satisfy specified conditions with respect to the given entity or pair of entities. However, the project did not consider the problems of retrieving entities as answers to queries, semantically typing entities, or retrieving related entities by type and condition.  The proposed PanAnthropon project takes up the aforementioned problems left out of the WikiPhiloSofia project. The dual objective is to enable retrieval of entities that directly answer a given semantics-based query and to enable retrieval of related entities by semantic type, subtype (role), and relation, by using information extracted/integrated from Wikipedia. The project applies the approach to the entities concerning the intellectual/cultural heritage – people, works, concepts, etc. The Web portal interface thereby constructed will allow the users to retrieve entities that directly answer their queries as well as to explore people, works, concepts, etc. in relation to other people, works, concepts, etc.    Conceptualization In the proposed project, “entities” are conceived of as things of all kinds that have certain properties (or attributes). The “type” of an entity is considered as a generic kind (or class or category) into which the given entity is classified, e.g., person, work, etc. In general, the type of an entity is fixed and exclusive in the sense that an entity that belongs to one type does not or cannot belong to other types. The “subtype” of an entity refers to a subclass or subcategory into which the entity can be classified, under a given type. The subtype of an entity is fluid and non-exclusive in the sense that an entity may belong to more than one subtype (under a given type). This is especially so in the case of person-type entities, and thus a subtype may better be understood as a “role” in this case. In general, there are multiple subtypes under a given type, and the former can be further classified into still more specific subtypes. A type or subtype of an entity can be considered as a special kind of property. A “fact” concerning an entity refers to a tuple consisting of <entity, attribute, value, [context]>, which adds the optional “context” element to the <subject, predicate, object> triple model. An entity can have “relations” to other entities, given its properties. The kinds of properties and relations that are relevant or of interest concerning an entity, except certain basic facts, depend on the domain at issue. An entity may belong to multiple domains, but not every subtype, property, or relation is relevant or equally important in one domain as in another domain. The project therefore intends to build a portal consisting of sub-portals representing different domains.   Methodology  Data Extraction and Processing The pre-processing stage of the project (for each domain) concerns: (1) compiling a seed list of entities of interest by extracting names from various lists and categories in Wikipedia, (2) downloading Wikipedia article pages for each entry on the list, and (3) inspecting typical attributes and (types/subtypes of) values contained in infoboxes, wikitables, etc. The main processing stage concerns extracting information on an entity and related entities from each Wikipedia page. The semantic type/subtypes of a given entity are extracted and/or assigned. Semi-structured templates and portions of the article are processed so as to extract attribute–value (or predicate–object or relation–entity) pairs. The related entities are matched to the entities on the seed entity list. Additional Wikipedia pages are downloaded for entities not matched on the list, and the information on those entities is extracted. The (optional) post-processing stage concerns converting the data stored in a MySQL database to XML files and RDF triples, thereby creating a semantic data repository that can be linked to other resources involved in the Linked Data (http://linkeddata.org/) initiative in the latter case.    Semantic Search Interface The semantic search interface created will support three types of query and retrieval. The first type of query/retrieval concerns retrieval of entities that correspond to queries the expected answers of which are entities. The second type of query/retrieval concerns retrieval of entities related to an entity, according to type/subtypes and specified properties/values. The third type of query/retrieval concerns retrieval of facts concerning an entity. The interface will also incorporate some of the visualization features available in the WikiPhiloSofia portal interface.     Current Application The film domain has been chosen as the initial proof-of-concept domain of application. In my presentation I will demonstrate the entity retrieval functionalities with 1.5+ million (and growing) facts about 11370 films, 69545 persons, 74545 film roles, 253 places, 6033 dates, etc.   Related Work The task of retrieving entities in response to user queries using the information in Wikipedia has since 2007 been the focus of the INEX (Initiative for the Evaluation of XML Retrieval) XML Entity Ranking (XER) Track (de Vries et al., 2008; Demartini et al., 2009). Unlike in the INEX XER Track, the proposed project addresses the task by extracting information from the HTML Wikipedia files and building a knowledge base based on it. The task of constructing a knowledge base by extracting information from templates in Wikipedia such as infoboxes has been attempted in large scale by, e.g., Auer and Lehmann (2007) and Suchanek et al. (2007). There is also the DBpedia (http://dbpedia.org/) knowledge base, which contains the information extracted from Wikipedia. The proposed project, however, utilizes the information in the main content of Wikipedia articles, as well as templates, to enable and enhance entity retrieval. It will also provide a more flexible working search interface for both general and entity-specific queries.    Conclusion The PanAnthropon project utilizes Wikipedia as a semantic knowledge source for entity retrieval and applies the approach to materials concerning the intellectual/cultural heritage. The semantic search interface created will allow the users to retrieve entities that directly answer their queries as well as to explore various semantic facets concerning those entities. As such, it will provide a useful resource for digital humanities.  ",
        "article_title": "Using Wikipedia to Enable Entity Retrieval and Visualization Concerning the Intellectual/Cultural Heritage",
        "authors": [
            {
                "given": " Sofia J.",
                "family": "Athenikos",
                "affiliation": [
                    {
                        "original_name": "Drexel University, Philadelphia, USA",
                        "normalized_name": "Drexel University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04bdffz58",
                            "GRID": "grid.166341.7"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-28",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This paper examines the spatiality of three contemporary literary narratives using a digital humanities approach. By this I mean a few things: firstly, I regard spatiality as a complex and dynamic historical dimension on par with temporality, and not just as a static, passive container in which events independently transpire. Secondly, I am interested in examining not only space and place as represented in texts, but also the spatiality of the texts themselves, i.e., the materiality of language. Thirdly, I have built the Litmap digital mapping platform (http://barbarahui.net/litmap) for the purpose of visualizing space and place in/of texts, which I use in conjunction with traditional close reading methods in order to carry out my scholarship. The definition of spatiality I employ follows from arguments made by spatial theorists including Henri Lefebvre, David Harvey, Doreen Massey, Edward Said and Edward Soja, who push for an understanding of space and place as socio-historically produced rather than somehow existing a priori. I argue further that networked spatiality is a prevalent trope, organizing principle, and way of understanding the world in contemporary texts. I show how this presents itself in the narratives I examine (in quite a different way in all three) and is a particularly useful, even crucial inroad into understanding them. My assertion is that the three texts at hand can be characterized as displaying three kinds of topographical networks:   In W.G. Sebald’s Rings of Saturn (1997), geographical places are connected to each other via a historical network of events, and the nodes of the network are primarily man-made architectural structures.  In Emine Sevgi Özdamar’s Seltsame Sterne starren zur Erde (2008), geographical places are connected to one another via the transnational migrations of people, and the nodes of the network are these moving embodied subjects themselves.   In Steven Hall’s Raw Shark Texts (2007), language, thought, and memory are material and have spatial dimensions. Places are connected to each other via these material traces, and the nodes of the network, which are constituted by human subjects and their linguistic traces, are ephemeral and unstable, with “un-space” figuring as an otherworldly yet very real dimension in the narrative’s spatial imaginary. In addition, the text of Raw Shark Texts itself is figured as a material body of language and textual image, with patterned connections running throughout the book.    Both the core observations listed above and the sub-arguments presented in the thesis were arrived at via a combination of Litmap-based and traditional print-based research methodologies. The current Litmap interface displays a map image of the Earth, with place names and corresponding information from each text keyed to that location’s coordinates on the map. In the case of Rings of Saturn, this allows for a fairly complex mapping since the nodes of the network in that narrative correspond to unambiguous geographical place names and locations. In the case of Özdamar’s and Hall’s texts, however, this becomes increasingly challenging as space and place become more subjective and fluid, requiring new and creative ways of visualizing data. The use of digital media to map literature is thus useful both for revealing what it can and can’t do, and I argue it is important to recognize both the strengths and constraints of the medium as we continue exploring this new area of research.  Moving forward, I plan to develop and extend the Litmap platform both in order to better address the crucial issues of how to visualize ambiguous data, and also to improve upon the existing functionality for searching, filtering, and browsing. The database underlying the current system is flexible and extensible enough to accommodate information from far more narratives, and I intend to enable users to upload other books for teaching and research. Once a large corpora of texts is uploaded, this will open up the ability to search across time and space and do other macro analyses of literature. Perhaps most of all, however, I look forward to making Litmap a truly collaborative project. My hope is to assemble a team of colleagues who will be invested in working together on creative technical and design solutions for the platform.  ",
        "article_title": "\"Litmap\": Networked Narratives",
        "authors": [
            {
                "given": " Barbara",
                "family": "Hui",
                "affiliation": [
                    {
                        "original_name": "UCLA, USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-22",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  You even have my field guide. It's you I love. I have believed so long in the magic of names and poems I hadn't thought them bodiless at all. Tall Buttercup. Wild Vetch. \"Often I am permitted to return to a meadow.\" It all seemed real to me last week. Words. You are the body of my world, root and flower, the brightness and surprise of birds. I miss you, love. Tell Leif you're the names of things.—Robert Hass, “Letter” It's volatile because anciently painted with wings in this manner whence came this character ☿ for mercury. — Sir Isaac Newton, “Praxis,” Babson Collection (Burndy Library Collection) MS. 420, Huntington Library   Digital humanities scholarship often integrates humanities scholarship (literary studies, historical studies, and so on) with technological research and development. Some of this technological work takes the form of standards development. The most noteworthy example of such standards development in the digital humanities community is the Text Encoding Initiative (TEI). The TEI provides Guidelines for encoding texts for scholarly and general use. The TEI is pervasive in digital humanities and digital library contexts. It is a de facto standard developed and evolved over the past twenty some years through the efforts of a number of dedicated scholars, librarians, and technologists, and with input from the larger community of TEI users. Another standard of significance to the digital humanities community is Unicode. Our paper presents a case-study of a successful effort to have included in the Unicode standard dozens of characters required by the Chymistry of Isaac Newton, an ongoing digital humanities project to digitize and edit, study and analyze the alchemical works of Isaac Newton and to develop various scholarly tools around the collection. Unicode has become the universal character encoding standard. Unicode is nothing more, as it is certainly nothing less, than a massive mapping of characters to numbers, a mapping that seeks to accommodate all the world’s languages and writing systems, including symbols of all sorts—mathematical symbols and operators, astronomical and astrological symbols, Zapf Dingbats, and many more. Operating systems, and the applications built upon them—databases, word processors and text editors, browsers, graphics software, and games—depend on such mappings, or encodings, to reliably reference, store, input, output, and display textual data. The Unicode Consortium’s “What is Unicode” page http://unicode.org/standard/WhatIsUnicode.html accurately reports the standard’s significance: Unicode is required by modern standards such as XML, Java, ECMAScript (JavaScript), LDAP, CORBA 3.0, WML, etc., and is the official way to implement ISO/IEC 10646. It is supported in many operating systems, all modern browsers, and many other products. The emergence of the Unicode Standard, and the availability of tools supporting it, are among the most significant recent global software technology trends.  In spite of Unicode’s impressive comprehensiveness, it does not include every character ever used. It does not at present, for instance, include many of the alchemical symbols found in Isaac Newton’s alchemical writings. Unicode provides a “private use area,” a series of reserved code points (the numbers assigned to characters) for projects and products to use “privately” for mapping to characters not represented in Unicode. A project like the Chymistry of Isaac Newton can make use of this private use area to map to characters that are not already described in the standard. A pitfall of the Private Use Area is that it is meant to be used privately; it is not suitable for easily interchangeable or interoperable data. One project’s implementation of the Private Use Area could conflict with another project’s. And fonts would not typically include characters for Private Use Area code points, since by their nature these codepoints are not assigned permanently to any one character but are perpetually open for private assignment, not as part of the public standard. So when a project stumbles upon a rich collection of important characters and symbols that are relevant and useful beyond the interior confines of one’s own project, one can make a significant scholarly contribution by documenting and describing these characters and proposing them for inclusion in the Unicode encoding standard. The alchemical symbols so common in Isaac Newton’s chymical manuscripts, are common also throughout manuscript and print alchemical literature. The graphically and semantically rich symbols also have potential utility in design, computer art, and even gaming applications. Even the few symbols that are potentially unique to Newton are worthy of consideration in the Unicode standard, given Newton’s stature as one of the giants of science and the vast wealth of scientific, historical, biographical, and popular literature related to Newton.  Figure 1. Basil Valentine. “A Table of Chymicall & Philosophicall Charecters with their signs.” The Last Will and Testament of Basil Valentine, 1671. These and other symbols are commonly found in Newton.   The process by which one moves a Unicode proposal through the development, review, and approval process is formal and rigorous. It is very rewarding in fostering a better understanding of one’s source material and in pointing the way to undiscovered or avoided basic research questions. To encode and identify characters and symbols, one must name the things, and naming is indeed a very difficult and powerful task, a task often challenged and enriched by puzzling ambiguity and obscurity. The process is very rewarding also because it is very much peer-reviewed. Our proposal greatly benefited from an iterative review and excellent advice, challenging questions, and constructive criticism from a number of very smart, helpful, interested experts serving on the Unicode Technical Committee (UTC). Our paper provides a case-study of one project’s navigation through the Unicode proposal, review, and approval process. We also provide a more theoretical discussion, illustration, and examination of the mutually beneficial relationship between technical standards development and basic humanities research.  ",
        "article_title": "“It’s Volatile”: Standards-Based Research & Research-Based Standards Development",
        "authors": [
            {
                "given": " John A.",
                "family": "Walsh",
                "affiliation": [
                    {
                        "original_name": "Indiana University USA",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            },
            {
                "given": " Wally",
                "family": "Hooper",
                "affiliation": [
                    {
                        "original_name": "Indiana University USA",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-28",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  This story begins in a small Sichuan village over fifteen years ago as a historian (John Flower) and an anthropologist (Pamela Leonard) began their study of the cultural landscape of a contemporary Chinese village. The story evolves as they strive to pioneer digital ethnography and later, in collaboration with The Institute for Advanced Technology in the Humanities (IATH), build interactive presentation of focused, long-term fieldwork research results in the form of an online monograph, media archive, and information repository, entitled Moral Landscape in a Sichuan Mountain Village: A Digital Ethnography of Place. The original and ongoing field study explores the histories, beliefs, livelihoods, and local identities in Xiakou Village, located in the mountains of Ya'an County, in western Sichuan Province of the People's Republic of China. The goal of the project is to understand Xiakou Village as an evolving cultural landscape, defined as the interwoven field of physical environment, historical memory, and moral agency, in which particular places gather a people’s sense of themselves and serve as sources of belonging and identity. This understanding attempts to establish a basis to consider questions such as: What does it mean to belong in a place? How do people understand who they are in terms of where they live? What is the relationship between history and place? How do memory and landscape inform the ways in which people define their communities? The ethnography uses the new possibilities of digital technology to create interleaving essays, primary source multimedia artifacts, and GIS maps. The purpose of this digital form is to render more transparent the relationship between source and interpretation, to open up non-linear narrative paths through the ethnography, and therefore to more vividly reveal the interconnections among different dimensions of village life that are the core content of the project. Indeed, we revisit the village study model to highlight the overlapping fields of interaction that link the village to broader regional, national, and even transnational identities. Another fundamental aim of the project is to reframe modern Chinese history away from the big narrative of the nation and toward local stories of the grassroots. How do the villagers of Xiakou understand their history? What memories and meanings from the past still animate their place, and how are they remembered and explained? Moral Landscape in a Sichuan Mountain Village is multidisciplinary, using the perspectives of history, anthropology, economics, folklore, and religion to try to understand the interconnected facets of life expressed in the village landscape. The common thread running through the ethnography is the idea that the landscape holds moral values. When people in Xiakou talked about place and history they were talking about what was good and bad, right and wrong. We understand digital ethnography to be an online interactive monograph with integrated archive and database. This digital format of the ethnography evokes an understanding of place through interactive essays that localize the broad trends of China's modern history in the lived experiences of Xiakou's villagers. The interactive essays are the project’s main narrative tissue, interconnected by a searchable archive of digital artifacts. These artifacts consist of multimedia information—photographs, scanned documents, audio and video recordings, GIS maps—contextualized in a thick setting of related metadata, and shared across essays. The project’s digital format is essential for realizing the rich potential of the ethnographic and historical content of the research: a central database and interconnected xml content enable the transparency, connectivity, and interactivity that comprise the key innovative characteristics of this form of narrative. Transparency means that the ethnography will reveal not simply \"what we know\" but also \"how we know it,\" by providing the reader access to primary source materials in the database. The architecture of the interactive interface will also use the database to encourage connections across thematic categories, making it possible for the reader to explore alternatives to a set, linear narrative.    Ways of belonging: new village studies and mapping the cultural landscape What is a village in China? A wide range of scholarship has addressed this central question, from the perspectives of regional systems analysis (Skinner 1964) to cultural landscape studies (Knapp 1992, Feuchtwang 1997). Our approach tries to give priority to villagers' conscious representations, analyses, and understandings of their relationship to \"their place\". The resulting geographical scope goes beyond the village itself to encompass the communities along the North Road and, under some conditions, extends to include the broader eight county Ya’an region. In Moral Landscape in a Sichuan Mountain Village we advocate a return to the ethnographic tradition of village studies, but using new tools of the digital humanities that emphasize the ways in which place is not simply a fixed and unchanging location, but rather a nexus of evolving relationships and historical connections to other places. Thus, one of our goals is to highlight the multiple, overlapping fields of interaction that link the village to broader regional, national, and even transnational identities. We see our project as complementary to the much larger and comprehensive initiatives that aim to create complete datasets, such as the China historical GIS project (Bol 2006). In contrast, our project does not attempt to be comprehensive, but rather celebrates the particularity of place. We hope that our qualitative interpretation of landscape will provide the kind of unique local portrait of place from which comprehensive projects can create a more vivid broad tableau of China as whole.   Beyond Revolution: an inductive approach to local history Another fundamental goal of our project’s landscape approach is to reframe modern Chinese history away from the master narrative of the nation and toward local stories of the grassroots (Duara, Prazniak). How do the villagers of Xiakou understand their history? What memories and meanings from the past still animate their place, and how are they remembered and explained? How does that local understanding of history reiterate or differ from historical narratives based on the nation-state, China, as subject? While there are excellent village-based histories (e.g. Chan, Madsen, Unger 1992; Selden, Friedman, Pickowicz 1991) that focus on the local impact of national events, particularly the Chinese revolution, in Moral Landscape in a Sichuan Mountain Village we try to adopt a more localized, inductive approach. The historical scope of our project thus largely corresponds to the way villagers mark the turning points in their past, based on their personal experiences in local places and marking events that fall within their horizon of memory. Methodologically, we understand that the essays and artifacts represent our synthesis of a dialogue with local villagers and with local historical source-materials on the topic of social and environmental change. In confronting the subjective reality of fieldwork and analysis, anthropologists have emphasized the need to be transparent in presenting the politics of the research encounter. We believe digital technology allows us to go further in meeting this aim.    History, environment, and agency in the moral landscape In trying to understand the significance of the environmental changes that have taken place in this valley, we frame issues of environment and economic development within local cultural practices and historical knowledge. How do local people draw on their historical understanding of place in adapting to economic development policies introduced from outside? How do those development policies in turn influence their livelihoods, and change their understanding of the landscape?   Structure, content, and logic of the digital ethnography The structure of the ethnography’s online monograph comprises eight chapters: History, Landscape, Belief, Folklife, Authority, Work, Gazetteer, and Biography. Chapters are not airtight divisions, but rather groupings that highlight the dominant themes of the essays within them. There are three main types of content within this chapter structure: essays, interactive maps, and artifacts. Essays are the basic interpretive building blocks of the ethnography and are accessed through the chapters. The interactive maps under the Gazetteer chapter will offer spatial representations of sites in the cultural landscape, dynamically presented through GIS layers, sorted by kind and historical period. Both the maps and essays are illustrated and documented by \"artifacts\", i.e., foci of evidence that link to multimedia content—photographs, video and audio recordings, image maps, diagrams, supplemental texts, primary source documents, and field notes. The artifact frames this multimedia content within supplementary metadata and highlights thematic overlaps and interconnections within the ethnography. The essay/artifact structure allows us to experiment with different approaches to conceptualizing and presenting the ethnographic research. These artifact-centered essays are intentional inversions of the more familiar text-driven narrative presentation, and they point the way to readers who want to engage the ethnography more interactively. To enable that level of engagement, our goal is to code each artifact and each essay subsection with selections from a finite set of keywords, making the whole site fully searchable through the site’s integrated information structures.    Proposed Presentation We will discuss the information structures in which the base materials are created and maintained. Then we discuss the interactive interface through which those materials are accessed by scholars and the general public. Finally, we will justify our claim that these techniques embody the methodologies expressed above.  ",
        "article_title": "Xiakou: A Case Study in Digital Ethnography",
        "authors": [
            {
                "given": " John",
                "family": "Flower",
                "affiliation": [
                    {
                        "original_name": "Sidwell School, Washington, D.C USA",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Pamela",
                "family": "Leonard",
                "affiliation": [
                    {
                        "original_name": "Independent Scholar",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Worthy",
                "family": "Martin",
                "affiliation": [
                    {
                        "original_name": " Institute for Advanced Technology in the Humanities, University of Virginia USA",
                        "normalized_name": "University of Virginia",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0153tk833",
                            "GRID": "grid.27755.32"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-05-01",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "  The promise of eLexicography stems not only from the transformation of the production medium, but also from the technological feasibility of representing linguistic complexity. Even though modern lexicography is unimaginable without computer technology (Hockey, 2000a; Knowles, 1989; Meijs, 1992), the sheer use of computers in producing a dictionary or delivering it electronically does not automatically transform a dictionary from \"a simple artefact\" to a \"more complex lexical architecture,\" to use Sinclair's (2000) formulations. Calling dictionaries “simple artefacts” is itself a rhetorical oversimplification: there is certainly nothing simple about a dictionary — whether we look at it as a material object, cultural product or a model of language. Yet the overall structure of dictionaries as extended word lists has not changed in centuries (Hausmann et al., 1989; Fontenelle, 2008; Atkins and Rundell, 2008). Admittedly, a great deal of factual information is packed into a prototypical lexicographic entry, but a defined term often remains in isolation and insufficiently connected or embedded into the language system as a whole. This is what Miller refers to as the “woeful incompleteness” (Miller at al.) of a traditional dictionary entry, and what Shvedova sees as its “paradoxical nature” — dictionary entries tend to be “lexicocentric” while language itself is “class-centric” (Шведова, 1988). Furthermore, the advances in digital humanities, textual studies and postmodern literary theory do not seem to have had a profound effect on the way we theorize or produce dictionaries. Surely, many important lexicographic projects have been digitalized and gone online; web-portals increasingly offer cumulative searches across different dictionaries; and eLexicography is a thriving field (Lemberg et al., 2001; Hockey, 2000a; de Schryver; Hass, 2005; Nielsen, 2009; Rundell, 2009; Hass, 2005), yet dictionaries — often commercial enterprises which are guided by predominantly economic concerns — remain by far and large discrete objects: no more and no less than digitalized versions of stable, print editions. We still consult dictionaries by going to a particular web site. Dictionaries do not come to us. The time is ripe to ask — both in theoretical and practical terms — a new set of questions: how has the electronic text changed our notion of what a dictionary is (and ought to be); how have the methods of digital humanities and the advances made in digital libraries altered our idea of what a dictionary can (and should) do? And, finally, where do we go from here? The dictionary is a kind of text. In print culture, the dictionary, like every other text, had its material and semantic dimension. The semantic dimension was represented on its visible surface, whereas its depth was in the mind of the reader, or what Eco refers to as the \"encyclopedia of the reader.\" (Eco et al., 1992; Eco, 1979). Yet if we — as we should — start thinking of the dictionary as a kind of electronic text, the way Kathrine Hayles and others have done for electronic literature, we will have no choice but to strip the dictionary of its finality and its \"object-ness\" and see in it, instead, only one possible manifestation of the database in which it is stored (Hayles, 2003; Hayles, 2006; Folsom, 2007). A digital text can be not only edited, transformed, cut and pasted — as part of our computational textual kinetics — but is always part of other activities: search, downloading, surfing. In other words, an electronic text is unimaginable without its context (Aarseth, 1997; DeRose et al., 1990; Hockey, 2000b). The dictionary, then, should be seen as a kind of semantic potential that can be realized through its use. But in order to truly fulfill this potential, the dictionary needs to be embedded in the digital flow of our textual production and reception. That is why we cannot think of dictionaries any more without thinking about digital libraries and the status which electronic texts have in them (Andrews and Law, 2004; Candela et al., 2007; Kruk and McDaniel, 2009; Maness, 2006; Miller, 2005; Novotny, 2006). To be truly useful for any kind of textual studies, the digital library must \"explode\" the text (by providing full-content searchability, concordances and indexes, metadata, hyperlinks, critical markup etc.) instead of \"freezing\" it as an image, which, albeit digital, is computationally neither intelligible nor modifiable as text. In smart digital libraries, a text should not only be an object but a service; not a static entity but an interactive method (Tasovac, forthcoming). The text should be computationally exploitable so that it can be sampled and used, not simply reproduced in its entirety. This kind of atomic approach to textuality poses a host of challenges (legal, ethical, technical and intellectual, to name just a few), but it opens up the possibility of creative engagement with the digital text in literary studies (text mining, statistical text comparison, data visualization, hypertextual systems etc.). The consequence of this \"explosive\" nature of the electronic text is of paramount importance for eLexcicography and the reformulation of the dictionary not as an object, but a service. We should start thinking of and building dictionaries as fully embeddable modules in digital libraries, or, to put it differently, build digital libraries which integrate dictionaries as part of their fundamental infrastructure and allow an ever-expandable process of associating words in an electronic text with an equally changeable record in a textual database. The changeability of the dictionary entry will, in turn, defer ad infinitum the notion of a particular dictionary edition — other than as temporary snapshot of the database. The dictionary as an evolving process will be in a permanent beta state. The future of electronic dictionaries undoubtedly lies in their detachability from physical media (CD, DVD, desktop applications) and static locations (web portals). If we think of the dictionary as a service with an API The first publicly available dictionary application programming interface was made available by the Wordnik project in October 2009. See http://api.wordnik.com/signup/. that can be called from any Web page, we can actually start thinking about any (electronic) text as a direct entry point to the dictionary. If every word in a digital library is a link to a particular entry in the dictionary, electronic textuality as such becomes an extension of lexicography: the text begins to contain the dictionary in the same way that the dictionary contains the text. The Center for Digital Humanities (Belgrade, Serbia) is putting these theoretical considerations into practice while working on its flagship Transpoetika Project (Tasovac, 2009). Transpoetika (see Figure 1)  is a collaborative, class-centric, bilingualized Serbian-English learner‘s dictionary based on the architecturally complex, machine-readable semantic network of the Princeton Wordnet (Fellbaum, 1998; Vossen, 1998; Stamou et al., 2002; Tufis et al., 2004). It is part of a scalable, web-based, digital framework for editing and publishing annotated, fully-glossed study editions of literary works in the Serbian language, primarily aimed and students of Serbian as a second or inherited language. Transpoetika has been designed to be deployed as a web service and therefore linked from and applied to a variety of textual sources online. Portions of the project, such as the Serbian Morpho-Syntactic Database (SMS) already function as a web service internally and will also be made public and free once the sufficient funding for the project has been secured. Transpoetika can also interact with other web services: by using Flickr as a source of illustrations, and Twitter as a source of \"live quotes\" in the entries, the Transpoetika Dictionary explores the role of serendipity in a lexicographic text. The overarching goal of the Belgrade Center for Digital Humanities (CDHN) is to produce a pluggable, service-based, meta-lexicographic platform for the Serbian language, which will interact with various Web-based digital libraries, and contain not only our own bilingualized Serbian Wordnet, but also historical Serbian dictionaries that the CDHN is digitalizing, such as, for instance, the classic Serbian-German-Latin Dictionary by Vuk Stefanović-Karadžić (1818 and 1852). The platform could, in theory, be extended to include and consolidate a number of other, more specialized, lexicons. This is, in any case, the general direction we would like to take. I would like to conclude with a hysteron-proteron, which, in Samuel Johnson's Dictionary of the English language was defined as \"a rhetorical figure: when that is last said, which was first done.\" From the very beginning of this paper, I spoke of the dictionary, which every careful reader would have marked as a serious lexicographic faux-pax. There is and never was such a thing as a singular and uniquely authoritative source of information about words and their meanings. There is no such thing as the (Platonic, ideal) dictionary but rather a myriad manifestations of its imagined hypertextual prototype. I believe, nonetheless, that we should, in the digital age and with the ongoing developments of the digital humanities, reclaim the dusty notion of the dictionary and boldly, though not without self-irony, keep trying to imagine what that \"thing\" — the dictionary — could be. If only with the goal of making it — in its traditional, leather-bound, sense — completely obsolete.   Figure 1     ",
        "article_title": "Reimagining the Dictionary, or Why Lexicography Needs Digital Humanities",
        "authors": [
            {
                "given": " Toma",
                "family": "Tasovac",
                "affiliation": [
                    {
                        "original_name": "Center for Digtial Humanities (Belgrade), Serbia",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-04-25",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " On March 18th, 2009 over 90 people participated in a collaborative documentation project called A Day in the Life of Digital Humanities. The participants blogged what they did that day in the spirit of digital humanities as a form of autoethnography that could help answer the question, \"just what do we do?\" In this paper we will:  Discuss the conception, design and delivery of the project, Discuss the intellectual property paradigm that we adopted to make this project one that produces open documentation for use by other projects, Reflect on the lessons learned about such social research projects and theorize the exercise, and Discuss the second Day of DH project that, building on the strengths of the first, will be run March 18th, 2010.    From Conception to Delivery The original idea for the project was to develop a communal response to questions asking exactly what it is that we do in the digital humanities. In 2006, \"The State of Science & Technology in Canada\" from the Council of Canadian Academies reported humanities computing as an emerging field of strength in Canada. Since then, there have been requests in various forms for an explanation of what the previously unnoticed field was.Council of Canadian Academies, \"The State of Science & Technology in Canada (Summary and Main Findings)\", 2006. . The form of the response was inspired by a lecture by Edward L. Ayers (currently now President of the University of Richmond) that we had heard about, titled \"What Does a Professor Do All Day, Anyway?\" Ayers was an early computing historian whose \"The Valley of the Shadow\" project was one of the two founding IATH projects. In that lecture, he reflected on how people, including his own young son, know little about what a professor does. As he put it, \"In the eyes of most folks, a professor either portentously and pompously lectures people from his narrow shaft of specialized knowledge, or is a bookworm – nose stuck in a dusty volume, oblivious to the world.\"Ayers, Edward J. \"What Does a Professor Do All Day, Anyway?\" Lecture given in 1993 at the University of Virginia on receiving the \"Teacher of the Year\" award at the Fall Convocation and Parents' Weekend.  The situation is even worse in the digital humanities, where not only do people not know what we do as academics, they also don't know what \"humanities computing\" or the \"digital humanities\" are. It's not even clear if practitioners agree with each other on these basic matters. Ayers's approach to answering this question was the simplest and most cohesive: simply to describe each part of his day, task by task. A Day in the Life of Digital Humanities scales this approach up to a participatory project. We hoped to address the questions about the nature of digital humanities academic work by reflecting as a community. The Day of DH (as we call it) was thus conceived to provide one form of response to the definition of the field: not through speculation, but through observation. In this context we will also briefly demonstrate the WordPress setup and the wiki that was used to coordinate materials.Day in the Life of Digital Humanities wiki.    Intellectual Property Paradigm: Collaborative Publishing As for all projects with human participants in Canadian academia, we first had to apply for ethics review. We presented the project not simply as a study of what the participants are doing, but as a collaborative publication. The paradigm therefore was that we were organizing a collective documentation project where the results would be a form of publication that would be returned to the community for further study. Some participants went so far as to run visualization tools on the RSS feed of all the entries as they were being posted, thus returning a feed of the posts live to participants, which allowed study to happen as the day proceeded. One of the problems we encountered was cleaning up the data after the day. The cleaning up of the data involved four broad steps:  To comply with ethics, we had to go through and edit (with the participants) the images posted to make sure the images conformed to the ethics regimen we agreed to. We read and indexed the posts with a uniform set of terms, helping draw out semantic relevance in the data.See  for the category tags we used. These were developed iteratively going through the data. We converted the XML output from the native WordPress format to a more tractable form. Irrelevant fields were removed and content was unescaped, requiring additional editing toward well-formedness. The final cleaned dataset is being review by project participants with notable experience with markup. Finally, we proofed the entire dataset also deleted empty comments. However, in order to preserve the authenticity of the posts, we did not change the prose of the participants.     Crowdsourcing in the Digital Humanities The Day in the Life of Digital Humanities is a modest example of a collaborative \"crowdsourcing\" project. It is not the first such project in the humanities. For instance, Suda On Line is an excellent example of how a \"crowd\" can participate in a larger project.Mahoney, Anne. \"Tachypaedia Byzantina: The Suda On Line as Collaborative Encyclopedia.\" Digital Humanities Quarterly, V. 3, N. 1. Winter 2009.  Reflecting on the level of participation in the Day of DH, we believe that some of the strategies we adopted to encourage participation were successful:  A participant's required contribution was limited to only one day of posting. We hypothesize that if small, flexible tasks contribute to broad participation. We did not assume people would participate. Instead we invited people personally, creating a personal connection before issuing an open call for participation. We believe that the personal human contact makes a real difference in explaining to people why they would want to participate. The project was structured as a collaborative publication so that participants could get credit for their work and use the results in their own experiments. We tried to make the idea simple to grasp, which is why we chose the \"Day in the Life of\" title. The title gives the prospective participant an idea of the level of participation and the results. A steady but light feed of updates was maintained through a discussion list. We sent about an e-mail a week to keep in touch as the day approached. Human contact and communication are essential at all levels - participants are, after all, volunteering their effort to make the project work. For that reason we had a number of people assigned to answer different types of questions quickly, and we spent some time developing online materials to help explain the project and connect people. The technology used by participants was reasonably familiar and worked.     Reflections and Theory What then have we learned about the digital humanities from the project? To some extent the project speaks for itself. The project doesn't provide a short answer to questions about what we do. Instead it provides a wealth of detail and reflections. Nonetheless we do have some conclusions based on readings of the entries:  Many who do computing in the humanities feel isolated and welcome venues for participating in larger concerns. This project gave some of those isolated a way to feel part of a peer community and to be heard. In Humanities research, there is often an inverse relationship between depth and breadth. At their most qualitative and meticulous, humanists may spend years analyzing a short text. To broaden the corpus often necessitates a colder, more mechanical approach to the data. Though perhaps at the expense of structure, the format of Day of DH has resulted in content that is both deep and broad. Community projects don't simply document an existing community - to some extent they create it. This is an age-old pattern where a community, in becoming, presents itself as already mature. One participant told us that they were thinking of running something similar at their university as a community-building exercise. While the data is not necessarily an objective representation of what we typically do (if there is such a thing) it is representative of what some of us think about what we do. One aim of the Day was to explore the usefulness of autoethnography as a methodology for studying the digital humanities. Nicholas Holt defines autoethnography as a \"writing practice [involving] highly personalized accounts where authors draw on their own experiences to extend understanding of a particular discipline or culture\".Holt, Nicholas. \"Representation, Legitimation, and Autoethnography: An Autoethnographic.\" International Journal of Qualitative Methods 2 (2003): 1-22. Page 1.  This reflexive study of the participant-researcher’s own role in a greater culture thus has created a dataset far richer and more complex that would have otherwise been available if digital humanists had been given a set of parameters, such as a questionnaire, in which to define themselves. Willard McCarty proposes that we think of our practice as one of modeling where we are both modeling as a process of exploration and creating models that represent interpretative process.McCarty, Willard. 2005. Humanities Computing. Palgrave MacMillan: Basingstoke. This project can be thought of a collaborative modeling of the field where for one day we used some of our own tools and new methods to think about our research in community.   Further observations we leave for you; after all, the point was to leave the community data stream to think about and with.   The Second Day of DH On March 18th, 2010 we plan to run the Second Day of Digital Humanities. This second project will try to address some of the limitations of the first:  We hope to invite more graduate students to participate. Students appeared resistant to the idea that they had any meaningful contribution to make. One participant, Domenico Fiormonte, engaged his students by having them comment on his posts, an approach we will encourage others to do. Another alternative is to encourage students to share a single blog so they don't feel they have to write more than one post. We hope to involve more international participants outside Anglophone regions. In particular we hope to involve more Francophone participants in Quebec, but we also plan to invite participants from a broader range of regions and provide them with support early so they feel comfortable posting. We hope to find a technology for posting that outputs clean XML without forcing participants to learn markup. The technology will be chosen in conjunction with participants and may be hosted by a participating centre. We hope to encourage use of a common set of categories built on those we used for the post-day tagging. We plan to better incorporate micro-blogging (Twitter) so that participants could use that technology as an alternative.     Conclusion There are a couple of different lenses that might be appropriate to the discussion of the Day of DH. First, it can be seen as an exercise by the participants and the larger community in building social capital. Bourdieu's work on social capital emphasizes both the actual and potential resources available to the individual through participation in a network.Bourdieu Pierre. 1985. «The forms of capital.» In Handbook of Theory and Research for the Sociology of Education. Ed. J. G. Richardson. New York: Greenwood, pp. 241-58. Coleman focuses on the potential benefits to the individual.Coleman, James S. “Social Capital in the Creation of Human Capital.” American Journal of Sociology. Volume 94, Number S1: S95. January 1988. Supplement. DOI: 10.1086/228943.Putnam highlights the value of social capital to the community, equating community participation with civic virtue.Putnam, Robert D. 2000. Bowling Alone: The Collapse and Revival of American Community. New York: Simon and Schuster.Individuals involved in the DDH have had an opportunity to increase, extend, or consolidate existing social capital through self-revelation within the framework of the day. The DH community in the larger sense has had a moment of opportunity for critical self-reflection. The second possible lens deals primarily with that possibility for self-reflection. Much as every design can be read as a comment on the act of designing and the discipline of design, or every building as a contribution to the ongoing discussion of architecture, so DDH provides a moment of self-directed reflection on what it means to be a digital humanist in a world where other digital humanists are also active.  ",
        "article_title": "A Day in the Life of Digital Humanities",
        "authors": [
            {
                "given": " Geoffrey",
                "family": "Rockwell",
                "affiliation": [
                    {
                        "original_name": "Philosophy and Humanities Computing, University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": " Stan",
                "family": "Ruecker",
                "affiliation": [
                    {
                        "original_name": "English and Humanities Computing, University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": " Peter",
                "family": "Organisciak",
                "affiliation": [
                    {
                        "original_name": "Humanities Computing, University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": " Megan",
                "family": "Meredith-Lobay",
                "affiliation": [
                    {
                        "original_name": "Arts Resource Centre, University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": " Ranaweeram",
                "family": " Kamal",
                "affiliation": [
                    {
                        "original_name": "Arts Resource Centre, University of Alberta, Canada",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            },
            {
                "given": " Stéfan",
                "family": "Sinclair",
                "affiliation": [
                    {
                        "original_name": "McMaster University, Canada",
                        "normalized_name": "McMaster University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02fa3aq29",
                            "GRID": "grid.25073.33"
                        }
                    }
                ]
            }
        ],
        "publisher": "Centre for Computing in the Humanities, King's College London",
        "date": "2010-05-01",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    }
]