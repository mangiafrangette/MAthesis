[{"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "The UCLA Encyclopedia of Egyptology (UEE) is a digitally born online publication which provides users with several interfaces to access and reproduce content. Supported by several grants from the National Endowment for the Humanities, the UEE is a highly mediated, peer reviewed information resource on Egyptian history, art, archaeology, geography and language, in which authors selected by an editorial committee are commissioned to write substantial articles with thorough bibliographies and web links. Articles will be regularly updated and previous versions of the text and other assets will remain available throughout the lifetime of the resource, which in principle is built for digital eternity. This time scale may prove not to be of the same longevity as the preservation of ancient Egyptian cultural heritage, but is nevertheless fitting the mindset of scholars who routinely deal with objects of 4000 years old.The UEE is an English language resource, while all head words or entry titles are translated also in Arabic, French and German. The English abstracts are also translated in Arabic and a standard feature of each article. The content of the UEE is available in two forms: the Open Version makes use of eScholarschip, the online publication platform of the University of California. Articles are presented in alphabetic order of the titles, and can be downloaded as PDF (http://escholarship.org/uc/nelc_uee). The UEE Full Version provides a much more sophisticated platform, where users can access information through a wide range of searches, either based on the underlying subject structure, article links, metadata, or through an interactive time-map, which provides access to articles, images and 3D VR reconstructions which refer to the same area, the same time period or both. The granularity of the time map encompasses regions (using either modern or ancient subdivisions), ancient sites, or particular features of the latter, such as a specific gate way, or altar. At present the Full Version is not yet publicly online, but will be moved from development to production in the near future. Information on the URL will be provided on the UEE project development website at http://uee.ucla.edu.The presentation will focus on the many lessons learned while developing the project, including the workflow of all the tasks which are literally performed “behind the screen”. Since this is an international project with editors in the United States, Canada, France, Belgium, Great Britain, and Egypt, with authors as well as peer reviewers from all over the world, the project relies on a large number of disparate web services, which are partly for free, partly subscription based. The commissioning phase is tracked through a number of online spread sheets through Google Docs. Authors receive an invitation by email, which provides them with the scope of the entry, a document with clear indications of what should (not) be included, in order to avoid duplication with related articles. Once an article has been submitted, the peer review platform provided by eScholarship, is used, which enables tracking and automated prodding for authors and peer reviewers. Since many of the authors are non-native speakers of English, the UEE offers a substantive copy-editing service, which streamlines the terminology, spelling and links of the articles. The project coordination, which involves communication on the progress of the extensive mark-up in TEI which is the next phase of bringing an article to online publication, makes use of the commercial project management software BaseCamp (http://37signals.com/).An important point of discussion is the digital and financial sustainability and the different solutions the UEE has proposed to enable the project to expand and be constantly renewed, which is perhaps the greatest asset of an online resource. The history of the venerable printed predecessor of the UEE, the Lexikon der Ägyptologie (published from , shows that bibliographies are outdated in five years after publication, while Egyptological scholarship begins to be outdated in approximately 20 years. Authors are therefore asked to provide twice an update of their article, and after that potentially a new entry will be created, because the development of the discipline is not only reflected in article content, but also in the structure of the resource, the selection of entry titles, and the emphasis on particular sub fields. The strict version control has, therefore, an added benefit: over the course of time the UEE will become effectively a history of Egyptological thought and methodology.This requires, however, that the digital content remains available, and that the editorial process will keep on running, two very demanding conditions. As for digital stability: all assets of the UEE are housed in the UCLA Digital Library, and are accessed from a front-end server which is at present housed at UCLA’s Academic Technology Services. This agreement does not necessarily guarantee digital preservation, because the libraries, at the forefront of digital repository preservation as they are, are also faced with the enormous costs and practical problems of digital preservation. The awareness of the problem is, however, a considerable part of the solution. The financial sustainability is an enormous conundrum. Users have come to expect free, high quality content, academics celebrate the virtue of open access, and yet to build a high quality resource comes with very real costs that need to be covered. The presentation will outline some of the avenues explored to ensure that the UEE will have a long prosperous life.", "article_title": "The UCLA Encyclopedia of Egyptology: Lessons Learned", "authors": [{"given": "Wendrich", "family": "Willeke", "affiliation": [{"original_name": "UCLA", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Interdisciplinary and multidisciplinary projects are becoming an important part of academic life. Research questions are becoming more complex and sophisticated, requiring a team approach to address (Newell et al., 2000, Hara et al., 2003). At the same time, funding agencies are also encouraging these types of projects through their granting programs. One result of this increased level of research collaboration is that teams have members located at other institutions, whether nationally or internationally. Digital Humanities as a community is becoming increasingly international in focus. For example, the Digging into Data Challenge was jointly sponsored by Canada’s Social Sciences and Humanities Research Council (SSHRC), the British Joint Information Systems Committee (JISC) and the American National Endowment for the Humanities (NEH) and National Science Foundation (NSF) and required each research team to have membership from at least two of the participating countries (Office of Digital Humanities, 2010). Further, the Digital Humanities Summer Institute at the University of Victoria has had participants from every continent, except the Antarctic. Finally, the Text Encoding Initiative Consortium has members and users from around the world, with a growing number of projects based in Asia (Siemens, 2008c). These collaborations are possible given the advances in computers and telecommunications. The recruitment of the right person is no longer limited by geography (Cramton et al., 2005).However, these types of teams encounter the general challenges associated with collaborative work, but also with more specific ones relating to the geographical distribution of members. Despite this increasing use of research teams, protocols to prepare individual researchers to work as part of a team, particularly within those groups which cross language, culture, and country lines, are not widely developed. Our work is designed to identify effective work patterns and intra-team relationships and describe the support and research preparation that is required to develop international research collaborations. The results will enable those who work in such teams to recognize those factors that tend to predispose them to success, and perhaps more importantly, to avoid those that may lead to problematic interactions, and thus make the project less successful than it might have otherwise been.While there is a growing knowledge base of the benefits and challenges inherent in academic research teams (See, for example: Bracken et al., 2006, Choi et al., 2007, Fiore, 2008, Kraut et al., 1987), little knowledge exists about the ways in which teams with memberships across universities and disciplines work together (Garland et al., 2006), much less about teams with members from multiple country, culture and language groups (Setlock et al., 2004, Shore et al., 2005).At a practical level, geography presents relatively simple challenges. As time zones increase, the flexibility in scheduling meetings decreases while the cost of face-to-face interactions increases. Technology may not always be capable of overcoming these challenges. Distance between team members limits the amount of interaction needed for creativity and innovation (Cummings et al., 2005, Lawrence, 2006). Further, some technologies and software may not be available in some countries due to infrastructure gaps or government policy (for example, the Great Firewall of China).At a more complex level, differences in culture and language may impact on various aspects of team work such as structure, management, communication, conflict expression and resolution, decision making, and appropriate team behaviour and may be further complicated by professional and academic cultural differences (Dafoulas, 2002, Setlock et al., 2004, Shachaf et al., 2007, Fry et al., 2007, Lee-Kelley et al., 2008, Dekker et al., 2008). At a very basic level, teams must decide a working language, a decision that may be political in nature (Butler, 1998, Deepwell et al., 2009, Bournois et al., 1998). And even with a common language, members may find that they must still translate terms. For example, institutions in different countries define a research assistant (RA) in a variety of ways. In Canada, an RA is generally a graduate student who works on a research project on a part-time basis while in the United Kingdom, an RA is a post-doctoral fellow who is on a full-time contract for a specified period of time. As a result, confusion can occur among team members when they use common terms in different ways.Research suggests several factors that may minimize the impact of the above challenges. First, education and training may mitigate the impact of cultural differences because team members may share professional norms. University education is fairly similar across countries (Dafoulas, 2002, Lee-Kelley et al., 2008, Nason et al., 1998). Teams may also find it beneficial to spend time in members’ cultures to create understanding of differences and similarities (Nason et al., 1998, Bagshaw et al., 2007). Finally, teams may also consider creating a cultural profile, both by country and professional/discipline culture (Dafoulas, 2002, Zakaria et al., 2008). This profile can be combined with team norms to express understandings of time, deadlines, language, communication channels, conflict resolution mechanisms, and other issues (Saunders et al., 2004). However, more research is needed to understand the supports and research preparation that is needed to ensure effective collaborations in teams with memberships from many countries, cultures, and language groups.This paper is part of a larger project examining research teams with multi-country, culture and language representation, led by a team based in Canada, United Kingdom and Germany (For more details, see Siemens, 2010). The larger study uses a combination of data collection methods including an ethnomethodological approach with diary/log studies of research teams in the midst of their collaboration (Garfinkel, 1984) and semi-structured interviews with individuals who have experiences in the types of teams under investigation (Marshall et al., 1999). This paper will report on the findings from the interviews.Digital Humanities community will serve as an exemplar case study population given the community’s international focus and collaborative networks. To achieve their objectives, and because of the variety of skills and expertise that these projects require, DH researchers must work collaboratively both within their institutions and with others nationally and internationally and often across disciplines. Team members include humanities, social science and computer science scholars, undergraduate and graduate students, research assistants, computer programmers and developers, librarians, and others. At present, several research projects involving national and international teams with funding ranging from thousands to millions of dollars are already in place with others in development. In addition, several DH research centres have faculty and research staff drawn from several countries, creating a mix of languages and cultures.This research project builds on earlier work on DH research teams presented at previous digital humanities conferences (Siemens, 2008a, Siemens, 2008b, Siemens et al., 2009a, Siemens et al., 2010).At the time of writing this proposal, interviews are being conducted and data analysis completed.The benefits to the Digital Humanities community will be several. First, the study contributes to an explicit description of the community’s work patterns and relationships, particularly as the Digital Humanities community continues to become international in focus (Alliance of Digital Humanities Organizations, 2010, Tei-C, 2010). This research demonstrates that much of digital humanities research is accomplished within interdisciplinary research teams, which are developing tools and processes to facilitate this collaboration. One particular issue highlighted in this research relates to challenges experienced within teams with members from various countries and cultures (Siemens et al., 2009b). Second, it is designed to enable those who work in such teams to recognise factors that tend to predispose them to success, and perhaps more importantly, to avoid those that may lead to problematic interactions, and thus make the project less successful than it might otherwise have been.", "article_title": "A Trip Around the World: Balancing Geographical Diversity in Academic Research Teams", "authors": [{"given": "Lynne", "family": "Siemens", "affiliation": [{"original_name": "School of Public Administration, University of Victoria", "normalized_name": "University of Victoria", "country": "Canada", "identifiers": {"ror": "https://ror.org/04s5mat29", "GRID": "grid.143640.4"}}]}, {"given": "Elisabeth", "family": "Burr", "affiliation": [{"original_name": "Institute of Romance Studies, University of Leipzig", "normalized_name": "Leipzig University", "country": "Germany", "identifiers": {"ror": "https://ror.org/03s7gtk40", "GRID": "grid.9647.c"}}]}, {"given": "Richard", "family": "Cunningham", "affiliation": [{"original_name": "Acadia Digital Culture Observatory, Acadia University", "normalized_name": "Acadia University", "country": "Canada", "identifiers": {"ror": "https://ror.org/00839we02", "GRID": "grid.411959.1"}}]}, {"given": "Wendy", "family": "Duff", "affiliation": [{"original_name": "Faculty of Information, University of Toronto", "normalized_name": "University of Toronto", "country": "Canada", "identifiers": {"ror": "https://ror.org/03dbr7087", "GRID": "grid.17063.33"}}]}, {"given": "Dominic", "family": "Forest", "affiliation": [{"original_name": "École de bibliothéconomie et des sciences de l'information, Université de Montréal", "normalized_name": "University of Montreal", "country": "Canada", "identifiers": {"ror": "https://ror.org/0161xgx34", "GRID": "grid.14848.31"}}]}, {"given": "Claire", "family": "Warwick", "affiliation": [{"original_name": "UCL Centre for Digital Humanities, Department of Information Studies, University College London", "normalized_name": "University College London", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/02jx3x895", "GRID": "grid.83440.3b"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "A complex network is usually conceived of in the form of a graph consisting of nodes representing individual, or atomic, entities and edges linking them according to information about semantic attributes or some weighting value. This intellectual intuition can be applied to the world of language where a sign makes sense through the concurrent presence of other signs (Saussure, 1916). In particular, the semantic aspect of a document, as a group of more or less coherent sentences, depends on co-occurrence information about the words being bound together to form topics. When a content-bearing word is found within a neighborhood of other words, one may assume that they produce a constellation of meaning (Schütze, 1997; Takayama, 1999). Conceptual interrelatedness can be represented in a graph form called a semantic network. Similarly, in the field of digital humanities, the graph-based approach to document analysis is becoming a major research trend (Miyake, 2009).In semantic networks, graph coefficients are useful for examining the features of language data, such as a document or corpus, from the perspective of meaning. Hubs with the highest degree values can be regarded as being as key words (excluding functional words), that are highly involved in producing the document context. In normal cases, the bias of degree distributions to follow a power law (or, more concretely, Zipf’s law) has been handled by the concept of scale-free (Barabási et al., 1999) for complex networks. However, one graph index that researchers are increasingly paying close attention to is the concept of ‘intrinsic weights’ that are not distributed to edges but to vertices (Caldarelli et al., 2002; Boguñá et al., 2003; Masuda et al, 2004). The emergence of this weighting within some network settings certainly leads a situation where vertex degree generates a phenomena known as the ‘Rich Club’ (Zhou et al, 2004; Colizza et al., 2006), consisting of hubs with high intrinsic weights. However Masuda et al. (2006) have revealed a contrastive kind of subgraph area, which they figuratively refer to as a ‘VIP Club’, where vertices with similar weighting values are exclusively connected to form a confined circle of privileged elites. This “homophily” tendency (Axelrod, 1997; Barrat et al., 2004; Centola et al.2007) makes the average shortest path length significantly longer, so that the innermost graph area becomes harder to access, forming a preserve for the so-called ‘masterminds’ (Masuda et al., 2006).These elite entities may be, in some sense, regarded as ‘hidden keywords’ within complex networks, where all the words are interconnected—at dense intervals—from a paradigmatic perspective (Jacobson, 1963). These mastermind words are unquestionably of moderately high frequency or degree, but they are tightly related to one another by ‘homophilous’ ties to form very important but discreet lexical patterns. The term homophilous here means a significantly high value of ‘degree correlation’ (Boguñá et al, 2003; Boccaletti et al., 2006), which serves as a marker for the VIP-Club phenomenon (See Figure I). These mastermind words, which are relatively difficult to retrieve during comprehension within the reading process, sometimes play a crucial role in producing well-calculated subliminal effects or hinting at authors’ obsessions with their long-cherished themes.Be that as it may, the Incremental Advancing Window (IAW), a windowing method (Burges, 1998; Lemaire et al., 2005) that Akama et al (2008) have proposed in order to extract word association patterns from a whole document, clearly satisfies the requirements for creating a homophilous semantic network from lexical co-occurrence information. From such a graph, it is possible to extract hidden keywords with moderate frequencies as members of the clusters regarded as being VIP-Clubs. In this method, the window proceeds step by step through an entire document (after the removal of noise words) to collect all word pairs appearing at least once inside the frame. The list of all pair instances thus obtained with their frequencies makes it possible to generate a semantic network. However, two parameters are set to some specific values to adjust for recall and precision in the data gathering: namely, the window size (diameter) is changed from 1 to m and the threshold for word pair frequency (theta; θ) is changed from 1 to n (m and n are both natural numbers larger than 1). For example, if the theta value is 3, word pairs appearing less than 3 times in the window are ignored. Precisely, this means that, no matter how frequently a word appears in a text, keywords are rejected from the list of vertices for the semantic network representing the text, if the instances of paired words are extremely rare with recorded frequencies lower than θ. This is why a graph derived by IAW exhibits the homophily tendency, if we consider degree itself to be an intrinsic weighting for a vertex.For instance, let us cite a study conducted by Akama et al (2008) which applied IAW to Saint-Exupéry’s novel \"Le petit prince\" (original French version). The sample consisted of 1,312 content-bearing words remaining after a stop list was applied. If window size (diameter) is at the smallest level and threshold is similarly maximally strict, then the words extracted by IAW are numerically low, but they form tightly cohesive aggregates, suggesting that the precision rate, P, and the recall rate, R, are always in a trade-off relationship. A severe windowing condition with a threshold value set to 6 allowed us to recall the 38 most important words from the standpoint of co-occurrence patterns, but some of them were not included in the list of the 38 most frequent words. These hidden keywords, or mastermind words, were «apprivoiser», «boa», «cent un», «consigne», «fermé», «manger», «monde», «posséder», «ramoner», «région», «unique», and «volcan», which could all be characterized as homophilous in terms of degree similarity.In contrast, words that were among the top words but excluded from the most severe IAW computational conditions were «aimer», «ami», «baobab», «connaître», «croire», «dessin», «jour», «nuit», «grande personne», «rose», «sérieux», and «venir», which are all ordinary keywords sharing the feature of heterophily in terms of degree-frequency—a tendency to be linked, or even collocated, with many different words (Figure II). In the smallest semantic network containing both groups of words (window size = 7, threshold for pair frequency = 6), the average shortest path lengths from all dangling nodes representing peripheral words to the vertices of these 12 masterminds (the first group) and to those of the purely general keywords (the second group) were 4.172 and 3.747, respectively, representing a highly significant difference according to a conducted t test (Figure III).This result reveals much about the characteristic traits of mastermind words, which when coupled with graph clustering, permits us to understand how they shape communities that deserve the label of VIP-Clubs. To prove this, Markov Clustering (MCL) was applied to the smallest semantic network with IAW parameters of window size = 1 and threshold for pair frequency = 6 (thus screening out many weak co-occurrence patterns and focusing on the most frequent instances of bi-gram). MCL proposed by Van Dongen (2001) is well known as a scalable unsupervised cluster algorithm for graphs that decomposes a whole graph into small coherent groups by simulating the probability movements of a random walker across the graph. It is assumed that when MCL is applied to semantic networks, it yields clusters of words that share certain similarities in meaning or that appear to be related to common concepts (Dorow et al., 2005; Jung et al, 2007). As a matter of fact, the present clustering results produced some clusters which consist almost exclusively of homophilous masterminds, such as {monde, unique}, {boa, fermé, serpent}, and {éteindre, volcan, ramoner}. These clusters suggest a series of subtopics that are not so dominating within the novel, but that convey a persistent and deep resonance to the readers.As described, a text, when treated in the form of a graph, exhibits some hidden keywords that can be enumerated as mastermind entities through the analysis of a homophilous semantic network. Furthermore, if a graph clustering method, such as MCL, is applied to the network, the vertices with such features are categorized into sub-topic clusters, known as VIP-Clubs. Despite the moderate degrees (frequencies) of these vertices (words), they are inconspicuously combined to create lexical patterns which, although they are minor, or subsidiary, in nature, are yet effective.  Full Size Image   Figure I : A homophilous graph (left) and a heterophilous graph (right) : an agglomeration of ‘VIP-Club’ is recognizable in the homophilous graph, while homogeneity underlies the heterophilous graph. Both networks with the same number of nodes (50) and the same connectivity (38.5%) are produced by respectively applying different pruning functions--which consist of trimming edges with probabilities varying according to the weight correlation--to an identical random graph whose distribution of ‘intrinsic weights’ follows the degree distribution of the equal-sized BA model (scale-free graph). The two pruning functions are  Full Size Image   where abslogdiff stands for the absolute value of the logarithmically-transformed difference of ‘intrinsic weights’ between any two vertices.  Full Size Image   Figure II: Subgraph (extracted from the semantic network made under the condition of window size = 7, threshold for pair frequency = 6) around the homophilous words (masterminds: blue) and the heterophilous words (purely general keywords: red). The size of a vertex corresponds to the degree.  Full Size Image   Figure III : The average shortest path lengths from the words with degrees 1, 2 and 3 respectively to the homophilous words (masterminds) and to the heterophilous words (purely general keywords)", "article_title": "Automatic Extraction of Hidden Keywords by Producing “Homophily” within Semantic Networks", "authors": [{"given": "Hiroyuki", "family": "Akama", "affiliation": [{"original_name": "Tokyo Institute of Technology", "normalized_name": "Tokyo Institute of Technology", "country": "Japan", "identifiers": {"ror": "https://ror.org/0112mx960", "GRID": "grid.32197.3e"}}]}, {"given": "Maki", "family": "Miyake", "affiliation": [{"original_name": "University of Osaka", "normalized_name": "Osaka University", "country": "Japan", "identifiers": {"ror": "https://ror.org/035t8zc32", "GRID": "grid.136593.b"}}]}, {"given": "Jaeyoung", "family": "Jung", "affiliation": [{"original_name": "Tokyo Institute of Technology", "normalized_name": "Tokyo Institute of Technology", "country": "Japan", "identifiers": {"ror": "https://ror.org/0112mx960", "GRID": "grid.32197.3e"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "The Victorian novelist and Christian Socialist Charles Kingsley (1819-1875) is now known mainly for his children’s book, Waterbabies (Kingsley 1863), though he also wrote political and historical novels. Long after his death, his daughter, Mary St. Leger Kingsley Harrison (1852- 1931), discovered an unfinished and unexpected novel manuscript entitled “The Tutor’s Story” among his papers. Mrs. St. Leger Harrison, writing under the name Lucas Malet, was herself one of the most famous novelists at the turn of the twentieth century, one who explored daring themes like incestuous desire, lesbianism, sadism, and prostitution (Schaffer 1996:109). Malet finished her father’s novel and published it in 1916 (Kingsley and Malet 1916).In her preface, Malet describes the state, size, and nature of the manuscript, and this description gives a fairly solid basis for assigning at least parts of the novel to the two authors. She tells us that the beginning of the manuscript, and so presumably also the novel, was “fairly consecutive,” so that we can expect the early chapters to be Kingsley's. But she also tells us that there were other “chapters and skeletons of chapters” from much later in the story, without further indicating where these occur in the novel. Finally, she reports that the plot was unresolved, and that she doubled the size of the text in completing it (Kingsley and Malet 1916: vi). This suggests that the late chapters of the novel are probably mostly by Malet. This complex and difficult scenario provides a good opportunity for testing the effectiveness and limitations of some old and some new methods of authorship attribution, including t-tests, Burrows's Delta (Burrows 2002, 2003; Hoover 2004a, 2004b), and Craig's version of Burrows's Zeta (Craig and Kinney 2009; Hoover 2010).As is so often true in the real world, some aspects of this authorship problem are not exactly what one would want. Some of Kingsley’s novels are as much social commentary as fiction, dealing with issues like the plight of the rural poor, poor sanitation, child labor, and the exploitation of workers. Others are historical novels, set in Anglo-Saxon times, during the reign of Elizabeth I, and fifth-century Alexandria. Two others are children’s books. Given this varied output, it is difficult to assemble sufficient similar Kingsley texts for testing. Furthermore, Malet tells us that she has tried to match her style to that of her father, and contemporary reviews of the novel comment that the book sounds just like Kingsley (Book Review Digest 1917). Finally, while most of Kingsley’s fiction is third-person, this is a first-person novel. Malet’s fiction is less varied, but it is also mostly third-person.In spite of these difficulties, initial PCA, Cluster Analysis, and Delta tests on a group of novels by Kingsley and Malet all very successfully distinguish the two authors. Delta results remain quite accurate even for short sections, typically about 90% accurate for Kingsley, and often 100% accurate for Malet, even on large numbers of 500-word sections. Because we can expect some relatively short passages by each writer interspersed with passages by the other, it seems reasonable to test the entire novel divided into sections of 524 words (the novel divides almost exactly into sections of this size). In order to identify changes of authorship in such brief passages, the novel is tested with rolling segments of 524 words. The first section comprises the first 524 words; the next section comprises the 524 words that begin at word number 132, the next the 524 words that begin at word number 263, the next the 524 words that begin at word number 394, and so on through the rest of the novel. Rolling segments have been put to good use in several authorship attribution and stylistics studies; see Craig (1999), Burrows (2010), and van Dalen-Oskam and van Zundert (2007).I am testing the rolling sections of the novel in three ways. The first uses a list of 2873 marker words that t-tests identify as being used significantly differently by the two authors (p < .05). The percentage of the word types (really individual spellings) in each section that belong to each author’s set of marker words is graphed in Fig. 1. The upper set of lines show the percentages of Kingsley marker words and the lower set the percentages of Malet marker words. For example, in the first sections of Chapter 1, about 33% of the types are Kingsley marker words and about 18% are Malet marker words. The separation of the two sets is nicely distinct for the first three chapters, all of which, as expected, are attributed to Kingsley. The beginning of Chapter 4 seems to contain some of Malet’s writing, and about the first two-thirds of Chapter 6 is attributed to Malet. Full Size Image  The t-test results for chapters 4-6 are repeated in a slightly different form in Fig. 2 (upper two lines; 20% has been added to the percentages for the t-test marker words to create a separation between the two sets of lines), along with results from Craig Zeta tests on the same sections (lower two lines). Rather than showing a separate line for each starting point, as in Fig.1, all the testing points for each set of marker words in Fig. 2 are joined by a single line. The graph for Craig Zeta shows the percentage of types in each section that are among the 500 most distinctively used Kingsley and Malet marker words. The smaller percentages for Zeta than for the t-tests reflect the fact that only 1000 marker words are used here, compared to the 2873 ttested marker words. Nevertheless, it is easy to see that Craig Zeta and t-tests give similar results and agree generally on the attribution of various parts of the chapters. Delta tests on similar-sized sections usually agree with these results as well. Many Full Size Image  Many of the chapters of the novel seem to be largely by one or the other author, but others seem thoroughly mixed. These results fall in line with what Malet’s preface leads us to expect, and overall they seem fairly persuasive. A recent discovery makes them both more compelling and somewhat frustrating. After I had completed the testing described above, the problem seemed fascinating enough to deserve further research, and I began by trying to find out whether Kinglsey’s manuscript might still exist. Although I was not able to find any record of the manuscript, I came across a record of a copy of the novel in the Princeton Rare Books collection with Malet’s penciled notes about which parts of the novel were written by Kingsley and which she wrote herself. For some chapters, her notes are quite precise, and they indicate that the attributions in Fig. 1 and Fig. 2 are essentially correct. For other chapters, she notes only that they are “mostly my father.” Most frustrating of all is the fact that all markings cease after chapter 28 (of 41). The fact that the tests described above disagree with her notes for only 5-7 chapters suggest that, even texts involving mixed, joint, or collaborative authorship can be usefully investigated using these methods.", "article_title": "The Tutor's Story: A Case Study of Mixed Authorship", "authors": [{"given": "David L.", "family": "Hoover", "affiliation": [{"original_name": "New York University", "normalized_name": "New York University", "country": "United States", "identifiers": {"ror": "https://ror.org/0190ak572", "GRID": "grid.137628.9"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This paper describes a number of ways in which a temporally-sensitive electronic dictionary resource, the Historical Thesaurus of the Oxford English Dictionary (2 vols; Oxford, 2009 [=HTOED]), may be employed in the automatic dating of words and entire texts. We investigate how the text captures time: most expressly, how the residue of the present (or the different ‘presents’ of language history) have managed to become trapped in the linguistic matrix of a narrative so that we sense, for instance, the difference between a period being represented and the narrator’s temporal positionality, or even the gap between an author and his or her narrative stance. Through computer-assisted means we analyze the impact of later historical and linguistic events on the reporting of earlier events. To this end, we have developed an automatic system for retrieving dating information and a colour-coded browsing interface for searching and viewing the time-coded text, calling it the ‘Time Machine’. Novels capture worlds, but however disparate the materials that may go into them, something of the space-time in which they have been written remains as a residue. This, in part, is a function of language itself: the instabilities, changes and, above all, affordances at any one moment of that linguistic mesh that Lotman (1990) might have called the semiosphere. In part, too, it is a function of what, within Cultural Imagology, might more concretely be called the texture of the iconosphere (Johnson 2005, 2006): the distinctiveness given to the world at any particular moment by the concatenation of signifying objects present within it. This is why it is an attractive idea to apply a tool such as a time-coded dictionary to novels written at time t purporting to convey events taking place at a time t-x. Within this frame of thought, the case of the historical novel is a particularly pronounced one. By definition, the genre tries to capture something of the iconosphere of a world that has passed us by (even though its semiosphere may remain that of a contemporary reader). And even in cases where the linguistic texture of the semiosphere is deliberately archaized – or localized by the use of dialect forms – the residue of the present remains. As a test case, we examine Diana Gabaldon’s Cross Stitch (1991): a text which flaunts traditional temporal typologies by figuring a protagonist who crosses from the iconosphere of the mid-twentieth century to that of the seventeenth century and becomes trapped there: perceiving the past in a lexis and syntax which palpably belong to a different age.Time has previously been explored within documents in several ways. Some work has concentrated on identifying expressions of time within text in an attempt to build models of the succession of events. This has been particularly fruitful in the case of, for example, medical discharge records and road accident reports, where the sequence of events is of great importance (Hirschman 1981, Kayser and Nouioua 2009). Other work has used a training set of time-associated words and a Naïve Bayes Classifier to detect temporal concepts in blogs (Noro et al. 2006). While this work is promising in analyzing writings about daily life in a compact time frame, it seems ill-equipped for investigating iconospheres that deal with spans constituting years or even decades. Thus a tool that can retrieve time-related information from the HTOED automatically offers a very promising way forward for the literary and linguistic scholar.Using the ‘Time Machine’, we map out the iconospheric precision with which Gabaldon represents different characters in her fiction (not to mention the humour generated by the gradual blending of their discourses as the novel progresses). But beyond this, by linking our tool with the powerful additional resources which the HTOED has now opened up for those studying the ‘external’, ‘mental’ and ‘social’ worlds of the novel from a historical and etymological perspective, the project hopes to facilitate the achievement of a more nuanced understanding of the interrelationship between ‘real’ and ‘fictional’ time in the historical novel than has been possible before.In order to better study iconospheres, we sought to develop a tool that would automatically look up dating information and definitions for words, processing entire texts at a time, thereby removing the need for manual queries using a dictionary. Furthermore, we wanted the tool, on the one hand, to enable users to specify time periods of interest for closer inspection while, on the other hand, it left them free to browse the material through diverse visualization schemes in order to discover trends or new time periods of interest.At present the tool is a prototype, running inside a web browser, in order to enable rapid experimentation with new visualization schemes using CSS. We use a local SQL database to store the HTOED data. Texts can be uploaded via a browser interface and are processed in any user-defined units tagged in the text, e.g. page by page or speaker by speaker, or in the text as a whole. The tool reads both XML and plain text. To finish verifying that the visualization schemes we have chosen are useful, we wish to bring the tool to the digital humanities community, in addition to the poetics and linguistics community (Johnson et al. 2010). Following this, we intend to develop the final tool in Java for inclusion within the LICHEN toolbox.What the HTOED is able to offer to the ‘Time Machine’ is the ability to isolate different experiential modes within particular iconospheres at the same time as it reveals the range of etymological meanings open to the reader at any given moment. (This, of course, is an invaluable aid for critics who wish to avoid anachronism in their own readings.) In our preliminary development of the ‘Time Machine’ we concentrated on its capacity for isolating different lexical categories within a given iconosphere and indicating the etymological choices available for particular readings. At the top of the screen, the Source section allows the user to choose either an entire text or some part of it (see Figure 1 below: a case in which the speaker Jamie has been chosen). In the Filter section, the user can choose to narrow the search down to one particular word, or to all words that were in use at a particular time (choosing either first use date, last use date or both). To produce the present screen we started by choosing all the words that entered the language after 1742, in other words after the time period in which Jamie speaks. The Colour-coded text section then highlighted all those words which entered the language after the given date, as did our Wordlist section.Our initial investigation found that the tool is able to pick out swathes of temporal incongruities from this playful text or, further, search out instances relating more specifically to the ‘external’, ‘mental’ and ‘social’ worlds of the novel. It spots moments when the eighteenth-century clansman Jamie seems prescient (mentioning ‘aesthetics’ for instance, or re-circulating the word ‘sadist’, which has been bandied to him by his twentieth-century wife). It detects instabilities not only in the iconosphere of the 1700s – which Gabaldon has carefully researched – but also in the representation of mid-twentieth-century England (which she appears to have taken more for granted).However, despite the manifest advantage of using even this approach to the ‘Time Machine’ to spot faultlines and incongruities within the fictional world of a novel, some teething-troubles remained: the most significant being that, unlike human readers, the prototype cannot, of course, intuit the ‘correct’ lexical choice from the range of possible meanings thrown up by a search. Accordingly, we have tweaked the search and display capability of the ‘Time Machine’ so that it can also narrow its lexical catchment area by trawling parts of speech (such as substantives) in which cultural and temporal change exhibit their highest visibility. Figure 1 demonstrates how, using these restrictions, the prototype is able to flag up the way in which Gabaldon has inadvertently endowed Jamie’s lexicon with three words stereotypically associated with Scottishness (‘Sassenach, shinty, sporran’) which were not, in fact, recorded until some time after the period in which Jamie is meant to be speaking.In sum, our study indicates that automated access to chronological information, such as the date of first use for any given word, and full etymologies has promising applications in literary and historical research that has until now relied mostly on intuition and laborious manual methods to combine dating information and texts. And beyond this, with some adaptation, it is also clear that the ‘Time Machine’ could be of significance within areas such as forensic linguistics, collocation studies, and the study of micro-linguistic change over time in large corpora.", "article_title": "The Time Machine: Capturing Worlds across Time in Texts", "authors": [{"given": "Ilkka", "family": "Juuso", "affiliation": [{"original_name": "University of Oulu, Finland", "normalized_name": "University of Oulu", "country": "Finland", "identifiers": {"ror": "https://ror.org/03yj89h83", "GRID": "grid.10858.34"}}]}, {"given": "Lisa Lena", "family": "Opas-Hänninen", "affiliation": [{"original_name": "University of Oulu, Finland", "normalized_name": "University of Oulu", "country": "Finland", "identifiers": {"ror": "https://ror.org/03yj89h83", "GRID": "grid.10858.34"}}]}, {"given": "Anthony", "family": "Johnson", "affiliation": [{"original_name": "University of Oulu, Finland", "normalized_name": "University of Oulu", "country": "Finland", "identifiers": {"ror": "https://ror.org/03yj89h83", "GRID": "grid.10858.34"}}]}, {"given": "Tapio", "family": "Seppänen", "affiliation": [{"original_name": "University of Oulu, Finland", "normalized_name": "University of Oulu", "country": "Finland", "identifiers": {"ror": "https://ror.org/03yj89h83", "GRID": "grid.10858.34"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "In Representative Irish Tales, Yeats identified two basic categories of Irish fiction characterized by what he called “two different accents, the accent of the gentry and the less polished accent of the peasantry” (Yeats 1979). Writing of this distinction, John Cronin notes in The Anglo-Irish Novel how “Maria Edgeworth and William Carleton fit obviously enough the two extremes Yeats has defined but a middle-class figure like Gerald Griffin belongs a little uneasily somewhere in between” (Cronin 1980). Other critics including Thomas MacDonagh (MacDonagh 1916), Thomas Flannagan (Flanagan 1959), and most recently Charles Fanning (Fanning 2000) have all focused attention on the specific use of language in Irish narrative and the extent to which linguistic style and choice of theme and form reflects, or does not, the unique position of these Irish and Anglo-Irish writers in a country where the use of English was to evolve in a rather dramatic fashion. Though Mark Hawthorne has written that the “Irish were not accustomed to the English language and were unaware of its subtleties and detonations” (Hawthorne 1975), Charles Fanning has argued that the Irish in fact became masters of the English language and employed a mode of “linguistic subversion” that allowed them to comment upon and even satirize the British who all the while seem to miss the point that the joke is on them (Fanning 2000). Cronin argues along similar lines to Fanning when he writes that the. . . idiomatic unease in their novels is not caused by any lack of ability on their part in the writing of a standard English idiom. It derives, rather, from the tangled situation in which they find themselves as novelists, directing their efforts towards an English-speaking public but trying to give that public a creative insight into a linguistically piebald area . . . they turned their very difficulties in regard to idiom to constructive account by confronting head-on the blending of the two idioms and two cultures. . . they turn this linguistic ragout to splendid account, making use in the process of English, Irish, and Anglo-Irish” (Cronin 1980). The subject of this research paper, then, is the matter of exactly how 19th century Irish novelists uniquely employ style, setting, and theme. The critics seem to agree that something specific is going on in terms of language, form, and setting, and yet none gets to the heart of the matter, to the details of the prose and to the specific uses of language. Leveraging the tools and techniques from the authorship attribution and computational text analysis literature—specifically natural language processing, machine learning, and topic modeling—this paper compares and contrasts both linguistic style and narrative theme in a corpus of over 500 British and Irish novels from the 19th century. The results of this work show the precise extent to which Irish prose is stylistically different from English prose, and I identify and explore those linguistic and thematic features that mark the Irish novel as distinctly different from the British. Specifically, my research examines style through an analysis of sentence and word level features. The results show, among other things, that Irish writers tend toward expressions that are both longer and more indeterminate than their British counterparts. Favoring the long sentence and greater use of the comma, the Irish write in comparatively complex, flowing sentences that favor (as measured by relative frequency) words denoting indeterminacy, words such as “most,” “some,” “may,” and “yet.” British writers, on the other hand, show a preference for shorter, more determinate sentences featuring words such as “know, never, no, nothing, must, not, only, all, should, last, first, and great.” This result tends to confirm anecdotal observations made by scholars, including (Cronin 1980) who suggest that though the Irish may have sought to imitate and appeal to the stylistic preferences of a British dominated industry, they ultimately invented their own style of prose, which captured both the rhythms of the local language and the anxieties of a country struggling with its position vis-à-vis the colonizing presence of the British. In addition to probing and comparing the stylistic habits of the two nations, this work further analyzes the prose at the level of theme and argues that there is an important link to be made between style and theme in Irish prose. To harvest latent themes, I employed the unsupervised topic modeling tools of the UMASS machine learning toolkit (McCallum 2002). A run of the model, which sought to identify the 25 most prominent topics in the corpus, resulted in one particular topic appearing with greater frequency in the Irish novels of the corpus. This topic, which was labeled as “the big house theme,” is composed of words clearly relating to tenant-landlord relations and the familial issues that are so often explored by Irish writers attempting to characterize these troubled relationships. The big house theme was found to be the most prominent topic in 35% of the Irish novels analyzed in this corpus, and it is present to a lesser degree in many of the others. My analysis concludes by tracing the links between distinctly Irish themes and the elements of Irish style identified in the first part of the research. From the macroanalytic data derived at the corpus level, I present a chronological charting of Fanning’s notion of linguistic subversion, and then I move to the micro level and offer a closer reading of several exemplary passages from works in the chronology. I discuss how linguistic subversion is inherent to the tradition of the \"Irish Bull” and offer a brief discussion of Richard and Maria Edgeworth’s 1835 essay on the subject in which they write with some humor that: “English is not the mother tongue of the natives of Ireland; to them it is a foreign language, and consequently, it is scarcely within the limits of probability, that they should avoid making blunders both in speaking and writing . . . Indeed, so perfectly persuaded are Englishmen of the truth of this proposition, that the moment an unfortunate Hibernian opens his lips they expect a bull, and listen with that well known look of sober contempt and smug self satisfaction, which sufficiently testifies their sense of safety and superiority.” (Edgeworth 1835)  As early as Castle Rackrent (1800), Edgeworth had demonstrated her own command of linguistic subversion and an acute awareness of how to form her narrative and bend language to provide not simply a distinctly Irish novel but a seminal novel within the larger novelistic tradition. My work provides quantitative evidence of how, where, and why Irish style is different from British. ", "article_title": "Detecting and Characterizing National Style in the 19th Century Novel", "authors": [{"given": "Matthew", "family": "Jockers", "affiliation": [{"original_name": "Stanford University", "normalized_name": "Stanford University", "country": "United States", "identifiers": {"ror": "https://ror.org/00f54p054", "GRID": "grid.168010.e"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": " Land deeds were the only proof of ownership in pre-1900 Taiwan. They are indispensable for the studies of Taiwan’s social, anthropological, and economic evolution. We have built a full-text digital library that contains more than 30,000 land deeds. The deeds in our collection range over 250 years and are collected from over 100 sources. The unprecedented volume and diversity of the sources provide an exciting source of primary documents for historians. But they also pose an interesting challenge: how to tell if two land deeds are related. In this paper we describe an approach to discover one of the most important relations: successive transactions involving the same property. Our method enabled us to construct over 3,300 such transaction pairs. We also introduce a notion of land transitivity graph to capture the transitivity embedded in these transactions. We discovered 2,219 such graphs, the largest of which includes 103 deeds. Some of these graphs involve land behavior that had never been studied before. Until the turn of the 20th century, hand-written land deeds were the only proof of transaction of lands in Taiwan. Such a deed may involve activities such as selling/buying, lending of land to smaller farmers, dividing the land among children or shareholders, and cultivation permits. The deeds usually follow, depending on their types, a typical but not standard format, and are drown up in ad hoc manner. Indeed, even the name of the location may be written in a local convention unfamiliar to the outsiders.While each land deed may have significance only to its owner, a large collection of them provides a fascinating glimpse into the pre-modern Taiwanese grassroots society. Historians have studied them to investigate the economic activities, community development, and the relationship among the various ethnic groups (Chen, 1997; Ka, 2001; Shih, 2001; Hong, 2005).In the past few years we have built a full-text digital library of primary historical documents of Taiwan called THDL (Taiwan History Digital Library). Among its corpuses is a collection of over 30,000 land deeds, spanning from 1666 to the first decade of the 20th century, and collected from over 100 sources of origin (Hsiang, Chen, Tu, 2009). This collection is unprecedented in terms of volume, time span, geographic distribution, and variety. While THDL presents an exciting source of primary materials for historians, it also poses a challenge: how to find the relationship between two land deeds, or, how to find all the land deeds involving the same piece of land. Although it was customary to hand down earlier deeds to the new owner during the transaction of land, most of these links were broken when the Japanese, during their colonial rule of Taiwan between 1895 and 1945, modernized the land management system (Li, 2004). That is because the officials only recorded the last deed as the proof of ownership but ignored the previous ones. Consequently many of the older deeds were either destroyed or (later) sold as collector’s items because they had lost their original value. In this paper we present a semi-automated method to discover the transaction relations among land deeds. We shall focus on two important relations: successive transaction pairs and allotment agreements. We further connect the transitive activities on the same piece of land into a concept called land transitivity graph, which captures the history of the land over time. The largest such graph that we found has led to a discovery of a new type of land use that had never been observed before.We start by describing the two relations among land deeds that our method tries to capture.Successive transaction pairs: A piece of land could be sold from A to B, then from B to C. In this case there should be two land deeds recording the two transactions. We call them a successive transaction pair. Note that the situation could be rather complicated. For instance it could have been B’s son who sold it to C. If B divided the land among his descendents, the first selling transaction and the ensuing allotment agreement (see below) also form a successive transaction pair.Allotment agreements: An allotment agreement is a deed that records how a land is divided among the owner’s descendants or among the shareholders. In both cases the usual practice is to first divide the land into several parts, then to have each participant drawing from the lot. Once the decision is agreed upon, an agreement is written, and several copies are made and given to each person involved. In the case of division among shareholders, the allotment agreements should be preceded by a cultivation permit, a permission from the government to allow a group of people to cultivate the land. In this case, the cultivation permit and the ensuing allotment agreement also form a successive transaction pair.  Fig. 1 The process for discovering land transaction relations Full Size Image  To tackle this problem of finding successive transaction pairs, we developed a 3-step semi-automatic process (Fig. 1). We first used text processing technology to extract features of each land deed from its metadata and full text. Such features include the transaction type, the general location of the land and the four reaches (boundaries identifying the land via some obscure way such as “bordering Lee’s house on the south,” “a large camphor tree on the west,” etc), the names of the people involved in the transaction and their roles (seller, buyer, scrivener), description of the source of the land (how and when the current owner obtained it), the size, the price, and the amount of taxes paid (Lu, 2008; Huang, 2009). Fig. 2 is an example of a typical land deed. We designed an XML format to hold this information (Fig. 3). Second, we defined rules to identify deeds that may be related. Fig. 4 shows the rules we used for identifying the successive transaction pairs. We then wrote a program to compare every pair of land deeds in THDL to see if any pair satisfied the rules. Finally, we give all the pairs produced to human expert to verify.  Fig. 2 An example of a typical land deed of Taiwan Full Size Image      Fig. 3 The features of a “selling” type of land deed, stored in XML Full Size Image   We further remark that a criterion that allows certain degree of fuzziness was used when performing matching. This is because the names used in different deeds may sometimes be slightly different even if they are the same place or person (Huang, 2009).  Fig. 4 The rules for identifying successive transaction pairs Full Size Image   The precision rate of the algorithm for successive transaction pairs is 63.9% and that for allotment agreements is 94.4%. We have found 2,409 successive transaction pairs and 878 sets of allotment agreements among the 30,820 land deeds in THDL (Table 1).   Table 1 The result of reconstructing land transaction relations among the land deeds in THDL Full Size Image   Among the former, 358 are cross-generation (A sold to B, and B’s descendent sold to C). Some of the pairs/sets are from different sources (the “cross sources” column in Table 1), and are quite impossible to find manually. Some others are from the same source but are not adjacent to each other in their original order. These are also difficult to identify by hand. When further examining the transaction pairs, an interesting transitive phenomenon emerged. There may be a deed of A selling a piece of land to B, and some years later B divided the land among his sons, then one of them, C, rented it to D to farm. Such transitive activities on the same piece of land could last for decades. By connecting all these transactions into a graph, it may capture the evolution of a property over time. This is exactly what we did. We call these graphs land transitivity graphs. Using the relations we discovered early, we came up with 2,219 such graphs. The result is listed in Table 2.   Table 2 The land transitivity graphs constructed among the land deeds in THDL Full Size Image   Fig. 5, the third largest graph, contains 36 deeds, dating from 1850 to 1910. The head of the family, Liao Jiafu (廖佳福), was among the shareholders who received a cultivation permit from the Qing government, and obtained this piece of land through allotment in 1850 (the first deed).   Fig. 5 The 3rd large graph, containing 36 deeds Full Size Image   Liao farmed the land for 50 years and divided it among his descendents in 1901 (the second deed). The rest of the deeds described the various activities such as further divisions or selling in the next 10 years. By 1906, only 2 of the 8 divided pieces of land remained in the Liao family.  Fig. 6 The largest graph, containing 103 deeds  Full Size Image   Fig. 6 is the largest land transitivity graph with 103 deeds. Tu, a historian, studied this graph and discovered that the deeds involved demonstrated a unique case of land use that had never been studied before (Tu, 2010). It is unlikely for human to notice this possibility without the computer-generated transitivity graph.To help historians take advantage of these graphs, we developed an integrated environment to analyze the information embedded in each graph (Fig. 7). In addition to the graph itself and its zoomable navigation facility, we also added tag cloud, chronological distribution, and a location map.  Fig. 7 The integration environment for land transitivity graphs Full Size Image   Land deed research has been an important topic among historians of pre-1900 Taiwan. In this paper, we presented a method to discover the transaction relations among the 30,820 land deeds in THDL, the largest existing full-text database of land deeds. Our method discovered 2,049 successive transaction pairs and 878 sets of allotment agreements. They, in turn, are transformed into 2,219 land transitivity graphs, each of which describes the transaction evolution of a piece of land. One such graph has already led to the discovery of a unique pattern of land development that had not been studied before (Tu, 2010). We feel that our work demonstrates how IT tools can be used to help historians conduct research that could not be done otherwise.", "article_title": "Discovering Land Transaction Relations from Land Deeds of Taiwan", "authors": [{"given": "Shih-Pei", "family": "Chen", "affiliation": [{"original_name": "Department of Computer Science, National Taiwan University", "normalized_name": "National Taiwan University", "country": "Taiwan", "identifiers": {"ror": "https://ror.org/05bqach95", "GRID": "grid.19188.39"}}]}, {"given": "Yu-Ming", "family": "Huang", "affiliation": [{"original_name": "Department of Computer Science, National Taiwan University", "normalized_name": "National Taiwan University", "country": "Taiwan", "identifiers": {"ror": "https://ror.org/05bqach95", "GRID": "grid.19188.39"}}]}, {"given": "Hou-Ieong", "family": "Ho", "affiliation": [{"original_name": "Department of Computer Science, National Taiwan University", "normalized_name": "National Taiwan University", "country": "Taiwan", "identifiers": {"ror": "https://ror.org/05bqach95", "GRID": "grid.19188.39"}}]}, {"given": "Ping-Yen", "family": "Chen", "affiliation": [{"original_name": "Department of Computer Science, National Taiwan University", "normalized_name": "National Taiwan University", "country": "Taiwan", "identifiers": {"ror": "https://ror.org/05bqach95", "GRID": "grid.19188.39"}}]}, {"given": "Jieh", "family": "Hsiang", "affiliation": [{"original_name": "Department of Computer Science, National Taiwan University", "normalized_name": "National Taiwan University", "country": "Taiwan", "identifiers": {"ror": "https://ror.org/05bqach95", "GRID": "grid.19188.39"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "The Digital Dictionary of Buddhism (DDB) (http://buddhism-dict.net/ddb/subscribing_libraries.html), now on the Web for more than 15 years, has come to be regarded as a primary reference work for the field of Buddhist Studies. Containing over 54,000 entries, it is subscribed to by more than 38 university libraries (http://www.buddhism-dict.net/ddb/subscribing_libraries.html). It is supported by the contributions of over 70 specialists, many of these recognized leaders in the field. It can perhaps be described as example of the type of web resource hoped for by Jaron Lanier in his book You Are Not A Gadget, or something similar to the sort of thing envisioned by Joseph Raben in his Busa lecture at the DH2010, in the sense of being the fruit of the collaborative efforts of a community of scholars that has reached a degree of status and sustainability such that it has been able to grow and thrive—despite having little funding or the support of a major organization or team of programmers—in the age where such resources are so readily eclipsed by the combination of Wikipedia and Google. The field of Buddhist Studies has its own reliable, scholarly-edited, fully documented and responsible online reference work that has developed a center of gravity sufficient for it to continue to grow as the resource that specialists turn to without hesitation, and to which they may contribute knowing that they will be clearly accredited, and that what they write will not be deleted or changed in the following moment by, for example, a junior high school student.With the DDB having a history almost equal in length to that of the WWWeb as we know it (it went online in 1995), there is a wide range of issues that can be discussed beyond its present technical structure. Of great importance are the management strategies that have allowed its continued progress through the long series of changes the Web has witnessed during this first epoch of the Internet. How, exactly, can a project that is based on the continual development of quantity and quality of reference data can continue to grow to the extent of becoming the de facto primary field reference work after exhausting the first couple initial grants, without either becoming a fully “pay-for” resource (perhaps being bought out by a commercial enterprise of some sort), or being supported by some private organization — an alternative fraught with the danger of forcing the resource to co-opt its principles and its objectivity?I began the compilation of the DDB and its companion CJKV-English Dictionary (CJKV-E) in 1986, originally simply envisioning the eventual publication of the usual printed work. In 1994, however, the Web made its appearance, and the potential advantages of trying to develop a reference work online in a collaborative manner were immediately apparent. So in the middle of 1995, I converted my WordPerfect word-processor files to HTML, and placed the dictionary on the web. To my great elation, I was soon contacted by a few good scholars with similar interests, who were willing to offer both content and technical advice.During its first few years on the web, the DDB was maintained in a simple, hard-linked HTML format. With the help of Christian Wittern, this source was converted to SGML, and then XML. A major turning point in the history of the project came in January 2001, when Michael Beddow offered to help with Web implementation, and for the first time, the raw XML data was searched and presented to users through a combination of Perl and XML/XSLT technology. At that time, building a search engine that could deal with mixed Western/CJK text in UTF-8 encoding was a not at all a simple matter, so Michael's search engine was a bit of a novel creation—and was able to serve its purpose with only minor tweaks up through most of 2010, for almost a full decade.When Michael Beddow's search engine was set up in 2001, usage of the DDB increased dramatically. Yet despite our repeated pleas for user contributions, except for a very small number of “enlightened” individuals who somehow naturally grasped the meaning of this strange new thing called “web collaboration,” it became apparent that there were very, very few people willing, on their own, to take five or ten minutes to write up and send us even a couple of terms from their own research work. This lack of interest on the part of users in making contributions was extremely disappointing. Thus, while our password security system was originally set up to ward off hacking attempts, we decided to experiment with using this apparatus to institute a two-tiered system of access. In the first level, any user could access the data a limited number of times in a 24-hour period, logging in as guest. In the second level, contributors were granted unlimited access. We started off setting the guest limit at fifty, but leaving it at this amount for a few weeks we received neither complaints nor contributions. We then began to gradually drop the number down to forty, thirty, and then twenty searches in a day. At twenty, there was still nary a complaint made nor contribution to be seen. But when we hit the number of ten, everything changed. We were first bombarded with indignant complaints, but holding the line, and at the same time lowering the minimum required level of contribution to the equivalent of one A4 page for two years of access, eventually these complaints began to turn into contributions. This was a watershed moment for the project, because we found that once people contributed one time, most of them continued to do so, whether voluntarily, or by continued prompting through this same arrangement.At the time of my first public presentation of the DDB at a meeting of the Electronic Buddhist Text Initiative (EBTI http://buddhism-dict.net/ebti/) in 1996, the DDB contained approximately 3,200 entries. That number is now over 54,000, with a present average growth rate of 4,000 terms per year. The continued growth in popularity of the DDB, especially as a reference work for graduate and undergraduate courses in Buddhist Studies in North America and Europe generated one more access problem that needed resolution—that of how to allow for the use of the DDB in the case where an instructor wanted to use the dictionary for an university course. To deal with these kinds of situations, we decided to begin to offer subscriptions to university library networks for a modest fee. This policy brought about an unforeseen benefit, in that we could now provide a list of reputable institutions that had deemed the DDB to be an academic reference of high standards. It also generated a small but steady income, which allowed us to pay for hardware and software, and a couple of part-time workers to do input and editing. Finally, in order to encourage contribution from qualified scholars, great effort was expended toward letting members of the field know of the contributions being made by their colleagues. Thus on the dictionary's web site itself, as well as on associated news and mail lists, information regarding new contributions is energetically distributed.This presentation will start off with a short demonstration of the most advanced functions of the DDB, to be followed by a brief overview of its technical framework (P5- influenced XML, delivered through XSL and Perl). We will then outline the above-introduced key factors of the management of the DDB that we believe have most directly contributed to its great success.", "article_title": "The Digital Dictionary of Buddhism: A Collaborative XML-Based Reference Work that has become a Field Standard: Technology and Sustainable Management Strategies", "authors": [{"given": "Charles A.", "family": "Muller", "affiliation": [{"original_name": "Center for Evolving Humanities, University of Tokyo", "normalized_name": "University of Tokyo", "country": "Japan", "identifiers": {"ror": "https://ror.org/057zh3y96", "GRID": "grid.26999.3d"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Digital Humanities (DH) has been described as an 'emerging' field for some time now but many (Borgman 2009; Friedlander 2009; Presner 2009) agree that this is a crucial moment for the discipline. The DH community now has established research methods, scholarly conferences and journals (Borgman 2009), DH centres and labs (Svensson 2010; van den Heuvel et al. 2010) and MA studies (Clement 2010). However, it could be argued that an important area for further development is the true internalization of the DH community. Up to now much of the discussion has centered on DH projects in a handful of mainly English speaking countries (Terras 2006). In order to consolidate as a discipline, an important challenge for the DH community is to extend its international reach and incorporate work from a broader range of academic institutions and languages.One of the main issues of course, is attempting to integrate with groups of scholars that have not necessarily identified themselves yet as a community or do not even know that DH exists. Our project has four main objectives:  raise awareness about DH identify key scholars and projects investigate key local issues in the development of DH projects consolidate a community and find ways of linking with the international DH community The aim of this paper is to present our initial experience with two workshops as a methodological approach to investigating DH work in an unknown landscape and present a preliminary report on the DH situation in Mexico in particular.To our knowledge this type of work has not been done before, in particular for Latin America. The fact that there has been little participation of Latin American scholars does not necessarily imply that no DH work is being done in this region but could be rather a lack of connection with the DH community. Japan for example, reports that despite having a long tradition in DH projects, one problem has been cooperating with similar projects overseas (Muller 2010).It is well documented that finding digital humanities resources (Dunning 2006; Pappa et al. 2006, Warwick et al 2006) and DH tools (Juola 2008) can be a difficult task and charting an unknown territory poses further challenges. We decided that workshops would allow us to both raise awareness of DH as well as serving as exploratory method to identify key scholars, projects and local issues. Additionally time and financial constraints were a major factor. At a later date we hope to use more quantitative methods such as a national survey.Two workshops were carried out at the Universidad Nacional Autonoma de Mexico (UNAM). As there is no record of DH activity in Mexico it was difficult to define where to start and so we relied heavily on personal experience and contacts to produce a list of possible participants. We contacted everybody we knew who had experience working on a DH project and asked them in turn to invite other participants in order to generate a snowball effect. The invitation included a very broad definition of DH projects. The workshops generated a large amount of interest and were well attended.In order to provide a framework for the discussion we identified seven key topics from the literature:  organizational context including institutional recognition and support (Siemens et al. 2010; Warwick 2008b) planning and development intellectual property and copyright (Rehm 2007); human resources and training (Warwick 2009) dissemination and use (Warwick 2008a) completion and sustainability (Brown et al. 2009; Kretzschmar 2009; Sewell 2009) digital humanist career (Siemens et al. 2010). These topics were addressed as a series of questions which participants answered reflecting on their particular experience and the projects they had worked on.The first workshop had fifteen participants and the second twelve. Table 1 shows a breakdown of participants by subject. Examples of the types of DH projects carried out by the participants were digital collections and libraries (modern short novels and poetry, XIX century manuscripts marked in TEI), linguistics (text mining, corpus building, corpus of Mexican Spanish), digital images (research in pre Hispanic mural paintings, visualization archeological sites) and Anthropology (sound files for linguistics research indigenous languages). Almost all participants were project leaders, with the exception of two programmers, one MA publishing student and one graphic designer.Of the people attending only a few were aware of the field of DH. Some had links with international projects (such as Biblioteca Virtual Cervantes) but in general most work was local and individual. Participants were pleased to discover \"that there are other people like me\".Almost all projects are personal and not institutional initiatives. Several scholars remarked that university authorities had a vague idea of the importance of registering and managing digital materials but that this institutional support in practice lacked coherent policies or structures for them to have any real impact. None of the projects for example, had any specific physical location assigned and many had to improvise working spaces in order to cope with human resources and equipment.A few people remarked however that working marginally was actually a good thing. \"Being ignored can also be an advantage. Being independent and invisible to the institution allows you to be dynamic and creative\".Surprisingly, funding was not a problem for any of the projects. Most scholars applied and received funding from government or the UNAM. However, quantities were not particularly large (around 15,000 US dollars) and no project had permanent institutional funding. Participants did not mention a lack of access to computational technologies which had been suggested as a possible problem for DH projects in developing countries (Terras 2006).Projects were rarely documented with the exception of Linguistics. This is similar to other DH projects worldwide (Warwick 2009). Due to one to three year funding periods many felt that they were in a race against time to complete and documentation was left out.Finding, training and retaining human resources are also key issues. All participants agreed that it is difficult to find human resources with the necessary skills and training was required. Additionally participants themselves went through a steep learning process whilst developing their project and found little learning support. In terms of training in Mexico there are no DH centres or courses. However, a couple of MA DH related classes are in development. Sharing information and pooling resources was considered fundamental towards developing the field. Participants noted the urgent need to compile best practice and guidelines as there seemed to be duplication of efforts and no communication.Long term sustainability of resources is a major issue. For example, many scholars had purchased their own server to host the project as there are no university guidelines for hosting projects. However, it is not clear what will happen once the servers have to be replaced, or if the researcher left the university. In other cases, projects were hosted at the Computing Services department but usually as a personal rather than a formal collaboration. Others have hosted their projects on external servers, sometimes even at their own personal expense. As one participant remarked \"when does a project become a university service and therefore somebody else's responsibility?\" This is a common issue for DH (Brown et al. 2009; Kretzschmar 2009; Sewell 2009). We detected a notable absence of the library community whose skills are essential to these issues. Participants were aware of preservation but had not addressed the issue at all.Another major issue was evaluation and recognition of DH work. Many felt that their work, although funded, was later not taken into account for evaluation purposes. However, it was also noted that it is difficult for evaluation committees who have no experience or knowledge about these types of projects to assess them. Many had worked individually with their departmental boards but it was agreed that providing tools, acting as a consultative body and lobbying collectively would be a more effective approach.Results from the workshop indicate that forming a DH community is possible as we found sufficient projects, scholars and interest to sustain a working group. All participants were enthusiastic about forming part of a local DH group. Workshops were by invitation only but have since resulted in other DH scholars coming forward and wanting to participate. Initial results indicate that issues and challenges regarding DH projects are similar to other countries and collaboration would be possible and fruitful. However, with some issues such as university and governmental recognition and support, guidelines and best practices and community awareness there appears to be a significant lag behind other countries. One main difference is the almost complete absence of the library community and this issue should be addressed. Main challenges are now: to discover and register more research and projects; develop best practices and guidelines in Spanish; incorporate the library community, build a directory of DH scholars; expand the group and develop mechanisms to increase national and international collaboration. In the next few months we will continue to work on more specific actions and report on them in due course.", "article_title": "Is There Anybody out There? Discovering New DH Practitioners in other Countries", "authors": [{"given": "Isabel", "family": "Galina", "affiliation": [{"original_name": "Instituto de Investigaciones Bibliograficas, Universidad Nacional Autnoma de Mexico", "normalized_name": "National Autonomous University of Mexico", "country": "Mexico", "identifiers": {"ror": "https://ror.org/01tmp8f25", "GRID": "grid.9486.3"}}]}, {"given": "Ernesto", "family": "Priani", "affiliation": [{"original_name": "Facultad de Filosofía y Letras, Universidad Nacional Autnoma de Mexico", "normalized_name": "National Autonomous University of Mexico", "country": "Mexico", "identifiers": {"ror": "https://ror.org/01tmp8f25", "GRID": "grid.9486.3"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This paper discusses the re-development of The Image Markup Tool in two parts: (1) as a crossplatform desktop application and (2) as a web-based, html5 standard, client-side browser application. Currently, a few text-based tools allow for markup of documents (most often) in XML/TEI. They range from legacy software such as the Analytical System Tools and SGML/XML Integration Applications (Anastasia)(ITSEE, University of Birmingham; http://www.sd-editions.com/anastasia/index.html) and Editing Digital Interactive Texts in an Online Network (EDITION),http://www.sd-editions.com/EDITION/ to more recent projects such as eLaborate,Huygens Instituut KNAW (a research institute for text edition and textual scholarship of the Royal Netherlands Academy of Arts and Sciences); http://www.e-laborate.nl/en/ TextGrid,http://www.textgrid.de/ and TEXTvre.http://textvre.cerch.kcl.ac.uk/ TextGrid uses a collaborative document markup interface, as well as a project and user management system to facilitate the markup of texts. TEXTvre, which will be modeled on TextGrid, has been described as a “a working exemplar VRE for textual scholarship.” While each of these tools offers specific methods of text-based editing, The Image Markup Tool is image-based, allowing for the markup of encoded digital images of remediated textual objects.We are building on the current iteration of The Image Markup Tool (v1), developed by Martin Holmes at the University of Victoria. Version 1 of the Image Markup Tool was first written in 2006, and has gone through several versions. It was originally conceived as part of the project Le mariage sous L'Ancien Régime,http://mariage.uvic.ca/ where it was used to mark up engravings (Carlin, Haswell and Holmes 2006; Carlin and Holmes 2008). Early versions of the tool used SVG code embedded into a TEI file to delineate areas of interest on an image, but the current version makes use of the Facsimile module recently added to the TEI schema, which enables the use of native TEI elements to define rectangular “zones” on “surfaces” (often pages) which are part of a facsimile.However, version 1 of the Image Markup Tool suffers from a number of serious limitations, most of which were apparent from its inception. Firstly, it allows only rectangular areas (hereafter “zones”) to be specified on images. This was in line with the original specification of the Facsimile module in TEI, in which <zone> elements were similarly constrained, but users have been demanding the ability to specify polygonal shapes ever since the first release, and recent modifications to the TEI schema now allow the use of polygonal <zone> elements. Secondly, IMT version 1 can handle only one image per file. This was sufficient for its original projected use as part of the Mariage project, which focused on individual engravings, but makes the tool inadequate for serious facsimile work; most documents have multiple surfaces or pages. Thirdly, the program can handle only a one-to-one relationship between a single <zone> on a <surface>, and a single <div> in the <body> of the document. This is inadequate. It is a common requirement to link, for instance, a single block of text on an image to an original transcription, a modernized version of the transcription, and an editorial note or interpretation. Similarly, a single <div> (or in fact any other element) in the body of a text might conceivably be linked to more than one <zone>; multiple views of a particular surface or page might be provided in the facsimile, each with an equivalent <zone>. Finally, IMT version 1 was written as a Windows application using Borland Delphi. It will run on Linux using Wine, but there is no simple way to run it on a Macintosh computer.Nevertheless, IMT version 1 has a number of strengths. As a compiled desktop application, using a very sophisticated open-source graphics library (Graphics32), it can to do high-quality resampling of images on-the-fly, enabling effective and rapid zooming of high-resolution images. Most file operations are very fast, and the interface itself is simple and relatively easy to use. For Windows and Linux users, it is easy to download and install.The desktop version of IMT 2 is being written using Nokia's QT Creator tools. This will enable us to compile the application for Windows, Mac and Linux. Our intention is to build on the strengths of version 1 -- in particular, the speed and efficiency of graphics handling and file i/o, and the user-friendliness and simplicity of the interface -- while adding three important improvements: Handling of an unlimited number of images. Many-to-many linking between <zone>s and any elements with @xml:id attributes in the <text> section of the file. Support for polygonal zones. These screenshots of early development pilots show how we envisage the user interface.  Full Size Image   The first shows the main window, with multiple folios (pages) listed down the left side. Clicking on a folio shows the corresponding image, and the polygonal zones defined on that image (in red). The selected zone has draggable circular nodes at all of its corners. The context menu for the selected zone is displayed, showing that the zone is linked to three different elements in the <text>.  Full Size Image   The second screenshot shows the corresponding dialog box, where the user can select an element in the <text> of the TEI file for linking to a zone. Three views of the file are available: the first is a read-only syntax-highlighted text view, the second consists of a list of all the @xml:id attributes in the file, along with a text box which can be used to filter them, and the third is an “outline” or tree view of the file. The user could use any of these to find and select an element to be linked to a zone. In addition, the user will be able to edit any element in the <text> of the TEI file directly, to add transcription, markup, @xml:id attributes, etc. We do not envisage that the majority of XML editing will be done in the Image Markup Tool itself; rather, the base transcription would be done using an XML editor such as oXygen, and the file brought into the IMT for the definition and linking of images and zones. However, it will be important to allow direct editing of the XML code so that corrections and changes can be made without moving the file back into an XML editor.The web platform of the Image Markup Tool fills a gap in current collaborative editing models, and will provide a lightweight “edit-anywhere” version of the desktop application. The web-platform will be built for a large population of needs, but our first user-case study will be the Editing Modernism in Canada Project (EMiC).http://editingmodernism.ca Rather than following past practices of transcribing texts and marking up transcriptions in the creation of electronic texts, EMiC and its partners will pioneer image-based editing, semantic markup, analysis, and visualization of texts in a field of emergent practices in digital-humanities scholarship. Instead of producing reading environments based on linear-discursive transcriptions of texts, EMiC will produce in collaboration with its partners techniques and technologies for encoding and interpreting the complex relations among large collections of visual and audial objects in non-linear reading environments.Our rationale for a web-based browser application includes facilitating RESTful architecture and interoperability with other systems via API (including, for example, Scripto,http://www.scripto.org developed at the Centre for History for New Media at George Mason University). The IMT web-platform will allow a user to load images and XML documents into a browser window (either locally or via URL). The drawing and linking of the polygonal <zone>s now supported in the TEI schema are made possible with the HTML5 <canvas> element. As in the desktop application, it is assumed that the user will perform most TEI markup with an XML editor (such as oXygen), but the application will also support lightweight XML editing. The most powerful feature of the web-based application will be the potential to feed linked XML documents and images directly into a collection-builder such as Omekahttp://www.omeka.org to facilitate scholarly edition building and electronic publishing via a suite of tools.", "article_title": "Image Markup Tool 2.0", "authors": [{"given": "Martin", "family": "Holmes", "affiliation": [{"original_name": "Humanities Computing Media Centre, University of Victoria", "normalized_name": "University of Victoria", "country": "Canada", "identifiers": {"ror": "https://ror.org/04s5mat29", "GRID": "grid.143640.4"}}]}, {"given": "Meagan", "family": "Timney", "affiliation": [{"original_name": "Electronic Textual Cultures Laboratory, University of Victoria", "normalized_name": "University of Victoria", "country": "Canada", "identifiers": {"ror": "https://ror.org/04s5mat29", "GRID": "grid.143640.4"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Lurking is a term that has gained popularity with the advent of online communities. Museums are no exception to this renewed interest in community building, both online and onsite. The Internet has helped museums to better serve their communities, connecting physical events and exhibitions with online services, information, and activities. The greater museum community is compartmentalized into different groups, including educators, scholars, teenagers, families and more, but their dues-paying members are perhaps the closest to what is more commonly known as affinity spaces (James Gee) or knowledge-sharing communities of practice (CoP). Many of the membership groups organize events at the museum, raise funds for the museum, socialize regularly, and even have online profiles, blogs, or pages on social media sites. The term lurking has arisen because online communities have high expectations for their members to participate and contribute, in particular with online games and chat forums. The web usability expert Jakob Nielsen (2006) proposed the well-known 90-9-1 rule of user participation in online communities, which states that 90% of users are lurkers, 9% of users contribute from time to time, and 1% of users account for most contributions. The more passive acts of being present (virtually or physically), listening, watching, and reading – that is, lurking – are considered negative when contrasted to the more dynamic acts of writing, contributing information, performing tasks, or discourse that are all viewed as essential to the formation and maintenance of community. Even the spectatorial can be considered negatively, as in voyeurism that is often perceived as leading to perverse and criminal acts. Analogous to the conventional concept of community is the idea of a social network as a system of individual nodes that are all related, first proposed by J. A. Barnes in the early 1950s. The concept of the network society has been best developed by Manuel Castells (2000, p. 12) who states that, “The ability of an actor in the network – be it a company, individual, government, or other organization – to participate in the network is determined by the degree to which the node can contribute to the goals of the network… This leads to a binary process of inclusion and exclusion from the network. The people at the bottom are those who, with nothing to offer the network, are excluded.” Similar to lurking but more related to economics is the free rider problem, as it can be argued that every community – regardless of its size or nature – offers a public good and is based upon some type of exchange system. Art museums are spaces that have traditionally encouraged lurking, as visitors are invited to leisurely appreciate works of art in a reverent environment that prioritizes observation, contemplation, learning, and personal interpretation. There are some exceptions, however, such as with participatory art practices that gained prominence in the early 1960s and continue to be exhibited in museums today, dependent on visitors’ active participation for their realization. Museums have entered the digital age just as have other traditional socio-cultural institutions, and consequently they are incorporating new technologies for the purposes of facilitating exhibition, interpretation, education, and participation. The modern museum presumes that visitors – especially younger ones (digital natives) – expect a more interactive museum experience that allows them to actively participate and even share their opinions within a community that is becoming perceptibly less hierarchical and authoritarian. New technologies offer tremendous possibilities for all visitors to engage more deeply with art, but they can also distract from the passive acts of contemplation and observation if they demand too much physical interaction. Nevertheless, art museums remain a trusted and respected place in which to observe, think, feel, and learn. This paper will assert that lurking is a necessary and useful part of community engagement and learning in the digital age, and that art museums are a valuable and unique space for such activity. Some specific technologies used by art museums today will be discussed in the context of whether they promote a more active or passive experience. A critical distinction must also be made, however, between the terms interaction and participation, the latter of which is more open-ended than the former. In writing about participatory culture and the digital age, many scholars such as Henry Jenkins and Mikuzo Ito discuss the importance of participation amongst youth while also stressing the importance of other skills, characteristics, and stages of learning that could easily be construed as lurking. We understand from Richard Bartle’s (1996) taxonomy, D.T. Schaller et al.’s (2007) four learning preferences, and Ito et al.’s (2008) three genres, that there are many ways to engage in activities, including both play and learning. The more active forms include creating, producing, sharing, contributing, playing, and commenting. These forms of participation are the most visible to the community and as such, are most prized in that they serve as an example for other members to emulate. More passive forms include listening, reading, watching, and browsing, as well as the introspective acts of thinking, reflecting, evaluating, and forming opinions. When these passive acts are shared with other individuals, discourse arises (physical or virtual), which can then be considered a more active and public form of participation. Different personalities also emerge within groups, and more active contributors will try to lead others less inclined to participate. Jenkins et al. (2008, p. 7) assert that, “In such a world [participatory culture], many will only dabble, some will dig deeper, and still others will master the skills that are most valued within the community.” This paper will show that all types of participation are essential to the development and maintenance of communities. The formation of knowledge and learning is a linear process that begins with more introspective and often individual acts, which in many instances then become public when thoughts, creations, and knowledge are shared with others. If thoughts are not shared and knowledge remains private, then the learning process merely continues with the individual. The negative implications of lurking arise mostly from within the communities themselves. What cultural communities tend to forget, however, is that lurkers play an important role; they provide an audience, they carefully observe community norms and practices, and they contemplate and interpret that which they observe to perhaps share later with others. Performers do not want to perform without an audience, writers do not want to write books that nobody reads, and museums cannot open without visitors. Certainly, artists have a personal drive to create that does not require an audience or even a client, but even the most dedicated artist desires feedback, validation, or public acknowledgment. Physical spaces and organizations have long measured success in quantitative terms of the number of seats filled, visitors through the front door, or books or tickets sold. Online spaces are no different; the success of websites are most commonly measured by the number of clicks, page views, or downloads, rather than the more interactive number of comments, links, or uploads. As Web 2.0 features become more common on websites, these interactive metrics will surely begin to matter more. Still the number of people “passively” watching, reading, and listening are a strong measure of success, as well as a strong incentive for creators, organizations, and funders alike. Museums need to protect the act of lurking. Any expectation of active participation, interaction, and sharing may inhibit lurkers from eventually participating, for lurkers need not be defined as having rigid characteristics but rather as representing merely one stage in the long process of learning and civic engagement. In its recent report Museums & Society 2034: Trends and Potential Futures, the American Association of Museums (2008, p. 19) states that, “While technological progress has brought much value to society, one byproduct of these emergent structural shifts in communication technologies is almost certainly going to be a world with fewer and fewer places where the public can find respite and retreat.” Through their expertly researched and curated exhibitions and related public programming, museums are best able to teach their visitors and members how to observe, how to critically think, and how to develop opinions in order to more effectively act on them if so desired. They can also teach the value of being an audience within a socially networked environment, which is the first step to recognizing the importance of community and public goods. This paper will discuss how museums might continue to encourage lurking in synergy with new digital technologies, and likewise how youth can become empowered to not only act, but also to observe and contemplate. As Jenkins et al. surmise (2008, p. 39), “…knowing how to act within the distributed knowledge system is more important than learning content. Because content is something that can be ‘held’ by technologies such as databases, websites, wikis, and so forth, the curricular focus is on learning how to generate, evaluate, interpret, and deploy data.” Those who lurk in museums (online or physically) are not evading their role; they do have a very important role within their community and within the process of acquiring, forming and sharing knowledge, and even more so in the participatory culture of the digital age.", "article_title": "Lurking in Museums: In Support of Passive Participation", "authors": [{"given": "Susana", "family": "Smith Bautista", "affiliation": [{"original_name": "University of Southern California", "normalized_name": "University of Southern California", "country": "United States", "identifiers": {"ror": "https://ror.org/03taz7m60", "GRID": "grid.42505.36"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "I have already argued against the widely held belief that Henry James's switch from handwriting to dictation caused a radical change in his style (Hoover 2009). However, the wider question of how mode of composition affects literary style remains open, and James might be the exception rather than the rule. I report here on some preliminary studies for a more comprehensive examination of writers who changed their mode of composition either temporarily or permanently. The three authors examined here, Thomas Hardy, Joseph Conrad, and Walter Scott, present clear cases of changes from handwriting to dictation (and back), ones in which the details of composition are well known, and in which the changes in mode of composition take place within a single text.The case of Thomas Hardy is slightly problematic because he was not always truthful about his wife's role in the production of his books, and in some cases burned some MS pages in her hand. For A Laodicean, one of his less important novels, however, the facts seem fairly clear. After sending the first three (of thirteen) installments of the serial version to the printer, Hardy fell ill, suffering from some kind of bladder inflamation. He struggled through the fourth installment, but his doctor then gave him the choice of lying with his feet higher than his head or an operation. Choosing the former required him to dictate much of the novel, though he was able to correct proofs. He eventually became “less and less dependent on dictation, writing the final sections of the manuscript in his own hand.” He mentions writing parts of installment twelve in a letter (Milgate 2006: 204), and this suggests he also wrote part thirteen.To test for any dramatic effect of the switch to dictation, I divided the novel into four parts, the handwritten installmens1-4, the dictated 5-11, and the partially or wholly handwritten 12 and 13, and further divided these installments into sections of about 9,000 words. As can be seen in Fig. 1, the sections of the novel strongly tend to group together chronologically, though the beginning, 1-4 HW (1), is somewhat unusual, as often happens with the beginnings of novels, and 5-11 D (5) is also an outlier. At first, Fig. 1 seems to support a distinction between handwritten and dictated parts, but the presence of the largely handwritten installments 12 and 13 among the dictated parts suggests that narrative structure is a more potent force than mode of composition. Further work will be necessary to test other characteristics of the text, but the analysis shown in Fig. 1, along with many others based on shorter sections, does not suggest that Hardy's mode of composition radically affected his style.  Fig. 1–Handwriting and Dictation in A Laodicean Full Size Image   Joseph Conrad presents a more complex problem. Several of his texts were partly dictated, including three I will examine here, the novellas The End of the Tether and The Shadow-Line, and novel The Rescue. Conrad dictated the second serial installment of “The End of the Tether” to Ford Maddox Ford under time pressure after part of the manuscript was accidentally burnt. I have separated the beginning and the burnt installment from the rest of the story, and have analyzed the parts in sections of about 2,600 words. As Fig. 2 shows, the first two sections of the dictated (burnt) installment cluster with the handwritten beginning of the story, while the last section clusters with the handwritten rest of the story. The narrative structure is again quite clear here, but there is nothing to suggest that dictation altered Conrad's style.  Fig. 2–Handwriting and Dictation in “The End of the Tether” Full Size Image   About one-fourth of The Shadow Line, beginning a little more than half-way through the novella, were dictated. Conrad himself ponders the possible effect of mode of composition, suggesting that “it will be curious for critics to compare my dictated to my written manner of expressing myself” (Conrad 1983: 543) . As with The End of the Tether, however, there is little evidence of any affect of dictation on the style of the novel, as Fig. 3 shows. Analyses based on different numbers of words vary somewhat, but the separate cluster containing the first four sections of the novel is very stable, and all analyses group the fifth handwritten section of the beginning of the novel with the dictated sections that immediately follow it. Again, narrative structure trumps any effect of the change in mode of composition.  Fig. 3–Handwriting and Dictation in The Shadow-Line Full Size Image   The composition of The Rescue is unusual in that Conrad first worked on this novel from 1896 to 1898, but did not finish it until 1918-1919. The early part was handwritten, while the end was dictated. Although Conrad suggests that the novel might prove interesting as a case of style evolution, readers have found the style “homogeneous” (Karl 1979: 816). Here I divided the novel into dictated and handwritten parts, and cut them into sections of about 5,000 words for analysis. As Fig. 4 shows, the first two dictated parts cluster with the handwritten parts, and the 7th handwritten section clusters with the dictated parts. This pattern is extremely stable in analyses based on the 990-600 MFW, and all analyses show a mixing of sections produced by the two modes. Further tests based on other textual features will be needed to make the case more strongly, but, for Conrad, as for James and Hardy, the mode of composition has no obvious effect on style.  Fig. 4–Handwriting and Dictation in The Rescue Full Size Image   Turning to Walter Scott, we find a different scenario. About a third of the way through his career, Scott was writing very rapidly in an attempt to pay off an enormous debt. While writing The Bride of Lammermoor in 1818-19, he suffered from increasingly severe stomach pains (probably from gall-stone disease) that prevented him from writing, and finished the novel by dictation, though it is not entirely clear exactly where the dictation begins. The final extant MS leaf corresponds to Chapter 26 (of 33), but it ends with a catch word and has corrections for later leaves on the reverse (Milgate 1987: 170). We can be sure, however, that much, and probably most, of the last seven chapters were dictated. I have divided the novel into the part corresponding to the MS and the rest, and have divided both parts into sections of about 5,000 words. The important peculiarities of the pattern shown in Fig. 5 remain constant over many analyses. The first part of the novel for which no MS exists clusters (loosely) with the final two chapters of MS (though also with handwritten sections 6-12), but the handwritten sections 13-17 cluster with the handwritten beginning of the novel and the other three sections for which no MS exists. This peculiar pattern needs further study, but it does not support a difference between Scott's dictated and handwritten styles.  Fig. 5–Dictation and Handwriting in The Bride of Lammermoor Full Size Image   Because of continuing stomach pain, Scott also dictated about half of Ivanhoe, but finished the novel by hand after he began to recover. I have divided the novel into handwritten and dictated sections of about 9,000 words. The analysis in Fig. 6 shows that here, as before, there is no evidence of a significant shift in style when Scott's mode of composition changed. The last dictated section clusters with the following MS sections, while the next to last MS section, MS (9), clusters with the dictated sections. When fewer words are analyzed MS (9) shifts to the MS cluster, but Dict. (9) then also moves into the MS cluster. The most reasonable interpretation of this behavior is that the last dictated sections are similar to the adjacent MS sections, and that the important factor is again narrative structure rather than mode of composition.  Fig. 6–Handwriting and Dictation in Ivanhoe Full Size Image   More analysis of other cases, especially those involving typewriting and word-processing, where some evidence for significant effects exists from composition studies, will be needed before any strong generalizations are possible. Yet the evidence from James and the three authors examined here strongly suggests that mode of composition has remarkably little effect on authorial style.The tempting conclusion that the authors' revisions may have erased any effects of the changes in mode of composition has some support from the heavy revisions of James and Conrad, but Hardy was apparently not a heavy reviser, and Scott famously revised very little.", "article_title": "Modes of Composition in Three Authors", "authors": [{"given": "David L.", "family": "Hoover", "affiliation": [{"original_name": "New York University", "normalized_name": "New York University", "country": "United States", "identifiers": {"ror": "https://ror.org/0190ak572", "GRID": "grid.137628.9"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This short paper presents the ongoing work and the considerations behind the project “Visualising Textuality – New Interfaces to Historical Texts”.The project, supported by the EU FP7 Marie Curie Scheme, aims at implementing a “knowledge environment” (Siemens et al) to explore and understand better early medieval textual practices and pre-scholastic Christian scholarship. The project starts from a manuscript from the University Library of Würzburg (M.p.th.f.61), dated back to the second half of the 8th century AD and most likely of Irish provenance. This is a parchment manuscript with 34 leaves containing a text of Matthew’s gospel along with extensive interlinear glossing and 30 cedulae (parchment slips) containing commentary material bound between the pages (Fig. 1). Fig. 1: Excerpt from M.p.th.f.61 with comments on parchment slips. Full Size Image  It is also an intriguing object of historical studies: its numerous glosses (interlinear and marginal) as well as commentaries (on parchment slips) allow insight into practices of compilation and use of this manuscript (cf. Fig. 1). However, it poses many challenges to editors and researchers: different layers of writing (strata) have been identified; the arrangement (mise-en-page) of glosses and commentaries in relation to the Gospel text and to each other conveys important information but is not easy to follow; the “intertexts” cross the logical segments of the text and the physical boundaries and dimensions of the document pages; the texts themselves, especially the commentaries, have their own history of transmission; and they recite, vary from and refer to other commentaries such as Eusebius, Jerome or Isidor, who also refer to other commentaries and biblical texts etc. Cahill summarizes that the Würzburg Matthew is “a complicated jumble and not a tidy bundle” and requires further research (2002:25). His statement applies to all levels of investigation: the physical (document), the logical (texts and contexts) and the level of space and time (cf. Fig. 2). Fig. 2: Model illustration of the complex network of relations based on the Würzburg glosses (not complete). Full Size Image  It is the aim of this research project to find adequate means for representing such a complicated network of information, for visualising these relations and for allowing the researcher to navigate through this data in form of a “knowledge environment”. The fact that this all is a “complicated jumble” has to be seriously taken into consideration. The data that is outlined here consists of numerous combinations – of semantic relations, chronological dependencies, spatial transmissions, topological information – each of which may carry an important detail for research. What serves for the first time for an in-depth and comprehensive research, has its risk in getting lost in information. Thus, it seems to be crucial for the scholar to focus his or her attention up on what (s)he is interested in. Support for this needs to be provided by the knowledge environment.The project is in the first year of its three years implementation plan. At the time of the DH conference (June 2011), most of the textual work on the manuscript shall be completed, so that preliminary considerations about the setup of the envisaged knowledge environment and what is more important about the underlying data model will be discussed and presented. This data model is an abstraction of document, text and context and encompasses entities of different types (such as segments of texts, zones in a document, time, space, scribes, sources, metadata) and relations among them (such as “origins from”, “uses”, “was written by”) as well the interface to external data (such as the Patrologia latina database) to link to text variants and contexts.", "article_title": "A Data Model for Visualising Textuality – The Würzburg Saint Matthew", "authors": [{"given": "Malte", "family": "Rehbein", "affiliation": [{"original_name": "Würzburg University", "normalized_name": "University of Würzburg", "country": "Germany", "identifiers": {"ror": "https://ror.org/00fbnyb24", "GRID": "grid.8379.5"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "The growing amount of digital texts and tools is slowly but definitely changing the way literary scholars design their research. This paper will describe the effects for one research topic: ‘literary onomastics’, the study of the usage and functions of names in literary texts. Research in literary onomastics usually is qualitative in nature and focuses on 'significant' names in literary texts. No quantitative or comparative studies have been published yet. Several researchers, however, have pointed out that names can only be called significant if they are studied in the context of all the names - the so-called 'onymic landscape' - in a text, oeuvre, genre etc. (e.g. Sobanski 1998). This question is comparative by nature and implies the wish for a more quantitative, computer-assisted approach.As to name usage, first we have to find out what is normal. We want to know how many name forms usually occur in a novel, and how many of these are personal names, place names, and other names. As to name functions, a useful disctinction is between plot internal and plot external names. Plot internal names refer to characters, places or other entitities which only ´exist´ in the fiction of the story. Plot external names refer to persons, places or other entities which are known to exist or to have existed in the real world. Most place names are plot external, referring to real countries, cities, streets, etc. and thus have a reality enhancing function. In fantasy novels, however, place names are usually fictional and thus plot internal, enhancing the unreality, the fantasy of the story. Plot external personal names seem to function as characterizations of the fictional characters, describing e.g. their political or cultural preferences. It is expected that different authors, genres, time periods or even languages apply these different types and functions in different ways, showing different trends which we want to discover in what we like to call comparative literary onomastics (Van Dalen-Oskam 2005, 2006). The ultimate aim of the research is to develop a method to compare the name usage and functions on as large a scale as possible, explicitly also across languages. So what is needed to do that?The amount of names in a text can be expressed in the percentage of the total amount of tokens in the text. For that, we need digital texts of fair to good quality. To tag all the names, we need named entity recognition and classification (NERC) tools. Different forms of the same name (e.g. Patrick and Patrick's) need to be grouped by a lemma (PATRICK in this case). Different name forms for the same person or place need to get the same identification (e.g. the name Alfred and the name Issendorf both belong to the character identified as ALFRED ISSENDORF). To find out whether we can compare the resulting percentages across languages, we focused on a corpus of modern Dutch and English novels and their translations into the other language. We found we have to include two other levels of aggregation: mentions and name tokens. Mentions are occurrences of a name irrespective of the number of tokens used. So several name tokens can be used in one mention. This distinction is necessary because different languages have different tokenization rules. The Dutch personal name Gerrit-Jan, e.g., with a hyphen and therefore counted as one token, is translated in English as Gerrit Jan, resulting in two tokens. Our corpus consists of 22 Dutch and 22 English novels, added with the translation into the other language of ten in each group.Comparative research can only be done when many scholars collaborate. We will have to make sure that all those scholars encode their texts in the same way, considering the same tokens as names. This may sound easy, but it is not. Even name theorists have different definitions of what a name is (Van Langendonck 2007). Guidelines had to be set. We decided to limit the tagging to the 'prototypical' names, so those names that are considered names by readers in general. Something is a name if it refers to a unique person, place, or object. So we excluded currencies, days of the week, months, etc. For cases leading to discussion we defined additional rules.We found that not many modern novels are digitally available yet. Furthermore, NERC tools proved to be not good enough yet and are using partly different categories than our research needs (Sekine and Ranchod 2009). This meant much more manual work than we expected. The tagging and the statistics are now done by means of several perl scripts written by André van Dalen and an ingenious Excelsheet with macros designed by David Hoover. These tools can be seen as a prototype for the webservices we need for this type of comparative stylistic research.In Figure 1, the percentage of name tokens in a set of novels is shown. The selection of novels in this graph consist of sixteen novels of which fifteen are Dutch and one is (American-)English (Of the Farm), added with the English translation of one of the Dutch novels (The Twin, translation of Boven is het stil).The graph gives as a first impression that we can expect around 2-3 percent of the tokens in a novel to be names. For this small selection, it is noteworthy that the four novels with the highest percentages (around 5 percent) are all books for children or young adults. Furthermore, the English novels do not show up in extreme positions compared to the other ones. Figure 2 gives more insight in the role of plot internal versus plot external names in the same set of novels. The novels are presented in the same order as in Fig. 1. We can see that the four books for children/young adults are still exeptional in the percentage of plot internal names. We also find that three of them have a rather small amount of plot external name tokens. The paper will present the case of names in Gerbrand Bakker’s novel Boven is het stil / The Twin. Dutch readers have the impression that the novel does not contain many names, while readers of the English translation have the opposite impression and point out that especially geographical names abound. The comparative computational approach shows that both reader groups are correct and wrong at the same time. The novel has around 2% of name tokens, which is at the lower end of what seems to be normal. But in the amount of different names (lemmas), geographical names (place names) do have a special role when we look at the ratio between personal names and place names (Figure 3). The trend here is that a novel contains more different personal names than place names. In only two cases (Boven is het stil and its translation The Twin, and Fenrir) a novel has more different place names. This suggests that place names have an extra stylistic function in these two novels. It will be shown in which ways place names function in the Bakker novel and how this analysis enriches the understanding of the novel and of the stylistic possibilities of place names for an author.We could only show a small part of all interesting observations to be made about the usage of names in a corpus of modern Dutch and English novels. But these first results make us anxious for more, in the expectation that this approach may lead to an acceptable method for a.o. across language comparison of stylistic elements. We conclude that the preliminary results are sufficiently interesting to go into the stylistic analysis of name usage and functions in novels more deeply. Names also seem promising stylistic elements for a comparison across languages. The currently available tools which could be expected to be helpful for this type of research, proved to be insufficient. We therefore plan to develop a set of interrelated webservices which will assist the scholar in the recognition, categorization, further tagging, and statistical analysis of names in novels.", "article_title": "Names in Novels: an Experiment in Computational Stylistics", "authors": [{"given": "Karina", "family": "van Dalen-Oskam", "affiliation": [{"original_name": "Huygens Institute for the History of the Netherlands - KNAW, The Hague", "normalized_name": "Huygens Institute for the History of the Netherlands", "country": "Netherlands", "identifiers": {"ror": "https://ror.org/04x6kq749", "GRID": "grid.450092.a"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Evaluating and receiving credit for digital scholarship within traditional disciplinary areas in the humanities has been a concern much discussed, not only within the digital humanities community, but at think tanks on the future of scholarly publishing, within institutions, at professional associations in the various disciplines of the humanities, and in journal and newspaper articles.For just a small sampling of recent postings and articles see Scott Jaschik, ‘Tenure in a Digital Era’. Inside Higher Education (26 May 2010); ‘Evaluating Digital Scholarship, Promotion & Tenure Cases’, Office of the Dean, University of Virginia,  (link) ; ‘Guidelines for Evaluating Work with Digital Media in the Modern Languages’, Modern Language Association,  (link) ; Julia Flanders, ‘The Productive Unease of 21st-century Digital Scholarship’. Digital Humanities Quarterly (Summer 2009); Jeanne Glaubitz Cross, ‘Reviewing Digital Scholarship: The Need for Discipline-Based Peer Review’. Journal of Web Librarianship (December 2008); Joan F. Cheverie, et al, ‘Digital Scholarship in the University Tenure and Promotion Process: A Report on the Sixth Scholarly Communication Symposium at Georgetown University Library’ Journal of Scholarly Publishing (April 2009) 219-230.Over the past few years The Modern Language Association has taken the lead in encouraging the recognition of digital scholarship in promotion and tenure cases. Its 2006 Report of the MLA Task Force on Evaluating Scholarship for Tenure and Promotion ( (link) ) offered unequivocal support of digital scholarship. Among its recommendations are the following: The profession as a whole should develop a more capacious conception of scholarship by rethinking the dominance of the monograph, promoting the scholarly essay, establishing multiple pathways to tenure, and using scholarly portfolios . . . .[and] departments and institutions should recognize the legitimacy of scholarship produced in new media, whether by individuals or in collaboration, and create procedures for evaluating these forms of scholarship.Report of the MLA Task Force on Evaluating Scholarship for Tenure and Promotion.  (link) This report and others (see \"Our Cultural Commonwealth: The report of the American Council of Learned Societies Commission on Cyberinfrastructure for the Humanities and Social Sciences\" etc.) make it clear that the Humanities must broaden traditional definitions of scholarship and reconceptualize its methods of evaluation. But it still falls to us in the digital humanities to articulate the scholarly content and value of our work and to propose explicit procedures for effectively evaluating it.In 2008 the MLA’s Committee on Information Technology released an Evaluation WikiThe Evaluation of Digital Work.  (link) (based on the work of Geoffrey Rockwell who served as a member of the Committee from 2005-08) that provides a framework for departments to evaluate this new scholarship. As a result of this work, the authors of this paper led a workshop in evaluating digital work for tenure and promotion at the 2009 MLA convention in Philadelphia. The workshop was designed to provide both a framework from within which digital scholarship could be evaluated, as well as a forum for the authors to evaluate just how difficult it was for non-specialists to come to terms this new work. With this in mind, several themes were addressed in the workshop design: The impediments involved in evaluating digital scholarship, particularly in terms of interdisciplinarity, authority, and use of standards; Issues of disciplinarity: digital scholarship does not fit comfortably into the what is currently valued in the profession; The absence of expertise at the departmental level in evaluating digital scholarship handicaps both the evaluators and the candidates; The intellectual stakes of such work; The value and credit accorded to interdisciplinary, multi-institutional, collaborative work. In the field of literary studies the gold standard for tenure and promotion is the publication of the monograph. Editorial scholarship -- textual criticism, book history, scholarly editing, the sociology and bibliography of texts --particularly in North America, has been consistently devalued since the rise of literary theory. That the digital is conducive to the kinds of projects that have been denigrated by the academy (certainly since new literary criticism), including pedagogy, public humanities, and the creation of scholarly editions, has made the argument for including this work in tenure and promotion cases all the more difficult. These aforementioned themes and questions were meant to engage the group in questioning such values.The workshop was structured around case studies based on the experiences of individuals in the field. In some instances the case studies were anonymized (as several of the workshop leaders created studies based on their own scholarship), others were not (as when it was clear who the author was from the online scholarship under discussion).Attendees were seated at tables of eight and for each of three rounds had two case studies to choose from. The first round of case studies were what might term fairly traditional digital scholarly editions. This type of digital scholarship was chosen because it most resembled analogue scholarship. Nevertheless, when we opened the conversation up for discussion, the opening remark by a Departmental Chair questioned that the edition under discussion should be counted towards scholarship at all, as in her opinion, the creation of scholarly editions fit squarely into the category of service. This became a recurring theme of the workshop: individuals who did not understand the theoretical and technical imperatives behind the scholarship consistently categorized the work as service. It became clear that the production of editions, whether distributed in analogue or digital form, were not viewed as scholarship on par with critical theory.Moreover, when the authors of case studies explained the theoretical methodology implicit in their research in terms of the technologies used, non-technical participants consistently complained that the authors were engaging in techno-speak jargon. Participants also complained that the work was undertheorized: however, when it was pointed out that theories common to digital scholarship (for example, the limitations of particular metadata schemes, issues arising from the hierarchical nature of XML, or the impediments to using a standard such as TEI for genetic editing) were evoked and challenged, these participants felt that it was not sufficient to rely on theoretical perspectives not normative to the discipline of literature (i.e. coding was not satisfactory as a basis for the field).Subsequent case studies teased out issues of the implications of, in effect, outsourcing the evaluation of scholarship to (scholarly) presses and journals, the vast majority of which have not developed an economic model for publishing digital scholarship. Discussions circled around alternative models of validating digital scholarship, focusing on the work, for example, of NINES. Indeed, the success of the NINES model was so apparent in these discussions that NINES has committed its next two Summer Institutes (generously funded by the National Endowment for the Humanities) to evaluating digital scholarship.Although the writing is on the wall that our field must change to encompass new scholarship and new scholarly forms as engendered by the new technologies, the impediments to evaluating this scholarship by non-specialists are paramount. The digital archive may have been the first form to challenge the primacy of print scholarship, but new forms of scholarship are emerging that will even more radically challenge the status quo.It is clear from the research carried out by the authors in the preparation and delivery of this workshop, the creation of the CIT Guidelines, and the development of the NINES Summer Institutes, that it is imperative for the Digital Humanities community to take a more proactive role in supporting departments as they are increasingly called upon to evaluate digital scholarship. This burden cannot continue to be borne solely by each individual candidate, as is currently the norm. The overriding question for these activities must be how do we provide a framework so that evaluators can evaluate the research within the technological context within which the scholarship is undertaken.The discussion of the case studies by the participating department chairs, senior faculty members, and junior faculty members revealed to us some of the specific difficulties departments and institutions have in recognizing scholarly activity in some of its new digital forms: The evolving definitions of scholarship in language and literature over the past 50 years (Eagelton Terry Eagelton. \"The Functions of Criticism,\" How To Read a Poem (Malden, MA: Basil Blackwell 2007), 1-24.), in particular the conflict between criticism and philology as the dominant mode of scholarship; The discounting of scholarly activities like textual editing and translation, and the mislabeling of much digital scholarship as serviceConnie Moon Sehat and Erika Farr. ‘The Future of Digital Scholarship: Preparation, Training, Curricula Report of a colloquium on education in digital scholarship’. (2009).  (link) (McGann,McGann, Jerome. “A Note on the Current State of Humanities Scholarship.” Critical Inquiry 30/2.  (link) GablerGabler, Hans Walter. \"Theorizing the Digital Scholarly Edition.\" Literature Compass 7/2 (2010) 43-56.); The ongoing crisis in scholarly book publishing (Waters,Lidnsay Waters. “A Modest Proposal for Preventing the Books of the Members of the MLA from Being a Burden to Their Authors, Publishers, or Audiences.” PMLA 115 (2000): 315–17. Guillory,Guillory, John. \"Evaluating Scholarship in the Humanities: Principles and Procedures.\" ADE Bulletin 137 (Spring 2005), pp. 18–33. GreenblattGreenblatt, Stephen. “A Special Letter.” 28 May 2002. 1 Nov. 2010  (link) ) Our paper will summarize what happened at the workshop, in response to which we will present action items to be undertaken by organizations such as MLA and ACH.", "article_title": "Evaluating Digital Scholarship: A Case Study in the Field of Literature", "authors": [{"given": "Susan", "family": "Schreibman", "affiliation": [{"original_name": "Digital Humanities Observatory", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Laura", "family": "Mandell", "affiliation": [{"original_name": "Digital Humanities Observatory", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Stephen", "family": "Olsen", "affiliation": [{"original_name": "Digital Humanities Observatory", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Canonical citations are references to Classical (i.e. Greek and Latin) texts that are expressed by scholars by means of an abridged canonical format (Romanello 2008; Romanello 2007). They fulfil the function of providing an abstract reference scheme for texts (somehow similar to the function of geographical coordinates to express references to places) since they allow us to express references to them no matter what particular text edition we are actually looking at. For example, the reference to the twelfth book of Homer's Iliad expressed as “Hom. Il. XII 1” can be resolved to the text of the same passage as established in various critical editions of that work, where “XII” and “1” are the “coordinates” that allow us to locate that precise text passage within all critical editions of Homer's Iliad.Historically, canonical references are the result of an effort – whose origins can be traced back to the Renaissance (Martin 2003; Berra 2011) – made by the scholarly community as a whole to provide a precise, stable and shared way to refer to Classical texts. Since the early stages of Humanities Computing and Digital Humanities (Bolter 1993; Crane 1987; McCarty 2005), canonical references were regarded as the ideal candidate on which to experiment the potentialities of hypertext: indeed they can be seen as hyperlinks in potentia pointing a text from within another. More recently (Crane et al. 2009) they were considered as a discipline-specific kind of named entities that Classics scholars should be provided with tools to search for within their texts.J.D. Bolter describes classical philology as “the art of explicating an ancient text by exploring its relationships to other specific texts and to the corpus of ancient literature as a whole”. In such a discipline the act of referring to texts – that J. Unsworth has listed among the “scholarly primitives” (Unsworth 2000) – becomes even more crucial than in other disciplines since texts are the very research objects of classical philology and references to them play a key role in constructing argumentations.As N. Smith (2009) has already pointed out, canonical citations reflect an ontological view of texts in this specific domain and specifically how classicists perceive ancient texts as objects. In this paper we present the Humanities Citation Ontology (HuCit) (link) , an ontology that aims at characterising the semantics of citations as they are normally conceived and used in humanistic disciplines. We claim that the specification of such an ontology is worthwhile for at least the following reasons:       it allows us to disentangle from an ontological point of view the complex relationships between, for instance, a canonical reference found in a journal paper and the manuscripts and editions of the text that we can access via that reference; it allows us to define types of references and alternative representations of the same reference: this is an important step towards tools that allow automatic formatting of such references according to various styles (as it happens already for modern bibliographic references with Zotero); it provides us with a way to access the meaning of canonical references beyond their surface appearance, which might vary substantially as in the case of “Hom. Od. I 1” and “α 1”, two canonical references to the same passage but conforming to different citation styles.From an initial analysis of the relevant literature we concluded that none of the existing ontologies actually model the deep meaning of canonical references. An interesting attempt to formalise citations by means of an ontology is CITO (Shotton 2010) which however looks exclusively at modern bibliographic references and focuses in particular on citation types. As it was observed already (Smith 2009; Mimno et al. 2005), the distinction made by FRBR (Functional Requirements for Bibliographic Records) between a work, its expressions and its manifestations can be adapted to represent texts in the Classics domain as well. In this paper we propose an initial implementation of a canonical reference ontology based on FRBRoo which is the result of a process aimed at harmonising FRBR with the CIDOC Conceptual Reference Model (CIDOC-CRM) (Doerr & LeBoeuf 2007).                     A key aspect we have to face is to determine at which ontological level of the cited object a canonical reference is pointing to. A citation such as “Hom. Od. I 1” is it referring to the abstract notion of Odyssey (i.e. a work in the FRBR model) or to a particular version (e.g. edition, translation, etc.) of that work (i.e. a FRBR expression)? It might help to observe that this reference can be solved by a human reader for example into the text of that passage in French translation: therefore it is not being specified at the expression level. The textual coordinates of the citation, namely “first line of the first book”, expressed by the string “I 1” clearly refer to a logical citation scheme that applies already to the abstract notion of Odyssey (i.e. a FRBR work). Thus we can say that a canonical citation follows a given citation scheme that characterises a particular literary text and might differ from one to another. That citation scheme is a conceptual object and is the result of the work of scholars to guarantee the ability of citing literary texts.To illustrate the notion of logical citation scheme as opposed to a physical one let us examine a single case, that is the Athenaeus' Deipnosophistae. Scholars cite this work by means of canonical references that follow a logical citation scheme derived from a physical one (e.g. “Ath. Deipn. XV 694 e-f”). The textual coordinates “694 e-f” refer to the pagination of the edition of the text by Isaac Casaubon dated 1598, and specifically to sections “e” to “f” of page 694 of that edition. At first it seems a physical citation scheme. But since all editions after Casaubon's provide the readers with marginal numbers referring to that pagination it became a logical citation scheme: indeed 694 does not refer anymore to a physical page within more recent editions such as Olson's.Canonical citations have both form and content. Different citations might differ by form but they can still have the same content. The content of a citation is the abstract reference of which that citation is an expression. For example, a citation to the first line of Homer's Iliad can be written in several ways according to different citation styles. Nevertheless “Hom. Il. I 1”, “Α 1” and “Homer, Iliad 1.1” are different expressions of the same canonical reference to Homer's Iliad (cited by book and then by line). Given all these reasons, we propose to introduce - among the others - the classes Citation, Reference and Citation_Style to the \"E28 Conceptual Object\" branch of CIDOC-CRM (we will discuss the details of this approach at the conference, also in the light of most recent activities to harmonise CIDOC and FRBR). Further work is then required in order to extend this conceptual model so that it can support more complex reasoning tasks, such as translation mechanisms among different citation schemes, or the automated extraction of citations from non structured materials.To sum up, in this paper we describe the implementation of an ontology to model canonical references that builds upon the solid conceptual models already defined by CIDOC-CRM and FRBRoo. In the framework of a Classics cyberinfrastructure (Crane et al. 2009), such an ontology is meant to support the interoperability of tools that are being currently developed to extract (Romanello et al. 2009), retrieve (Smith 2009) and solve (Ruddy & Rebillard 2009) canonical references.", "article_title": "An Ontological View of Canonical Citations", "authors": [{"given": "Matteo", "family": "Romanello", "affiliation": [{"original_name": "Kings College London", "normalized_name": "King's College London", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/0220mzb33", "GRID": "grid.13097.3c"}}]}, {"given": "Michele", "family": "Pasin", "affiliation": [{"original_name": "Kings College London", "normalized_name": "King's College London", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/0220mzb33", "GRID": "grid.13097.3c"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Racing the Beam: The Atari Video Computer System, by Ian Bogost and Nick Montfort, inaugurated the Platform Studies series at MIT Press in 2009. We’ve coauthored a new book in the series (currently under contract and final review, MIT Press), Codename: Revolution: the Nintendo Wii Video Game Console. Platform studies is a quintessentially Digital-Humanities approach, since it’s explicitly focused on the interrelationship of computing and cultural expression. According to the series preface, the goal of platform studies is “to consider the lowest level of computing systems and to understand how these systems relate to culture and creativity.” In practice, this involves paying close attention to specific hardware and software interactions--to the vertical relationships between a platform’s multilayered materialities (Hayles; Kirschenbaum), from transistors to code to cultural reception. Any given act of platform-studies analysis may focus for example on the relationship between the chipset and the OS, or between the graphics processor and display parameters or game developers’ designs. In computing terms, platform is an abstraction (Bogost and Montfort), a pragmatic frame placed around whatever hardware-and-software configuration is required in order to build or run certain specific applications (including creative works). The object of platform studies is thus a shifting series of possibility spaces, any number of dynamic thresholds between discrete levels of a system. As with the “text” in recent textual studies (McKenzie; McGann), the “platform” in platform studies is actually observed in action, as one or more transactional events, defined in the act of observation or performance. In this sense, platform studies examines the ways in which material conditions of computing systems determine (by constraining and affording) the experience of creative or expressive works.Although there are now competing systems coming on to the market from Microsoft (Kinect) and Sony (Move), Nintendo’s Wii (2006) was the first major video game console to be based on motion control, to make use of what Jesper Juul has called a mimetic interface to capture player movements in physical space and represent those movements in the game world. This paper will explain and demonstrate in precise terms how that’s made possible through the use of a relatively off-the-shelf multichannel system of infrared (IR) and Bluetooth communications, along with data collected dynamically by the Analog Devices ADXL330 triple-axis accelerometer, a MEMS (Micro Electronic Mechanical Systems) device, a chip inside the wand-like Wii Remote--a tiny machine, with moving parts that measures motion along X, Y, and Z axes in relation to gravity. In this paper we’ll show how this piece of hardware with accompanying code is used to design new software for the Wii, and we’ll illustrate some of the specific constraints and affordances of the accelerometer by tracing the particular case study of third-party software developer Ubisoft’s Samurai-Western FPS games, Red Steel I and II. The two games in this franchise mark points on a brief timeline, starting with the release of the first realistic action adventure game in 2006, as an exclusive title for the original Wii platform, followed by its relative critical failure, and then, four years later, the subsequent introduction of a “reboot” sequel, with completely revamped, cel-shaded, manga-style aesthetics, and all-new swordplay mechanics. This second game was timed to coincide with Nintendo’s release of the controller add-on, Wii MotionPlus, a second MEMS device that plugs into the base of the Wii Remote and contains a “tuning-fork” gyroscope (the Invensense IDG-600), and thus adds more measurable dimensions to the system, giving the controller pseudo-global sensitivity and something closer to 1:1 mapping of player motions with in-game events. (This retrofit solution has now been superseded with the release of a new Wii Remote that ships with integrated versions of the two MEMS devices combined in one controller.)That timeline, from 2006-2010, clearly marked a learning curve for Ubisoft’s developers (including moving to the LyN 3D game engine, designed for the Wii, to make the second game), and the radical changes they made over the intervening four years serve as a vivid example of creative works being shaped in response to (and anticipation of) the constraints and affordances of a specific platform. The story of the two Red Steels highlights an important fact--that a video game console is in effect a platform for production, transmission, publishing, and player reception, all in one system. In this way, a game platform--even at the lowest level--is an inherently social phenomenon, determined by developer as well as consumer and player expectations. Drawing from textual studies’ idea of the social text, we’ll argue that the “social platform” that is the object of platform studies is “social” in a particular sense: it’s based on the relational materialities that both constrain and afford the production, transmission, and reception of creative works, the whole process that links hardware and software design to the player and the wider culture. This paper will illustrate this larger theoretical point by explaining one chain of concrete materialities on multiple levels in the case of Red Steel for the Wii--from tiny MEMS devices to in-game graphics, programming to marketing, transistors to cultural contexts.", "article_title": "The Object of Platform Studies: Relational Materialities and the Social Platform (the case of the Nintendo Wii)", "authors": [{"given": "Steven E.", "family": "Jones", "affiliation": [{"original_name": "Center for Textual Studies and Digital Humanities, Loyola University Chicago", "normalized_name": "Loyola University Chicago", "country": "United States", "identifiers": {"ror": "https://ror.org/04b6x2g63", "GRID": "grid.164971.c"}}]}, {"given": "George K.", "family": "Thiruvathukal", "affiliation": [{"original_name": "Center for Textual Studies and Digital Humanities, Loyola University Chicago", "normalized_name": "Loyola University Chicago", "country": "United States", "identifiers": {"ror": "https://ror.org/04b6x2g63", "GRID": "grid.164971.c"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This paper reflects on the process of designing and building a documentation and archiving tool named CloudPad on the basis of its first evaluation at Stanford Libraries and the San Francisco Art Institute in September 2010. The paper explores the value of CloudPad and its ability to document individual users’ replay of an artwork within the context of performance documentation and new media archiving, speculating on its possible use within a number of curatorial, educational and creative contexts that are relevant to digital humanities.The CloudPad was developed in 2010 by a team in Horizon RCUK-funded digital economy research and involved staff in performance studies and computer science from the Universities of Exeter and Nottingham, with partners from Stanford Libraries, the Ludwig Boltzman Institute Media.Art.Research, The San Francisco Art Institute, British Library, Blast Theory, and the University of Sheffield. The work developed out of the team’s intention to research novel theoretical and practical approaches for the documentation and archiving of mixed reality performances and artworks that span both digital and physical entities (Benford and Giannachi 2011), allowing users to engage with the materials creatively over time and from different locations. The project benefitted from previous research conducted by members of the team through the AHRC-funded Presence project (2004-9), which used second life and a wiki to document practices spanning from performance art, to video art and new media, including work in virtual reality CAVE, and the EPSRC-funded Creator project (2008-9) which used an e-science tool, the digital replay system, to generate synchronised annotations about a mixed reality performance (DRS). The project also benefitted from the findings of the e-dance project (2007- 9), which was jointly funded by AHRC, JISC and EPSRC, and conducted by colleagues from the Universities of Bedfordshire, Leeds, Manchester and Open University. This adopted access grid technologies for developing new approaches to choreographic composition, involving the use of the Memetic toolkit for recording, replaying and annotating sessions in access grid. Finally, the project was developed in dialogue with artworks such as Lynn Hershman Leeson’s RAW/WAR feminist film archives (2010), sosolimited’s interactive archival performances, and current thinking in new media documentation (e.g. Costello 2005, Depocas et al 2003, Jones and Muller 2008 and Dekker 2011, among others).Technically the CloudPad was designed as a customisable web-based platform aiming to facilitate the synchronised playback and mash-up of cloud-based media entities such as video or audio files, as well as webpages and photographic materials, together with layers of user annotations. It took a novel approach to the archiving and replay of pervasive media experiences by making use of Web 2.0 technologies (DiNucci 1999) rather than grid technologies. CloudPad users were empowered to view the repository as a living document in which they could leave their own impression of an experience (both of the original event recordings as well as any thematic connections or annotations provided by other visitors and subject experts). Previous interactive systems designed for the replay of events for analysis lack this level of emergent reflection (see Brundell 2008), treating the corpus of recorded material as essentially immutable. To enable this, the CloudPad made use of internet-based storage, which means that media from a wide variety of different sources could be included in a presentation (for example YouTube videos can be included and synchronised with images from Flickr). This was accomplished by the use of HTML5 (see w3.org), an emerging web standard that enables collaborative interactive applications to be developed which run inside a web browser (Murray 2005).As an initial form of content to assess the operation of the CloudPad we utilised a ‘bespoke’ documentation of Blast Theory’s Rider Spoke that was recorded by our team when the work occurred at the ars electronica festival in Linz in 2009. Rider Spoke is a location-based game for cyclists developed by Blast Theory in collaboration with Mixed Reality Laboratory at the University of Nottingham as part of the European research project IPerG. The work encouraged participants to cycle around a city in order to record personal memories and make statements about their past, present and future that were associated with particular locations (see figure 1).   Blast Theory, Rider Spoke. Participant listening to recordings. Copyright Blast Theory. Full Size Image   To collect a documentation that addressed the complexity of this work, we developed a hybrid approach. This included the collection of documentations pertaining to the artists and technologists’ descriptions of the works (in terms of original aims, interim analyses and final evaluations), as well as documentations of the user experience (see Jones and Muller 2008 and Depocas et al 2003), the latter recorded from a variety of points of view (e.g. first person, third person) and through a number of technologies (e.g., video, GPS, Wi-Fi) and perspectives (see figures 2, 3 and 4).   Linz documentation. Participant captured via first person point of view. Full Size Image    Linz documentation. Participant captured via third person point of view. Full Size Image    Linz documentation. Participants journey through the city captured on googlemaps. Full Size Image   The overall analytical approach was interdisciplinary, thus including different and potentially even contrasting accounts of the event (see Chamberlain et al 2010). These accounts were presented through a number of historic, canonic and participant ‘trajectories’ (see figure 5).   Matt Adams’ annotation about a participant linking first and third person perspectives in a canonic trajectory. Full Size Image   By historic trajectories we defined a historic event, i.e. a participant’s experience as documented in a video; by canonic trajectories we defined an expert user’s set of annotations through these materials; and by participant trajectories we defined the CloudPad user’s own annotations (Giannachi et al 2010). This architecture does not privilege a single viewpoint and encourages creative use of both the historic materials and their canonic annotations. Arguably, every replay, producing participant trajectories, re-constitutes the work.The CloudPad evaluation showed that users did not only envisage adopting the CloudPad for purposes of documentation and archiving, but also wanted to use it curatorially, to present work to others and engage users in annotating materials, for example in an online exhibition, academically, to write ‘visual essays’, and creatively, to make artwork. We have seen that the CloudPad offers scholars, artists and students the possibility to document, archive, curate and create synchronised variable media mash-ups from existing digital resources. These mash-ups, which show how users have engaged with the original documentation stored on CloudPad, build an invaluable resource for those who may be interested in how a core documentation or archive is navigated and interpreted over time. In other words, the CloudPad is not only a documentation and archiving tool, it also documents and archives itself, generating contextual footprints or traces and possibly even re-enactments of every replay of the original materials. This paper reflects on the advances generated by this particular functionality in terms of performance documentation, preservation, and re-enactment.We gracefully acknowledge the RCUK funded Horizon digital economy research and the AHRC funded Riders Have Spoken project. We would like to thank Blast Theory and staff at the Ludwig Boltzman Institute Media.Art.Research, Katja Kwastek, Dieter Daniels and Ingrid Spoerl in particular, who facilitated the documentation of Rider Spoke in Linz, and our participants and volunteers who gave their time to make this documentation possible. We would also like to thank the staff and students at the San Francisco Art Institute, Stanford Libraries and St Jose State University for providing crucial feedback that informed the writing of this paper.", "article_title": "CloudPad – A Cloud-based Documentation and Archiving Tool for Mixed Reality Artworks", "authors": [{"given": "Gabriella", "family": "Giannachi", "affiliation": [{"original_name": "Centre for Intermedia, University of Exeter", "normalized_name": "University of Exeter", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/03yghzc09", "GRID": "grid.8391.3"}}]}, {"given": "Henry", "family": "Lowood", "affiliation": [{"original_name": "Stanford Libraries, Stanford University", "normalized_name": "Stanford University", "country": "United States", "identifiers": {"ror": "https://ror.org/00f54p054", "GRID": "grid.168010.e"}}]}, {"given": "Duncan", "family": "Rowland", "affiliation": [{"original_name": "Mixed Reality Lab, University of Nottingham", "normalized_name": "University of Nottingham", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/01ee9ar58", "GRID": "grid.4563.4"}}]}, {"given": "Steve", "family": "Benford", "affiliation": [{"original_name": "Mixed Reality Lab, University of Nottingham", "normalized_name": "University of Nottingham", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/01ee9ar58", "GRID": "grid.4563.4"}}]}, {"given": "Dominic", "family": "Price", "affiliation": [{"original_name": "Mixed Reality Lab, University of Nottingham", "normalized_name": "University of Nottingham", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/01ee9ar58", "GRID": "grid.4563.4"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Faceted browsing is a recent paradigm in search interfaces that allows users with little familiarity of a subject domain to quickly explore the contents of databases or other structured data sources. The underlying principle of this approach can be traced back to the work of Indian librarian S.R. Ranganathan, who, in contrast with traditional top-down, taxonomical approaches to subject classification, in 1933 developed a method for organizing subjects in a bottom-up and non-hierarchical fashion. According to this model, a classification system can be created by combining together subject descriptors chosen from a number of non-exclusive and non-hierarchical facets – e.g., in the context of classifying books, these can be genre, date or author. This method supports the generation of a flexible system that better represents the multitude of perspectives we could use to represent knowledge (Broughton, 2004).As a result of the adoption of these ideas in computer science, researchers have been creating search interfaces that allow the exploration of digital resources through the manipulation of filters describing important features of a subject domain (Broughton 2002). A well-known pioneer in this area is Marti Hearst with her work on the Flamenco faceted browser (Hearst, 2002), followed by a number of similar approaches that, in general, aimed at creating more compelling and easy-to-use user interfaces (Hearst, 2008) (Capra, 2007) or at providing software packages that can work with different types of data sources; these may vary from manually editable JSON files (Huynh, 2007) to databases (Stuckenschmidt, 2004) and RDF triplestores (Oren, 2006). A number of projects have also proven the usefulness of this type of interfaces to the aim of facilitating the navigation of large repositories of humanities data, such as artwork images (Hildebrand, 2006), music resources (Bretherton, 2009) or multi-genre collections (McGann, 2007).The success of faceted interfaces can be related to the fact that they implement a schema-less approach to classification, that is to say, they make available to the user a number of co-existing search dimensions that can be simultaneously used to browse and preview the contents of a digital repository. Many are the proven advantages of such an approach (Perugini, 2010): first, users are never asked to 'guess' the right search terms, as it happens in classic keyword search interfaces; second, inconclusive searches are prevented; third, non-experts can easily 'get a feeling' for the significance and meaning of the data available just by looking at the available facets, thus increasing their understanding of the domain. In conclusion, this type of interfaces simplify enormously the exploration of a digital repository, and, using the words of Nowivskie, they make it easy to “explore lateral relationships” to the point that they open “possibilities for algorithmic serendipity in research” (Nowivskie, 2007).An important feature that most of the existing faceted browsers have in common is that the different facets available equally concur to the selection of a single result-type. For example, by manipulating variables such as the color and the making of cars, we can navigate a data-space of available cars; by choosing filters representing information about genres, publishers and years of publication we can easily narrow down a result list of books; or, in the context of a prosopographical database, by accumulating descriptors about people's forenames, surnames, or gender we would be able to refine our search for the individuals mentioned in the database.In our work, we intended to push the boundaries of this approach by creating a faceted browsing engine that, given the same set of selectable facets, can be used to search for ‘ontologically distant’ entity types. For example, in the context of a prosopographical database, by manipulating the same group of filtering options, we aimed at letting users search not just for people, but also for factoids and sources (cf. Figure 1). In doing so, we assumed that rich and highly interconnected humanities databases call for more powerful search mechanisms; such mechanisms should be capable of revealing the intricacies of a subject domain to the casual learner, and, at the same time, of providing a higher level of 'algorithmic serendipity' to the academic scholar. In other words, we aimed at making more visible the large number of search pathways a highly structured database can make possible - as opposed to hiding this complexity by providing a unique entry point to the wealth of data available. This means that, as shown in fig. 1, by using facets typical of the 'people' result-type (such as surname or gender) we would like to be able to search for 'sources' or 'factoids'. Or, by choosing facets typical of the 'sources' result-type (such as document category or language) we may want to filter results when searching for 'people'.With this vision in mind we created DJFacetThe software is open source and freely available online at the url  (link) , a faceted browsing engine that lets users create powerful, multi-result search interfaces. DJFacet is written in Python and is based on Django, a popular web application framework (link) that facilitates the development of database-driven websites by providing functionalities that speed up the creation of repetitive tasks. In particular, one key component of this framework is the Object-Relational Mapper (ORM), that is, a set of functions that provide programmers with a level of abstraction between the database and the application language; as a result, it is possible to invoke complex database queries without having to write any SQL code. This makes the whole application easier to manage and more portable across different database engines.By building on the functionalities of Django ORM, DJFacet provides a customizable and easy-to-use environment for creating database-driven faceted search applications. The underlying idea of DJFacet is that since a data-structure had already been designed and fine-tuned when the database was created, that same structure could be used to define the search dimensions of the faceted browser with little extra work required. An instance of DJFacet can run just by creating an initialization file in which we define which are the available facets and what 'behaviour' they have with respect to the database schema. The software then calculates automatically all the remaining query-paths needed to show the results in the various searches.At the time of writing, we tested DJFacet's approach with two humanities databases. The Paradox Of Medieval Scotland project (link) (POMS) investigates how a recognizably modern Scottish identity was formed during the period 1093-1286. Drawing on over 6000 contemporary charters, it provides biographical information about all known people in Scotland during that period. In this context we built a search interface that features 29 facets, organized into 5 groups (cf. figure 2). The result types are 3 (people, factoids and sources).The Early Modern London Theatres (link) (EMLoT) project provides its users with a major encyclopedic resource on the early London stage, as well as a comprehensive historiographical survey of the field. EMLoT identifies, records and assesses transcriptions from primary-source materials relating to the early London stage, as found in secondary-source print and manuscript documents. In this case the faceted search interface contains 24 facets, organized into 5 groups. The result types are 7 (transcription records, primary sources, secondary sources, events, people, troupes, venues).By using DJFacet it was possible to allow the formulation of queries that might not be immediately obvious to the user. For example, in POMS it became trivial to search for Charters mentioning transaction events in which 'beneficiaries' of name 'William' acquire something on the day of the 'Feast of St Patrick '. Also, the search interface provided users with more chances of coming across interesting connections in the available materials. This was made possible by the fact that the facets used in the search are ontologically distant from the respective result-types.However, despite the fact that this approach proved to be, from the logical and computational point of view, completely feasible, it also opened up a number of research questions from the point of view of the meaning of these multifaceted searches across different results types. In other words, we realized that often the accumulation of filters ontologically distant from each other could be hardly translated by the end user into real-world questions; analogously, the opposite may happen, in so far as simple type of searches may be impeded by the highly structured architecture of a faceted browser.In order to provide some answers to this issue and lay the ground for a more scientific discussion of the problem we are currently carrying out a user evaluation study with humanities scholars. The purpose of the experiment is to discover the degree to which humanities scholars can make sense of the search mechanisms provided by our faceted browser, and, indirectly, of the complex data structures often necessary for representing humanities subjects. We will report on these findings at the conference, together with a deeper analysis of the implications of using multi-result faceted browsers in the context of complex humanities datasets. The facets allowing 'entry' to a prosopographical database. Full Size Image   Screenshot of the faceted browser for the POMS database. Full Size Image  ", "article_title": "Browsing Highly Interconnected Humanities Databases Through Multi-Result Faceted Browsers", "authors": [{"given": "Michele", "family": "Pasin", "affiliation": [{"original_name": "Department of Digital Humanities, Kingʼs College, London, UK", "normalized_name": "Bansomdejchaopraya Rajabhat University", "country": "Thailand", "identifiers": {"ror": "https://ror.org/03e0h3d81", "GRID": "grid.443695.9"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "In 1991, historian Laurel Ulrich’s A Midwife’s Tale swept a little-known 18th-century midwife named Martha Ballard into the national historical consciousness. Ulrich’s work centered on the analysis of nearly 10,000 diary entries penned by Ballard between 1785 and 1812, leading to an exploration of issues such as shifting family structures, the professionalization of obstetrics, and debtor patterns in a rural economy (Ulrich 1991). My research examines the same diary, but instead of a traditional close reading of the source, I use topic modeling to mine a digitized transcription, iterating through hundreds of thousands of words in order to search for textual patterns.One of the fundamental challenges to applying text processing techniques to historical sources is one of data quality. Older, hand-written documents are often difficult to transcribe into a digital format, while the shorthand style of diary writing is often filled with abbreviations and misspellings. For instance, Ballard employs a vocabulary peppered with variations: the word “daughter” is spelled fourteen different ways: “daught,” “dagt,” “dat,” etc. One way to overcome this challenge is to use topic modeling, a method of computational linguistics that attempts to group words together based on their appearance in the text.My short paper session focuses on an analysis of a historical source using topic modeling (Blei and Lafferty 2009). As a form of linguistic analysis, topic modeling has been employed over the past several years to examine large-scale, multi-author textual databases, including historical newspapers (Block 2006), journal articles (Gerrish et al. 2010, Hall et. al. 2008), and social network data (Ramage et al. 2010). My application of topic modeling differs from many of these investigations by focusing on multiple, short texts by a single author: in this case, Ballard’s diary entries.I employed the machine learning toolkit MALLET (McCallum 2002) in order to topic model each of Ballard’s entries as separate pieces of text. MALLET, identified thirty topics, which I then labeled for clarity. The following sample topics were some of the most coherent (my own labels in bold and uppercase):Although topic modeling was useful for overcoming some of the challenges of spelling variations, its real value lies in its ability to quantitatively measure the relative thematic content of each piece of text. In the case of Ballard’s diary, MALLET assumes that each diary entry is compromised of some combination of thirty topics. An entry in which Ballard attended a sermon and purchased supplies from the general store might contain, for instance, scores of 50% for the CHURCH topic, 25% for the SHOPPING topic, and minimal or zero scores for the remaining twenty-eight topics. Associated temporal metadata (day, month, year, day of the week) allowed me to chart the behavior of certain topics over time.As a simple barometer of its effectiveness, I used one of the generated topics that I labeled COLD WEATHER, which included words such as cold, windy, chilly, snowy, and air. Aggregating its entry scores by month shows exactly what one would expect over the course of a year ( Figure 1).  Figure 1 Full Size Image  This approach also can chart patterns over the course of the diary, which covers the final twenty-seven years of Ballard’s life. Two topics tended to involve words related to HOUSEWORK. Aggregated by year, they demonstrate a steady increase in the frequency with which Ballard writes about daily chores ( Figure 2).   Figure 2 Full Size Image   Both topics moved in tandem and steadily increased as she grew older (excepting a curious divergence in the last several years of the diary). This is somewhat counter-intuitive, as one would assume the household responsibilities for an aging grandmother with a large family would decrease over time. Yet this pattern bolsters the argument made by Ulrich in A Midwife’s Tale, in which she points out that the first half of the diary was “written when her family’s productive power was at its height.” (Ulrich 1991, pp. 285) As her children married and moved into different households, and her own husband experienced mounting legal and financial troubles, her daily burdens around the house increased. Topic modeling quantifies and visualizes this pattern, one not immediately visible to a human reader.Topic modeling allows for patterns to crystallize that are imperceptible to a human reader. One topic was particularly intriguing, and included the words: feel husband unwel warm feeble felt god great fatagud fatagued thro life time year dear rose famely bu goodThese were words that seem to cover EMOTION and spiritual reflection – an abstract topic that is difficult enough for a human reader to describe. Yet the computer did a remarkable job in identifying a cohesive group of words. The topic follows a fascinating trajectory in Ballard’s diary ( Figure 3).  Figure 3 Full Size Image   Not only did Ballard write about this topic more as she grew older, but there was a dramatic leap from 1803 to 1804-1805. This corresponds quite well to the period of intense family travail: Her husband was imprisoned for debt and her son was indicted by a grand jury for fraud, causing a cascade effect in Martha’s own life. Topic modeling not only reveals the trajectory of tangible themes (housework, births, gardening, etc.), but also begins to quantify and visualize abstract themes by charting Ballard’s emotional state of being.My short paper session focuses on the results of my existing work on topic modeling Ballard’s diary while outlining some of the future paths this research could take. In particular, I am interested in pairing trends in topics with trends in Ballard’s social network. What topics correlate with what kinds of people? Are women or men described alongside particular themes? In what broad context do ministers, doctors, neighbors, or family members appear? In conjunction with traditional research and analysis, topic modeling presents a valuable methodology for examining historical sources.", "article_title": "Topic Modeling Historical Sources: Analyzing the Diary of Martha Ballard", "authors": [{"given": "Cameron", "family": "Blevins", "affiliation": [{"original_name": "Stanford University", "normalized_name": "Stanford University", "country": "United States", "identifiers": {"ror": "https://ror.org/00f54p054", "GRID": "grid.168010.e"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Uncovering and detecting patterns of information in library and museum collection items requires the integration of scholarly and scientific data analyses. Research into the materials substrate (paper, parchment, etc) and media (ink, pigment, colorant) that comprise the historic object inform the scholar as to the provenance of a particular document. This may be through characterization and identification of the media and substrate on which the information is contained, or watermark identification that can effectively date the document to a specific time period. The challenge of linking disparate materials characterization and identification databases and scientific analyses is a critical research issue requiring the development of a knowledge representation (KR) to facilitate interpretation that enables this humanities research. With cultural heritage objects, the KR must maintain linkage between the original document that contains a wealth of knowledge stored contextually, and digital surrogates that represent the document. These links will be explored in reference to cultural treasures of South America, where the integration of scientific analyses and historic scholarly information led to the generation of knowledge that enriched the contextual interpretation of the original object. Scientific analyses of textile treasures from Llullaillaco provided an understanding of their use and purpose, while environmental information of Pre-classic Mayan structures in the Mirador Basin allowed an assessment of preservation requirements. Investigations of maps with specific links to South America yielded information on the source of pigments and geographical location of their original creation.Historic documents and cultural heritage objects do not generally lend themselves to ease of context analysis, since documentation about the creation of the document is not readily available. Discovery of data with more detail about the context and circumstances that surround the creation of the artifact allows researchers to visualize information previously not detected. This visualization is achieved in Library of Congress studies through advanced spectral imaging techniques that incorporates data from both visible and non-visible regions of the spectrum to create an integrated “digital cultural object” (DCO). The additional contextual information is not apparent in conventional digitization techniques for these objects, so the integration of the spectral data assists in mining the layers of data stored within the objects. In this way the DCO provides a range of information that allows a shift from the use of interpretive virtual heritage applications that focus on the artistic rather than the investigative and inferential, towards the development of interdisciplinary scientific data analyses as part of cultural heritage humanities scholarly studies. In this way, cultural heritage, science and technology are intertwined, advancing the capacity to mine and analyze historic data from multiple viewpoints. Interaction and interpretation of these additional dimensions allow the description of new relationships between constituent elements, connecting patterns and mining the data for trends, and correlating formerly disparate components. For example, the new identification of pigments in an object that come from a different geographical location, can then suggests trade routes and exchange of materials and artistic techniques. The UNESCO Charter on the Preservation of Digital Heritage has recognized the importance of digital versions of cultural materials, referring to digital heritage as “resources of information and creative expression” being “increasingly produced, distributed, accessed and maintained in digital form.”The composite of images of the maps and textiles that forms the new DCO is related to, but distinct from the originals. This digital object is not a surrogate for the original, but provides new knowledge through the integration of scientific analysis of these cultural heritage objects within libraries, museums and archives. The range of data this new digital object contains enhances interaction between a range of professions, allowing multidisciplinary collaboration for integration of preservation, sociological and cultural information. Digitally generated and accessed data for these maps and textiles balances the opposing goals of libraries and museums and optimizes preservation of the original objects, while increasing access to information from the original. Hyperspectral imaging allows the DCO to create a new interpretation of the original object, as apparent from the 3-dimensional reconstruction of the original woodcut of the Waldseemüller 1507 world. Since these original manufacturing materials no longer exist, the DCO allows the representation of this new scientific knowledge to link with geography and map curatorial knowledge of the era, and assist a greater understanding of the printing techniques and possible location of where these materials originated. This was the first time America was referred to as America, on any map, printed or manuscript, with a unique perspective of South America.Image capture and processing of these and other objects is important for the interpretation of cultural heritage by allowing layers of data to be analyzed and linked. This offers an archeological examination of the information strata, the materials, inter-connections between the materials used, discovered text and information, and the relative associations between the components of the artifact. The creation of an image data-cube deconstructs layers of data into discrete components, while conversely also integrating and utilizing the application of scientific methods to the recognition of areas of interest within the artifact. The explanations generated from these processes expand the associations and extracted knowledge of the original cultural heritage objects.There is dynamic interaction between re-examination of the original or source materials and the DCO, with raw and processed data that can enhance obscured and specific features. Inter-connections and relationships between the source and the generated interpretations based upon analysis of generated data are an iterative process. The process of interpretation relies upon the use of implicit assumptions, inferences or internal filtering. For both scientific and scholarly researchers, these processes are often based upon prior knowledge and experience. Additional filtering for scientific analyses are introduced by reference databases that match known reference materials with “pixel” samples taken from the spectral images to non-invasively characterize materials. These categorizations are objective in nature and reduce the potential error sometimes introduced by subjective assumptions. However, the essential element that should not be overlooked in this iterative process is the power of strong collaborations between a range of disciplines. In 1999 on Llullaillaco's summit, an Argentine-Peruvian expedition co-directed by Johan Reinhard and Argentine archaeologist Constanza Ceruti found the perfectly preserved bodies of three Inca children, sacrificed approximately 500 years earlier. These were accompanied by a range of textiles, figurines and other ceremonial sacrificial materials. A close collaboration between the American Museum of Natural History, preservation scientists, engineers from Argentina and the USA, conservators and curators was critical to ensuring the preservation of these unique materials. The interweaving of the assessment of current condition of the cultural artifacts such as strength and chemical degradation to aid their preservation, with scholarly analysis of the unique patterns of construction enabled the knowledge of both to be linked so as to create a rare collaboration of the forensic type recovery of the history of these materials. This is the highest Inca burial so far discovered and the world's highest archaeological sit. There was intense pressure for exhibition of the materials so close collaboration between all parties to gain as much scientific and cultural information about the origins of the materials and the mummies was a unique component in ensuring their longevity while on exhibit. This sharing of data aided the control of exhibit conditions in South America and long-term management of the materials to allow further studies. The critical component in the generation of knowledge in these studies is recognizing the importance of skilled people and work processes that efficiently add value and meaning. In order to analyze the original source material, high resolution spectral imaging creates the DCO, and data processing and further scientific analyses revolve around the interactions of the people involved – preservation scientist, curator, scholar and technology specialist. This iterative process is reliant not only upon the effective use of technology to assimilate process and disseminate the information with standardized processing, metadata and data management, but also the quality of the collaborative interaction between professionals from different fields. While the relationship between the original and the DCO is provided through metadata that maintains the spatial links between the scientific data, the original material, and the new knowledge generated from these linkages, the integration of diverse opinions and perspectives is an integral component of this new knowledge generation. Standardization of file formats and structures across these different fields provides a method of ensuring continued access and integration to the information, by maintaining and creating effective associations while generating new knowledge. The above examples illustrate the requirement for this standardization of both scientific and scholarly files, to enable true international collaboration and sharing of resources between countries and disciplines. These protocols can support effective data exchange with conventions that provide a local structure for a scientific data network. This sustains diversity in scientific research and scholarly studies. This requirement for a structural framework for cultural heritage institutions allows both user access and functional usability of information to support research.Effective visualization of these data connections is essential for further associations, with access to both spatial and temporal data for the maps, textiles and other objects directly linked back to the original source material through the DCO. Visualization tools and interfaces offer potential for open dialogue between multidisciplinary fields and ease of navigation through layers of data, since knowledge generation is reliant on the cohesive interaction and collaboration between science and humanities researchers. The knowledge representation and underlying interpretation system needs to be appropriate for the application and the types of analyses that are needed for integrating and expanding cultural heritage research. The development of XML-based knowledge representation languages and standards include the Resource Description Framework (RDF). A major benefit of RDF is the ability to utilize the features that facilitate the merging of data, even if the underlying schemas differ. The further advantage for this heritage science application is the capacity to support the evolution of schemas over time, allowing both structured and semi-structured data to be combined and shared across different areas within the application. The large volumes of digital data generated require a repository that can cope with a diverse range of object types, as is true for with collections of data used for scientific analyses. Data files from these objects include images, spectra, reports, and other extant files, requiring additional metadata mechanisms to accommodate a range of data, while retaining access and associations. An RDF model offers greater flexibility and opportunity for integration across various disciplines, since it is designed to accommodate multiple ontologies with a structured approach. The model or ontology being exploited within this structure allows us to coordinate a wide range of data and file types with extensive metadata and multiple data formats. For example, metadata from the scientific instrument is included so the measurement could be reproduced, and in addition the data can be imported in multiple formats – as spectra for visual representation and as a .csv standardized sharable data file. Scriptospatial mapping of the textiles and maps involves an accurate coordinate system that links scientific and scholarly analyses to the DCO, and allows inferences to be drawn to generate new knowledge. This approach to viewing the DCO in relation to multiple dimensions applies an essentially archeological methodology toward uncovering and interconnecting information strata of cultural heritage artifacts. Utilizing an object-oriented approach in conjunction with the data layer allows the mapping of spatial and temporal data with increasing complexity. Examining and explaining the physical, spectral and chemical properties of the maps and textiles permit the humanities scholar to link these scientific analyses to the social aspects of how they were created. These links therefore create meaningful scientific outcomes of the content: When obscured or faded text can be retrieved; inks and pigments characterized and traced to specific geographical locations; analysis of the intensity of handwriting imparts understanding of the author’s original intent; and the provenance and source of paper is gleaned through the capture and analysis of the watermark in the paper. A continued focus on collaboration between people, data and processes is a major factor in promoting access and integration of scientific and humanities research, emphasizing the importance of linking the original artifact with digital tools and techniques for visualizing and disseminating new knowledge in the arts and humanities. Concentrating on generating new knowledge from content derived from these maps, textiles and other objects enhances the importance of the DCO. This allows improved access, interpretation and preservation of fragile items of significant cultural heritage. However the extracted information is only as important as the strength of the collaborative partnerships set up to create a constant iterative loop for access to and interpretation of new scholarly and scientific information. This requires a strong and committed association between previously disparate fields to incorporate and share the generation of new knowledge by mining additional data and forging new advances in humanities research. These related but previously disparate disciplines comprise humanities scholars, scientists, researchers, technology and data management specialists to form an open yet interconnected digital exchange of humanities research.", "article_title": "Knowledge and Reasoning: Connecting Scientific Data and Cultural Heritage", "authors": [{"given": "Fenella G.", "family": "France", "affiliation": [{"original_name": "Library of Congress, United States of America", "normalized_name": "Library of Congress", "country": "United States", "identifiers": {"ror": "https://ror.org/04p405e02", "GRID": "grid.241525.5"}}]}, {"given": "Michael B.", "family": "Toth", "affiliation": [{"original_name": "R.B. Toth Associates", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Poland’s first literary Nobel Prize winner, Henryk Sienkiewicz (1846-1916), owed his great if short-lived fame to the very numerous if very mediocre translations by Jeremiah Curtin (1835-1906), diplomat, ethnographer, polyglot. Born in a Catholic Irish family in Wisconsin, he graduated from Harvard and was posted as a secretary to the American mission in St. Petersburg, Russia; his fluency in Russian made him a popular figure among the local aristocracy. Paradoxically, it might have contributed to his conflict with Ambassador Clay and precipitated the end of his diplomatic career (1869). Curtin switched to two professions he continued till the end of his life: that of the ethnographer (employed, for a time, by the Smithsonian Institution) and of the literary translator. In 1872, he met and promptly married Alma Cardell (1847-1938), who soon abandoned her post as a teacher in a Soldiers’ Orphans’ Home (for which her studies at the Barre Academy had made her more than qualified) to become her husband’s secretary, amanuensis and editor; until her peripatetic husband’s death, her life was to be led in hotels and boarding houses around the globe – especially after the Curtins stroke gold with Sienkiewicz’s international bestseller Quo vadis (1896). Alma devoted much work to virtually all publications signed by Jeremiah: his translations of Sienkiewicz and of other Polish authors (Orzeszkowa, Prus and Potocki); his translations from the Russian (Gogol, Zagoskin’s and Alexy K. Tolstoy); and his ethnographic studies on myths of Native Americans, Ireland and Slavic peoples. She also published and edited three books on Mongols after her husband’s death.Yet the story of Sienkiewicz’s translator is most extensively told in Memoirs of Jeremiah Curtin (1940), published after the death of Alma Cardell Curtin. Although written in first-person narrative, they have been since proven to be the work of the wife. Michael Jacek Miko? has shown that Curtin’s alleged memoirs are in fact a compilation of “somewhat edited\" fragments of Alma’s diaries and letters to her family (Miko? 1990). The same diaries (and not the Memoirs) show the extent of Alma’s contribution to Jeremiah’s translations: after a whole day of taking her husband’s dictation, he would go to sleep while she would copy and correct the day’s work (Miko? 1994). And although she knew no Polish and, at most, but a little Russian, it is not implausible to suspect that some traces of the translator’s wife’s hand might have been left on Sienkiewicz’s fiction in English. In the most radical hypothesis by Cheryl L. Collins, Alma, “held hostage by Jeremiah’s almost pathological restlessness,\" could have been “his full partner\" in his literary work (Collins 2008).This presents a nice authorship attribution problem. All that could have been done in this respect with traditional methods has been done by Miko?; the rest is the attributor’s nightmare, as all work published under Curtin’s name has been preserved in manuscripts in Alma’s hand alone. And while traces of the style of the Memoirs could perhaps be found in Alma’s editions of Jeremiah’s ethnographic works, it is highly uncertain if similar traces can at all be found in his translations.All hope there is lies in non-traditional methods of authorship attribution, developed at least since the seminal Inference and Disputed Authorship: the Federalist (Mosteller, Wallace 1964) and proven to be helpful in plagiarism detection (although Alma’s is plagiarism a rebours). This study applies Cluster Analysis to normalized word frequencies in texts; as shown by (to name but a few) Burrows (1987, 2002), Hoover (2004, 2004a) or Daren-Oskam (2007), this is one of the most precise methods of “stylistic dactyloscopy.\" A script by Maciej Eder, written for the R statistical environment, converts the electronic texts to produce complete most-frequent-word (MFW) frequency lists, calculates their z-scores in each text according to the Burrows Delta procedure (Burrows 2002); selects words for analysis from various frequency ranges; performs additional procedures for better accuracy (Hoover’s culling and pronoun deletion); compares the results for individual texts; produces Cluster Analysis tree diagrams that show the distances between the texts; and, finally, combines the tree diagrams made for various parameters (number of words, degree of culling) in a bootstrap consensus tree (Dunn et al. 2005, quoted in Baayen 2008: 143-147).The analysis included all original works by Curtin (12 extensive studies) and a great majority of his translations (21 novels or long novellas).Figure 1. shows the patterns of similarity and difference of word frequencies in all texts studied. The bootstrap consensus tree neatly divides Curtin’s oeuvre into two discrete groups: the upper branches are his translations while his Memoirs and his ethnography lie below. Yet the Memoirs are placed away from his ethnographic studies; also, what has been proven by Miko? to be Alma’s work, lies close to two of Jeremiah’s books on the Mongols, published by Alma after his death.  Figure 1. Consensus tree for all texts Full Size Image   The length of the word list used in this study (all the way to the 5000th most frequent word in the corpus) can raise doubts as to the validity, in this context, of the term stylistic similarity: after all, words so far down the frequency-ordered word list are often quite meaningful and might reflect differences of content as well as of style. Any chance of finding traces of the translator’s wife’s hand in the translations as well as in the Memoirs has to rely on somewhat shorter lists (tentatively, from 10 to 150) from the top of the frequency-ordered most-frequent-word list; dominated as it is by functions words and aided by a 100% culling rate (which limits the analysis to words that appear in all texts studied) and personal pronoun deletion, it might help purge any impact of the texts’ content to try to bring, say, a book on Irish myths and a translation of Sienkiewicz’s historical romance, to a common stylistic, or perhaps simply lexical, denominator.The new result is presented in Figure 2. The texts attributed to Mrs Curtin either by Miko? or by Figure 1. are still immediate neighbours; more interestingly, the Memoirs (and the two “Mongol\" works) now grow on the same branch with two translations: Lillian Morris and For Bread.  Figure 2. Consensus tree for short most-frequent-word lists and a 100% culling Full Size Image   So far, attempts at finding stylistic traces of the translator or the editor have been only partially successful. Word-frequency-based stylometric methods have shown that they are better at attributing the author of the original than the translator (Rybicki 2009, 2010; Rybicki, Eder 2010), or that Henry James’s switch from writing to dictation left no stylistic trace (Hoover 2010).This time, however, it is possible that the translator’s wife has left her mark. First, Miko?’s attribution of the Memoirs of Jeremiah Curtin is visible in the consensus tree diagrams; second, the diagrams seem to validate the suspicions of Mrs Curtin’s significant contribution to The Mongols: A History and The Mongols in Russia.Even more importantly, the most frequent words in two of the Sienkiewicz translations seem to exhibit a greater affinity to the Memoirs. It is hardly a coincidence that the two short stories depart from Sienkiewicz’s usual male- and Poland-dominated writing. Indeed, Lillian Morris and For Bread are two of Sienkiewicz’s so-called “American stories\" (written during or immediately after his stay in America, 1876-1879) and both feature a strong female protagonist. If it ever made sense for Curtin to give his wife a somewhat freer hand at shaping the final version of the English version of this or that Sienkiewicz text, it would have been on the occasion of these two stories. After all, during the work on Lillian Morris and For Bread in the Irish summer of 1893, Jeremiah was busy, struggling with several other projects at the same time: he was collecting material for one of his Irish mythologies, translating a couple of other short works Sienkiewicz, and writing a lengthy “Translator’s Preface’ to the Polish writer’s another historical romance, Pan Michael.", "article_title": "Alma Cardell Curtin and Jeremiah Curtin: the Translator’s Wife's Stylistic Fingerprint", "authors": [{"given": "Jan", "family": "Rybicki", "affiliation": [{"original_name": "Pedagogical University of Kraków, Poland", "normalized_name": "Pedagogical University of Kraków", "country": "Poland", "identifiers": {"ror": "https://ror.org/030mz2444", "GRID": "grid.412464.1"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "The Cairo Genizah is a collection of about 250,000 manuscript-fragments of mainly Jewish texts discovered in the late 19th century. Most of the fragments were written between the 10th and the 14th centuries. Today, the fragments are spread out in libraries and private collections worldwide. The Friedberg Genizah Project (www.genizah.org) is in the midst of a multi-year process of digitally photographing all of the extant fragments. As of March 2011, the virtual library of the project holds over 250,000 digital images, and 200,000 additional images are expected to be integrated over the next few years. Unfortunately, this huge collection is far from being entirely cataloged, despite the ongoing effort to document and catalog all extant fragments. Moreover, the existing catalogs differ greatly in the amount and type of the data they present. Many of them record briefly the content of the fragment, without any information regarding its physical attributes.We present a system for collecting all catalog data that can be extracted automatically from the fragment’s image, mainly: the exact dimensions of the fragment; number of columns; number of lines; size of the margins; the fragment’s physical status (torn vertically, horizontally, missing corners); and several additional features. Our system differentiates between bifolios and single pages, and in the first case collects the above data for each page separately. Besides the above attributes, which are expected to be found in every modern catalog, the system extracts some finer data that may be relevant to paleographic studies, such as density of lines (line height, inter-line space) and density of characters (number of characters in a fixed unit of length).In addition to the detailed physical description of a single fragment, the huge database generated by the system serves for supporting identification of “join” candidates in the Cairo Genizah. A join is a set of manuscript-fragments that originate from the same original codex, but are scattered today under different shelfmarks, possibly in several different libraries. In a previous work, we described a system for the automatic identification of joins by ascertaining the degree of handwriting similarity between pairs of fragments. By querying the database and applying some basic rules for a good match, taking into account the completeness or incompleteness of the fragments, we can significantly improve on the quality of the results obtained by just analyzing the handwriting similarity.Another aspect introduced in this paper is the proper conditions for taking digital images of manuscripts that are necessary for achieving this kind of results. We argue that, today, the function of such digital imaging is not only conservation and accessibility, but these images should be considered as potential inputs to image-processing algorithms and processes, and the computer should be therefore taken into account as one of the “clients” of the images. Hence, appropriate conditions should be considered in advance when digitizing manuscripts. Among these conditions we mention the following: Choosing the optimal background for foreground-background separation. The background color should contrast, not only with the color of the fragment material, (vellum or paper), which is some hue of light brown, but also with the color of the ink, usually dark brown or black. Otherwise, text will be erroneously recognized as part of the background, and characters will be interpreted as holes in the fragment. The common practice in some libraries to digitize manuscripts on white, brown or black background should be considered therefore as an imperfect one, because these colors do not contrast well with the manuscript and the ink colors. Our study shows that the best contrast for these colors is provided by a blue background. Indeed, when we started digitizing the huge Genizah collection at the Cambridge University Library, we used blue as the standard background color for all images and the same practice was followed in the digitization of the British Library Genizah collection. Note that since with such contrasting colors the computer can very effectively differentiate between the fragment and its background, it is possible to automatically change the color background to any color desired by the user.  Avoiding the use of clips, weight bags, notes, etc. Every significant element in the image should be easily identified and recognized by the computer, and the best segmentation is achieved by color separation. On the other hand, when there is a need for use of extra elements with no significance to appear, such as elements to hold the fragment or keep it flat, we recommend that they be of the same color as that of the background. Notes (such as shelfmark numbers) should be of a fixed size and shape, with some apparent icon on them, so as to enable the software to identify them easily. Use of a ruler in the image. Placing a ruler in the image enables the software to automatically determine the exact dpi of the image, and thus assess the various measures in some recognized unit, such as cms or inches. This practice is crucial especially when different images are taken with different lenses or when the camera is not fixed in the same position throughout the entire process. The ruler should be distinctive from the fragment; hence a wooden brown ruler or a see-through plastic one will not make a good choice. Unfortunately, when such aspects are neglected, the application of computerized methods as described above and harvesting their results become unnecessarily difficult, and the quality of obtained results is adversely affected.", "article_title": "Automatic Extraction of Catalog Data from Genizah Fragments’ Images", "authors": [{"given": "Roni", "family": "Shweka", "affiliation": [{"original_name": "The Friedberg Genizah Project, Israel; Tel Aviv University, Israel", "normalized_name": "Tel Aviv University", "country": "Israel", "identifiers": {"ror": "https://ror.org/04mhzgx49", "GRID": "grid.12136.37"}}]}, {"given": "Yaacov", "family": "Choueka", "affiliation": [{"original_name": "The Friedberg Genizah Project, Israel; Tel Aviv University, Israel", "normalized_name": "Tel Aviv University", "country": "Israel", "identifiers": {"ror": "https://ror.org/04mhzgx49", "GRID": "grid.12136.37"}}]}, {"given": "Lior", "family": "Wolf", "affiliation": [{"original_name": "The Friedberg Genizah Project, Israel; Tel Aviv University, Israel", "normalized_name": "Tel Aviv University", "country": "Israel", "identifiers": {"ror": "https://ror.org/04mhzgx49", "GRID": "grid.12136.37"}}]}, {"given": "Nachum", "family": "Dershowitz", "affiliation": [{"original_name": "The Friedberg Genizah Project, Israel; Tel Aviv University, Israel", "normalized_name": "Tel Aviv University", "country": "Israel", "identifiers": {"ror": "https://ror.org/04mhzgx49", "GRID": "grid.12136.37"}}]}, {"given": "Masha", "family": "Zeldin", "affiliation": [{"original_name": "The Friedberg Genizah Project, Israel; Tel Aviv University, Israel", "normalized_name": "Tel Aviv University", "country": "Israel", "identifiers": {"ror": "https://ror.org/04mhzgx49", "GRID": "grid.12136.37"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "TextGrid’s Text-Image-Link-Editor (TBLE “stands for for Text-Bild-Link-Editor, German for Text-Image-Link-Editor”) is used to link segments of text with sections on the corresponding image. A typical application is linking of scans of facsimiles with their transcriptions, though these texts can also be created during the linking process, which allows e.g. also for image annotations. The information on the linking between manuscript fragments and the corresponding transcription is itself stored in TEI. TextGrid is as a virtual research environment (VRE) for the humanities disciplines dealing with texts in a wide sense (philologies, epigraphy, linguistics, musicology, art history etc.). The joint research project TextGrid is part of the D-Grid initiative, and is funded by the German Federal Ministry of Education and Research (BMBF) for the period starting June 1, 2009 to May 31, 2012 (reference number: 01UG0901A).TextGrid consists of two principal building blocks, the grid-based backend TextGridRep that hosts both infrastructure services and the repository layer for access to research data and longterm archiving, and the user-facing TextGridLab. The TextGrid Laboratory (TextGridLab), a single point of entry to the virtual research environment, will provide integrated access to both new and existing tools and services via a user friendly software [TG]. TBLE is a key component of the TextGridLab that has been under continuous development since 2008 and is by now in practical use.The integration of manuscript scans with their transcription and indeed the critical edition itself is a desideratum of modern editions: “While some people continue to think of electronic texts as exclusive of images, the fact is that digital images of manuscripts are electronic texts, as well. The most compelling scholarly editions of the future will make full use of markup schemes such as XML [...], but not without extensive integration of images” [Kiernan2006]. In this context TextGrid is not the only project that recognized the need for an tool to facilitate this linking of image sections with transcriptions. The Edition Production & Presentation Technology's (EPPT's) [EPPT] tool box for integrating images and text operates in much the same solution space. [Parker2009] proposes the development of a web-based Text-Image Linking Environment (TILE), and for much the same reasons as the TBLE, namely to facilitate “the linking of images and textual information [which] remains a slow and frustrating process for editors and curators”. [TILE] is currently under development.For the current status of the project cf. also its homepage  (link) Consulted 2011-03-14 Unlike the Eclipse-based TBLE, TILE is Ajax-based, extending the Ajax XML Encoder.Similar in objective to TBLE and developed in much the same timeframe is Tapor’s / the University of Victoria’s Image Markup Tool [Holmes2010]. Both independently decided to use formats based on TEI P5 to store linking information, though at this stage unfortunately not the same. Unlike TBLE, the Image Markup Tool is a desktop program only for the MS Windows platform and cannot be integrated into the TextGridLab.[Cayless2008] reports on experiments to partially automate the linking between manuscripts and their transcriptions. TBLE plans to integrate similar functionality using OCR technology (cf. below).As required in [Huitfeld2010], TBLE allows, in Peirce’s terminology, multiple “types” to be associated with one “token”, or in other words to associate one section in a manuscript with multiple, possibly contradictory interpretations / transcriptions. Image sections can overlap, so that divergent segmentations are possible.As an aside, this type of linking is very different from the research field of automated image analysis and image annotation which attempts to automatically establish key metadata for an image, e.g. by identifying the objects or persons shown on a photo.The following use case is a typical example for working with the TBLE: A scholar wants to publish manuscripts as collection of images, which offer a digital representation of the original work, but also wants to publish his take on its correct transcription in view of establishing a critical edition. The solution is to write the content of these hand written documents as text in a human/machine readable format e.g. XML and this text can be divided into logical related segments for example: verses, lines and then these text segments can be easily linked with the corresponding sections on the images using the TBLE.The TBLE can be used for:Text and image are opened, then the corresponding components (text segment and selected image section) are marked by pairs and the linkage is confirmed. The results can be saved as a new file (local or in the TextGridRepository), which contains the linking information (image coordinates, text segment identifiers, URIs of the used text and image files). Sections can be rectangles or arbitrary polygons.The content of the new created file represents the saved information as a TEI document with an embedded [SVG] section (see section 4 “The TEI-Model” for more details). Once a file is saved, double clicking it reloads used images, texts and links to continue editing. Changing or adding new links as well as modifying the linked text is possible at any time.Instead of starting out with an existing transcription and linking it with the image data, the scholar can also decide to start from scratch with an empty text file. The new text segments can be inserted stepwise or at once.Any number of different and possibly conflicting transcriptions and segmentations can be linked against one set of digitized manuscripts.The Text-Image-Link-Editor offers many other useful features, that help annotating specific links or image sections. For example: It's possible to build logical groups of links (e.g. verses, comments, etc.) using the layers-tool. A text-direction (e.g. left-to-right & top-to-bottom) can be assigned to the links. The output file of the Text-Image-Link-Editor follows the TEI model with embedded svg description elements. The following is a list, which crudely describes the structure of the TEI document: <teiHeader> this element could be used to save the metadata of the document. <facsimile> in this element is the svg element embedded, which keeps the topographic descriptions of images and links. <body> in the body element are the link groups, that contain the link elements. These link elements represent the relationship between the image sections and the corresponding text segments. The relationship between images and texts and links is represented in the following figure:  The TEI output file of the Text-Image-Link-Editor to describe the relationship between images and texts and links [Maynooth2010] Full Size Image  The Text-image-Link-Editor is a part of the TextGridLab and is implemented as a group of Eclipse plugins following the [MVC] patternThe Model-View-Controller (MVC) pattern separates the modeling of the domain, the presentation, and the actions based on user input into three separate classes.This tool consists of a Toolkit and two views in addition to the XML Editor and the generic Navigator: Image View: shows the images and enables you to select individual image sections to be linked. Thumb View: is used for navigation. It displays a reduced version of the entire image and the active image detail (which is enlarged in the Image View) which can easily be moved and zoomed. Toolkit: provides different functions for working on the Image View. XML Editor: allows you to open or create texts as well as to select individual text elements. TBLE is already actively used in a number of projects, but continues to be enhanced, taking into account new user requirements coming up in the field. In particular, we plan to implement a new feature in the Text-Image-Link-Editor to enable an automated segmentation of facsimiles using the [OCRopus] OCR-system, which offers a possibility to partially automate the linkage process.OCRopus (tm) is a state-of-the-art document analysis and OCR system, featuring pluggable layout analysis, pluggable character recognition, statistical natural language modeling, and multi-lingual capabilities. http://code.google.com/p/ocropus/ . Consulted 2011-03-14", "article_title": "The Text-Image-Link-Editor: A tool for Linking Facsimiles & Transcriptions and Image Annotations", "authors": [{"given": "Yahya Ahmed Ali", "family": "Al-Hajj", "affiliation": [{"original_name": "Worms University of Applied Sciences, Germany", "normalized_name": "University of Applied Sciences Worms", "country": "Germany", "identifiers": {"ror": "https://ror.org/031ph8d53", "GRID": "grid.440515.1"}}]}, {"given": "Marc Wilhelm", "family": "Küster", "affiliation": [{"original_name": "Worms University of Applied Sciences, Germany", "normalized_name": "University of Applied Sciences Worms", "country": "Germany", "identifiers": {"ror": "https://ror.org/031ph8d53", "GRID": "grid.440515.1"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This paper will present a critical analysis of our attempts to build Virtual Research Environments (VREs) for everyday Humanities research tasks using digital archives. Numerous specialised VREs have been developed for addressing particular tasks in various humanities disciplines. The Silchester VREs addressed data integration in archaeological excavations, the SDM VRE developed services for sharing and annotating manuscripts, while TEXTvre is concerned with TEI-based resource creation. Building on these experiences, gMan addressed the issue of moving beyond support for specific, focused tasks, and instead building services to enable more general-purpose humanities research activities, such as integrating and organising the heterogeneous and often unstructured digital resources, and support for ‘active reading’ processesBrockman et al. 2001. Scholarly work in the humanities and the evolving information environment. Washington, DC. through advanced discovery facilities. Such services regularly top the list of humanities user requirementsBenardou et al. 2009. Understanding the Information Requirements of Arts and Humanities Scholarship. International Journal of Digital Curation.. This paper describes work to this end, firstly by the DARIAH project, and subsequently consolidated by the gMan project, funded by JISC’s VRE Rapid Innovation programme.These experiments were based on use cases identified by the earlier LaQuAT (Linking and Querying Ancient Texts) projectJackson et al. 2009. Building bridges between islands of data—an investigation into distributed data management in the humanities. Proceedings of the Fifth IEEE International Conference on e-Science. Washington, DC., which investigated how to integrate scattered, heterogeneous and autonomous data resources relating to ancient texts, mainly databases but also XML corpora. LaQuAT attempted to solve these issues by offering an integration framework based on the OGSA-DAI grid middleware, which provided an integrated interface to the various data resources that followed a relational database model. However, this approach had certain limitations for our purposes, as such models are optimised for dealing with datacentric resources - that is, resources consisting primarily of structured data such as numbers, dates or very short text fields - rather than text-centric resources containing significant quantities of unstructured text. The approach worked well where the structural context of the information was clear and the query aimed at exact matches. More commonly, however, humanities researchers work with text-centric resources, perhaps enhanced with XML mark-up to capture document structure and additional metadataNentwich, M. 2003. Cyberscience. research in the age of the internet. Vienna., and they look for resources for further investigation based on looser criteria of relevance, e.g. by searching for all Roman legal texts in one resource containing information on punishments that are also mentioned in papyri from another resource.These conclusions were further elaborated in the use cases that were developed from them, which are the main drivers for the work described here. Complementing this is a body of methodological investigation concerning scholars and their use of sources, particularly their use of data and archives. Before describing our current work, we will survey briefly these investigations.The difference in scholarly practices between the sciences and the mainstream humanities is highlighted in a studyPalmer et al. 2009. Scholarly information practices in the online environment. that investigated the types of information sources used in different humanities disciplines, based on results from the US Research Libraries Group reports. Structured data is relatively little used, except in some areas of historical research, and data as it is traditionally understood in the sciences, e.g. the results of measurements, even less so. It is true that the study is partly outdated, and that data in the traditional sense is increasingly important in the humanities, particularly in linguistics and archaeology where scientific techniques have been widely adopted. Nevertheless, it is clear that in general humanities research relies not on measurements as a source of authority, but rather on the provenance of sources and peer-assessment, and that what data repositories are for the sciences, archives are for the humanitiesDuff et al. 2004 Historians’ use of archival sources: Promises and pitfalls of the digital age. The Public Historian.. Archival records are primary sources about the past and may take many forms, including government papers, financial documents, photographs, sound recordings, etc. All this information is unstructured in nature.Thus, our work is driven partly by the requirements fromDuff et al. 2004 Historians’ use of archival sources: Promises and pitfalls of the digital age. The Public Historian., interpreted so as to relate to methods of research in archives. Retrieval is to happen in real time, and traditional finding aids are to be complemented by more sophisticated retrieval mechanisms, including the ability to create relevance indexes on unstructured resources, as well as the ability to combine resources in new ways. In particular, we aimed to implement the personal copy of a finding aid that is often quoted as an important prerequisite for specialised research in archives.Our work investigated how (digital) archival content can be delivered to humanities researchers more effectively, independently of the location and implementation of that content, and with special facilities provided for customising the retrieval, management and manipulation of the content. We investigated how the UK and European research infrastructure (RI) can be exploited to support data-driven, collaborative research in the humanities by using the gCube environmentCandela et al. 2009. On-demand Virtual Research Environments and the Changing Roles of Librarians. Library Hi Tech., which was developed by the EU-funded D4Science project. gCube allows virtual research communities to deploy VREs on demand by making use of the shared resources of the European RI, and provides services that match closely the sort of information organisation and retrieval activities that we identified as being typical in humanities research.D4Science provides an easy way of scavenging online data resources. It has a consistent mechanism to import data for rich user interaction within the deployed VREs. Its data resource staging framework, based on a well-defined workflow of data analysis, data modelling and data generation, is one of the key innovations of D4Science. The analysis and modelling phases define how data collections are loaded into gCube compound objects using its simple but powerful data model. In the data generation phase, descriptive metadata and provenance information are added.Using the gCube data staging framework, the following datasets were brought together in our experiments: The Heidelberger Gesamtverzeichnis (HGV) der griechischen Papyrusurkunden Aegyptens, a collection of metadata records for 65,000 Greek papyri from Egypt. Projet Volterra, a database of Roman legal texts, currently in the low tens of thousands but very much in progress, stored in a series of themed tables in Microsoft Access. The Inscriptions of Aphrodisias, a corpus of about 2,000 ancient Greek These datasets were the same as those used in the LaQuAT project and thus and allow a critical comparison of results. They overlap in terms of time, places and people – specifically looking at the first five centuries or so of the Roman Empire – although their contents are otherwise quite different. The provision of an environment for working with this data in an integrated form would be highly fruitful for the researcher.The presentation will describe the use cases that we used for evaluating gCube. Our approach was to break down the scenarios identified in interviews at KCL and within DARIAH into a number of common, atomic actions. Specific instances of these actions can be combined to model a variety of \"real\" research scenarios, for example the ability to assemble heterogeneous resources (or parts of resources) into a virtual collection, to share this virtual collection within a specific community and to search across a virtual collection, where specific search parameters (such as the importance of specific locations) can be set according to preference. Specific communities also require specific search services such as geo-referenced and date-range searches. Finally, the researcher wants to share links between research objects and annotations (including related documents publications) in her community.In our experiments, we confirmed that most of these use cases could be supported by the features already provided by the core D4Science systems. For the Digital Humanities 2011 presentation, we will address our subsequent activities: the analysis that we carried out to identify gaps in the existing service provision; some results that demonstrate a clear distinction between the viewpoints of humanities and science research, in respect of such features as image search; our move to develop gMan as a production service for humanities researchers; and the recently-funded European Holocaust Research Infrastructure project, which aims to integrate Holocaust research material from archives across Europe. The main aim of the project will be to make accessible existing Holocaust research collections but the second priority will be to deploy virtual research environments to make use of these resources. D4Science services were seen to support initial requirements well.", "article_title": "gMan: Creating General-Purpose Virtual Environments for (Digital) Archival Research", "authors": [{"given": "Tobias", "family": "Blanke", "affiliation": [{"original_name": "Centre for e-Research, King’s College London", "normalized_name": "King's College London", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/0220mzb33", "GRID": "grid.13097.3c"}}]}, {"given": "Richard", "family": "Connor", "affiliation": [{"original_name": "University of Strathclyde", "normalized_name": "University of Strathclyde", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/00n3w3b69", "GRID": "grid.11984.35"}}]}, {"given": "Mark", "family": "Hedges", "affiliation": [{"original_name": "Centre for e-Research, King’s College London", "normalized_name": "King's College London", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/0220mzb33", "GRID": "grid.13097.3c"}}]}, {"given": "Conny", "family": "Kristel", "affiliation": [{"original_name": "Netherlands Institute for War Documentation (NIOD)", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Mike", "family": "Priddy", "affiliation": [{"original_name": "Centre for e-Research, King’s College London", "normalized_name": "King's College London", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/0220mzb33", "GRID": "grid.13097.3c"}}]}, {"given": "Fabio", "family": "Simenoni", "affiliation": [{"original_name": "University of Strathclyde", "normalized_name": "University of Strathclyde", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/00n3w3b69", "GRID": "grid.11984.35"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Despite the ground-breaking work of graphics visionaries like Alan SutherlandAlan Sutherland built the first graphics program named “Sketchpad” as part of his PhD. work in 1963. Sketchpad allowed users to draw geometric shapes and create copies of them. This work was not only important in the realm of computer aided design (it is viewed as the grandfather of modern CAD software), but also in its organization of the code in objects, which is the basis for modern object oriented design. and Ed CatmullBeyond his current role as CTO of Pixar, Catmull is perhaps one of the most important people in the world of computer graphics, discovering methods to apply images to geometries, smoothing lines drawn on computer screens, among many others. He was also a student of Sutherland... in the 1960s and 1970s, which unlocked computer screens as interactive tools, hardware portability issues have constrained computer interaction to a two-dimensional space which often simulates the real world. However, the considerable market growth of sophisticated mobile devices over the last several years has begun to push the boundaries of interaction in virtual environments. Instead of experiencing a simulated environment sitting at a computer, users are shifting to experiencing physical environments with a computing device capable of enriching their subjectie experience of the space. The success of augmented reality systems like BionicEye, RobotVision, TATAugmented ID, and Layar provides glimpses at how this technology might be leveraged by cultural heritage institutions, individual academics, and even local, municpal officials, to provide opportunities for students and the general public to interact with space and place in new and exciting ways.The University of Virginia’s Department of Architectural History holds its training-oriented field school, under the direction of Professor Louis Nelson, in the city of Falmouth Jamaica each year. Falmouth is a fruitful city for study because of its unique history, which makes it the best preserved example of Georgian architecture in the Caribbean. Founded in 1769, the city was originally designed as the main northern port for the island’s thriving sugar trade. Tied closely to the infamous “Triangle-trade”, the slave-based sugar economy allowed the city to grow until the emancipation of slaves in the British Empire in 1840. After 1840, the town saw a significant decline and experienced very little construction over the next 170 years, preserving its architecture and urban design as an early nineteenth-century time capsule.In 2009, the Jamaican government approved development of a cruise ship terminal in Falmouth. Its first ships are scheduled for docking in the summer of 2011. The predicted influx of tourism to the town will be an economic boon to residents, but will also bring significant changes to the architectural identity of the town. The 11-acre port is of such size that, while ships are docked, they will tower over the Trewlany Parish Church of St. Peter, the tallest building in the town. New dining and recreation facilities will be constructed around the terminal, and a simplified interpretation of early nineteenth-century life will be constructed for the enjoyment of tourists. This will result a better standard of living for most residents of the town, but will mean the loss of an historical laboratory for architecture. As businesses sprout up, residents will be pushed away from areas around the terminal and century-old houses will be torn down and replaced with store-fronts.Realizing that the introduction of the cruise terminal to the city will forever alter the architectural identity of the town, the UVa Architectural History Field School recently shifted its focus from deep analysis of a handful of buildings in a single summer, to a more general effort to survey the status of the buildings of the entire city. This survey includes measured sketches, colorcoding of a given structure’s overall condition, images of the structure as it stands today, and information on when the building was constructed, construction material, and other items of interest to architectural historians. These findings were then submitted to UVa Library’s Special Collections for long-term archiving.Sensing a real opportunity to provide access to this important work, the Scholars’ Lab partnered with Louis Nelson, chair of the Architectural History department, to investigate ways to make this nearly decade’s worth of research available to a wider audience. We identified three groups of users for the content: academics interested in the underlying data of the city, government officials who need to plan city restoration efforts, and tourists interested in finding out more about the town they will visit. The Scholars’ Lab was particularly interested in interface design decisions that would serve each of these communities well.In order to provide different mechanisms of access for three distinct user groups, we employed several open-source tools to create a solution that would expose this valuble architectural and historical data to a large and varied audience. Leveraging our own expertise in open-source Geographic Information Systems infrastructureIn 2009 and 2010, the Scholars’ Lab hosted an NEH-funded “Institute for Enabling Geospatial Scholarship:”  (link) . At DH 2009, several members of the Scholars’ Lab presented a panel discussion (“New World Orderings”) on GIS, including a description of our open source , web services-based Geospatial Data Portal:  (link) . And at DH 2010, Scholars’ Lab director Bethany Nowviskie presented a poster on possibilities for spatial humanities, “Inventing the Map.” with the flexibility of the Omeka collections and exhibits framework, the Scholars’ Lab has constructed several different ways in which these different audiences may interact with library-curated spatial data.In order to allow city planners access to the underlying data, maps originally drawn in AutoCad were converted to a GIS format and loaded on to a web-accessible server. Web services were created to allow high-end users of GIS software access to the information with proprietary tools like ArcGIS. We then utilized those same web services to create map interactions within Omeka using the open source mapping library OpenLayers. Images and metadata (along with the full architectural survey report) were uploaded to Omeka and a custom VRA Core metadata standard was created to allow more appropriate description of the architectural elements in the collection. Long-form academic essays are also in the process of being written for approximately 30 of the most important structures in the city. These will be presented along with maps highlighting where important structures are located, to allow visual methods for browsing the collections to function in tandem with scholarly interventions to highlight specific structures of interest. Importantly, this approach also allows us to expose underlying geographic information as web services, allowing other scholars to reuse the information.While the site boasts other advanced features, including a faceted Solr-based search, we also wanted to explore methods for exposing the same data in new ways to the different target groups for the project. With the growth of web-enabled smart devices, we plan to try two experiments at the Vernacular Architecture Forum’s Annual Meeting, in June 2011 in Falmouth, Jamaica. The first will place QR codes at selected structures around the town to allow individuals with web-enabled phones to access all of the information about the structure presented on the Scholars’ Lab website. We are also building a Layar-based augmented reality browser for the city which will allow individuals with smart phones (iPhones and Android-based devices) to install a simple application that will overlay information about buildings on a viewport, accessed simply by pointing their phones at the building.This is an ongoing project, with field experiments and user feedback scheduled to be conducted before the Digital Humanities 2011 conference. We hope to model a technical approach to providing access to library-curated information for multiple audiences, using different technological approaches and techniques to frame the data not only in terms of scholarly use, but for tourism and cultural heritage appreciation, and for the more practical employment of the data by city planners to optimize restoration efforts. We also hope that these tools will help raise awareness about the fragility of the town to tourists, and can act as a way to expose UVa students’ and scholars’ research in multiple formats to those interested in underlying historical and spatial data, and the multimedia and embodied arguments which have been crafted using new tools and methods.", "article_title": "Historic Interpretation, Preservation, and Augmented Reality in Falmouth Jamaica", "authors": [{"given": "Wayne", "family": "Graham", "affiliation": [{"original_name": "University of Virginia", "normalized_name": "University of Virginia", "country": "United States", "identifiers": {"ror": "https://ror.org/0153tk833", "GRID": "grid.27755.32"}}]}, {"given": "Bethany", "family": "Nowviskie", "affiliation": [{"original_name": "University of Virginia", "normalized_name": "University of Virginia", "country": "United States", "identifiers": {"ror": "https://ror.org/0153tk833", "GRID": "grid.27755.32"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "For presenters and attendees at DH2011, there is no need to sing the praises of the digital world. We are the early adopters, the converted, the evangelists. But our colleagues across the humanities are not yet entirely convinced, and of more concern to me, neither are the students. I direct the museum studies track of the masters in public history at the University of South Carolina, one of the oldest public history programs in the country. It is a nationally competitive program, and our graduates have an impressive placement record: the Smithsonian; the National Park Service; federal, state, and local government. And yet, since I joined the faculty three years ago, I have been shocked that the students – the so-called digital natives – have little interest in the digital world as part of their professional training. They may communicate with each other using Facebook, share photos on Flickr, or post to their personal blogs, but when it comes to coursework they expect, and sometimes demand, a traditional graduate seminar where we read and discuss books. More than one student has balked at my assignments, whining, “I don’t need to learn how to program. I just want to be a regular historian.” Unyielding in my persistence, I argue back that it is no longer an option. Wikis, blogs, and tweeting are everyday realities for museum professionals. At the very minimum, all curators and collections managers need to have a basic understanding of database architecture in order to structure their object databases and construct useful queries. More importantly, two decades of digitization has created new questions for curators of three-dimensional objects: What does material culture look like on the web? How do you curate it? How does the public interact with virtual objects? What is the relationship between virtual and physical museum artifacts?Each fall I teach HIST 787: Material Culture Studies, the foundational graduate seminar for the museums track in our masters program. On the first day of class, I ask the students to bring in five objects that describe either themselves or their research interests. I tell them to choose wisely, as they will be using those objects every week for the entire semester, but otherwise I give no guidance to object selection. The objects serve multiple purposes throughout the semester, but most importantly they are part of a larger project to create an object database that represents the changing attitudes towards material culture in the digital age. Each year the students must create an online exhibit drawing from the objects in the database, the objects of both their classmates as well as the students of previous years. Clearly each year the number of objects in the database increases, and the distance from the early contributors becomes greater. I am in Year 3 of what I anticipate to be a decade long study, and this short paper presentation is designed to give preliminary results. Because this course is part of a two-year masters degree, this is the first year where the students do not have direct access to the owners of objects from previous classes.I have chosen Omeka as the platform for this assignment. Although I am well aware of the limitations, as well as the potential, of the open source software, Omeka has a low barrier for entry. Omeka was developed by George Mason University’s Center for History and New Media with non-IT specialists in mind. CHNM’s goal was (and continues to be) to provide museum and library professionals with a tool that allows them to concentrate on content and interpretation without worrying about programming. I am concerned that by using a black box application my students don’t fully understand the implications of engaging with the virtual world. However, that is one of the compromises I have made in order to encourage budding historians to get their toes wet in the digital arena.For their final assignment, students must create records for each of their objects, which includes uploading images, entering Dublin Core metadata, tagging objects with key words, and writing short descriptions. The students then must curate their own exhibit, either by using one of the theme templates provided by CHNM or by creating their own. The open source software allows students who are more skilled or interested in web design to create more elaborate exhibits.So far the results of the online exhibits have been mostly disastrous. As a whole, the exhibits are terrible (available at  (link) ). They have clunky navigation, lack any elegance in design, and often are just plain boring. In many ways, the exhibits are proof of my distrust of black box software for developing online exhibits and are an indicator that anyone who wants to engage seriously in the virtual world needs significantly more training (either formally or informally) than a few hours of online tutorials can provide. More generously, these online exhibits are often the first experience students have in curating, and so one of the assignment’s goals is for students to gain skills in developing effective narrative techniques (useful in both physical and digital curation). In assessing their work, it is important to be mindful of the learning process; remember that they are professionals in training, and they should not be judged on their first attempt but rather on the progress they achieve by the time they graduate.However, as a pedagogical device, the assignment has been tremendously successful. By working through the process of creating an online exhibit, the students naturally confront the many epistemic questions relating to the use of physical objects in a virtual environment. Students immediately recognize the diverse challenges of working in the digital format, from the pedestrian, such as how to search for items when a previous user failed to enter appropriate metadata, to the substantial, such as questioning the ethics of using an object as a metonym in an exhibit that is antithetical to the physical object’s authenticity. My goal for the assignment is not for students to become master web designers, but for them to engage in the questions confronting digital curation.Although I could talk at length about the implications of this ongoing assignment, this short paper will focus on the joint challenges of curating digital resources and the role of digital humanities in the academic curricula – how are universities training the next generation of museum professionals who will have to confront digital curation. My presentation will be a snap shot of a longitudinal study that is currently in progress. I will briefly describe the assignment, its goals, and the unexpected lessons learned thus far. I hope to reach fellow members of the academy to discuss effective teaching techniques while at the same time seeking feedback from museum professionals as to what skills they believe graduating students should have. How do professors balance the need to provide theoretical training in how to read and interpret material culture while fostering the development of technical skills in an ever-changing digital landscape?", "article_title": "Omeka in the Classroom: The Challenges of Teaching Material Culture in a Digital World", "authors": [{"given": "Allison", "family": "Marsh", "affiliation": [{"original_name": "University of South Carolina", "normalized_name": "University of South Carolina", "country": "United States", "identifiers": {"ror": "https://ror.org/02b6qw903", "GRID": "grid.254567.7"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "In the house of non-traditional authorship attribution are many mansions, or methods, based on statistical analysis of authorial style. They all compare text samples of disputed or unknown authorship to texts written by known authors, or “candidates”. The degree of similarity or dissimilarity between samples allows informed guesses on the possible authorship of a given text. The so-called machine-learning methods are supposed to be among the most effective; they include Support Vector Machines, Nearest Shrunken Centroid classification, Burrows’ Delta and so on (for a comparison of their effectiveness cf. Jockers and Witten 2010).The general feature of the methods in question is a two-step supervised analysis. In the first step, the traceable differences between samples constitute a set of rules, or a classifier, for discriminating authorial “uniqueness” in style. The second step is of predictive nature – using the trained classifier, one can assign other text samples to the authorial classes established by the classifier; any disputed or anonymous sample will be assigned to one of the classes as well.The procedure described above relies on a pre-processed corpus of samples. Namely, the clue is to divide all the available text samples into two groups: primary (training) set and secondary (test) set. The first set, being a collection of texts written by known authors (“candidates”), serves as a sub-corpus for finding the best classifier, while the second set is a pool of texts of known authors, anonymous samples, disputed ones and so on. The better the classifier, the more samples from the test set are attributed correctly and the more reliable the attribution of the disputed samples.Such procedures have been successful in social and medical studies; no wonder, then, that it soon made its way into authorship attribution. Yet, contrary to the former applications where the researcher usually enjoys a high number of test samples (e.g. patients), authorship attribution has to struggle with a limited number of samples available to train a convincing classifier. This makes the classifier sensitive to statistical error. What is more, the generally-accepted division of data studied into a training set and a test set further limits the texts that can be attributed.This sensitivity of machine-learning classifiers to the choice of samples in the training set has already been observed (Jockers and Witten 2010: 220). Intuition suggests composing the training set from the most typical texts (whatever “typical” means) by the authors studied (thus, for Goethe, Werther rather than Farbenlehre). In practice, this can be quite complicated: in a small corpus, to change a single training set sample for another can upset the delicate mesh of interrelationships between all other texts. This potentially heavy impact on the effectiveness of attribution tests has not been lost on Hoover: “As a reminder of how much depends upon the initial choice of primary and secondary texts, consider what happens if the same 59 texts are analyzed again, but with different choices for primary and secondary texts [...]. If the analyses that are the most successful with the initial set are repeated, Delta successfully attributes only 16 of the 25 texts by members of the primary set” (Hoover 2004a: 461).Last but not least, any manual selection of texts to both sets must be highly arbitrary. To further quote Hoover: “The primary novels for this test are intentionally chosen so as to produce poor results, but one might often be faced with an analysis in which there is no known basis upon which to choose the primary and secondary texts, and nothing prevents an unfortunate set of texts like this from occurring by chance” (Hoover 2004a: 461-62).Machine-learning methods routinely try to estimate the potential error due to incorrect choice of the training set samples. This cross-validation consist in a few random changes to the composition of both sets, followed by a comparison of the classifier’s success, ten-folded cross-validation being the standard solution (Tibshirani et al. 2003: 107; Baayen 2008: 162; Jockers and Witten 2009: 219). The question arises whether ten trials are sufficient for a classifier which, based on but a few samples, can be unstable.Assuming that the training set contains 10 samples by 10 authors, and the test set another 10 samples by these authors, there are 210 = 1024 possible combinations of members of the training set. For a corpus of 60 novels by 20 authors, this number becomes so large that testing all possible permutations of both sets is unrealistic. Instead, the impact of the composition of the training set on attribution success can be assessed basing on several hundred random permutations; this can be done with a variety of bootstrap procedures (Good 2006).To test this problem, we have selected several corpora of similar size and similar number of authors studied (with the obvious caveat that any comparison between different languages can never be fully objective). For each of these corpora, we have performed 500 controlled attribution experiments, each with a random selection of the training and the test sets. We have compared the number of correct authors guessed, with the hypothesis that the more resistant a corpus is to changes in the choice of the two sets, the more stable the results.All tests featured the simplest, the most intuitive and the most frequently used of machine-learning attribution methods: Burrows’s Delta (Burrows 2002; Hoover 2004b). Delta was run for 100 MFWs, then for 200 and then, at increments of 100, all the way to 2000 MFWs. This was performed at five different culling settings (0-100% incrementing by 20), giving a total of 1000 results, and a mean of these was recorded. The above procedure was then repeated for 500 random permutations of the texts in the training set. The density function was estimated for the final results thus obtained.It can be assumed that the distribution of these 500 final results should be Gaussian rather than anything else. The peak of the curve would indicate the real effectiveness of the method, while its tails – the impact of random factors. A thin and tall peak would thus imply stable results, i.e. those resistant to changes in the primary set.The analysis of the results begins with the corpus of 63 English novels by 17 authors. As expected, the density of the 500 bootstrap results follows a (skewed) bell curve (Figure 1). At the same time, its gentler left slope suggests that, depending on the choice of the training set, the percentage of correct attributions can vary, with bad luck, to below 90%.  Figure 1. Density (vertical axis) of attributive success percentage rates (horizontal axis) in the English novel corpus Full Size Image   It is quite natural that the stability of the results might also depend on the number of authors and/or texts analyzed. The same Figure shows that, with fewer authors, a higher number of texts has no significant impact on the stability of the results at any permutation of both sets (the dashed line), as already observed by Hoover and Hess (2009: 474). With more authors (i.e. when guessing becomes more difficult), the curve widens and a perfect match is even less frequent.And this is still good accuracy and a fairly predictable model. However, it has to be remembered that Delta has been shown to be somewhat less perfect in other languages (Rybicki and Eder 2011).  Figure 2. Density of attributive success rates in the French novel corpus Full Size Image    Figure 3. Density of attributive success rates in the German novel corpus Full Size Image     Figure 4. Density of attributive success rates in the Italian novel corpus Full Size Image    Figure 5. Density of attributive success rates in the Polish novel corpus Full Size Image   Indeed, the discrepancies in Figures 2-5 seem to question the validity of attribution tests based on arbitrary choice of training sets. Although peaks for some combinations of numbers of texts and authors may be at acceptable levels, the left slopes of the curves tend towards dangerously low values; and the wide tails of the curves show that a high success rate outliers might be a stroke of luck rather than a consequence of the method, the data and the statistical assumptions – the most ominous memento appearing here from the inexplicable dispersion in the corpus of 39 Polish novels by 8 authors (Figure 5, grey solid line). Therefore, the ideal authorship attribution situation is not only that of many texts by many authors; it is equally important to assess the validity of the training set with a very high number of trials. This seems to be the only way to escape the quandary of arbitrarily naming each author’s “typical” text.", "article_title": "Do Birds of a Feather Really Flock Together, or How to Choose Test Samples for Authorship Attribution", "authors": [{"given": "Maciej", "family": "Eder", "affiliation": [{"original_name": "Pedagogical University of Kraków, Poland", "normalized_name": "Pedagogical University of Kraków", "country": "Poland", "identifiers": {"ror": "https://ror.org/030mz2444", "GRID": "grid.412464.1"}}]}, {"given": "Jan", "family": "Rybicki", "affiliation": [{"original_name": "Pedagogical University of Kraków, Poland", "normalized_name": "Pedagogical University of Kraków", "country": "Poland", "identifiers": {"ror": "https://ror.org/030mz2444", "GRID": "grid.412464.1"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "No one reads or writes a book alone. Proof may be found in the paratextual letters and other prefatory material that often accompanies a book into the world to meet its readers. This is especially true of early modern books, whose prefatory letters stand as a threshold where the book’s material and symbolic production come together—sometimes as a well-executed plan (ex.: the 1518 editions of More’s Utopia), and sometimes as a collision of intentions (ex.: the 1590 edition of Spenser’s Faerie Queene, or the 1623 Shakespeare First Folio). However, print-based methods of representing paratextual networks—especially in their temporal dimension, across multiple editions and translations—tend to emphasize the published book as product, at the expense of the book as a process. This paper takes the textual tradition of Thomas More’s Utopia, with its unfolding process of paratextual change between editions, as a test case for the design of an open-source interface component to help digital editors visualize networks of paratexts in early modern books.The study of paratexts has been reinvigorated in recent years, crossing national and period boundaries in the tradition of Gerard Genette’s Paratexts, but more recently drawing energy from intersections between book history and digital humanities as interdisciplinary fields. Building on ongoing research on the digital modelling and visualization of paratexts and similar materials (Fekete & Dufournaud, 2000; Monella, 2008; Johnson), this paper argues that creating a digital visualization component for mapping has two benefits: first that a well-designed digital visualization can represent the structured fluidity and temporality of publication as a process that unfolds in time; and second, that the process of creating such a visualization affords a reciprocal opportunity to interrogate the digital tools and systems we use to represent the past.This paper develops its argument in four sections:  Research context: archive and interface in digital textual studies Utopia as a modelling challenge Visualization strategies Conclusion: visualization prototypes as essays The small-scale project outlined in this paper is part of a larger project titled Archive and Interface in Digital Textual Studies: From Cultural History to Critical Design, funded by the Social Sciences and Humanities Research Council of Canada. This project is premised on two linked arguments. The first is that we need to understand how the figure of the archive operates in the cultural imagination, and how perceptions of digital archives are partly coded in advance by historical fears and desires about the continuity of knowledge. The second premise is that we need to develop traditions of digital interface design native to the humanities, and which reflect the humanities’ uniquely valuable understanding of the cultural histories and material complexities of texts (Kirschenbaum, 2004; Drucker, 2011). The Archive and Interface project therefore seeks to bridge between textual studies and the design of digital interface tools in the humanities. It does so first by investigating the cultural history of the humanities archive through case studies such as Utopia, and second by building an online library of interface components designed to be part of that cultural history.The interface library will focus on critical design strategies in four key areas: textual variation (when sources diverge in significant ways); paratexts (documents such as prefatory letters, often published with literary works in complex configurations); materiality (the relation of physical documents to digital versions); and performance (the relation of written texts to reading or enactment in physical spaces). This project’s interface library focuses on putting humanities-designed interface components in the hands of electronic editors, and disseminating the methods by which those components may be created and modified by the larger community of computing humanists (the open-source model). Granting that large-scale editing systems like Anastasia and Edition Production & Presentation Technology have their place in the digital humanities, this paper will argue that small-scale and (relatively) rapidly prototyped interface components, built by individuals or small groups with inexpensive tools, can reflect the critical, experimental nature of humanities design in ways that large projects cannot. Such experimental capacity and structural flexibility is necessary in digital humanities projects if they are to learn from challenging materials like Utopia, and not simply take their representability for granted.The specific nature of Utopia’s challenge to a digital editor is that Utopia’s publication, as a collaborative project between early modern humanists, thematizes the very ideals and anxieties about the dissemination of knowledge that digital humanists have inherited. The book form—and by extension, the emergent network of humanist print culture—is not merely a delivery system for Utopia, but also one of its chief objects of scrutiny. In particular, Utopia simultaneously embodies and critiques the early modern archive with unusual perspicacity. This paper’s analysis follows Warren Wooden and John Wall by approaching Utopia “not as an object of knowledge but as an occasion for an act of perception, an instrument for ‘seeing’ designed to call attention to what is involved in perception” (1985, p. 233). In this light, Utopia itself serves as a kind of visualization of the early modern humanist archive of texts. We know from correspondence that More began writing Utopia with what is now the second book. From a reader’s perspective, the text was written in reverse, with book 1 (written second) placed before book 2 (written first), and various prefatory materials (written last) accumulating before book 1. These prefatory materials—the letters, verses, diagrams, and maps that constitute the paratext of Utopia—increase and vary across the four editions published from 1516 to 1518, and change even more radically in subsequent early modern and modern editions. It is common for modern editions to completely remove or reconfigure Utopia’s carefully constructed paratexts (Allen, 1963). Yet, paradoxically, there may be no single ideal configuration of paratexts, making the interpretation of Utopia as a material text especially reliant on representational methods and tools (Jardine, 1993; Leslie, 1998; Vallée, 2004; Kinney, 2005). Those tools have tended to take printed form, culminating in Terence Cave’s printed guide to Utopia’s paratextual tradition. However, Utopia can be taken as an early experiment itself in humanist print culture, no less than the digital experiments we discuss at digital humanities conferences, which makes Utopia anything but passive material for representation and editing.Given Utopia’s playful, experimental nature, this paper argues for the need for a visualization strategy based not on static representation, as in traditional forms of data visualization that represents the results of analysis, but based rather on the idea of modelling. Unlike a static representation, a digital model embodies the process-friendly dynamism we expect of digital visualizations, but also a certain “rough-and-ready” form and heuristic flexibility (McCarty, 2004). These latter qualities we associate not with commercial software but with the community-designed code libraries found at SourceForge and similar places, which serve as the dissemination model for the Archive and Interface project’s visualization components. (On humanities approaches to visualization, see Drucker, 2010, and other articles in the same special issue of Poetess Archive Journal on “Visualizing the Archive.”)The design methodology for the interface library will be consistent with Ajax web applications, a type of architecture that distributes processing between a server and a user’s web browser, and which integrates well with XML databases and object-oriented design. HTML 5’s new capabilities on the client side permit animation and time-based interactivity to be incorporated into interfaces in ways that used to be exclusive to Flash. This paper will include a brief demonstration of a browser-based interface component that uses animation to model paratextual change over time. The prototype presented here will rely on the encoding structures proposed by Monella (2008), but will approach the topic from the browser and interface side instead of drawing conclusions about the encoding.The two strategies proposed above, modelling and browser-based design, will serve to illustrate the paper’s broader conclusion that a digital humanities project organized around many small interface prototypes may yield more publishable components, respond more quickly to critical discourse about the material, and involve less risk than a single, large interface project. The nature of paratexts calls for an interpretive approach to digital representation, especially with material as complex as More’s Utopia. This paper concludes that the humanistic critical tradition, embodied by Utopia as a collaborative project in publishing technologies of its own time, calls digital humanists to think of their visualizations and interface prototypes not just as finished tools, which emphasize utility, but also as essays that put forward arguments and serve as pretexts for debate—like Utopia itself.", "article_title": "Approaching the Coasts of Utopia: Visualization Strategies for Mapping Early Modern Paratexts", "authors": [{"given": "Alan", "family": "Galey", "affiliation": [{"original_name": "University of Toronto, Canada", "normalized_name": "University of Toronto", "country": "Canada", "identifiers": {"ror": "https://ror.org/03dbr7087", "GRID": "grid.17063.33"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This paper analyzes the historical behavior of several semantic fields of “abstract values” in a corpus of 2,779 19th century British novels. The corpus is a composite archive of canonical and non-canonical texts drawn from Project Gutenberg, Internet Archive, and Chadwyck-Healey’s 19th-century Fiction Collection. In his classic study,  Culture and Society, Raymond Williams claimed that a group of keywords that arose and/or changed dramatically in the nineteenth century offered “a special kind of map [of the] wider changes in life and thought” of the age (Williams 1958). We develop Williams’s insight by applying quantitative methods to a much larger corpus than available at the time of his study. Using a tool we built specifically for our research, we were able to aggregate words whose historical frequencies follow similar trends, thus identifying particularly dynamic semantic cohorts; from these, we found dramatic declines and transformations in fields of social restraint, moral valuation, sentiment, and partiality over the nineteenth century – and an equally dramatic increase in the use of concrete description fields in the same period. We examine the implications of these findings with respect to broader ideological and narrative patterns of the British novel.In prior applications of semantic fields to quantitative literary studies, researchers have tended to measure the relative presence of certain fields, “themes” or otherwise-labeled word-groups in individual texts (e.g. Louwese 2004; Ide 1989; Fortier 1989). We hope to complement such comparative work by tracing the diachronic behavior of particular semantic fields across a corpus of nineteenth-century novels. In addition, we aim to specify our theoretical object of the semantic field more precisely, both by developing our fields through an empirical method of word-cohort correlation, and by grounding them in their original conceptualization by early twentieth-century semantics. In “Bedeuntungssysteme,” R. N. Meyer influentially defined a semantic field as “the ordering of a definite number of expressions from a particular point of view”—or in other words, from a particular “differentiating factor” (Meyer 1910). To borrow Meyer’s example: the sense of purposefulness, present in the transitive verb  ersteigen(to climb) but not in the intransitive verb  steigen(to rise), could serve as the “differentiating factor” around which a particular  Bedeuntungssystemderived its identity.In its period focus and objectives, this project is indebted to Raymond Williams’s  Culture and Society, which analyzes the historical semantics of a period of unprecedented change for Britain. He contends that changes in discourse help reveal broader sociocultural changes. These wider changes, he argues, are of no small consequence; indeed, they introduced many of the social elements and ideas central to what we now think of as distinctive to our modern way of life (Williams 1958). Of course, Williams’s ambitious attempt to analyze an entire social discourse, astonishing as it is, lacked the tools and corpora now available to digital humanities scholars. This paper represents some first steps in pursuing Williams’s objectives by applying quantitative methods to a large novelistic corpus in order to explore specific but dramatic changes in language and culture in this volatile period.Our method of field-creation consists of two stages: discovery and development. First, to  discoverpotential fields, we developed a technique of word-cohort correlation. We input “seed” words considered significant by previous literary historical work and query the corpus for words whose historical frequency-trends most resemble those of the “seed” words. When this automatically generated cohort of correlated words shares a specifiable differentiating factor and their overall trend is significant, we consider such a word-cohort the embryo of a dynamic semantic field ripe for development. We then  developthe field further by employing semantic taxonomies from within the humanities and linguistics such as the OED's historical thesaurus to identify the semantic content of these word cohorts and subdivide them into specific semantic fields to track. This method, which oscillates between semantic taxonomies and empirical word frequency correlations, ensures that the semantic fields we generate satisfy two characteristics: semantic coherence and coherence of historical behavior. We consider this dual requirement as a pragmatic move. The first requirement ensures that our results are semantically and culturally interpretable. The second requirement ensures that the aggregate term frequency results of the semantic fields are actually representative of the behaviors of their constituent words.In particular, this paper reports on a study of the historical behavior of four semantic fields of “abstract values.” We identified these transforming semantic fields through the methods described above. This study presents and analyzes the dramatic decline and transformation of four semantic fields [ Fig. 1] discovered under this method (each is named after what we considered the differentiating factor of that field).  Fig. 2 Full Size Image  Tracing the diachronic behavior of these fields over the nineteenth century, we found the four abstract values fields exhibited strikingly parallel downward trends [For their individual plots, see  Fig. 5-8]. Collectively, the aggregate term frequency for the fields of abstract values decreases step-wise through the nineteenth century [ Fig. 2], from ~1% of all words in the period of 1800-1810, to ~0.6% of words (~1 in every 170 words) by the 1860s, a decrease of about 40%.Given the range and scope of our corpus and the magnitude of this trend, we consider our results reflective of broad changes in the 19th century British novel. The data indicates a significant decrease in the usage of these fields. Without positing a simple reflective relationship between literary and sociocultural currents, we nevertheless take seriously Raymond Williams’s approach to social changes through changes in discourse. Thus, we consider the data to suggest the responsiveness of novelistic language to fundamental shifts in British value systems and social norms in this turbulent and transformative period, specifically shifts away from values of restraint, virtue, objectivity, and sentiment.  Fig. 3 Full Size Image  The historical behavior data of an entirely different set of words, discovered under the same method, helps to contextualize and interpret this trend. Instead of a semantic field tightly organized around a specific differentiating factor, this highly correlated word-cohort (named “Hard Seed” after its seed word) comprises a variety of semantic fields and types of words—colors, body parts, numbers, locational and directional adjectives and prepositions, action verbs, and physical adjectives. This word cohort can be collectively characterized as concrete description words. In contrast to the values fields, the aggregate term frequency of this latter group [ Fig. 3] increases steadily across the 19th century from 3.5% of all words (~1 in every 30 words) to 6.5% of all words (~1 in every 15 words)—an increase in usage of about 85%.  Fig. 4 Full Size Image  Plotting the term frequencies of the abstract values field against those of the hard seed field in each of the novels in our corpus reveals a strongly inverse relationship between the two [ Fig. 4]. Given the observed tendency for novels with higher frequencies of hard seed words to have lower frequencies of abstract values words and vice versa, we produced two rankings of the novels to see the types of narrative that correspond to the emphasis of one field over the other [see Fig. 9 for a ranking of a subset of the corpus, the Chadwyck-Healey fiction collection]. Strikingly, ranking novels by these two features indeed separates out clusters of genres into a spectrum. The resulting distribution of novels allows us to interpret these two major correlated historical trends in novelistic language as deeper shifts in narrative mode. The spectrum shows an overarching movement from narratives with small social spaces organized by highly polarized, evaluative, and uniform fields of social norms to narratives with far larger social spaces where the fields of social norms are more diverse, conflicting, ambiguous, and ultimately, less constraining. Simultaneously, there is a stylistic move away from abstract, explicitly evaluative language to concrete, physical language whose valuation, if any, is more ambiguous, variable, and indirect. That the expansion of narrative social space corresponds to this stylistic shift suggests a systemic tendency in which the representation of wider, more diverse, and less constraining social spaces is made possible by a more physical, concrete language.This study represents initial steps in developing the quantitative analysis of the historical semantics of literary discourse in a corpus robust enough to allow the study of large-scale historical change. The methods developed herein have proven promising in identifying and analyzing robust semantic fields whose dynamics can rigorously be interpreted as reflections of literary and cultural trends. A wide field of potential inquiry remains for future studies in this vein.", "article_title": "Abstract Values in the 19th Century British Novel: Decline and Transformation of a Semantic Field", "authors": [{"given": "Long", "family": "Le-Khac", "affiliation": [{"original_name": "Stanford University", "normalized_name": "Stanford University", "country": "United States", "identifiers": {"ror": "https://ror.org/00f54p054", "GRID": "grid.168010.e"}}]}, {"given": "Ryan", "family": "Heuser", "affiliation": [{"original_name": "Stanford University", "normalized_name": "Stanford University", "country": "United States", "identifiers": {"ror": "https://ror.org/00f54p054", "GRID": "grid.168010.e"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Over the years, digital libraries and textual archives have collected, described, and classified texts and multimedia objects. These kinds of repositories are effective in compiling, describing, and disseminating the cultural heritage such as the artistic and literary expressions. Also, many of them, following the developments of Computational Linguistics, have incorporated tools for textual analysis as part of their end-user services. Nevertheless, these systems are weak in terms of relationships. Of course, they are configured in such way that it is possible to relate the digital objects compiled; thus, for instance, it is easy to retrieve a set of visual artefacts sharing the same subject matter. However, these relationships are based on the traditional criteria of classification and description (metadata and keywords), without any intention of exploring the nature or specific characteristics of the relationships that the Art History discipline’s phenomena maintain among them.We should not overlook the intrinsic relationship that exists among texts, concepts – or ideas –, words and visual artefacts in the construction of art-historical knowledge (Mitchell, 1994). As Heffernan (2006) argues for the case of words and images, this relationship should not taken as a simple reproduction of art works by a set of words, but rather the conjunction of visual artefacts and words generates new knowledge. This is one of the reasons for which we can contend that these relationships deserve to be treated as a research object in themselves.Therefore, our intention is to explore the potential relationships that could be established among these entities: texts, concepts, words, and visual artefacts with the aim of investigating how these relationships are able to produce new significant knowledge or are able to open new understandings.Religo is a system that enables the construction of interpretations based on relationships. According to the researcher’s needs and the art historian’s research habits and procedures, the question of text and image induces us to design a system to establish relations between various kinds of objects (texts, images, videos, etc.), to provide new possibilities for analysis and research, which are offered only partly by the state of the art.Currently, in the most significant projectsImage Markup Tool  (link) . Pinakes  (link) . VLMA  (link) . EPPT  (link) . Flickr  (link) . TextGrid  (link) . Talia  (link) . working on texts and images – although with interesting and useful features – some limitations can be found in terms of:Taking into account this state of the art, Religo does not reduce the relationship to the concept of pure tagging or to the idea of simple connection between entities, but treats it as basic element for interpretation and analysis, making itself the subject of research in order to create new knowledge.Religo relates the domain entities creating two logical levels: the expression one, consisting of digital objects on which the interpretation can be developed; and the semantic one, consisting of digital concepts (the relationships between digital objects and the predications on themselves) that allow the interpretation to be built (Buzzetti, 2004)This theoretical model has been used by Signum to develop a system for facilitating semantic research and text reading in Text and Semantics  (link) ..This means that, when a digital concept is the subject of interpretation, it is placed on the expression level, becoming a digital object itself.An example is the Michelangelo’s masterpiece The Last Judgement, analysed and interpreted by the Spanish author Francisco Pacheco in his 17th treatise The Art of Painting (1649). In this case, The Last Judgement would be the digital object, and the interpretation given by Pacheco, the digital concept. However, insofar as the Pacheco’s interpretation is also subject of interpretation and analysis by the modern historiography, it in turn becomes a digital object.The entities of the domain can be submitted to a number of general operations (such as selection of parts, links, free tagging or metadata encoding, etc.) and others more specific according to their particular features (specific operations on texts and image).In every operation the centrality of the relationship is clear: from its creation, made easily by a simple drag and drop of selected portions of objects, up to reach the composition of documents as result of the different entities relationship that themselves constitute the new knowledge of the study process.A more evident utility of this new use of relationships can be seen in the search and navigation functions, for example to improve the search capability because it ensures a higher degree of precision and recallPrecision can be seen as a measure of exactness or fidelity, whereas recall is a measure of completeness..As an example, searching for the word emblema, Religo returns both entities containing emblema as textual occurrence (W) and those where emblema does not appear (NW) but which are related to W (Figure 1). Thus, the relationship gives relevance and importance to the entity NW, which otherwise, from a purely textual standpoint, would go unnoticed or simply would not exist. Figure 1C. Ripa, Iconologia, overo Descrittione dell’Imagini universali […], Roma 1593, p. 96. G. Vasari, Ragionamenti di Giorgio Vasari pittore ed architetto aretino […], Firenze 1832-38, p. 1404. Full Size Image  Relationship also affects the display of the entities: that is, in addition to the classical view as list, Religo provides a view as graph, creating a network between the various domain entities which on one hand allows the reconstruction of interpretive reading by simply moving the focus between digital objects, on the other hand enables a contextualized vision of every digital object. These features, typically used during the work process, are also useful in order to share and exploit the research results.As a more concrete example, let us consider the following domain:The Art of Painting by Pacheco discusses Michelangelo and his works, including The Last Judgement. It also treats other painters such as Velázquez or Tiziano, other works such as The Final Judgement painted by Pacheco himself, the portrait of King Phillip II painted byTiziano, and other artists such as Dolce, Paleotti, Lomazzo, or Céspedes.Religo allows relationships to be created between these objects:[Pacheco] author of [The Art of Painting][Michelangelo] painter of [The Last Judgment][The Last Judgement by Michelangelo], [The Last Judgement by Pacheco] described in [The Art of Painting][Phillip II by Tiziano] mentioned in [The Art of Painting]Notice the difference between describe, which implies a detailed explanation of the painting; and mention, which only means that the painting has been cited.[The Last Judgement of Michelangelo] influences on [The Last Judgement of Pacheco][The Last Judgement of Michelangelo] used as example by [Dolce, Pacheco, Lomazzo, Céspedes][Dolce, Paleotti, Lomazzo, Céspedes] cited in [The Art of Painting][The Last Judgement of Michelangelo] illustrates concepts of [deviations of decoro, terribilità, movements and affetti][The Last Judgement of Michelangelo] described with terms [artificioso, espantoso, terrible, horribilidad, feroz]These relationships themselves become new entities of the domain, forming an interconnected network and producing the following new knowledge level:[Lomazzo] [Céspedes] cited by [Pacheco] to define the concept of [painting]At this level, we can see how the result specifies the connection among the different theorists cited by Pacheco and the concepts that he defines in The Art of Painting until creating another richer level:[The Last Judgement] used as example by [Pacheco, Dolce] to illustrate idea of [deviations of decoro][The Last Judgement of Michelangelo] used as example by [Pacheco, Lomazzo] to illustrate the idea of [movements and affetti][The Last Judgement of Michelangelo] used as example by [Pacheco, Céspedes] to illustrate the idea of [terribilità]What we can deduce from this result is that, from the second half of 16th century, The Last Judgement by Michelangelo plays the role of universal reference to illustrate or exemplify a wide range of aspects concerning the visual arts, being used by each author in a different way. The Art of Painting, as an encyclopaedic treatise, brings together many of these interpretations, which Pacheco unifies into a single point of view.The most interesting results arise when we use a complete repository of works, images, and texts. For example, if we consider a repository of Spanish 17th treatises, as ATENEA Project (link) , we might find the following types of relationships:[The Last Judgement of Michelangelo] described by [Pacheco, 1649] and [Carducho, 1634] only mentioned by [Martínez, ca. 1675][The Last Judgement of Michelangelo] described by [Pacheco] and [Carducho] with coinciding terms [confusión, temor, horribilidad, terrible].As an initial task, Religo is provided with all the typical features to operate on texts and images in terms of combination of interacting tools for example to describe and catalogue visual artefacts, to analyse images, to manipulate images, or to annotate images (whole or partly).Together with the standalone version, an online should be allowed in order to ensure content sharing and social tagging in expert contexts of usage.Moreover, the system would be generalized for use in other different domains and would have the capacity to handle other types of entities such as audio and video.", "article_title": "Religo: A Relationship System", "authors": [{"given": "Nuria", "family": "Rodríguez", "affiliation": [{"original_name": "University of Málaga (Spain)", "normalized_name": "University of Malaga", "country": "Spain", "identifiers": {"ror": "https://ror.org/036b2ww28", "GRID": "grid.10215.37"}}]}, {"given": "Alida", "family": "Isolani", "affiliation": [{"original_name": "Scuola Normale Superiore (Italy)", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Dianella", "family": "Lombardini", "affiliation": [{"original_name": "Scuola Normale Superiore (Italy)", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Daniele", "family": "Marotta", "affiliation": [{"original_name": "Scuola Normale Superiore (Italy)", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This paper presents the intermediary results of our ongoing research project at the Université Paris-Diderot as part of a post-doctoral bursary awarded by the Mayor of Paris’s  Research in Paris 2010 program. The project aims at combining traditional research methods (research, annotation and publication of early modern texts and documents) with open source tools and standards (Omeka, Zotero, TEI), with the goal of publishing an online encyclopedia of case studies on medicine and melancholy in the late Renaissance. The principal research question asked is: how did the Renaissance physician position himself in relation to his patient, and how does he attempt to document his ‘clinical’ experiences in writing? The case histories of those suffering from melancholy are instrumental in understanding this issue: tormented by various hallucinations and deliria, the melancholy see what is not there and live in a world of strange delusion, variously believing that they have no head, or are made of brick, or of butter, and so forth. The patient who famously believed his body to have been transformed into butter feared even approaching the oven (an awkward situation since his line of work was in baking bread), while yet another was convinced he was missing one leg, bitten off by an imaginary crocodile. Cases such as these are at the heart of our research: we have examined not only early modern medical documents, but also many important collections of commonplace books in our search for case studies, patient descriptions, medical observations, and so-called ‘curative epistles’.Rather than a traditional publication in print, the results are being progressively published online with the aid of a number of open source tools. The principal aims of this paper are to present the various preparatory research stages, the choices made in implementing the digital methods and tools, and finally to reflect on the evolution of the project in the years to come.This project uses the TEI recommendations for the transcription and the encoding of early modern medical texts. The TEI has been demonstrated to be the most comprehensive way of transcribing rich, complex texts by a number of major projects See for example the ongoing Transcribe Bentham: A Participatory Initiative. Available at:  http://www.transcribe-bentham.da.ulcc.ac.uk/td/Transcribe_Bentham  [Accessed October 4, 2010].. TEI documents are then stored in an online database that uses and adapts the open source CMS Omeka Omeka is a project of the Center for History and New Media, George Mason University. Available at:  http://omeka.org/about/  [Accessed March 10, 2011]., now a standard tool for the creation of online repositories of documents and virtual exhibitions. We shall present the way in which we have used and adapted Omeka’s plugins and themes. We shall also discuss the metadata structuring choices we have made. Since this is handled directly by Omeka, it facilitates the creation of an OAI repository, which can be made directly accessible to data-harvesters See the forthcoming ‘Isidore’ developed by the French Centre National de la Recherche Scientifique (CNRS): ISIDORE - Accès aux données et services numériques de SHS. Available at: http://www.rechercheisidore.fr/ [Accessed March 10, 2011]. and eases integration with research management applications such as Zotero.The textual documents transcribed and added via the Omeka database are to be accompanied by critical annotations, literary transpositions or references and by a collection of images or an index of commonplaces. The website, unlike a scholarly publication, will be more easily accessible and reach a wider audience, while the database, making use of Web 2.0 technologies, will function as a virtual exhibition, an online ‘cabinet of curiosities’, allowing readers to interact with, comment on and contribute to the published materials. With this in mind, the ultimate aim of this digital humanities project is to generate a broader interest in early modern research and history by focusing on melancholy, a subject that has never ceased to influence and inform disciplines from medicine to literature.", "article_title": "Medical Case Studies on Renaissance Melancholy: Online Publication Project", "authors": [{"given": "Radu", "family": "Suciu", "affiliation": [{"original_name": "Université Paris-Diderot, France", "normalized_name": "University of Paris-Sud", "country": "France", "identifiers": {"ror": "https://ror.org/028rypz17", "GRID": "grid.5842.b"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Burton Pike described Rainer Marie Rilke’s style in Rilke’s only prose novel, The Notebooks of Malte Laurids Brigge, as “… arresting, haunting, and beautiful, but it is not smooth. His style is explicit, direct, almost laconic, and it has an edge.” (2008, p. xvii) Pike argues that this “edge” was not sufficiently emphasized in previous English language translations and he thus wrote a new translation. The goal of this research is to explore the degree to which automated text analysis tools can capture the different styles used by Burton Pike and Stephen Mitchell in their respective translations. We are particularly interested in what kinds of similarities and differences can be captured between two renderings of Rilke’s novel and in the implications of these findings on the reviews, reader reception, and critical analysis of the original work in two translations.Two candidate analysis tools were used to identify similarities and differences between the Pike and Mitchell translations of Rilke’s novel The Notebooks of Malte Laurids Brigge. The first approach used a syntactic representation of the texts which was generated using the Stanford Lexical parser ( (link) ). Before running the parser, each translation was tokenized into sentences using the Natural Language Toolkit tokenizer ( (link) ). The generated lexical dependencies, which capture grammatical relations between words in a sentence, were then uploaded to the oracle database for subsequent analysis.The second approach used a statistical approach–principal component analysis–to compare the manifestations of the novel. Our experiment reflects McKenna et al.’s study (1999) of similarities and differences between Samuel Beckett’s French and English translation of Molloy. Three matrixes were produced comprising the 99 most common words in each of Pike and Mitchell’s translation and the original text. The rows of the matrix reflect 8 text blocks of 7,500 words. Principal component analysis, which measures the variance of 99 most frequent words in the 8 blocks, was then applied to each matrix and the top eigenvectors produced were mapped into a two dimensional space for visual analysis.Dependency grammar analysisTable 1 provides the summary statistics of grammatical relations which showed the largest difference captured in the parser for each translation. Although the two translations show a remarkable degree of similarity in the frequency and type of grammatical relations they employ, the dependencies revealed several differences between the two renderings. The main areas of difference were found in the use of negation modifiers, prepositional modifiers, object of preposition, parataxis, and in the word choices for adjectival and adverbial modifiers.The dependency grammar results show that Pike used many more negation modifiers than Mitchell, which is confirmed by word frequency analysis. Pike uses the non-contracted verb, which places more emphasis on negation and thus also stays closer to the original text whereas Mitchell rarely uses a non-contracted verb form. The word “not” is not the only word in Pike’s translation which indicates a negation that is used with higher frequency. Pike uses “no” with higher frequency than Mitchell, and also “nothing,” “never,” and “none.”The frequencies of prepositional modifier and object of preposition relations in two texts indicate a difference in how the prepositions are used in the text. This result shows a higher overall frequency of prepositions in Mitchell’s translation (Table 1). The sentence level analysis of two texts, however, revealed that Pike is more likely to repeat and thus emphasize the same preposition throughout the sentence whereas Mitchell is more likely to leave out the preposition rather than repeat it. This finding suggests that in addition to the overall difference in the frequency of prepositions captured through the parser, there are indicators that the use of prepositions, their placement and distribution throughout the sentence, show differences in two texts.The comparison of two translations revealed a higher frequency of sentences that use a semicolon to separate sub-clauses in Mitchell’s translation (parataxis). Mitchell frequently arranges independent sentences using semicolons and colons, and in this way follows closely the original text, while Pike occasionally intersperses “but” and “and” in place of semicolons and colons. This finding is supported by the frequency of conjunction “but” in two translations which is found at higher frequency in Pike’s translation, 559, than in Mitchell’s, 506. This difference in sentence structure may provide the “edge” which Pike claims was missing in previous translations.An examination of the unique adverbs and adjectives used in each translation revealed that although Mitchell and Pike may use similar grammatical relations in their respective renderings, their word choices are frequently different. This suggests that a semantic rather than the syntactic analysis may reveal additional differences.PCA analysisFigure 1 shows the score plots of the translations and suggests that the distribution of frequent terms is very similar between the two translations. However, the grouping of the 7th block with 4th, 5th, and 6th rather than with 8th block, in Mitchell’s translation, calls for a closer analysis of the 7th block of the novel and its comparison with the 7th block in Rilke’s original text and Pike’s translation. The last part of the novel, which corresponds to the 8th block of text, is visibly different in style and tone from the rest of the novel and this difference is indicated by the location of the 8th block in the far right corner of the plot in each version.Our results thus far suggest that syntactic grammatical relations reveal differences between the two translations that are not captured when using a bag-of-word approach (word frequencies of function words and 1,000 most frequent words). In contrast, the PCA analysis using matrices of frequent terms in the 8 blocks of text suggests that only small differences exist between the translations, with the exception of the 7th block of text, which warrants further investigation.The similarity captured between translations using the PCA analysis brings to mind Jan Rybicki’s article “Burrowing into Translation: Character Idiolects in Henryk Sienkiewicz’s Trilogy and its Two English Translations” in which Rybicki analyzes the distinctiveness of character idiolects in Henryk Sienkiewicz’s trilogy and concludes that: “… the patterns of difference and similarity are almost mysteriously preserved in the translations—so well that the above-mentioned linguistic differences might be the sole reason for the small differences between the original and the translation. In the greater picture, characters differ one from another in the translations just as they do in the original.” (Rybicki, 2006, p.102)The differences between two translations that were established will help create the linkages between these findings and the reader reception, reviews, and critical analysis of two translations. We also hope that they will help trace the contours along which the creation of the new variant and new literary rendering begins to emerge and the differences that speak directly to the explicit, direct, and laconic style of Pike’s translation.We plan to extend this work to include M. D. Herter Norton’s translation (1949) of The Notebooks of Malte Laurids Brigge and to investigate how well this method generalizes to different translations of different genres. Table 1 - Dependency distributions with the largest difference between the Pike and Mitchell translations of The Notebooks of Malte Laurids Brigge by Rainer Maria Rilke Full Size Image    Figure 1 - Score plots Full Size Image   ", "article_title": "Comparing the Similarities and Differences between Two Translations", "authors": [{"given": "Ana", "family": "Lucic", "affiliation": [{"original_name": "Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign", "normalized_name": "University of Illinois at Urbana-Champaign", "country": "United States", "identifiers": {"ror": "https://ror.org/047426m28", "GRID": "grid.35403.31"}}]}, {"given": "Catherine", "family": "Blake", "affiliation": [{"original_name": "Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign", "normalized_name": "University of Illinois at Urbana-Champaign", "country": "United States", "identifiers": {"ror": "https://ror.org/047426m28", "GRID": "grid.35403.31"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "In the past, humanities scholars have primarily used text-based computational approaches to engage questions of authorship. In the area of visual arts, computational analysis of authorship is a growing field, but it is one that features diverse questions and requires complex algorithms, significant computational resources and a wide variety of experts from diverse disciplines to combine the results of visual inspections with computer generated results. Furthermore, when approaching the broad field of authorship-related questions in visual works, the variety of digital images representing cultural artifacts poses a formidable challenge on the robustness and accuracy of computer algorithms. The motivation of our work is to explore technologies that facilitate enquiry about authorship in visual art work and to address the challenges related to algorithm development, computational scalability of algorithms, distributed software development and data sharing, efficient communication tools across diverse disciplines, and robustness and general utility of algorithmic development when applied to a spectrum of authorship questions from historical images. In other words, we wanted to develop specific methods for new image-based research as well as build and model for future work in image processing and humanities research. We approached these challenges by selecting image subsets from the collections of 15th-century manuscripts, 17th and 18th-century maps, and 19th through 21st-century quilts that often have corporate and anonymous authors working in community groups, guilds, artisan shops, and scriptoriums, and report technologies designed to support authorship discoveries in these collections. Crucially, the questions our algorithms and experts address are concerned with using authorship as a trope for analysis and generating new data, not just verifying the heritage or identity of a given artifact.The research being presented as part of this paper submission is derived from the Digging into Data to Answer Authorship Related Questions Grant awarded as part of the Digging into Data Challenge Competition (www.diggingintodata.org). An international, multi-disciplinary team of researchers from the University of Illinois (US), the National Center for Supercomputing Applications (US), Michigan State University (US), and the University of Sheffield (UK), the DID team works to formulate and address the problem of finding salient characteristics of artists from two-dimensional (2D) images of historical artifacts. Given a set of 2D images of historical artifacts with known authors, our project teams aim to discover what salient characteristics make an artist different from others, and then to enable statistical learning about individual and collective authorship. The objective of this effort is to learn what is unique about the style of each artist, and to provide the results at a much higher level of confidence than previously has been feasible by exploring a large search space in the semantic gap of image understanding. Team members are geographically distributed and have very different backgrounds and expertise. While the discoveries require involvements and interactions of experts in computer science and in humanities, we had to design a methodology for communicating, coordinating web design and public relationship interfaces, large size data sharing, collaborative software development, software sharing and testing, and hardware sharing. We approached this spectrum of collaborative project challenges by (a) establishing communication and coordination channels (ooVoo videoconference, mailing lists, legal point of contacts regarding licenses and intellectual properties), (b) designing and deploying a content repository called Medici, (c) designing and documenting a library of content based file comparisons with standard application programming interfaces (API) for software development called Versus, (d) deploying software source control and bug tracking systems accessible to all team members (SVN and JIRA), (e) designing web-based workflow systems that could give access to hardware resources at any site for execution of algorithms called Cyberintegrator, and (f) providing additional tools and user interfaces for humanity scholars to view large size images and contribute to the interpretation of the computer generated results.Emphasizing the aspects of data-sharing, collaborative software development, distributed hardware resources, and interactions of experts from diverse domains in the Digging into Data project, we designed, developed and deployed technologies supporting a wide spectrum of team activities. The data, software and hardware sharing technologies include the Medici Content Management Repository (see Fig. 1),  Figure 1: User interface to the Medici management repository for data sharing, annotations and visualization of large size images. Full Size Image   the Im2Learn library of basic image processing and visualization algorithms that can be applied to various image analyses (see Fig. 2),  Figure 2: An example of a segmentation algorithm in Im2Learn library that was applied to historical map analyses (top) and manuscript illustration analyses (bottom). Full Size Image  Full Size Image   the Versus library for content-based image comparison (see Fig.3),  Figure 3: Initial user interface to Versus in order to support image comparison based analyses. Full Size Image   and the Cyberintegrator workflow for managing computations on distributed computational resources. The Medici Content Repository System is a web and desktop-enabled content management system that allows users to upload, collate, annotate, and run analytics on a variety of files types allowing for portable and open representation of data with extensible analytical tools. The analytical capabilities come from the Im2learn library that provides a plug-and-play interface for adding new algorithms and tools. Due to the fact that the authorship questions are frequently based on a comparison operation, we have designed additional API called Versus which allows everyone to contribute with comparison methods. Once the algorithms for image analyses and comparisons have been developed, they can be integrated into workflows (a sequence of algorithmic operations to reach the analytical goal) in Cyberintegrator workflow environment. Cyberintegrator is a user friendly editor to several middleware software components that:  enable users to easily include tools and data sets into a software/data unifying environment annotate data, tools and workflows with metadata visualize data and metadata share data and tools using local and remote context repository execute step-by-step workflows during scientific explorations gather provenance information about tool executions and data creations.  In order to support visual explorations of large size images and contribute to the interpretation of the computer generated results by humanists and computer scientists, we have also integrated Microsoft Live Lab’s Seadragon library to build image pyramids and support fast zoom in and out operations.Our paper addressed each logistical and computational facet of a distributed, international collaboration. Based on our initial effort, all team members have responded positively to the technologies introduced and also helped in defining requirements for executing such complex projects involving collaborative humanities research. Based on our current observations, the web technologies for data, software and hardware sharing provided the foundation blocks for addressing the authorship discovery challenges. We have also concluded that the open nature of joint software development is necessary for overcoming intellectual property right and other legal hurdles.We would like to acknowledge the NSF ITS 09-10562 EAGER grant and the NSF/NEH/JISC Digging into Data (NSF grant ID: 1039385).", "article_title": "Supporting Scientific Discoveries to Answer Art Authorship Related Questions Across Diverse Disciplines and Geographically Distributed Resources ", "authors": [{"given": "Peter", "family": "Bajcsy", "affiliation": [{"original_name": "National Center for Supercomputing Applications", "normalized_name": "National Center for Supercomputing Applications", "country": "Bulgaria", "identifiers": {"ror": "https://ror.org/03g9ch715", "GRID": "grid.432563.5"}}]}, {"given": "Rob", "family": "Kooper", "affiliation": [{"original_name": "National Center for Supercomputing Applications", "normalized_name": "National Center for Supercomputing Applications", "country": "Bulgaria", "identifiers": {"ror": "https://ror.org/03g9ch715", "GRID": "grid.432563.5"}}]}, {"given": "Luigi", "family": "Marini", "affiliation": [{"original_name": "National Center for Supercomputing Applications", "normalized_name": "National Center for Supercomputing Applications", "country": "Bulgaria", "identifiers": {"ror": "https://ror.org/03g9ch715", "GRID": "grid.432563.5"}}]}, {"given": "Tenzing", "family": "Shaw", "affiliation": [{"original_name": "National Center for Supercomputing Applications", "normalized_name": "National Center for Supercomputing Applications", "country": "Bulgaria", "identifiers": {"ror": "https://ror.org/03g9ch715", "GRID": "grid.432563.5"}}]}, {"given": "Anne D.", "family": "Hedeman", "affiliation": [{"original_name": "University of Illinois", "normalized_name": "University of Illinois at Urbana-Champaign", "country": "United States", "identifiers": {"ror": "https://ror.org/047426m28", "GRID": "grid.35403.31"}}]}, {"given": "Robert", "family": "Markley", "affiliation": [{"original_name": "University of Illinois", "normalized_name": "University of Illinois at Urbana-Champaign", "country": "United States", "identifiers": {"ror": "https://ror.org/047426m28", "GRID": "grid.35403.31"}}]}, {"given": "Michael", "family": "Simeone", "affiliation": [{"original_name": "University of Illinois", "normalized_name": "University of Illinois at Urbana-Champaign", "country": "United States", "identifiers": {"ror": "https://ror.org/047426m28", "GRID": "grid.35403.31"}}]}, {"given": "Natalie", "family": "Hansen", "affiliation": [{"original_name": "University of Illinois", "normalized_name": "University of Illinois at Urbana-Champaign", "country": "United States", "identifiers": {"ror": "https://ror.org/047426m28", "GRID": "grid.35403.31"}}]}, {"given": "Simon", "family": "Appleford", "affiliation": [{"original_name": "University of Illinois", "normalized_name": "University of Illinois at Urbana-Champaign", "country": "United States", "identifiers": {"ror": "https://ror.org/047426m28", "GRID": "grid.35403.31"}}]}, {"given": "Dean", "family": "Rehberger", "affiliation": [{"original_name": "MATRIX: The Center for Humane Arts, Letters, and Social Sciences Online, Michigan State University", "normalized_name": "Michigan State University", "country": "United States", "identifiers": {"ror": "https://ror.org/05hs6h993", "GRID": "grid.17088.36"}}]}, {"given": "Justine", "family": "Richardson", "affiliation": [{"original_name": "MATRIX: The Center for Humane Arts, Letters, and Social Sciences Online, Michigan State University", "normalized_name": "Michigan State University", "country": "United States", "identifiers": {"ror": "https://ror.org/05hs6h993", "GRID": "grid.17088.36"}}]}, {"given": "Matthew", "family": "Geimer", "affiliation": [{"original_name": "MATRIX: The Center for Humane Arts, Letters, and Social Sciences Online, Michigan State University", "normalized_name": "Michigan State University", "country": "United States", "identifiers": {"ror": "https://ror.org/05hs6h993", "GRID": "grid.17088.36"}}]}, {"given": "Steve M.", "family": "Cohen", "affiliation": [{"original_name": "MATRIX: The Center for Humane Arts, Letters, and Social Sciences Online, Michigan State University", "normalized_name": "Michigan State University", "country": "United States", "identifiers": {"ror": "https://ror.org/05hs6h993", "GRID": "grid.17088.36"}}]}, {"given": "Peter", "family": "Ainsworth", "affiliation": [{"original_name": "University of Sheffield", "normalized_name": "University of Sheffield", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/05krs5044", "GRID": "grid.11835.3e"}}]}, {"given": "Michael", "family": "Meredith", "affiliation": [{"original_name": "University of Sheffield", "normalized_name": "University of Sheffield", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/05krs5044", "GRID": "grid.11835.3e"}}]}, {"given": "Jennifer", "family": "Guiliano", "affiliation": [{"original_name": "University of South Carolina", "normalized_name": "University of South Carolina", "country": "United States", "identifiers": {"ror": "https://ror.org/02b6qw903", "GRID": "grid.254567.7"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "To support the development of curricular content for the Data Curation Education Program (DCEP) at the Graduate School of Library and Information Science (GSLIS), University of Illinois at Urbana Champaign, a needs analysis case study focusing on digital humanities centers was carried out in late 2010. Collectively the results paint an interesting picture of the perception of humanities data curation needs by directors and key staff. Several results were contrary to what we anticipated; for instance, there was only modest agreement on critical areas of expertise needed to sustain meaningful access to digital humanities scholarship over time. Most importantly, there was one result that, if confirmed, could have a substantial impact on the design of data curation education programs. In the humanities, center directors and managers appear to resist the notion that a particular staff role, that of data curator, is specifically needed, preferring instead to develop distributed expertise and responsibilities as part of existing staff roles, calling on institutional resources as needed. This suggests, among other things, that the standing recommendation to place data curation professionals “upstream” in projects may need to be re-envisioned in this context.Data curation has been described as “the active and ongoing management of data throughout its entire lifecycle of interest and usefulness to scholarship” (Cragin et al., 2007). Curation activities enable data discovery and retrieval, maintain quality, add value, and provide for re-use over time. Originally conceptualized as an e-Science problem precipitated by large amounts of data in digital formats, data curation is an emerging problem for the humanities as well, as both data and analytical practices become increasingly digital (Renear et al., 2009; Crane, Babeu, & Bamman, 2007).GSLIS received a grant from the Institute of Museum and Library Services (IMLS) to extend the existing data curation specialization within the school's ALA-accredited master's program to include humanities data as well as science data (Renear et al., 2009). Among the activities carried out under the grant was a study of data management and curation practices in the digital humanities. The study was designed and directed by social science researchers from the Center for Informatics Research in Science and Scholarship (CIRSS). The main goals were to assess the levels and types of data curation expertise needed by researchers actively engaged in digital humanities projects and to better understand the potential roles that information professionals trained to meet the unique challenges of working with humanities data in digital formats might fulfill.To develop a rich picture of data curation practices in the humanities, we employed a case study method, taking digital humanities centers as our case. We chose to focus on established digital humanities centers in preference to libraries, repositories, or individual research teams or scholars for a number of reasons. First, many significant early projects were likely to be located at or affiliated with centers—meaning these centers have experience handling data over longer time scales (Daigle, 2005). Second, centers bring together faculty and staff, and we believe this makes them sites where the most sophisticated curation practices are likely to be found (Zorich, 2008).We interviewed directors and upper-level staff members at 14 digital humanities centers located in the United States, the United Kingdom, Europe, and Australia with one interview per site. High profile, established centers were chosen that were available and willing to participate in the research. Most centers chosen were located at large research universities but the size of the centers ranged from several staff members to large units. Our intuition was that researchers working at the level of a single project might see data curation as something inextricably tied to their specific job and thus not be able to envision it as a stand-alone function; therefore, upper-level staff with responsibilities for hiring and coordination between projects were chosen in order to elicit views of the curator position from that overarching perspective. Participants were asked about a range of topics related to data management including formats and standards, data storage, security and redundancy, staff roles and background, and significant unsolved problems.Study participants completed a pre-interview worksheet and their responses were used to guide and focus semi-structured interviews. The pre-interview worksheet also included a series of questions asking participants to rank various categories of skills on a Likert scale ranging from “very important” to “not important at all” for curating humanities data. From the ranked list of skills we were able to develop an overview of researchers' views of data curation in the context of multi-project digital humanities centers. From the interviews, we were able to capture more in-depth information such as complex discussions of tradeoffs and rationales that could not be adequately represented in a simple survey. We are therefore able to report both quantitative and qualitative results from our case study.When asked to rank the importance of various areas of expertise needed for a data management professional to be effective working with humanities data, participants revealed an unexpectedly high degree of variability in their answers.From among a list of 30 kinds of expertise provided in the pre-interview worksheet, at least one study participant gave each category of knowledge the highest score, indicating it was “very important.” While we can rank order the areas of expertise according to an average score, the differences between rankings are not statistically significant by Chi-squared analysis. However, we observe that a handful of skills were both highly ranked on average and showed higher positive skew: every respondent ranked expertise in areas such as interoperability, markup, database design, and metadata as being of moderate importance or higher. Project management also had a high rank order. One surprising result among our findings was the strong emphasis on skills related to teaching and training. This may be due to staff at digital humanities centers being tasked as consultants to scholarly projects or it may simply be due to the expectation within the community that skills will be developed through peer-to-peer training in the course of carrying out job duties. Overall, our results coupled with interview data could not identify a consensus as to the most relevant areas of expertise needed by staff engaged in humanities data curation.Our interviews reveal a picture of current practice in which the work of data management and curation at digital humanities centers is parceled out among multiple staff members at multiple levels in the organizational hierarchy. Important data curation tasks may be left for scholars or managers of projects to decide individually, or they may be handled by staff, who work on multiple projects for a center, or they may be outsourced to other campus units above the level of the center.The staff who did have responsibility for data management and curation at the centers we studied were often either those with programming, systems administration, or other IT training, or were people who had received advanced training in a humanities discipline and had taught themselves technical skills.In keeping with the emphasis on interoperability noted in our quantitative results and also perhaps in response to a changing funding environment and newly available services, we observed a trend in which efforts were being made to move data management expertise from the staff member who had developed it in the course of his or her duties to a center-wide or perhaps institution-wide level where it would be a part of documentation and institutional memory rather than personal memory.However, our interviews with managers also suggest that even though data management and curation is beginning to be elevated to a higher position in organizations, there is skepticism about the potential role for a data curator at digital humanities centers. Participants in our study were interested in adding skills relevant to data curation to their organizations but rather than doing so in the form of a dedicated position for an information professional, they appeared to be looking for staff with computing or disciplinary skills who also had some training in data curation.This finding is consistent with another trend we observed. Just as digital humanities centers are already using outside groups such as campus IT or vendors for certain aspects of data management, we noted an increasing orientation to and interest in working with campus-wide services such as institutional repositories to curate humanities data.Since effective curation, management, and preservation of data in digital formats involves intervention at every stage of the data lifecycle from creation onwards, it has been a common belief in the data curation community that information professionals trained in curation will need to work “upstream” in scientific labs and digital humanities centers (Swan & Brown, 2008). The current resistance of directors of humanities data centers to such dedicated data curation staff must be taken seriously as it undoubtedly reflects relevant experience and judgment, and their sense of the sorts of arrangements that are likely to succeed. However, our case study in combination with prior work on the role of information work in scientific research suggests that models of provisioning data curation expertise may need to be more nuanced.As the humanities become increasingly “data-rich,” information science research on data management in the natural sciences becomes increasingly relevant (Choudhury & Stinson, 2007; Renear, Muñoz, & Trainor, 2010). For example, intensive case studies in neuroscience suggest that information services for researchers are likely to be most effective at project stages when information work is most routine or when it is highly speculative, as is often the case with new interdisciplinary research questions or in emerging collaborations (Palmer, 2006; Palmer, Cragin, & Hogan 2007). In the humanities we have also seen that conceptions of what constitutes information or support work and what constitutes professional work within disciplines change in response to the introduction of digital methodologies (Flanders, 2005; Bradley, 2008; McCarty, 2009). While the distribution of curation activities may not follow the same types of (re)arrangements we are seeing in the sciences, we still believe that some data curation work will be most effective upstream and integrated into scholars' research endeavors, such as at decision points about project planning and re-use value.As digital curation practices evolve, libraries and institutional repositories will likely take on a larger role in curating humanities data in the future. The results of our study can serve as a useful point of comparison for future work in this area. In addition to having someone who owns data curation problems and manages solutions on a research-center-level, institutions may explore both how to provide services from a central organization (such as the university library) and also, ways to increase the formal, in-service training available to researchers in the digital humanities.This work was funded by a grant from the Institute of Museum and Library Services (RE-05-08-0062-08). We have benefited from the expertise of Melissa Cragin, Carole Palmer, and other staff from the Center for Informatics Research in Science and Scholarship in designing and carrying out this research.", "article_title": "Tasks vs. Roles: A Center Perspective on Data Curation Needs in the Humanities", "authors": [{"given": "Trevor", "family": "Muñoz", "affiliation": [{"original_name": "Center for Informatics Research in Science and Scholarship, University of Illinois, Urbana-Champaign, USA", "normalized_name": "University of Illinois at Urbana-Champaign", "country": "United States", "identifiers": {"ror": "https://ror.org/047426m28", "GRID": "grid.35403.31"}}]}, {"given": "Virgil", "family": "Varvel", "affiliation": [{"original_name": "Center for Informatics Research in Science and Scholarship, University of Illinois, Urbana-Champaign, USA", "normalized_name": "University of Illinois at Urbana-Champaign", "country": "United States", "identifiers": {"ror": "https://ror.org/047426m28", "GRID": "grid.35403.31"}}]}, {"given": "Allen H.", "family": "Renear", "affiliation": [{"original_name": "Center for Informatics Research in Science and Scholarship, University of Illinois, Urbana-Champaign, USA", "normalized_name": "University of Illinois at Urbana-Champaign", "country": "United States", "identifiers": {"ror": "https://ror.org/047426m28", "GRID": "grid.35403.31"}}]}, {"given": "Kevin", "family": "Trainor", "affiliation": [{"original_name": "Center for Informatics Research in Science and Scholarship, University of Illinois, Urbana-Champaign, USA", "normalized_name": "University of Illinois at Urbana-Champaign", "country": "United States", "identifiers": {"ror": "https://ror.org/047426m28", "GRID": "grid.35403.31"}}]}, {"given": "Molly", "family": "Dolan", "affiliation": [{"original_name": "West Virginia University, USA", "normalized_name": "West Virginia University", "country": "United States", "identifiers": {"ror": "https://ror.org/011vxgd24", "GRID": "grid.268154.c"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "In the last Digital Humanities conference in London (July 2010), several of those present expressed the need for educational strategies based on digital projects. The reasons can be summarised as follows: firstly, given the difficult sustainability of these types of projects, the help of students in their development could be a key factor in their upholding. Secondly, participating in real digital projects could help these students gain an array of essential competences in the digital society in which they will develop their professional work. Bearing this in mind, the aim of this paper is to present a case of educational innovation, financed by the University of Malaga since 2006, which I believe can respond to the concerns expressed in London. The Desingcrea/Diseñoteca project was originally created to redefine the practical aspect of certain subjects within the degree of Technical Industrial Design Engineering given in the Polytechnic Institute of the University of Malaga. Specifically, our aim was to develop collective learning strategies that could lead to significant and relevant training, involving the student in the development of his/her own knowledge. In order to do this we decided that the new possibilities of virtual interaction brought about by the Information and Communication Technologies, as proposed in the Technology Enhanced Learning (TEL) theory, social participation web environments (2.0), combined with the educational principles of Pedagogic Constructivism (Mitchel Rescnick, 1996) and Conectivism (G. Siemens, 2006), offered the perfect framework on which to base this project. Taking an objective based educational model as a starting point, we decided that all these strategies revolved around a common objective: the development of an industrial design database, which we called Diseñoteca, which in turn would be integrated into a web portal also dedicated to industrial design (Desingcrea). The idea was that Desingcrea/Diseñoteca would be developed progressively through the collective work of students, who would be responsible for the preparation of its material and content, through tasks incorporated into the teaching programme of certain subjects of the degree. Once developed, the Desingcrea/Diseñoteca system would work as an online resource available to all students, who could use it as a tool for consultation, study and debate. Desingcrea/Diseñotca is a web application developed specifically by computer engineers from the University of Malaga for this project1. Updates carried out on the system aimed to transform an informational system, based on the «database» philosophy, into a learning community, based on the «social network» or «2.0» theory, bringing together users interested in industrial design, amongst them our students, who were responsible for its dynamization. Desingcrea/Diseñoteca focuses on the industrial design, therefore the entire information published on this website is around this subject matter. As well as informative sections, created by the students, such as directories, bibliographical indexes, etc., the Desingcrea portal has resources common to web 2.0, such as the publication of news and articles as blogs, the possibility for users to evaluate the contents, RSS, tag clouds and the capacity to link these contents to other existing resources on the Network (text, images, web pages, etc.). It is therefore possible to create a real, open and global network about industrial design, which students use as a personalised learning environment. [Fig. 1 and 2].  Fig. 1. Desingcrea website. 1 <designcrea.uma.es>  Full Size Image    Fig. 2. Blogs written by students at Desingcrea website.  Full Size Image   The Diseñoteca database, which is integrated into the Desingcrea website, is also being developed through the collective work of students. Industrial design objects are registered in a structured manner within Diseñoteca. It is made up of data records in which participants describe and classify the design objects according to a protocol of standards and metadata. [Fig. 3]  Fig. 3. Diseñoteca’ s record. Simple display showing the assessment given by students. Full Size Image  In order to create this data register, the principal source used was The Categories for the Description of works of Art (CDWA), of the Getty Research Institute (1998), which was updated in the work Cataloging Cultural Objects (Baca, 2006), of American Library Association Editions. The result is a more specific example of this standard that we call Categories for the description of industrial design objects (CDOID), and that will be the object of a future publication. The system offers students the possibility to discuss the records of their classmates, contributing data and information that enrich the description of the object. Besides, in this way the collective participation in the creation of contents and the exchange of ideas is increased. Likewise, students can assess these records using tools common to the repositories of the social web. This assessment will act as a co-evaluation, complementing the final grade given by the teacher. [Fig. 3]. Management and monitoring of students. Each teacher is assigned a specific number of students to monitor. A series of features have been installed on the system so that students are monitored in the most complete way possible. In this way it is possible for the teacher to see the records that have been created and edited by any student and consult his/her record history, seeing the different actions and tasks that have been carried out. This way the system not only shows us the amount of time that the student has been connected to the Internet, but also the tasks that he/she has actually carried out and therefore if the student has really been working. [Fig. 4].  Fig. 4. Display for the teacher profile with the report of the student´s work and actions.  Full Size Image   Finally, given that one of the aims of Desingcrea/Diseñoteca, as mentioned, is to work as an information and meeting website for all those interested in industrial design, a validation tool has been incorporated into the system as a means to guarantee the quality of the process. That is, information introduced into Desingcrea/Diseñoteca is not made public to users until it has been corrected and validated by the teacher in charge. According to results obtained since this project began in 2006, I list the following aspects which could be applied to any similar, digital based projects. During the paper, these ideas will be developed in more detail and other significant results will be presented. ", "article_title": "Development of Digital Projects as Learning Strategies. The Desingcrea/Diseñoteca Project", "authors": [{"given": "Nuria", "family": "Rodríguez", "affiliation": [{"original_name": "University of Malaga (Spain)", "normalized_name": "University of Malaga", "country": "Spain", "identifiers": {"ror": "https://ror.org/036b2ww28", "GRID": "grid.10215.37"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "A growing online phenomenon is that of crowdsourcing, where groups of disparate people, connected through technology, contribute to a common product. It refers to the collaborative possibilities of a communications medium as flexible and as populated as the Internet. If many hands make light work, crowdsourcing websites show how light the work can be, breaking tasks into hundreds of pieces for hundreds of hands. Building from the growing body of research in the area including the author’s work on crowd motivations, this paper outlines the necessary steps and considerations in enriching projects through crowdsourcing.Though not new, crowdsourcing as it exists online has been enabled by emerging technologies. It has grown out of increasingly efficient – and affordable – forms of communication. Since such collaboration has expanded so quickly, there have been few investigations into the design of crowdsourcing. At the same time, the most successful projects have emerged in an organic nature that many deliberate attempts have failed to replicate, suggesting the need for more investigation in the area. Jeff Howe, who first defined the term and popularized the trend, has explained that “we know crowdsourcing exists because we've observed it in the wild. However, it's proven difficult to breed in captivity” (2008).The gaps in knowledge of online crowds are quickly being filled however, allowing projects to move away from reliance on serendipity. This presentation derives from recently completed thesis work on the motivations of crowds within crowdsourcing (Organisciak 2010). While it will reflect that study’s findings on how, its primary focus is on the equally important questions of why and when in light of those findings. For which tasks is crowdsourcing an appealing option and what resources should be present for a project to adequately motivate the users? A bottom-up classification of crowdsourcing categories is proposed, followed by a checklist of needs that an institution must consider before attempting their own crowdsourcing.In this study, a sample of 300 crowdsourcing sites was examined and classified. Synthesizing these classifications resulted in a proposed list of eleven non-exclusive categories for crowdsourcing, six describing method and five describing structure. Methods include encoding, creation, idea exchange, skills aggregation, knowledge aggregation, and opinion aggregation. Additionally, there are financial, platform, gaming, group empowerment, and ludic structures observed within these systems. Derived from existing systems, these categories and their variants offer unique design patterns and best practice cases that can assist in assessing the types of tasks at which they excel.Appropriateness of the task is just one facet of running a crowdsourcing project. The other consideration is whether a project offers a return that potential participants would find rewarding. In addressing this, a content analysis was used to identify site design mechanics related to user experience in thirteen cases spanning the breadth of the identified categories. These mechanics were then discussed in a series of user interviews to determine what users truly care about. In this study, a collection of primary and secondary motivators are proposed as foundational considerations in running a project. The primary motivators seen in the user interviews were interest in the topic, ease of entry and of participation, altruism and meaningful contribution, sincerity, and appeal to knowledge. A final one, financial incentive, is perhaps the most blunt. Secondary motivators include indicators of progress and reputation (i.e. “cred”), utility, fun, system feedback, social networking, and fixed windows (i.e. well-groomed quality).An understanding of the nature of crowdsourcing holds notable benefits to scholarship in the humanities and social sciences. Most significantly, this is because it allows large-scale insights into the qualitative and the abstract, those areas inextricably linked to the limits of manpower, unable to be delegated to computing power. “What is the sentiment of this sentence”, is the type of question a crowdsourcing site may ask (Mechanical Turk, May 2nd 2010), if not always as directly.  Since much work in the arts cannot easily be quantified, logistics and resources often limit humanities research to a balance between breadth and depth; crowdsourcing offers an escape from this issue.Consider one task that is often seen in existing crowdsourcing sites: crowd-encoded classification. Classification tasks are dependent on the person-hours available, because person-hours are the only dependable way to approach these tasks. Whether directly or incidentally, online crowds can effectively encode or classify content. Though the reliability of the end product is often far below that of a professional encoder, large-scale crowd projects can often account for this through multiple independent classifications, measuring consistency and reliability through agreement.  Galaxy Zoo, an effort from Oxford to classify galaxies, found crowdsourced data to be within 10% agreement with the same data classified professionally (Lintott et al. 2009). The high quality of work is especially notable because the experiment and its follow-ups received their 60 millionth classification in April 2010.Flickr Commons, an initiative to put photo archives on a photo-sharing community, is a similar project that – by way of community-based research, information and tagging – has enriched the metadata of hundreds of Library of Congress photographs in the United States of America (Springer, et al. 2008). Another pilot project involving public tagging, by the National Library of Australia, concluded that “tagging is a good thing, users want it, and it adds more information to data. It costs little to nothing and is relatively easy to implement; therefore, more libraries and archives should just implement it across their entire collections” (Holley 2010). The National Library of Australia followed through on this recommendation.Such projects are often greeted with suspicion in professional or scholarly communities. The National Library of Australia report notes that \"institutions who have not implemented user tagging generally perceive many potential problems that institutions who have implemented user tagging do not report\" (Clayton et al. 2008 qtd. in Holley 2010). The Library of Congress report similarly notes many concerns that critics provided, such as: “Would fan mail, false memories, fake facts, and uncivil discourse obscure knowledge? … Would the Library lose control of its collections? Would library catalogs and catalogers become obsolete?...Would history be dumbed-down? Would photographs be disrespected or exploited?” (Springer et al. 2008). In both cases, the reports state that the concerns, within the respective project’s experiences, have not manifested.Encoding is a notable use of crowdsourcing in academia, but not the only one. Some projects, such as the Suda On Line, benefit from collected contributions of expertise and knowledge. Suda On Line is a project to translate a Byzantine encyclopedia, Suda, into English for the first time. It has been steadily progressing since 1998, producing a comprehensive resource while staying at a manageable participation scale (Mahoney 2009). In other cases, crowdsourcing allows public and volunteer projects to compete with the scale and quality of commercial projects, as has been seen in OpenStreetMap, Project Gutenberg, and many open source projects.As crowdsourcing continues to be tested – and if it continues to be successful – in public institutions, understanding how to undertake such projects will become more important. The benefits are being stated, and the scale and openness on which public institutions operate makes them a compatible beneficiary of crowdsourcing activities. Users appear especially altruistic toward public projects, emphasizing in this study their preference for meaningful engagement with institutional workings over symbolic outreach.The study informing this work is large, and my hope is to provide a digestible account of its results. The reason for this goal is straightforward: there is still much work to be done in understanding the mechanics of crowdsourcing, but the potential is great. I hope that the sharing of this foundational work will encourage others to explore further.This study owes a great debt to Lisa M Given, my thesis advisor, as well as additional committee members Geoffrey Rockwell and Stan Ruecker.", "article_title": "When to Ask for Help: Evaluating Projects for Crowdsourcing", "authors": [{"given": "Peter", "family": "Organisciak", "affiliation": [{"original_name": "University of Illinois, United States of America", "normalized_name": "University of Illinois at Urbana-Champaign", "country": "United States", "identifiers": {"ror": "https://ror.org/047426m28", "GRID": "grid.35403.31"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Alan Liu’s “Imagining the New Media Encounter” (2008) calls for “a poiesis of digital literary studies” through which we can renegotiate the relationships between new and old media as productive encounters rather than as something other than “conversion” encounters. Liu helps us open up a critical space in which to rethink how the problematic notion of “conversion,” with its implications of oppositional media, complete transformation, and religious fervor, shape our understandings of related pairs: writing and encoding, mimesis and creation, imagination and simulation. We suggest that our understanding of text markup is closely implicated in our reimagination of writing, and that the modes of modeling suggested by possible worlds scholars may destabilize our understanding of mimesis and its role in both literary composition and text markup. In short, we propose considering markup as a “world-constructing” (Doložel 1997) form of discourse.Our starting point of reference is the long-standing conceptual tension within the markup (and especially the TEI) community between two models of markup. The first is rooted in mimesis and surrogacy: the domain of transcriptional and editorial markup. The second is more concerned with meaning creation and the domain of annotation, interpretation, authoring. These two models have different textual commitments and establish different relationships between text, markup, reader, and encoder. In the first, the encoder uses markup to transact a connection between a text and a reader; it is understood that the markup is non-transparent, but its role is to communicate about the text and about its own role as transmission medium, so that the reader can (to the greatest extent possible) apprehend some truth of the text. The primary commitment, the goal of the exercise, is for the reader to have access to the text (that is, to some textual artifact that pre-existed the markup relationship). Some form of this approach is extremely common in current applications of the TEI Guidelines: for thematic research collections, scholarly editions, linguistic corpora, oral histories, digital archives, and the like.The second model, though much less common in practice, is of great importance theoretically as a counterpoise to the first, and its importance has been shadowed forth by a number of key interventions during recent years. Theorists like Renear (2000) and McGann (2004) in very different ways have suggested that the performative and illocutionary qualities of markup bear close scrutiny. Sperberg-McQueen and Huitfeldt (2000) explored how markup represents meaning (and by extension, suggest a shift of emphasis onto markup itself as a meaning-bearing system, apart from the text it marks). Flanders (2006) and Flanders and Fiormonte (2007) have turned attention to the rhetoric of authorial markup and to its significance for scholarly communication, thinking of markup as a discourse that is situated at the boundary of production and reproduction. In this more “authorial” model, the encoder uses markup to transact a connection between herself and a reader that concerns a text. The role of markup is to instantiate, to bring into communicative reality, the encoder’s ideas and beliefs about a textual ecology that is oriented towards a particular textual artifact but is not limited to representing that artifact. Rather, the markup may represent a much broader context of interpretation, related information, and argumentation for which the text itself is only the catalyst or point of inspiration. The most common examples in the present day include annotation, “interpretive” markup such as the association of themes and keywords with spans of text (e.g. using the TEI @ana and interp mechanism), and the creation of new documents such as articles using an XML markup language as an authoring system. But these examples do not really give us a field within which to consider what —in a radical sense—we might mean by “authorial markup”, or to pursue the full critical pressure of McGann’s challenge to the markup world: “No autopoietic process or form can be simulated under the horizon of a structural model like SGML” (McGann 2004, 201-2). The way these questions are framed within the digital humanities—as an opposition between mimetic representation and presentation on the one hand, and generative or creative authorship and interpretation on the other—has a correlate in literary history. As Doležel argues (1997), possible world texts, with their “world-constructing” features, contrast with the strongly mimetic and material world truth claims of “world-imaging” texts. We suggest that an exploration of a “possible worlds” ontology of non-mimetic discursive modes may offer a critical vocabulary for thinking about the relationship of authorial markup to other encoding models. It can also help us describe how authorial markup might leverage the formal tools of a structural model in order to enact a generative or poetic mode of markup.We focus in particular on the authoring, interpretation, and markup of texts from the early modern period that concern themselves with precisely this domain. While Gottfried Leibniz nominally inaugurated discussion of possible worlds (in Essays on the Goodness of God, the Freedom of Man and the Origin of Evil, 1710), early modern writers from Thomas More and Sir Philip Sidney to René Descartes and Margaret Cavendish were concerned with the ability of the written word or number to, as Descartes put it, write about that “which does not actually exist…but is capable of so doing” (Descartes, Writing, 332). Such propositional discourse refers less to a verifiable “real” than to a set of possibilities without a fixed ontological status. Instead, as Ruth Ronen suggests, “their state of being is confined to what meaning-units of the text reveal” (Ronen 1994, 98-9). The early modern romance in particular (of which Cavendish is a major exemplar) is the single prose genre most invested in explicit exploration of the mimetic/poetic distinction and in which narrative structures themselves enact the generative logic of world-construction. Cavendish herself argues in her 1667 A New World Called the Blazing World, that her “romancicall” tale offers readers a model by which “they may create worlds of their own, and govern themselves as they please\" (Cavendish 225). Her model of romantic poiesis draws on a poetic tradition exemplified by Sidney’s Defense of Poesie (1579), which refutes directly the critique that poiesis is a form of feigning, by suggesting an epistemological and ethical role for generative authoring.When we turn from Cavendish’s own authoring process to our own use of markup as a means of representing and interpreting her text—and of creating a new work of scholarship of which we are the authors—these arguments have a double impact. Within the world of an encoded text, the “meaning-units” include (for our purposes, most significantly) the markup itself. The markup itself becomes a world-generating mode of knowing that must carry several registers of meaning arising from different kinds of scholarly agency. There are also important questions which we will address in the full paper about how we can anchor and access the domain of meaning such a world establishes. Following Saul Kripke’s assertion that “possible worlds are stipulated, not discovered by telescopes,” we see in both Cavendish’s authoring and our reading and encoding of her text a series of generative or poetic “stipulations” that carry both epistemological and ethical implications (cited in Doložel 1998, 787). We stipulate that a given structure can be characterized as a poem, or that a new paragraph begins in this place. Because there is such agreement (e.g. on disciplinary grounds) about how we read and name literary structures, this stipulation reads like a statement of fact, but if we consider more unfamiliar genres (or genre-resistant texts, like the confounding recursive narratives of Mary Wroth’s Urania or entries in commonplace books), the contentiousness of the assertion becomes more apparent. Possible worlds theory elaborates on what such stipulations accomplish, and on the terms in which we can understand the truth-value of what is created thereby. In the full version of this paper, we will consider how schemas may operate as another way of modeling the possible worlds of texts and genres, and also how possible worlds theory may help us envision more radically authorial forms of markup that may push our ideas of text encoding as scholarly communication into new terrain.", "article_title": "Possible Worlds: Authorial Markup and Digital Scholarship", "authors": [{"given": "Jacqueline", "family": "Wernimont", "affiliation": [{"original_name": "Scripps College", "normalized_name": "Scripps College", "country": "United States", "identifiers": {"ror": "https://ror.org/00p55jd14", "GRID": "grid.421979.0"}}]}, {"given": "Julia", "family": "Flanders", "affiliation": [{"original_name": "Women Writers Project, Brown University, United States of America", "normalized_name": "Brown University", "country": "United States", "identifiers": {"ror": "https://ror.org/05gq02987", "GRID": "grid.40263.33"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Linguistics is facing the challenge of many other sciences as it continues to grow into increasingly complex subfields, each with its own separate or overarching branches. While linguists are certainly aware of the overall structure of the research field, they cannot follow all developments other than those of their subfields. It is thus important to help specialists but also newcomers alike to bushwhack through evolved or unknown territory of linguistic data. A considerable amount of research data in linguistics is described with metadata. While studies described and published in archived journals and conference proceedings receive a quite homogeneous set of metadata tags — e.g., author, title, publisher —, this does not hold for the empirical data and analyses that underlie such studies. Moreover, lexicons, grammars, experimental data, and other types of resources come in different forms; and to make things worse, their description in terms of metadata is also not uniform, if existing at all. These problems are well-known and there are now a number of international initiatives — e.g., CLARIN, FlareNet, MetaNet, DARIAH — to build infrastructures for managing linguistic resources. The NaLiDa project, funded by the German Research Foundation, aims at facilitating the management and access to linguistic resources originating from German research institutions. In cooperation with the German SFB 833 research center, we are developing a combination of faceted and full-text search to give integrated access through heterogeneous metadata sets. Our approach is supported by a central registry for metadata field descriptors, and a component repository for structured groups of data categories as larger building blocks. An increasing number of research institutions in linguistics is systematically archiving research data and making such data publicly available. Users can access such archives via institution-specific websites or purpose-built software where resources can be searched and, in part, downloaded. Some institutions provide access to their archives via OAI-PMH so that the archives’ public content can be harvested and fed into the metadata services of data centers in the community. The metadata provided by the various institutions differ not only in quantity and quality but also in the format of description, as Fig. 1 illustrates. Typically, a research organization designs a metadata schema that it deems to serve best its institutional setting and the number and types of resources it hosts. Different organizations will likely yield various schemas with their own structure and terminology for the schemas’ nodes. Such semantic heterogeneity may be complemented by syntactic heterogeneity as formats may vary (e.g., ASCII, relational database format, XML). An organization’s resources can thus be seen as a forest of trees of the same kind, whereas a tree — i.e., a description of one resource — may have deformations at the leafs as the result of not adhering to the schema. Moreover, trees will look rather naked when resources are described sparsely.  Figure 1: Metadata heterogeneity  Full Size Image   While we expect research organizations to continue managing their research data in their respective ways, we ask them (i) to make their data public in a well-defined XML format to obtain syntactic uniformity, and (ii) to reformulate their schemas to adhere to CMDI (Component MetaData Infrastructure), see (Broeder et al. 2010), a component-based metadata model that makes use of predefined metadata components and the ISOcat data category registry (International Organization of Standardization 2009). The organizations’ archive managers have the opportunity to redefine existing parts of their schema, but they can also choose to keep existing structures and terminologies. In this case the respective metadata descriptors need to be associated with their corresponding semantic points of references in ISOcat, being addressable via unique persistent identifiers. Moreover, archive managers can add new data categories to ISOcat’s private space for immediate availability, and initiate a standardization process to pave the way for their wider use. The storage of resource descriptions has to cope with a multitude of schemas the descriptions adhere to. The use of a relational database would require a mapping of all schemas to a single one, which is all but trivial. Instead, with CouchDB [ (link) ], a no-SQL database is being used that stores arbitrarily structured documents rather than records of some fixed form. The translation between the XML-based CMDI-format into CouchDB’s native JSON format is structure-and information-preserving. Facets serve to blaze the trail. Faceted search enables a user to find specific trees, i.e., resource descriptions, in the various forests by specifying (some of) their common properties. A facet partitions the search space where descriptions, i.e., CouchDB documents, in the same cluster share the same facet value. The selection of multiple facets corresponds to an intersection of clusters identifying resources that have all selected facet values. Faceted search also supplies a user with information about the number of documents in each cluster or intersections thereof — the “mileage” of following a trail. In the presently available data the following unconditional facets are adequate for the various schemas describing linguistic resources: “organization”, “modality”, “language”, “country”, “resourceType”, and “origin”. Facet values may stem from open or controlled vocabularies; controlled vocabulary facets have a stronger tendency to partition the search space into larger units, whereas open vocabularies induce larger search space fragmentations. Once faceted search has focused on a subset of resources, conditional facets allow the introduction of additional context-specific navigational user aids. When users select the facet “resourceType” with value “tool”, for instance, they restrict the search space to just encompass metadata that describes language-processing tools. Here, the conditional facet “toolType” is introduced that partitions the remaining search space according to the type of tool, e.g., language parser, spell checker. Moreover, conditional facets help lowering the complexity of computing search space clusters and their intersections. Metadata schemas vary in structure and terminology. Different names for nodes or leafs may be used to elicit the same meaning, and identical names may be used for semantically different concepts. This makes the mapping of facets to nodes of the various tree types (cf. Fig. 2) rather difficult, and usually requires some intricate knowledge of the metadata forests to be processed. When schemas make use of the aforementioned data category registry ISOcat, such ambiguities can be resolved automatically as names are linked to registry entries.   Figure 2: Mapping facets to the individual parts of the resource trees Full Size Image   Each facet corresponds to an elementary CouchDB view into the database of resource documents. These views serve as a starting point for the generation of complex views that correspond to the various possible navigational paths using facet selection, thus implementing the faceted browser back-end. Elementary views and complex views are generated automatically from a facet specification file, an enriched textual encoding of the table in Fig. 2. Data sets may have resources that are hard to find using faceted search alone. This is true for resources with sparse metadata, or with descriptors that can rarely be mapped to facets. We are therefore using CouchDB’s port to Lucene to perform full-text search across all resources or search spaces restricted by prior faceted search. Fig. 3 depicts a screenshot of the NaLiDa faceted browser; it shows a search state where users selected three facets (“country”, “modality”, “resourceType”) and where the system displays an overview of the remaining search space in terms of the facets, the number of resources available for each of their values, and access to all documents selected so far.   Figure 3: The NaLiDa Faceted Browser Full Size Image  Faceted search is gaining popularity as users can explore large data sets without an intricate understanding of metadata fields or schemas; they obtain an immediate overview of the search space and guidance how to conquer it. A faceted search access to language resources has been implemened by the last author [ (link) ] using Flamenco (Hearst 2006). Our new approach has four main advantages: CouchDB also stores the metadata documents (with varying schemas) and thus also serves as permanent storage; the use of conditional facets contributes to usability as only relevant facets are shown, guiding users’ navigation; index generation accommodates for incremental updates on the metadata sets, supporting regular harvesting without recomputing all indices and views anew; and the faceted browser’s back-end is generated automatically from a facet specification and can be configured easily for other datasets. ", "article_title": "Trailblazing through Forests of Resources in Linguistics", "authors": [{"given": "Reinhild", "family": "Barkey", "affiliation": [{"original_name": "Department of Linguistics, University of Tübingen", "normalized_name": "University of Tübingen", "country": "Germany", "identifiers": {"ror": "https://ror.org/03a1kwz48", "GRID": "grid.10392.39"}}]}, {"given": "Erhard", "family": "Hinrichs", "affiliation": [{"original_name": "Department of Linguistics, University of Tübingen", "normalized_name": "University of Tübingen", "country": "Germany", "identifiers": {"ror": "https://ror.org/03a1kwz48", "GRID": "grid.10392.39"}}]}, {"given": "Christina", "family": "Hoppermann", "affiliation": [{"original_name": "Department of Linguistics, University of Tübingen", "normalized_name": "University of Tübingen", "country": "Germany", "identifiers": {"ror": "https://ror.org/03a1kwz48", "GRID": "grid.10392.39"}}]}, {"given": "Thorsten", "family": "Trippel", "affiliation": [{"original_name": "Department of Linguistics, University of Tübingen", "normalized_name": "University of Tübingen", "country": "Germany", "identifiers": {"ror": "https://ror.org/03a1kwz48", "GRID": "grid.10392.39"}}]}, {"given": "Claus", "family": "Zinn", "affiliation": [{"original_name": "Department of Linguistics, University of Tübingen", "normalized_name": "University of Tübingen", "country": "Germany", "identifiers": {"ror": "https://ror.org/03a1kwz48", "GRID": "grid.10392.39"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "As the digital humanities have become more visible, attracting attention both from publications such as the  Chronicle of Higher Education and from universities eager to develop emerging areas of research and teaching, commentators have been debating what qualifies as digital humanities and whether the community is sufficiently inclusive. In part, as Geoffrey Rockwell suggests, this debate reflects digital humanities’ failure to provide multiple paths to entry. In the past, many entered the digital humanities by apprenticing with established practitioners, but such opportunities are not available to all (Rockwell 2010). New educational programs such as the MA in Digital Humanities at University College London and the proposed MA in Knowledge and Networks at Duke are being put forward to address these needs, joining established programs at Kings College London, University of Alberta, and other universities. As educational opportunities expand, the digital humanities (DH) community should examine how digital humanists are being trained (Hirsch & Timney 2010). Based on this knowledge, the community can create a more coordinated (though still flexible) approach to the DH curriculum that reflects its own commitment to openness, collaboration, interdisciplinarity and experimentation. Education is fundamental both to how the community comprehends itself and how it brings in new members. As Melissa Terras observes, curriculum “can be seen to define the field in the way the publication record cannot,” serving as a “hidden history” that reveals what knowledge experts believe to be crucial and how that knowledge is transmitted (Terras 2005, p.1). By examining digital humanities education, we can participate in a concrete conversation about how to make the field both more inclusive and more targeted toward the core knowledge and skills that digital humanists need.In order to understand how the digital humanities are taught at universities today, I will look at both curriculum and courses.I am in the process of collecting and synthesizing information related to this research, particularly through my Zotero group, “Digital Humanities Education” (http://www.zotero.org/groups/digital_humanities_education), and posts on my blog, “Digital Scholarship in the Humanities” (http://digitalscholarship.wordpress.com/). I am also working on a related project for Brett Hirsch’s collection Teaching Digital Humanities: Principles, Practices, and Politics proposing an open certificate for digital humanities.  First, to get a broader perspective on the digital humanities curriculum, I will examine degree requirements and curriculum plans for digital humanities undergraduate, masters, and Ph.D. programs. How are digital humanities degree programs structured? What courses are deemed necessary, and how are they sequenced? To what extent are projects and/or internships required? How do explicitly digital humanities programs compare to more traditional programs that include a digital humanities component? How are collaboration and interdisciplinarity inculcated? Case studies of several programs will be offered to elucidate different approaches.  I am also analyzing a collection of over 200 DH syllabiAll of these syllabi were found through web searches. Almost all are from universities in the United States, Canada and the UK. The earliest syllabus is from the late 1990s, but the bulk are from the late 2000s. for both graduate and undergraduate courses. These syllabi represent a variety of approaches to digital humanities, such as media studies, text encoding, programming, and information visualization, and come from a range of departments, including history, English, digital humanities, library and information science, and computer science. I am examining: I plan to use a combination of manual and automated methods to analyze the syllabi corpus. I am tagging the syllabi using a set of custom keywords so that I can sort them easily and see emerging patterns. I am also compiling a linked bibliography of readings assigned in DH courses. In addition, I will experiment with methods such as topic modeling and word frequency analysis to categorize the syllabi and extract key concepts. Although my research is still underway, my initial analysis of a subset of approximately 50 syllabi suggests that: In a sense, this research updates Melissa Terras’ 2005 study of the curriculum in humanities computing, charting how the field has changed in the last five years. I am returning to Terras’ question about whether the digital humanities curriculum reflects the field’s research agenda and whether DH has emerged as an “ academic subject,” capable of standing on its own (Terras 2005). I also hope my research helps to inform the digital humanities curriculum going forward. While the digital humanities curriculum should be nimble enough to keep up with the pace of technological change, diverse enough to encompass the variety of approaches to “big tent” digital humanities, and flexible enough to reflect the strengths of particular institutions, the community can benefit from greater coordination and sharing of educational resources to save labor, spread ideas, and provide greater coherence across programs. Of course, my analysis will focus on the characteristics of current DH education programs rather than on emerging areas of skill development, but I hope to document innovative educational approaches as well as points of consensus. Developing a DH curriculum may make it easier for departments (and individuals) to understand what they need to do to beef up their digital humanities portfolio and how they can specialize in particular areas of knowledge. Although my study will not define such a curriculum (which should be worked out by the community rather than an individual), I hope that it will illustrate trends and gaps in digital humanities education.", "article_title": "Knowing and Doing: Understanding the Digital Humanities Curriculum", "authors": [{"given": "Lisa", "family": "Spiro", "affiliation": [{"original_name": "Rice University", "normalized_name": "Rice University", "country": "United States", "identifiers": {"ror": "https://ror.org/008zs3103", "GRID": "grid.21940.3e"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This study aims to introduce how a Korean newspaper corpus, Trends 21 has been constructed and to explore how social, cultural, and linguistic characteristics are portrayed in the Trends 21 corpus. Newspapers contain enormous quantities of language resources which mirror social and cultural characteristics as they undergo gradual as well as sudden changes. Newspapers are regularly published and contain stories of events, personalities, crimes, business, entertainment, society, sports and others. Editorials discuss current or recent news of either general interest or a specific topic. Journalists are trained to write objectively and to show all sides to an issue. In addition, the sources for the news story are identified and are reliable. Therefore, we have employed the newspaper corpus to identify social or culture trends. Trends 21is the name of a project within the government-led humanities promotion program. The Humanities Korea (HK) project is an initiative aiming to foster world-class research centers that carry out interdisciplinary studies in the areas of humanities.The Research Institute of Korean Studies at Korea University has developed the  Trends 21corpus, a collection of four major Korean daily and national newspapers issued from the year 2000. The goal of the  Trends 21project can be summarized with the following three points: first, to construct language resources of newspaper articles as a large general purpose database; second, to identify linguistic/social/cultural characteristics and to analyze their changes in Korea; finally, to measure and to estimate any linguistic/social/cultural trends from patterns of language use. One of the outcomes from this project is the  Trends 21corpus. It is a collection of Korean newspaper texts covering most of the topics in print. In the next section, we present how the  Trends 21corpus has been built.In order to achieve the project goals, articles were culled from a number of newspaper companies. We collected newspaper articles from four major daily national newspapers issued in Korea for one decade, between 2000 and 2009. A daily newspaper is issued every day with the exception of Sundays and some national holidays. A national newspaper, in contrast with a local newspaper that serves a city or region, circulates throughout the whole country.The candidate dailies are Chosun, Dong-a, Joongang, and Hankyoreh.These daily newspaper companies have provided us with all the contents printed for ten years. In electronic form, the majority of newspaper services are provided in various Standard Generalized Markup Language (SGML) format, which is hard for us to unify the format by using NewsML. Due to this situation, we developed a ‘Trends 21 Markup Language (T21ML)’ in order to construct our raw corpus. With the availability of machine-readable texts, especially the collection of a large quantity of articles, it was possible to build a large-scale raw corpus. However, we did not upload all the contents from the newspapers into our corpus. Instead we eliminated irrelevant contents (like obituaries) in order to balance the contents.For our research purposes we established twelve classes of content, namely ‘T21 Class’, to classify the contents of news articles, namely: politics, international news, economics, society, culture, sports, science, columns, opinions, special issues, regions, and people. It excludes lists of names, lists of stocks, obituaries, advertisements, and weather. Although some contents are removed by design, our corpus contains various contents or topics as a whole. Saturation (McEnery  et al.2006) at the lexical level can be tested for representativeness of a corpus.Once the raw corpus was constructed, we employed an automatic morphological analyzer and tagger for Korean, KMAT (Lee & Rim 2009), to annotate parts-of-speech and morpheme information. We applied two-stage tagging processes to our raw corpus, in which an available annotated corpus consisting of 15 million words is corrected by humans and then is employed as a training corpus for the tagger. Further, human annotators not only corrected the erroneous analyses produced by the tagging system, but also improved the tagging system by finding problematic fixed expressions, picking out homonyms, and classifying unseen types of borrowed words or proper nouns. During the first three-year phase of the  Trends 21Project (2008-2010), this corpus has been fully annotated for parts-of-speech and morphological information. Figure 1 shows the processing architecture of building the  Trends 21Corpus.As of Oct 2010, the  Trends 21corpus consists of about 400 million words, and it is by far the greatest morphologically annotated corpus of Korean. In Table 1, statistical information is provided. This information is based on the compiled data between 2000 and 2008.In a case study, we focus on only the nouns that are included in the  Trends 21corpus. A network based approach is then introduced that can deal with visualizing related nouns. According to Stubbs (1996), frequently occurring patterns allow the observer to make deductions about what a group or society sees as valuable or important. Information about collocation means that new concepts and the range of associations of a word can be monitored. We select target words and extract their co-occurring words appearing nearby. Co-occurrence analysis assumes that two semantically related terms co-occur in the same text segments (Sinclair 1991). In contrast to most previous studies that observe co-occurrences within the same sentence, we propose as a search window size a paragraph (Kang 2010). A paragraph of news article is highly coherent in that its sentences are related to one another to describe one short story or an event.The extraction of co-occurring words is based on the statistical information about the co-occurrences of words. The mutual information or z-score has mainly been used in various studies as a statistical measure; however, both of the measures give skewed results to infrequently used words. To reduce this difficulty, we adopt t-score as a measure of how strongly word pairs (a target word and co-occurring words) are related (Kang 2010).Then the information is represented as a network, a formal graph based approach. We have employed  Pajek(Nooy, Mrvar & Vladimir 2005) for analysis and visualization of co-occurrence networks. The network structure typically consists of nodes connected by weighted links. Given the current data set, target words and co-occurrences assign a term or a concept to each node and the values of the t-scores to link. This network provides a graphic visualization of potential relationships between nouns that portray social/cultural trends with respect to their language use patterns.Figure 2 is the co-occurrence network of thirty Korean emotional nouns, such as: ‘love’, ‘hatred’, ‘hope’, ‘disappointment’, ‘happiness’, ‘unhappiness’ and so on. In Figure 2, ‘father’ ( abeciin Korean) co-occurs with ‘love’, ‘hope’, ‘happiness’, ‘hatred’, and ‘unhappiness’; on the other hand ‘daddy’ ( appain Korean) only co-occurs with ‘happiness’.   Figure 2. Network of thirty Korean emotional nouns with their fifty co-occurring words Full Size Image  The word ‘hatred’ co-exists with ‘terror’, ‘Islam’, ‘(human) race’, ‘religion’, and ‘media’. We notice that in the early twenty-first century there were many international conflicts. If we expand the number of co-occurrences, we may deduce different interpretations from the articles.This paper has presented how the  Trends 21corpus is built and how it is composed. We have proposed a visualization method to express co-occurrences of words in an overview network. The network approach to words in news articles represents contemporary Korean language use. Moreover, information about co-occurrences helps us understand social/cultural issues at a point of time.The construction of the  Trends 21corpus is not done yet. The same composition schema is going to be followed year by year in order for the corpus to be constantly updated. In that sense, the  Trends 21corpus serves us as a monitor corpus (Sinclair 1991). This corpus can also reflect language changes in constant growth. In the future we would like to apply cluster analysis as well as keyword analysis. We further plan to enhance the network analysis by displaying concept hierarchy. Finally we also plan to investigate networks according to topics and co-occurrences within an article, not only with in a paragraph.", "article_title": "Trends 21 Corpus: A Large Annotated Korean Newspaper Corpus for Linguistic and Cultural Studies", "authors": [{"given": "Heunggyu", "family": "Kim", "affiliation": [{"original_name": "Research Institute of Korean Studies, Korea University", "normalized_name": "Korea University", "country": "South Korea", "identifiers": {"ror": "https://ror.org/047dqcg40", "GRID": "grid.222754.4"}}]}, {"given": "Beom-mo", "family": "Kang", "affiliation": [{"original_name": "Research Institute of Korean Studies, Korea University", "normalized_name": "Korea University", "country": "South Korea", "identifiers": {"ror": "https://ror.org/047dqcg40", "GRID": "grid.222754.4"}}]}, {"given": "Do-Gil", "family": "Lee", "affiliation": [{"original_name": "Research Institute of Korean Studies, Korea University", "normalized_name": "Korea University", "country": "South Korea", "identifiers": {"ror": "https://ror.org/047dqcg40", "GRID": "grid.222754.4"}}]}, {"given": "Eugene", "family": "Chung", "affiliation": [{"original_name": "Research Institute of Korean Studies, Korea University", "normalized_name": "Korea University", "country": "South Korea", "identifiers": {"ror": "https://ror.org/047dqcg40", "GRID": "grid.222754.4"}}]}, {"given": "Ilhwan", "family": "Kim", "affiliation": [{"original_name": "Research Institute of Korean Studies, Korea University", "normalized_name": "Korea University", "country": "South Korea", "identifiers": {"ror": "https://ror.org/047dqcg40", "GRID": "grid.222754.4"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": " This paper describes a survey study Data gathering is under way at the time of this writing. The data gathering will be complete by the time of the conference. that is part of an ongoing effort (Paling & Martin, 2009; Paling, 2008; Paling & Nilan, 2006) to extend Social Informatics(Kling, 1999) to the study of literature and art. This series of studies has focused on the emergence of new forms of literary expression offered by information technology, and whether and how those possibilities are finding a place alongside traditional forms of expression in American literary writing. The study is meant to be complementary to more discursive, hermeneutic views of literary work. Discursive discussion of literary work provides rich descriptions of work by selected authors. In contrast, a survey enables us to look at the actions of literary community members in the aggregate. This study is based, in part, on the idea that various forms of inquiry into emerging literary practices, taken together, will provide a more complete picture than any one form of inquiry alone can provide. Different approaches to inquiry need not be seen as oppositional. The previous studies in this series (Paling and Martin, 2009; Paling, 2008; Paling & Nilan, 2006) developed, and found support for, a conceptual framework made up of four key values that typify American literary writing: Positive regard was operationalized as responses that indicated admiration or desire for particular qualities, e.g., a desire to read an author's work based on previous work by that author. Similarly, avant-garde was operationalized as valuation of particular characteristics, e.g., a preference for literary work that is fresh or innovative, or electronic work that has characteristics that cannot be produced in print form. The previous studies examined the ways in which literary authors could use information technology to change those key values. They posited, and found support for, the idea of Intensifying Use of Technology, which has three characteristics: For example, a fiction writer could use a hypermedia editor to produce non-linear, electronic fiction (recognition). She might prefer this newer form of fiction (greater emphasis on newer form of support), but also continue to write more traditional work (incomplete rejection). The original study in the series (Paling & Nilan, 2006) involved interviews with a purposive sample for heterogeneity (n=36) of editors of American little magazines. That study used primarily qualitative methods. The second study (Paling, 2008) involved the same sampling method (n=22), but focused on American literary authors. Paling (2008) used both qualitative data as well as quantitative data derived from Likert scales. The third study in the series (Paling & Martin, 2009) was a pilot survey with a random sample (n=84), and that study led directly to the development of the current study. The original studies all showed the presence of intensifying use of technology, but differed in terms of how common that phenomenon seems to be. Because two of the three samples were non-random, and all three were relatively small, a larger, random sample will lead to firmer conclusions about the research questions. The current study concentrated on two research questions:  RQ1: Do members of the American literary community show support for Positive Regard for Avant-garde-ism?  RQ2: Do the actions of members of the American literary community reflect Intensifying Use of Technology with regard to Positive Regard for Avant-garde-ism?  Positive Regard for Avant-garde-ism was selected from the four key values that make up the conceptual framework because it is the most relevant to the use of information technology. It is directly relevant, for example, to how much participants value the use of information technology to produce innovative writing that cannot be done in print. However, the other key values play an important role in establishing the context within which a value such as Positive Regard for Avant-garde-ism comes into play, and they were retained as an important part of this study. The current study represents a clear methodological progression along the line of research begun in the earlier studies. A total of 900 invitations for participation were sent out. The names were selected randomly from a sampling frame built based on publicly available lists from The Association of Writers and Writing Programs and the Modern Language Association, as well as print directories such the Council of Literary Magazines and Small Presses' Literary Press and Magazine Directory. This yielded a sample of exactly 400 participants. All of the potential participants live, work, or study in America, or work for a publisher whose primary presence is in America. An international sample would be desirable in the future, but was beyond the scope and funding level of this study. The respondents were asked to complete a mail survey composed of brief yes/no or checklist questions, as well as questions that included Likert-type numeric scales. The questions were refined versions from the interviews in the earlier studies in the series. Data, and respondent comments, from the earlier studies uncovered minor problems with question wording, scaling, etc. Because of that, the data from this study cannot be combined with data from the earlier studies. Conclusions from the current study, though, should be given greater weight than conclusions from the earlier studies because of the larger, random sample and the refined questions. Much of the apparent difference between the previous studies seems to have resulted from the different sampling methods used in the studies in this series. The first two studies (Paling, 2008; Paling & Nilan, 2006), as mentioned earlier, used purposive sampling for heterogeneity. In other words, an effort was made to seek out editors and authors who were actively involved in producing literature with a substantial electronic component. The strength of purposive sampling is that it allows this kind of effort to closely examine different segments of a community such as people who participate in American literary writing. The weakness of such sampling, though, is that it is very difficult to create a purposive sample that reflects not just the presence of particular phenomena, but also an accurate sampling of how widespread the phenomenon is. The pilot survey (Paling & Martin, 2009) did find limited demonstration of Intensifying Use of Technology. One respondent showed unambiguous evidence of Intensifying Use of Technology. A number of other respondents (8) demonstrated somewhat similar, but less pronounced, patterns of Intensifying Use of Technology. This would suggest that somewhere around 10% of the American literary community demonstrates Intensifying Use of Technology, although the conceptualization of that phenomenon may need to be altered to reflect the opinions of community members who place strong positive value on technological innovation in literature, but who do not actually place greater emphasis on such forms of literature. Taken together, the previous studies suggest that we need to modify, but not reject, the concept of Intensifying use of Technology. Intensifying Use of Technology is a real phenomenon, but is not, to date, widespread in American literary writing. More importantly, the American literary establishment demonstrates very limited levels of the phenomenon. This conclusion is very similar to the conclusion reached by Rettberg (2009). Rettberg argued that electronic literature constitutes a literary avant-garde, but an avant-garde that is not part of any institutionalized mainstream. However, Rettberg's work represents the analysis of an individual directly involved in those efforts, and did not involve structured data gathering to address the actions of literary community members in the aggregate. The current study takes the contrasting approach, gathering data from respondents across the literary community to begin developing a larger picture of how information technology is influencing contemporary American literary writing. The size and type of the sample used in this study should help resolve any ambiguities raised in the previous studies. ", "article_title": "The Cultural Impact of New Media on American Literary Writing: Refining a Conceptual Framework", "authors": [{"given": "Stephen", "family": "Paling", "affiliation": [{"original_name": "School of Library and Information Studies, University of Wisconsin-Madison", "normalized_name": "University of Wisconsin–Madison", "country": "United States", "identifiers": {"ror": "https://ror.org/01y2jtd41", "GRID": "grid.14003.36"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Ever since the cultural turn in literary studies, literary scholarship has focused on examining the cultural work performed by texts. Indeed, one of the enduring theoretical innovations of the 1980s was the reconceptualization of literary value according to the concept of cultural work—the idea that texts should be evaluated not by the innate aesthetic or formal qualities they manifested, but according to their “designs upon their audience” and their ability “to make people think and act in a particular way” (Tompkins 1985). As a result, previously overlooked or marginalized texts have often been deemed suitable for study in the ensuing years based largely the on degree to which they are said to perform particular kinds of cultural work. In one familiar example from the arena of nineteenth-century American literature, scholars have come to see a stereotypically sentimental novel like Uncle Tom’s Cabin as a canonical work primarily because its complex entanglements with the politics of the antebellum United States allow it to be read— to cite Tompkins once more—as part of a “monumental effort to reorganize culture.” But while this approach has become an important characteristic of much literary scholarship, the question of how to measure a given text’s capacity to perform cultural work remains vague and often myopically focused on the synchronic significance of the work in question at a specific point in time. In this paper we investigate the matter of cultural work from the vantage point offered by current scholarship in the digital humanities. We do so with an eye toward developing a model of literary history that draws on uniquely digital methods for structuring and formalizing intertextual relationships, and that make it possible to chart the cultural and formal significance of literary works across space and time. Specifically, we use Susanna Rowson’s late eighteenth-century sentimental novel Charlotte Temple as a case study in how digital literary history can evaluate the meaning and formal significance of a text diachronically. Often cited as one of the earliest American bestsellers, Rowson’s work was reprinted hundreds of times during the nineteenth century and was referenced repeatedly in an extensive body of writing, ranging from reviews and advertisements to melodramatic stage adaptations and ostensibly factual regional histories of the United States. Tracking the novel’s extended nineteenth-century afterlife through this complex network of external references, we demonstrate how the application of detailed interpretive markup to the multiple documents that reference Charlotte Temple (instead of the novel that would, more conventionally, be thought of as the “primary” text) enacts a theory of literary history in which concepts of cultural work may best be observed as phenomena that develop over time. While the idea of interpretive markup is not new, it is most often used either as a means of recording a layer of scholarly annotation on some particular document or classifying portions of a document according to some external taxonomy. Both uses are supported, for instance, by the current version of the Text Encoding Initiative (TEI) Guidelines, which provides examples of specific methods for encoding “semantic or syntactic interpretation” using a set of standard TEI XML elements and attributes. In practice, though, discussions of interpretive markup in digital humanities projects often revolve around questions of readability and the appropriateness of stand-off versus inline markup (e.g. does excessive interpretive markup pose problems for human readability of encoded text? [Campbell 2002]), or issues of preservation, curation, and reliability (e.g. what challenges for encoding textual information accurately and reliably are created by allowing extensive interpretive markup? [Berrie et al. 2006]). Although such questions are important in certain contexts, our project treats them as less important than the question of how interpretive markup may be enlisted as a surface for scholarly analysis—that is, how interpretive structures can themselves be classified and interpreted. Our project adapts several of the interpretive structures provided in TEI XML to represent the connection between specific formal properties in Charlotte Temple and external evidence of the novel’s broader cultural influence, as attested by external references to it. We record basic metadata about each external reference (author, title, date and location of publication, etc.) while also indicating which specific features of the novel the reference comments on, as well as the nature of that commentary. The result is a set of XML-encoded documents classified according to the interpretive work they do: a record, as it were, of how nineteenth-century readers interpreted and responded to the novel’s formal properties. At the same time, a secondary layer of interpretive markup further formalizes the relationships we identify across this primary layer of interpretation. Taken together, both layers provide substantial material for further abstraction: for instance, the automated generation of topic maps that represent a variety of cultural knowledge structures. The result is a web of connections in the form of citations, references, allusions, parodies, and comments spanning the nineteenth century, whose formalized structures constitute the interpretive tissue of digital literary history. The ability to map and visualize these structures, we argue, offers significantly new possibilities for observing cultural work as a phenomenon that evolves over time, and whose connection to particular texts is most productively understood as operating at the nexus of close reading and “distant reading” strategies. Our work constitutes an initial small-scale experiment, but it has relevance for larger questions of scale, purpose, and method in both the digital humanities and conventional modes of literary scholarship. In particular, it suggests that although digital scholarship offers a potential reconceptualization of literary history, practices of literary history also pose interesting challenges for work in the digital humanities. In recent years, projects in the digital humanities have increasingly responded to the question of scale by refining methods for aggregating, classifying, and analyzing enormous bodies of textual material—in other words, by treating text as a mass of data that can yield meaningful responses to statistical analysis of its language. Whether offering answers to now-familiar questions about scale itself— “What do you do with a million books?”—or taking up Franco Moretti’s challenge to peer into the “cellars of culture” represented by the tens of thousands of unread and unknown novels published in the nineteenth century, text mining and other forms of “computational humanities” have been increasingly held up as a means of “checking the generalizations of literary history”(Crane 2006; Parry 2010; Pasanek and Sculley 2008). At the same time, long-running digital humanities projects with deep investments in scholarly text encoding have tended to approach literary history from the opposite direction, emphasizing how “craft encoding,” for instance, employs editorial methodologies and modes of textual scholarship that participate in “making literary history” in the digital medium (Flanders 2009; Dimock 2007). While neither approach directly contradicts the other—indeed, in practice they often coexist harmoniously—they inflect the concept of literary history in crucially different ways. The former often treats the machine-processable language of texts as a primary axis along which historical change in writing manifests itself: intertextual relationships are revealed in changing patterns of linguistic borrowing, with the emphasis on facilitating the “comparability of texts” (Mueller 2008). The latter, while still valuing intertextual comparison, tends to prioritize the documentary and contextual over the linguistic—for instance recording textual variation across editions of the same text, or formalizing through encoding “rich environmental contextualizations” for the study of particular texts (Folsom 2007). Our project borrows from both strategies, and in doing so seeks to demonstrate in practice a method by which digital literary history negotiates this apparent bifurcation. ", "article_title": "Charlotte’s Web: Encoding the Literary History of the Sentimental Novel", "authors": [{"given": "John", "family": "Melson", "affiliation": [{"original_name": "Women Writers Project, Brown University", "normalized_name": "Brown University", "country": "United States", "identifiers": {"ror": "https://ror.org/05gq02987", "GRID": "grid.40263.33"}}]}, {"given": "John", "family": "Funchion", "affiliation": [{"original_name": "University of Miami", "normalized_name": "University of Miami", "country": "United States", "identifiers": {"ror": "https://ror.org/02dgjyy92", "GRID": "grid.26790.3a"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Research in the arts and humanities has created much digital material that represents a significant investment, both of funding and of intellectual effort. In the UK at least, given the current lack of national infrastructure for sustaining this material, these resources are typically hosted in their home institutions using a variety of approaches and technologies. This incurs a number of risks. At the most basic level, without ongoing maintenance a resource ceases to be usable at all as the technologies in which it was implemented become obsolete and unsupported. Even if hosting institutions apply preservation techniques to ensure continued accessibility of resources, this does not enable collections to make full use of technological advances that might greatly enhance their utility for and impact on research. Access to legacy resources may be limited to a simple download or browser access in a website. In neither case does this facilitate advanced research services, such as mashups or data/text mining, that will become increasingly common in future digital research. The impact of humanities research may still be felt many years after the original research was undertaken – the information produced has a long lifespan in intellectual terms. Sustainability does not just mean keeping the data alive, but enabling the exploitation of advances both in technology – making the data accessible in new ways – and in humanities research – forging connections between resources that lead to new discoveries and broader impact. Digital resources often exist in “silos”, lacking interoperability. Individual projects typically address focused topics, and may implement digital resources in idiosyncratic ways and to address their immediate needs. This results in a multitude of resources that are scattered and disparate in nature, yet related intellectually, resources that, linked up, would form a whole much more useful for research than the sum of the parts, much as fragments of a map, when combined, allow navigation from one place to another. Ultimately, the vision here is of a virtual and distributed “web of knowledge”. The digital resources in the humanities may be characterised by their diversity and complexity. Collections involve multiple media and standards. The material may be highly complex, with many structural and semantic relationships both internal and contextual; the interpretation of an object (e.g. an inscription) may depend on its relationships to other resources (e.g. other inscriptions/texts, surveys, concordances). One approach to this would be to develop enhancements to individual resources; however, to be truly sustainable we should avoid such ad hoc solutions. The primary question asked by our project is thus – how can we develop a generic framework for digital resources in the arts and humanities that addresses the above issues for a broad range of collections, and that is not a closed system but can be extended to support other digital material and (possibly unanticipated) future tools, technologies and research methods? We are attempting to answer this question in the CMES (Content Models for Enhancement and Sustainability) project, which is funded by the UK Arts and Humanities Research Council as part of its DEDEFI (Digital Equipment and Database Enhancement for Impact) programme. We are developing a framework using the Fedora digital repository software for sustaining and enhancing particular groups of digital resources produced by earlier digital humanities. We are addressing two groups of collections, each typical of a wide range of humanities research activities:  Digital texts, which may comprise complex networks of diverse information: images, markedup text, geospatial data, translations, standoff annotations/markup, and potentially extensive links to external resources. We are addressing two groups of resources managed at KCL: the Stormont Papers, and the Inscriptions of Aphrodisias. These contrasting examples – one modern and dealing with large, complete volumes, one ancient and dealing with small, fragmentary texts –facilitate development and testing of generic models. They also provide scope for demonstrating the utility of our framework for (i) developing new material (e.g. Stormont parliamentary papers and Inscriptions of Roman Tripolitania/Cyrenaica), and (ii) forging links with external digital material (e.g. Westminster Hansards and Pleiades).  Multimedia performing arts collections, specifically the following resources managed at KCL: Scottish Traditions of Dance, which contains text, images, video, interviews, audio and databases, Adolphe Appia, which contains images, 3D virtual reality models, and audio from the King’s Sound Archive.  Fedora is particularly good for modelling complex material and links between objects. Representations of digital objects within Fedora are formalised as “content models” (henceforth, CMs), which may be regarded as “data types” for digital objects. We will review the selected collection groups and develop a set of CMs that support them by providing consistent, standardised and interoperable (yet flexible) patterns for representing these collection types. We will need to go beyond Fedora’s relatively simple CM formalisation to produce these “Content Patterns” (henceforth, CPs) for complex collections, e.g. by using the Enhanced CM framework developed by SULD, which allows the specification of relationships and ontologies, and the definition of collection templates. We analysed the resources along with subject specialists in digital text and performing arts resources. Note that, given the variation in how legacy collections have been implemented, the CPs may be idealisations that do not directly match the collections, which may require a degree of reworking to make them fit. We will not be overprescriptive here – diversity arises naturally from the research material – but a degree of common practice would be beneficial for the creation and reuse of the material. Moreover, our CPs will provide foundations that can be extended easily to support diverse community practices. Each of the target collections had its own custom web interface, driven by quite different underlying data models. We are developing consistent delivery/publishing mechanisms for the different collection groups that are driven by the underlying CPs. This has the benefit that these mechanisms are available for any collection that conforms to the CP, leading to more consistent and interoperable interfaces for resources of similar type. However, this will not necessarily lead to homogeneity. Our approach enables the structure of collections to be represented with fine granularity, and interfaces are correspondingly modular. This facilitates the creation of more integrated web views across different collections, but it also allows content to be exposed as machinereadable feeds that can be used to provide addedvalue services, e.g. aggregating content, automated processing (e.g. text mining), mashups etc. The creators (or curators) of a resource will no longer be the arbiters of how information should be delivered and used. The resources produced by research are not just ends in themselves – they provide source material for subsequent research – and to maximise impact they should be made available in ways that allow scholars unrelated to the original editors to make transformative use of them, rather than just via a website. We are thus providing a framework whereby users (perhaps domain experts) can develop and integrate their own tools to process resources. The project is thus not only enhancing particular collections, but producing a framework that is extensible in several ways:   The generic CPs and associated tools provide templates for simplifying creation of new collections of similar form (e.g. digital texts), and guarantee certain functionality that conformant collections would inherit from the template.   The set of content patterns is itself extensible, following the same methodology, to other collection types.   The framework can be extended with new tools as technologies change (tools/services can be linked to CPs and inherited by collections that follow the pattern). The project thus builds on existing efforts and provides a foundation for a broader and longerterm programme for sustaining and enhancing digital humanities research. Developing this framework to support resources based around digital texts and performing arts will cover a significant amount of ground, and provide a springboard for future extensions. It will also ensure sustainability by integrating these initiatives into repository and curation infrastructures at KCL, and will allow a growing corpus of digital material to be integrated into this infrastructure. ", "article_title": "Content Patterns in Digital Humanities: a Framework for Sustainability and Reuse of Digital Resources", "authors": [{"given": "Sheila", "family": "Anderson", "affiliation": [{"original_name": "King’s College London", "normalized_name": "King's College London", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/0220mzb33", "GRID": "grid.13097.3c"}}]}, {"given": "Mark", "family": "Hedges", "affiliation": [{"original_name": "King’s College London", "normalized_name": "King's College London", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/0220mzb33", "GRID": "grid.13097.3c"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This paper describes a collaboration between eScientists and humanists; specifically Grid scientists and language and literature scholars, working together to create a repository of data sets and tools, combining our most advanced knowledge in all areas. Language and literature scholars make use of variety of language resources to conduct their research. Such resources include dictionaries, thesauri, corpora, images, audio and video collections. At present most of these resources are distributed, non-interoperable and license protected. As a result researchers typically conduct their research through direct access to independent data sets using multiple browser windows and multiple authorisations. This approach results in non-scalable and less productive research, and often leads to incomplete and/or non-verifiable results. The JISC funded project, Enhancing Repositories for Language and Literature Researchers (Enroller) is addressing these issues through development of a targeted eResearch environment. This paper presents the current state of progress and outlines how secure access to distributed data resources with targeted analysis and collaboration tools is supported. In the full paper we will also describe how Enroller is exploiting high performance computing infrastructures such as the UK National Grid Service and ScotGrid, and discuss a problematic issue that has arisen through the differing working practices of humanists and scientists.Consider a typical language and literature scenario where a researcher wants to search for a word such as \"bonny\" in the dictionary to find its meaning; in a thesaurus to look up the concepts and categories it is found in and in a corpus to find the documents containing it. The user may also want to see the concordances and word frequency of the word in each found document. In undertaking this process, they might want to save the different results; share them with others and possibly perform a comparison between many different resultant data sets. This scenario becomes more challenging when multiple dictionaries, thesauri and text corpora need to be cross-searched simultaneously, for example when the researcher wants to lookup the word \"bonny\" in the Oxford English Dictionary, in the Scottish National Dictionary, and in the Dictionary of the Older Scottish Tongue. The researcher may also want to search for the word in the Historical Thesaurus of English to look up synonyms and related concepts and categories and then search for all of the matching concepts in further corpus resources. Researchers will want to use the standard text analysis tools: e.g. concordances, word frequencies, collocation clouds. They may well wish to save and download the results for further analysis or use targeted tools to investigate, e.g. variant spellings of the word 'bonny'. The problem is further exacerbated if the researcher decides to search for multiple, possibly hundreds, of words at once and do all of the mentioned tasks at once. Most of the language and literature data environments do not permit scholars to do any of these activities, instead researchers are left with individual level data sets, coded differently (e.g. with different metadata and data formatting), accessible through individual web-based resources with individual access codes and methods.  The challenge for the project is to maintain the data integrity of its collaborators and the security of access-limited data, while facilitating research across and between each dataset for the benefit of researchers in multiple fields. Enroller provides an interactive research infrastructure that provides seamless, secure access to all of the different language and literature data sets in a user-oriented environment. Furthermore, since many of the searching and analysis efforts can be computationally intensive – especially when bulk searching or building of indexes is required - we provide secure access to high performance computing infrastructures such as the UK e-Science National Grid Service (http://www.ngs.ac.uk) and ScotGrid (http://www.scotgrid.ac.uk). In this project, language and literature researchers, including an associated network of international scholars, are now able to access large amounts of language and literature data and analysis services from a single, easy-to-use portal. Enroller is currently working with the following data sets:  The EPSRC and AHRC funded Scottish Corpus of Text and Speech is a collection of text and audio files covering a period from 1945 to the present. The SCOTS corpus is available in TEI-compliant XML. (http://www.scottishcorpus.ac.uk)  The AHRC funded Corpus of Modern Scottish Writing offers a collection of texts and manuscript images from the period 1700 to 1945. (http://www.scottishcorpus.ac.uk/cmsw/)  The AHRC funded Newcastle Electronic Corpus of Tyneside English provides a corpus of dialect speech from Tyneside in Northeast England. The NECTE corpus is encoded in TEI-compliant XML. (http://www.ncl.ac.uk/necte)  The Dictionary of the Scots Language encompasses two major Scottish language dictionaries: the Scottish National Dictionary and the Dictionary of the Older Scottish Tongue. DSL data is available in XML format. (http://www.dsl.ac.uk/dsl)   The Historical Thesaurus of English contains more than 750,000 words from Old English to the present in 250,000 categories. HTE was published in print form by Oxford University Press in 2009 and is a new and significant development for historical language studies. (http://libra.englang.arts.gla.ac.uk/historicalthesaurus/)   The Oxford English Dictionary is a commercial resource published by Oxford University Press and is the authority on English language vocabulary. It is accessible through our search interface by API. (http://www.oed.com)   The inclusion of other data sets is underway, e.g. we have incorporated Hansard, early C19th to late C20th, and are negotiating for the 1755 Dictionary of Samuel Johnson. Many scholars have no platform or assistance to put texts online and make them accessible to others, far less can they make them interoperable with other relevant data sets. Enroller provides a place where users can deposit their own texts. Texts are wrapped in a basic, TEI minimal XML envelope and indexed. The user can choose whether a text is available to all or private. Once deposited a text can be searched from the portal with the other data sets. The project has data at its heart, but it is also exploiting state of the art high-performance computing and security solutions. In particular the project is employing a Virtual Organisation Membership Service (VOMS)-based solution in accessing the NGS where pooled Enroller accounts are used by researchers accessing these resources through a targeted project portal. This includes use and exploitation of the Workload Management System (WMS) to provide resource broking based job scheduling across all of the NGS nodes. This job scheduling is targeted currently to supporting large-scale searching based upon the Google MapReduce application.  The full paper will describe the Enroller project in more detail and outline the capabilities that are supported and our experiences so far in implementing this infrastructure. This includes a description of how the portal has been made accessible through the UK Access Management Federation; the easy to use search system and the reasons for its human-computer interface choices; the advanced Grid-based information retrieval system, capable of executing linguistic workflows, taking advantage of HPC facilities such as NGS and ScotGrid; how the system supports large-scale data indexes for fast searching; support for tools for linguistic analysis, including concordance, collocation and word frequency analysis; support for seamless secure access to licensed data; support for data deposition and automated indexing services; documented analysis of our user experiences in using of the infrastructure provided thus far. We will outline the plans for future adoption by the wider research community and end with our reflections on this eScience/humanities collaboration. ", "article_title": "Enroller: A Grid-based Research Platform for English and Scots Language", "authors": [{"given": "Jean", "family": "Anderson", "affiliation": [{"original_name": "University of Glasgow", "normalized_name": "University of Glasgow", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/00vtgdb53", "GRID": "grid.8756.c"}}]}, {"given": "Mark", "family": "Alexander", "affiliation": [{"original_name": "University of Glasgow", "normalized_name": "University of Glasgow", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/00vtgdb53", "GRID": "grid.8756.c"}}]}, {"given": "Johanna", "family": "Green", "affiliation": [{"original_name": "University of Glasgow", "normalized_name": "University of Glasgow", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/00vtgdb53", "GRID": "grid.8756.c"}}]}, {"given": "Muhammad", "family": "Sarwar", "affiliation": [{"original_name": "National e-Science Centre, UK", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Richard", "family": "Sinnot", "affiliation": [{"original_name": "University of Melbourne", "normalized_name": "University of Melbourne", "country": "Australia", "identifiers": {"ror": "https://ror.org/01ej9dk98", "GRID": "grid.1008.9"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "I shall examine to what extent the BVH (Virtual Humanistic Libraries) project on Montaigne could be considered not simply another electronic edition, but also a component of a digital humanities infrastructure, observing the keywords of an integrated search: reliability, sustainability, dissemination, and above all, reusability. Is a project about Montaigne's work compatible with the \"genericity\" required for an undertaking that concerns a wide community?The Bibliothèques Virtuelles Humanistes (http://www.bvh.univ-tours.fr) offers facsimiles (jpeg and light pdf) of books or manuscripts, extracted graphics with their indexing systems, and a textual database called Epistemon. It offers two types of digital surrogates of the book: the facsimile and, for some documents written in French, the corresponding transcription without modifications.In itself, the idea of digitizing Montaigne's complete works is not original. A \"Corpus Montaigne\" already exists on CD-Rom but no online version is available at present, and access is limited to the few libraries that could afford it. Corpus Montaigne, Paris: Champion-Garnier électronique, 1999.The \"Montaigne project\" in Chicago (P. Desan) offers many documents related to Montaigne: it displays the \"Villey\" edition with every page of the \"Bordeaux copy\" (Exemplaire de Bordeaux, the so-called \"EB\"), but the distinction between the three main layers (1580-1582, 1588, 1588-EB) does not comply to the requirements of modern philology and scholarship: cancellations are not visible, and the editor modified the punctuation as well as the spelling.Montaigne project, http://www.lib.uchicago.edu/efts/ARTFL/projects/montaigne/The 1595 (posthumous) edition is already available in HTML format at the mysterious Trismegiste website, but there is no XML encoding, and the spelling is regularized. http://www.bribes.org/trismegiste/es1ch03.htm.In our project, all the editions will offer the double display of original/regularized spelling; indexes of names, places, errata, and a basic encoding appropriate for early printed books and manuscripts. Easy retrieval of both versions, in the format preferred (XML/TEI, HTML, PDF) will be the user's choice, according to the principle of reusability.As we share our expertise with cultural institutions, we borrow our techniques and methods of digitization of cultural heritage objects, such as rare books collections, from libraries and archive repositories: digitization, metadata organization and catalogs, and database management. Our membership in the European Library (Europeana) helps to understand the difference between a cultural heritage attitude and a research project.http://www.europeana.eu/portal/.The parallel display of the facsimiles and their transcriptions, TEI encoding, tools for scholarly annotations and an accurate query system are not simple challenges to take on: the uniqueness of every work of art, the complexity of the process of writing seems incompatible with the unified view of textual databases usually found in library websites (e.g. Shakespeare at the British Library) or linguistic corpora (Frantext database or ARTFL in Chicago). Scholarly annotation will be minimal, and limited to the accuracy of the transcription, in order to provide a basis for further commentary, encyclopedic information, and glossaries. The very process of building the corpus for online publication is a field of new research in this case, for it combines ergonomic full display and retrieval, complex and relevant extraction procedures, treatment of texts and graphics.The \"Montaigne at work\" project aims to support both the reading and the mining of the text, and to render the chronology. Our new data, expertise, and tools will try to fulfill the main goal we have always had of understanding Montaigne's Essays better: 1) to offer a genetic editionof the \"Bordeaux copy,\" containing several layers of handwritten additions that reveal up to seven moments of writing or re-writing; 2) to give access to what is left of the famous \"Librairie de Montaigne\". The main corpus (all the editions from 1580 to 1595, with their transcriptions) will be enshrined inside a wider set of later editions of the Essais (Marie de Gournay's copy, Rousseau's copy), of several other works of Montaigne (the translation of the Theologie naturelleby Raymond Sebond, the Journal de voyage), of all his surviving manuscripts (marginalia, letters and Parliament archives), and of facsimiles of about a hundred identified sources (mainly classics, but also books by his contemporaries).The genetic edition of the Bordeaux copy, compatible with the TEI schemas for manuscripts and prints, and the \"TEI Renaissance encoding\" protocol developed in Tours, Manuel d'encodage TEI-Renaissance, 2009, http://www.bvh.univ-tours.fr/XML-TEI/index.asp.raises the question of the relevance of such an undertaking. It must from with a benchmarking of other websites offering open access to digitized works of late Medieval and Early Modern period (Chaucer, Dante, Shakespeare, Cervantes, Descartes, Molière,...). What kind of textual properties do these sites represent? Do they use several models? Exclusive tools? Many literary projects, particularly in France, do not use TEI encoding (Flaubert in Rouen, Montesquieu in Lyons, Stendhal in Grenoble), and scholarly corpora seem to be specific to each author.Classicists and Medievalists have opened many doors, and they know quite well how to refine an ultra-diplomatic encoding and display. Rendering the writing process requires the adequate edition to feed every hypothesis about the moments of the gesture itself, the \"traits de plume\" (pen strokes), and the modifications the printing press of the time forced upon the original. Special software designed by our computer science partners (in Tours, Paris, Rouen, La Rochelle) is currently being developed to detect image similarity. Thus, Montaigne's different \"hands\" could be classified according to time and language, with the expert help of Alain Legros (researcher in Tours, and an expert in Montaigne's handwriting).LEGROS, Alain, Montaigne manuscrit, Paris : Garnier, 2010.We need also the clearest visualization of the readable parts, the possibility of displaying either a smoothed text or a page, which represents all the complex arrangements of the words in a spatio-temporal order. No models seem to be directly reusable: ours would take place between the very precise reconstitution of all the spellings of Medieval texts (e.g. the Actes des Apôtresproject) and the Madame Bovarydigital edition of manuscripts at the University of Rouen, but with a display system that would look like the Deutsche Text Archive (DTA): the facsimile of the page linked to the HTML text, and to the XML/TEI source, searchable with PhiloLogic (Mark Olsen, University of Chicago) and other NLP tools, with the XTF search engine. Actes des Apôtres, http://eserve.org.uk/anr/; DTA, http://www.deutschestextarchiv.de/; Madame Bovary, http://bovary.univ-rouen.fr/; XTF, http://xtf.cdlib.org/documentation/programming-guide/.All the quotations of the Bordeaux copy will be fully referenced and translated in French.In Tours, we have already begun the keyboarding and encoding of the main editions (1580-82, 1588, 1595). The genetic edition of the Bordeaux copy will be based on the principles of the ITEM laboratory (École Normale Supérieure, Paris), a leader in genetic analysis, which are compatible with the TEI tagging of the main operations (addition, deletion, inversion, etc.), according to the latest documentation of the TEI consortium. The COST \"Interedition\" project (funded by Europe) offers several tools to test (e.g. Collatex for the main editions), and discusses some issues close to our preoccupations, such as the limits of crowdsourcing: we plan to use collaborative annotation by scholars for corrections of errors. http://www.interedition.eu/.Every layer of text must be retrievable, to avoid incompatibility between the genetic and the generic, and to guarantee reusability to anyone who wishes to process the text (with permission) for other purposes. Ideally, a collaborative edition of Montaigne's Essaiswould blossom out of the accurate transcription of the Bordeaux copy, and/or of the posthumous 1595 edition: the debate is still pending among specialists.We will generate three levels of transcripts:  the \"quasi-diplomatic\" transcript, crucial for the comparison between the typeset and the handwritten passages (the spelling of which has never been thoroughly studied) the \"cultural heritage\" transcription that regularizes the distinction of I/J and U/V, expands the brevigraphs and normalizes the ends-of-lines, so that the corpus can be processed by the NLP tools and parallel corpora analysis the modernized version, so that powerful search engines can offer accurate results to anybody.  A prototype of I/J U/V normalization tool is already prototyped in Tours and Poitiers, with a set of rules and specific dictionaries; the modernization tool is in progress, and requires another set of rules, and other dictionaries.The development of these tools benefits one of the two Google awards that the University of Tours obtained in December 2010 for \"Full-text retrieval and indexation for Early Modern French documents\". New software will process a sentence such as:With these spellings, the user who is not a specialist will find only few results in his word or string query because of typographic abbreviations ( façōfor façon), obsolete morphology ( veusfor veux), and the frequent lack of hyphenation. In modern editions, one will find easily « estude » in the editions following former spellings; but if one looks for « étude » (in modern French), the old spelling will not be offered, and one will miss the variant « estude » in the corpus, where moreover the word is typed without hyphenation.Cf. the Old English variation analysis in the York-Helsinki corpora (http://www.helsinki.fi/varieng/CoRD/corpora/YCOE/index.html).Thus, Montaigne's library itself can be rebuilt through the comprehensive digitization of what remains of the hundred known copies with his signatures and annotations: 33 are preserved at the French National Library, 30 at the Bordeaux Public Library, others in at least 15 other libraries and private collections. Samples of his handwriting will be analyzed and compared to non-attributed manuscripts, in order to confirm or exclude dubious documents.Such a project will enlarge the knowledge we already have of Montaigne's method of writing, within the context of his favorite readings. If other projects provide data, this one offers also reusable sets of transcriptions, facsimiles and new tools for further analysis.", "article_title": "Reusability of Literary Corpora: the \"Montaigne at work\" Project", "authors": [{"given": "Marie-Luce", "family": "Demonet", "affiliation": [{"original_name": "Centre d'Etudes Supérieures de la Renaissance, UMR-CNRS , Université Francois-Rabelais, Tours, France", "normalized_name": "François Rabelais University", "country": "France", "identifiers": {"ror": "https://ror.org/02wwzvj46", "GRID": "grid.12366.30"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Dictionaries lie at the core of the human ability to conceptualize, systematize and convey meaning. But a dictionary (both print and digital) is many things at once: a text, a tool, a model of language, and a cultural object deeply embedded in the historical moment of its production (Tasovac, 2010). While it is true that we now live in the age of the electronic dictionary (de Schryver, 2003), dictionaries have always played an important role in the interplay between production technology and knowledge taxonomies (McArthur, 1986; Hüllen and Schulze, 1988; Hüllen, 1999). In this respect, historical dictionaries remain particularly valuable objects of study because they illustrate sociolinguistic perceptions and reveal culturally shaded conceptualizations of lexical knowledge of a particular epoch — often in stark contrast to our contemporary attitudes and values. Moreover, they pose a veritable challange for text encoding, semantic markup and database modeling (Fomin and Toner, 2006; Nyhan, 2006; Nyhan, 2008; Mooijaart and van der Wal, 2009; Lemnitzer et al., 2010). This is why all dictionaries, including retrodigitized historical dictionaries, are important for digital humanities, and why DH — with its concern for (abstract) modeling of knowledge and its (practical) implementations in humanities research — can integrate and propel different trains of lexicographic and metalexicographic thought at the intersection of language and technology. Many DH research projects have aimed to produce electronic editions of printed lexicons (see for instance Morrissey, 1993; Lemberg et al., 1998; Christmann, 2001; Fournier, 2001). In such efforts, retrodigitization is usually based on one of two approaches: either the production of “faithful“ digital copies (at the cost of reproducing factual or typographic errors), or the structural modelling of the content, which treats the print edition as a data source, rather than as an immutable text to be reproduced in its entirety (Lobenstein-Reichmann, 2008). In either case, retrodigitization projects tend not to involve any degree of re-editing or expanding the actual content of historical dictionaries. We agree with Kirkness (2008) that digitalizing historical dictionaries can increase and optimize their use value, especially in global, networked environments. But we also feel that one central aspect is often overlooked in current studies of retrodigitized dictionaries: users interacting with a historic lexicon do no longer necessarily have active command of the text's primary language. Even when historical dictionaries are retrodigitized with the user's needs in mind, the focus is usually on easy-to-handle navigation, presentation layout and retrieval of elements from a full-text search (Christmann, 2003); or on uniformization of existing data elements, such as dates (Kinable, 2006). While these efforts are worthwhile and necessary because they contribute greatly to editions that are more usable and eﬃcient than their hardcopy counterparts, electronic dictionaries remain in essence lookup tools (for words encountered in a given text) rather than exploratory tools (for unknown words or concepts). This, we believe, can reduce both their scholarly and popular appeal. It may seem unlikely at ﬁrst that historical dictionaries can generate non-academic interest, but experience has shown that there is a broad audience outside highly professionalized linguistic circles that is both curious and enthusiastic about exploring the historic and ethnographic fabric of a language (Kirkness, 2008). In our own web-project — “Reklakaza.la” (Serbian for “hearsay”) — we have been publishing online selected entries from the classic, 19th-century Serbian Dictionary by Vuk Stefanović Karadžić and linking them via social networks Twitter (http://twitter.com/ Vuk_Karadzic) and Facebook (http://facebook.com/reklakaza.la). The project has gained more than 24,000 fans on Facebook alone, becoming a platform for bringing meaningful humanities inquiry into the public conversation, fostering the sense of community, sharing, and mutual learning that proves the relevance of the humanities in today's world despite academic budget cuts and declining job opportunities. The success of our pilot project has strengthened our conviction that a modern, electronic edition of Karadžić's dictionary is long overdue. Vuk Stefanović Karadžić (1787-1864), the linguist, folklorist and reformer of the Serbian language, published his landmark Srpski rječnik, istumačen njemačkijem i latinskijem riječima — Lexicon Serbico-Germanico-Latinum in two editions (1818 and 1852). This ﬁrst lexicon of the modern Serbian vernacular, rather than the Church-Slavic hybrid language used by the educated elites up to the 19th century, has a unique place in the history of not only the Serbian language, but the South Slavic diasystem in general (Дмитриев and Сафронов, 1984; Wilson, 1986; Стојановић, 1987; Eschker, 1988; Potthoff, 1990; Ивић, 1990; Vitalich, 2005; Кулаковский, 2005). The text is rich with ethnographic and anthropological material. Not only do many entries contain examples of Balkan folk storytelling, but some are themselves structured as historical, cultural and ethnographic narratives that offer informative sketches and sometimes even very detailed accounts of the myths and realities of the Balkan past (see, for instance, entries for кмет, отмица, мора, хајдук, etc.). Though it was republished twice (in 1898 and 1935), the Lexicon has not been reprinted since (other than in facsimile editions). Meeting the needs of modern-day users, however, presents a host of editorial challenges. The entries in the Lexicon are written mainly in a dialect which is on the margins of contemporary standard Serbian. Thus, the lexicographic material is not always entirely understood by contemporary speakers, and can often appear obscure or unwieldy. It is hard for the average user to answer questions such as: what was the early 19th-century equivalent of a mod-ern-day Serbian word? What household objects, for instance, are listed in Karadžić‘s dictionary? What words were difficult or impossible for Karadžić to translate into German or Latin? Our “Annotated Digital Edition of Vuk Stefanović Karadžić’s Srpski rječnik” is therefore conceived as a resource that caters to access needs and habits of modern scholars, teachers, students, and, last but not least, general readers. The entries are marked up XML, in compliance with the Guidelines of the Text Encoding Initiative (Burnard et al., 2006). In this, initial phase of the project, we are focusing solely on text encoding, but in view of the potential use in a data-base driven web-application at a later stage. In addition to marking up existing structural elements of a dictionary entry (such as lemma, part of speech, senses, deﬁnitions, translation equivalents, examples etc.), our work supplies important additional information that will enhance the modern-day user's interaction with the dictionary, including:  standard Serbian equivalents to dialect word forms (e.g. бичевање vs. бичкарење, мешина vs. мљешина, енглески vs. инглешки);  Serbian ekavian word-forms to both standard and east-Herzegovina jekavian entries (e.g. терати vs. тјерати, терати vs. ћерати);  both the original 19th-century accentuation (e.g. кòчија̑шки) and its modern-day graphic equivalent (кòчија̄шкӣ);  indications when modern-day accentuation differs from the from found in the Lexicon (e.g. мо̑ре vs. мо̏ре, das Meer, mare);  an extension of the extant cross-reference system through linking synonymous and near-synonymous entries that have been overlooked by previous editors (e.g. жаба and напнигуша; обрљуга and неопера).  labeling of Turkisms overlooked by previous editors (e.g. була, инћар, џукела);  marking up persons, places and dates for easy indexing and analysis;  indications of word usage (eg. ист. and ист. кр. as <usage type=\"geo\">East</usg> for better statistical analysis and possible further processing and creation of geo-spatial word maps etc.);  marking up instances where Karadžić uses a ﬁrst-person narrative to explain an entry;  indications of the edition in which entries appeared for the ﬁrst time, etc.  Furthermore, we are assigning semantic domain labels to word senses in accordance with Magnini and Cavaglia, 2000; Bentivogli et al., 2004, cross-referencing senses with the Transpoetika Dictionary — a bilingualized, Wordnet-based Serbian-English dictionary (Tasovac, 2009), and providing English glosses in addition to the existing German and Latin. All of this will help us meet our goal of moving beyond the current paradigm of limiting retrodigital text editing to the creation of electronic replicas of hardcopy lexicons or semantically structured electronic representations of the original data source. We are interested in hybrid approaches that respect the integrity of the original text, but also take advantage of the digital medium to create modern, deeply-encoded, user-centered editions of historical dictionaries, which can not only provide look-up mechanisms for particular words, but also function as exploratory tools for various types of knowledge discovery. Some practical advantages of our edition of Vuk Karadžić's dictionary will include reverse look-ups, allowing a user to search an English, German, or Latin word and ﬁnd its Serbian equivalent in the Lexicon. The domain labels will provide researchers with valuable and, for the ﬁrst time, measurable information about the clusters of paradigmatically related terms, as well as the extent of domain ambiguity and domain variability. Users will be able to treat semantic domains as thematic entry points into the dictionary, looking up, for instance, all entries that belong to AGRICULTURE, FOLKLORE, HISTORY or GASTRONOMY; while our logically and semantically consistent markup of Karadžić's own usage notes will make it possible for users to easily explore regional and dialectological distribution of entries in the Lexicon, offering a basis for subsequent work that could involve data visualization, statistical analysis, text mining etc. ", "article_title": "A User-Centered Digital Edition of Vuk Stefanović Karadžić's Lexicon Serbico-Germanico-Latinum ", "authors": [{"given": "Toma", "family": "Tasovac", "affiliation": [{"original_name": "Center for Digtial Humanities (Belgrade, Serbia)", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Natalia", "family": "Ermolaev", "affiliation": [{"original_name": "Center for Digtial Humanities (Belgrade, Serbia)", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This paper will report on the first eighteen months of a 3-year grant funded by the Fonds québecois de la recherche sur la société et la culture led by playwright Patrick Leroux (overseeing the creative component) and Michael Eberle-Sinatra (overseeing the academic component). The specific nature of this group project is nestled in the promising dialogue to be established between Romantic literature scholars, a theatre practitioner, and a scholar preoccupied with the pedagogy of Romantic drama using hypermedia as a template and an engaging interface.When artists teach, they never quite relinquish their initial creative impulse. Historical works, while being taught for their intrinsic value and larger pertinence within a literary context, nevertheless solicit a resonant response. Classroom exercises in both academic and creative courses suggest that many students engage in a similar empathic manner when allowed to prod, question, and interact actively with a studied text.The “creative” component of this research-creation project with strong pedagogical intent is precisely linked to an artistic response to the source text, Joanna Baillie’s Witchcraft. In addition to the edited text, its scholarly annotations and commentary, and the filmed Finborough production of the play, we will create workshop situations with actors and students in which the play will be explored in rehearsal prompting us to investigate other manners of staging the work and illustrating, through filmed documentation, the process of reading a text for performance. Short video presentations of key creative and interpretive issues will be edited for inclusion in the hypermedia presentation. The actual process, whether filmed or not, will allow the actors and creative faculty to fully immerse themselves in Baillie’s world and work in order to fuel their resonant responses to them.This second creative component, the resonant response, will take the form of short theatrical pieces conceived for film. The nuance is essential as the pieces will not be short cinematic films but rather short-filmed theatrical pieces. The emphasis on speech, dramatic action, and relationship to a defined theatrical space will differentiate these pieces from more intimate, image-based cinematic pieces. The resonant responses could be as short as two minutes or as long as ten minutes, in order to fully explore very precise formal issues (a character’s speech, the subtext in a given dialogue, what we couldn’t stage during the 19th Century but feel we could now). These creative pieces will be developed with Theatre, Creative Writing, and English literature students and faculty.Existing TEI guidelines for scholarly encoding do not account for the unique relationship between a play script and performance practice and history. Scholarly encoding typically views the structures of texts in relation to the protocols that guide how readers interpret documents. But dramatic scripts require different kinds of reading and, thus, different kinds of encoding. Performance-informed inquiry into play texts depends on a reader’s ability to think about the range of possibilities—both historically distant and contemporary—for theatricalization of a line of dialogue, a bit of physical action, or a visual space.Additional historical materials on the theatre and culture of Baillie’s era will be provided by team members. For our hypermedia resource to organize multi-media materials in ways that will help students in literature classes to use the hypermedia edition of the play, we will need to develop innovative customizations of TEI encoding guidelines. Discovering how best to support a student reader’s work with a historically unfamiliar dramatic work provides an important test case for existing guidelines for XML encoding of drama.This project will take an innovative approach in several senses. It will use hypermedia to try to solve a classroom problem created by plays with little performance history or connection to familiar theatrical styles. It will also test the limitations of the TEI scholarly encoding guidelines by exploring how, in the case of play scripts, building hypermedia resources requires creative, user-oriented strategies of encoding. The research-creation program will illustrate how contemporary artists can engage with historical works, while shedding light onto the theatrical creative process. Finally, our Resonant Response to Joanna Baillie’s Drama will combine scholarly research on Romantic drama, practice-driven analysis, the creation of new work, and hypermedia expertise.This particular research-creation program is singular and innovative in its combination of academic close reading, dramaturgical analysis, dramatic writing and theatrical performance, filmed theatre, and a resolutely pedagogical preoccupation with a full and thorough exploration of the possibilities of hypermedia edition.In addition to creating a prototype hypermedia edition, the project seeks to find out:  what value performance annotations can add to a teacher’s work with students on a seldom-performed play; how the Text Encoding Initiative’s (TEI) scholarly encoding guidelines can best be customized to design hypermedia play editions; how the process of collaboration among faculty and students in humanities and communications disciplines can enrich understanding of technology’s interaction with interpretation. ", "article_title": "Joanna Baillie’s Witchcraft: from Hypermedia Edition to Resonant Responses", "authors": [{"given": "Sinatra", "family": "Eberle-Sinatra", "affiliation": [{"original_name": "Université de Montréal, Canada", "normalized_name": "University of Montreal", "country": "Canada", "identifiers": {"ror": "https://ror.org/0161xgx34", "GRID": "grid.14848.31"}}]}, {"given": "Tom C.", "family": "Crochunis", "affiliation": [{"original_name": "Shippenberg University", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Jon", "family": "Sachs", "affiliation": [{"original_name": "Concordia University", "normalized_name": "Concordia University", "country": "United States", "identifiers": {"ror": "https://ror.org/01qxhf360", "GRID": "grid.448967.0"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This paper presents preliminary results on using computational analysis to understand the representations and constructions of gender and the body in fairy tales. While scholarship on contemporary fairy tales utilizes various cutting-edge theories, ranging from postmodern narrative to feminist theories of gender performance (Bacchilega 1997, Benson 2008, Smith 2007, and Tiffin 2009), little of the research on canonical fairy tales or oral folktales incorporates these recent theories. Additionally, folkloristic research on fairy tales, whether contemporary or traditional, would benefit from incorporating computational methods such as network analysis. These methods allow scholars to test their theories more quickly and empirically.Our research utilizes nearly three hundred canonical fairy tales and oral folktales, deemed canonical because they are from well-known collectors such as the Brothers Grimm, or because the tales are examples of well-known plots spanning time and space in Europe (such as “Snow White” and “Cinderella”). We combine textual and network analysis with discipline-specific expert oversight for a large-scale, theoretically informed discussion on gender and the body that would not be possible without both in tandem. A feminist critique of fairy tales is predicated upon the notion that fairy tales construct and represent bodies differently according to gender, yet no studies have shown whether this difference actually exists in canonical tales, or have addressed what this difference would mean for studies of cultural values and narrative strategies (Bottigheimer 1987, Haase 2004, and Stone 2008). Computational analysis of how bodies and body parts are depicted in the text provides empirical evidence against which this and other aspects of feminist theory can be tested.Humanities scholars have already established a vast theoretical and methodological framework for interpreting texts, and they ought to be able to view their data in the context of those theories developed within their disciplines. This study combines traditional critical analysis with computational tools in an attempt to utilize the best of both worlds.Our analysis uses a hand-coded database representing a geographically and temporally diverse sample of tales. Careful attention was paid to the tale tellers and collectors for further study of the context in which bodies are depicted.Fairy tales as a genre span oral, communal performances and literary, single-author renditions. In order to represent this spectrum, our database tracks specific references to bodies in six tale collections. We collected 13 data points from nearly three hundred tales (Tale, Collection, Author, Teller, Collector, Year of Writing/Collecting, Year of Publication, Tale Type, Region, Original Language, Gender of Teller/Writer, Gender of Collector, Gender of Editor) and categorized another 14 data points for every mention of a body in each tale, some evident in the texts (Noun, Adjective, Surrounding Text, Page Number, Gender, Young/Old, High/Low, Quoted Speech, Skin Tone) and some requiring interpretation (Positive/Negative value, Grotesque, Violence, Nudity, Move).The database variables were chosen in light of pre-existing work on structure and theory, creating a layer of interpreted data that would not be found in full-text analysis alone. The “Tale Type” classification system gives tale plots numbers so that their transmission can be traced as tales migrate across linguistic and national boundaries. This is what allows us to generalize about the worldview contained within the tales, as the same plot with variations occurs between multiple ethnic groups. The concept of “Moves” breaks up tale plots into 5 distinctive plot pieces based on folkloristic theory of how tales are structured.We use co-occurrence and vector analysis to explore the database. Each field is compared against several others in order to find correlations. For example, “beautiful” may only be referenced with young women, or old rich men may only appear in tales from certain tellers. Using dimensionality reduction, we can find which body parts tend to be discussed in tandem in various situations. We also explore how the representations of bodies change throughout the plots of tales using Bengt Holbek’s “Moves.” Holbek built on the work of Vladimir Propp, who identified the most important plot points in sequence that could occur in a fairy tale (31 points, or “functions,” total) (Holbek 1998). Holbek condensed Propp’s functions into five “Moves,” or clusters of thematic actions that move the tale’s plot forward.Finally, networks of database data are generated and analyzed to test the hypothesis that fairy tales construct bodies differently according to gender. This analysis serves both as empirical evidence to test a theory and also as an exploratory tool, revealing possible correlations or links between body representations that are not immediately apparent in the texts.Folklorists approach fairy tale interpretation in many ways: ethnographic approaches seek connections between the taletellers’ lives and the tales’ content; historical approaches search for and analyze the origins and diffusion of tales; structural analyses seek to understand the underlying narrative of the tales; psychological approaches search for latent meaning in the tales; and feminist and sociohistorical approaches interpret the meanings of the tales as they relate to, convey values from, and inculcate values of the social world. Feminist scholars have been particularly active in critiquing the normative beauty scripts and gender roles promoted in fairy tales. This study investigates how gender roles are constructed and situated in fairy tales, which is why we encoded categories to investigate links between gender, age, and social position, as well as where in the tale’s structure these social values are relevant. We also hope to obtain information about how female and male bodies are valued differently, hence the relevance of variables like “grotesque”.Second-wave feminists such as Simone Beauvoir developed the notion of the universal masculine perspective, the idea that in Western culture, the public, unmarked, assumed universal position is in fact specifically male. Our data supports this assertion in terms of female bodies being marked within fairy tales, but we also believe that the same principle applies to the age of bodies. Youthful bodies are assumed to be the unmarked universal category in fairy tales.The descriptions of old bodies are strikingly polarized: old people are more likely than young to be described as evil or good, wicked or wise. These findings suggest that old bodies must be differentiated in fairy tales, because they are no longer in the supposedly universal category of youth. Old bodies are qualified with more descriptions in order to give audiences a sense of who these characters are, since they don’t fall into the category of the youthful protagonist, with whom listeners are supposed to easily identify. If the bodies in fairy tales had the same meanings across age and gender, we would have seen a proportional relationship between the number of references to types of bodies, and the number of adjectival descriptions attached to each. However, the data shows that young men are associated with adjectival descriptions less frequently than any other type of body. Old women, in contrast, are associated with adjectival descriptions more than any other grouping. Further, a wider variety of adjectives are used to qualify old bodies than young compared to the proportion they are mentioned. Figure 1 shows a sample of which adjectives are associated with mentions of various bodies.  Figure 1. Lines are drawn between adjectives (red) and the body-types they modify (yellow). Node size represents word usage counts and edge thickness represents frequency of co-occurrence. Full Size Image   Our method of layering computational analysis atop previous theoretical research can be used as a template for further studies, especially those of other folk narrative genres like legends or ballads. Some of the most intriguing questions in folklore research pertain to how verbal expressive genres relate to the lived experiences of their performers—and a method for digitizing and interpreting these texts could yield valuable insights.As digitization is interpretation (Tarte 2010), it is necessary to be especially careful and theoretically-grounded when choosing variables and selecting exactly what data will populate the fields. The scholar must also decide the most fruitful analyses to run on the data available. These studies ought to also include computational analyses that are not linked to previous critical theories (like word frequency or co-occurrence), however, in order to check against biases which might creep into variable choice. The ultimate goal is to turn well-researched, theoretically sound scholarly observations into machine actionable data which can be analyzed to test the scholar’s hypotheses and open the door for future studies.", "article_title": "Computational Analysis of Gender and the Body in European Fairy Tales", "authors": [{"given": "Scott", "family": "Weingart", "affiliation": [{"original_name": "Indiana University, United States of America", "normalized_name": "Indiana University", "country": "United States", "identifiers": {"ror": "https://ror.org/01kg8sb98", "GRID": "grid.257410.5"}}]}, {"given": "Jeana", "family": "Jorgensen", "affiliation": [{"original_name": "Indiana University, United States of America", "normalized_name": "Indiana University", "country": "United States", "identifiers": {"ror": "https://ror.org/01kg8sb98", "GRID": "grid.257410.5"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "The Victorian Women Writers Project (VWWP,  (link) ) began in 1995 at Indiana University under the editorial leadership of Perry Willett and was celebrated early on for exposing lesser-known British women writers of the 19th century. The VWWP’s original focus on poetry was meant to complement The English Poetry Full-Text Database, but soon Willett acknowledged the variety of genres in which women of that period were writing – novels, children’s books, political pamphlets, religious tracts. The collection expanded to include genres beyond poetry, and continued active development from 1995 until roughly 2000 at which point the corpus reached approximately two hundred texts. These nearly two hundred texts comprise only a small fraction of Victorian women’s writing. Encouraged by renewed interest among Indiana University’s English faculty and graduate students, the Indiana University Libraries and the English Department are exploring ways to reinvigorate the project. The real challenge lies in the project’s past and present susceptibility to graceful degradation, which can be defined as stagnation or “deterioration of a system in such a manner that it continues to operate, but provides a reduced level of service rather than failing completely” (“Graceful degradation”). This has been a recurring topic in the digital humanities community as evidenced by a recently published article cluster, “Done,” in the Digital Humanities Quarterly (Spring 2009, v 3 n 2) and research by Bethany Nowviskie and Dot Poter on the very topic: “Decline is a pressing issue for digital scholarship because of the tendency of our projects to be open ended. One could argue that digital projects are, by nature, in a continual state of transition or decline. What happens when the funding runs out, or the original project staff move on or are replaced? What happens when intellectual property rests with a collaborator or an institution that does not wish to continue the work? How, individually and as a community, do we weather changes in technology, the patterns of academic research, the vagaries of our sponsoring institutions?” (“Graceful Degradation: Managing Digital Projects in Times of Transition and Decline”). The Orlando Project is a text-based resource containing primary texts, archival documents, biographies, chronologies and bibliographies. Despite the collection’s extensive size, Brown et al. reveal their sustainability challenges in an article published as part of the “Done” cluster: Lack of people, time, or funding has consigned more than one project involuntarily to becoming a static tribute to its former activity. The reasons for this include people moving on, intellectually or institutionally, without taking their projects along with them, or people using electronic media to disseminate without particularly desiring to exploit their potential for continual updating, but even where the will to continue persists, inadequate funding mechanisms for sustainability may make it impossible. This is a shame, since, as we have argued here in the case of Orlando and many other digital publications not only does there remain the potential to enrich the contents, but the first iteration often merely begins to tap the potential of the project’s data architecture and potential for interface development (“Published Yet Never Done”). Unfortunately and to no surprise, the VWWP, quite modest compared to its Orlando Project counterpart, has also suffered from nearly all the challenges highlighted by Nowviskie, Porter, and Brown. In an effort to combat this “darker side of project management,” a framework for continual project support is being explored that reaches beyond any one individual or department (Nowviskie and Porter). At the crux of this framework is digital humanities-focused curriculum-building for the English department in partnership with the library, with a concentration on scholarly encoding and textual editing, working with English faculty, librarians, and technologists. The goals are to leverage domain expertise in the English Department; integrate the VWWP as a core research and teaching tool in the English curriculum; develop TEI and text encoding expertise in faculty and students; and through coursework, internships, and other opportunities, encourage English literature students— graduates and undergraduates—to continually contribute new content to the VWWP. Tools and workflows, such as robust encoding guidelines, quality control assessment, etc. will be provided to ensure proper markup, and the VWWP editorial board will additionally vet course output before submission to the project. Through our newly offered graduate English course (L501, Digital Humanities Practicum), an eager and curious group of students learned not only encoding skills but also began to develop the collaborative practices pervasive in the digital humanities. As part of our talk, we plan to explore whether cultivating “markup skills” are sufficient enough in establishing a digital humanities curriculum (Rockwell) and whether “majoring in English” today means the curriculum should include awareness of the possibilities that arise for new scholarship when technology is applied to literary studies (Lanham). Certainly Indiana University is not breaking new ground or alone in this endeavor, but the literature is scarce is terms of understanding successes of graduate level digital humanities curricula situated in an English or any other humanities department. As Diane Zorich reports in her recent review of digital humanities centers, “A Survey of Digital Humanities Centers in the United States,” archives such as the Willa Cather and Walt Whitman Archives are precisely leveraged for teaching and learning, and this reporting is promising for the Victorian Women Writers Project as a project reconceived to meet both teaching and research needs in a classroom setting (19). Currently, the VWWP is a standard e-text project, although current plans call for phased, modular development that will eventually include now commonplace “Web 2.0/3.0” functionalities. By garnering institutional commitment (at the risk of wavering priorities) across multiple departments (thereby minimizing risk), we hope to achieve the following:   Encourage English department buy-in and continual collaboration by updating the current state of the VWWP’s functionality and modernizing the look-and-feel (eliminate “first impressions” syndrome discussed by Brown et al.)   Establish a sustainable scholarly encoding infrastructure based in the English department curriculum   Provide a consistent mechanism (e.g., coursework output) for critical content to accompany the encoded texts   Facilitate connections between other DLP-supported Victorian projects like the Swinburne Project and the Victorian Studies Bibliography   Evolve the project’s encoding guidelines, inclusion of critical contextual materials, and advanced functionality (e.g., visualizations, textual analysis tools, blog integration, etc.) so that the VWWP becomes a dependable, growing and relevant online resource that can be adopted as a pedagogic and research tool for Victorian scholars  Our talk will introduce the Victorian Women Writers Project, explore curriculum-building strategies; and propose ways in which faculty and students can reliably and perpetually contribute to the VWWP. ", "article_title": "Victorian Women Writers Project Revived: A Case Study in Sustainability", "authors": [{"given": "Michelle", "family": "Dalmau", "affiliation": [{"original_name": "Indiana University Digital Library Program", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Angela", "family": "Courtney", "affiliation": [{"original_name": "Indiana University Digital Library Program", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This paper will demonstrate ComPair, a new tool to investigate and compare word usage, encouraging new ways to explore language variation. While remaining focussed on the usability and the promotion of navigation, this tool represents an evolutionary step forward from the author’s previous award winning visualisation applications. This paper will introduce the methods and technologies at its core, perform a demonstration of the tool and discuss opportunities for further collaboration.Firth in 1957 tells us ‘You shall know a word by the company it keeps’ leading to a contextual investigation of language which remains with us today. Identifying a word of interest and examining its collocates, often tells us more than a traditional dictionary definition ever could. Traditional corpus tools display collocates in tabular format, providing rich statistical data at the expense of giving the user an opportunity to see the overall linguistic landscape. Tools such as Beavan’s Collocate Clouds present this information very differently, visualising the collocates in cloud form, as in figure 1.  Figure 1. Collocate Cloud of node word ‘stars’ [http://www.scottishcorpus.ac.uk/corpus/bnc/collocatecloud.php?word=stars (accessed 1 November 2010)] Full Size Image   A collocate, if known, can be quickly located due to the alphabetical nature of the display. Frequently occurring collocates stand out, as they are shown in a larger typeface, with collocationally strong pairings highlighted using brighter formatting. Therefore bright, large collocates are likely to be of interest, whereas dark, small collocates perhaps less so.Louw introduced us to semantic prosody, which describes how synonymous words can actually take on positive or negative connotations. A natural way to investigate this would be to separately compare the collocates of each node word of interest. This can be performed by looking at multiple collocate clouds side by side, or by using statistical tools presenting tabular data. While these methods may be best suited to the comparison of many node words of interest, ComPair provides a solution to the comparison of two words, while keeping true to the aims of collocate clouds.Semantic prosody is illustrated in figure 2, comparing the collocates of ‘utterly’ vs. ‘absolutely’. Negative terms cluster near ‘utterly’ where as positive terms cling to ‘absolutely’. At face value these words are synonymous, but they are clearly used in different contexts and are not simply interchangeable. These are often issues which challenge learners of English as a foreign language.  Figure 2. ComPair visualisation of ‘utterly’ vs. ‘absolutely’ in the British National Corpus Full Size Image   ComPair calculates the collocates of both node words, ranking the results using a combination of frequency of co-occurrence, and by collocational strength (adopting the Mutual Information (MI) measure). A continuum is formed, with each extremity representing the separate node words (imagine a piece of string, with a label representing each search term at each end). The collocates are then distributed along this continuum, using the relative pull of collocational strength towards each node (the words near the end of the string are strongly associated with those end labels, those in the centre much less so).ComPair displays this continuum, displaying each node word and the collocates between them. The display uses a spectrum of colour, to further enforce the ordering of the collocates. In the MI tug of war, the words in the centre share similar MI scores with each input term. Typically these are fairly low MI figures and appear green. In the ‘utterly’ vs. ‘absolutely’ example above ‘ridiculous’ appears in pink, this indicates that while the MI scores (utterly- ridiculous’ vs. absolutely- ridiculous) are roughly the same, they are much higher than the surrounding collocates. Ridiculous is therefore a word used strongly with both utterly and absolutely. Those collocates appearing close to each node, and sharing its colour are used very strongly with that node word, and only that word. Figure 2 tells us that things can be ‘absolutely marvellous’ but not ‘utterly marvellous’. In comparison, someone can be ‘utterly ruthless’, but not ‘absolutely ruthless’.At present ComPair allows for the comparison of two separate words in a single corpus. One possible extension would be the facility to search for the same words across two corpora. Imagine two corpora of differing political parties. With a single search term, ComPair would help expose the views and attitudes towards that concept. Another avenue would contrast word usage in British vs. American English.Other applications would involve its use as a learning tool, allowing users to go beyond dictionaries and thesauri, to see in detail how different words actually operate. Visualisation of more than two node words should also be possible given different display techniques.", "article_title": "ComPair: Compare and Visualise the Usage of Language", "authors": [{"given": "David", "family": "Beavan", "affiliation": [{"original_name": "University of Glasgow, United Kingdom", "normalized_name": "University of Glasgow", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/00vtgdb53", "GRID": "grid.8756.c"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "How were computers introduced to the public and how did humanities issues figure in the introduction of computing? The time has come in the digital humanities to think historically about computing in the humanities as Willard McCarty has pointed out in Humanities Computing and other venues. This paper describes a study of public representations of computing in Canada using the Globe and Mail, our major national newspaper. This paper restricts itself to what we call the incunabular years when computing was still a curiosity and business applications didn’t yet dominate the public discourse.References to digital computers in the Globe and Mail start in 1950 with a report of the annual meeting in London, England of Ferranti Ltd. This report describes under the heading of “Instruments” the development of a digital computer that was probably the predecessor of the Ferranti Atlas which pioneers like Susan Hockey worked on.\"The instrument department has a design team of considerable strength working in conjunction with Manchester University on the development of an electronic digital computer.\" (Globe and Mail, Oct. 31, 1950, p. 21)From this first reference to digital computers buried in a business report, interest in and then anxieties about computers grow steadily through the 1950s and early 1960s until by 1964 the Globe runs a full page story in “The Woman’s Globe and Mail” on “Will Computers Replace the Working Girl?” by Michelle Landsberg (Globe and Mail, May 21, 1964) that warns of the effects of automation on women who do most of the clerical work that can be automated. Computers go from being objects of curiosity in research labs at the University of Toronto with speculative utility to instruments that are changing the nature of office work, especially for women. It is this “incunabular” period that interests us, partly because it is a period when the academy is one of the major sites where computers are being installed and because academics are explaining computers to the broader community. This paper, using these early stories about computers, will tease out a history of early representations of computing in Canada. Specifically we will:  Describe the content analysis methodology used in this study. Discuss the ways computers are presented to the public in the first decade and a half. Who is represented as having access to digital computers? What tasks are they presented as suitable for? Discuss how computing jobs like data-entry, training and programming are gendered in the public discourse of the Globe. Discuss the first references to the installation of computers at Universities and how these installations were presented to the public. How was research computing presented and how were humanities applications of computing presented? The Globe and Mail online archive, unabashedly titled “Canada’s Heritage from 1844,” provides a full-text historical database of articles, ads, editorials and special features. In order to document the early discourse around computers we searched and collected all references to the word “computer” for content analysis coding and reading. The first reference we found dates back to 1897, though in this case it is not a reference to digital computers, but the computer as a type of job. The first reference to digital computers dates from 1950 and the number of references per year remains fairly low until the early 1960s when computing takes off as a subject of news, advertising and opinion.Once gathered we coded the articles for content analysis. The coding rubric was developed iteratively as we read articles and developed hypotheses. For example, during the coding we became interested in gender and went back over the early stories to recode materials. Below is an example of the codes applied:  Type of reference (i.e. news, classified ad, feature) Photos (i.e. were there photos and what do the photos show) Category of Application (University, Science, Military, Automation, Industry, Government, and Arts and Humanities) Gender of Named People (male, female, both, none) Discourse like “Brain” (Is the computer described as a giant “brain”) Types of computers mentioned in the references where applicable Close reading and content analysis led us to identify and follow a number of themes running through the stories in these incunabular years. For example, related to the Ferranti mentioned in the first reference, is a Canadian turn from being oriented towards computing in the UK to being oriented towards computing from the USA. This turn is obviously a much larger issue for Canada after the war than just a change in where computers are coming from, but you see the turn in the articles from the 1950s. Bylines from London dominate in the early 50s, but by the end of the 1950s New York begins to be source of information about computers outside of Canada. You can see the early orientation towards the UK in titles like the 1955 article “Britain Leads in Office Automation.” But, by 1961, in a comprehensive pair of articles on the computer industry, it is clear that US companies dominate. As the author Hugh Munro puts it, “Ferranti is the only company that designs and manufactures in Canada – specializing in big installations – which means the country is heavily dependent on the United States, where the other companies are based, not only for supplies but for technological advancement in the computer field.” (Munro, “Surging Computer Industry Confident It Has Only Begun”, December 6th, 1961, p. 23.)One theme that stands out in the early representations of computing in the Globe is how women were excluded from computing. Advertisements for programmers are in the “Help Wanted Male” section. It isn’t until 1960 that a woman is named in a story and then she is discussed as an exception. The exclusion of women gets discussed explicitly in 1964 when Michelle Landsberg writes the extraordinary feature “Will Computers Replace the Working Girl?” mentioned above (Globe and Mail, May 21, 1964). This feature confronts the effects of automation on women who happen to do most of the clerical work that is being automated. In the full paper we will contrast the exclusion with what we know of pioneers like Beatrice Worsley who was one two staff hired initially by the University of Toronto Computing Centre.  Computers and the Working Girl; Title, Illustration and Text Full Size Image   The second story published about digital computers in the Globe is much more substantial than the Ferranti reference and it describes the first research computer installed in Canada at the University of Toronto. The story titled “Junior Electronic Brain Cost $100,000” dates from 1951, was accompanied by two photographs, and is about the UTEC Jr. computer installed at the U of T’s Computing Centre. It is really the first significant story about computing in the Globe, which is significant. In the full paper we will go into some detail about this first significant representation of computing as it illustrates many of the other points we want to make. Much could be said about the U of T Computing Centre and the birth of academic computing in Canada. Here we will restrict ourselves to the way computing was presented to the public as starting at the University of Toronto. The U of T Computing Centre stayed in the news for decades, along with its director, Dr. Gotlieb, who was the most frequently mentioned expert of the period. But academic computing is not just about Gotlieb and research at his Centre. By 1955 we see the first ads for computing courses at U of T and the first reports of computers at other universities. By 1957 we see an article about computing in the arts and humanities. This article, titled, “Strange Music Made By an Electronic Brain” reports about a music composition experiment.  \"To the casual observer, the squeaks, squawks, groans and hints of tunes were a harsh cacophony. To Professor C. C. Gotlieb and his colleagues, the sounds were the Iliac Suite, a string quartet composed by the electronic brain at the University of Illinois. It was an experiment in composition designed by Prof. Gottlieb and his fellow-workers with the university’s electronic brain to show that humans are not the only ones that can compose music.\"What stands out about this and subsequent stories is that they are about the computer as an extraordinary device best understood as an “electronic brain” performing tasks that are human. The stories don’t really report humanities applications differently from scientific and engineering ones. Instead this research brain is presented as answering questions and completing tasks from all fields – it is a general purpose inquiry engine that the U of T Computing Centre is turning to questions and academic tasks from one field to the next. The brain is curious as we are, and the stories convey a sense of the discovery as Gotlieb’s team crosses disciplines discovering new uses for the computer. What remains to be seen (in a future study) is how the public discourse matches or reflects the evolving discourse within the academy and especially in humanities computing circles. We humanists, after all, are also reading the news; did we come to computing influenced by news of its promise or were we concerned about how it might affect our work?", "article_title": "Computing in Canada: a History of the Incunabular Years", "authors": [{"given": "Geoffrey", "family": "Rockwell", "affiliation": [{"original_name": "University of Alberta", "normalized_name": "University of Alberta", "country": "Canada", "identifiers": {"ror": "https://ror.org/0160cpw27", "GRID": "grid.17089.37"}}]}, {"given": "Victoria Susan", "family": "Smith", "affiliation": [{"original_name": "University of Alberta", "normalized_name": "University of Alberta", "country": "Canada", "identifiers": {"ror": "https://ror.org/0160cpw27", "GRID": "grid.17089.37"}}]}, {"given": "Sophia", "family": "Hoosein", "affiliation": [{"original_name": "University of Alberta", "normalized_name": "University of Alberta", "country": "Canada", "identifiers": {"ror": "https://ror.org/0160cpw27", "GRID": "grid.17089.37"}}]}, {"given": "Sean", "family": "Gouglas", "affiliation": [{"original_name": "University of Alberta", "normalized_name": "University of Alberta", "country": "Canada", "identifiers": {"ror": "https://ror.org/0160cpw27", "GRID": "grid.17089.37"}}]}, {"given": "Harvey", "family": "Quamen", "affiliation": [{"original_name": "University of Alberta", "normalized_name": "University of Alberta", "country": "Canada", "identifiers": {"ror": "https://ror.org/0160cpw27", "GRID": "grid.17089.37"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This paper explores the nature of digital materiality as it resides in the visual and written culture of Early Christianity. Within archaeology and related disciplines, “materiality” is a theoretical approach that focuses on physical things - such as objects, books, or buildings - as one starting point for building an understanding of past thought and behavior (White 2009). As a term, \"digital materiality\" does not yet have a fixed meaning and can refer to the physical manifestations of the computer age (Manoff 2006), to the processes by which digital representations become physical architecture (Gramazio and Kohler 2008) or to the effects of digital information in the modern world (Leonardi 2010). Here, I mean \"digital materiality\" as the transport of information about the material culture of past societies, and particularly the material culture of Early Christianity. Looking for fluid relationships between thought and object in ancient evidence suggests that \"digital materiality\" is an appropriate metaphor for both recovering past interactions with material culture and for describing the role of networked information in modern archaeological and art historical scholarship. It is this intersection of past and present that is of particular interest. While stressing potential, this paper also looks to the practical consequences of current efforts to digitize ancient activity that survives in material form.Existing virtual representations have already exposed clear overlaps between the written word as object and the manifestation of those concepts in visual media. The Codex Sinaiticus is a fourth century codex bible removed from Saint Catharine’s monastery on the Sinai Peninsula in Egypt in the 19th century and now largely in the British Museum. Its early date makes it plausibly the first extant bible as that term is conceived in Christian terms. Most of the surviving pages of the codex are available online at the site http://codexsinaiticus.org/ . Among the passages found there is John 20:24-29, where the disciple Thomas, of “doubting Thomas” fame, demands to touch the wounds of the resurrected Jesus when he appears to his followers. The passage ends with the exhortation, “20:29. Because you have seen me, you have believed; blessed are those who have not seen and yet have believed.”The traction this concept had in Early Christian culture is clear from a hammered gold disk produced in Egypt and now in the collection of the American Numismatic Society that has a stable digital representation available at the URI http://numismatics.org/collection/0000.999.51006 . This physical object quotes the text of John 20:29 while illustrating Thomas in the act of touching Christ. This paper stresses that the modern opportunity to engage in such a self-referential illustration of the materiality of thought in the Later Roman Mediterranean is a serendipitous result of independent efforts to digitize the material record of that time and place. Just as the creation of the surviving material record should be recognized as the cumulative action of many individuals, it is likely that exploration of that record will be enabled by many projects and institutions working within their own areas of expertise and with content specific to their domain (Heath 2010, Terras 2010). It is the interactions of a series of self-digitizing and independent communities – here Early Christian textual studies and Numismatics – that can recover relationships between physical object and human thought that is a primary goal of materiality as a methodological approach.It is, of course, important to recognize that while the Internet will make evident the material implications of past human thought and action, it will not of its own bring scholars into direct contact with the material culture they study. Digital Humanities as applied to archaeology and visual culture will usually mean working with surrogates: one cannot download an object, one can only see its representation . The network does not take us to a site, it only provides access to descriptions and pictures. Digital materiality is therefore an act of transmission (Liu 2004) so that its deficiencies leave it open to criticism.Trends within the study of textual evidence as embodied in manuscripts suggest that this observation is not a barrier to analytical progress. Projects such as the Codex Sinaiticus digitization effort are showing that digital access to manuscripts is returning the material to a central place in the study of primary sources that had been abstracted in critical editions. The digitized page images show in great detail the large number of variants and corrections that make plain that written evidence for the ancient world does not exist independently of its physical media. The Homer Multi-Text project (http://chs.harvard.edu/chs/homer_multitext) is self-consciously engaged in enabling virtual access to multiple manuscripts of the Iliad and Odyssey that range from the Hellenistic to Medieval periods. Such initiatives indicate that material and thought become meaningfully unified in a digital domain. Accordingly, digital materiality is not a poor substitute for direct autopsy of material culture. Rather than de-emphasizing the physical, Digital Humanities will bring it to the fore. But it needs to be pointed out that at this moment, the best critical editions of the New Testament (Aland and Nestle 2006, Aland et al. 2006), with rich apparatus for the Gospel of John, is not available online so that commercial interests are an impediment to to the study of the text, whether considered as an idealized abstraction or a material object. This suggests that the constituent components of both modern and ancient digital materiality are at a transitional point where \"primary sources\" are more accessible than \"secondary works\".It is particularly important to stress this point when we recognize that it is no longer possible within archaeological scholarship to have hands-on access to all relevant material (Stewart 2008). To recall the ending sentiment of the doubting Thomas story, “blessed are those who have not seen and yet have believed.” This can be applied to the current state of archaeological and material studies and lightly reformulated as an invitation to both make use of the full potential of material and textual sources on the internet, and to aggressively pursue such availability.", "article_title": "The Digital Materiality of Early Christian Visual Culture: Building on John 20:24-29", "authors": [{"given": "Sebastian", "family": "Heath", "affiliation": [{"original_name": "Institute for the Study of the Ancient World, New York University", "normalized_name": "New York University", "country": "United States", "identifiers": {"ror": "https://ror.org/0190ak572", "GRID": "grid.137628.9"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "In digital humanities the word \"text\" (in both mass and count noun senses)By the end of the paper I hope the relation between the mass and the count senses of the noun \"text\" will be clear. That \"text\" has a count noun sense is embodied in the Text Encoding Initiative Guidelines (2008 passim). occurs ubiquitously; familiar uses include text encoding, full text search, there are six texts online, we are using the text of the first edition. Typically the word is not defined in the specific context of its use, nor is there an overarching definition or description so widely accepted that it is taken as given at all times. However, our various uses of \"text\" show we have a priori assumptions about the nature and scope of its reference. But what are those assumptions? are they justified? do they collectively define \"(a) text\" for us?Text encoding models have of course benefited greatly from the \"ordered hierarchy of content objects\" definition proposed in DeRose et al 1990 and its subsequent refinements such as in Renear et al 1996. Caton 1999 discusses the relation between the TEI element <text> and the concept of \"a text\". Caton and the INKE Research Group 2010 argues for greater precision in the use of core digital humanities terms such as \"text\".In this preliminary investigation I approach from the outside in. I take a number of marginal cases and of each one ask \"is this a text?\" - because any attempt to answer that question must draw out the assumptions that underlie our common usages. I focus on the count noun sense because discrete entities with boundaries ought to be easier to recognize. In our professional lives we talk about \"texts\" all the time: oughtn't we to know one when we see one?The marginal cases I discuss are an encrypted message (Figure 1), a sigilSigils in general are magical symbols, and as used by Austin Osman Spare \"are developed by fusion and stylization of letters\" (Frater, U. D. 1991, 7). The letters come from a sentence that expresses a particular desire of the magical practitioner. created by English occultist Austin Osman Spare (Figure 2), a minimal unit (Figure 3), and a poster with quoted words on it (Figure 4).  Figure 1 - encrypted messageFigure 1 is adapted from http://en.wikipedia.org/wiki/Caesar_cipher. Retrieved 28/10/2010. Full Size Image     Figure 2 - creation of a sigil from a message stringFigure 2 is from Spare 1913, page 50. Full Size Image     Figure 3 - minimal unit. Full Size Image     Figure 4 - Second World War posterFigure 4 is from http://en.wikipedia.org/wiki/Never_was_so_much_owed_by_so_many_to_so_few. Retrieved 28/10/2010. Full Size Image   I suggest the following are core assumptions underlying our collective use of \"a text\":representation of language: for any non-metaphorical use we think that language must be involved. Unlike a painting or a piece of music, which seem to affect us unmediated by language, for something to be \"a text\" it must resolve to language in our heads, even if what we see does not directly represent language. Thus we see Milton Glaser's famous logo \"I [heart symbol] NY\" and in our heads hear the words \"I love New York\", because the particular symbol-word association is so common that it resolves almost by default - especially given the linguistic context in which the heart symbol occurs. The encrypted message in Figure 1 is all linguistic symbols, but not directly interpretable as any language. Resolution to language depends upon knowing how to decipher the symbol sequence - though I suggest that as creatures of language even if we do not know the cipher (and so the sequence remains impenetrable to us) we think it likely that the sequence we see is a reversible transformation of a comprehensible linguistic sequence. We accord it an honorary status as \"a text\" whose lack of recognition is due to our ignorance, and not to its being something other than \"a text\". But Figure 2 is a different matter. Unlike the product of the cipher transformation - an incomprehensible string of what are recognisably linguistic symbols - sigilization transforms a comprehensible sequence of linguistic symbols into an almost purely graphic image. I suggest that seeing the final sigil without knowing its origin, we would not even accord it honorary status as \"a text\", because the deletions, substitutions, and spatial reconfigurations make it almost impossible to resolve back into language - there are so few clues that it started out as language in the first place. On the other hand, the linguistic message has not been replaced by a figurative image, in the way that a photograph of an emaciated child might replace the symbol sequence \"children are starving\" - indeed mimetic representation of the desire conveyed by the communication would not be to the purpose, \"[t]he idea being,\" writes Spare, \"to obtain a simple form which can be easily visualised at will, and has not too much pictorial relation to the desire.\" (50) For the person who creates the sigil the message is still completely present, implying that for them at least the sigil is \"a text\".communication: in the normative case for \"a text\" we assume that a linguistic symbol sequence has been created to communicate, which is the primary function of such sequences. We assume the sequence forms a message (or, in the case of a fragment, would form a message if the entire sequence were present). We have such a propensity to find a message, to make sense out of a sequence, that we will try to establish \"a text\" even in the least promising cases. Because the glyph shown in Figure 3 represents a character that (in addition to being a letter) is also a lexical item in English it triggers that response, but because the lexical item is supposed to function as a determiner yet here determines nothing it gives us no semantic purchase. Compare this to Figure 3b:  Figure 3b Full Size Image   This is another glyph that represents a character that is both letter and lexical item, and here in majuscule form as proper to the lexical item. Because pronouns carry more semantics than indefinite articles it gives us more 'traction'; I suggest that we would rank Figure 3b as closer to being \"a text\" than Figure 3, even if we wouldn't commit to saying that it is \"a text\".completeness: the completeness of the message embedded in the symbol sequence depends entirely upon the context. What is merely part of \"a text\" in one context can stand alone in another context, as the excerpt from Churchill's speech does in Figure 4. If we say that the poster in Figure 4 contains \"a text\", though, does that text contain the words \"The Prime Minister\" - or are we looking at two texts, one (just the quote) embedded in another (the whole linguistic symbol sequence)? When we hold a paperback book - an edition of Moby Dick, for example - how many texts are represented in that physical object?There is a type of thing called \"text\" which is a symbol or sequence of symbols that either directly represents language or can be resolved back into language by reversing an earlier, non-arbitrary transformation. In this mass noun sense, text exists, is independent of context, and independent of individual interpretation or experience. However, while text in the mass noun sense must be what makes up text in a count noun sense, there is no such thing as \"a text\" that is independent of context or of individual experience and interpretation. No linguistic symbol sequence is naturally \"organic\" or \"unitary\" (the adjectives used by the TEI Guidelines), though any complex sequence will have structural features that offer themselves as convenient boundaries. Nevertheless these boundaries are always artificial, as much recent work on genetic editions has shown.Particularly interesting examples are the work of Malte Rehbein on a medieval German town record book (2009), and of Justin Tonra on Thomas Moore's long poem \"Lalla Rookh\" (2009). In each case the multiplicity of symbol sequences that are candidates for being \"a text\" is striking. Being \"a text\" is a status we give some text in a particular context and at our choosing. In this sense \"a text\" is, as Renear and Dubin say in a somewhat similar context, \"a matter of contingent social/linguistic circumstances\" (2007 p.8) and is thus - as they similarly concluded about three of the four FRBR Group 1 entity types - not a type but a role. In other words I suggest that being \"a text\" is not what Guarino and Welty would term a rigid property of any instance of text in its mass noun sense.Guarino and Welty define a rigid property as \"a property that is essential to all its instances\" where by \"essential\" they follow Lowe in saying that \"an essential property of an object … [is one where] the object has that property always and in every possible world\" (2001 p.57). An example of a rigid property that they use several times is PERSON: \"if x is an instance of PERSON, it must be an instance of PERSON in every possible world\" (2001 p.57). They contrast PERSON with STUDENT; STUDENT is a property an entity can have and then not have without the entity changing: the same is not true of PERSON. A good deal of ontological work needs to be done, however, before this can be asserted with confidence. ", "article_title": "On the Meaning of the Term 'text' in Digital Humanities", "authors": [{"given": "Paul", "family": "Caton", "affiliation": [{"original_name": "Centre for Computing in the Humanities, King's College London", "normalized_name": "King's College London", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/0220mzb33", "GRID": "grid.13097.3c"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Digitization efforts by many museums, libraries and other institutions, and the massive growth of both user-generated and professional digital content opens us new possibilities for the study of media and visual cultures. To explore these possibilities, in 2007 we set Software Studies Initiative at University of California, San Diego. Since 2009, our research has been focused on exploring manga culture — specifically, 883 manga titles comprising 1 million digitized pages that were available as scanlations on onemanga.com in the Fall 2009. We have downloaded this whole image set along with the user-assigned tags indicating the genres and intended audiences for these titles and begun its analysis using digital image analysis, visualization, statistics, and data mining techniques.Our paper presents our methods and some of the key findings of this ongoing research. The discussion is framed by three larger issues: 1) What are the new possibilities for describing visual language of manga made possible by using digital image analysis? 2) What do we learn by combining the software-assisted analysis of individual manga pages and the analysis of user-assigned tags? 3) What does the analysis of large cultural data sets such as 1 million manga pages tells us about cultural categories such as style and genre?As an example of (1), we present 2D visualizations that show sets of manga pages organized by their visual characteristics measured by software (for instance, the presence of texture/detail and the amount of black in a page). Such visualizations allow us to compare visual language of individual titles, groups of titles, all titles by a particular artist, or 1 million manga pages and large sets of other kinds of images. As another example, we show visualizations that show how visual language of titles changes during the duration of their publication.To research the relations between visual language, genre and gender/age categories in manga market as manifested in our sample (i.e. issue 2), we analyzed connections between 35 user-assigned tags (4 for audience type — shoujo, shounen, josei and seinen — and 31 for genres) available for 883 titles. We found that all tags form a connected network — i.e. any two tags occur together at least once. While some genre tags are more likely to be assigned to a particular audience segment, none of these genre tags are exclusive to male or female audiences. These empirical findings support two ideas. First, rather than thinking of \"action,\" \"adventure,\" \"romance,\" or any other genre category as a distinct genre, we should instead understand them as genre traits that, according to the perception of manga fans, can be combined in a single title. Second, as constructed by these genre traits, the gender categories female / male strongly overlap.Digital image analysis and visualization of manga pages supports the similar conclusions. If we organize 1 million manga pages by their visual characteristics and then select all pages corresponding to shoujo and shounen titles, we find that these two sets of pages form overlapping fuzzy \"clouds.\" In other words, while large proportions of pages have distinct visual characteristics that identify them as belonging to shoujo or shounen category, a significant percentage of pages have an \"androgynous\" visual language.Finally, we discuss how the analysis of 1 million manga pages leads us to to rethink the concept of style. Consider visualization which shows ourcomplete set of 1 million pages. The pages in the bottom part of the visualization are the most graphic (they have the least amount of detail). The pages in the upper right have lots of detail and texture. The pages with the highest contrast are on the right, while pages with the least contrast are on the left. In between these four extremes, we find every possible stylistic variation. This suggests that manga visual language should be understood as a continuous variable.This, in turn, suggests that the very concept of style as it is normally used maybe problematic then we consider large cultural data sets. The concept assumes that we can partition a set of works into a small number of discrete categories. However, if we find a very large set of variations with very small differences between them (such as in this case), it is no longer possible to use this model. Instead, it is more appropriate to use visualizations and/or mathematical models to describe the space of possible and realized variations.", "article_title": "Digital Image Analysis and Interactive Visualization of 1000000 Manga Pages", "authors": [{"given": "Lev", "family": "Manovich", "affiliation": [{"original_name": "University of California, San Diego", "normalized_name": "California Coast University", "country": "United States", "identifiers": {"ror": "https://ror.org/05t99sp05", "GRID": "grid.468726.9"}}]}, {"given": "William", "family": "Huber", "affiliation": [{"original_name": "University of California, San Diego", "normalized_name": "California Coast University", "country": "United States", "identifiers": {"ror": "https://ror.org/05t99sp05", "GRID": "grid.468726.9"}}]}, {"given": "Jeremy", "family": "Douglass", "affiliation": [{"original_name": "Software Studies Initiative, Calit2", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "The challenge of how to handle glyph variants when encoding text has long been a dilemma for those working with historical text materials. How can a digital humanist specify a particular glyph of a Unicode character, even if the glyph might be known to be an error? Is it possible to search for the character, and find instances of the “error” glyphs? This short paper addresses issues involving glyph variants, in light of recent developments within the world of Unicode and W3C standardization, as well as OpenType specifications (also an ISO standard). The different options for handling glyph variants will also be explored in view of sustainability, and viewed from the general perspective of the Unicode character encoding standard, with particular discussion of Unicode variation sequences. One option available to text encoders is to use the “gaiji” module, a mark-up mechanism described in TEI P5 which offers a means to represent and distinguish specific characters and glyphs that the Unicode Standard considers as identical (TEI Consortium, 2010). Another possibility is to request users view the text with a particular font, one that contains the appropriate shape of the glyph(s). However, this is dependent upon the user having a particular font installed. Two recent developments affect this option:OpenType also includes a way to specify specific glyph shapes that are commonly used for certain language-specific letters. For example, there are specific forms of italic and cursive Serbian letters that differ from Russian, although they are the same Unicode characters. The OpenType “language system” table and “locl” (localized form) feature table are mechanisms that allow one to specify such variant glyphs. These features are activated by language tags (Microsoft Typography, 2008b). However, as noted above, OpenType support – while becoming more common - is still limited to certain applications, although it is an international standard (ISO/IEC 14496-22:2009 [OFF] [ISO, 2011]).One option occasionally mentioned as a way to represent a particular variant in a standardized way is to propose the variant as a separate character in Unicode. Technically, this is not allowed, since one of the core design principles is: “Unicode encodes characters, not glyphs” (Unicode Consortium, 2011a). However, some variants have been included in Unicode if they were present in earlier standards. The character/glyph model in CJK is particularly murky, in part due to the sheer number of characters involved (approximately 75,616 characters or 69% of all graphic characters in Unicode 6.0 are CJK). For the historic East Asian character sets, such as Classical Yi, the writing systems may be poorly understood and there is a tendency to encode glyphs. As a result, some character proposals have been based on glyphs (cf. the Classical Yi proposal, which proposed 88,613 “characters” [China, 2007]). Despite this, requesting the encoding of glyph variants into Unicode (as separate characters) is not generally advisable. A last option is to specify a Unicode variation sequence which is defined as a base character and a variation selector (Unicode Consortium, 2011b). This is a standardized means to indicate the glyphic variants of the base character. The advantage to this mechanism is that the variation is accessible in plain text, and does not rely on code points in the Private Use Area, which are not interoperable. This particular mechanism has not yet been widely publicized amongst in the world of digital humanities. It will likely become more widely supported in software, particularly as the Japanese government will be using variation sequences to handle rare ideographs used in proper names and place names, rather than proposing 2,621 new “compatibility” characters (Japan , 2009). In 2010, the Japan National Body put forward a large collection of ideographic variation sequences, which have been under review (Unicode Consortium, 2010a). Variation selectors have been mentioned as a way to handle variants for several historic scripts, namely Tangut and Manichaean. For Tangut, a historic script used in China until the 16c, the variation sequences were suggested as a way to handle cases where the lexical sources don’t agree (that is, there is disagreement whether a given glyph is a variant of a character or is a separate character), as a way to document when different scholarly opinions on unifications, and to address backwards compatibility issues (Cook and Lunde, 2008). In Manichaean, the variation selectors are mentioned as a way to indicate alternate forms which are not predictable, either by their position in a word, or in a line. The use of the variation sequences maintains the basic character identity (Everson et al., 2009). Figure 1 is an example showing the proposed shape for the Manichaean HE glyph, and the HE with Variation Selector-1. (See Figure 1) One drawback is that the variation sequences need to be proposed and approved by the Unicode Consortium, much as new characters are (or, for ideographic sequences, are reviewed as part of the Unicode Public Review Process). However, this hurdle will ensure the characters are standardized, and are publicly accessible (Unicode Editorial Committee Members, 2011; Unicode Consortium, 2010b). Another benefit is that search queries can ignore the variation selectors or the query can be written to only match a term with a specific variation selector. This mechanism could be useful as a way to display glyph errors, and be able to relate them to the base character. However, if a given application does not support variation sequences, the base character will display by default. Variation sequences provide a standards-based option, which has some advantages over font-based alternatives. However, to date, relatively few variation sequences have been defined, except for those used in mathematics, Mongolian, and the historic script Phags-Pa (Unicode Editorial Committee Members, 2011). At present, Ideographic Variation Sequences are only supported in the certain environments (Acrobat/Reader 9.0 and higher, Flash Player 10 and higher, InDesign CS4 and higher, Mac OS X 10.6 and higher, Windows 7 and higher, and Firefox 4 on all platforms [Lunde, 2011]). The dependency on limited implementations can pose a problem for digital humanists, however, if future software fails to support these variation sequences. In sum, several alternatives are available to text encoders to specify variant glyphs in text at present. This paper has provided new information on different options, which are still developing and may become more widely adopted, affecting choices available to text encoders. This author cautiously recommends the use of Variation Selectors if the glyph difference needs to be captured in plain-text, and the digital encoder is willing to go through the approval process to get the variation sequence approved by the standards committees. This work was supported by the National Endowment for the Humanities as part of the Universal Scripts Project [#PW-50441]. ", "article_title": "Handling Glyph Variants: Issues and Developments", "authors": [{"given": "Deborah", "family": "Anderson", "affiliation": [{"original_name": "Department of Linguistics, UC Berkeley", "normalized_name": "University of California, Berkeley", "country": "United States", "identifiers": {"ror": "https://ror.org/01an7q238", "GRID": "grid.47840.3f"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "In the past 150 years, the discipline of archaeology has changed dramatically; excavation procedures, field methods, and record keeping have both improved and become formalized. Ahead of the digital era, the physical records of an excavation (the papers, data tables, journals, and monographs) were preserved as artifacts alongside the excavated materials in museums and repositories. More recently, Archaeologists have been quick to adopt new technology from punch cards in the 1960s to spreadsheets, databases, GIS, and 3D scanning. Yet, these modern files, images, data sets, and documents, if not properly preserved, are more fragile than the objects they describe. With the Digital Archaeological Record (tDAR --http://www.tdar.org), we hope to change this.tDAR was designed as a domain-specific digital repository, focused on preservation of, and access to archaeological documents, reports, data sets and images. The most successful digital repositories provide additional value to their users beyond the core mission of preservation. Examples including ArXiv (http://arxiv.org/) or the University of Rochester's digital repository (https://urresearch.rochester.edu) are successful because of additional factors such as reputation or community. For tDAR, the additional value is created through research tools developed on top of the repository. These tools aim to promote new synthetic and comparative research using the data sets stored within the repository.tDAR's architecture includes three architectural components, a backend preservation repository based on the California Digital Library's Micro-Services model, an interactive web interface, and a research platform. Metadata is stored within tDAR using a extension of the Library of Congress MODS schema, modified to add archaeologically significant metadata. This includes descriptive metadata about the site, location, culture, materials found, among other attributes. Data sets ingested into tDAR function differently from data sets in a traditional repository. Once a data set has been uploaded, users are guided through the process of documenting their data set within the system. This process is designed to focus on identifying non-machine discernible information such as, whether numeric data represents a measurement or count and translating coded values or lookup tables into human-readable values. Once complete, tDAR's additional features provide unique opportunities to compare, contrast, and analyze data within the system. A significant challenge for many disciplines is the ability to perform synthetic research. Data from archaeological excavations commonly include a mixture of standardized observational data such as Munsell codes to record sediment color and GPS/GIS readings are combined with more qualitative assessments about artifact types, or the amount of \"burning\" on faunal elements. Within the context of a specific site, this is easily reconcilable --as the team develops a common understanding of these terms. However, utilizing these classifications outside of a given site, region, or community of archaeologists, can be challenging. Certain data may lend itself to the application of universal classification schemes --including data that is either more scientific or is derived from a well-documented period. However, more qualitative data may not be as easily mapped to a universal classification model --as definitions of terms will vary between archaeologists or over time. Instead, contributors may provide, or develop a unique classification scheme (ontology) to describe their data. These two approaches represent well-tread road within both research and practice, with distinct benefits to each side. However, to perform any useful comparison, a mapping must be developed.tDAR does not force users to map data to universal data models or classification schema. Instead, the application has developed a different approach --maintain the data in its original, and capturing the intent of the archaeologist. The application was been developed with reference ontologies available for certain data elements including faunal species data among others. tDAR enables users to create additional ontologies within the repository, or upload existing one using the OWL format. Once a column of data is associated with an ontology, users are presented with straightforward tools to map the unique data values to terms within the ontology. We believe that this process serves a number of purposes, not only does it maintain and represent the data as it was collected, but it provides opportunities for collaboration and communication within the discipline as archaeologists share data, and discuss intents.Once data has been mapped, the application guides the user through the data integration process of selecting data sets, identifying columns to compare, fine tuning any mapping issues, and producing the new combined data set. In an analog context, or outside of tDAR, this process can be time-consuming for one data set, and overwhelming for multiple. Within tDAR, what would have been a complex process taking days or weeks when performed manually becomes much more fluid, taking hours. With the technology performing much of the heavy lifting, it leaves the archaeologist to focus on the specific questions and details of their research. While tDAR is still developing, tDAR's data integration has already enabled Archaeozoologists to ask novel questions about the cultural and ecological circumstances under which species are overhunted or subsistence strategies change. It is our hope that tDAR's core values of access, preservation, and integration will enable us to ask, understand, and evaluate new questions and ideas otherwise impossible within the field of archaeology.", "article_title": "The Digital Archaeological Record--an Analytic Data Repository for Archaeology", "authors": [{"given": "Adam", "family": "Brin", "affiliation": [{"original_name": "Digital Antiquity", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Francis", "family": "McManamon", "affiliation": [{"original_name": "Digital Antiquity", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Allen", "family": "Lee", "affiliation": [{"original_name": "Arizona State University", "normalized_name": "Arizona State University", "country": "United States", "identifiers": {"ror": "https://ror.org/03efmqc40", "GRID": "grid.215654.1"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Why do novelistic genres end? Why do we see gothic and industrial novels, Bildungsromane and mysteries, all disappear after periods of popularity during the 18th and 19th centuries? How literary forms—novelistic genres in particular—come and cease to be has long been an area of inquiry, and the work of Franco Moretti (2005) in Graphs, Maps, Trees has given the topic new energy.The years since the publication of “Graphs” in 2003 have seen the resources available for investigating patterns in 19th century literary production expand immeasurably. For example, scans of over 7,800 volumes of 19th century British novels are now available from the University of Illinois-Urbana-Champaign’s collection alone. Those interested in extending or challenging Moretti’s observation that novelistic genres tend to arrive in “bursts” linked to social generations—or, in general, in contributing to Moretti’s proposed “sociology of literary form”—now have access to a wealth of new data. Given the continuing interest that Moretti’s work has generated among students of the human, social, and natural sciences, this represents an important opportunity.My contribution explores further the prospects for a “demography of literary forms,” building on Moretti’s proposal for research at the intersection of literary history and sociology.I owe the suggestion for a demographic approach to Shalizi (2006)’s review of Graphs, Maps, Tress: “There is a demography of businesses, of interest groups, even of medieval manuscripts of classical works, and so why not one of literary texts?” First, I consider opportunities to improve Moretti’s original “generational model” of cycles in literary forms, offering new methods to remedy identified evidential gaps. For example, Moretti’s periodizations of genres—e.g. Courtship Novel, 1740-1820—have been criticized as too neatly falling on certain “focal dates” such as years falling at the end of a decade (Shalizi, 2006). I present new evidence in support of this criticism and demonstrate a method for making periodizations reproducible by others. Being able to reproduce Moretti’s results will hopefully make the research program itself more open to experimentation and invite collaboration. My method uses bibliographic databases to establish the period during which the vast majority (~90%) of the novels in a given genre were published. This provides a reproducible periodization suited to an inquiry into social history.The bibliographic databases used are Garside’s British Fiction Database, 1800-1829 with 2,272 titles and Bassett’s At the Circulating Library, 1837-1901 with 7335 titles I also briefly discuss the application of classification algorithms from machine learning to identify possibly overlooked genres in Moretti’s dataset. The new method for periodization is demonstrated in detail for two of the forty-four genres in Moretti’s dataset, the silver fork and Newgate novels.The methods presented are designed to accommodate changes to the underlying dataset and can be used by others who may disagree with classifications of particular novels and even with the bibliographic records provided by Garside and Bassett.Second, in order to offer an alternative to the generational model, I argue for and attempt to test the hypothesis that the observed clustering of genre appearances and disappearances can also be explained by positing a “carrying capacity,” an upper limit on the number of established novelistic genres able to be supported by writers, readers, and publishers in any given year.Finally, I explore the suggestion that generational changes in “mental climate” might manifest themselves not (only) in changes in novelistic genres, as suggested by Moretti, but rather in topical changes within novelistic genres. In studying a topic model (Steyvers et al., 2007) of the 7,800 volumes in UIUC’s 19th century novels collection, I observe some evidence for topical trends cutting across multiple genres. For example, starting in the mid-19th century there is a proportional rise in a cluster of words suggestive of farming and rural life.Moretti’s work has been a touchstone for numerous discussions connected to the digital humanities—for example, the fate of “close reading”. Perhaps more significant in the long-run is the interest his work has gained from scholars in the social and natural sciences, with examples ranging from formal reviews like that of statistician Cosma Shalizi (2006) to more informal commentary on quantitative literary history from social scientists like Henry Farrell (2010) and Andrew Gelman (2010). Finding scholars outside of literary studies and literary history publicly engaging with research is an important development. Given the desirability of new models of research in the humanities, Moretti’s program can claim a interdisciplinary following that makes revisiting and extending his work of particular significance.", "article_title": "Toward a Demography of Literary Forms: Building on Moretti's Graphs", "authors": [{"given": "Allen B.", "family": "Riddell", "affiliation": [{"original_name": "Duke University", "normalized_name": "Duke University", "country": "United States", "identifiers": {"ror": "https://ror.org/00py81415", "GRID": "grid.26009.3d"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Civil War Washington (CWW) is a thematic research collection that strives to enable users to visualize, analyze and interpret the physical, social, cultural, and political transformation of Washington, D.C. The development of Washington, D.C., during the Civil War is pivotal in American history. When the Compensated Emancipation Act went into effect on April 16, 1862, Washington became the first emancipated city—and the country's largest and most important magnet for freed and runaway slaves. From that moment forward, the city would lead the nation in the sometimes tortuous route from slavery to freedom and from an entrenched system of legal inequality to a new commitment to equality for all. Our work on slavery, race, and emancipation in Washington, D.C., is crucial to our larger long-term study of the city in this time of crisis. We are already studying Civil War Washington from a medical perspective (the number of hospitals jumped from three to nearly one hundred making it a city of hospitals), from a military perspective (the city was the prized objective of Southern military strategy and in response the Union army made it the most fortified city in the world), and in fact from numerous other perspectives as well. With the assistance of a collaborative research grant from the National Endowment for the Humanities, our emphasis in 2010-2013 is to study the history of race, slavery, and emancipation in the city, a story of national importance. The transformation of the U.S. capital has received surprisingly little sustained examination. One reason for this, no doubt, is that developing a rich and accurate understanding of the city's remaking requires not only access to but also synthesis and analysis of large and diverse sets of data, most of which exist only in analogue form. Our project, for example, draws on government reports, journalism, legal documents, diaries, census records, correspondence, city directories, poems, maps, and photographs. A further (and we believe necessary) complication is added by our desire to make a temporally-aware and user-manipulable GIS a significant constituent of our project, perhaps in some ways even its core.  The number of projects, both emergent and established, that claim an interest in GIS and place-based scholarship seems to grow daily. This developing interest is reflected in an increasing number of seminars and training opportunities focused on GIS for the humanities, including at the University of Victoria's Digital Humanities Summer Institute, the Digital Humanities Observatory Summer School in Dublin, and the NEH-funded Geospatial Institute hosted by Scholars' Lab at the University of Virginia. In addition, there is a growing body of work on the value of spatial analysis in the humanities, and historians have taken a leading role in theorizing and conceptualizing the integration of GIS into humanities research, as well as in advancing arguments informed by GIS-enabled research.  In recent years, at conferences such as this, a fair number of projects have been presented as models for addressing the still-daunting set of obstacles to the use of GIS for humanities projects.Martyn Jessop, in his talk At the first Digital Humanities conference in 2006, “The Inhibition of Geographical Information in Digital Humanities Scholarship,\" presented a list of barriers to the application of GIS to humanities projects. This talk, later published in Literary and Linguistic Computing (April 2008), is largely still relevant. In addition, see also the report of the 7th Scholarly Communication Institute at the University of Virginia, \"Spatial Technologies and the Humanities\": http://www.uvasci.org/wp-content/uploads/2009/10/sci7-published-full1.pdf Our experience on Civil War Washington for the past five years, though, suggests that several important challenges have not yet been adequately investigated (let alone addressed). Our talk, then, is designed to be not a celebration of goals achieved but a case study for the consideration of several large issues that face our project and others like it.) Whereas most projects presented as models began with fairly well defined sets of data,For example, see the following DH2010 abstracts: Ian Gregory, \"GIS, Texts and Images: New Approaches to Landscape Appreciation in the Lake District,\" Elton Barker, et al., \"Mapping the World of an Ancient Greek Historian: The HESTIA Project,\" and \" Wayne Graham, \"A New Spatial Analysis of the Early Chesapeake Architecture,\" available at http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/ ours began with a research question to which we want to apply as comprehensive a variety of data as possible. Our goal is not the digitization of materials for the sake of digitization but the exploration of a complex set of questions about the transformation of Washington, D.C. Further, our goal ultimately is to produce both a scholarly argument of our own and a resource that will enable our users to perform original, and truly meaningful, research based on our data and interfaces. Given these aims, on what basis should one decide the best technologies and methods for data capture, storage, and retrieval? We have responded to these questions based in part on the expertise of project members and the technologies that are familiar to us and local support staff. This strategy has the benefit of quicker development and an ever-more-refined knowledge in specific tools and technologies. Too heavy a reliance on the technologies and methods that one already knows, however, can lead to overlooking a more appropriate approach or set of approaches. 2) Given the fact that capture and storage of geo-referenced textual data can be accomplished in several different ways (e.g., through wholly TEI-XML, through assigning atomistic textual units to database fields, or through a combination strategy that uses both XML and database), what principles should guide the adoption of a particular encoding strategy? What tools exist or are most easily imaginable for making such data available for a wide variety of research approaches? TEI began providing for the georeferencing of textual data only with the release of P5 in late 2007, and we believe that the integration of textual and geospatial data remains an underdeveloped area to which the DH community should give greater thought. Surely it has potential beyond its rather modest application in existing projects. 3) How should a project deal with the catch-22 situation of wanting to develop and enable geospatial and historical analysis using open source tools even as the most adequate tools for the task, regrettably, are proprietary and the use of these proprietary tools is taught in leading DH institutions? In other environments, work on open source GIS has emerged as part of the larger open source and open access movement. The Open Source Geospatial Foundation, for example, is working to “promote the collaborative development of open geospatial technologies and data.”See http://www.osgeo.org/content/foundation/about.html Humanities scholars, however, do not seem to be involved in the organization. What is and should be the role of humanities scholars in the developer community for GIS software? Does the existing open source GIS software originally conceived of by organizations such as the Army Corps of Engineers (Grass GIS) or NASA (MapServer) meet the needs of humanities scholars? We have considered these questions not merely in the realm of abstract ideals, but as immediate and pressing concerns; we wish to use our project's responses to them not as recommendations but as opportunities for critical reflection and discussion. ", "article_title": "Civil War Washington: An Experiment in Freedom, Integration, and Constraint", "authors": [{"given": "Ken", "family": "Price", "affiliation": [{"original_name": "University of Nebraska-Lincoln", "normalized_name": "University of Nebraska–Lincoln", "country": "United States", "identifiers": {"ror": "https://ror.org/043mer456", "GRID": "grid.24434.35"}}]}, {"given": "Brett", "family": "Barney", "affiliation": [{"original_name": "University of Nebraska-Lincoln", "normalized_name": "University of Nebraska–Lincoln", "country": "United States", "identifiers": {"ror": "https://ror.org/043mer456", "GRID": "grid.24434.35"}}]}, {"given": "Liz", "family": "Lorang", "affiliation": [{"original_name": "University of Nebraska-Lincoln", "normalized_name": "University of Nebraska–Lincoln", "country": "United States", "identifiers": {"ror": "https://ror.org/043mer456", "GRID": "grid.24434.35"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "From teaching students how to vet online websites, to formal peer review of digital publications, to evaluation of scholarship for tenure review, the need for a rigorous methodology to evaluate digital historical representations has never been more apparent. Sophisticated databases, digital libraries and archives, and virtual reconstructions have collectively reshaped engagement with the past that has challenged the boundaries of traditional historical practices. University professors encourage students to create mash-ups of historical multimedia content to be posted as YouTube documentaries. Museums employ mobile applications enriched with augmented reality to draw visitors further into an exhibition. Geospatial visualizations and virtual architectural reconstructions bring the past alive and challenge entrenched scholarly and popular perceptions.While attention and resources have (justifiably) been focused on content creation and tool development, the digital history community has, until recently, neglected the development of a methodology that can evaluate digital work while meeting the needs of this shifting landscape. This has begun to change, as leading scholars in digital history have taken up the clarion call for reform of the peer review process. The perception is that peer review has become outdated and stagnant, promoting conservative scholarship while also failing to exploit more dynamic means of communication and commentary through social media and Web 2.0 technologies. Robert B. Townsend, in a American Historical Association (AHA) blog posting proclaims, “The challenge lies in developing new forms of peer review better fitted to the online environment, both before publication (in the development and assessment stage) and after publication (as a means of validating the value and quality of the work).” Robert B. Townsend. “Assessing the Future of Peer Review.” AHA Today. 7 June 2010. http://blog.historians.org/profession/1065/assessing-the-future-of-peer-review. The absence of peer review standards was also captured in a recent article for The Chronicle of Higher Education and online commentary. Jennifer Howard, \"Hot Type: No Reviews of Digital Scholarship = No Respect,\" The Chronicle of Higher Education (2010), http://chronicle.com/article/Hot-Type-No-Reviews-of/65644/, accessed 31 October 2010. See also Dan Cohen. “Peer Review and the Most Influential Publications.” 19 October 2010. http://www.dancohen.org/. For a more comprehensive discussion of digital peer review, albeit slightly outdated in its conclusions, see Digital Scholarship in the Tenure, Promotion, and Review Process, ed. Deborah Lines Andersen. Armonk: M.E. Sharpe, 2004. While Townsend and others have highlighted the ills of the current peer review system, they have yet to propose a set of review guidelines or methodology that would bring together the interests and special knowledge of multiple disciplines. In my DH2010 presentation, “Thinking Archivally: Selection, Search, and Reliability as a Framework for a New Digital Historiography,” I proposed a framework for evaluating digital historical representations called digital historiography.The abstract for the DH2010 presentation may be found online in the conference program. Digital Humanities 2010: Conference Abstracts. Eds. The Alliance of Digital Humanities Organisations, The Association for Literary and Linguistic Computing, The Association for Computers and the Humanities, and The Society for Digital Humanities. London, 7-10 July 2010. http://dh2010.cch.kcl.ac.uk/academic-programme/abstracts/papers/pdf/ab-747.pdf. Accessed 13 March 2011. An expanded version of the paper will appear in a forthcoming issue of American Archivist. Digital historiography is the interdisciplinary assessment of digital historical representations across diverse formats. It promotes the rigorous and coherent use, recombination, and presentation of historical information through digital technologies. Digital historiography also accounts for the increased reliance on complex information systems to organize and represent historical data. The merging of historiography with archival theory and new media studies – along with numerous other fields such as museum studies, information science, and other humanities disciplines – ensures a comprehensive examination of a representation.For a history and examination of the contemporary “divide” between the historical and archival professions, see Francis X. Blouin, Jr. and William Rosenberg. Processing the Past: Contesting Authorities in History and the Archives. Oxford University Press (2011).Three fundamental areas of archival theory were raised in last year’s presentation as fundamental to unlocking the trustworthiness and soundness of a representation: content selection, search functionality, and metadata. Through a series of illustrative examples of historical representations it was shown that these areas convey a representation’s historical contextualization, where context is defined as the formal and humanistic relationships among data and resources. For an introduction to the relationship of archival theory and context, see for example, Jennifer Meehan, \"Towards an Archival Concept of Evidence,\" Archivaria 61(2006), 143. The discussion of archival context has been subsumed in a larger, more general area referred to as “information as evidence.” For further discussion of this concept, see Terry Cook, \"Archival Science and Postmodernism: New Formulations for Old Concepts,\" Archival Science 1(2001); Terry Cook, \"What Is Past Is Prologue: A History of Archival Ideas since 1898, and the Future Paradigm Shift,\" Archivaria 43(1997); Margaret Hedstrom, \"Archives, Memory, and Interfaces with the Past,\" Archival Science 2, no. 1-2 (2002); Joanna Sassoon, \"Beyond Chip Monks and Paper Tigers: Towards a New Culture of Archival Format Specialists,\" Archival Science 7, no. 2 (2007); Jennifer Meehan, \"Making the Leap from Parts to Whole: Evidence and Inference in Archival Arrangement and Description,\" American Archivist 72, no. 1 (2009).Whereas last year’s paper focused on the theoretical underpinning of digital historiography, this year’s presentation will provide a programmatic framework that scholars, archivists, librarians, curators, editors and technical specialists may adapt and apply to their own work. There are already a number of promising developments underway in establishing a set of guidelines and practices for digital humanities evaluation, particularly in literature; nonetheless, a similar approach in digital history has been lacking.Perhaps the most comprehensive guide to evaluating digital scholarship can be found with the MLA Guidelines for Editors of Scholarly Editions. Section V is devoted to electronic scholarly editions, and prompts reviewers to consider elements of a digital work such as TEI encoding and the user interface. An NEH-funded Summer Institute entitled “Evaluating Digital Scholarship” will be hosted by NINES at the University of Virginia 30 May – 3 June 2011, which will expand upon a one-day workshop held at Digital Humanities 2010 in London.This paper will provide a guide to evaluating digital historical representations using a series of exploratory questions. The following questions may provide an entry point with which to kick-start an analysis. It should be noted that these questions do not evoke clear-cut responses, but rather require deeper interpretation of the representation:  Is the representation’s historical content comprehensive or representative of the period/event/issue in question? How do metadata schema and other descriptive information shape the interpretative possibilities of the representation? What capacity does the user have to repurpose historical data for additional study? To what extent does a user have sufficient contextual information such as the content’s provenance to conduct such repurposing? How does the user search or navigate within the representation? In what ways does the interface facilitate or prohibit advanced humanistic inquiry? These questions apply a combination of historical and archival understanding to address a representation’s approach to selection, search, and metadata. At a more general level, they suggest the need within digital historiography to develop a peer review methodology that examines the intersection of technological and humanistic components of a representation.This paper will propose, based on the sample questions above, three interlocking axes for a peer review methodology: Historical Content, User Experience, and Creator Intent. Critiques of digital representations have all too often isolated one axis at the expense of considering the others. Scientifically driven user or human-computer interaction studies may overlook the unquantifiable nuances of historical contextualization. Similarly, an analysis of content often neglects a representation’s user interface and how it may affect information access. Perhaps the most disregarded of the three areas is the creator’s intent for constructing a digital historical representation. In the fierce competition for funding in the digital humanities, developers must be able to justify their resource-intensive project, which raises basic questions surrounding the representation’s purpose and its contribution to historical scholarship or programming.For the purpose of demonstrating how to apply digital historiography to peer review, a single digital historical representation will be selected and the audience will be guided through a brief, yet systematic evaluation. In terms of historical content, we will consider the representation’s engagement with relevant scholarship. Although a digital representation’s format may seem to belie traditional modes of historiography, it will be argued that a representation’s demonstrated recognition of historiographic trends is essential for establishing whether it advances new areas of historical understanding. Unlike a text-based monograph, analysis of content depends on the capacity of the user to generate, not just consume, plausible knowledge; therefore, peer review may focus more on the questions that may be posed through a representation rather than the construction of a single argument.In terms of user experience, this presentation will focus on the question of access to historical content. More than just a scientific optimization of keywords, a reviewer must consider the user experience as a culmination of design features, tool functionality, and search capability. This raises issues regarding the application of standards and best practices. The selection of an appropriate metadata schema or digitization standard will determine a representation’s interoperability with other resources, which will in turn influence historiographic assessment. Finally, consideration of authorial intent, embedded within elements such as a site’s introductory statement or contextual essays, provides a third axis that can anchor both a representation’s content and user experience. The core question to be considered here will be the representation’s intended or anticipated audience. Digital technology has enhanced access to historical knowledge for a wider public, which has placed additional responsibility on the part of the creator to consider how historical information is selected, presented, and most importantly, handled by diverse types of users. A peer review system that follows shared and accepted standards and methodology may have significant benefits for advancing digital historical scholarship and digital humanities infrastructure in general.For further discussion of how the digital humanities must re-consider infrastructural needs, see Christine L. Borgman. “The Digital Future is Now: A Call to Action for the Humanities.” Digital Humanities Quarterly. Fall 2009. Volume 3, Number 4 Representations may be assessed on the merits of content and usability. In terms of scholarship, digital historiography may liberate scholars and developers to consider how to harness technologies in service of historical inquiry when all too often the reverse seems to hold true. The time for experimentation for its own sake has passed, and we should begin to consider how technology might contribute to more sustainable development of innovative modes of scholarship.This presentation is not intended to provide a fixed method for peer review; rather it will encourage a revival of humanities-driven interpretative analysis that addresses central areas of scholarship, audience, argument, and most importantly, inquiry. Peer review must be contingent on the unique set of questions related to a representation’s subject area and formal qualities. Those in attendance will have the opportunity to take away a set of general questions that they may use to devise their own review criteria, or that may stimulate further dialogue about the peer review process. Perhaps most significantly, this presentation will defend the need for collaborative, transparent review that brings together subject and technical specialists, which can spark evaluation of noteworthy digital work that continues to elude mainstream academic recognition.The thoughts and ideas expressed in this paper are entirely my own and do not reflect those of the National Endowment for the Humanities or any other federal agency.", "article_title": "Reforming Digital Historical Peer Review: Guidelines for Applying Digital Historiography to the Evaluative Process", "authors": [{"given": "Joshua", "family": "Sternfeld", "affiliation": [{"original_name": "National Endowment for the Humanities, United States of America", "normalized_name": "National Endowment for the Humanities", "country": "United States", "identifiers": {"ror": "https://ror.org/02vdm1p28", "GRID": "grid.422239.c"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Now almost fifty years old, Walter Houghton’s seminal work, The Victorian Frame of Mind, 1830-1870, has influenced generations of scholars of the nineteenth century and remains the primary introduction to Victorian thought that every student in the field reads. From a close reading of famous Victorian writers such as John Stuart Mill and Thomas Carlyle, Houghton argued that the Victorians were characterized by specific, common personality traits such as optimism, hero worship, and earnestness. Houghton believed that these traits were visible in the rise (or decline) in the use of particular words and phrases, such as an increasing use of “light,” “sunlight,” and “hope” as illustrative of their optimistic world view.Despite the enormous impact of The Victorian Frame of Mind on generations of scholars across the humanities, it has not been accepted uncritically. Many concerns stem from Houghton’s myopic textual methodology: generalizing the character of a people—“the Victorians”—from the words of a select few. Although Houghton cites hundreds of primary sources in his bibliography, his book has been characterized as anecdotal, elite intellectual history. Despite such criticisms, Victorianists have been able neither to thoroughly assess the general validity of Houghton’s theses nor to offer alternatives.New digital tools and the vast digital library of Google Books now allow us to conduct a comprehensive survey of Victorian writing—not just the well-known Mills and Carlyles, but tens of thousands of lesser-known or even forgotten authors—to test whether the Victorians truly did use the kinds of words and phrases that Houghton claimed they did. Did metaphors of hope actually increase in real terms between 1830 and 1870? Or was this only true for the dozen prominent writers he examined for his chapter on optimism? How can we complicate Houghton’s characterizations and understand change over time through the vast index of Google Books? Can we refine the timeline for the emergence of his characteristics, moving beyond the disturbingly neat, rounded-year boundaries he set for his book? How can we correlate historical events with disturbances in the linguistic data? To what extent can we separate cultural history from printing history with a large corpus of digitized literature?Dan Cohen, my colleague at the Center for History and New Media, and I have begun work on a project to answer these very questions. With the help of a Google Digital Humanities Grant, we’re attempting what Franco Moretti calls a “distant reading” of the Victorians. My paper will explain how we’ve gone about querying the data available through Google Books, how we’ve been able to make sense of and interpret the results, and how we’ve dealt with the messiness of the data. I hope to solicit feedback about our methodologies and conclusions as part of a larger discussion about how the Google Books corpus (and similar datasets) could be made more usable for large-scale data mining projects relevant to the diverse research interests of the audience. How far can we push our methodologies beyond testing certain theses in order to allow the texts to speak for themselves?In terms of the Victorians, some preliminary results have proven quite intriguing. For example, the number of books published with “universal” in the title declined steadily throughout the century, but earlier than most interpretations in the secondary literature point out. A look at published titles suggests that the terms “God,” “Christian,” and “Bible” follow rather different contours, though explanations are not immediately apparent. Similarly, how can we explain the striking publication parallels between the terms “belief” and “Aristotle”? The median number of titles that use the word “hope” does not significantly change between 1830 and 1870: does this cast some doubt on Houghton’s characterizations? Or does it simply indicate that book titles are not an accurate gauge of popular sentiment? We have not yet been able to examine the full texts from the publications that are being counted, but we hope to do that soon. To what extent will a more sophisticated linguistic analysis of the full texts reinforce or contradict what the titles alone tell us? Does this have implications for similar large-scale research methodologies?", "article_title": "Moving Beyond Anecdotal History", "authors": [{"given": "Fred", "family": "Gibbs", "affiliation": [{"original_name": "George Mason University", "normalized_name": "George Mason University", "country": "United States", "identifiers": {"ror": "https://ror.org/02jqj7156", "GRID": "grid.22448.38"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "The bulk of the literary materials that survive from the Middle English period are scribal copies, rather than authorial compositions. Such copies pose a challenge to the stylometrist, the reason being that copies written in a single scribal hand may have non-identical orthographic profiles. Their non-identity is a product of their transmission history, as the typical copy will contain both spelling forms originating in the exemplars and other such forms introduced by the scribe. Historical dialectologists have developed methods for separating the mix of scribal usage and exemplar usage typically recorded in a single scribal copy. Although powerful, these methods rely on questionnaire-based interrogation of text samples and subsequent visual analysis of spelling forms arranged in tables. The arrival of digital transcripts has sped up the data collection process and has led to compilation of fuller profiles, but the questionnaire itself has stayed. Thus, these methods fail to take full advantage of the digital medium. This presentation demonstrates that a purpose of identifying and isolating locations in which the makeup of the spelling system changes during the full text of a longer Middle English literary manuscript may be met by probability-based comparison of text samples. What spelling forms happen to be attested in a given text is a function of what words happen to make up that text. The direct comparison of texts is therefore not readily possible. It has typically been made possible by considering profiles recording only the spelling forms of those words which may reasonably be expected to occur in every text, such as function words like \"such\", \"that\", and \"these\". The alternative solution proposed in this paper is to base assessment of similarity on \"models\" of text samples' spelling–exhaustive profiles of which letters and letter sequences occur in them and with which frequency. I shall refer to single letters as unigrams, ordered sequences of two letters as bigrams, etc. Such models are easily compiled from electronic diplomatic transcripts. The dissolution into n-grams is equal to identification of the between texts because comparison of the building blocks is in itself relatively independent of the word level. Similarity is measured between a text sample (the test sample) and a model derived from another text sample (the training sample). It is expressed as the overall probability that the test sample is an instance of the same spelling system as the system modelled. Computing this probability proceeds, with a trigram model, from consideration of each unigram in the context of the bigram preceding it–the reader may correctly recognise the \"Markov assumption\" in this description. What is output, however, is the reciprocal of the average probability per gram. This entity, called \"perplexity\", will conveniently always be a positive number larger than 1 with the present type of data. Moreover, techniques exist for \"smoothing\" a model, that is for reducing its dependence on the words constituting the training sample. This reduction is achieved by statistically manipulating the probabilities computed for the training sample n-grams. Smoothing additionally leads to probability being assigned to spelling forms unattested in the training sample. It is these properties of these techniques that makes their application on n-gram models based on Middle English texts further increase the comparability of those texts. Probabilistic modeling techniques have, however, as far as I am aware, rarely been applied for the stylometric analysis of Middle English materials, and it has yet to be established which specific smoothing technique produces the most satisfactory models of those materials. A simple example may illustrate. The spelling forms <suich, suyche, such> for the word \"such\" are found in text A, while the same word is spelt <suche> in text B. Intuitively, the text B spelling form <suche> falls within the range of variation characteristic of the spelling system of text A but happens to be unattested in it, while other known Middle English forms such as as <swylke, suilk> do not. The present methodology involves dissolving <suche> into <su, suc, uch, che, he> and establishing the smoothed conditional probability for each of these trigram building blocks in text A (thus obtaining a intuition that the spelling form is possible in the spelling system of text A. In practice, however, the quantification is not effected for the individual form but for the whole of text B in relation to text A. To illustrate the adequacy of perplexity-based comparison in stylometry, I trace changes in spelling in a large manuscript collecting several Middle English literary works. The corpus is the Auchinleck manuscript, Edinburgh, National Library of Scotland, Advocates' MS 19.2.1, produced in the London-Westminster area in the first half of the fourteenth century. The potential influences on the scribes include the literary structure, as the codex's total of almost 59,000 lines of text are divided between no less than forty-four literary works representing a range of genres. A map showing locations in which the spelling system changes during the full text of the Auchinleck manuscript may be expected to reflect the literary structure only if the exemplars did so and the Auchinleck scribes reproduced them slavishly. By contrast, it is the boundaries of the scribal contributions that will be visible in the map if each scribe thoroughly converted into his own spelling system when copying. Six scribal hands are present. Of these, Scribe 4's contribution is too short (551 words) to constitute a reliable sample, while the usages of the other five scribes should be visible. They are distinct in terms of their typological classification on the dialect continuum, although they fall into an eastern cluster and a western one. Dialect analysis has thus placed Scribe 1 in Middlesex, Scribe 2 on the Gloucestershire-Worcestershire border, Scribe 3 in London, Scribe 5 in Essex, and Scribe 6 in Worcestershire (McIntosh, Samuels, et al 1986, I: 88; LPs 6510, 6940, 6500, 6350, and 7820). To be able to compute and compare probability for sections of the Auchinleck manuscript against one another, I obtained a digital transcript of its text from the Oxford Text Archive (Burnley and Wiggins 2003). The transcript is suitable, because rather than modernise the spelling forms found in its source, it reproduces the source in conformity with standard practice of diplomatic transcription. My tool for constructing models and computing perplexity is the SRI Language Modeling Toolkit (SRILM; Stolcke 2002); this toolkit is freely available for noncommercial purposes from the website of its SRILM constructed and smoothed an interpolated model for every 200-line section; the smoothing technique selected was that described by Witten and Bell (1991). This technique was developed for purposes of text compression at the level of the word but it is appropriate for application on Middle English spelling data too. The reason is that the technique effectively assigns probability to collocating letters as if they were a single letter, rather than a series of independent units. The toolkit took the same modified transcripts of all the sections as the input and computed their similarity with the models. The computation resulted in a separate model for each section, and for every such model, a separate perplexity for every section. I established the mean perplexity and standard deviation for the perplexities obtained for each model. The box and whisker graph below shows the results. In this graph the vertical axis gives perplexity and the horizontal axis position in the text of the Auchinleck manuscript. The diamond represents mean perplexity and the T-bar represents half a standard deviation, so that one upright T-bar and its reverse together indicate an interval of one standard deviation from the mean. A dashed vertical line appears for ease of reference at the boundary of a scribal stint as established by palaeographers (Bliss 1951), with the outlined numbers identifying the scribes. As is apparent, the figure distinguishes the scribes of the Auchinleck manuscript. The rises and falls in mean perplexity during the text strongly correlate with the boundaries of the scribal stints, while mean perplexity is relatively constant within every such stint. Repetion of the experiment with other divisions of the text produced results sufficiently similar to Figure 1 to establish the pattern as being a property of the data rather than an artefact of the method. It would have been time-consuming indeed to conduct a questionnaire-based interrogation of the full text of the Auchinleck manuscript. Visual analysis of the resulting profile to identify and isolate locations in which the spelling system changes would, moreover, have been complex, because of the amount of data and difficulty of isolating the diagnostic features. Perplexity-based comparison as illustrated above, by contrast, requires little preprocessing of the transcript, is effected in an afternoon, and is based on all the available data.", "article_title": "Probabilistic Analysis of Middle English Orthography: the Auchinleck Manuscript", "authors": [{"given": "Jacob", "family": "Thaisen", "affiliation": [{"original_name": "University of Stavanger", "normalized_name": "University of Stavanger", "country": "Norway", "identifiers": {"ror": "https://ror.org/02qte9q33", "GRID": "grid.18883.3a"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Language resources are the bread and butter of language documentation and linguistic investigation. They include the primary objects of study such as texts and recordings, the outputs of research such as dictionaries and grammars, and the enabling technologies such as software tools and interchange standards. Increasingly, these resources are maintained in digital form and distributed via the web. However, searching on the web for language resources is a hit-and-miss affair. One problem is that many online resources are hidden behind interfaces to databases with the result that only a fraction of these resources are being indexed by search engines (He and others 2007). Even when resources are exposed to online search engines, they may not be discoverable since they are described in ad hoc ways that prevent searches from retrieving the desired results with high recall or precision.This paper describes work being done in the context of the Open Language Archives Community (OLAC) to develop a service that uses text mining methods (Weiss and others 2005) to find language resources located within the hidden web of institutional repositories. It then uses the OLAC infrastructure to expose them on the open web and make them discoverable through precise search.As set out in its mission statement, the Open Language Archives Communityhttp://www.language-archives.org/ is “an international partnership of institutions and individuals who are creating a worldwide virtual library of language resources by: (i) developing consensus on best current practice for the digital archiving of language resources, and (ii) developing a network of interoperating repositories and services for housing and accessing such resources.”With respect to best practices, the community has thus far focused on developing recommendations for the metadata description of language resourceshttp://www.language-archives.org/REC/bpr.html so that they can be discovered in search with high precision and recall. The OLAC metadata formathttp://www.language-archives.org/OLAC/metadata.html is an extension of Dublin Corehttp://dublincore.org/documents/dcmi-terms/—the dominant metadata standard in the digital library and World Wide Web communities (Bird and Simons 2004). To support the need for precise search, the community has adopted five specialized vocabularieshttp://www.language-archives.org/REC/olac-extensions.html for use in describing resources: subject language, for identifying precisely which language(s) a resource is “about” by using a code from ISO 639;http://www.sil.org/iso639-3/ linguistic type, for classifying the structure of a resource as primary text, lexicon, or language description; linguistic field, for specifying relevant subfields of linguistics; discourse type, for indicating the linguistic genre of the material; and role, for documenting the parts played by specific individuals and institutions in creating a resource.With respect to the network of interoperating repositories, there are now more than 40 institutions that are sharing their language resource metadata to create a virtual digital library with over 90,000 holdings. Participating archives publish their catalogs in the XML format of an OLAC repositoryhttp://www.language-archives.org/OLAC/repositories.html and these repositories are “harvested” thrice daily by the OLAC aggregator using the Open Archives Initiative (OAI) Protocol for Metadata Harvestinghttp://www.openarchives.org/OAI/2.0/openarchivesprotocol.htm (Simons and Bird 2003)—another standard of the digital library community.The Open Access movementhttp://en.wikipedia.org/wiki/Open_Access_movement has led to the widespread uptake of self-archiving of research results by university faculty and staff. It stands to reason that among the millions of resources deposited into open-access institutional repositories, there are thousands of language resources. But these resources are not typically accessible via general web search. This is because they are hidden behind the search interfaces of hundreds of repositories and they lack precise identification as language resources. The question is, “Can we find the language resources in institutional repositories and then make them easy for the language resources community to discover?”Our research addresses the problem by using text mining techniques. We have begun by training a binary classifier that identifies the likely language resources within an institutional repository. We used MALLET, the Machine Learning for Language Toolkit,http://mallet.cs.umass.edu/ to train a maximum entropy classifier. For data to train the classifier we needed a large collection of metadata records covering the full range of human knowledge that were already classified as to the nature of their content. For this purpose we used the collection of more than 9 million MARC catalog records from the Library of Congress collection that was deposited into the Internet Archivehttp://www.archive.org/details/marc_records_scriblio_net by the Scriblio project.http://about.scriblio.net/ We used bag-of-words features extracted from the title and subject headings of each MARC record. To label each record as to whether it was a language resource or not, we mapped the Library of Congress call number onto the appropriate binary label based on a prior analysis of the Library of Congress classification system. The resulting set of 9 million training records was then given to MALLET to train a binary classifier for language resource identification.The resulting classifier was applied to over 4 million Dublin Core metadata records that were collected by doing a complete harvest of nearly 700 institutional repositories using the OAI Protocol for Metadata Harvesting. The list of base URLs to harvest was found by going to the University of Illinois OAI-PMH Data Provider Registryhttp://gita.grainger.uiuc.edu/registry/ and querying for all repositories with the word “university” in their Identify response. When applied to a metadata record, the classifier returns a number between 0 and 1 representing the probability that the resource is a language resource. This probability was added as a new metadata element to each harvested record. We then implemented an extension to the OAI-PMH interface on our metadata aggregator that allows us to request a ListRecords response of a given size that is a random sample of the records falling within a given probability range. Figure 1 shows the result of evaluating the performance of the classifier by means of manually inspecting ten random samples of 100 records each representing the full range of probabilities assigned by the classifier. In the manual evaluation of the classifier results, each record was assigned to one of three categories: not a language resource, a resource about a specific language, or a resource about human language but no language in particular. Figure 1 plots the number of specific language resources found in each sample of 100 as the lower line; the upper line adds the non-specific language resources. (Not plotted are a sample of 500 records for .001 < p < .01 in which were found 0 specific language resources and 4 non-specific resources, and a sample of 200 records for p < .001 in which 0 language resources of either type were found.) The graph demonstrates that the probabilities assigned by the classifier accord well with the actual proportions discovered by manual inspection, thus providing evidence for the validity of the classifier. The notable deviation from the expected trend is in the highest probability range. Inspection of the records in question showed that the majority of false positives were items from computer science about programming languages and formal language theory, leading us to hypothesize that the training data from the Library of Congress catalog was underrepresented in this area.   Figure 1: Evaluation of language resource classifier Full Size Image   Of the 4 million harvested records, only 52,000 indicate a probability greater than .01 of being a language resource. Multiplying the proportion of actual language resources found within each probability range by the total number of records falling within each range leads to the estimate that there are approximately 8,000 specific language resources within the set of 4 million harvested records.The next step in our research is to apply a multiclass classifier for language resource types to the metadata records for the 52,000 candidate language resources, as well as a named entity recognizer for language names. The metadata records to which language resource type and language identification can be assigned with high probability will be enriched using the OLAC metadata vocabularies. They will then be entered into the combined OLAC catalog by creating a new OLAC data provider for these language resources that have been mined from institutional repositories. The final paper will report on the results of these efforts at metadata enrichment and show how the results are exposed to users through the two main OLAC services that support language resource discovery: an indexing service that provides a web page of relevant resources for each of 7,670 distinct human languages (as identified in the ISO 639-3 standard) and a faceted search service that makes it easy to find resources of interest by clicking on selected values of standardized descriptors to successively refine the search.", "article_title": "Mining Language Resources from Institutional Repositories", "authors": [{"given": "Gary F.", "family": "Simons", "affiliation": [{"original_name": "SIL International and Graduate Institute of Applied Linguistics", "normalized_name": "SIL International", "country": "United States", "identifiers": {"ror": "https://ror.org/00d57b563", "GRID": "grid.481394.3"}}]}, {"given": "Steven", "family": "Bird", "affiliation": [{"original_name": "University of Melbourne", "normalized_name": "University of Melbourne", "country": "Australia", "identifiers": {"ror": "https://ror.org/01ej9dk98", "GRID": "grid.1008.9"}}, {"original_name": "University of Pennsylvania", "normalized_name": "University of Pennsylvania", "country": "United States", "identifiers": {"ror": "https://ror.org/00b30xv10", "GRID": "grid.25879.31"}}]}, {"given": "Christopher", "family": "Hirt", "affiliation": [{"original_name": "SIL International", "normalized_name": "SIL International", "country": "United States", "identifiers": {"ror": "https://ror.org/00d57b563", "GRID": "grid.481394.3"}}, {"original_name": "Payap University", "normalized_name": "Payap University", "country": "Thailand", "identifiers": {"ror": "https://ror.org/05d8xgj52", "GRID": "grid.443788.4"}}]}, {"given": "Joshua", "family": "Hou", "affiliation": [{"original_name": "University of Washington", "normalized_name": "University of Washington", "country": "United States", "identifiers": {"ror": "https://ror.org/00cvxb145", "GRID": "grid.34477.33"}}]}, {"given": "Sven", "family": "Pedersen", "affiliation": [{"original_name": "Graduate Institute of Applied Linguistics", "normalized_name": "Graduate Institute of Applied Linguistics", "country": "United States", "identifiers": {"ror": "https://ror.org/02jr43a19", "GRID": "grid.446912.c"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Our presentation about the Google Ancient Places (GAP) project will demonstrate new techniques to computationally identify places referenced in scholarly texts. We will also discuss deployment of simple Web services that use resulting place identifications to help bridge across online literary and material culture collections.Funded through the Google Digital Humanities Award program (July 2010-June 2011), GAP <http://googleancientplaces.wordpress.com/> mines a portion of the Google Books Corpus <http://books.google.com/intl/en/googlebooks/history.html> to find books related to ancient locations identified by gazetteers of the Classical Mediterranean world.GAP builds upon the Herodotus Encoded Space-Time Imaging Archive (HESTIA) project. HESTIA <http://www.open.ac.uk/arts/Hestia/> was a two-year collaboration (2008-2010) between The Open University and the Universities of Oxford and Birmingham, funded by the UK Arts and Humanities Research Council. Its aim was to explore new methods for visualizing relationships in Herodotus’ Histories. The project explored multiple approaches, including:  mapping the frequency of references to specific locations (both spatially and in terms of linear narrative) manually and automatically generating maps of the network connections between places.  The project made use of Greek and English versions of the text from the Perseus Digital Library <http://www.perseus.tufts.edu/> which are marked up with the Text Encoding Initiative (TEI) XML schema, including geographical locations based on automated string-matching with the Perseus internal gazetteer and Getty Thesaurus of Geographic Names. Closer analysis revealed that many of the locations were misidentifications, however, and a relatively labor-intensive process was required to correct them. HESTIA’s use of Perseus Digital Library resources demonstrates the growing power of open infrastructure already established in Classical studies. HESTIA also helped to demonstrate the utility of visualizing locations within a narrative. However, could the approach be automated so as to scale beyond manually processing individual texts? GAP attempts to answer this question through more sophisticated computational methods and by using additional open infrastructure, especially new Semantic gazetteers (see below) such as GeoNames <http://www.geonames.org/>, and the Pleiades Project <http://pleaides.stoa.org/>. HESTIA’s focus lies in a seminal text, the Histories by Herodotus. While primary and secondary literary sources represent key resources for Classical Studies, Classics also draws upon diverse sources of material evidence gathered from art history, architecture and archaeology (Mahony and Bodard 2010:3-5). These different sources of evidence and their associated scholarship are often highly “siloed”. Reference to Perseus or Pleiades resources can improve their interoperability. To address this issue, GAP demonstrates how open digital humanities infrastructure together with the Google Books Corpus, can be used synergistically to bridge online literary and material culture collections. GAP uses Open Context <http://opencontext.org/> to test such services. Open Context is an open-access archaeological data publication system offering wide-ranging documentation of architecture, archaeological contexts, and objects from multiple contributors (Kansa and Kansa 2007). Open Context provides a map and timeline on its splash page to enable both providers and users to quickly identify related research. The ability to identify relevant scholarly literature relating to Open Context’s material culture collections would be a ground-breaking extension to this service. Prior experience with Herodotus’ Histories informs GAP’s methodology. In developing the HESTIA Narrative Timeline, we learned that places referenced in narrative texts generally cluster together to maintain narrative coherency. Given a set of toponyms with multiple possible identifications, the set of identifications with the shortest overall path between them is likely to be correct. In addition, we can add weight to the importance of each toponym by the number of possible locations it could refer to. Somewhat counter-intuitively, this means that small, obscure places with unusual names are much better guides to location than well-known places with many namesakes. While a useful starting point, several additional factors complicate accurate place identification:  The approach does not work well for fragments or with arbitrary higher-level structures such as the alphabetic organization of an encyclopedia.  The author may assume that the anticipated audience will be able to contextualize by other narrative elements (such as well-known individuals) and thus mention only a single location (or even none at all).  The author may contextualize by giving a territory in which the place is located. These can confuse point-based algorithms as there is no single ‘best’ point that represents them. The author may have confused the place they are discussing with another, especially if they are commenting on another work or reporting independent sources. Occasionally the pattern location clustering assumption simply does not hold. This is especially the case for places that do not perform an active function in the text such as personal names derived from places of origin (e.g. ‘Herodotus of Halicarnassus’). Fortunately, new open infrastructure, especially Semantic Gazetteers such as GeoNames and Pleiades, can improve the precision of place identification. Both GeoNames and Pleiades offer open, machine-readable data curated by dedicated communities. They provide unique HTTP URIs for each place to which multiple names (toponyms), locations (such as spatial coordinates) and categories (like ‘settlement’ or ‘region’) can be assigned. These gazetteers make it much easier to handle the problem of synonymy and allow us to assign non-ambiguous and easily resolved public identifiers to places identified in the Google Books Corpus.Nevertheless, even with the aid of gazetteers, the difficulties outlined above make place identifications highly probabilistic and uncertain, especially in cases where we find either insufficient or conflicting evidence. Hard cases can then be handled by a variety of methods, including more sophisticated but computationally expensive procedures or by manual effort of a scholar. Computationally, there are multiple levels at which we can look for clustering, including the chapter, book, and corpus (of the author or even genre). Looking at higher levels may provide us with broader contextual clues. A further advantage of working with massive digital corpora is that they frequently provide multiple translations and editions. In such cases we can use the linear chain of places in one edition to inform the processing of another and vice versa. Finally, as we process more books the system can record additional metadata about the places as well as the books. In particular it may see that in cases of homonymy, one location is much more frequently mentioned than all the others (such as the Egyptian Alexandria, as opposed to the many other cities of that name). This can help in cases where we have no other contextual clues to draw on. Google Books metadata and comparison of multiple editions found Google Books corpus may thus help resolve ambiguous place determinations in some cases.It is also important to remember that there are some hard limits imposed on the process and some pragmatic aspects to our goals. First, we are only able to identify those places for which we have an entry in a gazetteer. Natural Language Processing available to us will not identify places previously unknown. Secondly, we are not looking for a ‘perfect’ set of results for the simple reason that natural language is ultimately indeterminate. Continued improvement of computational methods, as well as more traditional forms of scholarship will be required. Scaling and adapting text processing methods developed for HESTIA for the larger Google Books Corpus represents one of the key challenges for GAP. To help evaluate the effectiveness of our approach, we first reconciled local identifiers used by the HESTIA project with Pleiades URIs. We will report on how our algorithmic approach to place identifications compares with places manually identified in HESTIA using the same raw text of the Histories as used by HESTIA. We will then report on results of our algorithmic method on the 1828 translation of the Histories provided by Google. Finally we will discuss application of our algorithms for general use on the Google Books corpus, focusing on public domain texts with Library of Congress Headings DE-DG (Greco-Roman World; Greece; Italy).GAP provides processing results in RDF-expressed annotations for each text. Such annotations are extremely useful to software but generally less helpful for humanities researchers who typically require a human interface. Thus, GAP also provides Web mapping tools, like those on the HESTIA and Open Context websites. These interfaces enable searches in both directions – from text to places, and from a place to the texts which reference it. To lower adoption barriers, we chose RESTful Web service (based on the Atom Syndication Format and GeoJSON) design patterns (see Blanke et al. 2009; Kansa and Bissell 2010). Such services enable other developers and digital humanists to incorporate our results into other research environments and applications.As discussed above, the GAP project makes extensive use of existing digital humanities infrastructure, especially place gazetteers such as Pleiades. In doing so, GAP helps to demonstrate the growing maturity of digital scholarship in Classical studies. Rather than standing alone as isolated, one-off efforts, digital projects increasingly complement one-another and enable future work. In this light, we hope GAP will catalyze continued research (see Rosenzweig 2007) in the text processing methods, systems design, and semantic standards required to bridge gaps across literary and material culture collections. ", "article_title": "Googling Ancient Places", "authors": [{"given": "Leif", "family": "Isaksen", "affiliation": [{"original_name": "Archaeology, University of Southamp", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Elton", "family": "Barker", "affiliation": [{"original_name": "Classical Studies, The Open University", "normalized_name": "Universidade Aberta", "country": "Portugal", "identifiers": {"ror": "https://ror.org/02rv3w387", "GRID": "grid.26693.38"}}]}, {"given": "Eric C.", "family": "Kansa", "affiliation": [{"original_name": "School of Information, University of California, Berkeley", "normalized_name": "California Coast University", "country": "United States", "identifiers": {"ror": "https://ror.org/05t99sp05", "GRID": "grid.468726.9"}}]}, {"given": "Kate", "family": "Byrne", "affiliation": [{"original_name": "Informatics, University of Edinburgh", "normalized_name": "University of Edinburgh", "country": "United Kingdom", "identifiers": {"ror": "https://ror.org/01nrxwf90", "GRID": "grid.4305.2"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "The world of humanities has seen an enormous growth in available digital text resources in the last decade. This development was driven by many factors like advancements in relevant technologies like OCR, increasing competence in the field of digital encoding and publication, and the spreading of widely accepted encoding formats. It is by now widely understood (among both researchers and funders) that publications and created resources have to be standardized to ensure their relevance for future work and their (re-)use in a linked data environment.More and more projects with partially very specific research questions are working on the encoding of their results in various (mostly XML based) formats. Encoding standards and formats were established that are widely used and supported by various tools and a helping community. In the field of encoding textual resources the standards of the Text Encoding Initiative (TEI) and their various dialects are well represented. To cover a wide range of data it is common to create a specific dialect that fits the own data best without losing all compatibility with other projects. As a consequence, various encoding variants exist that cope with similar data but create different schemata to represent them. A drawback of this specialism are problems with aggregating existing data stocks to one global resource: Even combining solely meta data of editions of the same work becomes an expensive (since labor intensive) task. Creating true hypertextuality in digital libraries (Berti et al., 2009) that will massively connect resources in a distributed infrastructure will intensify this problem. Hence data integration will gain relevance in the field of distributed resources.Since data storage solutions like relational database management systems are often used (for example when fast access to or complicated requests on the data is necessary) this issue does not only apply on the XML data model. These systems often use ETL-procedures (Extract, Transform, Load) to gain uniform and comparable data. The key problem remains the same: A lot of time is spent to gain a clean stock of data and consistent meta data. Experiences show that especially at projects using quantitative approaches, with a demand for large (and as homogeneous as possible) data sets, up to one third of all human resources are needed to overcome different kinds of heterogeneity.This paper concentrates on the question how existing schema matching techniques that are established in data warehousing and information integration can be used to identify identical structures of different editions of the same source material. Therefore a high similarity of the content can be assumed, whereas structural and semantic heterogeneity prevents fast integration. As all modern storage solutions rely on schema definitions, it was not the task to identify corresponding element instances of two documents but to identify correspondences between collections of documents. For this reason in the following the term 'element' will be used for the set of all elements having the same position in one schema (for example all TEI/teiHeader/fileDesc/titleStmt/titleelements in a set of XML files or all values for one column work-namein a relational database table).This approach has two advantages: generic profiles for every schema element can be created (thus minimizing the effects of outliers) and computational time is reduced by minimizing the number of comparisons that have to be made to find useful schema mappings.To illustrate the procedure different versions of the Duke Databank of Documentary Papyri (DDbDP) were used. These include the Perseus version of the DDbDP, its EpiDoc encoded equivalent (Epiduke) and an extraction from the latter, stored in a flat relational schema. These data are only to be seen as a first testing environment. Further evaluation on other text types is in progress.The whole process of finding corresponding elements or larger element structures can be separated into three major working steps:  Fingerprinting: By using various features a fingerprint is created for every element, taking different element properties into account. Linking: Elements of both schemata are chosen pairwise that are likely matching candidates. Scoring: Every linked pair is scored by a similarity measure. To identify corresponding elements of two schemata, for every addressable element various features are used. These features address different types of similarity like structural similarity (with the focus on schema information) or semantic similarity (with the focus on elements' content). Most of the used features do not depend on specific structures or access methods, hence every addressable element can be used and compared. As a consequence XML or SGML documents, columns in a relational database or every other (semi-)structured input schema can be used. Existing works have shown that using only a single feature is not sufficient to identify similarity (Algergawy et al., 2009). For this reason all measures are combined and normalized by a weighted sum.As there is a wide range of syntactic and semantic ambiguities it is unlikely to achieve a full automated matching. Hence it is the goal to establish an integration procedure that allows a more efficient handling of new data resources to minimize the effort of integrating these resources into an existing data stock.A wide range of different features are known in the field of data integration. Some of these make use of structural schema information (schema based) while others use the elements' content (instance based). A constraint based approach checks the type and limitations of data, e.g. the domain of numbers or the differences in cardinality or uniqueness of elements. These features work on different levels: some concentrate on the combination of elements, their hierarchy or their number of child nodes (structure level), while others focus on individual elements and their attributes (element level).The following features were tested on their usefulness for the described problem. Table 1 gives a short overview of used approaches and their classification (based on Rahm et al., 2009).   Name similarityuses the Levenshtein distance to compute string similarity of database column names, respectively XML element names. For example an element name authorshas a distance of 1 from an element named author, but a distance of 5 from the string work.  Path similaritycompares the structural depth of elements, under the assumption that similar elements have similar positions (and therefore similar distances to the root element) in their respective schema.  Cosine similarityuses the Vector Space Model by representing the content (i.e. the occurring terms) of every element as a vector in a high dimensional vector space. To reflect different importance of terms (for example stop words versus domain-specific keywords) all terms are weighted by using the tf.idfmeasure (Salton et al., 1988). The result vectors are compared using the cosine similarity:   Dice coefficientcalculates the ratio of words, that appear in both compared elements to all occurring words:  For example an element that contains the words {bank, money, account, credit} is similar to an element containing the words {bank, money, account, financial} (Dice coefficient = 0.75), but less similar to an element containing the words {bank, river, water} (Dice coefficient ~ 0.29)  Frequency similarityuses the assumption that similar content is encoded by a similar number of elements. Therefore this measure produces a high value if the number of occurrences of the compared elements are similar.  Content typecompares the ratio of numbers to letters. Hence an element with mostly numbers becomes dissimilar to an element containing mostly textual data. All results were normalized to the interval [0,1] (where necessary), `0' corresponding to no similarity and `1' to identity.Features that address the element's content use all available data: For example the union of all text addressable with the same XPath expression in a collection of XML files or all data in a column of a relational schema.Experiments have shown that many elements in the chosen XML collections occur very rarely. Therefore only elements were considered that occur in at least 50 percent of all documents of the respective collection to reduce the computation time. All other elements of both compared data sets were linked with each other.In general a more sophisticated approach would be useful to minimize the number of comparisons. This holds especially true in a distributed environment where network response and transmission time is a limiting factor. This was not considered in this work as the focus was on identifying useful features and all resources were locally available.The values of all similarity measures are combined by a weighted sum, yielding a similarity value between `0' (no similarity) and `1' (identity). Starting by identical weights for all measures the weights were iteratively adjusted to enhance the matching precision.The results show that especially the instance-based approaches (Dice and Cosine) were successful for identifying matching elements. These measures worked well on both XML-XML and relational database-XML comparisons. All other measures turned out to be strongly dependent on the compared formats.Especially structural differences between optimized (and redundancy-free) relational schemata and XML documents prevent good results when relying solely on schema information: in these cases matching precision drops significantly. Only instance-based measures (Dice, Cosine, and content type similarity) achieved good results, whereas all other measures could be ignored (hence weighted with 0).A more balanced result shows in the XML-XML analysis: as both document collections are based on subsets of TEI formats many relevant elements are similar regarding their name and position in the XML structure tree. Therefore especially name similarity turned out to be a useful indicator for matching elements. Nonetheless again instance based measures performed best on these comparisons.A graphical user interface was created to visualize the pairwise similarity of elements. In a matrix (c.f. Figure 1) every row stands for an element of the document collection 1 and every column for an element of document collection 2 (or database columns in case of a relational schema). Each cell contains the weighted sum of similarity values for the two respective elements. High certainty values are emphasized with a strong green color. A tooltip on each cell gives additional information about the comparison results (used features and similarity values).Figure 1 shows an excerpt of a result matrix where a relational database is compared to a collection of XML files. Every column represents a database column of the extracted version of the Epiduke, every row represents a path in Perseus' DDbDP XML files. As an example the element placeNamefrom the collection of XML files has a strong similarity to the geographycolumn in the database, even though different element names were used. By analyzing their content it becomes obvious that there is a strong extensional overlap between these elements' content. On the other hand there are no similar XML elements for the database column female(information about the author's gender) and work_id(a database-specific identifier for every work). This is due to the fact that both elements were created during the extraction process (or the following post-processing) and therefore have no corresponding elements in the original material.  Figure 1: Graphical output for comparison of epigraphic data hold in Perseus' DDbDP XML and an extracted version of the Epiduke. Full Size Image   Various comparisons have shown that especially semantic approaches are promising for identifying similar elements. Apparently these measures' results will degrade when data is compared with a very small semantic overlap (like editions of different domains or languages). As a consequence structural information could be taken into account. For the analyzed data these measures proved useful where complex structures exist, but failed on flat relational schemata.The existing system is only to be seen as a prototype that will be extended in the future. The further focus will be set on adding new features and analysis of an extended set of input. It is expected that more efficient and domain-specific profiles can be created that will be basis for useful weights and combinations of features. Additionally it is expected that the results can be improved by using a more in-deep view on the data (like identification of key words) or further exploitation of structural information (by including schema definitions into the classification process).", "article_title": "Integration of Distributed Text Resources by Using Schema Matching Techniques", "authors": [{"given": "Thomas", "family": "Eckart", "affiliation": [{"original_name": "Natural Language Processing Group, Institute of Computer Science, University of Leipzig, Germany", "normalized_name": "Leipzig University", "country": "Germany", "identifiers": {"ror": "https://ror.org/03s7gtk40", "GRID": "grid.9647.c"}}]}, {"given": "David", "family": "Pansch", "affiliation": [{"original_name": "Natural Language Processing Group, Institute of Computer Science, University of Leipzig, Germany", "normalized_name": "Leipzig University", "country": "Germany", "identifiers": {"ror": "https://ror.org/03s7gtk40", "GRID": "grid.9647.c"}}]}, {"given": "Marco", "family": "Büchler", "affiliation": [{"original_name": "Natural Language Processing Group, Institute of Computer Science, University of Leipzig, Germany", "normalized_name": "Leipzig University", "country": "Germany", "identifiers": {"ror": "https://ror.org/03s7gtk40", "GRID": "grid.9647.c"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "The series You Suck at Photoshop first appeared as a three-and-a-half-minute hoax circulating on the web in late 2007—a one-off Internet meme satirizing home-made, how-to videos ubiquitous on YouTube. Conceived and performed by advertising professionals Troy Hitch and Matt Bledsoe, the video featured an ironic, My-Last-Duchess-style commentary by a fictitious computer geek Donny Holye, who unintentionally reveals messy details of his marital, legal, and emotional life while demonstrating techniques of Photoshop. That same fall, in a special issue of PMLA on “Remapping Genre,” some of the leading voices in the emergent field of the digital humanities discuss the database as a cultural form, and debate the political, ideological, and practical impact of databases on the work of the humanities, and the practices of narrative in particular. The varied contributions to “Remapping Genre” provide a dramatic tableau of twenty-first-century academic humanities confronting the database as a cultural discourse, and speculating on whether it represents an alien nemesis from the world of corporate capitalism, the long-sought embodiment of implicit literary ideals, or a value-neutral complement to narrative linearity.  This presentation argues that “Remapping Genre” and You Suck at Photoshop not only share an historical moment, but constitute a common critical effort to understand and respond to the historical rise of databases in cultural practice. Invoking this unlikely affiliation might seem to announce another scholarly incursion on the popular realm under the pirate flag of Cultural Studies.  The relationship suggested here between academic and popular cultures, however, is less a territorial rivalry than a mutually transformational dialogue.  And, in fact, this transformation is well underway.  On one hand, as contributors to “Remapping Genre” amply document, literary studies is rapidly adopting frictionless, digital tools to perform its daily work: from personal, bibliographic browser plug-ins like Zotero, to online databases like The Walt Whitman Archive, to vast digitalization initiatives like Google Books.  Less recognized, on the other hand, is the degree to which popular, database-driven New Media like Facebook, online games, and the web itself are increasingly being constructed not only by the work of programmers and designers, but by narratives that users, players, and an emerging circle of professional critics employ to shape, remember, and publicize their experiences of virtual database environments.  Such narratives reveal how the texture of language—the friction of fiction—is an integral part of how databases are culturally constructed and experienced, and the extent to which that use of narrative language is conditioned by a literary sensibility. This talk will analyze clips from You Suck at Photoshop—as well as background on the series’ circulation and presentation on the web—to examine the interplay of narrative and database as textual and cultural logics, as well as to suggest the historical transformations that such an emergent dialogic is producing in the relationship between the academic humanities and popular culture. Like Donny Hoyle’s conflicted relationship with Photoshop’s remorselessly vast database of tools, the Digital Humanists featured in “Remapping Genre” express ambivalent responses to database-driven tools of their own academic trade, and debate Lev Manovich’s claim that database and narrative are “natural enemies…[c]ompeting for the same territory of human culture, each claim[ing] an exclusive right to make meaning out of the world” (225). Ed Folsom’s and Kenneth Price’s digital project The Walt Whitman Archive provides a common case in point, serving to ground, historicize, and set in relief the critical differences among these scholars, which echo concerns and anxieties within the profession at large: questions concerning the degree to which databases may or may not threaten narrative forms of thought and meaning (N. Katherine Hayles); the ideological, as opposed to the supposedly libratory, effects of databases, especially mediated by large, corporate interfaces like Google (Jonathan Freedman); the extent to which the seemingly new logic of databases remediates pre-digital, even ancient literacies (Peter Stallybrass and Meredith L. McGill); whether or not the database can be called a genre, perhaps even belonging to the “epic” genre that crosses national and historical boundaries (Wai Chee Dimock); and the problems of deciding whether a tool like The Walt Whitman Archive is really a database at all (Jerome McGann). Beyond particular insights into the database/narrative question, this talk will explore how the very unlikeliness of this dialogue of You Suck at Photoshop and “Remapping Genre” points to emergent affinities and even alliances among popular, academic, and economically “productive” discourses. If these potential alliances have not been sufficiently described, it is not from lack of trying, however. In his monumental book The Laws of Cool, for instance, literary critic and digital theorist Alan Liu attempts to imagine the relationship between the cultures of “the literary”— not just “works of literature as such,” he says, but “the underlying sense of the literary”—and of digital “knowledge work” characterized by an ethos of “cool” (1, 3). “In [capitalism’s] regime of systematic innovation [and creative destruction],” he asks at the beginning of the book, “is the very notion of the literary doomed to extinction if—or, rather, especially if—it dares to imagine a literature of the database, spreadsheet, report, and Web page?” (3). Essentially, Liu is questioning just how “the contemporary humanities and arts…[might] not only make contact with the generations of cool but lead them beyond the present limitations of cool” (381). In his somber Epilogue, Liu declares, “I am a believer at heart…. I think literature will indeed have a place in a new-media world…. But what the eventual nature and position of that literature will be among the convergent data streams of the future is something I do not yet know how to theorize” (389). Like Richard Lanham, Katherine Hayles, and other scholars leading the search for the Holy Grail of would-be database literature, Liu tends to look for examples in high-cultural practices of Internet Art, or what he more inclusively terms “ethical hacking,” but Liu ultimately finds them “too closely associated with anarchist, Situationalist, radical leftist, and/or high-theoretical paradigms…to offer persuasive models for an art that might affect the knowledge worker in his or her ordinary cubicle” (397-98). This talk will argue that such persuasive models, or perhaps models of models, can be found in the ordinary cubicles of slackers like Donny Hoyle—but only if academic culture can develop the critical idiom to describe them. ", "article_title": "You Suck at Narrative: Disciplinarity, Popular Culture, and the Database Logic of Photoshop ", "authors": [{"given": "Craig", "family": "Stroupe", "affiliation": [{"original_name": "University of Minnesota Duluth", "normalized_name": "University of Minnesota, Duluth", "country": "United States", "identifiers": {"ror": "https://ror.org/01hy4qx27", "GRID": "grid.266744.5"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Cinemetrics (www.cinemetrics.lv) is a collaborative, online tool to enable researchers to collect, store, and process scholarly data about film editing. The long-term goal of the project is to create an extensive, multi-faceted collection of freely accessible digital data on film editing - a digital laboratory for the study of film style. Over the last four years, over 600 individuals have contributed editing data on circa 6,900 films to the project. Our paper at DH2011 will focus on the types of film historical questions Cinemetrics can address today. In addition, we’ll provide a brief overview of how the software and the collaborative submission works, compare our work with similar efforts, and finally offer an early look at future directions for development.At present, Cinemetrics is programmed handle the aspect of editing known in film studies as cutting rates. Though we tend to perceive their unfolding as continuous most films consist of segments called shots that are separated by instant breaks called cuts. Shots differ in terms of space and in terms of time. We know enough about space-related distinctions between shots, which are easy to name (shot 1: baby playing; shot 2: man looking) and categorize (shot 1: medium long high angle shot; shot 2: facial close up). Time-related differences between shots are more elusive and harder to talk about for; unlike in music or poetry with their scaled feet and measures, variations in shot length are not ones of distinction but of degree.Shot lengths are sometimes convenient to present as the frequency of shot changes, or cuts, hence the term \"cutting rates.” Shorter shots mean a higher cutting rate. Unsurprisingly, cutting rates are linked to the story and its space-time articulations; likewise, montage sequences meant to cover larger spaces of story time have higher cutting rates than sequences shown in real time. Less evident, but just as important, is the relationship between cutting rates and the history of film. This is this gap in our knowledge that Cinemetrics is designed to bridge.Using Cinemetrics, we are able to obtain and present cutting-related data in a more flexible way than previously available. Rather than calculate average shot lengths (ASL) arithmetically, Cinemetrics records and stores the time-span of each separate shot. Distinct from the arithmetical ASL, which is a single datum, Cinemetrics treats each film as a database of shots and highlights its individual features. Specifically, it tells us about a film's cutting swing (standard deviations of shorter and longer shots from ASL), its cutting range (difference in seconds between the shortest and the longest shot of the film), and its dynamic profiles (polynomial trendlines that reflect fluctuations of shot lengths within the duration of the film). In the “Articles” section of our site are links to articles by a number of film scholars on movie measurement studies using Cinemetrics.The Cinemetrics database is an open-submission repository of data collected by people who use the client tool. All raw research data submitted to the site is freely available to anyone. The database's default sorting is alphabetic by film titles, but it can also be sorted by other parameters such as year, submitter's name, submission date, simple vs. advanced mode of measuring, and by the film's average shot length, median shot length, and standard deviation. By clicking on a film, title the user can access the page that provides basic statistics and interactive graphs related to this film.\"Cinemetrics Lab\" is the latest addition to our site, and a work in progress. It is envisaged to offer the students of film history a range of analytical tools that will help them dissect, visualize, and compare film-related data. We started with a large-scale comparative map that looks a little like a star map. It is a scatter graph, and each dot represents a film available on our database. If you find your film on this map, you will instantly see how it relates to thousands of other films on the x-axis on time (111 years of film history) and on the y-axis of average shot lengths.While Cinemetrics has no clones, there are a number of projects pursuing similar goals that complement our efforts. Jeremy Butler's useful \"Shot Logger\" (www.tcf.ua.edu/slgallery/shotlogger/) features a database of films (mainly TV) and offers statistics \"inspired by Cinemetrics\", but its database is still small, and the seven statistics values the site yields are numerically, not graphically, expressed. The francophone site \"Lignes de Temps\" (web.iri.centrepompidou.fr/pop_site.html), linked to the Georges Pompidou Center for Modern Art in Paris is mainly designed as a video-flow annotation and cut-detection tool.The above-mentioned \"Shotlogger,\" \"Edit 2000,\" and especially \"Research into Film\" are three sites on which Cinemetrics activities are actively echoed or discussed. Nick Redfern's \"Research into Film\" (nickredfern.wordpress.com/) uses Cinemetrics data to theorize statistical approaches to film studies. \"Edit 2000\" (www.data2000.no/EDIT2000/), launched in Norway in 2009, was made to represent Edit Decision List (EDL) files as numeric and visual summaries. Another group whose researchers deployed Cinemetrics raw data for their experiments in data visualization is \"Software Studies Initiative\" headed by Prof. Lev Manovich at UC San Diego. The site shows how Cinemetrics data can be variously represented using different visualization tools.", "article_title": "Cinemetrics: A Digital Laboratory for Film Studies", "authors": [{"given": "Arno", "family": "Bosse", "affiliation": [{"original_name": "The University of Chicago", "normalized_name": "University of Chicago", "country": "United States", "identifiers": {"ror": "https://ror.org/024mw5h28", "GRID": "grid.170205.1"}}]}, {"given": "Yuri", "family": "Tsivian", "affiliation": [{"original_name": "The University of Chicago", "normalized_name": "University of Chicago", "country": "United States", "identifiers": {"ror": "https://ror.org/024mw5h28", "GRID": "grid.170205.1"}}]}, {"given": "Keith", "family": "Brisson", "affiliation": [{"original_name": "The University of Chicago", "normalized_name": "University of Chicago", "country": "United States", "identifiers": {"ror": "https://ror.org/024mw5h28", "GRID": "grid.170205.1"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "This paper describes the methodology used in the creation of digital chapters and subsequent recreation of digital entities or objects derived, modified, transformed and visualised from XML encoded scholarship. It considers the changing function of traditional printed theses and how the use of technologies affects the representation and functions of graduate digital scholarship.This paper is based upon the working methodologies of two PhD theses. Specifically, Webb’s thesis examines the creation of factlets and subsequent visualisation of factoids, which inform not only the source information and encoding but also the development and completion of historical research outputs. These outputs, supported by XML, XQuery and factlets, demonstrate the use of digital technology as an essential feature of humanities research and its methodologies. Teehan’s thesis reflects upon current digital representation models for pre-existing sources relevant to humanities research. Focusing on transactional, or functional, documents, it proposes a methodology for contextually modeling and XML-encoding those resources, using established software engineering and computer science paradigms such as Use Case analysis and UML modeling, which foreground the User. Both theses examine procedures and strategies for conducting humanities research using digital tools and applications. Thus, this paper is central to a reflective and reflexive process resulting from, and in, the critical self-evaluation of the theses and their outputs.Traditionally, research outputs codified as chapters or sections can be seen as the final manifestation of a PhD thesis and reflect the use of print or static technology. The functionality of these outputs varies according to different headings and ranges from literature reviews, general narrative and concept generation, to the development of structured arguments based on theory and source material, to the provision of essential referencing and bibliographic material. These functions are referred to as “generic characteristics of academic discourse” (Mingwei, 2010) in linguistic structural analysis. Chapter functionality represents and reflects the original research statement and provides the means to convey and articulate traditional scholarship within the medium of print. The use of XML, and XSLT, along with the provision of software libraries, creates a framework to add dynamic functionality to an otherwise static text. “Generic characteristics” (Mingwei, 2010) are encoded, which enable the use of the described framework.This approach reflects the innate capability of the digital medium to layer extra functionality over the restricted functionality of printed works. Rather than creating just a single representation of scholarly output, the use of XSLT and software libraries generates and encourages a reflexive process between text, argument, narrative and source material.These methods change reader and user activity - one user may be a reader while another may have access to an interactive environment. Different user roles and environments transform the user from a passive participant to an active one. The realisation of various use cases enables the user to do more than just read the text and this activity realises the importance of data reusability.Figure 1 outlines the process involved in creating multiple representations of digital scholarship and will be used to detail the various stages involved in creating new digital objects based on specified use cases.  Figure 1: The stages of creating new research outputs, various chapters are defined by various use cases. Full Size Image   Text is innately encoded with semantics and functionality and each chapter or section in a piece of scholarship establishes or conveys various essential processes in the life of a text. These processes consist of deduction, concept development, narrative, consequence, etc., and it is these “lexical relations” (Eggins, 2004) within a text which develop specific research statements. Other bibliographic properties of a section are concrete rather than abstract and provide essential functionality e.g. references, footnotes, paragraphs and titles.Despite these multiple perspectives, transformation of a born-digital text (a thesis) into both the print and digital media relies upon the existence of a single, defining text-model. Figure 1 shows the process involved in creating new research objects. The first stage makes possible all subsequent processes; creating a unifying model allows the generation of XML schema and subsequent XML encoding in order to manifest the new research objects (the various chapters in a research thesis). The model is driven by specific Use Cases such as the production of both a static printed version of the text, and an interactive digital version.The source chapters are encoded at the final stages of the research process, rather than during the writing process. The model considers both presentation properties (chapter, paragraph, section), which allows for transformations specifically for presentation purposes, and semantic properties which encode the “textual semantics” (Eggins, 2004) of the text, its logical class (Teehan, 2010). This approach makes the text reusable and ensures “a single lexical can function very differently” (Landow, 2006) in different environments.The model is translated into a schema which allows us to mark up the content of the scholarship, including narrative which in historical research pertains to ‘logical’ rather than ‘ideological’ content. We view narrative as the logical information contained in the text that contributes to a narrative of the past (Coffin, 2002). The encoding of dynamic narrative and data supports the creation of new research outputs as non-linear components derived from the text.Stage 2 and 3 are the realisation of the various Use Cases. The XSLT transforms and software libraries are templates from which different text from different sources can be modified and transformed, in effect creating a suite of tools.These various macros are supporting tools for manifestations of a text. Our encoded texts depict the various functions embedded in standard print theses, but augment those capabilities for these born-digital theses. Here, two specific Use Cases address (i) the creation of a dynamic bibliographic referencing model, and (ii) the context-dependent presentation of boundary objects.A referencing model in XSLT can automatically create a dynamic bibliography for a chapter with features including “intertextual links” (Samraj, 2008) between the text and source material. Software libraries can be used to support the innate variability of a boundary object, which is defined as an object with user dependent functionality and meaning (Thomas). Thus, depending on the User’s activity and perspective, the presentation of the boundary object will change; for instance, a table diagram, static in the print version, may become interactive within a digital context.These low-level Use Cases support our higher level one; dynamic creation of static or interactive versions of a base text-model. The print model transforms the original text to a print ready text, and can account for various institutional templates. Embedding references to the various primary sources used in the XML encoding instructs an XSLT to create a hypertext of linked resources and creates “intertextual links” (Samraj, 2008) and boundary objects for user interaction between the narrative and various digital objects within the digital medium. Figure 2  Figure 2: Text encoding of this proposal and XSLT transforms Full Size Image  This paper demonstrates the process and production of support tools for digital scholarship, and how the creation of appropriate templates can make manifest various representations of Digital Humanities PhD theses from a single model. The Use Cases are reliant on the ability of the encoding and the schema to encapsulate both the functions of the text and the various transformations and software libraries. Figure 2 demonstrates the interactions between the encoded text, the transformations and the outputs.Current research students in Digital Humanities constitute a newly ‘born digital’ generation, the nature of whose outputs differs markedly from earlier generations. Reflections on this changing process should also include an analysis of new methods and techniques to create dynamic scholarship. The encoding of the final phase in a PhD thesis allows scholarship to be reused, modified, visualised and transformed, allowing for greater distribution and accessibility of digital scholarship. Thus the dissertation, in its multiple representations, can not only remain central to the discipline of Digital Humanities but shape its future development.", "article_title": "The Born Digital Graduate: Multiple Representations of and within Digital Humanities PhD Theses", "authors": [{"given": "Sharon", "family": "Webb", "affiliation": [{"original_name": "Department of History, National University of Ireland, Maynooth", "normalized_name": "National University of Ireland", "country": "Ireland", "identifiers": {"ror": "https://ror.org/00shsf120", "GRID": "grid.9344.a"}}]}, {"given": "Aja", "family": "Teehan", "affiliation": [{"original_name": "An Foras Feasa Research Institute, National University of Ireland, Maynooth", "normalized_name": "National University of Ireland", "country": "Ireland", "identifiers": {"ror": "https://ror.org/00shsf120", "GRID": "grid.9344.a"}}]}, {"given": "John", "family": "Keating", "affiliation": [{"original_name": "An Foras Feasa Research Institute, National University of Ireland, Maynooth", "normalized_name": "National University of Ireland", "country": "Ireland", "identifiers": {"ror": "https://ror.org/00shsf120", "GRID": "grid.9344.a"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "A scholar or student who wishes to engage in ‘non-traditional’ authorship attribution would be wise to choose a test corpus that is as free as possible of ‘interfering’ features such as genre, external editing, or a corpus that is stretched over a large number of years. The higher the consistency throughout the corpus, the larger the chance of a successful investigation of authorship and/or stylistic features.Medieval manuscripts are characterized by a much greater amount of, what we could call, ‘interfering’ features. Scribes manually copied texts again and again, not seldom altering the content and often altering the orthography. Most original works have been lost, just as much of the copied material. To add even more difficulties, what we nowadays will easily refer to as ‘original work’ is much less clear-cut in the Middle Ages. Our modern notion of ‘copyright’ is virtually unknown to medieval men and women and for a long time the concept of auctoritas (author) was primarily used in referral to classic writers such as Aristotle and Augustine. Many of the medieval texts are thus written anonymously. The situation could be characterized as chaotic by scholars used to relatively straightforward text corpora. Before you can begin your quest for a medieval author, you first have to find out what content is scribal related and what can be attributed to the author. And just when you think you are making some progress, you find out that your ‘author’ has been merely compiling source texts, who he (or she) is copying word for word. A scholar addressing these texts should therefore meticulously peal of the different layers of the text. When confronted with these scenarios, it might not sound that surprising that the number of studies involving the use of computational techniques and medieval texts is not that great. In recent years though, some progress has been made (e.g. Van Dalen-Oskam and Van Zundert 2007; Van Dalen-Oskam, Thaisen and Kestemont 2010). Most of all, these studies show that it is possible – using computational techniques such as Burrows’ Delta – to overcome some of the difficulties in distinguishing for instance scribal and authorial layers within a single text.In this paper I will contribute to this evolving field of research and discuss some possible methods that can be used to differentiate these different layers within medieval texts. Although the text corpus that I will be using originated from fully tagged TEI/XML files, for this purpose I will be using plain text files. This could be of great interest to scholars who are not able or not willing to spend large amounts of time, energy and skill in preparing their texts. The texts are transcribed without changing spelling to modern use (thus euen instead of even and Iherusalem instead of Jherusalem), but abbreviated words are expanded.One of the more interesting aspects of this particular text corpus is that it has been written by a single person, whose name is recorded in one of his charters: Hendrik Gerardsz. van Vianen, most likely secretary of the Utrecht Land Commander of the Teutonic Order. The Teutonic Order was one of the three major military orders – beside the Knights Templars and Hospitallers – that defended the Holy Church in the Holy Land, the Iberian Peninsula and the Baltic region and received goods and land all over Europe, including the Low Countries. Hendrik van Vianen wrote at least 25 Middle Dutch charters containing land contracts for the Teutonic Order between 1479 and 1491. He also wrote a few Latin charters between 1489 and 1509 that are not included in this study. Furthermore, he manually copied a manuscript containing a Dutch version of the popular Sachsenspiegel that belonged to two Land Commanders of the Teutonic Order in Utrecht. Last but not least, he was responsible for a chronicle on the history of the Teutonic Order, known as the Croniken van der Duytscher Oirden or Jüngere Hochmeisterchronik (Stapel and Vollmann-Profe 2010). Codicological evidence suggests that the manuscript of his hand, now in Vienna, is an autograph, although it is not sure whether parts of the text existed before. Autographs are relatively rare in a medieval context, a recent survey of Middle Dutch manuscripts mentions barely more than a hundred examples (Houthuys 2009).As a result, we have the unique opportunity to study a medieval text corpus – perhaps not as large as the ones modern literary scholars work with, but still substantial in medieval terms – of around 131.000 words that includes the work of a single person working as manuscript scribe; writer of land charters; and possible author of a history of his order.Ever since the Croniken van der Duytscher Oirden has been studied in a scholarly context, there have been questions about the original composition. The original Middle Dutch version of the chronicle consists of a long prologue, a part that contains the deeds and lives of the Grand Masters of the Order, alternated by privileges granted by popes and emperors. It ends with a history of the regional bailiwick of Utrecht and its Land Commanders. Especially that last part is often put aside as a later addition to the chronicle. It is an interesting question, the more so since it has historical significance in the sense that it has an influence on the original function and aspiration of the text.To examine the relationship between the different parts of the text corpus we have collected, I employed several methods, but for now, I would like to focus on the use of the Delta Spreadsheets, freely made available by David Hoover (Hoover 2009). Some primary samples were selected from of the text corpus using the Intelligent Archive (Craig, Whipp and Ralston 2010) to create word frequency lists: the combined charters; the Sachsenspiegel; the table of contents of the Croniken; its prologue; some of the privileges; three parts of the Grand Masters’ part; and the bailiwick chronicle. These were computed against 2000 word pieces, overlapping and 500 words advancing. The results are shown in Figures 1, 2 and 3.  Figure 1: Moving Delta, Charters Full Size Image    Figure 2: Moving Delta, Saxons’ Mirror Full Size Image    Figure 3: Moving Delta, Croniken Full Size Image   What stands out is that in all three instances, the primary samples of the table of contents and the bailiwick chronicle (in light and dark blue) outperform the other samples, especially if one leaves out the samples in Figure 3 which perform off course very well in their own consecutive areas.What can be concluded from this observation? I think the table of content and the bailiwick chronicle best describe the personal writing style and orthography of Hendrik van Vianen. In both the charters, the copied Sachsenspiegel and the Croniken this layer is present, and it raises the question if he indeed can be held responsible for the bailiwick chronicle and the table of contents. The table of contents is clearly written at a later stage, added at the end of project, as is shown by watermark evidence and the distribution of some abbreviations and specific letter forms, quantified in Figure 4 and 5 below.  Figure 4: Different forms of letter “w”. The TOC in front corresponds with the latter part of the chronicle. Full Size Image    Figure 5: Use or absence of abbreviations in “ende” (Dutch for “and”). Full Size Image   Moreover, to what extent can Hendrik van Vianen still be held responsible for the rest of the chronicle? Was there an older exemplar of a chronicle available to him? It is hard to say for certain. There is a good possibility that he was not so much the – what we would now call – author, but more a compiler for large parts of the chronicle. The old source texts would then form another layer within the text. Again, in the Middle Ages it is not always possible (and even helpful) to make a strict distinction between the two. An old-fashioned approach to research the sources of the chronicles and how these source-texts were implemented could be a logical step further and will be one of the issues addressed in the remainder of this project.", "article_title": "Layer upon Layer. “Computational Archaeology” in 15th Century Middle Dutch Historiography.", "authors": [{"given": "Rombert", "family": "Stapel", "affiliation": [{"original_name": "Fryske Akademy (KNAW)", "normalized_name": "Fryske Akademy", "country": "Netherlands", "identifiers": {"ror": "https://ror.org/05fcmfe52", "GRID": "grid.450022.1"}}, {"original_name": "Leiden University, Netherlands", "normalized_name": "Leiden University", "country": "Netherlands", "identifiers": {"ror": "https://ror.org/027bh9e22", "GRID": "grid.5132.5"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "The Roman aristocratic funeral of the Republic was an incredible show. It packaged the Roman spectacular trifecta, the procession, the eulogy and the subsequent games, which comprised gladiatorial and dramatic performances. While each of these components of the funeral has received individual treatment—in the case of the gladiatorial games, extensive—no detailed, comprehensive discussion of the aristocratic funeral of the Republic exists. Moreover, before gladiatorial games were held in the Colosseum and before dramatic performances were staged in a monumental theater, they were first held in ad hoc venues in the heart of Rome. No attempt has been made to situate the phenomenon within its surrounding context, the Roman Forum. My current digital/analog manuscript project, Spectacle in the Forum: the Roman Aristocratic Funeral of the Middle Republic, offers the first attempt to study the mid-Republican funeral in its totality and, in so doing, examines the most significant aspects of spectacular stagecraft of the Roman Republic.Spectacle has received considerable attention in recent years, but its study has been marred by deficiencies in method. Classics scholar Richard Beacham pinpoints the problem: “Spectacle is three-dimensional and sequential, realized by taking place over a period of time, and its place, circumstance, and unfolding fundamentally shape what an audience both expects and experiences.”  Ritual parades, political speeches, and religious rites are well described in ancient texts and frequently depicted in art.  Yet, most spatial and spectacular analyses attempt to reconstruct the monuments, imagery, actors and audience, which are inherently kinetic and multi-dimensional (changing over space and time), by means of textual description and two-dimensional plans.The impact of monumental structures on Roman performers and their audiences, what could and could not be seen during their performance, as well as the significance of monumenta memoriae, directly affected the shows when first performed, and the reading and interpretation of the records subsequently examined by scholars. Performance “stages” of the mid-Republic were ephemeral: extant temple podia, elevated balconies, and hillsides, might serve as caveae. Simple temporary structures may have been all that was needed to mount a production.Three-dimensional digital models offer a partial solution. There are now a growing number of projects that have used computerized reconstructions to visualize Imperial Rome. There have been very few similar attempts to represent the Republican city, and hardly any that make scholarly arguments set within the digital reconstructions. Most reconstruction projects tend to focus on the creation of a highly accurate, extraordinarily precise digital model informed by scholarship as the ultimate goal. Instead, this project uses hypothetical reconstructions as a digital laboratory. By injecting historical context—the performers and the audience—into the digital environment, the digital investigation transforms the quantifiable elements of the ephemeral experience of ancient spectacle into a digital object fit for experiential analysis. It uses the hypothetical reconstructions as a digital laboratory to explore the staging of Roman spectacle and develop the digital toolset necessary for scholarly interrogation and publication of spatial and experiential arguments.The laudatio funebris of the mid-Republic was genre-defying visual theater. While it is now generally agreed that the persuasive techniques of oratory comprised verbal (the content and delivery of the speech) and visual elements (gestures charged with meaning and explicit visual and topographic references), the degree to which the choreography of the funeral eulogy subordinated the words of the speech has not been fully examined. For much of the audience, the visuality of the event eclipsed the aural content. The laudatio, like the pompa before it, relied on a basic set of quasi-formulaic visual cues to communicate with the audience, or at least, to communicate some ideas to some of the audience. To call the laudatio a speech alone, and to classify it within the realm of oratory without qualification is to misunderstand much of the purpose and the choreography of the event. In this presentation, I will put the event in its proper place: the Forum. Through the use of textual analysis, experiential investigation, and geo-temporal argumentation, I will demonstrate that the laudatio funebris was a multivariate theatrical event, comprising two discrete elements targeted at two distinct audiences.Though one can use a laboratory built out of virtual world infrastructure to experiment, a researcher cannot (yet) “publish” the entirety of a laboratory experience and call it scholarly communication. Rather, the laboratory is the space where the research occurs; the results must be woven together into a narrative in order to engage with the larger scholarly conversation. Nonetheless, a text and image narrative is insufficient to convey the totality of the kinetic and temporal subject matter. Geo-temporal argumentation presents an innovative and more robust method of idea dissemination by offering:When the experience and creation of kinetic transitions are fundamental to an understanding of an argument the reader must, quite simply, walk in the footsteps of the authors in order to participate in the debate, critique the result, and modify the conclusions. In this paper I aim to demonstrate that, for space- and time-centric, phenomenological investigations, geo-temporal argumentation is a new and superior form of scholarly communication.As is always the case, digital humanities projects are collaborative endeavors. My “manuscript” project provides the domain-specific area of inquiry, but the digital platforms that facilitate the research are part of two, larger collaborative efforts of which I am but one of a number of co-investigators.GeoTemporal Publication Platform: The research results and assessment will be published within HyperCities, a geo-temporal content aggregation and publication platform. Rather than create an entirely new digital humanities tool, “chapters” from my manuscript are being used as case-studies to guide the development of 3D narrative and mark-up tools within the HyperCities platform that will facilitate exploration of the data and publication of this new form of scholarly inquiry. We anticipate a mid-winter release of the working 3D system.", "article_title": "Geo-Temporal Argumentation: The Roman Funeral Oration", "authors": [{"given": "Christopher", "family": "Johanson", "affiliation": [{"original_name": "UCLA Classics", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "Research in narrative intelligence applies artificial intelligence approaches to study human ability to organize experience into narrative form (Mateas and Sengers 2003). Narratives are traditionally defined as “a series of temporally ordered clauses” (Labov 1972, p360-361). The time-centric approach leads to a lesser consideration of space in narrative construction and analysis. In contrast, we advocate a geospatial narrative in order to stress the importance of space and time in understanding the ordering and spatial interaction of events.We define a geospatial narrative as a sequence of events in which space and time are equally important. Narratives are stories that constitutes sequential organizations of events (Franzosi 2010). Each event in a narrative relates sequential or consequential occurrence in space and time. The conventional Geographic Information Systems (GIS) center on information about spatial states of reality, and temporal information is handled as add-ons to spatial objects. Alternatively, we conceptualize a narrative GIS that emphasizes representing and ordering events in space and time as well as functional abilities to construct meaningful geospatial narratives. While an event is a complex, fuzzy term, we start with one basic linguistic element of narratives: action, as the primitive data construct to start building a narrative GIS. By relating action events across space and time, a narrative GIS aims to discover spatiotemporal correlates among actions and relate actions across scales.Depending on the perspectives, there are many kinds of events, e.g. instantaneous events, discrete events, cyclic events, transitional events, and others. In contrast to TimeMaps (Farrimond et al. 2008), our vision of a Narrative GIS goes beyond spatiotemporal visualization to spatial analytics. By using action events as the primitive data constructs, a narrative GIS can support spatial queries of sequential and consequential actions. A Narrative GIS is therefore capable of revealing how time unfolds change and space unfolds interactions (Massey 2005).We use two distinctive corpuses of histories in building narrative GIS databases and narrative analytics as a proof of concepts: Dyer's Compendium of the War of the Rebellion and the Richmond Daily Dispatch. Frederick H. Dyers, a Civil War veteran compiled the Compendium based on materials from the Official Records of the Union and Confederate Armies and other sources. The compendium lists organizations and movements of regiment cavalries mustered by State and Federal Governments for services in the Union Armies. Collaborating with the digital scholarship group at the University of Richmond, we have started with four files from Dyer's Compendium: the 45th Massachusetts Infantry, the 107th Pennsylvania Infantry,the1stCaliforniaInfantry,andthe1stNewYorkCavalry. The Richmond Daily Dispatch was one of the primary news media in the south during the Civil War. The newspaper was one of the most widely distributed newspapers of the south and included news from the entire east coast. The Richmond Daily Dispatch retained the reputation as politically unbiased was published throughout the Civil War.Our idea of a narrative GIS consists of (1) semantic elements (who did what), (2) temporal elements (when), and (3) spatial elements (where). A geospatial narrative object integrates the three elements and enables search for and analysis of spatial and temporal relationships among narrative objects. Input data for narrative GIS vary widely from structured to unstructured sources. In this study, both input data are texts, albeit in very different structures. Dyer's Compendium concisely lists regiment movements. Richmond Daily Dispatch consists of news articles. Spatial and temporal connections among units in these texts are considerably different. Nevertheless, the conceptual framework of a narrative GIS demands the identifications of semantic, temporal, and spatial elements from the texts to form narrative objects and relationships. As such, our workflow includes six key steps: (1) extract text analysis units; (2) identify action verbs; (3) identify time for words and text units; (4) identify locations for words and text units; (5) combine all identified elements into a GIS database; and (6) build spatial and temporal relationships among narrative objects. A schematic view of the workflow is presented in figure 1  Figure 1 Full Size Image   We begin with electronic versions of the historical documents. The texts are split into subsets such as newspaper articles or book chapters for processing. These subsets are typically written as a unit and need to be analyzed that way for successful interpretation. For each processing unit, we apply natural language processing to tokenize sentences and identify parts of speech (e.g. verbs and nouns). The parts of speech provide important clues to extract information.The work presented here is centered on the location, time, and other characteristics of events. The part of speech tagging is used to identify verbs. We are most interested in “action” verbs and refine our list of potential candidates by removing stative and modal verbs. Location referencing begins with recognition of a standard grammatical structure to the way locations appear in text. In general, locations are proper nouns that do not directly follow a determiner (except for physical features).Candidate words are matched to all their possible real-world geographic referents in the “Gazetteer Matching” process. A number of different gazetteers are utilized in the matching including the US Populated Places gazetteer and State hydrography datasets from the USGS, historical counties, states, and territories files from National Historical Geographic Information System, and the US Census Bureau’s historical 100 largest cities dataset (US BGN; US NHD; NHGIS 2008; Gibson 2008). These data are assembled in GIS and each location is identified with a historically and spatially appropriate hierarchy. The names of geographic locations are often highly ambiguous. For example “Georgetown” has over 70 possible locations among U.S. cities. Disambiguating a word to its true location is an important and difficult task. A substantial amount of work has already been done on location disambiguation under the heading of “Toponym Resolution” (Leidner2007;Leidneretal2003). Figure 2 illustrates the steps in the location referencing process.  Figure 2 Full Size Image  The temporal processing steps aim to extract dates, durations of events and the relative temporal ordering between events. Historical texts contain temporal information in a variety of formats. Most obvious are explicit dates that include information such as the day, month and year. In addition, these texts often include clues to derive dates and relative ordering of events. For example words such as “yesterday“ or “last week” allow the date to be derived based on a temporal relation to an anchor date (Han 2006). Similarly, relative temporal expressions allow explicit dates to be determined based on temporal relations to the current temporal focus (Han 2009).Figure 3 outlines the steps in our approach.   Figure 3 Full Size Image  We begin by extracting anchor dates such as the date of publication for a newspaper article and explicit dates found in the text. We use temporal indicator words to refine the date of events and help establish temporal ordering. Explicit dates contained in the text are modified by deictic or temporal expressions. Semantic relationships between events are extracted based on semantic indicators. When all of the temporal information is relative and there are no explicit dates to give an explicit order, thirteen temporal relationships are used to find the temporal ordering (Allen 1983).Thus far, our effort has been focused on extraction of events from the natural language text sources, anchoring the events to geographical locations and in time, and extracting information on the actors and objects involved in the events. Figure 4 illustrates results from Dyer's Compendium of the War of the Rebellion.   Figure 4 Full Size Image  This figure shows the activity of the 6th New York Regiment Calvary in Maryland and Virginia in September and October of 1862. The processing identified the events including the regiment’s movements and splitting off of a reconnaissance mission from Lovettsville to Smithville while the main regiment moved to Kearnysville. While this example from a single source, it illustrates the potential for the system to support more complex geospatial narratives with the addition of information from other sources.Figure 5 shows a visualization of a Richmond Daily Dispatch article.  Figure 5 Full Size Image   The article describes a letter from a Union colonel to his family. It discusses the Union’s plan to move troops to Alexandria Virginia the next evening. The article illustrates that in addition to working with events that had already occurred the approach can also be used to help investigate the thoughts and motivation leading to events that had yet to occur.These examples of preliminary results demonstrate the basic use of a Narrative GIS. As we continue building the event narrative database, additional functions will be built in for narrative analytics. For example, we are interested in deciphering the local, regional and national processes on emancipation and to identify scalar effects on military, political, and individual processes. One approach will be extracting reports on battles and run-away slaves and analyze spatial and temporal correlations among these events. When we extract events of different categories in space and time, a Narrative GIS will allow us to analyze spatial and temporal relationships among these kinds of events to draw insights into the integration of multiple perspectives and interpretations of geospatial narratives.", "article_title": "Towards a Narrative GIS", "authors": [{"given": "John", "family": "McIntosh", "affiliation": [{"original_name": "University of Oklahoma", "normalized_name": "University of Oklahoma", "country": "United States", "identifiers": {"ror": "https://ror.org/02aqsxs83", "GRID": "grid.266900.b"}}]}, {"given": "Grant", "family": "De Lozier", "affiliation": [{"original_name": "University of Oklahoma", "normalized_name": "University of Oklahoma", "country": "United States", "identifiers": {"ror": "https://ror.org/02aqsxs83", "GRID": "grid.266900.b"}}]}, {"given": "Jacob", "family": "Cantrell", "affiliation": [{"original_name": "University of Oklahoma", "normalized_name": "University of Oklahoma", "country": "United States", "identifiers": {"ror": "https://ror.org/02aqsxs83", "GRID": "grid.266900.b"}}]}, {"given": "May", "family": "Yuan", "affiliation": [{"original_name": "University of Oklahoma", "normalized_name": "University of Oklahoma", "country": "United States", "identifiers": {"ror": "https://ror.org/02aqsxs83", "GRID": "grid.266900.b"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}, {"url": null, "identifier": {"string_id": null, "id_scheme": null}, "abstract": "The problem of overlapping structures has long been familiar to the digital humanities community. Early on, the purely hierarchical structure has been identified as an important shortcoming of XML for the representation of meaningful features of complex works in the humanities. For example, the verse and line structures of a poem do not necessarily nest properly, but may overlap. To capture that complexity, it is necessary to host both structures in the same digital object, something which pure XML cannot do directly.It is customary to represent the structure of a marked-up document by deriving from it a graph in which the adjacency relation among nodes corresponds to the embedding relation among the marked-up elements of the document. It is well known that, through such a construction, XML documents correspond to the subclass of graphs known as trees.Numerous approaches to extend or specialize XML or SGML to handle overlapping structures in various ways have been proposed in the literature over the years (see for instance [B1995]). Some of these approaches work on the graph-structure view of XML. The graph-structure is no more required to be a tree, but it must still satisfy basic constraints (e.g., finiteness, non-circularity) that make them plausible models for “documents.” One such proposal is the GODDAG (General Ordered-Descendant Directed Acyclic Graph [SH2004]). Roughly, GODDAGs are like XML trees except that they allow multiple parenthood and do not require a total ordering on leaf nodes. (Thus, XML trees constitute a subset of GODDAGs.)Other proposals address the problem by working on the markup language side. In this group, we find pure XML solutions (which represent the non-hierarchical structures through coding conventions and a corresponding ad hoc layer of semantics, e.g., milestones, fragmentation, virtual elements, etc. [B1995] [SH1999] [W2005]) and non-XML ones, based on generalizations of XML allowing non-embedding constructs, such as overlapping elements. Among the latter is TexMecs [HS2003], an XML-like markup language allowing overlapping elements (as well as other constructs).An interesting question is how the various proposals compare with respect to their expressive power. Beyond a few obvious subset/superset relations that exist among the formalisms, very little is known in this area. In 2008, Marcoux established a result [M2008] linking a subclass of GOODAGs to a subset of TexMecs. Overlap-only TexMecs (or oo- TexMecs) is the subset of TexMecs that uses only embedding and overlapping elements (and none of the other constructs). A graph whose structure corresponds to an oo-TexMecs document is said to be oo-serializable. Marcoux showed (essentially) that the oo-serializable GODDAGs are exactly the completion-acyclic ones, thus showing that oo-TexMecs and the class of completion-acyclic GODDAGs have the same expressive power.In this paper, we extend that result in two ways. First, we define child-ordered directed graphs (CODGs), a class of graphs strictly larger than that of GODDAGs, and argue that, in spite of its generality, it still captures a plausible and interesting notion of “document.” Then, we show that the strictly stronger expressive power of the CODG, compared to the GODDAG, vanishes when oo-serializability is required. This constitutes a strong indication that overlap-only markup languages may be insufficient for representing complex document structures.More precisely, we show that the classes of single-root CODGs and GODDAGs coincide when restricted to completion-acyclic graphs. There are, however, completion-acyclic multiple-root CODGs that are not oo- serializable. We show that, for multiple-root CODGs, the stronger condition of full-completion-acyclicity characterizes oo-serializability.The definition of fully-completion-acyclic graph does not in itself suggest an efficient algorithm for checking the condition, nor for computing a corresponding overlap-only document when the condition is satisfied. We present ideas that could be exploited to accomplish those tasks efficiently.The main conclusion of this work is that markup languages that generalize XML only by allowing overlapping elements may not be expressive enough to represent complex document structures. Indeed, our results show that the requirement that a graph be serializable in an overlap-only language effectively prevents the presence of too complex structures in the graph. The new graph structure introduced in the paper, the CODG, is of interest in itself as an abstract document model. Finally, the ideas we present on how to detect whether a graph is serializable using only overlapping elements and on how to then compute a serialization would be helpful in authoring environments, because they would allow serializing documents using the most simple constructs (e.g., overlapping elements) whenever possible, while resorting to more complex constructs (e.g., virtual elements) only when absolutely necessary.", "article_title": "Expressive Power of Markup Languages and Graph Structures", "authors": [{"given": "Yves", "family": "Marcoux", "affiliation": [{"original_name": "Université de Montréal, Canada", "normalized_name": "University of Montreal", "country": "Canada", "identifiers": {"ror": "https://ror.org/0161xgx34", "GRID": "grid.14848.31"}}]}, {"given": "Michael", "family": "Sperberg-McQueen", "affiliation": [{"original_name": "Black Mesa Technologies", "normalized_name": null, "country": null, "identifiers": {"ror": null, "GRID": null}}]}, {"given": "Claus", "family": "Huitfeldt", "affiliation": [{"original_name": "University of Bergen, Norway", "normalized_name": "University of Bergen", "country": "Norway", "identifiers": {"ror": "https://ror.org/03zga2b32", "GRID": "grid.7914.b"}}]}], "publisher": null, "date": "2011", "keywords": null, "journal_title": "ADHO Conference Abstracts", "volume": null, "issue": null, "ISSN": [{"value": null, "type": null}]}]