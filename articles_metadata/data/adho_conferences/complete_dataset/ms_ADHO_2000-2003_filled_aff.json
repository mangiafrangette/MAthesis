[
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Documentation is arguably the most important part of a humanities computing project's long-term existence, in two senses. First, in the sense that without it a project cannot maintain continuity and consistency; and second, in the sense that without it a project cannot communicate its methods to other members of the larger community, offering them for critique if in need of improvement, and making them known if worthy of emulation. Without documentation a project is effectively without \"self-knowledge\", by which I mean the information which the project itself as an entity needs to know in order to survive and to perpetuate itself. It is crucial to distinguish here between the knowledge belonging to individual participants in the project's work, and the project itself. What individuals know is not necessarily accessible to other project members, and this knowledge is taken away with them when they leave. What the project knows, on the other hand, is an explicit part of its internal and public existence, with several important consequences. It can be found without recourse to private knowledge; it does not depend on any individual and is not vulnerable to changes in staff. And finally, this kind of self-knowledge has a particular rhetorical status within the project and as a public expression of identity, in that it has an understood authority: it is explicitly endorsed by the project and its truth or applicability are not in question. This is as much as to say that we need to take documentation seriously not only as a practical matter but also as a question of theory. Producers of documentation must negotiate between different rhetorical scenarios, from the didactic, developmental narrative of a training manual to the encyclopedic granularity of a reference guide. Treating this negotiation as a rhetorical problem seems to locate it in the writing itself. But in fact we can see, once we look more closely, that documentation is a specialized kind of data or content to be purveyed, and that these different scenarios amount to different approaches to data retrieval which must be accommodated. This complexity is compounded by the fact that we are concerned here with humanities computing documentation, to be used by humanists, with humanistic expectations about the relationship between that which is documentable - reproducible, deterministic, normative - and that which is subject to independent judgment and expertise. Finally, the challenges documentation poses - its peculiar embodiment of the Arnoldian tension between \"Hebraizing\" and \"Hellenizing\", between doing and thinking - also resonate with issues central to humanities computing. Documentation both embodies a project's self-reflection and calls it to a close, requires that reflection conclude in order that action may commence. And yet the normative statements which documentation strives to offer about a project's practice are inevitably, in a project of any scope, the occasion for discovering further issues which have not yet been decided. The perpetually unfinished work of documentation thus holds the project in a state of dynamic suspension, always trying to resolve issues and get back to work, always trying to finish work so that it can be documented. Some specific points are worth noting here, to be discussed at greater length in the finished paper. First of all is the issue already mentioned of the relationship between training documentation and reference documentation. The most apparent difference between these two forms is the kind of text being produced: in the first case, a developmental narrative which takes the trainee through the project's methods from basic to advanced in a way which maximizes memorability and comprehensibility; in the second case, a reference work which provides instant access to particular topics discussed as independent items with a high degree of granularity. These two modes are so different that it is often extremely difficult to convert from one to the other, increasingly so in proportion to how successfully the given mode has been realized. They also require quite different kinds of infrastructure to make them useful in the work environment: for instance, the reference model works best when accompanied by good metadata and a good retrieval system. It also requires attention to the level of granularity at which individual instructions are conceptualized, and to how related instructions will be identified and aggregated. Producing documentation in either mode requires that one conceptualize the consumer's needs and habits in detail, and this raises a second point which has already been mentioned above. What role do humanists allot to documentation, broadly considered? This question points to a larger issue for humanities computing, namely the role of judgment and interpretation in the creation of humanities data, and in what areas the exercise of these things is appropriate. If the documentation is framed for a workplace in which comparatively unskilled workers require explicit instructions on making absolutely consistent choices, then the documentation itself needs to be equally determinate, authoritative, and exhaustive in the way it communicates. It must anticipate every alternative and leave no opening for variation; from the retrieval standpoint, it must ensure that the correct information is always discovered no matter how inept or tangential the search strategy. In short, it must make the work resemble as little as possible the kind of intellectual environment envisioned by a liberal humanities viewpoint. On the other hand, if the intention is to guide the worker in exercising judgment - that is, to indicate the principles to be applied rather than the act to be performed - then the documentation will necessarily imagine its readers as part of an ongoing investigation into the project's methods and standards. To give these reflections some concreteness, the finished paper will also consider an actual documentation system currently in use in a major text encoding project, which has evolved over a period of seven years and is used both for training and reference. Although developed for a particular set of needs and by no means perfect, this system and the process of its development offer an example which may be of use to people currently designing or redesigning documentation systems of their own.  ",
       "article_title":"Writing about It: Documentation and Humanities Computing",
       "authors":[
          {
             "given":"Julia",
             "family":"Flanders",
             "affiliation":[
                {
                   "original_name":" Brown University, USA  ",
                   "normalized_name":"Brown University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05gq02987",
                      "GRID":"grid.40263.33"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Text Encoding"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   In this paper we present a hypermedia design and production methodology integrating markup languages and object-oriented techniques. This methodology tries to solve the main problems presented during hypermedia development, improving the communication between customers (content providers and interaction designers), and developers (software designers and programmers).   1. Introduction Hypermedia production is a complex and costly task with a specific need to involve experts of very different fields during all the phases of the software development. Usually in traditional software production, the customer gives the functional and operational requisites to developers, who in a very independent way implement the program. On the contrary, in hypermedia production we need more implication of the customer providing all the information needed to build the application. We have learned all these lessons during the construction of Galatea, an educational hypermedia for French text comprehension, developed in collaboration with a linguists team (complementary information about Galatea can be found in [Fernandez-Manjon et al. 98]). This customer presence, providing contents and interaction, originates a severe problem in the design phase. In this phase we need a systematic and well-defined formalism to represent the application in an abstract way that facilitates the relationship between customers and software designers. Two main approaches have been used to solve this problem. One solution is the use of hypermedia models in the design and development of these applications. The other one is to use object-oriented diagrams to cover the design phase. In this paper we analyze both approaches [Navarro 98], and propose a production hypermedia methodology integrating hypermedia models and object-oriented techniques. Our methodology [Navarro&Fernandez-Manjon 00, Navarro&Sierra 00] tries to solve some of the problems identified in previous approaches facilitating the interaction between customers and developers, easing the code generation based on design phase, and improving the application maintenance.   2. Hypermedia models and object-oriented design and development techniques Hypermedia models as Dexter Hypermedia Model [Halasz and Schwartz 94], Amsterdam Hypermedia Model [Hardman et al. 94], Hypertext Abstract Machine HAM [Campbell and Goodman 88], Hypergraph Model [Tompa 89], Trellis Model [Stotts and Furuta 89], and Hypertext Design Model HDM [Garzotto et al. 93] present important advantages [Garzotto et al. 93], and a few drawbacks. They are closed systems, making it impossible (or very difficult) to include complex computational activities in the hypermedia application, if the model doesn't support this activity; some of them are too hard to be managed by non computer science people; and none of them enables the design of an hypermedia application centred on the information structure, with a real independence of the presentation structure. Object-oriented software development methodologies, as Booch [Booch 94] or UML [Rumbaugh et al. 98], are extensively and successfully applied in computer projects development, because they improve software quality and maintenance, but these methodologies also present some drawbacks. They are not primarily intended for the development of hypermedia applications; and they use diagrams which are valuable for software designers, but very difficult to be understood by a customer team. Our approach combines ideas for both domains, and integrates them through the use of XML. XML, the Extensible Markup Language [W3C XML], is the evolution of the first attempt to represent markup languages in a standardized way, SGML, the Standard Generalized Markup Language [ISO/IEC SGML]. XML is based on descriptive markup (the tag semantic is not specified in the tag definition); the separation between the structure, content and treatment of a document; and the platform independence. To achieve these goals XML defines the set of tags that conform to the markup language (that is the document structure) through an XML construction called DTD (Document Type Definition). This DTD is the grammar that formally describes the structure of a class of text, and a document that includes the DTD tags to structure its content is called an instance of the DTD.   3. Our approach As previously stated, our methodology tries to solve the problems identified in previous approaches, improving the communication between customers and developers. In our approach, developers are divided into software designers that must provide a representation of the application (code independent), and programmers that translate this representation into real code. Customers also play a double role. They are the content providers that organize the knowledge included in the application (this knowledge has a double structure: natural and hyperlink structure), and they are the interaction designers who decide the time and space of content presentation. Interaction between the content providers team and the software designers team is one of the problems that our methodology solves. We use an XML DTD, called the content DTD, to represent the contents of the application, and the hyperlinks between these contents. The content provider team describes the structure of the contents (using natural language), and the software designers team use this information to build the content DTD. Then, the content provider team generates an instance of this content DTD that organizes the contents of hypermedia application in a formal way. The use of meaningful tags, and the inclusion of attributes (properties) in these tags, solves the problem of content providers and software designers interaction.  Our methodology also eases the communication problems between the interaction designers team and software designer team. We use another XML DTD, called the presentation DTD to characterize the presentational structure of hypermedia applications. The elements of the presentation DTD describe the application presentational elements (screens, windows, buttons, etc.) and the hyperlinks between them. This DTD is common to all (or most) hypermedias, and is provided by the software designers team. Moreover we are working in the assignment of a concrete semantic to the presentation DTD, based on an object-oriented windows class hierarchy, to provide a consistent connection between the markup view, and the object-oriented view of our methodology. This separation from content and presentation provides the means to associate different presentations with the same content. The relationship between content DTD and presentation DTD is accomplished through the overmarkup. Overmarkup basic idea is a very simple one: when we build the instance of the presentation DTD, to describe the presentation and interaction framework of the application, the elements of the content DTD are the content of the elements of the presentation DTD.  We apply this overmarkup in two phases. In phase 1, structural overmarkup, there are no real contents, and when the interaction designers (helped by software designers) build the instance 1 of the presentation DTD, the elements of the presentation DTD overmark the elements (only the name of the element) of the content DTD, enabling a better understanding of the structure of the application. In phase 2, content overmarkup, when the interaction designers (helped by software designers) build the instance 2 of the presentation DTD, the elements of the presentation DTD overmark the instances of the elements of the content DTD (the real content) to represent the final hypermedia application. If we need to represent some complex computational activity in the application (for example an exercise that evaluates the learner knowledge) we use object-oriented diagrams (mainly class and state transition diagrams) that are attached to instance 2 of the presentation DTD. This instance 2 is what we call the application design document, and provides a real representation of the total application used by customers and programmers. Customers (content providers and interaction designers) use the design document to evaluate if it conforms to its requirements, and make any change (obviously they ignore the object-oriented diagrams). Programmers use part of this document in the coding phase directly, whereas other parts represent the application design that they must translate in real code. This task is facilitated by the relation between presentation DTD and real object-oriented code, and facilitates the maintenance of the final application.   4. Conclusions and future work We think that our approach provides a solution for the development of hypermedia applications, solving the problems of hypermedia models and object-oriented construction techniques. Indeed our solution is not closed (we have integrated all the power provided by object-oriented development techniques), and is specifically created to deal with hypermedia software. Our experience in the Galatea development has showed us that XML markup (and its supporting tools) is easy enough to be used by customers (a similar approach is used in [Nanard and Nanard 95]), and the design phase is totally covered by overmarkup. Content and presentation DTD improve the communication between customers and developers, and provide the means to capture the content and presentation structure in different stages. Overmarkup phases integrate these structures: structural overmarkup represents a fast application \"prototype\", and the design document is a complete application representation that solves the interaction problem between customers and software designers. Moreover we can use the structure provided by the presentation DTD to generate part of the object-oriented code (improving the communication between software designers and programmers), and the existence of the design document facilitates the application maintenance. Present work includes the total assignment of an object-oriented semantic to elements of presentation-DTD. The next step is the development of a CASE tool that facilitates the overmarkup process, and that provides different views of the application (overmarkup view, window view - an application preview - and object-oriented view).   ",
       "article_title":"Integration of Markup Languages and Object-Oriented Techniques in a Hypermedia Methodology",
       "authors":[
          {
             "given":"Antonio",
             "family":"Navarro",
             "affiliation":[
                {
                   "original_name":" Universidad Complutense de Madrid, Spain  ",
                   "normalized_name":"Complutense University of Madrid",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02p0gd045",
                      "GRID":"grid.4795.f"
                   }
                }
             ]
          },
          {
             "given":"Alfredo",
             "family":"Fernandez-Valmayor",
             "affiliation":[
                {
                   "original_name":" Universidad Complutense de Madrid, Spain  ",
                   "normalized_name":"Complutense University of Madrid",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02p0gd045",
                      "GRID":"grid.4795.f"
                   }
                }
             ]
          },
          {
             "given":"Baltasar",
             "family":"Fernandez-Manjon",
             "affiliation":[
                {
                   "original_name":" Universidad Complutense de Madrid, Spain  ",
                   "normalized_name":"Complutense University of Madrid",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02p0gd045",
                      "GRID":"grid.4795.f"
                   }
                }
             ]
          },
          {
             "given":"Jose",
             "family":"Sierra",
             "affiliation":[
                {
                   "original_name":" Universidad Complutense de Madrid, Spain  ",
                   "normalized_name":"Complutense University of Madrid",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/02p0gd045",
                      "GRID":"grid.4795.f"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Text Encoding"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction The workbook facility for scholars in the humanities aims to help them organize the results of their research in a convenient and easily accessible manner. The recent proliferation of marked-up electronic corpora has produced a need for tools that would allow structured search and extraction from texts. However, not all potentially interesting features of a text can be described in terms of the mark-up hierarchy: some features involve overlapping elements of markup, others are too fine-grained to be marked up. Thus we need a mark-up independent method of searching, and tools that combine both. The text region-based approach to processing digital text resources is the proposed method of searching and extracting both marked-up and non marked-up information from a collection of texts. The workbook facility is a prototype application of this method that allows extraction and linking of portions of XML-formatted texts. The selection of the regions can be based either on their structural characteristics or on other features.   Concepts A text region (a.k.a. span) is a continuous portion of a document identified by its start and end offsets. It can be a complete XML element or just a string of characters. A text region can be created through a variety of operations described below. In our use of the concept, most XML elements are assigned a unique identifier within the document. The offsets for text regions are therefore relative to the nearest preceding ID within the document. A text occurrence object (TOO) consists of one or more text regions as well as notes and a user-defined name. The text regions can come from one or more documents and do not have to be contiguous. SGREP (structured grep) is a command-line search tool. It allows structured searches on XML and SGML formatted texts and collections of texts, as well as simple searches. The search results are returned as a set of text regions that can be organized into a text object occurrence. SGREP allows nested searches (for example an XML element labeled \"verse\" containing the word \"Hamlet\" within a DIV1 element) as well as unions and intersections of search expressions.   Workbook Facility The workbook facility aims to help humanities scholars to organize the results of their research in a convenient and easily accessible manner. It provides a way to bookmark and annotate documents without changing the original texts, and to store and link annotated extractions from texts. The workbook consists of a set of TOO's, each of which contains one or more text regions that can originate from different documents. A TOO can thus link portions of texts from different places in a document or from different documents. The workbook facility has a built-in XML parser that creates a DOM structure. We are using IBM's Java-based parser, and the rest of the workbook is also written in Java. The following operations are available to the user: Select a collection of XML documents with which he wants to work (the base collection). For each document, view the raw text or the DOM (Document Object Model) structure resulting from parsing the XML document. The DOM structure is displayed in a tree control. Create text occurrence objects in several different ways. Select a complete XML element from the DOM, in which case the TOO will consist of a single text region. Select any continuous portion of raw text from any document in the collection to create a TOO. Run an SGREP search on one or more documents in the collection. If the search is successful, SGREP will return one or more text regions which will be put together into a TOO. Name and annotate all TOO's in the same manner, regardless of how a particular TOO was created. The list of TOO's constitutes the workbook and is displayed separately. By clicking on any text region within a TOO, view the spot within the document from which the text region originated. Thus, it is possible to view the larger context of a particular extraction. Produce a word distribution list for any particular TOO, and compare word distributions between several TOO's. The word distribution list can be sorted by relative frequency of words or by alphabetical order, and two lists can be viewed side by side. Order and re-order the TOO's within the workbook. Organize the TOO's into folders with arbitrary depth of nesting, similar to how files are organized on a disk. Save the workbook (i.e. the individual TOO's along with folders or TOO's) and re-open it later. The ability to return to the original documents remains after the workbook has been re-opened. Save particular TOO's and sets of TOO's as separate XML documents and include them in the base collection. Run SGREP searches on documents created from parts of the workbook in the same way as on the original ones.    Impact The proposed workbook facility will be useful for several research goals. Extracting, ordering and naming textual fragments is a convenient way for an instructor to prepare for a lecture about a particular text. Scholars that study different versions of the same text (i.e. versions in different languages or different editions) can use the workbook to link parallel passages and annotate the resulting TOO. Since the creation of text regions can be markup-independent, this can be done even if the parallel passages in two documents are not contained within a single XML element. Moreover, extracting regions that share particular features can be automated with the help of SGREP. The word distribution feature of the program is intended to demonstrate that operations found in software like TACT and similar tools can be easily integrated with our workbook approach. Once a workbook is created, it still contains links to the original documents and the history of how the extractions were made. That is, the process is completely retraceable, and the user can view the context from which any text region came.   ",
       "article_title":"A Workbook Application for Digital Text Analysis",
       "authors":[
          {
             "given":"Worthy",
             "family":"Martin",
             "affiliation":[
                {
                   "original_name":" University of Virginia, USA  ",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          },
          {
             "given":"Olga",
             "family":"Gurevich",
             "affiliation":[
                {
                   "original_name":" University of Virginia, USA  ",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          },
          {
             "given":"Thomas",
             "family":"Horton",
             "affiliation":[
                {
                   "original_name":" Florida Atlantic University, USA  ",
                   "normalized_name":"Florida Atlantic University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05p8w6387",
                      "GRID":"grid.255951.f"
                   }
                }
             ]
          },
          {
             "given":"Robert",
             "family":"Bingler",
             "affiliation":[
                {
                   "original_name":" University of Virginia, USA  ",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Text Encoding"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   WebCAPE is a web-based implementation of the BYU Computer-Adaptive Placement Exam (CAPE) series. These exams use adaptive procedures to assess language ability, drawing from a large bank of calibrated test items. Tests are administered from a web server computer through the internet to a browser application on students' computers. Test security in WebCAPE is maintained through a combination of application design and standard web methods.   Background Students entering a university language program come with a wide range of previous language training and experience. Thus, determining which class students should enroll in becomes an enormous task for language departments. A placement exam can be used, but paper-based standardized placement exams bring their own headaches: students have to be brought in at a fixed time and place, the test takes a long time to take and everyone has to wade through all the questions, and then you have to wait while the tests are sent in for scoring and for the results to come back. The Humanities Research Center at Brigham Young University has developed a set of language placement exams that overcome these problems. The exams are delivered by computer and thus do not require a lock-step controlled environment. The exams are adaptive, effectively eliminating questions far above or below the students' ability. And since questions are drawn from a large bank of test items, each student gets what amounts to a unique test, thus avoiding problems with test security. The computer-adaptive approach also means the test need only take long enough to determine a particular student's ability level and produces a placement score on the spot. Underlying the BYU CAPE (Computer-Adaptive Placement Exam) application is a large bank of calibrated test items. Initially, several hundred questions were written, testing a variety of language skills: vocabulary, grammar, reading comprehension, etc. These were then statistically calibrated for difficulty and discrimination among ability levels. Test item banks have been developed for Spanish, German, French, Russian, and most recently, English as a Second Language. Originally CAPE tests were implemented as individual application programs. But more recently, a new implementation has been developed for an internet/web environment. This version, called WebCAPE, uses a web server for the core functionality and test item banks, but administers the actual tests over the internet through a standard browser application. Thus WebCAPE tests can be given on any computer with an internet connection and Netscape 4.0 or equivalent.   How It Works Students enter WebCAPE through their school's menu page. This page is customized for the school, incorporating their seal or logo, a background campus scene, and school colors into the page design. Page content includes a brief explanation of the tests and how the school utilizes their results, along with basic instructions for taking the test. When students select their language from the menu page, they go to a registration page. This page is also specific to the school. Students enter their identification information in the top section of the page. They may also enter their e-mail address for an e-mail copy of their test results. Clicking the Begin Exam button takes students into the actual exam. After some initialization, a new browser window opens to the exam environment. This is served completely from the WebCAPE server and is the same for all tests. The top frame contains title information. The middle section displays the current test item. The bottom frame contains the exam control panel where the students indicate their answer, then click Confirm Response to register their response. First, the exam presents six level check questions selected from the full difficulty range. Based on these answers, the algorithm computes a preliminary ability estimate. It then begins to probe with questions to fine-tune that estimate. In essence, it presents harder and easier questions until it can focus to a statistically reliable value. On average, the entire testing process takes 20-25 minutes. When the exam finishes, students are returned to the registration page and their results are posted in the bottom section of the page. Here their final ability estimate is mapped to a recommended course by reference to a table of cut-off points established by the school. Beginning and ending time-stamps are also posted, for validation and timing purposes. In addition, the exam returns details of the students' session. These are not normally displayed for the student to see, but are sent to the school for analysis. As a final step, students click Submit Results. This generates an e-mail message with all of the information to the school/department and a summary message to the student's e-mail address. A confirmation page is also generated by this process, which in turn takes them back to the menu page, ready for the next student.   Security WebCAPE is designed to be reasonably secure for its intended use. Access control is maintained through the menu page, registration pages only accept entrance from a corresponding menu page, and the exam environment can only be entered from a properly configured registration page. Both are maintained on the WebCAPE server and isolated from outside access. The host school controls access to their menu page, typically by either setting up links or bookmarks in the lab where they administer the exam, or by only giving the URL to their properly registered students. When needed, the WebCAPE server can also be configured to restrict access to a particular IP address or range (a designated lab, for example), or to require a userID and password. Test security is maintained mostly by the design of the page set. Test items are individual html files that do not contain answer information or other clues. The answer key is read into the programming of the control frame when it loads. And should a student manage to hack into the answer table, there is no way to identify which answer goes with which question. At the server level, hackers are thwarted by the built-in resistance of the server and operating system configuration. Foolproofing security is more problematic. Because it uses a standard internet browser, WebCAPE cannot keep students from doing things the browser allows: like closing the window or using the back button or even quitting the browser program entirely. All that can be done is to write clear instructions and incorporate warning alerts. This suggests administering WebCAPE tests in a controlled environment (like a student lab) with a standardized browser configuration and a proctor to monitor things. The biggest vulnerability to cheating in WebCAPE comes at the student workstations. The exam cannot prevent students from using a dictionary, getting help from friends or even having someone else take the test. This clearly demands a proctored environment. WebCAPE test security measures mentioned thus far try to prevent cheating to get a higher score. But students taking a placement test may also try to get a lower score. The current version of WebCAPE does not directly address this problem. However the results message it sends to the department does include details of the test session, which can be analyzed when needed - an alert human would immediately suspect a test with all-wrong answers. Other suspicious patterns like all-one-letter answers or only a few seconds between answers can also be easily recognized by a human. The next version of WebCAPE will probably include processing to at least flag patterns such as these. Ultimately there may still be a few students that get misplaced, either by faking out the test or by slipping through the statistical margin of error. These will still have to be dealt with by human administrative procedures. But WebCAPE should keep the number small enough to manage.   Implementations WebCAPE is implemented as a service rather than as a program package. Schools pay for access for their students to take the tests rather than buy the program itself. For most schools, the best alternative is a flat fee for unlimited tests, but a lower-cost option for a fixed number of tests is also available. In each case there is also a one-time setup fee for creating the customized menu and registration pages. Planning is underway to also implement a pay-per-test entrance to WebCAPE. This would allow someone to take the test on their own initiative to see how they might place into college-level courses. A third configuration as a high school exit exam has been proposed. This would be for high school language programs, allowing students to find out where they would place into college courses.   Future WebCAPE is currently available for French, German and Spanish, with Russian to be added shortly. For all four languages, the test items are strictly text-based questions. While different test items assess different aspects of language ability, they are still confined to written text. But the latest CAPE exam under development, English-as-a-Second-Language, goes beyond that text-only limit. ESL-CAPE incorporates sound clips into many test items and thus adds listening comprehension to the language skills it assesses. ESL-CAPE also has an option to calculate separate ability estimate scores for grammar, reading comprehension, and listening comprehension - the other exams only give a composite estimate. A stand-alone implementation of ESL-CAPE is now being piloted. Web implementation will be ready for production testing next year.   ",
       "article_title":"WebCAPE - Language Placement Testing over the Web",
       "authors":[
          {
             "given":"Charles",
             "family":"Bush",
             "affiliation":[
                {
                   "original_name":" Brigham Young University, USA  ",
                   "normalized_name":"Brigham Young University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047rhhm47",
                      "GRID":"grid.253294.b"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "The Electronic Classroom"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction This paper discuses the results of the application of a methodology typical to information science to humanities computing. User studies are widely performed in the library and information science community. However, although some research has been carried out into the information needs of researchers in the humanities, very little research has been done into the actual use of electronic resources. (Warwick, 2000). Modern languages is an area in which the usage of electronic resources in teaching is known to be widespread. Yet the recent HEFCE report (1998) found that over one third of universities felt that computer-assisted learning (CAL) and information and communication technology (ICT) resources were being under-utilised. It concluded that there is a need for more research into their use in HE and recommends a \"focus on the information and knowledge needs of the real end-users\". This paper seeks to address this need, and considers the way that both teachers and students of Modern Languages use electric resources, and what their perceptions about them are. It is based on work conducted in the department by myself and a Masters student as part of her dissertation. (Pine-Coffin, 1999). We argue that such research is an important contribution to the area of humanities computing, since without an accurate idea of the way in which resources are used and perceived it is impossible to tell whether computer methodologies are useful and successful in aiding teaching and learning. Without this type of user study it is difficult to plan for possible future developments. Methodology Research was of a mainly qualitative nature, and was undertaken by means of structured interviews and questionnaires. Questionnaires were given to students and both students and academics were interviewed. Three university Modern language departments were chosen as a sample, all of which were identified as having links with humanities computing projects, Sheffield (Humanities Research Institute), Hull (CTI Modern Languages) and Exeter (Project Pallas).   Results and discussion Use of ICT: We found a surprisingly small amount of computer use by students. They used computers mainly for Word Processing, reading foreign newspapers and accessing the internet. Despite the advice available from the excellent CTI centre at Hull, the use of CAL packages at all universities surveyed lagged behind all these generic applications in terms of frequency of use. Despite promotion by libraries, there is also a worryingly low level of use of library web pages and of subject gateways, of BIDS and other bibliographic packages. Attitudes to ICT usage: Preferences in terms of computer use (ie what applications the students liked using) do not always mirror frequency. For example, students found they often needed to use Word Processors, but did not especially enjoy doing so. We also found that despite what academics tend to assume, their students do not necessarily enjoy using electronic resources. The 'Nintendo generation' is, it appears, still technophobic and surprisingly conservative in its preference for paper resources. We also found that students make 'tactical' use of resources. Despite the perception amongst lecturers that students will enjoy playing with computers, once introduced to them, we found that they tended to use them only to the extent that they had been convinced of the necessity of doing so. If they became convinced that they could pass an assignment by limited use of an electronic resource, they were often unwilling to explore further or practise the use of it, even when some packages had been specifically designed for certain courses.   Departmental and academic attitudes A constant theme of the research was that student reluctance to use electronic resources can be combated, at least to some extent, by the recommendations of their lecturers. This is not always easy to achieve, however, as the attitude of academics themselves is vital. We found some interesting and imaginative use of ICT, whether in the form of internet usage or of CAL tutorials. It is perhaps not surprising that CAL is what academics used most enthusiastically, since it could be used in a unique fashion which printed resources could not replicate. However, we also found a lack of awareness of ICT amongst academics, many of whom were also wary of computer use. Many felt there was little incentive to use ICT when traditional resources were adequate for the job in hand. With multiple demands on their time, computer sceptics were also unwilling to give up time to learn new ICT skills. Even those who were enthusiastic about the use of computers were wary of publishing their research in e-journals. They expressed anxiety about whether conservatism on RAE panels would lead to electronically published articles being dismissed as insufficiently prestigious. Some expressed a view that older, more established scholars who tended not to use computers were likely to be on RAE and promotion panels, and so computer enthusiasts might find their work was undervalued. This all led to a sense of conservatism in research, even if in teaching they tended to use ICT more widely.   Support Even if students felt that the department was encouraging them to use ICT, the most important factor in its successful use was support. However, awareness of what support was on offer was still low, as was takeup of it. Students often hesitated to ask for help, even when aware of it, and chose always to ask friends for help in the first instance. They tended to prefer human advice to online help and would rather receive help from their academic tutors than computer support professionals. They tended to assume that lecturers were more important than 'some bloke from the computing services' who came in to show them how to use a package. Thus, if a lecturer could demonstrate use of ICT him/herself, students tended to presume that this was indeed important. The opposite assumption was also made, although lecturers were often unaware of the messages they were involuntarily delivering. We also found a large discrepancy between the level of support which students perceived they needed, and which academics thought was acceptable. Academics tended to assume that most students would cope easily with computer use, because they thought that all teenagers were computer enthusiasts who has been trained to a high level of ICT skill at school. Many students, however, felt that the amount and level of support was too low, and that they needed far more help than they received. In general the level of confidence which students expressed in the use of ICT was surprisingly low. Some students also felt that they lacked skills in important areas such as internet searching. The librarians we interviewed were aware of this problem, but the university lecturers tended not to be. Unfortunately, the librarians were pessimistic about their role in the official teaching of such skills, since they felt that students and academic staff alike tended to undervalue the skills they had to offer, and there seemed to be few channels of communication from librarians to academics.    Conclusions The paper will discuss various conclusions which may be drawn from this data. The most important one, however, seems to be that despite enthusiasm about the potential of ICT in modern languages on the behalf of some academics and many humanities computing professionals, there are several problems in its practical implementation. It is only when user surveys are performed that such problems come to light, and we discover the reality of the situation, as opposed to what we think ought to be happening. Another strength of the technique is to discover attitudes that users have to the technology available, and we will argue that this is vital, since attitudes must shape the way in which computers are used. Our research has also uncovered a significant discrepancy between assumed and actual levels of usage and enthusiasm. It is clear that despite the expectations and assumptions made by lecturers, students do not necessarily enjoy using ICT, nor do they always find it easy to use. This has important implications for the use and support of ICT in the field of modern languages, and we will end the paper by discussing how our findings might be used to improve the experience of students and academics alike.   ",
       "article_title":"Technophobes, or the Nintendo Generation? A Study of the Use of ICT in Teaching and Learning in Modern Languages",
       "authors":[
          {
             "given":"Claire",
             "family":"Warwick",
             "affiliation":[
                {
                   "original_name":" University of Sheffield, UK  ",
                   "normalized_name":"University of Sheffield",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/05krs5044",
                      "GRID":"grid.11835.3e"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "The Electronic Classroom"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Awareness of domain-tuned linguistic peculiarities present in expository texts is a relevant concept in helping students' reading and writing competency in terms of genre literacies. Support for this point of view comes from the analysis of academic written genres, competing demands for limited resources, the tyranny of scheduling and from graduate students' verbal protocols about their reading process (Sengupta 1997). Genre literacy or sublanguage approach in instructed SLA advocated in this paper tries to exploit lexical, morphological, syntactic and semantic restrictions on the specialized languages used by experts in certain fields of knowledge for communication or in particular types of texts (technical and scientific articles, instructions, installation manuals, etc.). Notions of sublanguage distinctiveness rely on linguistic knowledge concerning different kinds of sublanguage regularities and restrictions. (Kittredge and Lehrberger 1982). Sublanguages are special subsystems of a natural language with restricted vocabulary and grammar which, on the one hand, share some properties with a language as a whole and, on the other hand, are characterized by some deviations from \"general\" language. As far as language instruction is concerned, both defining the content of this knowledge and ways of sublanguage knowledge elicitation are problems which do not have a single answer. Despite a long-standing interest in the analysis of written genres (sublanguages), little research has focused on how to really use genre specificity in language instruction. This presentation explores critical issues in the selection of an appropriate methodological framework for the analysis of profession-related texts. It aims to provide suggestions as to the kind of sublanguage analysis method that is supposed to form the basis for developing a system of typological parameters useful in acquisition of teaching materials and thus tuning language instruction to the needs of professional communication. To describe a particular sublanguage it is necessary to study laws underlying natural language phenomena and laws which make a sublanguage differ from a language. Sublanguages can be described in many ways. Language instruction is influenced by such practical parameters as scope and nature of vocabulary, grammar specificity, potential for ambiguity, lexical and grammar correlation, if any, which can and should be discovered on the basis of corpus analysis (Biber et al. 1998; Wichmann et al. 1997). This study focuses on verbs, as they are central to the structure of a sentence and consequently to text structure (Levin 1993; Aarts and Meyer 1995). The reason is that in professional reading most problems usually derive not from technical nouns and noun expressions which are relatively easy to find in specialized dictionaries but from grammar which is often characterized by extended sentences with frequently long and telescopic embedded structures. The current study also proposes and tests a sublanguage-specific hypothesis of correlation between lexical meaning, morphological representation (tense, voice, finiteness) and syntactic realization (subject, object, predicate, attribute, etc.) of a particular verb in a sublanguage. Material for the research includes five corpora of 50,000 words each from different technical sublanguages: aerospace engineering, automobile engineering, mechanical engineering, technology engineering and patents. The sample corpora are taken from four technical journals (Space Flight, Automobile and Tractor, Materials Engineering, and Machine Design) and a corpus of US patent claims. The main method of analysis is a computer-aided corpus-based combination of qualitative and quantitative (statistical) techniques applied to a pre-tagged corpus, which proved to be useful for linguistic knowledge elicitation (Sheremetyeva 1998). Tagging, done manually by trained linguists, codes morphosyntactic realizations of sublanguage verbs. For example, in the sentence \"Making_TIA this apparatus they used_2IA a new technology\", the tag TIA means that the verb \"make\" is used as an adverbial modifier in the form of Present Participle, the tag 2IA shows that the verb \"use\" is realized as a predicate in the form of Past Simple Active. This methodology allows for a standard automatic frequency count procedure to be applied to provide: a) a verb inventory and its size in terms of verb occurrences; b) a verb morphology and grammar inventory and their sizes in terms of occurrences of specific values of tense, aspect, voice, finiteness/nonfiniteness and syntactic functions as well as in terms of co-occurrence of grammatical features (for example, in the sublanguage of automobile engineering the most frequently used nonfinite realization of verbs is the Past Participle in the function of attribute, while no realization of verbs as Gerunds or Infinitives in the function of subject was found); c) an inventory of lexical and morphosyntactic correlations (for example, in the sublanguage of automobile engineering the verb \"use\" is most often realized as the Past Participle in the function of attribute while the most frequent realization of the same verb in the aerospace engineering sublanguage is the Present Participle in the function of adverbial modifier).  Qualitative analysis of each of the above inventories included sense analysis. The number of senses for each lexeme in a sublanguage is, on average, much smaller than in the language as a whole. Thus, of the seven senses of the word engage in the Cobuild English Language Dictionary, the patent sublanguage uses only one, which includes this word in the following synonym set: engage, hold, attach, lock, join, clamp, fasten. Clearly,paradigmatic and syntagmatic relations are different in a sublanguage.   Conclusions The paper presents a computer-aided methodology and the results of selecting teaching materials for optimizing students' reading and writing competencies in terms of genre literacies on the material of four technical sublanguages.  The results of the study show \"deviations\" of every sublanguage from the general language and from each other. They also confirm that there exists a correlation between lexical meanings of many sublanguage verbs and their morphosyntactic realizations. These deviations can be used for selecting professionally oriented language teaching materials to most effectively foster language proficiency development. The approach was tested and proved to be very useful at the Department of Foreign Languages of South Ural State University (Russia). It is expected to be portable to other sublanguages and can be used both for developing theoretical and practical issues in applied linguistics.   ",
       "article_title":"Computer-Aided Acquisition of Language Teaching Materials from Corpora",
       "authors":[
          {
             "given":"Svetlana",
             "family":"Sheremetyeva",
             "affiliation":[
                {
                   "original_name":" New Mexico State University, USA  ",
                   "normalized_name":"New Mexico State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hpz7z43",
                      "GRID":"grid.24805.3b"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "The Electronic Classroom"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction The Orlando Project has developed prototype delivery software which gives end users access to our literary history textbase. Richly-tagged SGML data is automatically converted to XML, and presented to users through a custom application (which runs locally on their machine, and communicates with a back-end XML server). The design of the user interface has been developed through a formal user needs analysis, conducted with a local Pilot Users Group. In the process, we have learned a great deal about how to exploit the richness of a heavily-tagged textbase, and how to present this information selectively to end users (meeting their information requirements without overburdening them with complexity).   The Goals of our Project The Orlando Project is applying state-of-the-art software technology to traditional fields of study in the humanities. We are writing a literary history of women's writing in Britain, as both a conventional published text and as an SGML tagged textbase. At present (November 1999) we have documents on 850 British women writers and documents on 590 other writers. For each author we have a pair of interdependent documents - a biography and a writing life history. This material is supplemented by 13,600 events, which are discrete dated items providing the further essential and enriching political, social and cultural background to the work. Events vary in their depth of coverage, but are in every case in one way or another related to the literary history which we are writing. Here are three examples of events: 1863: Selective chronology: British Women writers: Florence Nightingale privately printed an anonymous pamphlet, Note on the supposed protection afforded against venereal disease by recognizing and putting it under police regulation. [keyword: law and legislation] [keyword: body/health - venereal disease] August 1863: Selective chronology: British Women writers: Florence Nightingale corresponded with Harriet Martineau, outlining the case against the Contagious Diseases Acts. (Vicinus 441) by 1871: Comprehensive chronology: Social climate: The Royal Commission on the Contagious Diseases Acts rejected a suggestion that soldiers and sailors be required to submit to the same regular examinations required of the prostitutes they frequented. The commission believed \"there is no comparison to be made between prostitutes and the men who consort with them. With the one sex the offence is committed as a matter of gain; with the other it is an irregular indulgence of a natural impulse.\" This illustrates the double standard that held women to be sexually unresponsive and men to be prey to strong desire; paradoxically, this belief coexisted with the notion that women were emotional and irrational, while men were more enlightened and controlled.   Delivery Plans The Orlando Project received SSHRC funding in 1994. Our grant proposal at that time argued that SGML was the only feasible means to capture and encode the complex thematic approach to literary history which the project required. As to the ultimate means of delivery for this information, we anticipated that the technology landscape would be utterly changed 5 years on. We believed that there would be ways to deliver SGML to end users at the very end of the 20th century (we were also aware, in 1994, that there were perfectly acceptable ways of converting and delivering SGML information). We have found that XML is the means to the end which we hoped would appear. XML is a rapidly developing W3 Consortium standard, which will permit the direct delivery of tagged information to end users. An XML audit of our textbase, carried out in 1998, showed us that (for delivery purposes) our textbase could be transformed from SGML to XML without any loss of its intellectual value. We are able today to deliver our richly-tagged information to a client program (running in an XML browser, such as Internet Explorer 5; or in a custom application which support XML though third party software such as IBM's XML toolkit.   User Needs  Assessment Having received a great deal of positive encouragement from the scholarly community that the information we are developing is of considerable interest to them, we began a formal process of user needs assessment. A richly-tagged textbase such as ours can be exploited by end users in a wide variety of ways: subject-specific searches can create customized chronologies and research texts for reading imposing chronological limits can highlight issues and create connections which standard \"period\" labels obscure consistent tagging allows one or more documents to be compared \"side-by-side\", to reveal new insights about authors and their context  The most important issue for us was to \"bridge\" between the complex tag set which we have created and the terminology and information expectations which will characterise our end users. The strengths of our tagging are their rigour, and the highly detailed descriptions of their meaning. Their deficiency (from the point of the end user) is that this knowledge is locked up in a single tag name which may be opaque (such as our Cultural Formation tag) or dangerously obvious (such as our Name tag, which has a precise meaning and occupies a specific niche in a constellation of about a dozen \"personal name\" tags). In order to drive the development of the software from the users' point of view (rather than our own), we struck a Pilot Users Group. This group (about a dozen people) were drawn from representative communities who we expect will be interested in accessing our information, including:  professors, graduate students, and undergraduate students scholars in fields such as English literature and History librarians and information scientists  The program for this group was devised in order to elicit their expectations and desires for our software, without raising the question of what the software would look like or how it would work. We began with meetings where the group were given only written and oral accounts of our Project's goals and content; we elicited the group's own descriptions and terminology for our areas of interest. In the fall of 1999, building upon our team's sense of what kinds of access we could provide to end users, the Pilot Users Group was asked to comment on an on-screen mock-up of our delivery software. These sessions were conducted as formal focus groups [Greenbaum; Jordan]; the sessions were recorded and team notetakers wrote down the comments and suggestions from the users group. Because the software on-screen was truly \"throw away\", we are able to genuinely encourage the users to critique it and explore their preferences and expectations. We also surveyed the computer equipment and level of experience of the user group; we will expand this survey, to make sure we create delivery software which our target users can run, and which they will be able to learn to use effectively.   Software Architecture Our prototype delivery software is being written in a client/server fashion. The client end is a Java program which uses XML-aware code to request XML documents from the server to process them (by sorting, selecting, and sub-setting), and then displays them using XSL (the XML stylesheet language). Although it is technically possible to execute this part of the process inside an XML-capable browser, the nature of our textbase and the kinds of interaction which we wish to provide are rather unlike the Web-page metaphor. Our textbase can be queried to draw together coherent document sub-sections from many documents at once, which can be presented to the user in various forms, such as a customised chronology or a synoptic view of relevant sections from the lives or works of many authors at once. For this reason we feel the creation of an independent delivery program is desirable. A similar consideration operates with respect to linking within our textbase. We are implementing a much richer form of linking that the web at present provides; a great deal of the linking which end users will be able to explore will be generated automatically through the carefully and consistently tagged text. Users who are viewing text of interest will be able to pursue that interest by traversing automatic links which will open up from our elaborately tagged text. The server side of this architecture will make available our tagged textbase (as an XML document collection) which will respond to user queries by selecting and sending XML documents to the client program. We have explored various technologies to provide this searching and delivery on the back-end, including Java and CGI formats (using both Perl and SGREP to handle the searching). The obvious advantage of this approach is that the server can be implemented in more than one way (and be revised and extended as new technologies appear), while the front end client program remains the same (or is extended and improved on an independent trajectory). We are making extensive use of standard technologies, such as XML, XSL, and HTTP (for the communication between client and server). This will aid the process of generalising this software to meet the needs of other users who wish to present SGML or XML text to users without \"rendering it down\" to display-only formats like HTML.   Issues XML is an emerging standard. The software support for XML is beginning to appear; our strategy will be more effective as XML becomes ubiquitous and a variety of robust XML-capable tools emerge. The current effort is a \"prototype\"; the exercise of deploying it will have both successes and failures, from which we will learn. We have been very careful to avoid using the \"Web\" metaphor - our textbase can be delivered in ways which are much more dynamic and more informative that a Web delivery metaphor would imply. This ambition is to some extent undercut by the expectations of our Pilot Users Group, who came to the material with \"Web on the brain\". A classic case of this was the specific comment that we ought not to use a certain shade of blue for text if it was not a link, because \"blue means link\".    ",
       "article_title":"Solutions for the Delivery of Thematically-Tagged Text",
       "authors":[
          {
             "given":"Terry",
             "family":"Butler",
             "affiliation":[
                {
                   "original_name":" University of Alberta, Canada  ",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          },
          {
             "given":"Greg",
             "family":"Coulombe",
             "affiliation":[
                {
                   "original_name":" University of Alberta, Canada  ",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          },
          {
             "given":"Sue",
             "family":"Fisher",
             "affiliation":[
                {
                   "original_name":" University of Alberta, Canada  ",
                   "normalized_name":"University of Alberta",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/0160cpw27",
                      "GRID":"grid.17089.37"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Text Encoding"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Markup is inserted into textual material not at random, but to convey some meaning. An author may supply markup as part of the act of composing a text; in this case the markup expresses the author's intentions. The author creates certain textual structures simply by tagging them; the markup has performative significance. In other cases, markup is supplied as part of the transcription in electronic form of pre-existing material. In such cases, markup reflects the understanding of the text held by the transcriber; we say that the markup expresses a claim about the text. In the one case, markup is constitutive of the meaning; in the other, it is interpretive. In each case, the reader (for all practical purposes, readers include software which processes marked up documents) may legitimately use the markup to make inferences about the structure and properties of the text. For this reason, we say that markup licenses certain inferences about the text. If markup has meaning, it seems fair to ask how to identify the meaning of the markup used in a document, and how to document the meaning assigned to particular markup constructs by specifications of markup languages (e.g. by DTDs and their documentation). In this paper, we propose an account of how markup licenses inferences, and how to tell, for a given marked up text, what inferences are actually licensed by its markup. As a side effect, we will also provide an account of what is needed in a specification of the meaning of a markup language. We begin by proposing a simple method of expressing the meaning of SGML or XML element types and attributes; we then identify a fundamental distinction between distributive and sortal features of texts, which affects the interpretation of markup. We describe a simple model of interpretation for markup, and note various ways in which it must be refined in order to handle standard patterns of usage in existing markup schemes; this allows us to define a simple measure of complexity, which allows direct comparison of the complexity of different ways of expressing the same information (i.e. licensing the same inferences) about a given text, using markup. For simplicity, we formulate our discussion in terms of SGML or XML markup, applied to documents or texts. Similar arguments can be made for other uses of SGML and XML, and may be possible for some other families of markup language. Related work has been done by Simons (in the context of translating between marked up texts and database systems), Sperberg-McQueen and Burnard (in an informal introduction to the TEI), Langendoen and Simons (also with respect to the TEI), Huitfeldt and others in Bergen (in discussions of the Wittgenstein Archive at the University of Bergen, and in critiques of SGML), Renear and others at Brown University, and Welty and Ide (in a description of systems which draw inferences from markup). Much of this earlier work, however, has focused on questions of subjectivity and objectivity in text markup, or on the nature of text, and the like. The approach taken in this paper is somewhat more formal, while still much less formal and rigorous than that taken by Wadler in his recent work on XSLT. Let us begin with a concrete example. Among the papers of the American historical figure Henry Laurens is a draft Laurens prepared of a letter to be sent from the Commons House of Assembly of South Carolina to the royal governor, Lord William Campbell, in 1775. Some words have lines through them, and others written above the line. The editors of Laurens's papers interpret the lines through words as cancellations, and the words above the lines as insertions; an electronic version of the document using TEI markup and reflecting these interpretations, might read thus: <P><DEL>It was be</DEL> <DEL>For</DEL> When we applied to Your Excellency for leave to adjourn it was because we foresaw that we <DEL>were</DEL> <ADD>should continue</ADD> wasting our own time ... </P> From the DEL elements, the reader of the document is licensed to infer that the letters \"It was be\", \"For\", and \"were\" are marked as deleted; from the ADD element, the reader may infer that the words \"should continue\" have been added. Software might rely on these inferences in the course of making a concordance or displaying a clear text; human readers will rely on them in interpreting the historical document. Note that the markup here stops short of licensing the inference that \"should continue\" was substituted for \"were\". The editors could license that inference as well by appropriate markup, if they wished. Human readers may make the inference on their own, given the linguistic context; software cannot safely infer a substitution every time an addition is adjacent to a deletion. A simple way to capture the meaning of markup is to define, for each markup construct, a set of open sentences - sentences with unbound variables - which express the inferences licensed by the use of that construct. In formal reasoning, such open sentences may be transformed into logical predicates in the usual way. For example, the TEI element type DEL is said by the documentation to mark \"a letter, word or passage deleted, marked as deleted, or otherwise indicated as superfluous or spurious in the copy text by an author, scribe, annotator or corrector\" (TEI P3, p. 922). We take this to mean that when a DEL element is encountered in a document, the reader is licensed to infer that the material so marked has been deleted. In formal contexts, we may write \"deleted(X)\"; we can specify the meaning of the DEL element and of the logical predicate \"deleted(X)\" by means of an open sentence: \"X has been deleted, or marked as deleted, or ...\" etc. The variable X is to be bound, in practice, to the contents of the DEL element. If we imagine a variable named 'this', instantiated to each element of a document in turn, and a function 'contents' which returns the contents of its argument, then the meaning of the DEL element becomes \"deleted(contents(this)))\", or equivalently \"contents(this) has been deleted ...\" etc. The TEI element type HI, similarly, \"marks [its contents] as graphically distinct from the surrounding text\" (TEI P3, p. 1013). We can capture the meaning of HI by the open sentence \"X is graphically distinct from the surrounding text\", or \"highlighted(X)\", where X is, as before, to be replaced by \"contents(this)\". Attributes may be treated similarly. The 'rend' attribute on the <hi> element \"describes the rendition or presentation of the word or phrase highlighted\". In the example<P><HI REND=\"gothic\">And this Indenture further witnesseth</HI> that the said <HI REND=\"italic\">Walter Shandy</HI>, merchant, in consideration of the said intended marriage ... </P> the HI elements convey the information that the contents of those elements are distinct from their surroundings, while the 'rend' attributes on the HI elements specify how. The meaning of the 'rend' attribute is expressed by the open sentence \"X is rendered in style Y.\" An HI element with a 'rend' attribute thus means \"X is graphically distinct from its surroundings, and X is rendered in style Y\". Perhaps the simplest method of interpreting markup is to assume that 1. The meaning of every element type is expressed by an open sentence whose single unbound variable is to be bound to 'contents(this)'. 2. The meaning of every attribute is expressed by an open sentence with two unbound variables, one of which is to be bound to 'contents(this)' and the other to 'value(this,attribute-name)' (i.e. to the value of the attribute in question). In other words, each attribute defines some relation R which holds between the contents of the element and the value of the attribute. 3. All inferences licensed by any two elements are compatible.  The set of inferences applicable to any given location L is then the union of the inferences licensed by all the elements within which L is contained. Let us call this the 'union model' of interpretation. The union model is simple, and provides a good first approximation of the rules of inference for marked up text. But it is not wholly adequate. First, it fails to distinguish distributed properties (such as 'italic' or 'highlighted') from sortal properties (such as paragraphs, sections, or - as illustrated above - deletion). It is as true to say \"The word 'And' is in black-letter\" as to say it of the entire phrase, and the meaning of the example given above would not change if the HI elements were split into two or more adjacent pieces each with the same 'rend' value. Conversely, two HI elements with the same attribute values can be merged without changing the meaning of the markup. Other elements mark properties which are NOT distributed equally among the contents, and cannot be split or joined without changing the meaning of the markup. From the markup<P>Reader, I married him.</P>we can infer the existence of one paragraph, but we cannot infer that \"Reader\" is itself a paragraph. Such properties we call 'sortal' properties, borrowing a term of art from linguistics. Elements marking sortals are usefully countable; those marking distributed properties are not. Second, the union model fails to allow a correct interpretation of inherited values and overrides, as illustrated by the TEI 'lang' attribute or the xml:lang attribute of XML. In fact, some inferences do contradict each other, and specifications of the meaning of markup need to say which inferences are compatible, and which are in conflict, and how to adjudicate conflicts. Third, the union model allows inferences about a location L only on the basis of markup on open elements (those which contain L); in order to handle common idioms of SGML and XML, a model of interpretation must handle upward propagation: the meaning of an element may depend in part on its contents; this is unusual in colloquial SGML/XML systems, but is a regular feature of proposals to eliminate attributes from markup languages. context dependency: the meaning of an element may depend on its context; trivial examples include TEI's HI and FOREIGN, which can mean 'not-Roman' and 'not-English' in one context, and 'not-italic' and 'not-German' in others. ordinal position, relative or absolute; dependence of meaning upon ordinal position is seldom an explicit feature of markup languages, but dependence of processing based on position is a standard feature of style-sheet languages. milestone elements; these convey information by position in the beginning-to-end scan of the linear form of the document, rather than by position in the tree. linking: out-of-line or 'standoff' markup conveys information about location L based not only on open elements, but on elements which point at L or some ancestor of L.  Other methods of associating markup with meaning are imaginable, but we believe a survey of existing DTDs will show that all or virtually all current practice is covered by any model of interpretation which encompasses the complications just outlined. Essentially, these can be handled by extending the rules for binding variables in the open sentences which specify the meaning of a given markup construct. The simple union model allows only 'contents(this)' and 'value(this,attribute-name)'; the constructs listed above require more complex expressions, roughly equivalent in expressiveness to the TEI extended-pointer notation or to the patterns of the XPath language defined by W3C. Complexity of the semantics associated with an element type or attribute may be measured by the number of unbound variables in the open slots, by the complexity of the expressions which are to fill them, and by the amount or kind of memory required to allow full generation of the inferences licensed by markup in a particular text.  ",
       "article_title":"Meaning and Interpretation of Markup",
       "authors":[
          {
             "given":"C.",
             "family":"Sperberg-McQueen",
             "affiliation":[
                {
                   "original_name":" World Wide Web Consortium, USA  ",
                   "normalized_name":"World Wide Web Consortium",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0059y1582",
                      "GRID":"grid.507688.4"
                   }
                }
             ]
          },
          {
             "given":"Claus",
             "family":"Huitfeldt",
             "affiliation":[
                {
                   "original_name":" University of Bergen, Norway  ",
                   "normalized_name":"University of Bergen",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/03zga2b32",
                      "GRID":"grid.7914.b"
                   }
                }
             ]
          },
          {
             "given":"Allen",
             "family":"Renear",
             "affiliation":[
                {
                   "original_name":" Brown University, USA  ",
                   "normalized_name":"Brown University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05gq02987",
                      "GRID":"grid.40263.33"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Text Encoding"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   1. Introduction The structure and content of lexical information has been explored in considerable depth in the past, primarily in order to determine a common model that can serve as a basis for encoding schemas and/or database models. For the most part, descriptions of lexical structure have been informed by the format of printed documents (e.g., print dictionaries), which varies considerably over documents produced by different publishers and for different purposes, together with the requirements for instantiation in some encoding format (principally, SGML). However, the constraints imposed by these formats interfere with the development of a model that fully captures the underlying structure of lexical information. As a result, although schemas such as those provided in the TEI Guidelines exist, they do not provide a satisfactorily comprehensive and unique description of lexical structure and content. We believe that in order to develop a concrete and general model of lexical information, it is essential to distinguish between the formal model itself and the encoding or database schema that may ultimately instantiate it. That is, it is necessary to consider, in the abstract, the form and content of lexical information independent of requirements and/or limitations imposed its ultimate representation as an encoded or printed object. This is especially important since these eventual representations will vary from one application to another; in particular, lexical information may be encoded not only for the purposes of publishing in print or electronic form, but also for creating computational lexicons, terminology banks, etc. for use in natural language processing applications. It is therefore essential to develop a model that may be subsequently transformed into a variety of alternative formats. In this paper, we outline a formal model for lexical information that describes (a) the structure of this information, (b) the information associated with this structure at various levels, and (c) a system of inheritance of information over this structure. We then show how the structure may be instantiated as a document encoded using the Extended Markup Language (XML). Using the transformation language provided by the Extensible Style Language (XSL), we then demonstrate how the original XML instantiation may be transformed into other XML documents according to any desired configuration (including omission) of the elements in the original. Because of its generality, we believe our model may serve as a basis for representing, combining, and extracting information from dictionaries, terminology banks, computational lexicons, and, more generally, a wide variety of structured and semi-structured document types.   2. Overview of the theoretical model The underlying structure of lexical information can be viewed as embedded partitions of a lexicon, in which no distinction is made among embedded levels. A model of lexical information can be thus described as a recursive structure comprised, at each level, of one or more nodes. This structure is most easily visualized as a tree, where each node may have zero or more children. That is, at any level n, a node is either a leaf (i.e., with no children) or can be decomposed as:T=[T1, T2, ..., Tn]where each Ti is a node at level n+1. Properties may be attached to any node in the structure with the prop predicate: PROP(T,P)indicates that the property P is attached to node T. Properties are associated with nodes either by explicit assignment, or they may be inherited from the parent node. The object of our model is to identify the ways in which properties are propagated through levels of structure. For this purpose, we consider properties to be Feature-Value pairs expressed as terms of the form FEAT(F,V), where F and V are tokens designating a feature (e.g., POS) and a value. In the simplest case, values are atomic (e.g., NOUN) but may also consist of sets of feature-value pairs. This representation is consistent with the base notation associated with feature structures, a common framework for representing linguistic information.   3. Propagating information across levels We define three types of features: Cumulative features that may take more than one value and may be thus inherited and combined along the structure. For example, for a cumulative feature DOMAIN, if the property FEAT(DOMAIN,NAVIGATION) is associated with a node at level n and FEAT(DOMAIN,LAW) is associated with its child at level n+1, by inheritance the node at level n+1 will be assigned the property FEAT(DOMAIN,NAVIGATION + LAW). Overwriting features that take only one value at a time. This implies that only one instance of an overwriting feature may appear at a given node and that the corresponding properties are propagated along the structure unless and until a new value is specified for that feature. In such a case, the new value \"overwrites\" the earlier one and is subsequently propagated to nodes in its subtrees. Local features, which apply only at the node with which they are associated; i.e., they are not propagated through the structure. Cross-references are an example of a local feature, since they apply only to the level of description with which they are directly associated.  The full paper will provide details of this formalism.   4. Creating representations Lexical information can be represented as a tree structure reflecting, in large part, the natural hierarchical organization of entries found in printed dictionaries. This hierarchical organization (e.g., division into homographs, senses, sub-senses, etc.) enables information to be applied over all sub-levels in the hierarchy, thus eliminating the need to re-specify common information. For example, consider the following definition from the Collins English Dictionary (CED):EX.1: overdressoverdress vb. (zzzz) 1. To dress (oneself or another) too elaborately or finely. ~n. (yyyy) 2. A dress that may be worn over a jumper, blouse, etc. This information can be represented in tree form as follows :[ orth : overdress] [ pos : verb pron : zzzz def: To dress (oneself or another) too elaborately or finely] [ pos : noun pron : yyyy def : A dress that may be worn over a jumper, blouse, etc.] Each node in the tree represents a partition of the information in the entry, and information is inherited over sub-trees. Thus in this example, the orthographic form \"overdress\" appears at the top node and applies to the entire entry; the entry is then partitioned into two sub-trees, for verb and noun, each of which is associated with specific information about part of speech, pronunciation, and definition. The final paper will provide similar examples from dictionaries as well as terminological data banks.   5. Extracting information from the tree We define a tree traversal as any path starting at the root of the tree and following, at each node, a single child of that node. A full traversal is a path from the root to any leaf; a partial traversal extends from the root to any node in one of its subtrees. As a tree is traversed, each node is associated with a set of features including: (a) features associated with the node during tree creation, and (b) features determined by applying the rules for propagating overwriting, cumulative, and local features. Thus, at any node, all applicable information is available for some unique partition of the lexical space. Nodes near the top of the tree represent very broad categories of partition; leaf nodes are associated with information for the most specific usage of the entry.   6. Encoding the information in XML We define an XML encoding format for the structures described above:  Elements <struct> represents a node in the tree. <struct> elements may be recursively nested at any level to reflect the structure of the corresponding tree. <struct> is the only element in the encoding scheme that corresponds to the tree structure; all other elements provide information associated with a specific node. <alt> alternatives are bracketed in parallel <alt> elements, which may appear within any <struct>. <brack> is a general-purpose bracketing element to group associated features. Base elements corresponding to various features, such as (for dictionaries) orth, pron, hyph, syll, stress, pos, gen, case, number, gram, tns, mood, usg, time, register, geo, domain, style, def, eg, etym, xr, trans, and itype, (analogous to dictionary elements defined in the TEI Guidelines.) >   Attributes Attributes are used to provide information specific to the element on which they appear and are not inherited in a tree traversal.  The following shows the corresponding XML encoding for \"overdress\": <struct> <orth>overdress</> <struct> <pos>verb</> <pron>zzzz</>  <def> To dress (oneself or another) too elaborately or finely</></> <struct> <pos>noun</> <pron>yyyy</> <def> A dress that may be worn over a jumper, blouse, etc.</></></>     7. Transforming the XML document The Extensible Style Language (XSL) is a part of the XML framework that enables transformation of XML documents into other XML documents. The best-known use of XSL is the formatting of documents for display on web browsers. However, XSL also provides a powerful transformation language that can be used to convert an XML document describing lexical information by selecting, rearranging, and adding information to it. Thus, a document encoded according to the specifications outlined in the previous section can be manipulated to serve any application that relies on part or all of its contents. The current version of the XSL transformation language is available at <>.  Lack of space prevents providing examples; the final paper will include these.   ",
       "article_title":"A Formal Model for Lexical Information",
       "authors":[
          {
             "given":"Nancy",
             "family":"Ide",
             "affiliation":[
                {
                   "original_name":" Vassar College, USA  ",
                   "normalized_name":"Vassar College",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/022x6qg61",
                      "GRID":"grid.267778.b"
                   }
                }
             ]
          },
          {
             "given":"Adam",
             "family":"Kilgarriff",
             "affiliation":[
                {
                   "original_name":" ITRI Brighton, UK  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Laurent",
             "family":"Romary",
             "affiliation":[
                {
                   "original_name":" LORIA/CNRS, France  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Text Encoding"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Ira Shor, in Critical Teaching and Everyday Life, proposes a student-centered pedagogy which theorizes that everyone immersed in mass culture is \"habituated to a dizzying pace of life.\" (63) Describing the factors today's teachers face, Shor writes about the \"addicting standard of stimulation\" set by radio, television and other illuminated media, certifiying that a \"hyped use of words in pictures fits into the whole accelerated gestalt of daily life.\" (63-64) Accepting this as contemporary circumstance, methods must be constructed and made readily available to help teachers bridge the gap between past and present in terms of technology and the humanities classroom. Digital technology is changing the whole nature of education in our society. This means that professors and students from all disciplines need to be prepared to read and transmit their work in new ways via the computer. My essay will outline, then describe in detail, successful methodology established in teaching \"Electronic Publishing\" classes to graduate and undergraduate students with interests across multiple disciplines at New Jersey Institute of Technology. The primary objective of \"Electronic Publishing\" is to enhance a previously untrained student's ability to use computers effectively and intelligently to create and design texts in academic, commercial, or other settings. Projects in this course of study intend to build understanding and functional skills in the visual presentation and online structuring of information. Students learn how to create interactive online documents that incorporate language with visual aspects of computerized text by combining graphics, sound, animation, text, and video into compelling content. The approach to teaching cybertext writing and design I have developed at New Jersey Institute of Technology since 1997 is effective for students presenting research in every area of the humanities, including languages and literature, history, philosophy, music, art, film studies, linguistics, anthropology, archaeology, creative writing, and cultural studies. As a pedagogue, I formulate this discipline as an investigative, processual endeavor that demands the understanding and application of two human principles in conjunction with four essential aspects of design. To learn and succeed as online producers of text, students must first embrace and attempt to embody the concepts of patience and organization; the fundamental areas of attention in creating hypertext documents are introduced as: language, image, linking, and thinking. Every technical and aesthetic aspect, or problem, of document construction may be addressed through a series of questions, and a checklist of formal considerations associated with these principles and areas of attention. All of the dimensions or elements within the principles and aspects of design highlighted above will be fully addressed and explained in the paper. Among the multiple subjects that arise in this discussion of how to teach students to produce cybertext are: gathering and formatting content, conducting research on the Internet, presenting effective visual communication, strategizing and solving technical problems, interlinking and layering documents, and otherwise establishing objectives and sensible schemes for online documents. In \"Electronic Publishing,\" students are eventually introduced to a completely different language: the relentlessly precise language of computer programming, HTML, which intervenes with content and re-creates sense and vision within cybertext writing and editing. Code is language that handles the work of online producers: writing, image, and sound; sometimes it is relatively easy to understand and use, at others it may also be fearfully complicated. Methods of conceptualizing (for students) what HTML is, and how to make use of it in humanities projects, will be outlined in this presentation. This essay will, in addition to covering materials listed above, offer a detailed account of the various components of the \"Electronic Publishing\" courses which consists of a month of unique design-oriented research followed by two months of \"hands-on\" work. Students in the course not only study electronic publishing, they do electronic publishing by editing two editions of a journal based on their personal academic or creative pursuits. An electronic portfolio of a student's work in every class they are enrolled in must also be completed as part of \"Electronic Publishing.\" The program of \"Electronic Publishing\" designs a technology plan for other Humanities-oriented departments interested in developing curricula around electronic publishing initiatives, Internet communication, and hardware/software management schematics. Methods of reading and presenting work using technologically sophisticated computers and networks are made clear by my process; students are quickly able to exhibit and exercise their learning in these courses. At the conclusion of my paper, I will present guidelines formulated for the assessment of student generated work. For current \"Electronic Publishing\" course materials on the World Wide Web, see: <>, <>  ",
       "article_title":"Teaching Cybertext Writing, Design, and Editing: Language, Image, Linking, Thinking",
       "authors":[
          {
             "given":"Chris",
             "family":"Funkhouser",
             "affiliation":[
                {
                   "original_name":" New Jersey Institute of Technology, USA  ",
                   "normalized_name":"New Jersey Institute of Technology",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05e74xb87",
                      "GRID":"grid.260896.3"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "The Electronic Classroom"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Writing about the new economy, Jeff Madrick remarks that the kinds of good jobs increasing most rapidly \"require communication skills, social ease, and basic reasoning abilities ...\" Acquiring such skills, he believes, \"may only be possible through higher education, where students are exposed to a sophisticated culture, a variety of experiences, and varying disciplines that require analysis of facts and concepts\" (33). In our view, the conventional classroom on a college or university campus remains the best facility for such an education, and we think the new technologies can be used to make it even more powerful. By contrast, much excitement about technology in governing bodies is lavished on various money-saving adaptations of distance education using the web. Perhaps as a result, some advocates of increased use of technology in higher education seem to believe the traditional classroom anachronistic (see Daniel, 1996 and 1997). In our opinion, such thinking bodes ill for the kind of learning we see as vital. Rather than using technology to replace teachers and conversation, we think it should take its place with more conventional tools as another way to enhance teaching and learning conceived in traditional ways. In what follows, we describe (and illustrate in presentation) a technologically enhanced classroom and accompanying tools that operationalize a philosophy of pedagogy that puts technology at the service of active learning. While we have previously presented some of the tools we use in this classroom (Havholm and Stewart 1996, 1998), this presentation aims at showing a style of teaching we believe to be particularly promising. While it is too soon to claim more than anecdotal success, we know it promotes active learning because it demonstrably extends students' powers of inquiry. We and colleagues increasingly favor this kind of use of technology - in a range of disciplines - at The College of Wooster. It saves no money in the short term, however. Rather than doing away with buildings or teachers, it adds technology to a conventional classroom housing a small number of people, one of whom is salaried. But over the long term, if we are right, our graduates have the intellectual and cultural capital that allows them to think and learn independently. They will not need expensive re-training every time their environment changes a little. Our electronic classroom looks like a seminar room, with a table in the middle, surrounded by comfortable chairs. It differs in that along its walls twenty to thirty networked computers stand ready, each linked to one another, to a screen/video projector overhead, and to the internet. Such a classroom clearly values physically proximate talk, but it also brings the huge resources of the internet to any conversation that wishes them. Moreover, it makes possible the easy use of a range of new tools that encourage active learning. For example, Peter Havholm and our colleague Jenna Hayward use a freeware beta version of PennMUSH in a course on dramatic structure to allow the class (of 29) to improvise a seven-episode serial drama. Students play characters and invent actions on-line, edit the logs of their online sessions into scripts, and then publish a final version on the web for friends (on- and off- campus) to read. Because of the technology, they can write and publish a play that belongs to all of them. Most important, however, is that this exercise is not done in a playwriting class but in a study of drama. Rather than honing writing skills, writing and publishing a play in this class tests the principles of structure students are learning from their reading of a dozen plays and Aristotle's Poetics. And because the technology makes publication so easy, the whole project takes only about 15% of class time. Having students write and publish a drama to test theoretical ideas was a natural development from another kind of project several of us in the English department use in our writing courses. In the Journalism course, in Introduction to Non-fictional Writing, and in English 101 as well as in the course Writing for Magazines, students spend two to five weeks writing, editing, designing, and producing a magazine, using page layout software, which they then either give away or sell on campus. The publishing projects have pleased several of us because students so much enjoy writing to intrigue and amuse their peers - and because the projects make self-evidently necessary the tasks of re-writing, careful consideration of audience and voice, and editing. No need for exhortation about these activities; one cannot make a magazine to impress one's friends without them. We also believe that preparing writing for publication - with headlines, pullquotes, illustrations, captions, and the rest - provides valuable experience in imagining oneself as one's reader and in visual thinking. Among the tools that have been particularly useful in courses in narrative or narrative theory is the Linear Modeling Kit (or LMK), a program the two of us designed and have worked with for several years (see Havholm and Stewart, 1996). The LMK is essentially an authoring system, and it allows users to create applications that generate any kind of text according to principles proposed by the user. For example, a student can use the LMK to create a \"folktale generator\" by entering what the student perceives to be the parts or elements of a folktale, any principles of order among those parts, and characteristic text for each part. Depending on the complexity of the input, the generator will produce hundreds, thousands, or millions of different texts. In our classes, students have created not only folktale generators but bildungsroman generators, romance generators, tragedy generators, and argument generators. As do all the activities we discuss here, working with the LMK acts as an heuristic, forcing students to move back and forth between theory and practice. To produce an LMK generator, students must first abstract principles from narratives they have read and then turn them into instructions for their generators. The generator then operationalizes the student's theory; it produces narratives created according to the principles the student has derived. Another of the tools we use in the electronic classroom is the Stylistic Analysis Kit (or SAK), a combination concordance and counting program with a nearly flat learning curve. Although the SAK is a fairly conventional program, its ease of use separates it from many of the tools used by professional researchers and makes it ideal for the student in the classroom. When analyzing their own papers, students are almost always driven back to their texts by, for example, discovering their average sentence length to be half that of the person sitting at the next computer or by learning that \"the\" comprises 14.7% of their total words. Here, students move between the abstraction of statistics and their own practice as writers. Even those who seem generally to lack curiosity are fascinated by the statistical record of their writing and eager to determine what practices account for those statistics. There has recently been much publicity about the ease with which new hardware and software can be used to create complex video projects. Our students have begun to find that - like desktop publishing software - the new video tools can be used to explore and test ideas. Ben Speildenner chose to make a video as his final project in our colleague Jenna Hayward's course in Post-Colonial Literature. In Urban Legends, he wanted to to evoke reflection congruent with one of the principal issues of the course. The class had talked about how easy it is to essentialize one's own culture while seeing other cultures as \"different.\" Ben chose several urban legends - that Disney makes heavy use of phallic imagery in The Little Mermaid, that there's a boy with a shotgun in the background of a scene in Three Men and a Baby, that there's a hanged Munchkin in The Wizard of Oz if you look closely enough at the right moment, and that spiders can lay eggs on your face - to shake our presuppositions. He wanted his presentations of these legends to push his audience into problematizing their own culture. He thinks that our urban legends show us to be more \"different\" than we think we are. But he wanted to stimulate thought, not to impose his ideas on the class. In Understanding and Cognition, Terry Winograd and Fernando Flores make a convincing case against the use of computers as \"restricted to representing knowledge as the acquisition and manipulation of facts, and communication as the transferring of information\" (78). Rather, they argue that we need to design computers as \"equipment for language\" so that they can \"create new possibilities for the speaking and listening that we do\" (79). Our version of the electronic classroom and our use in it of the tools we have described reflect this understanding of technology. In every case, students use the tools to interrogate ideas in ways novel in humanistic study. In an important sense, each tool allows students to test their understanding: the effectiveness of a serial drama tests ideas about dramatic structure; reader response to a published magazine tests convictions about rhetoric; the variety of lawful stories an LMK generator produces tests the powers of the theory of narrative it has been \"taught\"; the SAK's quantitative analysis leads to testing qualitative ideas about writing style; and his classmates' response to Spieldenner's Urban Legends tested his hypothesis that presenting urban legends can help us think in new ways about \"difference.\" In every case, we believe, the technology adds power to students' ability to question and therefore to understand - in the context of a kind of discussion as old as learning.  ",
       "article_title":"A Toolbox for the Electronic Classroom",
       "authors":[
          {
             "given":"Peter",
             "family":"Havholm",
             "affiliation":[
                {
                   "original_name":" The College of Wooster, USA  ",
                   "normalized_name":"College of Wooster",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/029zqs055",
                      "GRID":"grid.254509.f"
                   }
                }
             ]
          },
          {
             "given":"Larry",
             "family":"Stewart",
             "affiliation":[
                {
                   "original_name":" The College of Wooster, USA  ",
                   "normalized_name":"College of Wooster",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/029zqs055",
                      "GRID":"grid.254509.f"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "The Electronic Classroom"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In her exploration of ekphrasis, the relationship between visual and verbal arts, Amy Golahny reminds us that references to the interconnectedness of the language of pictures and words date at least from the fifth century B.C. when Simonides said, \"as in painting, so in poetry\". In the first century B.C., Golanhy adds, Horace said that \"painting is mute poetry and poetry a speaking picture\". Further consideration of the concept of ekphrasis by Murray Krieger and W.J.T. Mitchell brings our attention to this verbal-visual relationship up to date. However, at the end of the twentieth century, most undergraduate education in the humanities continues to approach these art forms separately or to focus on student-generated text alone for developing and communicating ideas. The ease with which the Internet now allows students to exchange, create, and manipulate text and images offers new opportunities for engagement with the composing process. Because our goal as teachers of undergraduate writing and literature classes is creative as well as critical communication and because our pedagogy emphasizes active learning processes, we introduce our students to computing in and about the humanities. Dialogic writing within and beyond their classes enables students to enter into new discourse communities and to explore collaboratively the concepts of their courses. Creating, selecting, and manipulating visual images alone or in conjunction with text introduces students to expanded and contemporary composing processes. Publication of their compositions on the Internet provides them with an audience of other learners. They need not strive to be professional poets or painters to be makers of poems and paintings as a way to learn. Although our students may read Blake at a Website or in an edition illustrated by his own drawings or read Auden's \"Musée des Beaux Arts\" accompanied by a reproduction of Brueghel's Landscape with the Fall of Icarus, the relationship between the visual and verbal has not been emphasized in undergraduate higher education, where science textbooks are likely to have more illustrations than literature anthologies. How do humanities teachers dramatize the connection between the visual and verbal for our students and thus help our students understand the interrelatedness of the linguistic and graphical arts? How do we revive their own creativity and cognitive skills with words and pictures? After all, our students probably illustrated their own words in elementary school but are seldom invited to do so in college. New technologies, in particular the World Wide Web, are bringing words and pictures together for us and our students in ways that might bring those connections back to our college classrooms. Document design now extends beyond the one-inch margin requirements of MLA student manuscripts. Instead, our students are learning with us about screens and color and negative space and visual communication as integral to rather than decoration for the word. We will describe undergraduate literature and writing projects in which student-generated words and graphics are central to communication of ideas. In these projects, publication of their compositions on the Internet encourages students to reflect on the connections between technology and art, word and image, private and public writing, and their own creative and critical processes. These projects give students opportunities to perceive and to communicate visually, orally, textually, kinesthetically - in other words, they provide multisensory learning experiences. Theoretical foundations for student-generated compositions in this project come not only from Golahny, Krieger, and Mitchell but also from chapters on teaching in Learning Literature in an Era of Change: Innovations in Teaching. Terri Pullen Guezzar (\"From Short Fiction To Dramatic Event: Mental Imagery, The Perceptual Basis of Learning in the Aesthetic Reading Experience\") applies the theories of Rudolf Arnheim and Allen Paivio, who argue that privileging the verbal over the visual limits our cognitive development and that separating verbal from visual perception fragments our understanding of and communication about literature. Pedagogical theory is featured in \"Figuring Literary Theory and Refiguring Teaching: Graphics in the Undergraduate Literary Theory Course,\" where Marlowe Miller maintains, \"Graphics help students conceptualize complex and abstract theories so that they can identify the central concepts and assumptions of those theories.\" Two accessible resources for teachers thinking about integrating new media into undergraduate education also are useful for encouraging colleagues to incorporate the Internet as a learning environment and to make computer-mediated student projects integral to the learning process. In Seven Principles for Good Practice in Undergraduate Education: Implementing with Technology, Arthur W. Chickering and Stephen C. Ehrmann describe ways the following tenets can be incorporated into computer-mediated instruction: contacts between students and faculty, reciprocity and cooperation among students, active learning techniques, prompt feedback, time on task, high expectations, respect for diverse talents and ways of learning. Cooperation among students and active learning techniques as well as respect for diverse learning styles all are supported by multisensory student online publications in which students create original works of art or combine text and images to learn and to communicate their learning. Additional encouragement for teachers and students comes from Engines of Inquiry: Teaching, Technology, and Learner-Centered Approaches to Culture and History by Randy Bass, director of the American Crossroads Project, Georgetown University. Bass identifies \"six kinds of quality learning\" that \"information technologies can serve to enhance\": distributive learning, authentic tasks and complex inquiry, dialogic learning, constructive learning, public accountability, and reflective and critical thinking. Once again, collaborative student-generated projects are emphasized as effective learning strategies. Teaching at two quite different types of institutions, Donna at a large multicampus urban-suburban open admissions community college on the Atlantic coast of Virginia and Art at a selective land-grant university emphasizing agriculture, engineering, science, and technology in the foothills of South Carolina, we both have found that opportunities to compose and share text and images has enriched learning for undergraduates. Examples from the work of our own students and of our colleagues' students will demonstrate some ways that novice scholars learn \"from the inside out\" by creating, selecting, combining, and manipulating text and images in electronic environments. Using either a live Internet connection (preferable) or files on disk displayed through a Web browser as well as an overhead projector, we will present and analyze student work that illustrates the conjunction of visual and verbal knowledge and its significance for introducing undergraduate students to the artistic life of their community and to computer-mediated composing as well as for fostering their creative and cognitive development. < >  ",
       "article_title":" Ekphrasis and the Internet: Connecting the Verbal and the Visual with Computer-mediated Student Projects in an Undergraduate Humanities Class",
       "authors":[
          {
             "given":"Donna",
             "family":"Reiss",
             "affiliation":[
                {
                   "original_name":" Tidewater Community College, USA  ",
                   "normalized_name":"Tidewater Community College",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/04x2ven38",
                      "GRID":"grid.438675.b"
                   }
                }
             ]
          },
          {
             "given":"Art",
             "family":"Young",
             "affiliation":[
                {
                   "original_name":" Clemson University, USA  ",
                   "normalized_name":"Clemson University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/037s24f05",
                      "GRID":"grid.26090.3d"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "The Electronic Classroom"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction: 'Henrik Ibsen's Writings' aims at producing and publishing both an electronic version and a book version of all Ibsen's writings: dramas, poems, drafts, letters, articles, notes. All manuscripts and editions in Norwegian/Danish from the playwright's lifetime will be encoded using SGML/TEI. Our encoding is rather detailed, as we wish to reproduce every text witness in full accuracy. In this paper we will examine some empirical problems which have surfaced when encoding text structure and textual features in Ibsen's plays. We will relate these to problems concerning how to encode the textual features in Ibsen's 'Norma' (1851). Based on genre one could define this text as a drama, but it could also be considered a covert article. An analytical perspective on the text would expose a good mixture of different genre elements. The text's physical representation, i.e. typography, shares the same ambiguity. These different approaches all form different 'logical' structures which overlap each other, and either approach forces us to consider an unfortunate priority in the encoding. To solve these problems we have looked into different solutions for encoding text structure and overlapping text features. Briefly we have asked the basic questions: What is text, what defines structure and other textual features, and how does this affect the encoding of texts? These questions have been much discussed within the text encoding society in the past, but the approach in this paper is somewhat different from much of the earlier work. We will try to examine the questions from a more practical point of view, with focus on the encoding of 'Norma', and we hope that we thus will be able to add a new perspective to the discussion.   Ibsen's 'Norma': The problems of overlapping and ambiguous textual features and structures are particularly manifest when one has to deal with texts with mixed features which cannot easily be combined. Ibsen's 'Norma', which was written and published in the newspaper 'Andhrimner' in 1851, is an example of such a text. Choosing which textual features to encode in 'Norma' is not easy, as it is problematic to define the text as a drama, but even more problematic to define it as something else. We have chosen to regard it as a drama, but several textual elements in 'Norma' are difficult to incorporate in the drama structure. Particularly problematic is e.g. a 'speech' from 'the curtain' (which has no reference in the cast list) typographically rendered not as speech, but as a stage direction. Other problems concern footnotes (both in the cast list and in the speeches), speeches in brackets that do not seem to be 'asides' (asides are marked by stage directions) etc. These problems, and others like them, will be presented and further discussed at the conference.   Encoding Text Structure: The nature of a text has been much discussed, in several different contexts. One answer to what a text is, is that it is made of several interwoven features, or content objects, which together form 'a text'. The nature of a text is thus complex, and it seems difficult, or even impossible, to find a single structure that 'is' the text. Some would argue that texts even seem to be able to include things 'outside' themselves. Another view on the text, much discussed within the text encoding society, is the claim that 'text is an ordered hierarchy of content objects', the so called OHCO-thesis. This claim has been thoroughly examined by Renear et al. (1993), and further discussed e.g. in Biggs & Huitfeld (1997). We will leave this discussion aside here, and only point out that the OHCO-thesis (at least in its simplest form) may be appealing from a text encoding perspective, but that it is far from unproblematic. When encoding texts one generally chooses either declarative markup languages based on SGML, or its subset XML. We are using SGML/TEI, but are considering moving over to XML. As Sperberg-McQueen & Huitfeldt (1998) have pointed out, SGML markup in its simplest forms uses a straightforward model for markup: elements nest within each other so that the SGML document forms a hierarchical structure. The basic model of SGML associates single occurrences of features with single SGML-elements. Tagging a textual element as a SGML-element of a particular type and giving it particular attribute values, thus claims that this element exhibits the textual features associated with that same element type and those attribute values. The relationship between SGML element types and text features depends, however, on the encoder's understanding and interpretation of the genre, structure and perspective of the text. The encoder defines text objects in elements matching the chosen hierarchical structure. One of the large challenges for application of SGML to existing texts is finding suitable representations in SGML's tree-based data model for multiple hierarchies and textual features which overlap each other. Such overlapping textual features seem to be an inescapable fact of textual life, but present a problem in the simple SGML-model because while two textual features/hierarchies may overlap, two SGML-elements may not.   Encoding Overlapping Features: The problem of overlapping features has been discussed before. In the TEI Guidelines there are several ways to overcome some of the problems: one may e.g. use 'milestone' elements, in which a feature is predicated by the span of text between one milestone element and the next. Other techniques rely on the fragmentation of one element into multiple SGML elements and then knitting the fragments into a whole; this is the method used for example in the part attribute of the <l>-element and in the <join>-element. There are also several possibilities permitting 'true' overlapping features. Within SGML/TEI the 'concur'-feature allows a document to be marked up concurrently using more than one DTD, with each tag labelled with the name of the DTD to which it belongs. Other none-SGML systems include MECS (Multi-Encoding-System), a system developed at the Wittgenstein Archives at the University of Bergen. MECS permits any two codes to overlap, but has on the other hand no specific document grammar, and therefore no SGML-alike document validation is possible. We have chosen not to use such systems in our project, as they do not seem suitable for our purposes, partly because they make text encoding too complex. Furthermore, encoding 'true' overlapping structures with concur, MECS, or similar systems, does not really solve the problem of deciding how to structure the text: you have the possibility to encode overlapping features, but you still have to decide which feature(s) to encode, and even which not to encode. The boundaries of the different features are not always clear and text features seem to exist both dependent and independent of each other. Our project uses the first printed editions as base texts for the edition. When encoding texts, we try to reproduce these texts as accurately as possible. When choosing to use standard TEI, we thus have to deal with the problem of encoding the textual features into hierarchical SGML-documents. This also means choosing which text features to encode and which not to encode, when that is necessary.   Encoding 'Norma': In the case of 'Norma' we have ended up encoding the text as an ordinary drama. The additional features of the text that do not seem to be part of a 'normal' drama structure, but still can easily be encoded into the drama structure (e.g. footnotes and speeches in brackets), are also incorporated, even though this means 'violating' the logical structure of a drama. On the other hand additional features that depend on redefining the whole conceptualization of dramas (e.g. speeches in stage directions) so far are ignored in the encoding (but documented elsewhere, e.g. in the header of the document). 'Norma' is an ambiguous text, and while we want to incorporate as much as possible of this ambiguity in the encoded version of it, not all ambiguity can be kept, and however the encoding is done, the encoded text could possibly be ambiguous in new ways. It may seem like a paradox that the encoding of features necessary to give possibilities for electronic processing and analysis of texts, at the same time includes interpretation that may restrict the use of the texts. There is no simple way out of this problem, but if the interpretation in the encoding is restricted at a reasonable level, the encoding can open up the text more than delimit it. Renear et al. (1993) state that 'It should be a commonplace that machine-readable texts are \"subjective\" and \"interpretive\", but not especially subjective or interpretative.', and that encoding a text in this aspect is much like making a traditional edition.   ",
       "article_title":"Text Structure vs. Encoded Structure - Dealing with Mixed Genres and Ambiguous Texts",
       "authors":[
          {
             "given":"Karl",
             "family":"Sæth",
             "affiliation":[
                {
                   "original_name":" University of Oslo, Norway  ",
                   "normalized_name":"University of Oslo",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/01xtthb56",
                      "GRID":"grid.5510.1"
                   }
                }
             ]
          },
          {
             "given":"Ingrid",
             "family":"Falkenberg",
             "affiliation":[
                {
                   "original_name":" University of Oslo, Norway  ",
                   "normalized_name":"University of Oslo",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/01xtthb56",
                      "GRID":"grid.5510.1"
                   }
                }
             ]
          },
          {
             "given":"Ellen",
             "family":"Nessheim",
             "affiliation":[
                {
                   "original_name":" University of Oslo, Norway  ",
                   "normalized_name":"University of Oslo",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/01xtthb56",
                      "GRID":"grid.5510.1"
                   }
                }
             ]
          },
          {
             "given":"Stine",
             "family":"Taugbøl",
             "affiliation":[
                {
                   "original_name":" University of Oslo, Norway  ",
                   "normalized_name":"University of Oslo",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/01xtthb56",
                      "GRID":"grid.5510.1"
                   }
                }
             ]
          },
          {
             "given":"Mette",
             "family":"Ekker",
             "affiliation":[
                {
                   "original_name":" University of Oslo, Norway  ",
                   "normalized_name":"University of Oslo",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/01xtthb56",
                      "GRID":"grid.5510.1"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Text Encoding"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Perdita Project was established in 1997 at the Nottingham Trent University, by Elizabeth Clarke and Victoria Burke of the English & Media Studies department, with Martyn Bennett of the History department, funded initially by the Nottingham Trent University. A substantial AHRB grant in 1999 meant that we could appoint research fellow Jonathan Gibson and researchers Jill Seal and Gillian Wright. Claire Warwick from the Department of Information Studies at Sheffield is electronic publication consultant to the project, which runs until 2002. We regard this paper as a collaborative exercise. In it we aim to introduce some of the dilemmas which we have faced during the project. We hope that our progress may be of interest to those at the conference, and that our discussions with the humanities computing community will also be of help to us in the continuation of the project. The Perdita Project is producing a comprehensive guide to manuscript compilations of early modern women. We are carrying out research on over 450 manuscripts written or compiled by women, which include miscellanies, commonplace books, account books, medical and cookery receipt books, religious writing, and autobiographical material. Our descriptions of the manuscripts will be encoded in SGML to allow extensive searching capacity, and will be published on the Internet in 2002. The interdisciplinary nature of the project is vital to our work, and is something that we will discuss throughout the paper. By describing previously unpublished materials by women of the 16th & 17th centuries, we aim to be a resource for scholars both literary and historical, enabling access to manuscript sources which are often very difficult to trace in comparison with published texts. We see ourselves as part of the movement to rewrite literary history, moving the emphasis from printed text and male and/or canonical works to a fuller picture of the writing of the period (Ezell, 1993). Electronic publication also allows us to address important issues to do with the dissemination of text in an electronic medium. This seems uniquely appropriate, since as Woodmansee (1994) argues, the transmission of scholarly electronic text shares some features with the coterie traditions of manuscript dissemination which we are studying in the early modern period. In the paper we shall explore some of the challenges we face in attempting to combine the two traditions and when working with the different media of manuscript, print and electronic text. We will discuss why we have chosen to encode manuscript descriptions rather than the texts themselves, and how far we should interpret the manuscripts in the descriptions that we provide. To what level, and in what way should these descriptions be encoded, and how will this affect their usefulness? How far should we try to consider the present user community? Standards, whether in manuscript description or in electronic text encoding, are also vital to our work. Projects which deal with the electronic publication of manuscripts tend either to provide users with digitised images of manuscript pages, and/or to transcribe and encode the text. However, we have chosen a different strategy, since we are not presently intending to transcribe entire manuscripts. Rather, we will present descriptions, which will take the form of an extended catalogue entry, including a list of contents, a physical description, and a biographical article on the compiler(s). We believe that there are valid scholarly reasons for doing so. There is already a certain amount of literary text available in electronic form, much of which is lacking any kind of commentary or contextual material. Given the time and financial constraints of our project, we therefore preferred to concentrate on a more novel research area. Our methodology has been designed as a response to the shift in focus in manuscript studies from the search for authoritative texts to the historical circumstances of manuscript production and circulation (Beal, 1980-, Marotti, 1995, Woudhuysen, 1996, Hobbs, 1992, Love, 1993). Rather than simply producing large amounts of transcribed text with no accompanying commentary or contextual research, we prefer to make an important scholarly contribution to this research area. We also consider that it is important that our resource should lead scholars to visit archives, and consult the manuscripts themselves, when possible. Since the provision of digital surrogates tends to increase the amount of usage of the original material (Lee 1998, Chapman, Kingsley & Dempsey, 1999), we feel that out efforts should be directed to descriptive scholarly research to aid researchers in their use of original documents. We do, however, acknowledge the problematic nature of our task and of classifications such as authorship, function and gender in looking at manuscript compilations, and pledge ourselves to giving \"as much information as possible to facilitate useful readings of the manuscript compilations\". But what are \"useful\" readings, how much information should we give, and in what form? We therefore intend to conduct a study of our potential user community in collaboration with Sheffield University DIS to try to answer these, and other, questions. However, we are aware of the potential problems of trying to ensure that the resource remains usable and accessible by a community of future users whose needs we cannot hope to predict. This means that we need to apply, and in some cases set, standards in various areas. Most obviously, we must apply the highest standards in manuscript cataloguing and description. There are also other areas which we are particularly well-equipped to explore, for instance, a standard vocabulary for describing handwriting. Most fascinating of all is the question of what a woman's hand might look like. Electronic delivery will provide us with an ideal opportunity to contribute to this discussion, by providing visual samples of women's hands. We are also concerned with the standards necessary for electronic publication. We must be aware of how far the standards necessary for text encoding and useful searching impose interpretations on the manuscript, since already, in our editing and cataloguing process, we are working at several removes from the original text (see fig. 1).We are conscious that the decisions we make in encoding the descriptive material must not be so prescriptive that they hinder usage, but at the same time we aim to aid searching by appropriate markup. We therefore approach this project in the spirit of the text-encoding initiative. However, further complications are caused by the fact that what we are encoding is essentially metadata, not simply transcribed text. We would like to explore the ideological, conceptual and practical differences between the TEI, metadata systems, and the use of controlled vocabulary. In the world of electronic resources there appears to be a culture clash between a post-structuralist, qualitative 'search for anything you like - create your own text' ideology and the controlled, quantitative approach to classifying objects taken by museums. This may represent the difference between dealing with text and dealing with objects, or the difference between English and History, where computing projects tend to deal with statistics. However, we at Perdita are faced with the problem of trying to balance the two approaches. We are aware that the TEI header is ideal for a project which wants to shape the data in the form of the original text, even if it does not encode it. Some historians are now beginning to recognise that 'data' is deeply embedded in text, which is why we believe that TEI is the best ideological option. Yet, database pioneers such as The Getty Institute in LA are encouraging us to adopt database methodology and to use terminology and Thesauri, because of their more systematic nature. This view is supported by research by DIS at Sheffield, which suggests that when constructing and using metadata, many users find the lack of a controlled vocabulary inhibits the ease with which they can search electronic resources. (Whittaker 1999) At the British Women Writers' Conference at Albuquerque in September, everyone recognised the need for a standard set of keywords. Unfortunately no such resource yet exists. We have therefore decided to view our work in the light of future users. We will describe our attempts to combine both standards and to provide some sense of the original text, using TEI markup, with a standardised search vocabulary for ease of searching.  ",
       "article_title":"Perdita's Progress: Raising Standards in a TEI-based Approach to Cataloguing Early Modern Manuscripts",
       "authors":[
          {
             "given":"Jill",
             "family":"Seal",
             "affiliation":[
                {
                   "original_name":" Nottingham Trent University, UK  ",
                   "normalized_name":"Nottingham Trent University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04xyxjd90",
                      "GRID":"grid.12361.37"
                   }
                }
             ]
          },
          {
             "given":"Claire",
             "family":"Warwick",
             "affiliation":[
                {
                   "original_name":" Sheffield University, UK  ",
                   "normalized_name":"University of Sheffield",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/05krs5044",
                      "GRID":"grid.11835.3e"
                   }
                }
             ]
          },
          {
             "given":"Elizabeth",
             "family":"Clarke",
             "affiliation":[
                {
                   "original_name":" Nottingham Trent University, UK  ",
                   "normalized_name":"Nottingham Trent University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04xyxjd90",
                      "GRID":"grid.12361.37"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The larger objective of this paper is to follow the lead of Matthew Kirschenbaum and the participants in his panel at last year's ALLC/ACH conference in Charlottesville, Virginia, in thinking about visual electronic resources as structured data. More specifically, I propose to turn the spotlight on a significant portion of the William Blake Archive's metadata: its SGML-encoded image descriptions. Introductory remarks will emphasize the image descriptions as a design feature intended to maximize use of and complement the Archive's other image resources, but the balance of the paper will consider more foundational issues: what exactly are image descriptions in formal terms? What is their relationship to their first-order objects? What kinds of problems do they present for the project team? What functions do they serve? My aim is to provide a broad overview of our practices and the challenges we face (both theoretical and practical) in describing images. The full version of this presentation will attend closely to the discursive features of the image descriptions and their principles of inclusion of pictorial data on the grounds that they serve as a barometer of the general reliability of the search and retrieval functions. The image descriptions and their characteristic terms - which are best considered as a unit - are an integral part of the Archive's paradigm of image search and retrieval. General descriptions are available via a link to a separate window from all Object View pages in the Archive, and more specific descriptions are returned in the course of an image search. The descriptive commentary is also available to the user who invokes an Inote session from any of several windows and pages. (Inote, which has received much press in humanities computing circles since its public debut, is a java-based image annotation tool developed at IATH that superimposes a four-quadrant grid on an image. Clicking an area of interest opens a separate annotation viewer containing the editorial commentary targeted to the selected region.) The image descriptions are conceptually inextricable from the Archive's characteristic terms, a menu of which the user selects from when launching an image search. Though it is tempting to devote a portion of the paper to an evaluation of how the Archive's controlled vocabulary measures up when compared to other established classification and vocabulary browsers (such as IconClass, the Art and Architecture Thesaurus, and the Library of Congress Thesaurus for Graphic Materials), that subject is sufficiently complex to warrant separate treatment at some future date. Strictly speaking, the Archive's search engine consults only the SGML-encoded characteristic terms when returning hits to a user. The exact function of the prose that appears between <illusobjdesc> tags is more difficult to define. My provisional account of the purpose of the descriptions is threefold: first, they illustrate that the visual-to-verbal transposition isn't a simple one-step operation: more complexly, images are first captured in prose and from the prose we extract smaller searchable units. Second, the descriptions provide a system of checks and balances that allow the user to cross-reference them with the characteristic terms, making the underlying logic of the hits returned in the course of a search session explicit. Third, they offer us a space where Willard McCarty's metadata rule of disambiguation can be violated: as I will explain in more detail in my talk, a fair amount of uncertainty and doubt is built into the descriptions as a response to iconographic ambiguities in the source material; this interpretive uncertainty is, if not effaced entirely, at least diluted at the characteristic level. These functions aside, it is my sense that the descriptions aren't being mined for information as effectively as they could be, though the beta version of WBA 2.0, scheduled for public release later this summer, should make it possible to harvest their data in new ways. Because the Archive's image search and retrieval software, Dynaweb, must consult an SGML information base that is, of course, textual rather than pictorial in content, the descriptions and their corresponding characteristic terms serve as a verbal proxy for their visual objects. To put it otherwise, the annotations function as metatext (rather than, as in conventional print relationships, paratext) to the primary data, creating an unusual onus of responsibility for the editors and assistants to provide descriptions and terms that show as much fidelity as possible to the originals in order to guarantee optimum results for the user searching across the visual collection. This process of faithfully, accurately, and thoroughly translating from one medium (the visual) to another (the verbal) is, as one would expect, beset by difficulties. But we are not without precedent or aid. In particular, the problems we encounter in the course of composing our image descriptions can be profitably understood by looking to the growing body of literature in Humanities Computing on encoding transcripts of source materials. My suggestion assumes that at heart the two endeavors (describing images and transcribing texts) are first and foremost acts of translation - the former from pictorial data to linguistic data, the latter from - to take the example of the Beowulf Project - chirographical marks in a codex manuscript to the descriptive codes of SGML - and as such are prey to all the difficulties inherent in the translation process. At last year's ALLC/ACH conference, Paul Caton and Julia Flanders raised the issue of the measurability of data as an important consideration for project teams interested in encoding renditional information: they stressed the necessity of  \"having a clear sense of what the units of information are, how to identify and distinguish them consistently, and how to record them accurately. It [the criterion of measurability] represents/requires an attempt to decide . . . what threshold of perceptibility will be maintained: . . . what will be considered either too small or too costly or difficult to record.\"  At the Blake Archive, our efforts to extract iconographic content from the source material for the purposes of image description are routinely complicated by just the kinds of measurability and perceptibility difficulties outlined by Caton and Flanders. Should Blake's interlinear motifs be captured in as much detail as the large-scale designs? Should the hatched lines that appear meaningless in the 100 dpi image but representational in the 300 dpi be described (and thus made searchable)? Should we even avail ourselves at all of the 300 dpi images as resources to transform our unaided human eyes into Argus eyes, or is this approach contrary to the central tenets of diplomatic editing? What about the left arm of that central figure on plate 16 in copy D of The Marriage of Heaven and Hell, which was printed but colored on the impression so that it blends almost imperceptibly with the adjacent figure's gown? Is that wispy line extending from the ascender of the letter \"b\" a bird or a vine, or is it both? Indeed variations on the last question may reflect the most common kind of perceptual ambiguity the annotator of Blake encounters, attributable in part to Blake's almost profligate use of metaphors; his peculiar syncretic insight makes it dangerous to ever rule out the possibility that a design motif has multiple referents: flora and fauna are often indistinguishable, a human figure may be a composite of several characters, and a lock of hair easily morphs into a coiled serpent. The final version of the paper will address our working solutions to the problems enumerated in the foregoing paragraph. I will conclude by briefly shifting the emphasis from the creation of image descriptions to their potential use, offering some parting thoughts and caveats on the kind of knowledge that the image descriptions might produce when used in conjunction with the Archive's other image tools and resources.  ",
       "article_title":"Image Description at the William Blake Archive",
       "authors":[
          {
             "given":"Kari",
             "family":"Kraus",
             "affiliation":[
                {
                   "original_name":" University of Rochester, USA  ",
                   "normalized_name":"University of Rochester",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/022kthw22",
                      "GRID":"grid.16416.34"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Text Encoding"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper discusses the goals, architecture, and usability of Kirrkirr, a Java-based visualization tool for XML dictionaries, currently being used with a dictionary for Warlpiri, an Australian Aboriginal language. While dictionaries on computers are now common, there has been surprisingly little work on innovative ways of utilising the capabilities of computers for visualization, hypertext linking and multimedia in order to provide a richer experience of dictionary content. Most electronic dictionaries present the search-dominated interface of classic information retrieval (IR) systems, which are only effective when the user has a clearly specified information need and a good understanding of the content being searched. The ability to browse often makes paper dictionaries easier and more pleasant to use than such electronic dictionaries. Search interfaces are ineffective for information needs such as exploring a concept. Some work in IR has emphasised the need for new methods of information access and visualization for browsing document collections (Pirolli et al. 1996), and we wish to extend such ideas into the domain of dictionaries, in part because indications are that current interfaces are unlikely to have much direct educational benefit for students (Kegl 1995). Our goal has been to provide a fun dictionary tool that is effective for browsing and incidental language learning. In particular we attempt to address Sharpe's (1995) \"distinction between information gained and knowledge sought\". The speed of information retrieval that e-dictionaries deliver, and the focused decontextualized search results they provide, can frequently lead to loss of the memory retention benefits and chances for random learning that manually searching through paper dictionaries provides. Within the Australian context, indigenous dictionary structure and usability are often dictated by professional linguists, while the needs of others (speakers, semi-speakers, young users, second language learners) are not met. Another major goal has been to design an interface usable by, and interesting to, young users and other language learners. From this viewpoint, the low level of literacy in the region, and the inherently captivating nature of computers suggests that an e-dictionary is potentially more useful than a paper edition. Among other benefits, we can provide an interface less dependent on good knowledge of spelling and alphabetical order. Our dictionary interface initially targeted Warlpiri, a language of Central Australia, for which there has been an extensive on-going project for the compilation of semantically-rich lexical materials (Laughren and Nash 1983, Hale and Laughren [to appear]). We converted this data from a non-standard format into a richly-structured XML version (XML 1999). The current version uses ad hoc indexing of this textual version for efficient access, but we expect to move to XQL, as this standard matures. Our system is written in Java, using the Swing API, and runs on all major platforms (Windows, Mac, Unix). For dictionaries with plain textual content behind them, there is little that they can provide in the way of output but an on-line reflection of a printed page. In contrast, XML allows definition of the precise semantics of the dictionary content, while leaving unspecified its form of presentation to the user. We exploit this flexibility in our application, by having the program mediate between the lexical data and the user. The interface can select from and choose how to present information, in ways customised to a user's preferences and abilities. One dimension is that as well as the definitions of words, users frequently want to know their relationships to other words, and the patterning in these relationships. Kirrkirr provides a color-coded network display of semantic links between words, which can be explored, manipulated and customised interactively by the user (Jansz et al. 1999) using the animated graph-drawing techniques of (Eades et al. 1998, Huang et al. 1998). In their spring algorithm, a network of words become nodes which are held apart by gravitational repulsion, but kept from becoming too far apart by springs which have a natural length. This graph algorithm differs from most others by providing iterative updating of the graph layout, which means that users can drag nodes across the screen, and the algorithm will cause other nodes to flee out of the way, while words related to another word are dragged along. The detailed semantic markup of the dictionary, with many kinds of semantic links (such as synonyms, antonyms, hyponyms, and other forms of relationships) allows us to provide a rich browsing experience. For example, the ability to display different link types graphically as different colors solves one of the recurring problems of the present web, with its one type of link: users have some idea of what type of relationship there is to another word before clicking. Thinking of the lexicon as a semantic network with various kinds of links was a leading idea of the WordNet project (Miller et al. 1993), but the simple text based computer interface they provide fails to do justice to the richness of the underlying data. Others have attempted to remedy this lack (e.g., Plumbdesign 1998), but we feel that our work is better aimed at providing the kind of simple network display suitable for our users. To augment traditional semantic relations in the dictionary, we provide also linkages derived automatically from collocational analysis (of the limited amount of online Warlpiri text), and present an interface derived from semantic domains. These interfaces both address the notion of \"terminology sets\" - words that belong together, a notion which seems particularly salient for native speakers (Goddard and Thieberger 1997). We discuss the determination of collocational bonds, using the method of Dunning (1993), and the limitations of what we can do with the data available. Formatted dictionary entries, displayed using HTML, are produced from the underlying XML by the use of XSL stylesheets (XSL 1999). These provide conventional hypertext for navigating between entries, in particular providing a color-coding of different kinds of semantic relationships between words which is consistent with that in the network display. A variety of XSL stylesheets are provided, which can give different formatting to the dictionary content appropriate to different users. For instance, items such as abbreviations for parts of speech, and other grammatical notes, and detailed decompositional definitions can be confusing for most Aboriginal users (Corris et al. 1999), and style sheets can provide just the desired information in large easy-to-read type. In addition to the above, the dictionary incorporates multimedia - the user can hear words and see appropriate pictures - and a conventional search interface. The dictionary provides a user-friendly console where search results can be sorted and manipulated. As well as standard keyword search, which can optionally be restricted to appearance within a specified XML entity, the system provides two features targeted towards two principal groups of users. Linguists often want to search for particular sound patterns (such as certain types of consonant clusters), and so the system allows regular expression matching for such expert users. On the other hand, the limited literacy level of many potential users means that they will have particular problems looking up words. In part this is due to particular problems whereby the phonetic orthography of Warlpiri does not match very closely to the (rather arcane) spelling rules of English in which their literacy skills are usually based. To alleviate this problem, we have implemented a \"fuzzy spelling\" algorithm which attempts to find the intended word by using rules which capture common mistakes, sound confusions and alternative spellings. We have performed some preliminary trialling of the dictionary through visits by Mim Corris to Yuendumu and Willowra, and Jane Simpson to Lajamanu. This has involved completing dictionary tasks, and observational use with primary and lower secondary students and trainee Warlpiri literacy workers, and comments from teachers and other adults. In general reactions have been quite enthusiastic, and the dictionary does appear to succeed in creating and maintaining interest. We have received suggestions on how to make it a better basis for classroom activities, which we hope to incorporate in future versions. The diversity of areas researched in this work is rare relative to past work in electronic dictionaries, which often addresses the problems of storage, processing and visualisation/teaching as unrelated. Despite some significant research into the construction of lexical databases that go beyond the confined dimensions of their paper ancestors, there has been little attempt at seeing this work through to benefiting people such as language learners, who could truly gain from a better interface to dictionary information. Additionally, the range of potential users here is considerably more diverse than encountered in typical studies of dictionary usability (e.g., Atkins and Varantola 1997). For instance, issues such as low levels of literacy are rarely touched on. Our system has attempted to reduce the importance of knowing the written form of the word before the application can be used, while having ample opportunities to learn written forms. Features such as an animated, clearly laid out network of words and their relationships, multimedia and hypertext aim at making the system interesting and enjoyable to use. At the same time, features such as advanced search capabilities and note-taking make the system practical as a reference tool. Having designed the system to be highly customisable by the user, it is also highly extensible, allowing new modules to be incorporated with relative ease. We thus think that it is a good foundation for an electronic dictionary, and while the focus of this research has been on Warlpiri, this research (and the software constructed) can be easily applied to other languages.  ",
       "article_title":"Kirrkirr: Software for Browsing and Visual Exploration of a Structured Warlpiri Dictionary",
       "authors":[
          {
             "given":"Christopher",
             "family":"Manning",
             "affiliation":[
                {
                   "original_name":" Stanford University, USA  ",
                   "normalized_name":"Stanford University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00f54p054",
                      "GRID":"grid.168010.e"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational / Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   I. Topic: Since September 1997, a small team of lexicographers and computer scientists at the University of Trier (Germany) have been developing an integrated electronic dictionary of Middle High German applying TEI Guidelines. The resulting integrated digital dictionary is expected to be finished by August 2000 and be published on CD-ROM as well as on the Internet. It is not only meant to facilitate the simultaneous use of the dictionaries concerned, but also to offer advanced query options to provide essentially new insights for those involved in vocabulary studies, metalexicography, and the composition of a new MHG dictionary.   II. Digitization as a necessity: The most important dictionaries of the MHG language were written in the last century and need to be replaced urgently by a new major work. This necessity arises not only from the enormous increase in the number of editions of MHG texts since the end of the 19th century, but also from changed insights into the structure of vocabulary and new ways of describing word usage. Consequently five years ago, two teams of lexicographers at the Universities of Trier and Goettingen started to lay the foundations for a new MHG Dictionary by creating an electronic archive of texts and quotations. It will probably take up to 25 years, however, for the whole dictionary to be finished, thus scholars of all disciplines having to deal with MHG sources will still have to use the older dictionaries for quite a while. The dictionaries that already exist, i. e. the \"Mittelhochdeutsches Woerterbuch\" by Georg Friedrich Benecke/Wilhelm Mueller/Friedrich Zarncke (1854-1866), the \"Mittelhochdeutsches Handwoerterbuch\" with its supplement, the \"Nachtraege\", by Matthias Lexer (1872-1878), and the \"Findebuch zum mittelhochdeutschen Wortschatz\" by Kurt Gaertner et al. (1992), are very closely interconnected and can only be used simultaneously, which is due to the fact that they must be considered, briefly speaking, as a kind of series of supplements to supplements to supplements. Therefore they were ideal candidates for the composition of an integrated digital dictionary. One of the major aims of the digitization is to make the lexicographical information of the dictionary entries accessible via a database and thus to enable sophisticated searches over all four dictionaries independently of headwords. Applying TEI Guidelines to machine readable versions of the printed dictionaries seemed the easiest and fastest way of creating the digital \"compound dictionary\".   III. (Semi-)Automatically generated markup according to TEI Guidelines: The MHG dictionaries consist of eight volumes with about 1,100 printed pages, containing more than 80,000 headwords. Therefore it is obvious that TEI compliant markup of the dictionary entries had to be generated automatically as far as possible. For the purposes of encoding we used TUSTEP, the Tuebingen System of Text Processing Programs with its variety of parameter-controlled functions for user-defined textdata-processing that facilitate structured entry-input. Some parts of the TEI design scheme were especially relevant for the dictionary encoding. Some advantages and problems when applying TEI have to be discussed in detail, such as the hierarchical embedding of elements within the articles, the use of global attributes for the markup of a wide range of lexicographical information, and the recoverability of articles. It should also be mentioned that TEI Guidelines should be improved with regard to the encoding of dictionaries of older stages of a language, for the description of such languages poses some problems seldom encountered when describing modern languages. It is apparent, however, that most problems which arose when using TEI did not stem from the application of TEI Guidelines as such, but were primarily due to the fact that the dictionary entries often appeared to lack clear structure and were rather discursive in style. This has often made automatic SGML encoding a difficult task. In many cases only manual markup led to TEI compliant documents. Nevertheless, the results achieved so far fully justify the decision in favour of TEI Guidelines.   IV. New ways of using dictionaries: Through the electronic version, the MHG dictionaries can be used much more easily and comfortably: hyperlinks connect all the corresponding headwords, the search for cross-references only takes a mouse-click's time; pop-up menus contain the relevant information about all sources of citation; bookmarks and notes can be created easily. PostScript files of all dictionary pages are interlinked with the electronic articles so that the compound digital dictionary can be used and cited as a work of reference in exactly the same way as its printed precursors. Far more important is the access to a database containing the relevant information for the entire contents of the four dictionaries within the composite whole. Access via that database not only offers full-text retrieval but also retrieval of selected information, e.g. of parts of speech, of word forms in MHG quotations, of definitions or of strings in the etymology sections of dictionary entries. Highly important for advanced and complex query options is the linking of a list of all dictionary sources with the electronic dictionary itself: all sources have been sophisticatedly classified according to geographical provenance, chronology and genre, categories that can be used to limit data base queries to a small, self-defined corpus of texts cited within the entries. Which words were directly borrowed from Italian, but not through Latin or French? Which words are only quoted from sources concerning legal issues? Which MHG words denote the same concepts? These are some of the questions that can now be answered without great expense of time. What is still more, the integrated electronic dictionary is especially important for the lexicographers involved in the creation of the new MHG dictionary where the older dictionaries are used as pointers to words for which references rarely exist.   V. Institutional frame: Some years ago, the Deutsche Forschungsgemeinschaft (DFG = German Research Council) initiated a program for the so-called \"Retrospective Digitization of Library Materials\". The main goal of the program is to facilitate the access to library holdings that may be rare or highly important for scholarly interests by providing electronic versions of these holdings. From the beginning, the program encouraged the use of SGML for full-text encoding. Since September 1997, the DFG has been funding the creation of an integrated digital dictionary of Middle High German to be published on CD-ROM as well as on the Internet. It is intended to serve as a prototype for the digitization of other historical dictionaries, including the digitization of the famous \"Deutsches Woerterbuch\" of Jacob and Wilhelm Grimm.   ",
       "article_title":"New Paths in Middle High German Lexicography: Dictionaries Interlinked Electronically",
       "authors":[
          {
             "given":"Johannes",
             "family":"Fournier",
             "affiliation":[
                {
                   "original_name":" University of Trier, Germany  ",
                   "normalized_name":"University of Trier",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02778hg05",
                      "GRID":"grid.12391.38"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational / Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In many languages, words can be used in different domains from those in which they originated. In English, sound verbs are commonly used in the context of human communication (1-4). (1) ('Shut up, Doreen,'[MESSAGE]) (Silas[SPEAKER]) barked, his face contorted by a scowl. (2) ('Darling,'[MESSAGE]) (Conrad[SPEAKER]) cooed as Lee entered the living room. (3) ('He's a thief, Hilary,'[MESSAGE])(he[SPEAKER]) grated almost savagely. (4) (Grandson Richard[SPEAKER]) rumbled (a reply[MESSAGE]). However, not all sound verbs have communication uses; the ones that do are restricted as to the type of message and/or speaker they can occur with. The syntactic patterns of sound verbs used for communication are not the same patterns found with true communication verbs. Several researchers (Goossens 1995, Miller & Johnson-Laird 1976, Levin et al. 1997) have explored these phenomena, paying particular attention to which verbs have or lack communication uses. Here we propose a unified and expanded corpus-based account of these cross-domain extensions in terms of Frame Semantics (Fillmore 1982) and with reference to theories of metaphor. This analysis has implications for the further description of the relationships between frames (e.g., inheritance and blending), and the development of cross-domain uses of words. The FrameNet Project (P.I. Charles J. Fillmore) is creating a lexical database with 3 linked components: the expanded lexicon, the Frame Database, and annotated example sentences (Baker et al. 1998). Files which represent senses of lexical items within a particular domain and frame (represented as domain/frame) are created, and constituents are annotated with the Frame Elements which are realized with respect to the target word. This annotation, and subsequent marking of phrase type and grammatical function, is further analyzed for the combinations of syntactic and semantic patterns realized in various senses. Even while still in progress, this project has become a valuable resource for lexical and other linguistic analysis. The authors, both researchers involved in all stages of the project, have examined annotated files for 201 verbs in perception/noise, 23 verbs in communication/manner, and 60 verbs in communication/noise (there are 314 communication verb files overall). Sound originates in the domain and frame of perception/noise. Since communication involves human verbal interaction, it necessarily overlaps with the sound domain. Two criteria determine which noise verbs can have communication uses. First, a noise verb is usable as a communication verb if the sound is produced by animate beings, especially animals (e.g. bark, yelp), but not when it is produced by objects (e.g. clink, thud). Nevertheless, some inanimate noises, such as rumble, are used for communication. It is possible that the physical profile of these sounds lends itself to a communication construal. Secondly, among animal sounds, imitative sounds (e.g. oink, quack) have no uses as communication verbs; the exact specification of the sound blocks the expression of a message (5). (5) *(Mr. Baker[SPEAKER]) oinked (an invitation[MESSAGE]) across the table. The noise verbs with communication uses (e.g. scream, bellow) do not behave like genuine manner of speech verbs (e.g. shout, whisper), differing from them both syntactically and semantically. We argue that this reflects the differences in the structure of the two domains and frames. In their home domain, noise verbs are usually intransitive, taking the sound SOURCE as subject (6-8). (6) Somewhere behind her (a horn[SOURCE]) blared. (7) (The long blades[SOURCE]) clashed and rang, their movement too fast for the eye to follow. (8) The ducks began quacking and (the frogs[SOURCE]) croaking. By comparison, communication verbs are normally transitive, with a SPEAKER subject and a MESSAGE object. ADDRESSEE and TOPIC prepositional phrases and MANNER adverbs frequently appear (9-11). (9) ('How's the shop?[MESSAGE]) mumbles (one balding sweating man[SPEAKER]) (to another[Addressee]). (10) One of his body squires heard (him[SPEAKER]) whispering (about it[TOPIC]) (to his Gascon favourite[Addressee]). (11) 'If (you[SPEAKER]) so much as whisper (a word[MESSAGE]) (about Dame Agatha[TOPIC])(to the Lady Maeve[ADDRESSEE]), you will regret the day I ever plucked you out of Newgate!.' Consider a communication use of the sound verb 'snarl' (`Do you have to?' she snarled at him as he took out a cigarette). Whereas `Do you have to?' [MESSAGE] and she [SPEAKER] look like canonical communication frame elements, (at him) is not a typical encoding of ADDRESSEE; compare the oddness of (I talked at John). The effect of (at him) is to make him seem more like the target of a directed sound emission as in (The dog barked at me). The difference between real manner of speech verbs and communication uses of noise verbs can also be observed in terms of complementation patterns and their frequencies. For example, more quoted MESSAGEs are found with noise verbs than with manner of speech verbs. The pattern is the reverse for that-clause MESSAGEs. ADDRESSEEs are less common with noise verbs used for communication than with regular manner of speech verbs (12). (12) (The housekeeper[SPEAKER]) left the room, muttering (about ingratitude[TOPIC]). This difference can be exemplified statistically by the analysis of proportional samples of representative verbs from each domain and frame. Noise verbs used for communication do not only differ from manner of speech verbs as a class, but also exhibit interesting differences among themselves. For instance, many verbs are specialized as to what kinds of speakers they accept: older people and females are better cacklers, while men and people in positions of authority are more likely to rumble, bellow, or grunt (13-15). (13) ('I'll warrant he is!'[MESSAGE]) (the old lady[SPEAKER]) cackled unexpectedly. (14) We passed (the police sentry who[SPEAKER]) grunted (a sleepy greeting[MESSAGE]). (15) ('Off now then?'[MESSAGE])chirped (the woman[SPEAKER]), dropping another sock. Also, inasmuch as the manner of the speech act is being emphasized, the quoted [MESSAGE] component frequently contains an alphabetic representation supporting that emphasis (16). (16) ('Th-that's b-blackmail,'[MESSAGE]) (she[SPEAKER]) spluttered. This analysis shows that in these cross-domain uses, semantic and syntactic factors from both source and target domains play a role in determining the structure of the utterance. While the target domain supplies a syntactic structure, the source domain's semantics constrain the degree to which that syntactic structure can be exploited. Although some of the domains' interactions resemble metaphorical mappings, e.g. the SPEAKER-SOURCE correspondence, the relationship between the domains is not that of metaphor. Both domains are concrete, rather than one being concrete and one abstract. Instead of being discrete domains, they have something in common, i.e. the presence of a sound source. Nor are they simple cases of situational metonymy between speaking and producing sound. This kind of evidence and data can be used to describe more the complex interactions of frames which are evidenced in natural language: frame blends and inheritance, metaphor, complex frames, and other cross-domain uses. The synthesis of linguistic theory, lexicography, and work with large-scale corpora is necessary for significant coverage of the data. The frame semantic approach, with detailed lexical analysis, provides a semantically and syntactically informative account.  ",
       "article_title":"Shouting and Screaming: Manner and Noise Verbs in Communication",
       "authors":[
          {
             "given":"Margaret",
             "family":"Urban",
             "affiliation":[
                {
                   "original_name":" University of California, Berkeley, USA  ",
                   "normalized_name":"California Coast University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05t99sp05",
                      "GRID":"grid.468726.9"
                   }
                }
             ]
          },
          {
             "given":"Josef",
             "family":"Ruppenhofer",
             "affiliation":[
                {
                   "original_name":" University of California, Berkeley, USA  ",
                   "normalized_name":"California Coast University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05t99sp05",
                      "GRID":"grid.468726.9"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   The Biblioteca Italiana Telematica (Italian Library Online) is a digital library of representative texts of the Italian cultural tradition from the Middle Ages to the 20th Century (literature, language, history, philosophy, art, music and cultural history in its general sense). The library aims to be a major research and teaching facility at the service of researchers and students of Italian language and culture throughout the world. The library has been set up by the CIBIT (Centro Interuniversitario Biblioteca Telematica Italiana), which includes interdisciplinary research groups in 16 Italian universities and also sees to the library's management. The project's software implementation has been entrusted to Eugenio Picchi of the Institute of Computational Linguistics (ILC) of the Italian National Research Council (CNR). The project is financed by the Italian Ministry of Higher Education (MURST): research programmes of relevant national interest, Biblioteca Telematica Italiana: the Italian cultural tradition on the Internet (1998-99), and Textual memory: editions, studies and tools for computer analysis of the Italian heritage (2000-2001). The design of the Biblioteca Telematica Italiana has been quite complex due to the many different functions that had to be developed for and integrated within it. The purpose of the present paper is to illustrate: the project's objectives; the competencies deemed to be necessary and how they have been provided for; system choices made and development procedures performed up to now; the results achieved to date; further lines of ongoing development.    Objectives The objectives established for the project can be summarised as follows: to create an online reference and research tool that satisfies the demands of a broad public of varying characteristics, including:  the simple student or general user interested in retrieving a text, obtaining bibliographical information on it, reading and/or downloading it; the researcher aiming at philological and linguistic research on single texts or on sub-corpora of texts that can be defined dynamically on the basis of numerous selection parameters; the researcher interested in carrying out advanced philological and linguistics research on particular sub-corpora of texts marked up according to various criteria (grammar, metrics, concepts, etc.);  managing texts that include graphics and sound; bringing the whole textbase into conformity with SGML-TEI standards; to endow the library with a true catalogue, that is, a rich professional body of bibliographical information on the texts; this, for two reasons: to furnish multiple access points for the selection of sub-corpora to be queried according to the most varied research demands; creating a catalogue able to interface with the online catalogues (OPAC) of the large libraries with the aim of integrating the data and making consultation of the Biblioteca Italiana Telematica available from such OPACs; securing the textbase by restricting download of texts through suitable login procedures and, possibly, download charges.     Competencies The project includes three main kinds of specialised expertise: content-related (literature, language, history, philosophy, art, music, etc.), spread widely throughout the research project participants; linguistic-computational (the team directed by Eugenio Picchi at the ILC-CNR); bibliographical - library science (A. Petrucciani, University of Pisa - A. Scolari, University of Genoa); digital graphics (Studio X-Lab, Pisa).    System choices and development procedures Particular attention has been paid to the quality of the services offered and their optimisation. Besides offering the possibility of reading and downloading texts, we have also aimed to provide online all of those text-analysis tools offered in stand-alone linguistics, philology and computational lexicography applications. We hold such tools to be crucial features for the very concept of an online library. The procedures for storage, analysis and querying of the chosen text corpus is the DBT system (Data Base Testuale) developed at the Pisa ILC by Eugenio Picchi. The main efforts in integrating the text-analysis tools with the telematics library have been directed at providing the same functionality and uses as the local programs over a network connection. Procedures for reading and querying texts are by far the services demanded most frequently. The utmost attention has therefore been devoted to response times, the ability to serve the greatest number of users contemporaneously, and to offering the maximum possible guarantee of recovery from most problems that may arise. Such considerations underlie the decision to develop the consultation and query system in the Java language, as it is able to satisfy a wide range of requirements: it is a stable tool on both the server and client ends; it makes the service accessible to all Internet-capable hard- and software platforms through a suitable browser; it provides state-sensitive sessions, that is, able to maintain information regarding the dynamics of the varied requests made by each user. A set of specialised applets is therefore able to guarantee access to the library's consultation functions: searching the catalogue of the available e-texts in order to select the desired text and/or sub-corpora using a whole series of pre-set bibliographical selection keys; reading single texts; querying the selected texts through the typical querying and text-analysis procedures of DBT. The applets have been developed with the aim of maintaining the greatest possible compatibility with the DBT system, to which many users are accustomed. While for the foregoing, most-used functions, our efforts were largely directed at providing optimal service, for other, more specialist functions, it was decided instead to favour simplicity of use, a feature furnished more readily by a development and access technology other than Java. The techniques adopted here are ISAPI/CGI scripts, which enable establishing lighter client/server connections, with on-the-fly creation of HTML pages in response to different user requests and interactions. Such scripting has been applied to the querying of lemmatised texts, that is, texts for which morpho-syntactic analysis and classification have been performed beforehand. Queries can thereby be launched also on the para-textual data containing a wide range of information on each word, such as: its lemma, the lemma's grammatical classification, the form's morpho-syntactic classification. Such a wealth of information stored in a text allows for qualitatively more precise and far more productive search functionality. Particular tools for the online querying of texts with metric markers have been developed using MS-Access. In anticipation of the forthcoming Java or ISAPI applications necessary to make such tools directly available online, they have for the moment been implemented using the Windows NT Terminal Edition that, through a special plug-in, allows browser access to the server-side OS through an Internet connection. Finally, by integrating the contributions of the different specialists outlined in the preceding section, we have set up the Biblioteca Telematica Italiana Web site (), within which easy access to the different functions has been organised ergonomically from four fundamental points of entry: Reading, Catalogue, Collections and Advanced Searches.   Results The textbase currently online (Dec. 1999) contains about 900 texts. The Biblioteca Italiana Telematica currently offers the following services through the Internet: consulting the general catalogue through different cross-indexed access points (Author, Prose/Poetry, Literary genre, Language, Chronology); these stem from a relational database of records containing bibliographical information concerning the text itself, its reference edition and its electronic edition; consultation of the graphics catalogue; accessing the texts and relative text-attributes, iconographic and sound add-ons (through the DBT Java applet described above); querying individual texts with advanced computational-linguistics search functions (through the DBT Java applet); querying the entire corpus or selected subsets (through the DBT Java applet); querying a sub-corpus (Dante Alighieri's vernacular and Latin works) with grammatical markers (through the lemmatised ISAPI/CGI DBT application described above); querying a sub-corpus (medieval-Renaissance lyric poetry) with metric markers (through the Windows NT Terminal Edition described above); downloading texts in varied formats, including SGML/TEI, upon completion of suitable identification and login procedures.    Further lines of development   Development of tools for online query of texts with metric markers (corpus of medieval-Renaissance lyric poetry), as well as musical texts (corpus of 17th century Italian poetry for music). Enhancing the search system to allow consultation of bibliographical databanks according to structured DBT procedures. Implementing a prototype able to natively process SGML/XML files, particularly in TEI format (Text Encoding Initiative). Testing procedures for interaction between, on the one hand, library catalogues and other bibliographical databanks based on the UNIMARC international standard and, on the other, digital libraries using forthcoming standards for network delivery (meta-data, particularly Dublin Core). Inter-communication and the greatest possible integration between the two environments is in fact essential to avoid a rift between the two major sources of publicly available information. Implementation of a terminological repository containing thesaurus relationships and the relative procedures for conversion to and from the UNIMARC and Dublin Core standards. Testing procedures for importing and exporting standard UNIMARC format data to and from the Biblioteca Italiana Telematica catalogue. This, with the aim of storing the electronic resources of the Biblioteca Italiana Telematica within various library OPACs, starting with those of the universities co-operating in the project.     ",
       "article_title":"The \"Biblioteca Italiana Telematica\" Project (http://cibit.unipi.it): a Progress Report",
       "authors":[
          {
             "given":"Mirko",
             "family":"Tavoni",
             "affiliation":[
                {
                   "original_name":" University of Pisa, Italy  ",
                   "normalized_name":"University of Pisa",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03ad39j10",
                      "GRID":"grid.5395.a"
                   }
                }
             ]
          },
          {
             "given":"Eugenio",
             "family":"Picchi",
             "affiliation":[
                {
                   "original_name":" CNR, Italy  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational / Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   1. Introduction One of Chomsky's early distinctions separated 'rule-governed creativity' and 'rule-changing creativity'. In the first, applications of a rule within a formal system produce new output, while in the second, some mechanism is used to change the set of rules themselves. While the overall concept of language as a rule-governed creative system has now gained general currency in linguistics, the distinction itself has found less favour. One group, however, which has used the concept, if not the term, is OuLiPo, the 'Ouvroir de Littérature Potentielle' (Potential Literature Working Group), a group of French poets, novelists and mathematicians which for the past several decades has dedicated itself to the exploration of constraints as a device for enhancing poeticity. Most notable among them have been authors like Raymond Queneau and Georges Perec. Two well-known examples of their work are 'Cent mille milliards de poèmes' (One hundred billion poems) by Queneau, which uses sheets of substitutable lines of a sonnet to produce 10 to the 14th power individual poems, and 'La Disparition' (The Disappearance) by Perec, a 300 plus page novel in which the letter e never appears. (See references for details.) More generally, OuLiPo has set as its goal the formulation and exemplification of a wide range of formal constraints which may be used to enhance and enrich poetic production. In what follows, we will survey some of the basic types of constraints. We will then show how some of these constraints may be embodied in a natural language generation system. The goal will be to illustrate how such a system may provide a prosthetic extension to humans' ability to exhaustively draw out the implications of a particular rule. Finally, we will show how such a system may also be used to modify the constraints themselves.   2. A brief overview of some OuLiPo constructs The operations described in OuLiPo texts may apply at all linguistic levels, from phonemes, to letters, to words, sentences and texts. On the systemic level, they include addition and subtraction of elements and restriction of choices, while on the serial level they include concatenation and permutation. Operations may be purely formal, or involve semantics as well. Finally, the elements involved may be comparable or disparate. Let us consider some examples:  Phonemes or letters  Lipograms: texts in which one or more letters is/are lacking (La Disparition, for example) Shared initial letter: le chat, le chien, le chameau, ... for example, where all words in the list are based on the orthographic constraint Sequences of letters: les agents, les ballons, les cambrioleurs... for example Anagrams: for example, the various permutations of a word such as 'rose', including 'eros'    Lexical level:  Synonymy: production of a text followed by systematic replacement of an initial lexical choice by a synonym S + 7: replacement of each noun in a text by the seventh following noun in the dictionary Word formation: for example, Raymond Queneau takes as a starting point the word 'dèvisager' (to stare someone in the face - le visage) and replaces this particular body part by two others: 'poitrine' (the chest) and 'jambe' (the leg) to create the new words 'dèpoitriner' (to stare someone in the chest) and 'dèjamber' (to stare someone in the leg). There is also an ironic secondary sense of 'removal of a body part'. Word contamination: for example, Queneau blends 'formation' (training) and 'sillon' (furrow or groove) to produce 'formasillon', with its ironic identification of training and the straight and narrow path.    Syntactic level:  Homosyntactic structures: repetition of the same syntactic frame. La Disparition contains many series of the sort, for example: L'Alhambra brûlait, l'Institut fumait, l'Hôpital Saint-Louis flambait. (The Alhambra burned, the Institute smoked, Saint-Louis Hospital flamed.) l'iris malin d'un cachalot colossal, narguant Jonas, clouant Cain, fascinant Achab. (the evil iris of an enormous sperm whale, mocking Jonas, striking Cain, fascinating Ahab) Suicida-t-il? Appuya-t-il un canon sur son zygoma? S'ouvrait-il au rasoir dans un bain chaud? Avala-t-il un bol d'acqua-toffana? Lança-t-il son auto dans un trou sans fond tourbillonnant sans fin jusqu'au soir du Grand Jour, jusqu'au jour du Grand Soir? (Did he commit suicide? Did he put a barrel to his zygoma? Did he open his veins in a hot bath? Did he swallow a bowl of acqua-toffana? Did he drive his car into a bottomless pit which swirled without end until the evening of The Day, or the day of The Evening?)      3. Formalisation and generation of constructs The examples described above are formalisable, but they have been generated by humans, using their intuition at a particular point in space and time. From the computational perspective, it is interesting to ask to what extent the computer may provide us with a prosthetic device to exhaustively explore the consequences of a particular construct. In this vein, we have used the VINCI generation environment to model examples such as those above. Briefly, VINCI is a collection of metalanguages which allows a linguist to specify grammatical information, and an interpreter which produces utterances based on the specification provided. It includes the following components: a. Context-free phrase structure rules for specifying sequences. These include terminals (N, V, etc.) and attributes (masc, human, etc.). Output is in the form of structured trees. Generation of successive levels may be controlled by the characteristics of the parent level. b.Transformation rules, which allow systematic modification of initial trees. c. A lexicon, including semantic information, formal characteristics, and pointers to related forms (synonyms, antonyms, etc.) d. Word-formation rules, which may be applied either globally to an entire lexicon, or dynamically. e. Morphology rules, for inflexion.  Lessard and Levison (1995) and Levison and Lessard (1995) provide additional details. Most of the preceding examples can be captured by the VINCI formalism.  Shared initial letter  The following rule produces a sequence of masculine singular noun phrases, where each noun begins with the letter 'm'. SN = DET[masc,sing,déf] N[masc,sing]/ \"m*\"  Sample output includes:le moyen, le maillot, le malheureux, le moment, le mètre, le mal  Lipograms in e  These are more complex, since they require several steps, including a pre-sorting of the lexicon, to remove words containing 'e' internally, as well as those words whose morphology rule adds an 'e' (cf. grand - grande). An additional step can include a device for choosing words which begin with a vowel, thereby allowing the masculine article (cf. l'ami), or which are feminine singular (la condition). The following simple rule illustrates a specification of a small part of this:  SN = {Masculine singular human nouns starting in 'a'} ( DET[masc, sing] N[masc, sing, humain]/ \"a*\" ADJ[masc, sing, humain] | {or} {Feminine singular human nouns followed by adjectives whose feminin is identical to the masculine form ($13) thus avoiding 'e'} DET[fém, sing] N[fém, sing, humain] ADJ[fém, sing, humain]/$13 | {or} {etc. etc.} Sample output includes: l'ami obscur, l'amant amusant, l'avocat assis, la condition citron S+7 This can be achieved by pre-processing the lexicon so that each noun points to its seventh successor. A transformation can then be applied which replaces each noun by the word pointed to. If the pointers are in lexical field 13 with tag s7, we may write: ROOT = S7 : SN SN = choose Ge : Genre, No : Nombre; DET[Ge,No] N[Ge,No] S7 = TRANSFORMATION DET N : 1 2/ @13:s7  For the sake of clarity, we ignore a possible gender change. Word formation VINCI possesses a rich set of devices for forming new words. For example, given the example 'dèvisager' described above, the following rule systematically searches a lexicon for nouns having the semantic trait 'partieducorps' (bodypart) and produces appropriate output: \"*e\"|N|partieducorps.suj|?|?|?| _makes_ [\"dé\" + #1 + \"r\"]|V| | | | | % dévisager déventrer détêter dépoitriner dépatter déorganer déoreiller démembrer délanguer délèvrer déjouer déjamber dégorger défoier défacer déboucher débarber déailer déépauler Again, we have left the rule underspecified. Additional steps are required to add an -s before nouns beginning with a vowel. Homosyntactic structures It is clear that all the previous rules can be used iteratively to produce multiple occurrences of the same syntactic structure.   4. Meta-generation In the cases discussed above, the goal has been to generate utterances based on grammatical descriptions, the rationale being that humans are less good than computers at exhaustively enumerating the products of a grammatical rule. It is however possible to carry the process one step further. Since VINCI grammatical specifications are themselves only text files, it is possible to use VINCI itself to generate grammatical rules which in turn generate new utterances. The advantage of such an approach comes in cases where the computer can be made to generate a large number of potential new rules, each of which generates a large number of possible products. The result can be to cause the human to see previously unthought-of rule possibilities. To illustrate this, consider the following problem. Languages like French (and English) allow for multiple prefixes to be added to a base form. For example, Queneau uses the word 'archidyssymètrique', which has two prefixes, while a form like 'non-anti-defoliant' is a possible English form. One way of capturing such possibilities would be to enumerate all rule combinations by hand. This would be tedious and prone to error. A better alternative would be to allow a meta-rule to generate a large number of possible prefixation rules and then to use these to produce typical examples. For example, consider the following meta-rules: MANYPREFIXES = (PREFIX: MANYPREFIXES | PREFIX ) PREFIX = (PREFIX_NON | PREFIX_RE | ...) ROOT = (MANYPREFIXES: N | MANYPREFIXES: V | MANYPREFIXES : ADJ ) where PREFIX_NON etc. are transformations. When applied, these meta-rules produce grammatical rules like: ROOT = PREFIX_NON : N ROOT = PREFIX_NON : PREFIX_RE : V ROOT = PREFIX_RE : PREFIX_NON : ADJ ROOT = PREFIX_NON : PREFIX_NON : PREFIX_RE : ... V  and so on, which in turn can be used to generate actual utterances. Obviously, additional elements would be required to control more precisely the nature of the prefixes, but the essential principle should be clear. An intermediate step is also possible, in which VINCI hands control temporarily back to a user. So, for example, a researcher might have put in front of him or her a partially developed meta-rule and be asked to insert particular values.    5. Conclusions We assume, with OuLiPo, that poeticity hinges at least partially on dynamic playing with constraints. Similarly, we have as a premise that at least some aspects of inspiration or creativity involve finding previously unseen patterns. Given these premises, a device which puts potential patterns before us (which prosthetically increases our power to envisage new devices) is of interest. In the proposed paper, we will illustrate these assumptions with a richer range of examples and with a more detailed discussion of the theoretical issues which underpin them.   ",
       "article_title":"Poetic Prosthetics",
       "authors":[
          {
             "given":"Greg",
             "family":"Lessard",
             "affiliation":[
                {
                   "original_name":" Queen's University, Canada  ",
                   "normalized_name":"Queen's University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02y72wh86",
                      "GRID":"grid.410356.5"
                   }
                }
             ]
          },
          {
             "given":"Michael",
             "family":"Levison",
             "affiliation":[
                {
                   "original_name":" Queen's University, Canada  ",
                   "normalized_name":"Queen's University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02y72wh86",
                      "GRID":"grid.410356.5"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational / Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction: The importance of corpora as resources has become more and more accepted over the years. Many types of corpora have been used for various different purposes but if one is searching for examples of \"general\" application and not restricting oneself to a particular sub-language, the development of a balanced corpus is of primary importance. Of equal importance is the adoption of a uniform standard annotation. The main areas of application of a text corpus are lexicography (also computational lexicography) and natural language processing, including specifically, adaptation to different domains and genres. For these purposes the corpus must be large (at least 100 million words), contemporary, heterogeneous, uniformly annotated and, for use in the United States, must contain American English. The size will ensure the adequate representation of infrequent words. The selection of contemporary texts is important for both lexicography and NLP, particularly in view of the significant changes in common text genres over the last few years brought about by electronic communication. Heterogeneity ensures that the range of language usage needed for the creation of \"general language resources\" is represented, and that one can explore a wide spectrum of language genres for NLP. Uniform annotation is paramount in any corpus and the collection of American texts ensures that the grammatical and lexical differences found in British English will not interfere with the classifying of American English.   Background: The first American text corpus that strived for this balance was the Brown Corpus developed by Kucera and Francis at Brown University in the 1960's. It was the model for many corpora that followed and is still being used today. However, it is a small corpus (one million words) and somewhat dated (the texts are at least 30 years old). It is true that a written language changes rather slowly over time with regard to grammar but there are changes in the structure and there are quite frequent additions of new lexical items. Recently, the British National Corpus (BNC) was released. It is a rather carefully balanced corpus and a very large corpus (one hundred million words). It also has the advantage of covering the time period from where the Brown Corpus left off until 1993. There are, nonetheless, two distinct disadvantages for Natural Language researchers and dictionary producers in the United States: (1) the corpus is, as yet, unavailable for use outside of Europe and (2) the corpus contains texts of British not American English.    Differences between American and British English: The grammar of American English (A.E.) varies from British English (B.E.) quite significantly. For example, British English often makes use of a to-infinitive complement where American English does not. In the following examples from the BNC, \"assay\", \"engage\", \"omit\" and \"endure\" appear with a to-infinitive complement; there were no examples found in our corpus of this construction although the verbs themselves did appear. Examples: B.E. \"Jerome crept to the foot of the steps, and there halted, baulked, rather, like a startled horse, drew hard breath and ASSAYED TO MOUNT, and then suddenly threw up his arms to cover his face, fell on his knees with a lamentable, choking cry, and bowed himself against the stone of the steps.\" B.E. \"A magnate would ENGAGE TO SERVE with a specified number of men for a particular time in return for wages which were agreed in advance and paid by the Exchequer.\" B.E. \" 'What did you OMIT TO TELL your priest?' \" A.E. \"`What did you OMIT TELLING your priest?'\"B.E. \"But Carteret's wife, who frequented health spas, could not ENDURE TO LIVE with him or he with her: there were no children.\" A.E. \"But Carteret's wife, who frequented health spas, could not ENDURE LIVING with him or he with her: there were no children.\" For the first two verbs, one can argue that there is not an equivalent verbal meaning in A.E. but, for the last two, the meaning can be paraphrased in A.E. by the gerund. Adverbial usage is also different. The B.E. use of \"immediately\" in sentence initial position is not allowed in A.E. For example, B.E. \"Immediately I get home, I will attend to that.\" is incorrect in A.E. where we would say \"As soon as I get home, I will attend to that.\" Other syntactic differences are formation of questions with the main verb \"have\". In B.E., one can say, \"Have you a pen?\" where A.E. speakers must use \"do\" (\"Do you have a pen?\"). Support verbs for nominalizations also differ. Note the B.E. \"take a decision\" vs the A.E. \"make a decision\". With these considerable differences and the fact that lexical items may be over- or under-represented or not present at all, it is clear that a corpus of American English is needed.   The proposed American National Corpus: As seen above, the corpora we have been working with are inadequate and the BNC although meeting our standards of size and balance does not deal with our language. In 1998, at the first LREC conference a proposal was made to create an American National Corpus (ANC) much on the lines of the BNC (Fillmore et al, 1998 [1]). The corpus should be as far as possible, contemporary (1990's). It should be both static (like the BNC) and dynamic (COBUILD). We will add regular increments but retain the capability to return to the initial corpus as well as the static stages between increments. The corpus will be both balanced and heterogeneous. The collection of more than 100 million words will make this possible. 100 million words of the ANC should be comparable in balance to the BNC to enable cross linguistic studies between British and American English. There is no set definition for what it means for a corpus to be balanced. The BNC made a principled effort to balance their corpus (see the BNC User's Reference Guide [2] for a break down of their corpus). The ANC will use this as a model. However, since it is also desirable to provide significant components from a wide range of styles, the remaining text will be varied rather than balanced (i.e. we will not try for differing percentages of texts according to their representative importance in the language but will try for smaller samples of a greater variety of texts). The corpus will be annotated at two levels, which serve two different user groups. Base Level will be annotated fully automatically with document, paragraph, sentence, token with POS marking. Level 1 will be heavily manual with the added text structure (titles, headers, footnotes, tables, captions, lists, etc.) which follow the CES standard (Ide, et.al [3]).    Progress towards the creation of the ANC: The ANC has progressed since its genesis at LREC 98. In May of 1999 the first ANC meeting preceded the Dictionary Society of North America (DSNA) meeting at the University of California at Berkeley. It was attended by a number of representatives of publishing houses. The idea of an American National Corpus was well received and plans for a second meeting were agreed upon.  The second meeting took place at New York University. Invitees to this meeting included not only those present at the May meeting but publishers from Japan and representatives from various software companies from the U.S. and Europe. More substantial issues were discussed including the structure of the consortium, questions of balance in the corpus, funding, time schedules and licensing agreements. Some questions were decided, others such as balance and licensing were referred to committees for further discussion.    The shape of the consortium and future plans: The licensing and base level annotation is to be done through LDC (UPenn). UPenn will obtain licenses from text providers and provide licenses to users. With regard to data rights, there will be multiple classes. The expectation is that there will be some subset of the data which can be made available under a form of general public license, and hence can be freely redistributed under this license. The membership agreement provides for paid memberships from commercial organizations. These members will receive the data as soon as it is processed and have exclusive rights to this data for a period of three years. They are expected to make monetary as well as data contributions. The data will be freely available to non-profit educational and research organizations (aside from a nominal fee for licensing and distribution).  Our plan is for the base level to be paid for with consortium fees. We have a 3-year time-frame starting Jan. 2000, with 10% of the corpus deliverable by summer 2000. Level 1 annotation which will require external funding, will proceed dependent on this funding. Therefore, this may lag as much as a year behind the base level corpus. Our goal is a fully annotated level 1 corpus compliant with the CES standard.   ",
       "article_title":"An American National Corpus: a Large Balanced Text Corpus for American English",
       "authors":[
          {
             "given":"Catherine",
             "family":"Macleod",
             "affiliation":[
                {
                   "original_name":" New York University, USA  ",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          },
          {
             "given":"Nancy",
             "family":"Ide",
             "affiliation":[
                {
                   "original_name":" Vassar College, USA  ",
                   "normalized_name":"Vassar College",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/022x6qg61",
                      "GRID":"grid.267778.b"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational / Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  What will be the critical editions in the electronic era? Hubert de Phalèse, a research center in La Sorbonne-Nouvelle University (Paris III), in accordance with its pragmatic approach to literary computing problems, decided to launch this debate by putting on line a critical edition of the complete works of Lautréamont / Isidore Ducasse (). This edition is an integral hypertext (in which nearly every word of the text is linked with a comment), which gathers all that one usually finds in the critical editions, but on a scale which does not have equivalents on paper: variants, philological, literary and encyclopedic comments, biography, bibliography, iconography, index, etc. This prototype poses, concretely, a certain number of problems, on several levels: Technical: Which interface is to be used? The purely automatic search engines (including the uses of Java and other script languages) appeared unsuited and a new device of computer-assisted indexing was developed. It makes it possible to provide to the user a lemmatized index and, especially, lexical cards which can be enriched at will. The current solution of setting on line presents some inconveniences but it has the advantage of proposing to the greatest number of users the consultation of the edition and of inviting them to take part in it. Contents: The new support is, virtually, infinite. What is a critical edition to contain now that we are not concerned any more with its volume? All the versions of the text, for example, can now be proposed with the reading. But does one have to publish the intertexts, contemporary works, criticism, etc? How can the interconnection, in network, of several resources enrich a critical edition? Under which scientific and legal conditions? Validation: Can this type of edition be regarded as more reliable than the paper editions? According to which protocols will such editions be judged? One of the risks is the apparition of a great quantity of work without scientific guarantee. How will the possibilities of collective work and permanent updating will modify our design of what a philological work should be? Publication: Who will deal with the building and the diffusion expenses of such electronic products? Will the redistribution of the budget headings in this type of edition lead the academics to transform themselves into diffusers or will the traditional editors change their practice? In addition, new prospects open with the critical edition, which it will be necessary to evaluate and explore to know the real potentialities of them. Work in group: Data processing and the Internet support the participation of a growing number of speakers around an intellectual work. Which will be the roles of each one (project director, data processing specialists, humanists, students, active readers, etc.)? The concepts even of authors and readers will not have any more the same direction. Real time: The possibility of permanent update offered by an Internet site makes it possible to revalue the traditional concepts. It is not indeed essential any more to put on line a completely completed work, and the noted errors can be immediately corrected. In addition, this type of edition makes it possible to account for the topicality of research in the field, which connects it with a review (of which the periodicity is much higher besides than for any scientific review). Interactivity: The possibility of putting in contact creators and users of electronic publishing, by means of the electronic mail, also connects this type of edition to a permanent conference. It is possible, in the long term, that scientific communities (specialists in an author, for example) gather around great electronic projects, that they would make live by publishing the results of their work there. Cost: The very low cost of setting on line such an edition (I except the working time of researcher)s makes it possible at the same time to consider some undertakings in the face of of which the traditional editors move back (very large corpus, work of interest only for few specialists) but also to allow researchers to publish under some good working conditions works of weak size or that don't fit in the framework of current university editions. Multi-media: What can be the contribution of multi-media to a scientific work like a critical edition? All in this field remains to be invented, because the traditional edition accustomed us to purely textual tools, primarily for reasons of cost. The sound and visual illustrations will bring to the literary text a very interesting dimension (publication of manuscripts, interpretations, iconographic documents, contemporary pieces of music, etc), from the teaching point of view as in the research field, but it is necessary to be wary of the easy effects which accustomed us, the general public, to electronic publishing. It is all the more urgent to answer these questions that the share of electronic documentation in literary studies would have, as in the other documentary fields, to increase until gradually replacing the traditional supports. Consequently, the survival of the texts and their formal characteristics will be closely related to the devices which will ensure their transmission, their conservation and their reading.   ",
       "article_title":"The FALMER Project: Toward an Electronic Critical Edition",
       "authors":[
          {
             "given":"Michel",
             "family":"Bernard",
             "affiliation":[
                {
                   "original_name":" Université de la Sorbonne-Nouvelle (Paris III), France  ",
                   "normalized_name":"New Sorbonne University",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/03z6jp965",
                      "GRID":"grid.17689.31"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Digital Resources"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper describes the philosophy behind what represents one of the most ambitious projects of its kind ever to have been undertaken in the Spanish-speaking world: The Miguel Cervantes Digital Library (). It explains the reasons behind its creation, the private-public sector alliance which has made it possible, and the new ground being explored by its creators in terms of innovative application of digital methods and of new services it offers to its audience world-wide. The Miguel Cervantes Digital Library is the result of a unique collaboration between Alicante University and Spain's biggest bank, the Banco Santander Central Hispano who have joined forces to create the world's biggest digital library containing Spanish-speaking works. It represents an example of successful partnership between university and business, with the Santandar Central Hispano Bank providing complete sponsorship for the full development of the project. The University, on the other hand, provides the academic expertise, technological know-how and qualified workforce necessary to fulfil objectives and ensures international use of the Library's resources by way of collaboration agreements with universities and institutions all over the world. The paper will address the issue of this partnering of academia and private enterprise as a case study of how two vastly different institutions have successfully worked together in the overall management and vision of a large, global project. The Miguel Cervantes Digital Library hopes to act as inspiration to other non-English speaking cultures to create their own novel digital tools which can be used by a multiracial and multilingual student and academic community of Internet users world-wide. Far from being a static collection of digitised books, the Library is envisaged as a vehicle for the Hispanic academy to promote their works, as a window to Hispanic literature and culture for scholars of Hispanic languages and cultures, and as a voice for the Hispanic university community world-wide. The actual content of the Library reflects this ambition as it includes sections such as: the digital publication of theses, which acts as a window to Hispanic research and a forum of academic debates a voice library with quality recordings of top Hispanic voices who have volunteered to recite one of the Library's most visited works. Well known Hispanic poets and authors also take part in this section, reciting parts of their own work a directory with over 3000 links to digital Hispanic resources and to outstanding digital collections, libraries and technology-related resources. All the links have a full explanation in Spanish for Hispanic users unfamiliar with the potential of the Web for study and research personalised pages on Hispanic authors, debate forums, its own electronic news bulletin, and a facility enabling readers, collectors and editors to interact and locate rare books initiatives aimed at bringing together \"the Old\" (printed material) and \"the New\" (electronically produced works). As well as a section called \"Trueque\" which brings together publishers, booksellers, collectors and readers in an attempt to locate rare and out-of-print editions, the project envisages traditional publication of text-books and critical studies accompanied by interactive electronic versions available in the Miguel Cervantes Digital Library \"Research\" section a section where the Library encourages and publishes in its pages the results of research projects and collaborations related to educational software development undertaken in or considered to be of interest to the Hispanic world. With the help of Latin-American governments and universities, we aim to equip villages, schools and communities with educational, literary and cultural material which, without modern technology, would be beyond their reach.  Another feature of this project is the use of technology to preserve and promote minority Hispanic languages, and to affirm different Hispanic cultures by way of specialised gateways and portals. The design of these sections does not only entail the use of new techniques, but also involves collaboration agreements whereby staff from the Miguel Cervantes Digital Library travel to other centres, universities and institutions all around different areas of Spain and Latin America in order to digitise \"in situ\" documents, objects and works of art. Existing language portals include Catalan, Basque and Galician, and gateways on Hispanic cultures are being developed on Cuba, Argentina and Mexico in collaboration with key institutions in these countries. Visits and contacts between academic institutions in these countries and the Library have illustrated the ground we need to make up in order to reach standards already achieved in other parts of the world as far as the use of digitised content for the study of the humanities is concerned. By opening up different channels of communication and collaboration within the different Hispanic communities (Spain, Latin America and the United States), we aim to provide academics all over the World, and especially in Spanish-speaking communities, with a major new research tool and to create a new concept of a digital library. During the past two years we have learned much from other worthy initiatives that are being undertaken in Spain, Latin America and further afield to digitise material reflecting Hispanic languages, literature and cultures. The author will provide a brief overview of the most notable projects in this area, and will offer a picture of the state of the art as to what techniques are being used to produce electronic resources in the Hispanic community at present and what the future holds. The final section of the paper will deal with the technical underpinnings of this project at present and in the future. Techniques used for digitisation of texts, manuscripts, images and voices will be discussed, and advances in this area made possible by certain strategic partnerships with universities, libraries and other institutions in Spain and abroad will be acknowledged.  ",
       "article_title":"The Miguel Cervantes Digital Library: The Hispanic Voice on the WEB",
       "authors":[
          {
             "given":"Andrés",
             "family":"Pedreño",
             "affiliation":[
                {
                   "original_name":" Universidad de Alicante, Spain  ",
                   "normalized_name":"University of Alicante",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/05t8bcz72",
                      "GRID":"grid.5268.9"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Digital Resources"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The advent of the electronic paradigm to the field of scholarly editing and textual criticism opens up new possibilities for both the production process and the delivery of products which may herald a new era in scholarly editing. A new practice including text encoding, automated tagging, automatic collation, the use of scripting languages, etc. creates new kinds of editions in which the record of textual variation becomes a central point of attention, both on the markup- and on the delivery-side. In this paper I will address both sides through a problematization of the apparatus criticus or variorum as an essential part of an electronic edition. The new reality of scholarly editing in which for instance archives become editions and editions include archives (McGann 1996 and Robinson 1996), and hybrid editions come into being as a combination of critical, diplomatic, facsimile and reading editions (De Smedt & Vanhoutte 2000), calls for a new theoretical framework and a thorough critique of the theory and practice of paper-based critical philology. The existing rationales of textual criticism cannot simply be transposed from the hard-copy to the electronic paradigm. This would for instance mean a transposition of the unpracticality and illegibility of the apparatus criticus or variorum to a medium which is essentially structured differently. The editor of a paper-based edition tries to design the apparatus as an economic and compact model in which to store textual variety, often through a combination of variants. This more than once results in an unsuperable density and thus a malfunctioning of a tool which should above all be transparent and consultable. The apparatus criticus or apparatus variorum of a printed critical edition fails, through its form and formality, in what it intends to do, that is to provide a substitute and a documentation of each complex source in such a physical form that it is usable for the interested scholar (Vanhoutte 1998). The possibility to include digital facsimiles in an electronic edition discharges the apparatus from the theoretical imperative of being a substitute. The need for documentation of textual variety, however, remains, but in electronic editions, compactness is being overruled by explicitness. On the markup-side, this means the use of a system which can tag every reading as well as genetic commentary, meta-data variation and variation over structural boundaries (Smith 1999). On the delivery-side, this means the use of a system which can supply the user with the possibility to consult every witness on its own and - facultatively - in combination with a suggested orientation text to an acceptable level of granularity. Over the past couple of years, several gentle solutions have been put into practice, which either function on the markup- or on the delivery-side (corresponding roughly with what Vanhoutte 1999 respectively calls the Archive- and Museum-function), but none of which provides a full answer to this documentation-maxim. Taking the three fundamental requirements of electronic scholarly editions - accessibility, longevity and intellectual integrity (Sperberg-McQueen 1994) - as parameters for an evaluation of possible designs of electronic editions, I will argue that there are at least three sorts of editions in the electronic paradigm: electronic editions, electronic editions with hypertext functionality and hypertext editions which do not meet the requirements of electronic editions. Further, this paper will (re)formulate additions to these requirements, focussing on modern literature. Because hypertext is the visualization of linking which DeRose & Van Dam (1999) define as \"the ability to express relationships between places in a universe of information\" and which are explicitly marked or can be generated automatically by making use of some sort of markup, the syntax of this markup and the markup-language become essential in designing a hypertext and/or an electronic edition. On the basis of the three fundamental requirements for electronic editions, the syntax of the markup (language) and the orientation towards a markup- or a delivery-side, I distinguish three sorts of editions in the electronic paradigm:  A. hypertext editions:  Are only concerned with accessibility: they explicitly link apparatus to base text, digital facsimile to base text, several versions to orientation text. Make use of (a combination of) HTML, JavaScript (for pop-up boxes), frames. Are being designed from a display point of view and are useful for didactic purposes and on-sight consulting. Hypertext editions of this sort are acceptable as a formatted output with a specific purpose for a specific audience, but are for visual purposes only.    B. electronic editions:  Are concerned with accessibility, longevity and intellectual integrity by making use of generic markup schemes in their design, such as those provided by the TEI (cf. chapter 19 of the P3 Guidelines), the MASTER scheme (Robinson, Burnard, Proffitt & Driscoll 1999), or a project specific SGML DTD. Can be concerned with linking external or internal apparatus to a base or an orientation text. Can be concerned with providing alternative views of the same document. Are being designed from a markup point of view and are useful to the scholarly community. Can be used to generate several spin-off products (including editions of category A).    C. electronic editions with hypertext functionality:  Same as B, but make extensive use of Pointer and Reference syntax to explicitly express relationships which can also be viewed in a browser, or make use of the present markup to programmatically generate linking.    From B and C it follows that the explicit documentation of textual variation in an apparatus criticus or variorum linked to a base text is no prerequisite anymore for an electronic edition. Textual variation can be documented implicitly by the textual description of each witness or document source. The extraction of alternative views of the witnesses, facultatively projected together with one version which functions as an orientation text, can be a possible solution to the problems concerning the instability of a (base) text. This paper will conclude with some notes on the use of XML and XSL in a construct which both caters for the markup- and the display-side.  ",
       "article_title":"Textual Variation, Electronic Editions and Hypertext",
       "authors":[
          {
             "given":"Edward",
             "family":"Vanhoutte",
             "affiliation":[
                {
                   "original_name":" Office for Scholarly Editing and Document Studies (BEB/OSEDS), Belgium  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Digital Resources"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Annotated material which is to be evaluated and possibly upgraded is used as training and test data for a machine learning system. The portion of the material for which the output of the machine learning system disagrees with the human annotation is then examined in detail. This portion is shown to contain a higher percentage of annotation errors than the material as a whole, and hence to be a suitable subset for limited quality improvement. In addition, the types of disagreement may identify the main inconsistencies in the annotation so that these can then be investigated systematically.   Background In many humanities projects today, we see that large textual resources are manually annotated with markup symbols, as these are deemed necessary for efficient future research with those resources. The reason that the annotation is applied manually is that there is, for the time being, no automatic procedure which can apply the annotation with an acceptable degree of correctness, typically because the annotation requires detailed knowledge of language or even of the world to which the resources refer. The choice of human annotators may be unavoidable, but it is also one which has a severe disadvantage. Human annotators are unable to sustain the amount of concentration needed for correct annotation for the amounts of time needed to annotate the enormous amounts of data present (cf. e.g. Marcus et al. 1993; Baker 1997). Loss of concentration, even if only partial and temporary, is bound to lead to a loss of correctness in the annotation. Awareness of this problem has led to the use of quality control procedures in large scale annotation projects. Such procedures generally consist of spot checks by more experienced annotators or double blind annotation of a percentage of the material. The lessons learned from such checks lead to additional instruction of the annotators, and, if the observed errors are systematic and/or severe enough, to correction of previously annotated material. Even with excellent quality control measures during annotation, though, it is likely that the end result will not be fully correct, and the measure of correctness can, at most, be estimated from the observations made in quality control. Obviously, it would be enormously helpful if there were automatic procedures to support large scale evaluation and upgrade of annotated material.   Methodology Unfortunately, as mentioned above, automatic procedures are currently unable to deal with natural language to a sufficient degree to correctly apply most types of annotation. However, although automatic procedures cannot provide correctness, they are undoubtedly well-equipped to provide consistency. Now consistency and correctness are not the same, but both are desirable qualities and, unlike other pairs of desirable qualities such as high precision and recall, they are not in opposition. Complete correctness is bound to be consistent at some level of reference and complete consistency at a sufficiently deep level of reference is bound to be correct. More practically, a highly correct annotation can be assumed to agree most of the time with a highly consistent annotation, which means that disagreement between the two will tend to indicate instances with a high likelihood of error. An example is provided by Van Halteren et al. (Forthcoming). One of the constructed wordclass taggers is trained and tested on Wall Street Journal material tagged with the Penn Treebank tagset. In comparison with the benchmark, the tagger provides the same tag in 97.23% of the cases. When the disagreements are checked manually for 1% of the corpus, it turns out that out of 349 disagreements, 97 are in fact errors in the benchmark. Unless this is an unfortunate coincidence, it would mean that we can remove about 10,000 errors by checking fewer than 40,000 words, a much less formidable task than checking the whole 1Mw corpus. In addition, the cases where the tagger is wrong appear to be caused in 44% by inconsistencies in the training data, e.g. the word \"about\" in \"about 20\" or \"about $20\" is tagged as a preposition 648 times and as an adverb 612 times. Such observations are slightly harder to use systematically, but can again serve to adjust inconsistent and/or incorrect annotation. In principle, the use of such a comparison methodology is not limited to wordclass tagging. Any annotation task which can be expressed as classification on the basis of a (preferably small) number of information units (e.g. for wordclass tagging the information units could be the word, two disambiguated preceding classes and two undisambiguated following classes) is amenable to be handled by a machine learning system. Such a system attempts to identify regularities in the relation between the set of information units and uses these regularities to classify previously unseen cases (cf. e.g. Langley 1996; Carbonell 1990). Several machine learning systems are freely available for research purposes, e.g. the memory-based learning system TiMBL () and the decision tree system C5.0 (). If we have a machine learning system and if we can translate the annotation task into a classification task, we can train the system on the annotated material and then compare the system's output with the human annotation. The instances where the two disagree can then (a) be used as prime candidates for rechecking correctness and (b) point to systematic inconsistencies to be reconsidered.    Overview of the Paper Using various types of annotated material and machine learning systems, this paper will attempt to answer the following questions: For which types of annotation is this method useful? How does the error rate in the 'highlighted' portion of the material compare to the overall error rate? At which levels of correctness of the annotation is the method useful? Are some machine learning systems better than others for the purpose at hand? Can we benefit from the fact that we have more than one system at our disposal and, if so, how? Should we use the full material in the training phase or is it better to use cross-validation?    ",
       "article_title":"Machine Learning Support for Evaluation and Quality Control",
       "authors":[
          {
             "given":"Hans",
             "family":"van Halteren",
             "affiliation":[
                {
                   "original_name":" University of Nijmegen, The Netherlands  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational / Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In this paper, seven uses of \"it\" are identified in English. These uses involve noun phrase anaphora (eg. \"Do not sweep the dust when dry, you will only recirculate IT.\"), verb phrase anaphora (eg. \"Raising money for your favourite charity can be fun. You can do IT on your own...\"), reference to clauses (eg. \"Not every city would be suited to this approach, IT must be admitted.\"), reference to entire discourse segments, (eg. \"Always use a tool for the job it was intended to do. Always use tools correctly. If IT feels very awkward, stop.\"), cataphoric reference to entities (eg. \"When IT fell, the glass broke.\"), pleonastic uses in which the pronoun has no reference and is used only due to a requirement of the grammar (eg. \"IT was {raining, 4 o'clock, All Saints' day, etc.}\", \"IT's recommended that...\", \"IT's easier to...\"), and uses in idiomatic constructions (eg. \"I take IT you're going now.\"). Due to the absence of a suitable term in the literature, the term \"non-nominal it\" is used to identify all the cases in which \"it\" is not in an anaphoric relationship with a noun phrase in the text. Numerous researchers have so far proposed hand-crafted rule-based pattern matching techniques to identify pleonastic \"it\". These methods have the drawback that they require recognition of potentially large and open-ended lists of trigger words and complex expressions in order to succeed. The goal here was to compare a rule-based method with a method devised to use machine learning to make the identification. It was hoped that information such as the position of the pronoun and its complex relation to the surrounding syntactic context would contribute to the accuracy of the identification. We implemented both methods. Corpora were constructed, annotated and used to classify and evaluate the accuracy of these programs. A comparison was made between them. The literature makes it easy to infer the importance of recognising non-nominal uses of \"it\" in the fields of anaphora resolution, information retrieval, machine translation and text summarisation. The task is especially crucial when it is considered that almost one third of the uses of \"it\" in our corpus of randomly selected texts were non-nominal. In the full paper, the treatment of pleonastic \"it\" in surveys of English usage is reviewed, as is work by Paice & Husk (1987), Lappin & Leass (1994) and Denber (1998) on methods for automatic recognition of pleonastic \"it\". The application of machine learning to a different problem in linguistics is described in the review of Litman (1996) on the automatic classification of cue phrases. One of the methods in the present paper applies machine learning to the automatic identification of non-nominal \"it\". A novel resource was required for this corpus-based research. A corpus was therefore constructed using 77 randomly selected texts from the BNC and stripped down versions of the Susanne corpus. We implemented a software tool that facilitates SGML mark up of instances of \"it\" that appear in the corpus by a human annotator. Non-Nominal uses of \"it\" are marked <PLEO ID=\"XX\">it</PLEO> whereas other instances are left unmarked by the annotator. On completion, the corpus contained 368830 words, 3171 occurrences of \"it\" and 1025 non-nominal uses. A DTD was defined for the annotated corpus and the SGML aware LT-Chunker (Mikheev 1996) was used to tokenise the corpus while preserving the prior mark up. The tokenised file was then processed by a Perl program written to report the paragraph, sentence and word positions of the non-nominal instances of \"it\". This information was written to a data file and used to evaluate the methods implemented and described later. We implemented a program based on Paice & Husk's (1987) method for recognition of pleonastic \"it\". In the first step, a plain text version of the corpus was tagged using Tapanainen & Jarvinen's (1997) SGML-blind FDG-Parser. The output from the tagger was converted to an SGML format by our software and then processed by our program based on Paice & Husk's pattern recognition method. In this way a classification was assigned to each instance of \"it\". Evaluation was performed by comparing the output of the program with the contents of the data file produced earlier. A machine learning approach was also implemented. It exploits Daelemans' (1999) TiMBL memory based learning method. TiMBL works by using a training file of feature-value vectors that have been given a classification: non-nominal; or not. The construction of the training file was made by processing a plain text version of the annotated corpus with the FDG-Parser and the SGML conversion program. The SGML file was input to a program that described each instance of \"it\" as a vector of feature values. The features used in our approach were designed to describe the position of non-nominal instances, the lemmas of significant \"following\" words such as verbs and adjectives, as well as the relation of \"it\" to other structures in the text, such as prepositions and noun phrases. A thorough description appears in the full paper. The vectors associated with the instances were classified by comparison with the data file constructed earlier. The set of classified vectors made this way was then used as the training file. TiMBL classifies query vectors according to their similarity to the examples in the training file. The method of ten-fold cross-validation was used to obtain an evaluation of the average accuracy of the technique over our corpus. The results of our automatic evaluation showed no major differences in the level of accuracy between the two methods. However, it was noted that the method based on work by Paice & Husk was slightly more accurate over this text (78.81% vs. 78.68%) but had a stronger tendency to misclassify instances as non-nominal (false positives: 265 vs. 243). If false positives are undesirable to the user, then the machine learning approach is better. Further experiments in which the classification of instances in the training set was extended using a 7-ary system, in accordance with the uses given in the introduction, showed some improvement in making the binary distinction between nominal and non-nominal uses. The classification accuracy rose to 78.74% and the number of false positive classifications fell to 209. Predictably, the detection rate for each of the different types of usage was low (50.35% on average). Given that TiMBL is reliant on a training file, it will also be beneficial to extend our resource in terms of size as well as information content. The present file, with 3171 instances, cannot be considered to be of sufficient size. The availability of a suitable resource for evaluation is also important for the application of optimisation techniques. Of course, non-nominal pronouns appear in languages other than English, and it would be valuable to generate resources in order to explore machine learning based methods to identify non-nominal pronouns for them.  ",
       "article_title":"A Corpus-Based Methodology for Identifying Non-nominal \"It\": Rule-Based and Machine Learning Approaches",
       "authors":[
          {
             "given":"Richard",
             "family":"Evans",
             "affiliation":[
                {
                   "original_name":" University of Wolverhampton, UK  ",
                   "normalized_name":"University of Wolverhampton",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01k2y1055",
                      "GRID":"grid.6374.6"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational / Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper presents the results of a large-scale corpus-based study (Hatzidaki 1999) of an important feature of English phraseology, namely binomial pairs (e.g. chalk and cheese, up and down, prim and proper, through and through, Laurel and Hardy). The purpose of the research was two-fold, firstly, to conduct, on the basis of a large and varied corpus of English textual data, a thorough and in-depth structural and functional analysis of a much-studied and yet not fully explored phraseological phenomenon; and secondly, to examine the hypothesis that the use of systematically collected samples of authentic language data results in more accurate and comprehensive descriptions of the form and function of linguistic phenomena than does the sole reliance on introspection. The analysis yielded an extensive and rigorous taxonomy of the various structural variations of binomials, as well as significant new information on their function in the communicative process. Binomials, namely sequences of \"two or more words or phrases belonging to the same grammatical category, having some semantic relationship and joined by some syntactic device such as 'and' or 'or'\" (Bhatia 1994:143), have long been objects of interest for idiomatologists and stylisticians. The several existing studies of this phenomenon have mainly focussed on its marked occurrence in the works of certain literary authors such as Chaucer, Lydgate, Shakespeare, Swift, Shaw, etc. (see, respectively, Héraucourt 1939 and Potter 1972, Tilgner 1936, Nash 1958 and Gerritsen 1958, Milic 1967, and Ohmann 1962), as well as in English legal texts; the semantic and syntactic characteristics and idiosyncrasies of the various paired forms, especially the semantic relationship between the linked members of a binomial (synonymy, able and talented; antonymy, boys and girls; complementarity, bow and arrow; Malkiel 1959), or the notion of irreversibility, i.e. the tendency of binomials to occur in only one sequence, as in here and there and not *there and here, and the possible causes of this phenomenon (e.g. 'proximal before distal'; Cooper and Ross 1975); and the incidence of binomials in languages other than English (e.g. Fix 1985 for German; Abraham 1950 for French and Italian; Malkiel 1959 for Russian, Portuguese, Spanish, Ancient Greek and Latin; Gold 1991 for Yiddish; Koch 1983 for Arabic; Szpyra 1983 for Polish; etc.). As opposed to literary studies, where binomials are treated as a flexible and interesting stylistic device which serves as a powerful means of expressing the authors' ideology and worldview, most studies of the occurrence of this feature in general language implicitly or explicitly regard binomials as a small and probably finite set of structurally and semantically idiosyncratic forms. Moreover, although many studies of the formal characteristics of binomials are available, there exists no comprehensive account of the full structural variability of the binomial pairs used by the average speaker, no detailed information on the distribution of the different patterns, and no organized taxonomy of forms. Finally, with the notable exception of studies of binomials as a distinctive feature of the language of the law which fulfils the requirements of legal draftsmanship for precision, clarity, unambiguity and all-inclusiveness (Mellinkoff 1963, Gustafsson 1984, Bhatia 1994), minimal attention has been given to the functions of binomials in non-literary language. Crucially, with very few exceptions (notably Gustafsson 1975), previous treatises on binomials have been intuition-based. A glance at a general corpus, however, instantly reveals a number of new and interesting facts concerning this feature. Firstly, numerous paired forms emerge, which appear to have been modelled on an abstract dualistic structure of the A + link + B type, very few of which, however, represent familiar, idiomatic locutions such as the oft-quoted rough and ready and out and out: the majority of the couplets appearing in corpus data constitute novel sequences such as calm and united, gently and effectively, inflation and unemployment, etc., whose formation seems to be governed by the specific lexicogrammatical, discoursal and pragmatic rules pertaining to the production of the texts in which they are encountered. Secondly, although couplets are extremely varied in their structural details, they all seem to fall into a set of identifiable lexicogrammatical patterns. And thirdly, the occurrence of the various dualistic patterns in textual sources with different situational characteristics demonstrates substantial distributional fluctuations. The above facts indicate that, in order to effectively account for the phenomenon of binomial pairing as it is observed in a corpus of texts, a new and more flexible data-driven framework needs to be devised. In the light of the data used in the present research, rather than a list of structurally and semantically peculiar couplets, binomials are analyzed as an abstract mechanism which speakers have at their disposal for the generation of a very wide range of paired types that serve a variety of important communicative purposes. As a theoretical model for the identification and extraction of binomials from the corpus and the classification of their various lexicogrammatical variants into a set of categories, we exploit the notion of phraseological frame or formal idiom, as posited and developed by Moon (1998:154f) and Fillmore, Kay & O'Connor (1988:505f). This, in very broad terms, represents an abstract structural formula which, as Fillmore et al. put it, 'serves as host' (ibid.:506) to institutionalized expressions as well as novel, spontaneously created forms. Binomials emerge as a major frame which can be represented by means of the general formula A link B. Our data analysis, which results in the construction of a detailed and comprehensive data-driven taxonomy of binomial patterns, involves, firstly, the identification and extraction of the various binomial forms from our corpus of textual data; secondly, the devising of a prototypical system of abstract representations to which each extant pair is assigned on the basis of its lexicogrammatical attributes; thirdly, the detailed recording of any interesting lexicosemantic preferences displayed by the patterns (for instance their semantic prosodies; Louw 1993 and Sinclair 1996); and, finally, the calculation of the frequency of occurrence of each pattern in the corpus.  We also discuss in detail the important but rarely addressed issue of the function of binomials in the communicative process. Specifically, we examine the incidence of the various binomial patterns in each of the six subcorpora comprising our corpus (a set of written publications in book form, both fiction and non-fiction; a broadcasting medium; a semi-specialized periodical publication; two daily newspapers, a broadsheet and a tabloid; and a set of spontaneous and semi-spontaneous spoken texts), and seek explanations for the very substantial distributional perturbations. The main purpose of this exercise is to establish the nature and extent of the correlation between the form and structure of binomial patterns on the one side, and the extralinguistic and situational factors pertaining to each subcorpus on the other, and, thus, to determine the precise functions served by each binomial pattern in communication. Our data strongly suggest that binomials constitute a phraseological device which makes a highly significant contribution to the communicative process. Our analysis demonstrates that, depending on their structure as well as the type of text in which they are encountered, binomials serve a wide range of communicative functions. For instance, it is shown that the abundant use of informationally dense binomials (e.g. government and parliament, political and monetary, commercial and investment banks) on the part of journalists serves most effectively the institutional requirements of the mass media for factuality, informativeness, precision, conciseness and stylistic uniformity (Crystal & Davy 1969, Tuchman 1978, van Dijk 1988, and elsewhere), whilst simultaneously disguising the highly fragmented process of production of news texts (Bell 1991). On the other hand, the frequent employment of repetitive, vague or informationally sparse pairs in conversation (ages and ages, here and there, try and get) reflects the efforts of conversationalists in the face of the exigencies of real-time communication. In the context of unplanned talk, binomials act as a lexicalized and, therefore, elegant and well-integrated temporal space which speakers create automatically and with the minimum of cognitive effort whilst coping with delays in the formulation of thought and argument. Binomials in extemporaneous conversation act as a crucial discourse-cohesive device, which helps keep speech 'glued together' (Johnstone 1987), whilst minimizing the effect of fragmentation (Chafe 1982) created by phenomena such as false starts, random repetition (Norrick 1987), etc. At the same time, binomials may be used by speakers as a means of expressing emphasis and emotional involvement and of creating rhetorical presence (e.g. faster and faster, ringing and ringing). On the whole, the corpus-based structural and situational analysis of binomials not only offers new and significant information on a well-known linguistic phenomenon, it also offers substantial empirical support for the hypothesis that phraseology plays a major part in the accomplishment of the communicative goals of speakers or writers (for a review of relevant studies, see Hatzidaki 1999).  ",
       "article_title":"Binomials and the Computer: a Study in Corpus-Based Phraseology",
       "authors":[
          {
             "given":"Ourania",
             "family":"Hatzidaki",
             "affiliation":[
                {
                   "original_name":" University of Birmingham, UK  ",
                   "normalized_name":"University of Birmingham",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03angcq70",
                      "GRID":"grid.6572.6"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational / Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   1. Introduction From 1922-27 Venn published the four volumes of Part I of Alumni Cantabrigienses, a biographical list of all known students, graduates and holders of office at the University of Cambridge, from the earliest times to 1751. This was followed from 1940-54 by the six volumes of Part II, covering 1752-1900. Subsequent archival research unearthed much more detail, and many more names, for the period up to 1500, and in 1963 Emden published his two-volume A Biographical Register of the University of Cambridge to 1500. Together, these twelve volumes cover approximately 180000 names, with some overlap. It goes without saying that all this information is of the utmost importance for historical research, covering as it does a large proportion of the religious, legal, administrative, medical, academic, and royal appointments in Britain, the Empire, and the Colonies, as well as many other countries. A good deal of social history is also included, albeit patchily. However, all these publications have a great defect for research: there is no index. Also, since Venn's work, many corrections and additions to the information have come to light, and without the incorporation of this new material it is easy to be misled by the original books. So, to find all the Vicars of Trumpington mentioned by Venn and Emden requires an exhaustive search of twelve large volumes and many card indexes of Addenda and Corrigenda.   2. Database We therefore set about the creation of an on-line database to make all this information accessible. Other sources, such as the Tripos Lists (lists of degrees awarded), and College Registers (especially those of the women's colleges, which were ignored by Venn) have been included. It is envisaged that the database will be made freely available for public searching on the World Wide Web, though it is not yet clear what mechanism for searching will be provided. Similar projects, based on Emden's Registers for Oxford and Cambridge, were undertaken in the 1970s.[3] In those studies, the data was highly coded to allow easy cross-tabulation. Several important articles using results from the studies have appeared.[1][2][4] There is, however, no resource for Oxford comparable to that of Venn for the later period. For many years we were unable to find a simple and reliable way to put the data into machine-readable form. Venn's books are in small hand-set type, printed on thick rough paper, and are full of italics, all of which proved completely intractable to the OCR packages available until recently. By chance, just as we had found suitable technology to cope with Venn's printing, we discovered that Ancestry.com had already prepared machine-readable versions of most of the volumes of Part II. Negotiations between Cambridge University Press (the copyright holders) and Ancestry.com soon led to an agreement to share this data, as their product and ours are for essentially different purposes, theirs being mainly accessed for genealogical information. Ancestry.com are also planning to put the remaining volumes of Venn into the computer, and to make the data available to us. Emden's Biographical Register, the Tripos Lists, and the registers of the women's colleges have proved relatively easy to read using OCR and the services of an excellent methodical proofreader.   3. Structural Analysis A typical entry from Emden looks like this (with references abbreviated): Dawson, John (Dauson).*  Entered in C.L. ET 1484; grace that study for 6 yr in C. and Cn.L. suffice for entry in Cn.L. gr. 1488-9; Inc. C.L., adm. June 1490 [Ref_1]; D.C.L.  R. of Debden, Essex, clk, adm. 17 May 1484;  till death [Ref_2]. Died 1492.  Will dated 10 Aug. 1492; proved 12 Feb. 1493 [Ref_3].  Requested burial in S. Michael's, Cambridge. and has the following structure: heading  event 1  event 2 ...   where each event in general comprises: topic  type  place  date(s)  reference(s) The initial form of the database is an SGML-tagged text, from which subsequent databases and searching/sorting structures can easily be obtained. My first attempts at analysis were written in Perl, a widely available string-handling language which allows complex regular expressions. (A regular expression is just a pattern which is used to match parts of the data and extract those parts which can vary.) It soon became apparent that the complexity of the regular expressions needed for the recognition of large-scale structures such as these entries uses too much memory in Perl, and the programs frequently failed. At Cambridge we have a locally-written programmable text editor called NE which has good regular expression handling. It may seem a retrograde step to use a one-off local program like NE in preference to a widely used standard such as Perl, but in our case only the product (the database) is useful; the process used to make the product is different for each text analysed, so the ephemeral nature of the analysis programs is not significant. Events will in general be split over several input lines, so it was first necessary to combine the lines of a complete paragraph into a single line, then to split them at punctuation such as semicolons, and to put references on separate lines. It was clear that some type of formal, structured, but readable output would be needed in the first instance. This could then be converted automatically as input to any required database package. SGML provides an adequate structure for these needs, and is widely used by publishers of machine-readable databases. First attempts at analysis were very heuristic, but served to clarify the problems in my mind. Writing a DTD for the SGML structure was then very helpful, as it forced me to take decisions about nesting of fields, etc.  Initially, my regular expressions tried to match complete events, including place names and dates, but two problems arose: the programs ran out of time or store, or NE's regular expression processor found the structure too complex to analyse. Automatically pre-tagging identifiable structures such as dates and place names enabled simpler regular expressions to be written.    4. Results A discussion of the complete DTD and the analytical processes used will be presented. Various types of results will be used to illustrate the processing, including complete updated entries amalgamated from all sources, and statistics about certain types of event such as religious appointments. The Figures (see below) are based on only part of the data (one volume of Venn), as the analysis is not yet complete. The Figures should be used with care, because although they represent approximately ten thousand individuals, they are constrained by having surnames beginning 'Abbey' to 'Challis', so a preponderance of one family attending one college may skew the results. Figure 1 will show the range of ages at admission to all colleges, and holds no surprises. Age at admission is given in only about half of the entries in Venn. Most of the older admissions are men who have already been ordained.  Figure 2 will show the admissions to the two largest colleges, Trinity and St John's, between 1752 and 1900, and illustrates the general increase in size of all the colleges, and hence the whole university, in that period. Figures 3 and 4 will show the admissions to other colleges during that period (except Downing, Selwyn, and the women's colleges, which were not founded until the nineteenth century). The outstanding feature of Figure 4 is the dramatic increase in admissions at Queens' College from 1821-1830. Figure 5 will show the number of religious appointments (Curate, Vicar, or Rector) per county.   ",
       "article_title":"ACAD - a Cambridge Alumni Database",
       "authors":[
          {
             "given":"John",
             "family":"Dawson",
             "affiliation":[
                {
                   "original_name":" University of Cambridge, UK  ",
                   "normalized_name":"University of Cambridge",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/013meh722",
                      "GRID":"grid.5335.0"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Digital Resources"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Roger Chartier rightly warns us that reading on a computer screen is not the same as reading a book when he says that  \"electronic representation of texts completely changes the text's status: for the materiality of the book, it substitutes the immateriality of texts without a unique location [...] in place of the immediate apprehension of the whole work, made visible by the object that embodies it, it introduces a lengthy navigation in textual archipelagos that have neither shores nor borders\" (18).  Hence any project in the electronic medium that lays claim to scholarly authority will require adaptations in both the presentation and the dissemination of its materials. My work on developing an electronic edition of Lyrical Ballads (by William Wordsworth and Samuel Taylor Coleridge, 1798-1805) helps illustrate how these changes may be realized. Bringing such an edition before the public has made it clear to me that the evolution of electronic publishing will necessitate innovations that are not just technological but also institutional. On the technical side, Peter Shillingsburg has set out the minimum requirements for a scholarly electronic edition. Such works need to have 1) \"a full accurate transcription and full digital image of each source edition,\" 2) \"a webbing or networking of cross-references connecting variant texts, explanatory notes, contextual materials, and parallel texts,\" and 3) \"a navigational system\" so that readers can thread their way through this complex hypertext environment (Shillingsburg 28). In preparing an electronic Lyrical Ballads we have tried to meet these standards, first by painstakingly transcribing in text files each of the four lifetime editions of the collection, and by gathering together digital images of every page of every edition. These images will complement our e-texts by being presented in windows set side-by-side on the computer screen. Second, our e-texts will be encoded using TEI-conformant SGML to ensure the preservation of their complex logical structure and to enable hypertext linking that will create a webbing of cross-references and make possible their distribution in a networked environment. The scholars engaged in this project have recently formed a decided preference for online delivery over CD-ROM distribution. Not only is Internet dissemination more efficient and cost effective, it also permits easy correction of errors that are likely to plague any scholarly edition. More important, the fluid nature of the World Wide Web will allow us to take ready advantage of new standards and new forms of interface as they develop. We can constantly add new materials and we can explore new paradigms for their presentation that will help accomplish Shillingsburg's third goal of ready navigation through a multiplicity of variant texts. Lately our new method of marshalling scholarly apparatus which I have called \"dynamic collation\" has been enhanced by the addition of popup windows generated by javascript which preview variant readings whenever the reader passes the cursor over a revised passage in the electronic text. This reconceptualization of how variant readings can be presented in the digital medium grounds in actual practice David Greetham's proposition that \"dismembering scholarly apparatus\" will be a consequence of the transition to the new medium (329). The real challenge for online publication, however, is what might be called the lack of mature institutional structures on the Web. So long as anyone can publish anything on the WWW, the quality of its materials remains questionable. Certainly, Matthew Kirschenbaum is right to remind us that \"publication entails a great deal more than simply the act of making public.\" Hitherto, some degree of authority has been conferred on a website by its association with an institutional host such as a university, as well as by the reputation of its author. But the imprimatur of an established publisher would be an even greater guarantee of the reliability of these immaterial texts. We are in the midst of negotiations that would see the electronic Lyrical Ballads published under the auspices of Cambridge University Press, through the good offices of the website of \"Romantic Circles\", a reputable online journal hosted by the University of Maryland. By reporting on the progress of these negotiations and their outcome, I hope to indicate ways in which both the Net and established publishers will need to evolve in an electronic world in order to guarantee standards of authority. The future of electronic publishing depends upon us asking questions about more than technical standards. Online scholarly archives must also meet standards of peer evaluation, editorial practice, and institutional approval that have traditionally ensured the quality of print publications. ",
       "article_title":"New Models for Electronic Publishing",
       "authors":[
          {
             "given":"Ronald",
             "family":"Tetreault",
             "affiliation":[
                {
                   "original_name":" Dalhousie University, Canada  ",
                   "normalized_name":"Dalhousie University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/01e6qks80",
                      "GRID":"grid.55602.34"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Digital Resources"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Despite the Herculean labors and remarkable achievements of the Perseus Project, today many essential resources for the study of the ancient Mediterranean world still remain accessible only to a few specially trained researchers because they have never been translated into modern languages or provided with sufficiently convenient interpretive materials. Our current work represents the first step in an attempt to address that problem by engaging the efforts of scholars world-wide in the production of substantial translated and annotated texts that will be made available exclusively through the internet. The text with which we have chosen to begin is the Byzantine encyclopedia known as the Suda, a 10th century C.E. compilation of material on ancient literature, history, and biography. A massive work of about 32,000 entries, and written in often dense Byzantine Greek prose, the Suda is nevertheless an invaluable source for many details that would otherwise be unknown to us about Greek and Roman antiquity, as well as an important text for the study of Byzantine intellectual history. The sheer size of the Suda (the most up-to-date printed edition runs to four hefty and tightly-printed volumes) and its lack of literary charm are sufficient to explain why no individual scholar has committed his or her career to translating it. Many scholars, each taking responsibility for selected entries or series of entries, can get the job done more effectively. Moreover, the vast breadth of subject matter covered by the Suda would challenge the expertise of even the most widely competent modern scholar. By sharing the load, individual translators can focus on those entries from the Suda that pertain to their area of expertise, thus producing better translations and more informed annotations. Begun in January of 1998, the Suda On Line (SOL) already involves the contributions (or promised contributions!) of nearly seventy scholars throughout the world. The general plan of the project is to assemble an SGML-encoded database, searchable and browsable on the web, with continuously improved annotations, bibliographies and hypertextual links to other electronic resources in addition to the core translation of entries in the Suda. Individual work becomes available on the web as soon as possible, with only the minimum necessary proofreading and editorial oversight. A diverse board of area specialists will eventually edit every entry, altering and improving the content as needed. The display of each entry will include an indication of the level of editorial scrutiny it has received. We want to encourage the greatest possible participation in the project and the smallest possible delay in presenting a high quality resource to a wide public readership. Collaborative efforts always generate questions about how to allot proper credit to individual contributors. Given the searchable database format of the SOL it is a simple matter for translators and editors to print out their own peer-reviewed work for inclusion in, for example, promotion and tenure dossiers. Moreover, we anticipate that translators will establish hypertextual links directly from their on-line rèsumès to their contributions in the SOL. Our choice of the web as the medium for publishing the SOL is crucial to the project's conception. This format has many advantages, of which accessibility and ease of use are perhaps the most obvious. Users can access the project's web page and search the database in various ways: by strings in a full text search, with Boolean combinations, by keyword, by translator, etc. The display for each entry includes the headword in English and Greek (options for displaying Greek suit the requirements of different systems), the translation, footnotes and other annotations, and bibliographical references (where available and/or appropriate). The SOL's interface automatically generates links to the complete Greek text of the entry from the database of the Thesaurus Linguae Graecae and to the relevant entries in Liddell and Scott's Greek Lexicon (via the Perseus Project); further links to both external and internal resources can be created in the text of the translations and the annotations. The on line format also allows for continuous editing and updating, which is crucial to our conception of the SOL as an evolving work forever subject to improvement by many hands. We believe that the specific ways in which we have enabled the process of editorial control and our plans for further enhancements are among the most sophisticated now available in any on-line scholarly resource: every aspect of communication among contributors is handled via web-based forms and dynamically generated e-mail. Any work goes onto the web immediately; individual authors do not have to wait for publication until the entire project is finished (as in the case of a print format). Thus, the project can be immediately useable even while the bulk of the work remains to be done. Furthermore, entries can be updated immediately whenever new information arises. Perhaps the most exciting aspect of a web-based publication such as the SOL, however, is the potential for interoperability with other projects. The SOL is one of many projects involved in a consortial arrangement at the Stoa, which is actively exploring ways to promote the interconnection of distributed projects. Although our goal is to have as much annotation and documentation as possible within the SOL database itself, our translation takes advantage of the natural capacity of web-based documents to be linked with other sources of electronic information. We want the SOL to be one important model for a new generation of hypertext commentary on ancient texts; a SOL fully outfitted with links to other electronic resources will provide not only the Greek text of the Suda and its translation, but also a wide variety of links to other relevant Suda entries, to the ancient vitae of any major authors or other figures mentioned in the text, to all the testimonia, and to essays by various scholars (both public-domain and new essays written specifically for this project). The same model may be used for on-line commentaries for other ancient works, which may in turn be linked to relevant entries in the SOL. The prospects for the on-line production of true variorum editions are vast and exciting. This copious annotation and hypertextuality will ensure that the on-line Suda is useful not only to classical scholars and historians, but to a much wider audience as well. Students at various levels will be able not merely to read the translated Suda entries but to understand their wider context. A bare translation would be of little use to most non-specialists, but a translation provided with a rich supply of links to other ancient works and to modern scholarship will open a whole world of information to the interested beginner and can still be a valuable research tool for the trained specialist. The goal of the SOL is not just to be a useful tool for researchers, but to provide a sophisticated model for the kind of scholarship made possible by open source technology and the internet, scholarship that is cooperative rather than solitary, communal rather than proprietary, worldwide rather than localized and evolving rather than static. Accordingly we aim at two principal results: in addition to our development of the Suda On Line itself as a respected scholarly resource, we plan to make a generalized, well-documented version of our software freely available for other scholars to adapt for their own purposes.  ",
       "article_title":" Academic Collaboration On Line: The SOL as a Case Study",
       "authors":[
          {
             "given":"Ross",
             "family":"Scaife",
             "affiliation":[
                {
                   "original_name":" University of Kentucky, USA  ",
                   "normalized_name":"University of Kentucky",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02k3smh20",
                      "GRID":"grid.266539.d"
                   }
                }
             ]
          },
          {
             "given":"Raphael",
             "family":"Finkel",
             "affiliation":[
                {
                   "original_name":" University of Kentucky, USA  ",
                   "normalized_name":"University of Kentucky",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02k3smh20",
                      "GRID":"grid.266539.d"
                   }
                }
             ]
          },
          {
             "given":"William",
             "family":"Hutton",
             "affiliation":[
                {
                   "original_name":" College of William and Mary, USA  ",
                   "normalized_name":"William & Mary",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03hsf0573",
                      "GRID":"grid.264889.9"
                   }
                }
             ]
          },
          {
             "given":"Elizabeth",
             "family":"Vandiver",
             "affiliation":[
                {
                   "original_name":" University of Maryland, USA  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Patrick",
             "family":"Rourke",
             "affiliation":[
                {
                   "original_name":" Nashoba Valley Technical Vocational High School, USA  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Digital Resources"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   What XSL Is The Extensible Style Language (XSL) is a specification currently being finalized (May 2000) by the W3 Consortium, the vendor consortium that proposes recommendations for web standards including HTML, CSS and now XML and its related technologies. XSL's immediate purpose is to support various kinds of presentation of arbitrarily marked-up documents in XML format. In an XSL system, any well-formed XML document could be formatted for print, displayed in hypertext (including on the web), or presented in other media, more easily and more effectively than is currently the case, and in a standards-based way. In a networked environment, processing documents for display on screen could happen on either server or client. In order to support its task of presenting XML (that is, applying to an arbitrary tag set a formatting description for a user interface such as screen or printer), XSL evidently has to provide for granular access to markup structures, so as to be able, for example, to derive tables of contents, text for running heads, indexes, and other common (presentational) expressions of underlying document architecture. In the course of working out XSL it became increasingly clear that (as is often the case with computer data processing problems) this problem was more easily, and more powerfully, addressed, if it was treated as a special case of a more general capability, namely the \"transformation\" of one markup structure into another. Accordingly, XSL is formally divided into two parts: \"XSL Transformations\" (XSLT): on a standalone basis, provides a language to describe many of the kinds of rearrangement and filtering of markup structures that a reasonably powerful XML presentation language requires. \"XSL Formatting objects\" (XSLFO): provides a vocabulary for describing, in a standard and abstract way, formatting of text for visual display in print or on screen (and possibly for alternative media presentation).  XSLT was accepted as an \"official\" W3C recommendation in November 1999. XSLFO is expected to be completed in mid-2000. The relative maturity of the specifications is reflected in the tools available: already by the end of 1999 a number of tools supporting XSLT were available on the Net for free. As of this writing, tools supporting XSLFO (which could, for example, convert XML via XSL into PDF format) are less mature. XSLT, on the other hand, was instantly in use by mid-1999, primarily (though not exclusively) as a way of converting XML into HTML. Because XML becomes instantly useful as soon as HTML can be reliably created out of it, this has in effect jump-started the XML presentation industry, at the price of keeping on-line published versions of XML source documents limited to the capabilities of HTML, the current state of the art in browsing on the web. As a result, even before the ink is dry, we are beginning to get a sense of XSLT's capabilities for processing - while at the same time we are still unclear as to what XSL's own \"design language\" (its formatting objects) will look like.   XSLT's Capabilities  -Presentational XSLT XSLT is already used to convert XML into HTML. In this, it is a ready alternative to a scripting approach (Perl, Omnimark etc.) or to the ISO standard DSSSL - and easier to learn than either. It also compares favorably in price: tools for XSLT conversions are free. -Analytic XSLT XSL processing is dependent on markup in the source text for navigation as opposed to (say) character offsets or line numbers. While very good at presenting information encoded in markup, it is not good at recognizing or construing implicit information such as character patterns. It does no tokenizing, hence cannot recognize \"word\" boundaries. By default, string processing and matching in XSLT is case-sensitive, and cannot readily be configured otherwise.  Somewhat surprisingly, however, XSL is nevertheless useful for certain kinds of analytical functions, including certain kinds of XML validation (cf. Rick Jelliffe's \"Schematron\"). For example, one could write an XSL stylesheet that would check the conformance of an instance of a TEI Header to a certain model, that went beyond the DTD to specify element dependencies - for example, reporting a warning (or providing defaults) if the publication statement were not filled out according to house standards. This could be done with a stylesheet and would not require altering the DTD. And because it can perform testing on strings, XSLT can also be used for generating rudimentary concordances. A concordancer in the form of an XSL style sheet will be demonstrated as a part of this presentation. The thing to keep in mind about any computer-facilitated analytic work is that, without being supported by information from an external source (such as a thesaurus of terms or a morphological dictionary), no algorithm is able to reveal something about a text that is not implicit in the text already. That is, while a computer can rearrange information in a text, and therefore perform such operations as counting incidences or providing indexes, it cannot actually add any \"knowledge\". What it does, is present a text, and information derived about the text, in such a way that a careful reader can come to conclusions about it that would otherwise be very difficult to demonstrate. This is merely to point out that, for example, a concordance is not an analysis, and by itself makes no argument, although it may facilitate the development of one. XSLT-based analytic work is no different, and since XSLT is not designed specifically with analytic work in mind, it is in some respects an unexpected benefit if it can support this work at all. Even given its fairly rudimentary capabilities, however, XSLT has certain incidental advantages: 1. It leverages investments made in markup:Many repositories have XML texts, or texts readily convertible into XML. These are all ready for XSL processing, and can be enhanced to support more sophisticated processing. 2. It produces \"publishable\" results as a natural work product: Since the end result of an XSL transformation can be HTML or an XML format ready for further processing, it is easy to generate results in a form that can be displayed as is. 3. An investment in XSL is worth making for other reasons: Since XSLT processors are so inexpensive (free), the real investment is in time to learn it. And XSLT is so portable and versatile, it pays off this investment in expertise fairly quickly. 4. It can be combined with other methods: An XSLT stylesheet can also be used to prepare XML texts for other kinds of work. An XSLT stylesheet can generate COCOA encoding from XML, that can be used to support TACT or another tool that takes advantage of COCOA markup of events in a text stream (such as chapter breaks or shifts in narrative voice). [An XSL stylesheet that creates COCOA markup from an XML TEI source can be demonstrated.]   Or, an XSLT stylesheet can be used to derive SVG (Scaleable Vector Graphics) files from descriptive XML source. SVG is a graphics format which is expected, by some, to revolutionize distribution of graphics for certain kinds of applications on the web. Graphic representations of phenomena accessible to XSL transformations can be already displayed in prototype SVG viewers. [SVG frequency distribution graphs of strings in an XML file can be demonstrated.] These are only two examples of ways XSLT can be applied to help prepare XML texts for a variety of further uses. The basic principle being applied is a layered architecture: the source data is maintained in a stable format, such as TEI XML, useful over the long term. Applied \"on top\" of this repository layer a separate process can expose a \"view\" or presentation of the source data (some readers may be familiar with the \"model-controller-view\" model of computer application design), ready for the special format requirements of an arbitrary application.   Role Of XSL/XSLT In The Future  - Possibilities for XSL extension: The XSL specification also provides allowance for its extension. Extension functions, in Java or an alternative scripting language, could be made available to an XSL processor. Tokenizing functions, sophisticated string processing and matching, database-integration services (for retrieving data such as morphological variants or checking values against an authority list) could all be addressable, given a good API, from within XSL stylesheets. It is unlikely, however, that such extensions (at least, those especially suited for the types of analysis academic humanists are interested in) would be developed in the private sector - not that they would be without profitable application there. But academic researchers, with clear focus on their own functional requirements, have to lead the way.    -An XSL browser as \"analytical engine\": XSL's potentials in these respects suggest that it could play a role in the markup-aware \"analytical engine\" that many of us keep envisioning (cf. the ELTA initiative). An XML browser that supported XSL stylesheets could be integrated with an editing environment allowing on-the-fly emendation of the stylesheets, and/or the extension functions they call. Stylesheets and function libraries could be pulled \"off the shelf,\" or written especially to address local problems and questions. Specialized functions would have the capability of integrating XSL's presentation/analytical capabilities with other tools such as databases or network applications. Not only would such a system be very versatile; also, in it, research results could take the form of ready-made publishable material, in HTML or any other markup-based form. Since it would basically be an XML web browser, it could also be readily networked, especially as concerns the XML source text (the text under analysis), which could be located anywhere on the Internet. Analytical stylesheets in XSL would be portable and applicable to any text that conformed to the same (sufficiently constrained) document model.   Present Advantages [as of the end of 1999]  -XSL tools are freely available: As of this writing, free XSLT processors are available in Java, and are not difficult to set up and run. Learning the stylesheet language itself is the biggest barrier to entry, and there are free and inexpensive resources for this as well.    -XSL is easy to get going with: By design, XSL is a declarative language, abstracted at a fairly high level. As a result, it is not difficult to learn, at least for most ordinary operations, and is very portable (making it easier to learn from others' work).    Present Disadvantages [as of the end of 1999]  -XSL is somewhat arcane:  Although the rudiments of XSL are not difficult, some users take to it less easily than others. It is a \"functional\" and \"declarative\" language unlike most scripting languages, so expertise in other computer languages is not readily applicable to it. Naïve users seem to have less trouble learning it than experts. The model of the text on which it operates, the \"document tree,\" although it leverages document markup in a very simple and powerful way, is not a self-evident approach to developers used to looking at text as a stream of characters.   -XSL processing is XML-based; requires well-formed XML to start: Obviously, XSL requires an XML text to operate on. Either this is a problem, or it isn't.   -Tools are rudimentary (although improving): Strong support for internationalization, for example, is envisioned by the specification but not yet widely implemented in interfaces or tools.   As mentioned above, it is unlikely that the private sector would, on its own initiative, develop function libraries that would provide for all the kinds of functions wanted by scholars in the Humanities. (Some, like support for sorting texts in major European and Asian languages, can be hoped for, although not necessarily for free.)     Conclusions  -What XSL will be good for: Presentation, filtering/rearrangement, markup-based processing such as indexing supported by markup. Some kinds of validation. Especially extended or in combination with other methods, XSL will also be capable of supporting sophisticated analytical functions on text marked up in XML.   -What the emergence of XSL tells us about our markup projects:  1. the up-front investment in the text (editorial work) remains the most difficult, interesting and important phase of work. Much or most further processing \"down stream,\" and the types of processing possible, are directly dependent on the features of the text represented through its markup. 2. investments in valid SGML/XML formats are demonstrating their resilience through readiness for new applications    Web Site  A web presentation of this paper will be made available at <>    ",
       "article_title":"XSL - Characteristics, Status, and Potentials for Text Processing Applications in the Humanities",
       "authors":[
          {
             "given":"Wendell",
             "family":"Piez",
             "affiliation":[
                {
                   "original_name":" Mulberry Technologies, USA  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper will address the theoretical and practical issues in devising and implementing a project-specific metainformation scheme for electronic resources. While one can argue that a scheme like the Text Encoding Initiative provides for encoding which greatly enhances plain text retrieval, in practice without extensive use of the keyword or indexing elements, retrieval of information is limited to what is explicit in the text. Searching for what is explicit in the text, even if that text has been encoded logically (as opposed to physically), does not provide the kind of functionality most humanists expect from digital archives. This paper then is an exploration of the advantages and disadvantages in creating a meta-meta information or classification scheme for electronic resources. For this talk I will draw heavily on theoretical models (both pre-and post-computer indexing models) from library and information studies. I will also adopt the position that creators of electronic resources are encoding their primary material in a SGML or XML-based metainformation scheme, such as the Text Encoding Initiative. I will also assume that the project directors have already made certain specific decisions in encoding what is explicit in the text in accordance with the project's goals. In other words, I am assuming that a digital project is already taking advantage of the tagging structure afforded in a scheme like the TEI in providing for the encoding of titles of text, place, personal, geographic and organisation names, etc., as deemed important to a particular project. There can be no doubt that this type of tagging greatly enhances retrieval, for example by distinguishing the occurrence of WB Yeats as a title as opposed to a personal name, or facilitating the searching of all strings within a <placename> element. And although this type of encoding of electronic resources gives users unprecedented access in locating very specific strings of text, in practice users are frustrated by limited and relatively simplistic search and retrieval strategies. In most electronic resources, users are limited to retrieving only what is explicit in the text, i.e. strings of text, some of which have been encoded logically. In the case of images, the situation is even more problematic. Unless a project has developed a header consisting of detailed metainformation, most images can only be retrieved by image title. Boolean and proximity searches go a very small way in solving the problem of retrieving more than single word searches, but do not provide the conceptually and theoretically rigorous searches most scholars in the humanities want and expect from electronic resources. Specifically, this paper will address the practical and theoretical issues raised by devising a classification or indexing scheme which facilitates search and retrieval by going beyond encoding what is explicit in the text. To this end, several points will be raised: although encoding what is implicit in the text facilitates retrieval of concepts not possible by explicit encoding, this process is much more subjective; how this subjectivity influences retrieval; the concept of granularity will be raised, and the problems of encoding to various levels; the problems of encoding implicit metainformation which is transparent to users.  While at past ALLC/ ACH conferences many papers have discussed the difficulties in consistent encoding of explicit text in large projects in which many people participate in the encoding process, the possibilities for inconsistent encoding of implicit text multiplies exponentially. Yet, I would argue, that without the development of classification or indexing schemes, digital archives remain hidden behind front ends which may look resplendent, but which barely reveal their complexity and richness. To this end, the rest of the paper will be divided into three parts. Part I will provide an overview of some of the major metainformation schemes which were developed in a pre-digital environment, such as AACR2, the Dewey Decimal Classification, and the Library of Congress Subject Headings. Topics to be covered will include: the theoretical impetus behind these schemes; how and why these schemes were conceived and made extensible; why these schemes cannot be transferred to a digital environment without adaptation.  The second part of the paper will explore current applications of some of these schemes to a digital environment, such as the Art and Architecture Thesaurus and the Thesaurus for Graphic Materials. Specifically, I will address how these schemes have been adapted from facilitating indexing codex-based texts to digital ones. In addition, the special case of indexing images will also be discussed. The third part of this paper will explore metainformation schemes devised for several specific digital archives, including The Blake Archive and The Thomas MacGreevy Archive, both published at the Institute for Advanced Technology in the Humanities at the University of Virginia. In the case of the Thomas MacGreevy Archive, I will demonstrate how we, working within the TEI, developed a metainformation scheme which facilitated very specific genre searching for both texts and images.  ",
       "article_title":"Metainformation Strategies for Electronic Resources",
       "authors":[
          {
             "given":"Susan",
             "family":"Schreibman",
             "affiliation":[
                {
                   "original_name":" University College Dublin, Eire  ",
                   "normalized_name":"RCSI & UCD Malaysia Campus",
                   "country":"Malaysia",
                   "identifiers":{
                      "ror":"https://ror.org/0474gs458",
                      "GRID":"grid.417196.c"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Text Encoding"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction In British newspapers, various 'vogue' prefix-like forms regularly appear (Renouf & Baayen, 1998; see also Baayen & Renouf, 1996, for standard affixation). This study considers three such forms: MOCK, COD, and FAUX. These forms are primarily used to modify nouns, as in  (1) he explodes in mock-outrage chain belts, studded with faux gems around the hips a quick snipe at the cod-mysticism   and to modify adjectives, as in  (2) a charmingly mock modest touch coming back on stage, faux sheepish a Russian night club - cod-glamorous   Occasionally, one finds instances of adverbial modification,  (3) Rabelais at his most mock heroically cloacal ordered him out of the house, faux-crossly   and for MOCK, examples also exist of verbal modification: (4) he mock bows, gallantly he has to mock-apologise for his tedious bleating   This study addresses the productivity of MOCK, COD, and FAUX by investigating their use in a British newspaper, The Independent, from a diachronic perspective. To this end, we extracted all occurrences of these forms from a corpus of this newspaper, compiled from 1988 onwards, and currently containing some 360 million word forms. For each of the years 1989-1998, we measured the type and token frequencies of these words in 4 successive 3-month chunks. For 1988, the last 3-month chunk was also taken into account. The questions to be addressed are whether changes in the frequency with which these vogue forms are used across time can be observed, and whether the forms with a hyphen (as in FAUX-SHABBY) reveal different patterns from the forms without a hyphen (as in FAUX SHEEPISH), which would suggest that syntactic context would co-determine the productivity of these combining forms.   Results Figure 1 will summarize the results obtained. The solid lines and the dots represent the numbers of tokens counted for the successive chunks. The dashed lines represent the numbers of new types observed across sampling time. Both line types were obtained using a non-parametric regression smoother (Cleveland, 1979). For reasons of space, we defer discussion of the type counts to the presentation at the conference. The left panels represent the forms without a hyphen, the right panels the forms with the hyphen. The top panels of Figure 1 show the results obtained for MOCK and MOCK-. The left panel reveals a slow but steady increase for the token counts (r = 0.388, t(39) = 2.6294, p = .0122). The right panel suggests that the use of MOCK- did not change in the last 10 years (r = 0.194, t(39) = 1.237, p = .2234). A closer investigation of the MOCK data revealed that the increase in the number of tokens is primarily carried by the lowest-frequency types - the highest-frequency types have a relatively stable use across the years.  The central panels of Figure 1 concern COD (left) and COD- (right). For both forms, we observe a reliable increase over time, which appears to be more linear for COD- (r = 0.656, t(39) = 5.437, p = .0000) than for COD (r = 0.473, t(39) = 3.357, p = .0018). For both COD and COD-, an autocorrelation analysis suggests a reliable correlation at short lags, suggesting that the extent to which a form is used in a given month is co-determined by the extent to which it was popular or unpopular in the preceding months. The bottom panels of Figure 1 suggest that FAUX and FAUX- were hardly used initially, but that in the second half of the sampling period they enjoyed greater productivity. Both non-parametric regression lines and parametric change point analysis suggest a breakpoint around the end of 1992, after which FAUX and FAUX- began to become more and more productive. What we probably are witnessing here is the birth of what may eventually become a fully-fledged new prefix of English. Intriguingly, all three left panels show a (local) minimum at this point in time (marked in the plots by a vertical dotted line). Possibly, the general fashion for vogue modification with any of the near synonyms MOCK, COD, and FAUX reached an all-time low around the end of 1992, from which both MOCK and COD recovered. Note that Figure 1 shows that both forms show an increase in use during the immediately following chunks. This general fashion for vogue modification may have led in its wake to the upsurge in productivity of FAUX and FAUX-.   Conclusions We have observed different patterns of diachronic change through an investigation of vogue modification in The Independent: a steady state (MOCK-), a linear increase over time (MOCK, COD-), a slightly oscillating pattern (COD), and a birth pattern (FAUX). The forms of MOCK- (with hyphen) show a clearly different pattern than the forms with MOCK (without hyphen), which shows that forms with and without hyphen should not be lumped together a priori. The syntactic contexts that favor hyphenation (e.g., modification of a prenominal noun or adjective) lead to slightly different diachronic patterns. Thanks to the accumulation of large diachronic computerized corpora, it is, at the end of the second millennium, finally becoming possible to directly monitor ongoing language change.   ",
       "article_title":"On Mock-scholarly, Faux-casual, and Cod Philosophy: Patterns of Diachronic Change in a British Newspaper",
       "authors":[
          {
             "given":"R.",
             "family":"Baayen",
             "affiliation":[
                {
                   "original_name":" University of Nijmegen, The Netherlands  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Antoinette",
             "family":"Renouf",
             "affiliation":[
                {
                   "original_name":" University of Liverpool, UK  ",
                   "normalized_name":"University of Liverpool",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04xs57h96",
                      "GRID":"grid.10025.36"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Stylistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The aim of this paper is to focus on a linguistic corpus in order to detect, by statistical methods and computational tools, either correspondences or contrasts between the hypotheses made by the linguist and the applied procedures made by the software experts. The corpus is selected from nineteenth century Victorian speeches; in particular, Parliamentary and political speeches delivered by Benjamin Disraeli, both as a member of Parliament and as Prime Minister. Analogous types of research have been recently carried out [see Labbè, Bolasco, Lebart, Salem, 1995] with reference to contemporary politicians such as de Gaulle, Mitterand and the Italian Berlusconi. In fact, computational research in political discourse is a highly significant new brand of criticism, which contributes to the modern notion of political contest by some more hidden truths that can be revealed. Nonetheless, we should say that, in most cases, where contemporary politicians are concerned, the use of \"ghost writers\" for writing public speeches definitely undermines that language structure/personality relationship which justifies work in textual analysis. Dealing with 19th century England, we have considered Victorian politics as being the first true \"arena\" for party politics and as constituting fundamental principles for political dialectics. The main source of reference was the collection of Selected Speeches by T. E. Kebbel in two volumes, borrowed from the Parliament Library in Rome. Disraeli's personal letters written to Lady Bradford and Lady Chesterfield also constitute a valuable source of reference, in that they unveil psychological traits and personal idiosyncrasies of the statesman. Moreover, in classical biographies we have found revealing hints of modernity and wisdom that we think useful to compare to our times. In a further perspective, we would aim at setting up a series of parameters which might characterise English political discourse, defining also its specific contexts by comparing corpora (for example Tory vs Whig discourse or Conservative vs Radical, etc.). Some intriguing questions have led us to pursue this goal, bearing in mind the old lessons of rhetoricians concerning oratory and trying to see them in the light of modern standard rules: how great is the analogy existing between oratorical qualities, original style and words, phrases and discourse markers more or less unconsciously uttered by an individual? By which standards can this analogy be assessed? And which is the \"benchmark\" for moral evaluative judgement, when we use a meta-linguistic code to filter the quintessence of discourse? Should, for instance, the most frequent items adhere in meaning and collocation to their speaker's actual socio-political presuppositions? And in which degree should their collocation be found in accordance with given parameters? Moreover, what can computational analysis discover beyond or in contrast to all previous evaluative criticism on the same subject to which so many scholars have contributed in the course of centuries? We have tried to answer these questions by examining, through data processing, which linguistic marks might define or confirm the features for linguistic excellence. Our stylometric study is concerned with the Victorian century, an age rich in syntactical perfection and lexical complexities. We have selected thirty-four speeches given by Disraeli between 1830 and 1870, and have processed them through the software programme \"Lexico 2\" and SPSS. The corpus has 205,800 occurrencesAccording to an empirical criterion, a corpus can be considered wide enough if it is greater than 200,000 words; moreover the ratio V/N (vocabulary or number of different words divided by total occurrences) is 9.8% (when this ratio is over 20%, the corpus is not to be considered wide enough. See Bolasco, Analisi Multidimensionale dei dati. Carocci Editore. Roma. 1999. p.203) , with a type/token ratio of 9.83% (known also as a measure of vocabulary richness) and a Guiraud's coefficient G=44.61. In table 1, for all the sub corpora in the decades, we show the the number of occurrences (N), forms (V) and hapax (V1), the word with maximum frequency for each decade. Hapax forms (words used only once) constitute nearly the 50% of the total amount of different words, which is a representative ratio marking a highly varied though integrated vocabulary. In table 2, we show a list of the fifty most important words considered as truly connotative of Disraeli's mind and world, displayed according to total decreasing frequency, making a distinction between the decades, the absolute frequency (F) and the normalized occurrences (x 1000 words). Moreover, we have disambiguated words (textual forms) in order to obtain the correct rank and we have made a distinction, for some forms, between pronouns, nouns, verbs, adjectives, adverbs and conjunctions. The choice was made to build up the so-called lexical universe of Disraeli's discourse: the chosen words (i.e. power, principle, democracy, oligarchy, empire, etc.) have first been seen as collocates and, for them, we have built their lexical universe, in order to define specific contexts and co-texts through the relationship of proximity and distance. We have patterned some meaningful words from a diachronic point of view, marking the evolution of his political thought, throughout his long parliamentary activity, detecting their characteristic trends and we have also carried out a factor analysis for some groups of words, clustered according to some semantic categories (i.e. self, key words, generic words, geography, negative words, etc.) What clearly appears by means of statistical exploitation is as follows: 1. High occurrence of a limited number of forms (mainly unspecific in meaning); 2. High occurrence of hapax forms and of various different forms, used with a very low frequency; 3. percentage of polysyllabic adjectives of Latin origin, used according to alliterative and symmetric structures, creating an appealing effect of harmony and balance. The same effect is obtained by collocating meaningful words used in pairs and the same occurs with words plus adjectives often used in pairs; 4. High percentage of adjectives in relation to the corpus as a whole and most of them belonging to the semantic category of \"greatness\" and of \"positive feeling\".  We think that Disraeli's discourse is powerful, confident, determined, endowed with energy and a brand of sincere English imperialism. He certainly cared for England and for himself and a true interest was also Ireland. What might appear strange is the zero occurrence of the word \"Judaism\", even considering the very low presence of all vocabulary concerned with race and religion. He supposedly divided with care his choice of language genres (novel, essays and speeches) according to some well defined goals to pursue. The high occurrence of generic words, i.e. with a neutral connotative value, seems to confirm some critical judgements expressed by his opponents for which Disraeli's political commitment was in most cases supported only by generic principles of well re-constructed political heritage and, only in a lesser degree he offered detailed policies on various occasions. A possible consequence that originates from this analysis and supports the original hypothesis made is that Disraeli's political success and his prolonged prominence as a first-rate politician (even when in opposition) was mainly due to his gifts of linguistic excellence and flamboyant oratory. From a different perspective, his Italian-Jewish ancestry adds an odd and mysterious flavour to his success, in the pure aristocratic context of Victorian England.  ",
       "article_title":"Primroses and Power: a Study on Linguistic Excellence in Political Discourse",
       "authors":[
          {
             "given":"Anna",
             "family":"Loiacono",
             "affiliation":[
                {
                   "original_name":" Università degli Studi di Bari, Italy  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Angela",
             "family":"D'Uggento",
             "affiliation":[
                {
                   "original_name":" Università degli Studi di Bari, Italy  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Barbara",
             "family":"Cafarelli",
             "affiliation":[
                {
                   "original_name":" Università degli Studi di Bari, Italy  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Rosaria",
             "family":"Romita",
             "affiliation":[
                {
                   "original_name":" Università degli Studi di Bari, Italy  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Stylistcs"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" This paper will analyse narrative style in selected fiction of Beckett (French and English versions). It will use computational stylistics for a formalist discrimination of patterns of language in the texts and will thus begin with a descriptive base. Previous published work by Burrows, Love, Craig, Holmes, Forsyth, Tweedie, Baayen, and Smith has shown the strength of computational procedures, particularly in such areas as the identification of authorship, genre, period, and character. This present work builds upon these foundations by examining issues in narrative theory and translation theory, with particular reference to Bakhtin's ideas on the way different discourses interact. Whilst Bakhtin recognised \"the positive influence of Formalism\" (1986: 169) he exposed its limitations, recognising the need to extend analysis to broader cultural issues. If, as Bakhtin argues, all utterance is ideologically governed and can never be neutral then the differentiations in language patterns revealed in our work cannot be construed as merely linguistic phenomena. That would be to rest with the formalist approach that Bakhtin wished to move beyond. The cultural questions arise as soon as one applies Bakhtinian concepts. Beckett is highly appropriate for such an investigation given the complexity, subtlety, and significance of his narrative experiments. When Beckett came to consider an English version of Molloy, which had been written in French in 1947 and published in 1951, he talked of producing a \"new\" text. To ask in what sense the text might be \"new\" is to open the large question of what can be and what cannot be achieved in translation - a question whose implications range from the immediate practical realities of searching for the nearest equivalent of a given word to the philosophical issues pertaining to language and how it means. At the philosophic level, translation raises ontological questions, the very questions raised by Molloy himself early in the text: \"But my ideas on this subject were always horribly confused, for my knowledge of men was scant and the meaning of being beyond me\" (52). The reference to a distinctive phrase in Heidegger's work alerts us to the link between the problem of translation and central ideas in Beckett's trilogy: commentators such as O'Hara argue, for example, that Beckett's work 'could almost be seen as a literary exploration of Heideggerian metaphysics', and that Beckett's fundamental inquiry in the trilogy centres around the question of how language means. This question then becomes refocussed through consideration of what can be achieved in translation. Heidegger's reservations about the way in which translation violates meaning in the source text appear eventually to be taken up by Beckett, who is reported as stating during a London rehearsal of Endgame: \"The more I go on the more I think things are untranslatable\" (Cockerham 1440. This issue of 'untranslatability', Steiner argues, 'is founded upon the conviction, formal and pragmatic, that there can be no true symmetry, no adequate mirroring, between two different semantic systems' (1975: 239). Expanding this argument concerning 'semantic dissonance' Steiner writes that 'Because all human speech consists of arbitrarily selected but intensely conventionalized signals, meaning can never be wholly separated from expressive form. Even the most purely ostensive, apparently neutral terms are embedded in linguistic particularity, in an intricate mould of cultural-historical habit. There are no surfaces of absolute transparency.' Patterns of prepositionality or conjunctivity, such as emerge in our analyses as a feature both of Beckett's English and French versions, impact upon our understanding of each text's meaning. How might these patterns influence our reading of Beckett? Does Beckett provide one work of literature called Molloy, or does that title mask two works of literature? How different is it really to read Beckett in French as opposed to reading him in English? These questions have not, I believe, been addressed in quite the way that this project proposes. The closest work is that of Opas (1995), who has studied translations of Beckett's How It Is and All Strange Away into Finnish, German, and Swedish. Using the University of Toronto's TACT program as the basis for her computational analysis, and applying the postulate of van Leuven-Zwart (1989,1990) that 'if there are enough consistent changes between a text and its translation on the microstructural level, it will affect the macrostructure of the text also', she has provided evidence of the ways in which common words influence syntactic structures and of how translations of them can influence the meanings we read in a text. The research data used in this paper derives from analyses of word frequencies, using established statistical techniques (e.g. principal component analysis, t-test, Mann-Whitney test). In order to produce this computational evidence texts are first prepared for the computer programs in accordance with protocols developed by Burrows. Frequency counts are established for each of the 99 most common words in the texts. These counts are standardized to allow for the variations in the total size of each section of text and each count is correlated with every other count so as to produce a matrix (using the Pearson product-moment method of correlation). We also use a technique of multivariate statistics known as principal component analysis and plot the results so as to show the relationships between the variables in the data. The plots show which words behave most like each other and which sections most resemble each other in their word-frequency patterns. The significance of this evidence is further tested by using distribution tests such as the t-test and Mann-Whitney test, which assess whether the variations in the data occur at a level of probability that statisticians would deem likely to be an effect of chance or a significant outcome. These procedures enable identification of the words that discriminate significantly - in computational terms - between narrative styles. The discriminating words will also be examined through \"scatter plots\" which generate the scatter of values for each word in each section of the text. This procedure will reveal how sporadically or consistently each word discriminates in a particular comparison. Although this research therefore begins with computational evidence, it will move from the quantifiable data to consider the literary significance of a word's use in context. As McCarty (1996) writes, \"no tool is 'just a tool' but is an agent of perception and means of thinking\". Common words are significant because they point to the larger linguistic structures in which they participate. With Beckett's work, investigations of stylistic differentiation show how translated texts maintain in the second language similar kinds of discriminations as those operating in the first language. The present work on a range of Beckett's early, middle, and late fiction extends previously published work by McKenna, Burrows, and Antonia on Molloy (1999) and on the trilogy (1999 forthcoming). ",
       "article_title":" The Statistical Analysis of Style: How Language Means in Beckett",
       "authors":[
          {
             "given":"C.",
             "family":"McKenna",
             "affiliation":[
                {
                   "original_name":" The University of Newcastle, Australia  ",
                   "normalized_name":"University of Newcastle Australia",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00eae9z71",
                      "GRID":"grid.266842.c"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Stylistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The discovery of the writing and stylus tablets from Vindolanda, a Roman Fort built in the late 80s AD near Hadrian's Wall at modern day Chesterholm, has provided a unique resource regarding the Roman occupation of northern Britain and the use and development of Latin around the turn of the first century AD. However, although papyrologists have been able to transcribe and translate most of the writing tablets, the meaning of the majority of the stylus tablets remains obscure. These small wooden plaques with a hollow recess which was once filled with wax and written on by means of a metal stylus are well preserved; however, the wax has deteriorated leaving a wooden surface with small strokes where the stylus pen has occasionally breached the wax covering. Making sense of such minute indentations by eye alone has proved impossible for papyrologists, and the problem is compounded by the fact that many tablets were reused again and again leaving the remains of many texts on the damaged surfaces. An EPSRC funded project at the Department of Engineering Science, University of Oxford, was initiated two years ago to analyse these tablets and develop new image processing techniques to retrieve information from small incisions in damaged surfaces (the techniques developed being applicable to a wide variety of engineering problems). Some headway has been made using wavelet filtering to remove woodgrain in images of the stylus tablets, and developing and appropriating Phase Congruency and Shadow Stereo techniques to identify candidate writing strokes. However, until some interface is put into place between these engineering techniques and the papyrologist, the stylus tablets' texts will not become decipherable because of the complex nature of the papyrology process and the difficulties encountered in expecting a non-mathematical user to utilise such algorithms. This paper gives a brief background to the project, before discussing the steps taken and problems encountered whilst developing the computer application and user interface for this system. Focussing on the interaction between historians, papyrologists and the application developer, and the design of the graphical user interface, the paper discusses the benefits and problems surrounding such a multi-disciplinary project.  ",
       "article_title":"Border Crossing: Engineers, Papyrologists, and the Graphical Use Interface",
       "authors":[
          {
             "given":"Melissa",
             "family":"Terras",
             "affiliation":[
                {
                   "original_name":" Oxford University, UK  ",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Digital Resources"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  One of the challenges when designing image databases in the humanities is to provide interfaces that are drawn from the object being represented rather than from the technology used. In this paper we will discuss an image database of over 400 images that was built by a team of graduate students and faculty at McMaster University. The images relate to carving technique on Trajan's Column, one of the most extensive and important surviving sculptural monuments of ancient Rome. In addition to various text-based search tools, we also created a cartoon sketch interface to the entire frieze of Trajan's Column that allows users to move around a virtual column to access the images and to see them in narrative context. This virtual column is an example of a content driven interface to an image collection that is based on the subject matter of the collection. Trajan, Roman emperor from AD 98 to 117, has generally been remembered as a 'good' emperor. A gifted general, he was close to his soldiers and led them on a number of campaigns which substantially expanded the Roman empire. Two of these campaigns, the first and second Dacian wars, are commemorated on Trajan's column: a 100 foot tall marble column decorated with a continuous, upwardly spiraling, sculpted frieze. The scenes on this frieze are an invaluable source of information about many aspects of Roman military practice, imperial ideology, and also sculptural technique. The column still stands today in its original place in the Forum of Trajan in Rome, but, due to its great height, visibility of individual scenes from the ground is quite limited; students and scholars must often rely on old photographs and plaster casts for the study of many details. In 1997 a team of graduate students and faculty at McMaster University were given access to a unique collection of over 400 slides taken by Peter Rockwell and Claudio Martini when Trajan's Column was being restored and had scaffolding around it. The images are primarily of details that show carving technique. In addition we were given access to a series of cartoon sketches of the entire column that show the narrative of the frieze. The challenge was to create an image database that would provide a context for these photographs so that scholars and students of Roman art could use these resources easily. The WWW site is located at <>. In this paper we will do the following: 1. Discuss the background of the project.This project was initiated by Peter Rockwell who was looking for a venue for making his collection of slides available to the research and education community. While he had published on the column, the full collection of images on which his research was based could not be published economically. A WWW site seemed the most effective way to make this collection available. The team was limited by the fact that the slide collection was in Rome, Italy and could not be removed to Canada for digitization. In this paper we will discuss the steps taken to digitize the images on site and build the database from information gathered by Peter Rockwell as part of his research. 2. Discuss the technical design of the WWW site.After initially experimenting with a design that used hundreds of static WWW pages we developed a dynamic database that could generate the pages for the individual images on the fly. This gave us the freedom to easily alter the interface of the image pages and make site-wide changes in real time. This in turn allowed us to refine the design of the site in an interactive fashion as suggestions came in and users reported back. Finally the design of the database allowed editors involved in the project to make corrections over secure connections. In the paper we will comment on the virtues of moving to dynamically generated information for projects that involve teams of people, continuous correction and iterative interface design. We will contrast the development of this site with two previous projects, the Emily Pauline Johnson Archive () and the Cradle of Collective Bargaining site (). Both of these other sites use static WWW pages to hold the images. We will also comment on the choice of image resolutions that we settled on. In the Trajan site we make available small thumbnail images within WWW pages for timely access and links to medium and high resolution copies of each image and cartoon. This allows the user to look at and download images suitable for teaching and research. 3. Discuss the interface problems we faced and the solutions we settled on.The Trajan Project provides three ways for users to navigate the images. The first is to search the image database. The second is by using indexes generated from the database which give the user an overview of the keywords used. The third and most visual is to navigate the column using the cartoon sketches and click on the links to individual images. In effect we have two means of access that are driven by the technology and one that was driven by the content of the database. In the paper we will discuss at length the need for content-driven interfaces that provide access to users to images in ways that correspond to the original artifact. Content-driven interfaces cannot be standardized because they vary according to the artifact(s) represented, but they provide a way of organizing information that the user familiar with the area can use. In our case we used the column as a whole as a visual index to the collection. The visual index not only provides access but creates a context for the images which would be hard to abstract from the database. This visual context augments the interpretative essays that introduce the site. 4. Conclude by discussing uses and benefits of the site.As part of this project we have been showing the site to interested academics to get their feedback on the interface and to get ideas about how it might be used. As many of the images were taken to document carving techniques used in the sculpting of the frieze, this site is of greatest use to those interested in Roman carving. Many of the images, however, contain enough detail to be useful to those interested in Roman sculpture in general or those interested in studying Roman military equipment and activities. The WWW site provides an economical alternative to print for research image collections to be made available. In addition, the cartoons provide a continuous sketch of the narrative of the column that can be downloaded and annotated by those interested in this depiction of the events around the Dacian wars.   Viewers invited to try the site have commented on how they could use the high-resolution images and cartoons for teaching purposes since they are freely available and easily accessed. These types of comments have led us to think about the image database as not only a coherent research and teaching collection, but as an accessible collection of free images that can be mined by those who are shifting to electronic teaching tactics. Whenever you make a digitized collection of images available you are also providing the equivalent to academic clip art which can be reused in ways that are only remotely connected to the original content. This unintended outcome from the digitization of images will be discussed in the conclusion of the paper.   ",
       "article_title":"Trajan's Column: Building a WWW Image-Database",
       "authors":[
          {
             "given":"Geoffrey",
             "family":"Rockwell",
             "affiliation":[
                {
                   "original_name":" McMaster University, Canada  ",
                   "normalized_name":"McMaster University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02fa3aq29",
                      "GRID":"grid.25073.33"
                   }
                }
             ]
          },
          {
             "given":"Gretchen",
             "family":"Umholtz",
             "affiliation":[
                {
                   "original_name":" McMaster University, Canada  ",
                   "normalized_name":"McMaster University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02fa3aq29",
                      "GRID":"grid.25073.33"
                   }
                }
             ]
          },
          {
             "given":"Michele",
             "family":"George",
             "affiliation":[
                {
                   "original_name":" McMaster University, Canada  ",
                   "normalized_name":"McMaster University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02fa3aq29",
                      "GRID":"grid.25073.33"
                   }
                }
             ]
          },
          {
             "given":"Martin",
             "family":"Beckmann",
             "affiliation":[
                {
                   "original_name":" McMaster University, Canada  ",
                   "normalized_name":"McMaster University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02fa3aq29",
                      "GRID":"grid.25073.33"
                   }
                }
             ]
          },
          {
             "given":"Paul",
             "family":"Barrette",
             "affiliation":[
                {
                   "original_name":" McMaster University, Canada  ",
                   "normalized_name":"McMaster University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02fa3aq29",
                      "GRID":"grid.25073.33"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Digital Resources"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper will describe the design and development of a web-based distance education course in Art History at the University of Glasgow. In planning such courses the interplay between new media, distance education and the existing systems in a campus-based institution raises numerous issues which need to be resolved before such a course is feasible. Not least of these are the obstacles still inherent in accessing and delivering online images: in terms of locating the desired resources, copyright and costing implications, institutional agreements with other bodies such as the CLA and its digitising licence and dual-mode delivery both on and off-campus. We suggest that, two years on from Dan Greenstein's call for \"new and creative ways to manage relations between those who 'own' rights in image resources and those who have an interest in acquiring access to them\" (1) Greenstein, Daniel (1997). Digital Images and Virtual Scholarly Collections. Abstract, Joint International Conference of the Association for Computers and the Humanities and the Association for Literary & Linguistic Computing, Queen's University, Kingston, Ontario, CANADA, June 3 - 7, 1997 <> no simple methodology, clear guidelines for use or satisfactory search interface yet exists. In common with other distance education developments in recent years, the work has been carried out in close partnership between the academic department concerned and GUIDE (Glasgow University Initiative in Distance Education). Following successful small-scale, campus-based attempts to implement new educational methods in teaching Art History, and in response to encouragement within the department, the Level 2 class convenor approached GUIDE for advice on potential distance options. Accordingly, in keeping with institutional policies and national pressures to widen participation (in particular flexible, distance and part-time provision), a 'flagship' course is to be offered at level 2 of a undergraduate degree course from January 2001. The course will consist of two 30 credit modules (a total of 600 notional learning hours). One module will be on Scottish Art and the other on Art, Culture and the Avant-Garde. On the understanding that development work is never right first time, the initial module will be run once, closely evaluated, analysed and amended where appropriate. If this proves to meet our goals in the second presentation it will serve as a template, or model, for the design of the second module. A record of the history of the production of the course is being kept, to show how and why decisions to use particular methods of delivery and/or technology are made. It is intended that such a record will contribute to decision-making in the design of similar online distance education courses. This development work constitutes a new departure, raising research issues in several areas. One aspect of this is in the innovative nature of these plans, and their place within the existing setting. For GUIDE, and indeed the University as a whole, this will be one of the first ventures into part-time distance provision at undergraduate level. If the course proves a success in pedagogical terms and there is sufficient demand, we envisage the development of future courses, with the long-term aim of offering a distance education route through access to postgraduate provision. Achieving this aim requires a long-term commitment to developing other part-time distance courses that articulate with these modules in a cross-disciplinary degree structure within the Humanities. Ultimately, it requires the existence of sufficient comparable courses to enable a broad range of studies on the basis of part-time distance learning, in any subject, and at any level across the institution. As a flagship undergraduate distance course, it must also meet University requirements to an exemplary degree, and specifically those Quality Assurance guidelines being developed by GUIDE for distance education. The template we produce must therefore be of the highest possible quality, whilst remaining reasonably adaptable to future requirements. The development experience must inform and enable GUIDE to better assist future projects employing digital resources. GUIDE has been working with a lecturer in the History of Art department to develop the first module on which the template will be based. In the first year, the module will run parallel to the face-to-face one, although it will cover different themes and use very different methods. It is not simply a distance version of the existing campus-based module. The process of designing a new distance learning course is very different from that involved in attempts to translate existing teaching materials into distance learning format. Existing Level-2 provision is heavily reliant on lectures. It is partly in order to investigate more exploratory, active learning approaches that this course has been conceived. It will be largely, though not entirely, web-based, and make use of group and peer-learning methods. We see the medium of online distance learning as enabling the transformation of the relationship between student, lecturer and the sources of learning materials - a transformation which, according to constructivist advocates of networked learning, will be necessary for the success of these methods. In the development of this course, therefore, we have sought to identify the main features of this transformation with particular reference to art history. We have designed the course with a view to minimising the negative effects of losing face-to-face contact and maximising the benefits of technology-assisted distance education. We do this with the understanding that this constitutes not merely a compromise, but a fundamental change in what it means to teach, learn and communicate. The fit with existing provision is only one of the issues raised. Another results from these different aims for the distance modules. The teaching of the discipline of art history and indeed the discipline itself - would have been unthinkable without the photographic transparency. Although it is exponentially more portable than a work of art, the transparency is also exponentially less portable than its digital equivalent. The new methods and web-based delivery will only be adding value to lecture-based provision if the technology is harnessed to provide easy, 'on-demand' access to source materials, especially digital resources. Ironically, the emergence of the technology for high quality digitisation of images promises a utopia of accessibility whilst simultaneously aggravating the problem of copyright. It has sometimes seemed as though the complexity of copyright law has resulted in a situation where the ability to access and use images is more restrictive now than before digitisation. Other complex issues in the course design include the need to retain and extend successful features of the on-campus provision in Art History without competing too heavily for recruitment from the full-time student population (especially in the early stages when the options for continuation by these methods will be severely limited), and generic issues in online teaching and learning, especially those affecting developments in the Humanities. The effects of these considerations will be demonstrated in the design and evaluation of the course which will, by July 2000, be preparing for delivery. Some suggestions which might facilitate the use of visual resources in teaching, drawn from our experience, will be offered to digital image service providers. Specific complications we encountered will be explored and those solutions we have found described. We hope these will help to ease the path for future users of these exciting resources in Humanities teaching.  ",
       "article_title":"Digital Images for Online Distance Education in the Humanities",
       "authors":[
          {
             "given":"Sue",
             "family":"Tickner",
             "affiliation":[
                {
                   "original_name":" University of Glasgow, UK  ",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          },
          {
             "given":"Richard",
             "family":"Hooker",
             "affiliation":[
                {
                   "original_name":" University of Glasgow, UK  ",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          },
          {
             "given":"Francis",
             "family":"Halsall",
             "affiliation":[
                {
                   "original_name":" University of Glasgow, UK  ",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Digital Resources"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" Online Public Access Catalogs (OPACs) in libraries now include bibliographic records for WWW sites. In most or all cases, these records contain direct links to the web resources themselves, so that someone using a WWW-based catalog could go directly from a bibliographic description of a website to the website itself, just by clicking on the URL in the catalog record. OPACs also have records for individual items within digital libraries, so that readers will be aware of the existence of a particular text or digital object within larger collections. However, once someone leaves the OPAC and enters a digital library on the WWW, the advantages of careful bibliographic control may be lost. Most digital libraries are based on SGML-encoded files, whether full-text TEI-encoded files, or Encoded Archival Description (EAD) formatted finding aids (or some other markup language) while library catalogs use records in the MAchine Readable Cataloging (MARC) format. It will become increasingly important to find ways to get these SGML-based digital libraries to interact with MARC-based library catalogs. In cataloging an item, catalogers spend time determining bibliographic information and forms of names and titles as a way of making sure the item is exactly described. Also, subject headings for each item are determined. By not providing links to online catalog records or including full information within the digital library searching and browsing mechanisms, readers may be misdirected or misinformed. The issue is more complex than reproducing or reformatting a MARC record within the TEI or EAD Header. There are at least three reasons why digital libraries should be linked dynamically to online library catalogs: 1. The difficulties presented by names 2. Accepted forms of names and subject headings change 3. Digital libraries may combine both cataloged and uncataloged materials  Catalogers take great care in creating and maintaining Name Authority Files in an attempt to keep straight all of the various authors that might share the same or similar names, in addition to pseudonyms, variant spellings, and married or maiden names. Names are a source of contention among scholars, almost as much as texts themselves - the name by which any given author is known may change, and consistent rules for establishing standard names are difficult to establish. Just to give a few well-known examples, Charlotte Bronte published under the pseudonym \"Currer Bell,\" yet is known by her real name; Marian Evans published under the pseudonym \"George Eliot,\" and is known by her pseudonym. Following U.S. Library of Congress rules, works by Mark Twain were formerly filed under \"Samuel Clemens.\" This changed a few years ago, and now are under \"Mark Twain.\" There are countless such cases, much more vexed and complicated than these examples. The intent of a Name Authority File is to group together an author's works, no matter under which name it was published, so that they can be found by searching under any variant name or pseudonym. Of course, Name Authority Files are neither comprehensive nor perfect, but in developing digital libraries, it seems counterproductive to try to duplicate the effort already expended in creating a Name Authority File. Catalogers also expend a great deal of effort in maintaining the information contained within library catalogs, and online catalogs are dynamic databases under constant revision. Accepted forms of subject headings and authors' names change over time, and libraries routinely perform global changes within library catalogs. If subject headings are included within the header of an electronic text, it is doubtful that the header will be updated should the accepted form be changed in the OPAC. Over time, the information in digital libraries will grow out of synch with the OPAC. Catalogers also expend a great deal of effort in maintaining the information contained within library catalogs, and online catalogs are dynamic databases under constant revision. Accepted forms of subject headings and authors' names change over time, and libraries routinely perform global changes within library catalogs. If subject headings are included within the header of an electronic text, it is doubtful that the header will be updated should the accepted form be changed in the OPAC. Over time, the information in digital libraries will grow out of synch with the OPAC. Not everything in a library collection is cataloged, and this is especially true for manuscript and archival collections. Catalogers and archivists have rules for what gets cataloged and what does not. Letters, photographs, and other items within manuscript collections generally are not cataloged separately with MARC records. Instead, catalogers create collection-level MARC records for the online catalog, and archivists then create finding aids that describe the contents of manuscript collections in more detail. Some archival collections combine cataloged materials, such as books, recordings, or films, with uncataloged items, such as photographs, sheet music, or letters. Digital libraries created from such collections will need to consider the various sources of bibliographic description available, which may include both MARC records and finding aids. As digital libraries become larger and more complex, it will become essential that they draw from and interact with online library catalogs. Digital libraries will not want to duplicate the bibliographic descriptions and subject headings available from online catalogs. I will look at how two projects at Indiana University have begun to address this problem by integrating information from the online library catalog with digital library collections, and some of the problems and pitfalls encountered. The Hoagy Carmichael Collection >>, which has digitized most of the Carmichael collection available at Indiana University, combines three sources of information: an EAD finding aid for the music, lyrics, photographs, correspondence and other materials; MARC records for the sound recordings, extracted from the library catalog and converted to a MARC SGML format developed by the U.S. Library of Congress; and finally, the TEI-encoded full-text correspondence. At present, we are extracting MARC records and converting them to SGML using batch processes, but are working on ways that this interaction can occur in real time. I will focus here on the use of the MARC records as part of the overall metadata for the project, and the process of conversion to SGML. Second, the Victorian Women Writers Project (VWWP) <> has begun a project to use the Name Authority File (NAF) records from the online catalog to keep track of authors and their variant names. The VWWP currently has works by only 42 authors, but even this small sample presents some complex issues surrounding authors' names. I will demonstrate the process by which Name Authority File records are integrated with the VWWP collection, allowing for more complete information on authors' names. ",
       "article_title":"Textbases and Databases: Integrating Library Catalogs with Digital Libraries",
       "authors":[
          {
             "given":"Perry",
             "family":"Willet",
             "affiliation":[
                {
                   "original_name":" Indiana University, USA  ",
                   "normalized_name":"Indiana University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01kg8sb98",
                      "GRID":"grid.257410.5"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational / Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":" One of the crowning achievements of the 18th century Enlightenment was the Encyclopédie ou Dictionnaire raisonné des sciences, des arts et des métiers, par une Société de Gens de lettres, edited by Diderot and d'Alembert. Published in Paris between 1751 and 1772, in 17 volumes of text and 11 volumes of plates, it contains 74,000 articles written by more than 140 contributors. The Encyclopédie was a massive reference work for the arts and sciences, as well as a machine de guerre which served to propagate Enlightened ideas. The impact of the Encyclopédie was enormous. Through its attempt to classify learning and to open all domains of human activity to its readers, the Encyclopédie gave expression to many of the most important intellectual and social developments of its time. The scale and ambition of the Encyclopédie inspired the editors to adopt three modes of organization: dictionary, encyclopedic, and the renvois. The interaction of these three modes has led modern scholars to describe the Encyclopédie as the \"ancestor of hypertext\" and depict Diderot as \"l'internaute d'hier\"1. Brian, Eric (1998). \"L'ancêtre de l'hypertexte\". Les Cahiers de Science et Vie, 47 (oct. 1998), pp.28-38. . Diderot makes the importance of the organization of knowledge explicit in the Discours Preliminaire: As an Encyclopedia, it is to set forth the order and connection of the parts of human knowledge. As a Reasoned Dictionary of the Sciences, Arts, and Trades, it is to contain the general principles that form the basis of each science and each art ... and the most essential facts that make up the body and substance of each.  2. English translation cited in Nelly Hoyt and Thomas Cassier, \"Introduction\" to Encyclopedia (1965), p. xxiii (our emphasis).     Of the three modes of organization, the dictionary mode (organization of entries in alphabetical order) is certainly the simplest. The second mode of organization is encyclopedic, where each entry is assigned a \"class of knowledge\", placing it within the \"order\" of human knowledge, as was described by d'Alembert's Systême Figuré des connoissances humaines. Reflecting Enlightenment theories of epistemology, the tree of knowledge has a three-fold trunk comprised of memory, reason, and imagination, from which spring many branches, twigs, and leaves (Image of Pank's tree). Simply placing an entry into this hierarchy of knowledge was insufficient to indicate the inter-connections of knowledge. Thus, Diderot created an extensive system of renvois (cross-references), the third mode of organization, providing a lattice of inter-connections between individual leaves of the tree as well as between classes of knowledge. The reader is encouraged to follow the renvois, which unite fields of knowledge into what was hoped to be a seamless totality. The ARTFL Project's creation of a computer implementation of the Enlightenment's \"war machine\"3. Andreev, Leonid et al (1999). \"Re-engineering a War Machine: ARTFL's Encyclopédie\". Literary and Linguistic Computing Vol 14, No. 1 1999, pp. 11-28. has attempted to provide this three-fold access and navigation to scholars and researchers. Furthermore, the electronic Encyclopédie provides a unique tool to study the interactions of these three modes of organization of knowledge. Systematic examination of Diderot's hypertext is important not only for the light it sheds on the function and construction of the Encyclopédie itself and the contours of 18th century knowledge, but may also be informative for modern conceptions of hypertext. The two first modes of organization (dictionary order and encyclopedic classification of knowledge) can be envisioned directly by the reader of the Encyclopédie. The third, the structure of the renvois, is impossible to grasp as a whole because of its more diffuse nature. The 61,700 individual renvois are unevenly distributed across the 74,000 articles of the Encyclopédie, reflecting the wide variation in size and scope of articles. 22,955 articles or subarticles have one or more renvois. As might be expected, articles with renvois are significantly longer (568 words) than those without (158 words). The renvois simply refer to one or more articles by head word. Thus, these connections are what might be termed leaf to leaf (or node to node), which allow the user to traverse leaves, without regard to the classes of knowledge. As a reader, one can only see the links stemming locally from a particular entry: much as someone hiking in a chain of mountains, it is not possible to step back and get a general vision of the whole landscape. The online version of the Encyclopédie allows us to draw a very coarse first map of the links \"landscape\". Our methodology was to group the entries into the classes of knowledge to which they belong and to measure the strength of relationships of all of the renvois between these classes of knowledge using a simple Z-score calculation. The resulting graph shows the statistically significant (or \"privileged\") links between classes of knowledge. A given renvois does not specify a precise sense of the word we should look for, and for any given word one often finds several distinct entries corresponding to different classes of knowledge (polysemy). There is no automatic way to determine the \"true\" link initially intended by the author - assuming that the author had a single sense in mind. However, a statistical study taking into account several independant renvois between classes allows us to get a more reliable picture and to disregard insignificant links. The encyclopedic landscape shows some striking features. First, there is a high local connectivity between close classes - in other words, there are several different ways to go from \"physique\" to \"géometrie\", for example. Secondly we can see a clear division of the links landscape into two \"hemispheres\", namely, knowledge related to direct observation of nature (histoire naturelle), heuristic sciences (chimie, médecine), rural life (économie rustique) on the one hand; and knowledge related to or contructed by the human mind on the other (abstract sciences such as mathematics; philosophy; history; laws of human society). Not surprisingly, the \"human hemisphere\" is much more important than the \"natural hemisphere\" in terms of sheer size. The general structure of the renvois can be related to the encyclopedic order pictured in d'Alembert's Systême Figuré. For example, mathematics, geometry, and arithmetic are closely grouped in our landcape drawn from the renvois and in the encyclopedic order defined by d'Alembert. The two hemispheres detected in this preliminary examination, however, do not clearly correspond to d'Alembert's general schema. The power of the Encyclopédie's \"hypertextuality\" arises from the combination of a general map of the structures of knowledge and the lattice of related cross-references. Rather than conceive of the renvois as simply node to node links, Diderot's hypertext assumes an abstract representation of knowledge. He writes, in the Discours Préliminaire, On a tâché que l'exactitude & la fréquence des renvois ne laissât là - dessus rien à desirer; car les renvois dans ce Dictionnaire ont cela de particulier, qu'ils servent principalement à indiquer la liaison des matieres; au lieu que dans les autres ouvrages de cette espece, ils ne sont destinés qu'à expliquer un article par un autre. [...] Ainsi trois choses forment l'ordre encyclopédique; le nom de la Science à laquelle l'article appartient; le rang de cette Science dans l'Arbre; la liaison de l'article avec d'autres dans la même Science ou dans une Science différente; liaison indiquée par les renvois...  4. Encyclopédie, Vol 1, p. xviii.    The key role that abstract representation of knowledge plays in the construction of Diderot's hypertext may be particularly instructive for current hypertext designers. In addition to node to node links, hypertext design may be enhanced by working within the context of organizational framework such as Diderot's representation of classes of knowledge. ",
       "article_title":"Mapping 18th Century Structures of Knowledge: the Renvois System in Diderot's Encyclopédie",
       "authors":[
          {
             "given":"Gilles",
             "family":"Blanchard",
             "affiliation":[
                {
                   "original_name":" Ecole Normale Supérieure, France  ",
                   "normalized_name":"École Normale Supérieure",
                   "country":"France",
                   "identifiers":{
                      "ror":"https://ror.org/05a0dhs15",
                      "GRID":"grid.5607.4"
                   }
                }
             ]
          },
          {
             "given":"Mark",
             "family":"Olsen",
             "affiliation":[
                {
                   "original_name":" University of Chicago, USA  ",
                   "normalized_name":"University of Chicago",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/024mw5h28",
                      "GRID":"grid.170205.1"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational / Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction Most of the problems in information retrieval systems occur from three sources: impreciseness of query and document representations, the changing state of mind in document relevant judgement, and the discrepancy of retrieval technique to match query and information need. The traditional approach to information retrieval systems, such as Boolean based retrieval technique, cannot solve these problems (Belkin and Croft, 1987). According to McCain (1989) and Pao and Worthen (1989), retrieval by keywords and cited documents will end up with different sets of documents retrieved. There were relevant documents retrieved by keywords but not retrieved by references, and vice versa. Hence, a new approach to retrieval technique is needed. This study is intended to build a new approach to information retrieval systems by employing the inherent structure of a document collection in an effort to learn more about document components that might improve information retrieval performance. The document components examined are the pattern of keywords, citing documents, and cited documents. Three independent variables were studied: co-keyword, co-citing document, and co-cited document (Hasibuan, 1995). These three variables constitute the multi-dimensions concept-based information retrieval system. By providing such variables as entry points to search relevant documents, it is widening the naturalness of the system in order to accommodate users' information needs.   Methodology A test collection was constructed from a collection of research articles published by National Atomic Research Agency (BATAN), covering the period 1985 through 1998. An automatic program was written to build indexes of keywords, citing documents, and cited documents for each document. The relationship that may occur between two documents can be depicted as in Figure 1. Pair-wise document similarity is calculated on those three variables. As in Figure 1, the similarities between documents A and B can be viewed in terms of documents Q and S, which cite documents A and B (co-cited), and documents Y and Z, which are cited by A and B (co-citing). In addition to that, the similarity of documents A and B can be counted on the number of shared terms (co-keyword).  Figure 1. Relationship of Two Documents (A and B)  Document similarity is measured by using the simple matching coefficient proposed by Van Rijsbergen (1979) and Salton (1989). The similarity of document A and B is calculated as follows:Similarity (A, B) = | X Ç Y |  The variables X and Y represent the sets of index terms occurring in two documents. Hence, the similarity of document A and B is measured in terms of the number of shared index terms. These sets of index terms can be expanded to include values for citing documents and cited documents. The retrieval technique used is based on this document similarity.   Results The preliminary results of the research showed that relevant documents retrieved by one component did not always agree with other components (see Figure 1). Figure 1 shows document 0376 and document 0419 have 11 shared index terms, two shared cited documents and three shared citing documents. As we expected, most of the document pairs have zero frequency of shared cited document and shared citing document. This finding is in line with our previous research finding in Hasibuan (1995). According to these results, it is suggested to build a multi-dimension concept-based retrieval system. The system built is able to provide users with a facility to search several interrelated options of search strategy. Given that kind of facility, a user can be more flexible to start his/her search by using one of the dimensions, says keyword, then navigates the system using other dimensions of document similarity. At any moment, the documents retrieved can be viewed, evaluated and judged for their relevance.  Figure 1. A Portion of Search Results of the Multi-dimension Concept-based Information Retrieval System  The search results shown in Figure 1 are posted in a hypertext based, so that a user can continue browsing uninterruptedly, in order to further his/her search to retrieve more on the other possible relevant documents. For each pair of documents retrieved, the system will provide the frequency of its co-keyword, co-cited documents and co-citing documents. Furthermore, the non-zero frequency of each entry in the columns of co-citing and co-cited documents becomes an active icon. If we click 2 in column co-cited, then we can see the documents that are co-cited by document 0376 and 0419 (see Figure 2). There are documents 0523 and 0531. The abstract of each document retrieved can be viewed by clicking the number of the document (see Figure 3).  Figure 2. An Example of Co-cited Documents    Figure 3. An Example of Abstract Document Retrieved    Conclusion Multi-dimensions concept-based retrieval system provides relaxed facility in order to search information by utilizing the components of documents - co-keyword, co-cited documents, and co-citing documents. With this facility, the system can widely accommodate the range of user's search strategy in information seeking. The drawback of the system compared to the traditional system is that this new approach needs more space of computer storage in order to accommodate all its index files. Furthermore, it can slow down the search process. However, this trade-off is compensated for by the flexibility of the system to provide more search strategies, more comprehensive retrieval of relevant documents, and easy to browse from one document to another document. Ultimately, by utilizing these three components of a document, the system can reduce the possibility of low retrieval system performance due to the impreciseness of query and document representations, lack of relevant judgement, and lack of matching function between query and document.   ",
       "article_title":"Multi Dimensions Concept-Based Information Retrieval System",
       "authors":[
          {
             "given":"Zainal",
             "family":"Hasibaun",
             "affiliation":[
                {
                   "original_name":" University of Indonesia, Indonesia  ",
                   "normalized_name":"University of Indonesia",
                   "country":"Indonesia",
                   "identifiers":{
                      "ror":"https://ror.org/0116zj450",
                      "GRID":"grid.9581.5"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational / Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction Since 1997, the Ruskin Programme at Lancaster University have been exploring various technical options in order to create a facsimile-based edition of John Ruskin's Modern Painters vol I. Editorially, we were above all interested in showing the history of the writing process and the context of the many revisions Ruskin made to this work over a twenty year period. We wanted to allow the user to choose, and toggle between, all the important editions of this work in facsimile, at the same time as bringing the editorial commentary to his or her attention. We wanted to do this in a way that did not impinge on the historical veracity of these facsimiles; we wished to find an unobtrusive way of displaying hyperlinks that did not result in a visually-compromised facsimile-based edition. At the same time the feature of central importance for any potential technical solution for our project was that it should allow us to automate the creation of this environment. There are around 500 pages in this work and as we are using five different editions (plus some manuscripts) there are over 2,500 images of pages, all of which will need to display editorial information to the user. After looking into commercially-available software options, we decided that the best solution was to build our own tools. With the help of computer scientists, we have now designed a suite of tools that allow us to analyse images of text in a variety of ways. We are now in a position to automate the production of our edition. This paper details what we required of these tools, looks at how we went about creating them, demonstrates their use in a high-volume processing environment and finally suggests further potential developments that could come out of this research, such as the possibility of enhancing searches of digitised resources that use facsimile images, and the possibility of automating mark-up of spatial textual features, such as paragraphs, indented verse, footnotes etc..   The Interface In order to preserve the authenticity of the facsimile texts, the interface that suited our needs was one that did not impose hypertext links directly onto these images, but rather placed the hyperlinks beside the text, on the right-hand side of the screen, parallel to the pertinent word or phrase. These editorial links are not displayed as text; rather, in the interests of concision, an icon indicates which type of hypertext note (e.g. whether it concerns People, Places or Works) is available to the user, with a 'mouse-over' facility to display the title of the note should there be any uncertainty about the destination of these links (e.g. if there were more than one of a given category of note in a given line).   Creation of Resources To adapt the facsimiles of the various editions of the printed book into this format, we needed three types of information: first, electronic versions of the texts containing mark-up indicating page and line breaks; second, the desired location and type of each note (which edition, page and line it belongs to) and which kind of note it is (e.g. People, Places or Works); finally we need to know the pixel co-ordinates of each individual line of text in each image (for 2,500 pages, that is around 100,000 sets of co-ordinates).   The electronic text The electronic text is being produced by hand-checking OCR-generated versions of specific editions. A manually-generated collation of all editions is also being produced. This has been entered into a database, and this information is automatically compared with the various OCR-generated electronic versions. Mismatches between the various versions/collations and lists of note titles are then investigated and resolved manually.   Note identification The identification of the location and type of hypertext links is achieved through codes associated with each note that contributors to the edition integrate into their submissions. These codes were established before any notes were written, but as the sequence of the notes is of importance within the automation process, we built into the code enough flexibility to allow us to add additional notes later while still maintaining the relationship between the linear succession of notes and the alphanumeric sequence of the codes.   Locating the pixel references within the images We found Java the most suitable language for analysing images. First we wrote a program to straighten and crop the scans automatically. Once correctly aligned, the program then analyses the average colour values of horizontal lines of pixels to distinguish one line of text from the white space that follows it. The co-ordinates of each line of text are identified in this way and recorded in a database. Other procedures extend this paradigm.   Constructing the interface The program loads in a page of facsimile, and ascertains whether there are any hyperlinking icons required for that page. By combining the information from the note identification codes and the electronic texts, the program can paste the appropriate icon(s) next to the appropriate lines of text and give them the desired functionality. A similar process allows context-specific information about collation to be pasted onto the right-hand side of the facsimile.   Conclusions  These tools are fairly flexible, and could be adapted to most printed material where appropriate (several projects have expressed an interest in utilising some of the tools we are developing). As this kind of procedure becomes increasingly straightforward, it can be envisaged that the pixel co-ordinates of any line (or word, or even letter) of a text could become a generally accepted aspect of mark-up for the digital scholarly edition (using the <MILESTONE> tag, for instance, and/or extended pointers). Certainly through automating and formalising the linking of the word to its original physical image, it will become possible to historicise electronic texts in an immediate and unambiguous way. Through integrating this analysis with mark up, we can facilitate a host of bibliographic analyses of texts that place 'traditional' modalities of scholarly work right at the centre of humanities computing.   ",
       "article_title":"Image Analysis as a Tool for Automating the Integration of Editorial Hypertext with Facsimile-based Digital Editions: the Experience of the Electronic Edition of Ruskin's Modern Painters I",
       "authors":[
          {
             "given":"Lawrence",
             "family":"Woof",
             "affiliation":[
                {
                   "original_name":" Lancaster University, UK  ",
                   "normalized_name":"Lancaster University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04f2nsd36",
                      "GRID":"grid.9835.7"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Digital Resources"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This presentation will start with a look at some of the problems encountered so far in a number of projects that tried to apply TEI [TEIP3] markup to premodern Chinese Buddhist texts. I have been working with the TEI Guidelines for more than seven years and published the first text, rather heavily marked up in TEI fashion, in 19951. The Chan-Buddhist genealogical history Wudeng Huiyuan (first printed in 1253) on the ZenBase1 CD-ROM, see [App et al 95]. . Since then I became involved with some other projects digitizing Chinese Buddhist texts, most prominently the work by the Chinese Buddhist Electronic Texts Association (CBETA)2. The CBETA project website (mostly in Chinese) is at <>. We now have about 200 MB of texts basically marked up3. This basic markup follows the general ideas lined out in [Wit96]. according to the Guidelines. All of these projects worked from printed editions published 80-100 years ago. One of the most obvious problems we encountered is the large amount of non-standard characters found in these texts, but TEI and SGML in general is quite able to handle this elegantly - nevertheless there are some important details that should be noted4. I will not go into detail for this audience, but some references to these problems can be found in the work by the Chinese Characters Analysis Group. More recently, we based our efforts on the work done by the Mojikyo Font Institute in Japan <>.. Some of the more subtle problems involve structural elements specific to texts of the sphere of Chinese cultural influence. Examples of these elements include the notion of a scroll, that is carried over from the time when the documents were actually written on scrolls, but still mark divisions in the printed editions. Being based on the physical medium, they fall into a similar category as the LB, PB and MILESTONE elements in TEI, but they are usually associated with some other heading-like text, colophons and the like. While this could be taken care of with the FW in some way, we decided to come up with our own solution, which was to introduce a new element, JUAN, (Chinese for scroll) and encode the information therein. Other structural elements that presented difficulties include colophons or other backmatter-like text at the end of a scroll, but in the middle of a DIV element that continued on the next scroll and sound glosses in the text. A second part of this presentation will give an overview of the recent developments in the SMART (System for Markup and Retrieval of Texts) project5. The project website is at <>.. This project aims at providing a working environment for research and markup on East Asian texts by utilizing the TEI Guidelines (see also [SpMcQ91]) and other international, open standards. The environment tries to enable network based collaboration and layered, private markup added to a central repository of texts, but it is intended to make it possible to use it on stand-alone machines without a live connection to the Internet. So far, the basic framework has been outlined and some of the utilities built. Originally, the plan was to develop this into a collection of open modules, that can interact through an open protocol in the spirit of presentations at ACH/ALLC 1999 by Michael Sperberg-McQueen, Jon Bradley and others. However, since such a protocol specification is far from being finalized, I found that I would rather have a concrete implementation to play with and to iron out problems. I therefore recently decided to build the tools I would need on top of the Zope6. For more information on Zope see <>. Web-Application platform. This is an OpenSource&trade; project build mainly with Python, implementing an object-oriented database and a complete framework for developing dynamic Web-Applications. It has a strong support for XML and related standards and thus seems especially suited for the purpose at hand. All the methods are exposed through a URL-based interfaced, but also callable through XML-RPC. The presentation in the context of the ALLC/ACH conference aims at contributing to a discussion of how such an open framework can be implemented, while at the same time showing some of the problems that arise when dealing with East Asian languages (see [ApWi96] and [CCAG80-85]). East Asian languages do not normally mark the word boundaries and even the definition of a word is highly disputed among linguists. In this situation, a list of all occurring words in the manner of a word-wheel cannot be applied. Additionally, the texts used here contain markup of textual variants, which complicates the creation of an index. Furthermore, different representations of the same character in machine-readable encodings have to be accounted for. An indexing method that takes these problems into account and also provides an abstraction from indexing of actual low-level locations in the text has been developed7. More information can be found in [Wit99]. The SMART project will be utilized in two different contexts: 1. As a retrieval and interface engine for the Buddhist text database produced by the Chinese Buddhist Electronic Text Association. SMART will allow for retrieval with enhanced queries, and add markup based on these queries, thus providing a powerful way to gradually enrich the markup. 2. As the central research platform for a research project of texts of the Chan school in Chinese Buddhism. A smaller corpus of texts is here used for building not only text with rich markup, but also supporting databases of proper names, sites and historical dates to allow for knowledge-base centered retrieval of the texts.  A demonstration of both applications will be given in this presentation.  ",
       "article_title":"SMART Project: Methods for Computer-based Research of Premodern Chinese Texts",
       "authors":[
          {
             "given":"Christian",
             "family":"Wittern",
             "affiliation":[
                {
                   "original_name":" Chung-Hwa Institute of Buddhist Studies, Taiwan  ",
                   "normalized_name":"Chung-Hwa Institute of Buddhist Studies",
                   "country":"Taiwan",
                   "identifiers":{
                      "ror":"https://ror.org/00sy58v94",
                      "GRID":"grid.501010.5"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Stylistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Background For some time, the Servei de Lexicometria at the University of Barcelona has been working in conjunction with the Latin Linguistics Group (Catalan acronym GLLUB) of the same university on the promotion and application of quantitative methods and computerized analysis of texts in the field of corpus languages, in this case Latin. Most of the work carried out to date has focused on questions of authorship attribution. Our main object of study is Sextus Iulius Frontinus, a writer of technical prose who was active during the first century AD. Parts of his work present problems of attribution; to add to the difficulty, there are few other candidates for the authorship of the doubtful text. Three texts by Frontinus have survived: De agrimensura (\"Agrimensura\", fragments on land survey and its legislation), Stratagemata (\"Stratagems\", a set of instructional anecdotes for Roman army officers, which illustrated the principles of the art of warfare via examples of strategems selected from Greek and Roman history) and De aquaeductu urbis Romae (\"On the aqueducts of the city of Rome\", a treatise on water supply for Rome). The problem of attribution arises with the fourth and last book of the Stratagemata. The hypotheses proposed by philologists for the date of book IV do not coincide: due to the lack of qualitatively distinctive linguistic features, the pseudo-Frontinus has been placed in the first century (thus a contemporary of the author himself), at the beginning of the second, and between the fourth and fifth. For this reason we decided to work on this text of doubtful authorship by applying quantitative statistical analysis methods with computerized support (Espinilla-Nofre: 1998). In that study, we used some of the quantitative methods that are generally accepted for questions of authorship attribution (Holmes: 1994): the ratio of simple forms/occurrences, the ratio of forms/occurrences with a fixed number of occurrences (fixed N), the ratio of simple occurrences/forms, the ratio of hapax legomena/forms, the R-HonorE function, the ratio of hapax dislegomena/forms and the study of the length of forms. These data allowed a comparison between the doubtful text and the rest of the works of Frontinus. The results in that first study highlighted two points: i. Between the doubtful text and the texts reliably attributed to Frontinus there is no inconsistency (this finding underlines the difficulties facing traditional hypotheses). ii. Another point of reference is required, i.e. another author, with whom to compare the data obtained.  So as the second stage of the project we have decided to approach the problem from another perspective. Following on from previous studies (Tweedie-Frischer: 1999; Frischer-Holmes-Tweedie, et al: 1999) and others, we are keen to analyze the order of the forms in the text in question and to compare them (1) with the texts recognized as Frontinian, and (2) with another text of a later date (control author, Tweedie: 1998). This analysis assumes that there was a change in word order in the Latin sentence between the classical era and the later period (Linde: 1923, Marouzeau: 1953). In spite of the fact that the use of the computer is a considerable aid in performing quantitative analysis of the texts, our study has faced two particular problems from the very beginning:  I. The first derives from the premise of an established word order in Latin. The generalized opinion is that word order is basically S(ubject)-O(bject)-V(erb). However, there are a number of deviations, and certain scholars have questioned the assumption of this standard word order in Latin prose (Pinkster: 1991):  A. Deviations according to sentence type: unlike assertive sentences, in imperatives the verb is usually placed at the beginning. B. Deviations according to type of clause (main or subordinate) and the use of different types of subordinate clauses. C. Deviations deriving from the internal structure of the constituents of the sentence: the general tendency in Latin is to place the syntactically relevant constituents (the heavy material) on the right, and the constituents of less syntactic importance (the light elements) as near the beginning as possible, even though this tendency may be altered for pragmatic and semantic reasons; questions of theme and rheme, or topic and focus. Nonetheless, in our study, we subscribe in principle to the premise that in the classical era the most common order followed by authors in Latin prose was SOV, and, in the later period, SVO.  II. The second intrinsic difficulty when working with corpus languages can be summarized as follows (Ramos: 1996):  A. Productivity: the corpus does not show which of the linguistic rules that can be extracted are the most productive. B. Grammaticality: to clarify the grammatical differences observed between authors it is obviously impossible to consult a native speaker. C. Representativity of the corpus: the corpus at our disposal is a set of materials that has been preserved due to a particular sequence of events. It is not specifically selected for study by linguists.     The Corpus Studied For our study, we compared book IV of the Stratagema of Frontinus with books I, II and III, and also with the work of a control author: De diversis fabricae architectonicae, by Caetius Faventinus, another writer of technical prose who lived in the later period.    Methodology   I. Technical data A. Computerization of the texts in the corpus (ASCII format) B. The computer program used to analyze the corpus was TACT (Textual Analysis Computing Tools), version 2.1. gamma. C. The corpus was coded with COCOA labels, following the marking guidelines of the MAKEBASE module in TACT. D. The data were obtained using the USEBASE module in TACT.  II. Methods of analysis A. We examine whether the verb is in final position in the various texts in our corpus. B. We study the position of the direct object in relation to the verb that governs it. C. We establish the type of clause (main or subordinate) in which the verb is found. D. We establish differences between the position of the verb according to the type of subordinate clause. E. We do not restrict ourselves to cases of direct objects represented by nouns or pronouns in the accusative, but also study cases of governed complement (in genitive, dative or ablative) and those in which the direct object is represented by a subordinate clause.      Working Hypothesis And Results Obtained The aim of our study is to provide arguments to corroborate or reject our working hypothesis: following the traditional assumption of Latin word order, the text of the Stratagemata recognized as authentically Frontinian (books I, II and III) must follow word order SOV, while the work of the control author will predominantly follow word order SVO. According to the word order we find in the doubtful book IV we will be able to place it in one or other era. We will thus have a set of data which, though unable to date the writing exactly, will lend support to one of the traditional philological hypotheses.   ",
       "article_title":"Word Order in Latin Prose Applied to a Case of Authorship Attribution: Book IV of the Stratagemata by Sextus Iulius Frontinus (1st century AD). The Contribution of Quantitative Methods via Computerized Text Analysis.",
       "authors":[
          {
             "given":"Empar",
             "family":"Espinilla Buisan",
             "affiliation":[
                {
                   "original_name":" University of Barcelona, Spain  ",
                   "normalized_name":"University of Barcelona",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/021018s57",
                      "GRID":"grid.5841.8"
                   }
                }
             ]
          },
          {
             "given":"Montserrat",
             "family":"Nofre Maiz",
             "affiliation":[
                {
                   "original_name":" University of Barcelona, Spain  ",
                   "normalized_name":"University of Barcelona",
                   "country":"Spain",
                   "identifiers":{
                      "ror":"https://ror.org/021018s57",
                      "GRID":"grid.5841.8"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Stylistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction Attempts to assign authorship of texts have a long history. They have been applied to influential texts such as the Bible, the works of Shakespeare and the Federalist Papers. A wide variety of techniques from many disciplines have been considered, from multivariate statistical analysis to neural networks and machine learning. Many different facets of texts have been analysed, from sentence and word length to the most common or the rarest words, or linguistic features. Holmes (1998) provides a chronological review of methods used in the pursuit of the authorial \"fingerprint\". A key issue raised at the panel on non-traditional authorship attribution studies at the ACH-ALLC conference in Virginia, 1999, by Joe Rudman is whether authorial \"fingerprints\" do in fact exist. Is it truly the case that any two authors can always be distinguished on the basis of their style, so that stylometry can provide unique stylistic fingerprints for any author, given sufficient data? Despite the long history of authorship attribution, almost all stylometric studies have been carried out on the assumption that stylometric fingerprinting is possible. However, often control texts are inappropriately chosen or not available. In addition, the imposition of editorial or publisher's style can distort the original words of the author. To our knowledge, no one has yet carried out a strictly controlled experiment of authorship attribution, with texts of known authorship being analysed between and within genres as well as between and within authors. In this abstract we present such an experiment. The next section describes the design of the experiment. This is followed by a description of the analysis carried out, then by the results and our conclusions.   Experimental Design The experiment was carried out in Dutch. Eight students of Dutch literature at the University of Nijmegen participated in the study. All the students were native speakers of Dutch, four were in their first year of study, and four were in their fourth year. The students were asked to write texts of around 1000 words. Each student wrote in three genres: fiction, argument and description. Three texts were written in each genre, on the following topics. Fiction: a retelling of the fairy tale of Little Red Riding-Hood, a detective story about a murder in the university, and a romance of chivalry. Argument: defending a position about the television program 'Big Brother', the unification of Europe, and smoking. Descriptive: football, the upcoming new millennium, and a book-review of the book read most recently by the participant.  The order of writing the texts was randomised so that practice effects were reduced as much as possible. We thus have nine texts from each participant, making a total of seventy-two texts in the analysis. The main question is whether it will be possible to group texts by their authors using the state-of-the-art methods of stylometry. A positive answer would support the hypothesis that stylistic fingerprints exist, even for authors with a very similar background and training. A negative answer would argue against the hypothesis that each author has her/his unique stylistic fingerprint.   Analysis There are many methods proposed for the analysis of texts in the attempt to identify authorship. In this abstract we describe three, and a fourth will be described at the conference. The first is that proposed by Burrows in a series of papers, see e.g. Burrows (1992), and used by many practitioners. Here we consider the frequencies of the forty most common words in the text. Principal components analysis is used to identify the most important aspects of the data. The second method considered is that of letter frequency. Work by Ledger and Merriam indicates that the frequencies of letters used in texts may be indicators of authorship. We use the standardised frequencies of the 26 letters of the alphabet, with capital and lower-case letters being treated together. As above, the standardised frequencies are analysed using principal components analysis. Thirdly, we consider methods of vocabulary richness. Tweedie and Baayen (1998) show that Orlov's Z and Yule's K represent two separate families of measures, measuring richness and repeat rate respectively. Plots of Z and K can be examined for structure. Finally, we are planning to tag the text and to annotate the text for constituent structure. Baayen et al. (1996) show that increased accuracy in authorship attribution can be obtained by considering the syntactic, rather than lexical vocabulary. The results from this part of the analysis will be presented at the conference. The texts written in this analysis are available from the authors upon request and, once all annotation has been completed, will be made available on the Web as well.   Results Each student was asked to write around 1000 words in each text. In fact, the average text length is 908 words. The shortest text has 628 words and the longest 1342. The texts were processed using the UNIX utility awk and the R statistics package. We first consider all of the texts together. The Burrows analysis of the most common function words shows no authorial structure. Genre appears to be the most important factor, with fiction texts having negative scores on the first principal component, while argumentative and descriptive texts have positive scores on this axis. In addition, argumentative texts tend to have higher values on the second principal component than descriptive texts. It appears that fiction texts are more similar to other fiction texts than they are to other texts by the same author. Analysis of letter frequencies gives similar results, while the measures of vocabulary richness show some indication of structure with respect to the education level of the writer. Those in their first year of studies appear to have lower values of K, and hence a lower repeat-rate. In addition, higher values of Z are the province of first-year students also, indicating a greater richness of vocabulary. When all of these measures are incorporated into a single principal components analysis the genre structure becomes even clearer. Fiction texts are found to the lower left of a plot of the first and second principal component scores, while the other genres are found in the upper right of the graph. Given the structure evident in the principal components analysis, it seems sensible to split the texts by genre and consider each separately. In each case, within fiction, argumentative, and descriptive texts, again the education level is the only factor to be apparent.   Conclusions It is apparent from the results described above that in this study, differences in genre override differences in education level and authorship. The absence of any authorial structure in the analyses shows that it is not the case that each author necessarily has her/his own stylometric fingerprint. Texts can differ in style while originating from the same author (Baayen et al., 1996; Tweedie and Baayen, 1998), and texts can have very similar stylometric properties while being from different authors. Of course, it is possible that larger numbers of texts from our participants might have made it possible to discern authorial structure more clearly. Similarly, it may also be that more fine-grained methods than we have used will prove sensitive enough to consistently cluster texts by author even for the small number of texts in our study. We offer, therefore, our texts to the research community as a methodological challenge. Given what we have seen thus far, we believe our results must alert practitioners of authorship attribution to take extreme care when choosing control texts and drawing conclusions from their analyses.   ",
       "article_title":"Back to the Cave of Shadows: Stylistic Fingerprints in Authorship Attribution",
       "authors":[
          {
             "given":"R.",
             "family":"Baayen",
             "affiliation":[
                {
                   "original_name":" University of Nijmegen, The Netherlands  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Fiona",
             "family":"Tweedie",
             "affiliation":[
                {
                   "original_name":" University of Glasgow, UK  ",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          },
          {
             "given":"Anneke",
             "family":"Neijt",
             "affiliation":[
                {
                   "original_name":" University of Nijmegen, The Netherlands  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Hans",
             "family":"van Halteren",
             "affiliation":[
                {
                   "original_name":" University of Nijmegen, The Netherlands  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Loes",
             "family":"Krebbers",
             "affiliation":[
                {
                   "original_name":" Max Planck Institute for Psycholinguistics, The Netherlands  ",
                   "normalized_name":"Max Planck Institute for Psycholinguistics",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/00671me87",
                      "GRID":"grid.419550.c"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Stylistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   1. Introduction One of the aims of modern linguistics, particularly of the computational persuasion, is to infer from the ever-growing mass of actual data available, the implicit, virtual organization underlying the apparent disorder and diversity of surface phenomena. This ever-present crucial duality is also at work in computational linguistics where the chief question is how to reach, beyond the teeming, bristling surface of observed individual facts, for the latent abstract organisation, thus enabling the observer (i.e. the linguist) to gain access to knowledge that can be generalized.   2. Trees Tree-representation is a powerful means of evincing the inherent structure of mutually dependent data. Scholars in the main fields of taxonomy regularly and successfully avail themselves of tree-structures, e.g. genealogies, pedigrees and phylogenies. Chomsky's syntagmatic trees have grown under every clime but they are far from being the sole way of imaging linguistic dependence or independence of the represented objects by means of a hierarchic tree where clearly outlined categories are paired and embedded. Frequently enough, modern linguists tend to be interested more in the relative closeness of objects than in their belonging to this or that closed class. Additive, as opposed to hierarchic, trees do away with watertight partitions between objects and lay the stress on notions such as proximity and opposition. Figure 1 illustrates this new way of representing textual data. The linguistic units under scrutiny here are modals and auxiliaries in a body of contemporary English poetry. The information contained in figure 1 is rich and clear. The organisation of the whole structure rests on the notions of proximity and opposition. The present auxiliaries have and be are closely associated while their past form counterparts had, and was and were form another distinct pair in the opposite part of the tree. More generally, the top of the tree can be seen as gathering the past forms whereas the present tense forms congregate in the bottom part. Deleting one of the edges of the structure breaks the tree down into two connex components. The case of should and would is interesting in that the two modals occupy an intermediate position between past and present which reflects their specificities in the actual texts.   3. Going further with trees : The unrooted-tree representation (figure 5) makes conspicuous properties of coordinating conjunctions that were, of course, impossible to discern in the table of occurrences, let alone in the lines of the original text (Day Lewis, complete poetry). The figure opposes the very tightly-knit pair but-and to the rest of the data which, in turn, form three other groups of coordinators (respectively either-or-which, neither-nor, then-yet-than) that are similar in behaviour, although more independent of each other than the previous two (and-but). Figure 6 illustrates the behaviour of the same grammatical units in a body of contemporary English novels. Without going into unnecessary detail, it is clear that this tree imposes unity or, at any rate, very close proximity on elements that evinced more independence in the previous tree (figure 5), neither-nor and either-or now forming more conspicuous pairs, while than becomes more closely associated to the structure as it teams up with but.  3.1. Fusion of the previous two trees : The question of course arises of the possibility of representing the two distinct sets of original data in one tree-figure, considering that they correspond to the same grammatical units at work in two provinces of literature (poetry and the novel) but in the same language and in the same period of time. Since these two original sets of data are technically disparate, it is impossible to start from the initial numbers as such - for instance, by adding them up. The only procedure available is to attempt to achieve a consensus by fusion of the original two trees by means of a new algorithm which we have just devised. Figure 7 is the product of this fusion algorithm.  It is interesting to observe first of all that this representation does sum up the information of trees 5 and 6. Not only are the properties of each single separate tree preserved, which is indeed a prerequisite, but there also emerges a more legible picture of the actual syntagmatic roles and affinities of the function-words under scrutiny here. The correlative conjunctions either-or and neither-nor are more satisfactorily grouped together, then enters into a close set with but and and, while the proximity of than and then on the tree is evocative of their common etymology, although they do not form a set stricto sensu.   3.2 The fusion algorithm The fusion algorithm is derived from the topological properties of the tree. Consider two trees A and B . Let VA and VB be the matrices of the corresponding neighbourhoods. One notes as VAB the Cartesian product whose elements are (x,y) with x Î VA and y Î VB . We shall build on VAB a preorder induced by the preorders of the neighbourhood levels of VA and of VB and define a neighbourhood relation on VAB compatible with the topologies of A and B. The fusion of the two trees shall ensue. Preorder:Two elements (x,y) and (u,v) of VAB are:  - ordered by the relation < if and only if x+y < u+v - equivalent by the relation @ if and only if x+y = u+v   Neighbours:A set G of elements of X is made up of neighbours if and only if all the pairs of distinct elements of G are:  - minimal by the relation < in VAB and - equivalent by the relation @ in VAB .  Algorithm Calculate the matrices of the neighbourhoods of A and of B.  Build VAB.  (iter): Look for the minimal elements of VAB. Use VAB in order to determine the neighbours. - Each set of neighbours G is represented by a single of its elements z. For each set G, of k elements, remove from X the k-1 elements other than z. Delete in VAB the corresponding lines and columns. If the numbers of lines and columns are larger than 3, goto (iter) else end of the algorithm.     ",
       "article_title":"On Consensus between Tree-representations of Linguistic Data",
       "authors":[
          {
             "given":"Michel",
             "family":"Juillard",
             "affiliation":[
                {
                   "original_name":" Université Nice-Sophia Antipolis, France  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Xuan",
             "family":"Luong",
             "affiliation":[
                {
                   "original_name":" Université Nice-Sophia Antipolis, France  ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational / Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   New York University (NYU) has recently completed Nomlex, a dictionary containing detailed syntactic information about 1000 common English nominalizations. This dictionary, which has been developed for use in natural language processing, is freely available from NYU.   History: Previous dictionary work at NYU includes COMLEX Syntax [Macleod'97], a large syntactic dictionary with detailed information on the syntactic properties of English words, especially with regard to complement structure. This dictionary is available through the Linguistic Data Consortium (LDC) and is being presently used by several natural language processing (NLP) groups.  Nominalizations present a special problem, in that one wants not only to analyze their syntactic structure, but also to relate them to corresponding verbal structures. This is essential for natural language interpretation in applications such as question answering and information extraction. For example, the answer to a question, \"How badly was the city bombed?\" could be phrased \"The city was destroyed completely\" or \"The destruction of the city was complete.\" These answers though syntacticly diverse contain exactly the same information.   Dictionary structure: The challenge in designing the NOMLEX entries is to provide, in reasonably compact form, all the lexically-specific syntactic information required to relate nominal arguments to the corresponding verbal arguments. This task is complicated because of the wide range of nominal argument structures, which makes a direct enumeration of all possible correspondences hopelessly unwieldy. In particular, the core arguments and oblique arguments behave differently in this respect. The core arguments of the verb (subject, object, indirect object) may appear as possessive determiners (DET-POSS), noun noun modifiers (N-N-MOD) or in a prepositional phrase commonly preceded by \"of\" (PP-OF). Examples of this are: \"His death\" (where \"his\" represents the one who dies and therefore the subject of the verb \"die\"), \"the price adjustment\" (where \"price\" is the object of the verb \"adjust\"), and \"the discussion of the case\" (where \"case\" is the object of the verb \"discuss\" and the analysis would be \"X discuss the case\"). More complex verbal complementation such as sentential and verbal complements are found following the nominalization and often retain the same structure. For instance, the complement \"that he came\" is the same for the nominalization as for the verb, seen in the following examples. \"Someone report(ed) that he came.\" \"The report that he came.\" Some verbal complements must have an introductory preposition in order to appear as nominalization complements. For example, \"Someone questioned whether it was a wise plan.\" versus \"The question of whether it was a wise plan\". We encode all these possibilities in the lexical entries of the nominalizations. The NOMLEX entry for \"appointment\" is as follows. (NOM :ORTH \"appointment\"  :VERB \"appoint\"  :PLURAL \"appointments\"  :NOUN ((EXISTS))  :NOM-TYPE ((VERB-NOM))  :VERB-SUBJ ((N-N-MOD) (DET-POSS)) :SUBJ-ATTRIBUTE ((COMMUNICATOR))  :OBJ-ATTRIBUTE ((NHUMAN))  :VERB-SUBC ((NOM-NP:OBJECT ((DET-POSS) (N-N-MOD) (PP-OF)))  (NOM-NP-PP:OBJECT ((DET-POSS) (N-N-MOD) (PP-OF))  :PVAL (\"for\" \"to\"))  (NOM-NP-TO-INF-OC:OBJECT ((DET-POSS) (PP-OF)))  (NOM-NP-AS-NP:OBJECT ((DET-POSS) (PP-OF))))) This sample entry, in combination with a set of defaults, provides us with a lot of information, including the following: A homograph noun of the nominalization \"appointment\" exists (\"a dental appointment\" is not the act of \"appointment\")  The subject of the verb tends to be a human, company or other entity capable of communication. The object of the verb is a human. The nominalization is of type VERB-NOM, meaning that the nominalization does not play the grammatical role of any of the arguments of the verb. In contrast, \"fighter\" is a SUBJECT nominalization and \"interviewee\" is an OBJECT nominalization. The subject of the verb can occur in three positions: N-N-MOD - \"The IBM appointment of Mary Smith\" DET-POSS - \"IBM's appointment of Mary Smith\" PP-by - \"The appointment of Mary Smith by the president\" (PP-by is assumed as a subject marker by default.)  When the verb is followed by one noun phrase and no other complement phrases, that object may occur in the nominalized phrase (NOM-NP) in one of three positions: DET-POSS - \"Mary Smith's appointment\" N-N-MOD - \"The Mary Smith appointment\" < PP-OF - \"The appointment of Mary Smith by the president\"  Object plus prepositional phrase complements (\"for\" and \"to\") share the same object mappings as simple NP complements.  In addition, the prepositions in the verb complement are realized as the same preposition in the nominalization complement. Complements consisting of an NP object plus either an infinitival complement or an AS-NP complement allow only DET-POSS and PP-OF complements. In addition, the verbal complement phrases TO-INF-OC and AS-NP map to these same types of phrases in the nominalization (by default).  Due to space considerations, we must leave out a lot of detail regarding the interpretation of this dictionary entry. In particular, we have a system of rules and defaults which prevent spurious ambiguity and allow us to keep the lexical entries compact. The example cited above is that PP-by is so often a marker of the subject of the verb, that we treat this as a default, marking nominalizations that do not allow this mapping with NOT-PP-by. For more detail about NOMLEX, please see our web site: <>   Using NOMLEX: To demonstrate the utility of NOMLEX we constructed two NLP applications that depend on NOMLEX entries to analyze nominalization phrases. 1. A program that transforms a nominalization phrase into one or more sentences, corresponding to the possible senses of the nominalization phrase. For example, \"Rome's destruction of Carthage\" has exactly one sense, which our program would paraphrase as \"Rome destroyed Carthage\". The program takes a grammatical analysis (a parse) of a nominalization phrase as input and uses NOMLEX to create grammatical analyses of the corresponding sentences, copying noun phrases and complement phrases from the various nominal positions (N-N-MOD, DET-POSS, post-noun, etc.) into the appropriate sentential positions (SUBJECT, OBJECT, etc.). Actual sentences are then generated from these parses. The output of this program could be used as input to any NLP application designed to operate on full sentences. 2. A program which converts an information extraction pattern designed for sentences into a set of information extraction patterns designed for nominalization phrases ([Meyers'98]). For our purposes, an information extraction pattern is a pattern used to identify events in text and correctly mark the participants of these events. One such system extracts information about corporate hirings, firings, resignations, etc., including the identification of who left which company, who joined which company, the positions they left, the positions they attained, dates, etc. Our program converts a pattern that analyzes \"IBM appointed Alice Smith as Vice President\" into a pattern that analyzes: \"IBM's appointment of Alice Smith as President\", \"Alice Smith's appointment by IBM\", \"IBM's Alice Smith appointment\", \"IBM's appointee\", \"The appointee of IBM\", etc. Nomlex helps determine where information identified by a sentence pattern would be found in a nominalization phrase. This would enable an information extraction tool to easily extract information from nominalizations.    Process: The menu-based entry program used for COMLEX was adapted to enter NOMLEX. As for COMLEX, the entry program gave us access to a large corpus of text (including the Brown Corpus and large amounts of newspaper articles) and we had limited access to the British National Corpus (BNC) as well. Two half-time ELFs (Enterer of Lexical Features) worked for 5 months and then one half-time ELF completed the lexicon in two years.   Concluding Remarks: We have created a dictionary with the goal of solving a pervasive problem in NLP. Most grammatical analyses are designed to process sentences, but in order not to miss information these need to be applied to nominalization phrases, as well. NOMLEX provides a bridge in the form of: (1) a set of rules and defaults; and (2) a dictionary record of idiosyncratic mappings between nominalizations and their related verbs. Before the creation of NOMLEX, developers and researchers have had to adapt processes which handle sentences to enable them to handle nominalizations also. Due to the high overhead of this endeavor, many systems do not handle nominalizations at all or use simple, but imprecise heuristics. NOMLEX makes it less expensive to fully integrate nominalizations into an NLP system.   Future work: In order to make NOMLEX an even more useful resource, we plan to add support verbs to the entries. The use of support verbs changes the relationship of the nominal and verbal arguments. For example, in \"his visit to Mary\" the possessive determiner is the subject of \"visit\" (i.e. \"he visit Mary\"); in \"he made a visit to Mary\" the subject of the support verb \"make\" is now the subject of \"visit\" (i.e. \"he visited Mary.\"). The demonstration that this is idiosyncratic and a matter for lexical interpretation is the appearance of \"he had a visit from Mary\" where the object of the preposition \"from\" is the subject of \"visit\" and the subject of the support verb becomes the object of \"visit\" (i.e. \"Mary visited him\"). We have been exploring Mel'cuk's notation as a means of capturing these relationships for NOMLEX.   ",
       "article_title":"An Electronic Lexicon of Nominalizations: NOMLEX",
       "authors":[
          {
             "given":"Catherine",
             "family":"Macleod",
             "affiliation":[
                {
                   "original_name":" New York University, USA  ",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          },
          {
             "given":"Ralph",
             "family":"Grishman",
             "affiliation":[
                {
                   "original_name":" New York University, USA  ",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          },
          {
             "given":"Adam",
             "family":"Meyers",
             "affiliation":[
                {
                   "original_name":" New York University, USA  ",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       "keywords":[
          "Computational/Corpus Linguistics"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction This paper presents seventeen previously unknown articles that we believe to be by Stephen Crane, published in the 'New-York Tribune' between 1889 and 1892. The articles, printed without byline in what was at the time New York City's most prestigious newspaper, report on activities in a string of summer resort towns on New Jersey's northern shore. Scholars had previously identified fourteen shore reports as Crane's; these newly discovered articles more than double that corpus. The seventeen articles, witty and often hilarious, confirm how remarkably early Stephen Crane set his distinctive writing style and artistic agenda; more than a century after their publication in the 'Tribune' they remain delightful reading. Stephen Crane began his career as a professional writer in the summer of 1888, when he was sixteen 1. Wertheim, S. and Sorrentino, P., eds. (1988). The Correspondence of Stephen Crane. 2 Vols. Columbia UP, New York.. His assignment was to assist his brother J. Townley Crane, Jr., almost twenty years older than Stephen, who had established Crane's New Jersey Coast News Bureau in 1880 when he arranged to serve as correspondent for the Associated Press and the 'New-York Tribune'. For three-quarters of the year, Townley Crane's duties must have been light, as he ferreted out news in the sparsely populated shore towns of Monmouth County. However, during the summer months, the news bureau's duties exploded. New York City newspapers of the1880's devoted remarkable amounts of space to chronicling the summer vacations of the city's upper and upper-middle classes. Every Sunday edition of most New York newspapers and, during July and August, most daily editions as well, carried news articles from the summer resorts popular with the more affluent citizens of Gilded Age New York. The format of these articles was standardized: a lead proclaimed the resort's unique beauties and the unprecedented success of the current summer season, a few brief paragraphs recounted recent events, such as a fund-raising carnival or the opening of a new hotel, and the article concluded with a lengthy list of names of recent arrivals and where they were staying. Working within the boundaries of this restrictive format, Stephen Crane developed a highly original, distinctive style. His shore reports are as ruthlessly ironic as 'Maggie', the novel he was writing during the same period, but, instead of directing his irony towards the inhabitants of the Bowery, he aimed it at the hotel proprietors and summer visitors of the New Jersey shore.   Discovery And 'Traditional' Attribution During the 1940's and 1950's, scholars familiar with Crane's style and interests were able to identify several other unsigned articles in the 'Tribune' as his. By coincidence, all of these articles originated in three adjoining towns on the Jersey shore: Asbury Park, Ocean Grove and Avon-by-the-Sea. When Fredson Bowers began editing his massive volume of Crane's works 2. Bowers, F. ed. (1973). Tales, Sketches and Reports. Vol. 8 of The University of Virginia Edition of the Works of Stephen Crane. UP of Virginia, Charlottesville. he evidently decided to limit his search for additional unsigned 'Tribune' articles by Crane to reports with datelines from those three resorts. Combing the 'Tribune' during the summer months from 1888 to 1892, Bowers identified as Crane's three articles overlooked by previous scholars, bringing the total of New Jersey shore reports to fourteen. No one questioned Bowers' decision to focus on the three adjoining shore communities. However, during research on a book concerning Stephen Crane's journalism, we came across an item in the Schoberlin collection at the Syracuse University Library that threw into doubt Bowers' procedure. A one-page prospectus for Crane's New Jersey Coast News Bureau was found which provided evidence of an attempt by Townley Crane to expand his business. In particular, the body of the prospectus lists shore towns ranging from Atlantic Highlands in the north to Seaside Park in the south. With this evidence of the Crane news bureau's wide geographical range, we began to question why all of the shore articles attributed to Stephen originated from Asbury Park and the two towns just south of it. Would it not make sense for Townley to send his teenaged brother to cover news in the resorts a few miles distant from their home base of Asbury Park and save himself the trouble? We searched the 'New-York Tribune' for the summers of 1888 to 1892, when Stephen was fired, looking for articles with a dateline from the wider base of New Jersey shore towns named in Townley Crane's prospectus. The Crane brothers' writing styles are widely divergent. Reading Townley's articles (written before Stephen began his journalistic career), it is evident that his style is that of brief, invariably flattering prose, while Stephen delighted in gleeful irony. This search revealed seventeen articles datelined from the shore towns of Long Branch, Belmar, Spring Lake and Sea Girt that appear to have the style of Stephen Crane. Hotel proprietors, baggage handlers and \"summer maidens\" are all written about with disdain. The articles are so stylistically distinctive in their irony and verbal inventiveness that they clearly look to be from Stephen's hand rather than from Townley's.   'Non-Traditional' Attribution: Stylometry Stylometry provides an alternative and objective analysis. The stylometric task facing us was to examine the seventeen articles and attribute them to either Stephen or Townley Crane. Suitable control samples in more than one genre are required, so, within the genre of fiction, several textual samples of about 3,000 words were obtained from 'The Red Badge of Courage' and Joseph Conrad's 'The Nigger of the Narcissus', the latter being chosen because we know that Crane and Conrad read and admired each other's novels. For journalistic controls, we turned to Richard Harding Davis and Jacob Riis, who were, along with Crane, the most prominent American journalists of the 1890's. Examples of Stephen Crane's New Jersey shore reports, New York City journalism, and war correspondence were taken from the University of Virginia edition of Crane's work; samples of Townley Crane's journalism were taken from the 'New-York Tribune'. The seventeen anonymous articles were first merged, the resultant text then being split into two halves of approximately 1800 words each. The \"Burrows\" technique 3. Burrows, J. L. (1992). \"Not Unless You Ask Nicely: The Interpretive Nexus Between Analysis and Information\". Literary and Linguistic Computing, 7, pp. 91-109. , which works with large sets of frequently occurring function words, is a proven and powerful tool in authorship attribution. Essentially it picks the N most common words in the corpus under investigation and computes the occurrence rate of these N words in each text or text-unit. Multivariate statistical techniques are then applied to the resultant data to look for patterns. The first phase in the investigation was designed to establish the validity of the Burrows technique on the known textual samples detailed above. Using principal components analysis, the Crane and Conrad fiction samples are clearly distinguishable from each other. Turning to the three genres within Crane's journalism, the principal components analysis on the occurrence rates of non-contextual function words shows, quite remarkably, how their rates of usage differ between his New York City, shore and war journalism, yet remain internally consistent within these genres. A final analysis incorporating the samples of journalistic writing from Townley Crane, Richard Harding Davis and Jacob Riis provides further validation of the Burrows method with a clear distinction visible between the shore journalism of Townley Crane and Stephen Crane. Discarding the control samples, which have served their purpose, we then focus on the main task, namely the attribution of the seventeen anonymous articles to either Stephen or Townley. Both cluster analysis and principal components analysis provide mutually supportive results in attributing the anonymous articles to the youthful ironist Stephen Crane. The \"non-traditional\" analysis has supplied objective, stylometric evidence which supports the \"traditional\" scholarship on the problem of authorship. We believe that this joint interdisciplinary approach should be the way in which attributional research is conducted.   ",
       "article_title":"Stephen Crane and the 'New-York Tribune': a Case Study in Traditional and Non-traditional Authorship Attribution",
       "authors":[
          {
             "given":"David",
             "family":"Holmes",
             "affiliation":[
                {
                   "original_name":" The College of New Jersey, USA  ",
                   "normalized_name":"Princeton University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hx57361",
                      "GRID":"grid.16750.35"
                   }
                }
             ]
          },
          {
             "given":"Michael",
             "family":"Robertson",
             "affiliation":[
                {
                   "original_name":" The College of New Jersey, USA  ",
                   "normalized_name":"Princeton University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00hx57361",
                      "GRID":"grid.16750.35"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction This paper explicates the what, why, and how of a substantially completed but ongoing project to identify and categorize all style-markers in written English that are quantifiable (e.g. type/token ratios, word length distributions, word length correlations, hapax legomena). Section I treats the what - defining the project - and then addresses the why - the rational for the project. Section II outlines the how. Section III gives a status report of the project. Although the final mapping will have value in various disciplines (e.g. stylistics, corpus linguistics, computational linguistics, and computer science), the impetus for the project is from non-traditional authorship attribution studies. Non-traditional attribution practitioners define style in the seemingly narrow framework of only those stylistic traits that are quantifiable. The main hypothesis behind this project (and all non-traditional authorship attribution studies) is that every author has a verifiably unique style. If we look at style as an organism, style-markers are its genetic material - making this project analogous to the human genome project. This analogy is somewhat of a stretch because each style-marker is analyzed by an independent study, whereas all of the loci of an autoradiogram are obtained in one scientific analysis. The identification of a quantifiable style-marker does not necessarily mean that that particular style-marker should be included in an authorship study (e.g. the orthography might be dictated by an editor or typesetter).   Section I This section treats the what - defining the project - and then moves into the why of the project. A short overview of what is in this section follows: The style-marker mapping project is a study to identify every style-marker in written English that can be quantified. The project began in a preliminary fashion in 1983 when I started recording the various style-markers that were used in non-traditional authorship attribution studies so that I could use them in my studies of the canon of Daniel Defoe. The project continued in this vein (along with a few attempts on my part to come up with \"new\" style-markers) until five years ago when I realized the importance of identifying all of the quantifiable style-markers. There is no one style-marker or even a combination of several style-markers that have proven to be a definitive discriminator in all non-traditional authorship studies. What works in one case often does not work in others. Word length distributions and sentence length distributions are two examples of style-markers that seemingly work in some cases but not in others. The idea is to look at style as a combination of all of the quantifiable style-markers and then to do the analysis as if each style-marker were a locus in the autoradiogram (See RUDMAN for a more detailed explanation).  Another reason for using all of the quantifiable style-markers is to eliminate any charges of statistical cherry-picking.   Section II This section treats the how. References to all of the literature and techniques will appear in the final report. For example, HOLMES, DELCOURT, and ELLIOTT AND VALENZA are three of the references under non-traditional authorship studies.  1) Search the literature, e.g.: Stylistics Grammar Non-traditional Authorship Attribution Studies Linguistics Computational Corpus Text  Rhetoric Discourse Analysis  2) Query the practitioners in all of the above fields, e.g.: Professor Erwin R. Steinberg of Carnegie Mellon for stylistics, Professor Paul G. Hopper of Carnegie Mellon for Grammar - all of the active practitioners in the non-traditional authorship studies.  3) Establish a clearinghouse on a web page that allows anyone to query the up-to-date mapping and allows anyone to suggest \"new\" quantifiable style markers that would be added by the curator. This will lead to a continually updated list. Negotiations are under way to make this site an extension of the Carnegie Mellon University English Web Site. 4) Use various strategies to identify new style-markers. This is where the innovative work supplements the drudge work, e.g.: Neural networks Pattern searching programs Brainstorming sessions     Section III This section gives a status report of the project and reports a timeline for its \"completion.\" References will be given for all of the style-markers in the final report, e.g. MOSTELLER AND WALLACE is one of the function word references. Only a few representative examples of each section are listed for this abstract.  1) META-WORD Paragraph Sentence Phrases Clauses  2) WORD Part of speech Ratios Positions Function Words Most frequent words  3) SUB-WORD Syllables Letters Phonemes Morphemes  4) OTHER Punctuation Imagery Rhetorical Devices Zeugma Chiasmus      Conclusion The questions, \"Can this project ever be completed?\", and, \"Is the number of style-markers infinite?\", are addressed. The success of this project will not solve all of the problems of non-traditional authorship studies. This project does not address the problems that gender, genre, time constraints, or conscious vs. unconscious style bring to the table. Nor does it treat the problem of lemmatization.  Identifying the style-markers is only a small part of the overall problems with non-traditional authorship attribution studies. The statistics that should be used in any study for each of these style-markers and the statistics for combining all of the markers into a \"final\" answer is the subject of another ongoing project.   ",
       "article_title":"The Style-Marker Mapping Project: a Rationale and Progress Report",
       "authors":[
          {
             "given":"Joseph",
             "family":"Rudman",
             "affiliation":[
                {
                   "original_name":" Carnegie Melon University, USA  ",
                   "normalized_name":"Carnegie Mellon University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05x2bcf33",
                      "GRID":"grid.147455.6"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   1. Introduction  1.1. Text variety in mediæval Slavonic texts The mediæval Slavonic written tradition is characterised by a high level of variety on all linguistic levels. This is usually explained with the use of a vernacular language in the Mediaeval Slavonic texts. The spread of the texts to regions with substantial differences from the language in the region where the text originated reflected the transmission. In this setting, the study of text variety is an important source of information about the synchronic and diachronic development of the Slavonic languages. However, up till now the study of text variety was done by traditional research methods based extensively on human collection and processing of scattered data. Taking into account the substantial number of mediæval Slavonic manuscripts, one can imagine that the bulk of the texts can not be physically covered by traditional investigation.   1.2. Case Study The Psalter has always had a leading role among the religious texts since its first appearance in the Slavonic countries, representing a very important source in the process of the development of the Slavonic written tradition. Its importance is also linked to the didactic function it had in the Slavonic lands where a new alphabet was recently introduced. This led to the need to educate the scholars in the new literary environment. Due to its own rhythm, to its diffusion, the Psalter was then studied by heart by people in order to make acquaintance with the written culture. That is why the Psalter has always been one of the most widespread texts [Karaèorova 89]. Its tradition is considered constant by almost all the researches, besides the structural differences that allow us to speak about different typologies of Psalter (simple, with commentaries, etc.).  In the light of these considerations (wide dissemination and lower variety due to its religious use), we decided to investigate some textual peculiarities of the Psalter's transmission.   1.3. Experimental Setting We focussed our attention on six manuscripts, choosing the spatial and temporal settings of each manuscript as reference points. The linguistic peculiarities referring to an origination from a certain region are reported using the term 'revision'. We used the following manuscripts: Sinaitic Psalter (10th c., written in the Glagolitic alphabet; all other witnesses were written in Old Cyrillic) Bolonian Psalter (13th c., Bulgarian revision) Norovian Psalter (13th c., middle Bulgarian revision) Serbian Psalter (13th c., Serbian revision) Kievian Psalter (14th c., Russian revision) Gennadian Psalter (15th c., Russian revision)  We used as text excerpts 15 Psalms scattered through the Psalter (1, 39, 40, 41, 44, 45, 64, 73, 74, 75, 89, 91, 98, 102, 134; the exception is the Serbian Psalter where the 1st psalm was damaged).    2. Study of the lexical variety We started with a study of the lexical variety in order to follow the tradition of the Psalter diatopically and diachronically, to set out the lexical variants and to verify the unity of its tradition. We used for this study the DBT (Data Base Testuale), a program of textual analysis developed at the ICL-Pisa, which produces different types of information in real time. Among its functions, the DBT provides lists of alphabetical frequencies, decreasing frequency order, concordances, co-occurrences, indices locorum, lists of suffixes and endings and lemmatization procedure. Comparing our witnesses we found the following: We obtained 115 pairs of variant words. Of these 115 pairs, only 38 times does the word vary in only one manuscript (77 times the same word varies in more then one manuscript). The variants seem to be linked to some specific words and realia. The typology of the variant pairs shows that the most frequently appearing are those whose semantics reflect eyesight, desire, law violation (in particular of the divine one), forgiveness, fire (as a symbol of purification), rebirth, oil, and unction. We can think that the scribe inserted these variants since he felt them to be more correct, both dogmatically and linguistically and, in any case, more specific for the different ages and chessboard of the written tradition. We notice a tendency to use synonyms or words with a very small semantic difference from those of the Sinaitic Psalter.  We can consider the chosen texts divided in 4 blocks: 1) Sinaitic Psalter; 2) Bolonian and Serbian Psalters (the closest in time to the Sinaitic); 3) Norovian Psalter; 4) Kievian and Gennadian Psalters. In 115 variants as regards the Sinaitic Psalter, we noticed that: 78 belong to the Norovian Psalter (67%) 36 to the Serbian Psalter (31%) 29 to the Bolonian Psalter (25%) 57 to the Kievian Psalter (49%) 75 to the Gennadian Psalter (65%)  The Norovian Psalter is the text that more than the others is detached from the Sinaitic and, as Cesko says \"it's one of the first trials to normalize the literary language thanks to a correction based on the Greek model\" [Cesko o77]. On the other side, the Serbian and Norovian Psalters show a lower level of variants and they were written in a period very close to the creation of the Sinaitic Psalter. Concerning the two Russian Psalters, the Norovian and Gennadian Psalters have almost the same number of variants (78 and 75). Thus we can think that they belong to the same revision. But this is not true: of the 78 variants present in the Norovian, 54 are different from those of the Gennadian, that only 28 times offers variants different from those of the Kievian. We can verify in this way the big linguistic similarity between the two Russian texts and the atypicality of the Norovian Psalter in respect to this group of texts. In conclusion, we have the Norovian Psalter vs. all the others, that can be very well accepted if we consider the basic homogeneity of all the examined manuscripts and the declared adhesion of the Norovian to the Greek original.  This confirms the unity of the Psalter's tradition in the Slavonic countries.    3. Study of the Orthographic variety We studied also a lower textual level - the orthographical one - in order to investigate the correlation between revisions and quantitative data on orthographic features. The traditional approach to the orthographic variety study is a qualitative one. The decision about the time when a text was written and the localization of its creation is based on the occurrences of specific orthographic features. Such features reflect the use of the jers; nasal vowels and groups containing - r - and - l - [OBG, 1993]. We studied the relative frequencies of all letters from the Old Cyrillic alphabet and all strings belonging to such characteristic groups. We aimed to check whether the qualitative characteristics relevant to the text origin lead also to differences in the quantitative study of the texts. The data were processed in STATISTICA for Windows. This data organization allowed us to choose a subset of texts, and some of the letters or groups for analysis. From the view-point of the specialist in Slavonic studies, this would mean that we are able to study the usage of all or selected graphemes in a chosen group of texts. We applied cluster analysis in order to check whether texts belonging to different revisions are grouped correctly. The cross-study of the texts leads us to the following conclusions: For experiments with texts from one revision the smallest size of the text excerpt should be 5,000 letters For experiments with text from different revisions the size of the text excerpt leading to correct clustering is 2,000 letters The best results are obtained when we use in the study the relative frequencies of 5 experimentally approved vowels The results showed that the regional differences within texts, which form a common tradition in the mediaeval Slavonic literary history, reflect the orthographic quantitative characteristics of the texts    4. Conclusions Our study showed the importance of conducting experiments on different linguistic levels. The first approach was oriented towards a study of lexical variance aimed at investigating the distribution of synonyms in the different witnesses and at forming groups of similarity between the witnesses. This study confirmed the hypothesis about the unity of the Psalter's transmission amongst the mediæval Slavs. The second approach was oriented towards a study of quantitative data on the distribution of orthographic characteristics. The comparison of the results shows that the application of both approaches could help the specialists in mediæval Slavonic textual studies to acquire new data about the similarities and dissimilarities of their object of study. Moreover, the complexity of the studied field presupposes the use of methods applied to the different linguistic levels because they enlighten different aspects of the studied written tradition.   ",
       "article_title":" Comparative Study of the Lexical and Orthographic Variety in the Mediæval Slavonic Psalter",
       "authors":[
          {
             "given":"Milena",
             "family":"Dobreva",
             "affiliation":[
                {
                   "original_name":" Institute of Mathematics and Informatics, Bulgaria  ",
                   "normalized_name":"Institute of Mathematics",
                   "country":"Belarus",
                   "identifiers":{
                      "ror":"https://ror.org/00vsg3p53",
                      "GRID":"grid.425294.c"
                   }
                }
             ]
          },
          {
             "given":"Monia",
             "family":"Camuglia",
             "affiliation":[
                {
                   "original_name":" University of Pisa, Italy  ",
                   "normalized_name":"University of Pisa",
                   "country":"Italy",
                   "identifiers":{
                      "ror":"https://ror.org/03ad39j10",
                      "GRID":"grid.5395.a"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Glasgow ",
       "date":"2000",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Although texts exist in social contexts, models of hypertext authoring generally exclude this aspect of the structure and function of texts. Readers and writers require situational and cultural contexts in order to understand the meanings negotiated in texts. These contexts are generally ignored because of the lack of any language model underlying hypertext authoring or the models used do not reflect relationships between text and context. In this paper we describe how Systemic Functional Linguistics can be used to create Systemic Functional Hypertexts (SFHT), which includes many aspects omitted from conventional models: (i) text forming resources, (ii) intra- and intertextual relations occurring within their (iii) situational and cultural contexts. SFHTs are described with the help of the unified modeling language.   1 Introduction According to Systemic Functional Linguistics (SFL; Halliday 1978; Martin 1992) texts are produced/received as functional units in patterns of wordings and grammar dependent on social context. Within SFL, 'context' is theorized using a bistratal organization of genre and register: whereas registers manifest variety according to situational use, genres correspond to the staging of social processes as part of the context of culture. It is a fundamental insight of SFL that the constitutive dependence of text on context is bilateral. That is registers (genres) are not only instantiated, but also constituted by means of texts. This relation of mutual dependence can be observed on the basis of covariation of textual and contextual features (Lemke 1995). According to this view, changes in context are correlated with changes in texture (i.e. lexico-grammatical choices). Thus, covariation can be used as a source for predicting intertextual relations: the more similar two registers (genres), the stronger their dependence, the higher the probability that their textual realizations are intertextually related. From this perspective, hypertext authoring becomes crucial in the sense that it has to reflect the contextsensitivity of intertextual relations in order to offer their readers value-added information. The perspective of traditional two-level-hypertext systems (Mayfield 1997) does not suffice, since it only formally distinguishes representational formats for the derivation of links and their presentation without taking context layers into account. What is needed is a system which does not only allow to model thematic progression (Kuhlen 1991), and adaptivity to varying situational, user centered views (Kuhlen 1994), but also genres and registers and their covariation with texture. In the following, the integration of these perspectives (i.e. hypertextual superstructures and contextsensitivity) on the basis of SFL is described using the concept of systemic functional hypertext.   2 Systemic Functional Hypertexts Applying SFL to hypertext relates to the question of how to specify which links of which textual units on the background of which linguistic and social semiotic entities. In order to approximate an answer, the concept of Systemic Functional Hypertext (SFHT) is introduced as an n-level hypertext system which includes at least a genre layer (for modeling genres, their constituents and staging), a register layer (for modeling registers, their constituents and accessibility constraints), a texture layer (for modeling types of texture forming resources, their syntagmatic and paradigmatic dependencies and linguistic realizations patterns), and a text layer (for modeling intertextual relations of texts as links in hypertext). Whereas the first three layers describe different resources for text linkage, the last layer deals with the organization of concrete links.  2.1 Systemic Functional Links In SFHT the concept of a link is concretized in two respects: (i) links are introduced as a kind of sign. (ii) This allows to ask for the role of links in the superisation (i.e. constitution) of higher level signs and their communicative function. To be more concrete: whereas the expression plane of a systemic functional link (SFLink) is given by its surface structural manifestation by means of anchor, reference, and referent, its content plane is determined by the intertextual relations it expresses. Since these relations are seen to co-vary with relations of genres and registers contextualizing social (communicative) processes, the view of SFLinks as being signs allows to introduce the concept of function in hypertext. To take a step forward in specifying SFLinks, two dimensions are distinguished: 1. Foundation: does a link manifest linguistic cohesion or situational and generic coherence of textual reference and referent? 2. Structure formation: does a link associate pairs of texts or does it participate in chains of interlinked texts as manifestations of systemic functional progressions?  Based on these criteria, several types of links can be distinguished: At the lower end purely associative, context-insensitive links are distinguished-the typical case of links produced in the area of automatic hypertext authoring (Salton et al. 1993). The construction of lexically cohesive as well as path sensitive links has an intermediary status. More coherent links are produced if context layers are taken into account. Thus, on the upper end of the matrix situational as well as generic coherent, contextsensitive paths of interlinked texts which serve to manifest systemic functional progressions are distinguished. The systemic functional perspective chosen to reconstruct the concept of hypertext allows to reflect the specific complexity of signs. Instead of describing SFHT by means of contextfree rule systems, contextsensitivity is taken as the starting point. Following this premise, several moments of informational uncertainty can be distinguished as necessary conditions for the functional impact of SFHT: (i) Contextsensitivity: the same link may have multiple linguistic, situational and generic sources (polymorphism). (ii) Polyfunctionality: the same link may be used to link different contextual units. (iii) Vagueness: the source of a link may be vague, dissonant, fuzzy, etc. (iv) Ambiguity: the same reference can be linked with different referents. (v) Dynamics: links may disappear, emerge or modify as the text base grows. These moments of complexity correlate with the constitution of social semiotic entities on the background of emerging/disappearing usage regularities of signs. SFHTs serve to make explicit this co-variation of linguistic and social semiotic system.    3 Conceptual Modeling In order to make hypertext more sensitive to the situational and cultural context the operationalization of the concept of SFHT is needed. Beyond formalization it is conceptual modeling which precedes their algorithmization and implementation. This paper presents a conceptual model of SFHT using the unified modeling language (UML). Besides structural aspects including the enumeration and specification of constituents of the linguistic and social system, dynamic aspects of SFHT are modeled, too. Structural building blocks of SFHTs including genres, registers, texture forming resources, intratextual/intertextual relations, systemic functional progressions, links and paths are interrelated. Furthermore, co-variation and co-evolvement of text and context are described as procedural building blocks. The conceptual model is used as a starting point for the specification of data structures and algorithms for the computer-based implementation of SFHT systems.   4 Conclusion This paper applies SFL to the area of hypertext authoring based on the manifold covariation of social and linguistic system, which allows the same text to be embedded into different contexts and thus to have different functions. In order to reflect this moment of polymorphism and polyfunctionality, the concept of SFHT and their main building blocks are described. On this background, three principal conclusions can be drawn: 1. Because of the dependence of human information processing on varying cognitive, situational, and social contexts, a given text corpus does not have a static, predefined hypertext structure. Thus, text conversion demands a fundamental reconstruction of the concept of hypertext from the perspective of a sociosemiotic theory of discourse which views this dynamics as a precondition for (hyper-)texts to be functional. 2. This reconstruction is necessarily interdisciplinary in the sense that it includes insights from semiotics, linguistics and social as well as computer science. 3. The concept of SFHT, used to formally manifest this reconstruction, demands a stepwise operationalizing in order to prepare its implementation. For this sake, UML is used, since it allows to model structural and procedural aspects of SFHTs.   Thus, this paper contributes to the reconstruction of hypertext from a semiotic point of view, or more concretely: from the perspective of computational semiotics.   ",
       "article_title":"Systemic Functional Hypertexts",
       "authors":[
          {
             "given":"Alexander",
             "family":"Mehler",
             "affiliation":[
                {
                   "original_name":" University of Trier",
                   "normalized_name":"University of Trier",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02778hg05",
                      "GRID":"grid.12391.38"
                   }
                }
             ]
          },
          {
             "given":"Rodney",
             "family":"Clarke",
             "affiliation":[
                {
                   "original_name":" University of Wollongong",
                   "normalized_name":"University of Wollongong",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00jtmb277",
                      "GRID":"grid.1007.6"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Overview This paper will present an innovative treatment of the literary phenomenon of Roland, alleged historical personnage and an epic hero almost unique in his breadth of influence from the eighth century to today. The treatment, which attempts more successfully than a printed book to present all the contributor works as equal, is made possible for the first time by the electronic medium. The combining of works with such different cultural contexts is reconciled by highlighting similarities and explicitly juxtaposing the differences between them, and by thematically connecting the works using both hyperlinks and critical discussion. At the same time, the reader is given a chance to create his own mental picture of the composite Roland character by following carefully explicated links. I will argue that the hypertextual experience of Roland is closer to the way in which the character was experienced originally--both by storytellers and by audiences--an experience that reading a paperbound book cannot replicate. Among the issues discussed will be the relationship between oral tradition and hypertext, the issue of character identity across cultural boundaries, and the role of the reader/audience in the epic experience.   Abstract Vanhoutte 2000 speaks of \"the advent of the electronic paradigm to the field of scholarly editing and textual criticism,\" which facilitates the creation of \"new kinds of editions in which the record of textual variation becomes a central point of attention, both on the markup- and on the delivery- side.\" RolandHT is a work which, while it is not a critical edition of any one text, certainly speaks to text variation as its main focus. RolandHT began as a critical exposition and literary experiment. At its center is the protagonist of the 11th-century Song of Roland and of many other works in Europe's literary canons. Currently in its second year of development at Brown University, RolandHT uses hypertext theory and Jean Baudrillard's idea of fragmentary writing to weave Roland storylines from different literary traditions into a single multi-pathed narrative. A new, composite, often self-contradictory virtual character is thus created, drawing upon such contributor works as the French Song of Roland, the Italian Orlando Furioso, the Welsh Can Rolant and many others. In RolandHT, passages from a set of primary sources are combined into a heavily interlinked web. The hyperlinks, which lead directly from one quoted passage to another in the same work or in a different one, highlight both similarities and differences among the themes and imagery in the storylines. This is true both for works whose plots parallel (for example, in the German, Welsh and Norse retellings of the French Song of Roland) and for writings with differing plots (such as a rendition of the Song of Roland on one hand and the Italian Renaissance epic Roland in Love on the other). Some of these hyperlinks point to the next passage in the work through an intermediary paragraph which explicates the connection between the two passages. The project's aim is not a coherent linear narrative. It is, rather, an exploration of this character, so pervasive in Europe's writing between the Middle Ages and the present day, that reaches beyond the confines of any one work written about him. Roland evolved as an oral epic hero for three hundred years before the first extant written work about him was created. The electronic environment is an invaluable and unique tool for exploring the relationship between orality and literacy in the context of the evolution of this character. Before we had the technology we have now, this sort of study was performed in a one-sided fashion, by studying and trying to understand orality in its own context, as Walter Ong has done, and viewing literacy separately. Now an attempt can be made to reproduce the fragmentary nature of oral storytelling in a written context, and let the readers experience it in a fashion which will enable them to better understand how people learned about Roland in the first place, during his proliferation in the age of primarily oral storytelling. The project cannot possibly reproduce fully the literary aspects of Ong's primary oral culture. The Carolingians (inhabitants of France and Germany in the eighth and ninth centuries, when the Roland legend was conceived) were not a primary oral culture (McKitterick). Their storytelling tradition, however, was oral, as was the Roland legend, which first appears in writing only in the eleventh century. Instead, the new approach to reading epic poetry that RolandHT presents is arguably closer to the way in which epic poetry was perceived by its audience when it was presented orally--no story was seen as an isolated phenomenon, but rather as part of a cultural network. In the process of reading a work of print literature, even a well-annotated one, it is easy to forget the impossibility of its existence outside of its cultural context. This fact, while certainly true when cultural boundaries are being crossed, becomes even more relevant when there is a large difference between the reader's temporal context and the time period in which the work was written. In this case, especially if the work's original language is the same as that of the reader, one can be mislead into a false sense of understanding of the work, and fail to delve into it more deeply. This new electronic version of the Roland texts simply does not allow one to think of a work of literature as an isolated event. In the traditional study of literature one is compelled to study one text at a time in depth, subsequently branching out. Here we have the opportunity instead to piece knowledge together from many starting points and eventually to converge toward an understanding, instead of branching away from generalized, theoretical knowledge to specific points. The knowledge to be gained from this ongoing investigation will fill a void in Roland scholarship. This work provides its readers with a tool to help them visualize the complexity of the Roland canon by presenting it as a whole. At the same time, it makes it possible to trace themes and imagery common to some or all of the works, presenting them together, interwoven as they are in the source materials but clarified by critical commentary interspersed with fragments of primary texts illustrating the points. These analyses could be presented on paper, each of them separately; however, this would make visualizing the whole much more difficult, and be discordant with Roland's multi-faceted nature.   ",
       "article_title":"The Myth of Roland and Its Function as Cultural Hypertext as Expressed in 'RolandHT'",
       "authors":[
          {
             "given":"Vika",
             "family":"Zafrin",
             "affiliation":[
                {
                   "original_name":" Brown University",
                   "normalized_name":"Brown University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05gq02987",
                      "GRID":"grid.40263.33"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper will address our work in the area of software models and architectures for supporting the development of software tools for text corpora and digital libraries. Previously we have presented a region-based approach for defining and manipulating things found inside text documents [6]. We believe that combining a region-based approach with a markup-centered approach that supports XML and SGML has many benefits. For example, our approach allows software tools to manipulate text features that are not marked-up and features that are non-hierarchical in nature (including arbitrary user selections of parts of a document). Both approaches can be integrated into one tool; for example, a prototype in Java has been developed in earlier work [4] that can manipulate an XML document using a DOM (Document Object Model) implementation while also supporting the region-based operations that we propose. In our paper we will specifically address one benefit of our approach: a general method for the visualization of query results. Before presenting this method, we must first provide the terminology used to define it. Our region-based approach is largely inspired by sgrep, a free utility for searching structured text documents. It takes a query from a user and returns sections of a text file that match the query. Both its query language and the results it finds are based on an algebra of *regions* and *region sets*: A *region* is a chunk of text, determined by a starting and ending byte position in the text file. A *region set* is collection of regions that can be manipulated in powerful and well-defined ways. For example, two sets can be concatenated, or merged to produce all regions in the first set that are contained with some region in the second region set.  A simple query in sgrep might be to find a single string in a file; this returns a set of regions, each of which is the pair of starting and ending byte offsets in the file where that string occurred. Using regions in text processing is not a new concept. sgrep's notion of regions is identical to the concept of a *span* defined in the TIPSTER architecture [3] for information retrieval software and implemented by systems such as GATE [1]. The use of byte-offsets in some way to identify index terms has also been used in earlier systems. For example, in a PAT tree [2] tokens to be indexed are defined using semi-infinite strings (sistrings) that begin at one byte position. This data model supports efficient retrieval in this manner. But our approach differs from this because our reason for using regions is not to achieve efficient retrieval but to support the notion of \"nesting\" of text features. This supports interesting manipulating of these features to achieve end-user goals beyond efficient retrieval. We have also developed an important enhancement to this approach for XML documents by which regions and nesting are defined based on the markup hierarchy without relying on byte positions. A region set is general enough that it can be used to represent many different things in a text: all words in the document; all occurrences of a given token; all DIV1 elements in an XML document; all XML elements that have an attribute with a given value.  Region sets can also represent a subtext or \"selection\" made from a document. A region set could represent some selection of XML elements or arbitrary byte positions that that overlap the markup hierarchy. We will use the term *text object* (TO) to mean any \"thing\" that a user wants to define and manipulate in a text document. A *text object occurrence* (TOO) is one occurrence of one of these. A TOO is simply represented by a region (If a TOO is non-contiguous, then it would be represented by a region set. Also, if a document is composed of more than one file, our definition of a region must be adapted. Both of these issues add a level of complexity that we will ignore for the sake of this abstract.) Thus \"words\" and markup-elements are both text objects in our model, with occurrences of each one represented by a region set. As users of sgrep understand well, many useful queries can be defined in terms of the nesting or inclusion of two region sets. It becomes simple enough to find or count occurrences of a particular word (or set of words) in a \"context\" such as any markup-element unit (e.g. words, lines, paragraphs) if both word-tokens and markup are stored as region sets. And since we have a common model where all of there are represented as text objects, it is just as simple to find or count occurrences of one markup-element inside another, or inside a user-defined selection. This is why we argue that a region-based approach as described here enhances a markup-centered view of documents in software that processes text. Our paper will demonstrate this by focusing on how queries to find occurrences of text objects (e.g. words, markup elements) can be visualized. Our approach is a modified version of the TileBars technique [5]. In our approach, a user would define one or more query terms; each might be one or more text objects. In addition, the user would choose a search context to indicate the segments or units in which the software should find occurrences of each of the query terms. Results of such a query can be shown graphically; for example, this URL shows TileBars resulting from a query on a small set of documents (from the original paper on this technique):  In this output, a document is represented by a rectangle. Each of these has a \"row\" for each query term, and the \"columns\" represent the segments that the document is divided into. A shading of grey in each row-column intersection indicates how often each query term occurs in that segement. This visualization technique displays the results of queries in terms of several dimensions: documents, terms, and segments. Various attributes of the query results can easily be seen. First, the size of each document (in terms of segments) is shown by the size of the tile. Second, occurrence of terms by segment is indicated when a cell is grey, revealing the relative location within a document where the terms occur. Third, strength of occurrence of each term is indicated by the shade of grey. Finally, cooccurrence of multiple terms is indicated by looking for columns where both rows are shaded. Our modifications to the original TileBar method are as follows. First, our query terms are not limited to simply word types. Any text object or combination of text objects can be used. These could include of course markup-elements. Second, the context that defines the segments in each document can be completely controlled by the user, as long as a region-set is selected. The context could be quickly and easily changed to allow a user to see results by, say, act, scene or line in a play. Each of these improvements is directly supported by our region-based model for documents and their contents. In conclusion, we believe region-based approach can be combined with a markup-based view of documents to produce software architectures that can meet a variety of user needs. For example, our approach provides a common model for representing words and markup that can more flexibly support the needs of users of digital text resources. This common model shows its advantages in developing a useful and generalized visualization scheme based on the TileBars approach.  ",
       "article_title":"Query Visualization, Markup, and a Region-based Document Model",
       "authors":[
          {
             "given":"Thomas",
             "family":"Horton",
             "affiliation":[
                {
                   "original_name":" University of Virginia",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          },
          {
             "given":"Sunish",
             "family":"Parikh",
             "affiliation":[
                {
                   "original_name":" Florida Atlantic University  ",
                   "normalized_name":"Florida Atlantic University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05p8w6387",
                      "GRID":"grid.255951.f"
                   }
                }
             ]
          },
          {
             "given":"Robert",
             "family":"Nash",
             "affiliation":[
                {
                   "original_name":" Florida Atlantic University  ",
                   "normalized_name":"Florida Atlantic University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05p8w6387",
                      "GRID":"grid.255951.f"
                   }
                }
             ]
          },
          {
             "given":"Abhijit",
             "family":"Pandya",
             "affiliation":[
                {
                   "original_name":" Florida Atlantic University",
                   "normalized_name":"Florida Atlantic University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05p8w6387",
                      "GRID":"grid.255951.f"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Tradition is a vital element of all cultures. It serves as central means for the maintenance and propagation of collective or social memory, binding the present to the past in almost innumerable ways. While there is no shortage of studies of particular traditions, it is surprising find that \"scholarly analyses of the history and present meaning of the concept of tradition remain remarkably few.\"Mark Phillips, \"What is tradition when it is not 'invented'?; an historiographical introduction\", in M. Phillips and G. Schochet, eds. Questions of Tradition (University of Toronto Press, forthcoming). Edward Shils made the same claim, 20 years ago, in his ground breaking Tradition (Chicago, 1981). Derived from the Latin traditio, a form of handing property down from one generation to another, the meanings and associations of this term have developed differently in English and French from the early modern period. The last decade has been marked by the appearance of an impressive array of very large textual databases containing significant samples of several national literatures. These collections cover many genres and document types, including different kinds of reference works as well as canonical and noncanonical literary holdings, spanning many centuries. The chronological scope and wide coverage of these databases open the possibility of performing systematic analysis of the history of the idea of \"tradition\" in individual languages and comparisons of the history of this notion between languages. The appearance of so many large textual databases has created a new set of problems, which might be thought of as an embarrassment of riches. The number of occurrences of even relatively rare terms can exceed even the most dedicated scholar's ability to review all of the results. As part of the initial work on this project, I implemented two interactive (or real-time) extensions to PhiloLogic that I found useful in grasping \"the big picture\". The first, based on earlier implementations, allows the user to generate word frequencies and corrected frequencies broken down by time periods. The second is a simple collocation generator which calculates frequencies of all words within a user selected span to the left and right of a keyword with an optional filtering mechanism to eliminate high frequency function words from the report.Susan Hockey provides a useful overview of \"collocation\" in Electronic Texts in the Humanities, (Oxford, 2000), pp 90-91. See also Mark Olsen, \"Gender representation and histoire des mentalités: Language and Power in the Trésor de la langue française\", in Histoire et measure VI (1991): 349-73. The broad outlines of the senses of the word tradition provided by the Dictionnaire de l'Academie française (DAF) from the first edition (1694) to the eighth (1932-5) are consistent. The first edition provides 3 senses of tradition; a legal term for the delivery of property; a means (voye) of transmission of unwritten knowledge from the past, primarily in a religious context; and the contents of knowledge coming to us by la voye de la Tradition. Two and a half centuries later, the Academy still classified traditionas a term de Jurisprudence et de Liturgie leading the definition with senses drawn from legal and Catholic citations. The primacy of the religious cast of tradition in 17th century French is confirmed by an examination of the most frequent collocates of tradition, which include e[s]criture, eglise, sainte, and constante. There were, however, non-Catholic traditions could not be accorded the same authority, for which a different, secularized word was used: \" Traditive s.f. Il a les mesmes sens que Tradition, mais il ne se dit point dans les matieres de Religion.\"DAF, 1st edition (1694) vol. 2, p. 583. This non-Catholic \"tradition\" disappears in later editions of the DAF and appears to loose currency in the language. A search of the ARTFL database reveals 9 occurrences dated between 1620-1660 while a search of the Corpus of Philosophy finds 13 citations from the end of the 16th century, and it is found in earlier dictionaries, such as Jean Nicot's Thesor (1606). The disappearance of the traditive in the early 18th century is marked by recasting of tradition as a form of knowledge which can be verified, most remarkably by the philosophes, and by the appearance of adjective (traditionel, elle) and adverbial (traditionnellement) forms, suggesting that tradition moves from an identifiable thing to a characteristic. The entries for tradition in the Encyclopédie are classified as being part of \"sacred criticism, religion, or law\", the article in which the word tradition is most frequently used is Certitude. This article is an examination of the means by which one may achieve certitude morale when assessing knowledge based on reports of witness through three channels canals) or lignes traditionelles, oral, written and monumental.  Il paroît par ce que j'ai dit jusqu'ici, qu'on doit raisonner sur la tradition comme sur les témoins oculaires. Un fait transmis par une seule ligne traditionelle, ne mérite pas plus notre foi, que la déposition d'un seul témoin oculaire; car une ligne traditionelle ne représente qu'un témoin oculaire; elle ne peut donc équivaloir qu'à un seul témoin. Encyclopédie, vol. 2, p. 854.  Tradition retains the possibility of being authoritative, even for the philosophes, because it may be critically evaluated.  The authority of tradition in English in the early modern period is far more questionable because of the influence of the Protestant attack on Catholic tradition. The most frequent collocates of tradition in the English Poetry database between 1550 and 1699 include vain(e), man's, men's, superstition and false. Many citations equate tradition with papism, falsehood, and superstition. In his Voyage Around the World..., Dunton points out that travel is an important corrective given \"how fatal into Truth the dependence upon the Tradition and Authority of Men\".EEPF: p. 290.By contrast to the extensive treatment in the Encyclopédie, the Encyclopedia Britannica, half a century later, fully defines tradition as something handed down from one generation to another without being written. Thus the Jews pretend, that besides their written law contained in the Old Testament, Moses had delivered an oral law which had conveyed down from father to son; and thus the Roman Catholics are said to value particular doctrines supposed to have descended from the apostolic times by tradition. 5th edition (1817), vol. 20, pp. 458-9.   Similarly, both Locke and Hume suggest that tradition is an almost completely unreliable form of knowledge. In contrast to the examination in the Encyclopédie, Hume writes An historical fact, while it passes by oral tradition from eyewitnesses and contemporaries, is disguised in every successive narration, and may at last retain but very small, if any, resemblance of the original truth, on which it was founded. BritPhil: David Hume, Natural History, p. 312.  The English representation of tradition, into the 19th century and even among prominent intellectuals, is shaped by anti-Catholic positions from a much earlier period. The French representation of tradition in the 19th century sees the development of social and cultural usages, which also appears to be typical of English usage, an issue to be examined in the full paper. This is suggested both by the frequencies of the use of tradition in particular works and terms most commonly associated with tradition. For example, 127 of 481 occurrences of tradition in the 19th century (CPhil) are found in one work: Ballanche, Pierre-Simon [1818], Essai sur les institutions sociales, with 70 occurrences in Proudhon, Pierre-Joseph [1860], De la justice dans la révolution et dans l'église. Examination of collocation tables for French databases, such as ARTFL, suggest that in the 19th century, traditions are localized by geographic region, family, and class, associated with customs, while retaining clear religious connotations. The twentieth century introduces a new strong pattern of associations, featuring national, French, political and revolutionary traditions as the most frequent collocates, which take some precedence over the personal, local and familial patterns of 19th century tradition. There are also important shifts in the \"ownership\" of tradition in French. The construct \"our tradition[s]\" rarely appears in the 17th or 18th centuries, but becomes fairly common in the 19th and 20th centuries. By contrast, the 3rd person possessives (his/her, their) also increase during the period, but not nearly as quickly.The first person plural possessive and all forms of tradition: 1 occurrence in the 17th century, 44 in the 20th. Third person plural possessive and all forms of tradition: 17 occurrences in the 17th century, 84 in the 20th. In the 17th and 18th centuries, other people (Jews for example) had their traditions, but by the 19th and 20th centuries tradition becomes associated with local, national, or cultural identification. The use of relatively simple methodologies applied to large textual databases may result in useful contributions to the history of concepts and histoire des mentalités, by facilitating examinations of contrasts between long term developments across languages and the isolation of individual texts or authors that may introduce significant changes in these trends. The examination of tradition, in French and English, reveals evolving patterns in relationship of contemporaries to their past, authority of the past, and the importance of the past in self-definition.  ",
       "article_title":"Handing it Down: a survey of the use of tradition in French and English from the 16th to the 20th centuries",
       "authors":[
          {
             "given":"Mark",
             "family":"Olsen",
             "affiliation":[
                {
                   "original_name":" University of Chicago",
                   "normalized_name":"University of Chicago",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/024mw5h28",
                      "GRID":"grid.170205.1"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In \"Stephen Crane and the 'New York Tribune': a Case Study in Traditional and Non-Traditional Authorship Attribution,\" a paper delivered at ALLC/ACH 2000, David Holmes and Michael Robertson argue for the use of \"objective, stylometric evidence\" to support \"the 'traditional' scholarship on the problem of authorship\" and suggest that \"this joint interdisciplinary approach should be the way in which attributional research is conducted\" (175) This paper presents a case study to support the argument that such an interdisciplinary approach may prove as important in literary analysis as in attribution studies. Of course, in one sense, this recognition of the symbiotic relationship between traditional and quantitative analysis is not new. J. F. Burrows' groundbreaking Computation into Criticism brilliantly and seamlessly moves between statistical and traditional literary analysis and clearly recognizes the dialogue between the two. As well, any number of papers coming out of this conference use quantitative analysis to test or illustrate insights from literary criticism. (See, for example, Opas and Tweedie, \"The Magic Carpet Ride\" and their \"Come into my World: Styles of Stance in Detective and Romantic Fiction\" or McKenna's \"The Statistical Analysis of Style: How Language Means in Beckett.\") However, the value of using statistical analysis to augment traditional criticism appears not to have been widely recognized and it seems important to continue to insist on the insights that each approach may bring to the other. This case study concerns texts of Charles Brockden Brown, usually recognized as the first professional novelist in the United States. For many years, Brown was considered a relatively imprecise and careless stylist, who gave little attention to character creation and particularly to narrative voice. However, a revival of interest in Brown during the last fifteen to twenty years has brought with it increased admiration for Brown as a stylist but with little close examination of that style. The purpose of this study was to look more closely at that style, particularly by examining the narrative voice in two texts: Wieland or The Transformation (1798) and Carwin, The Biloquist (1803-1805). Wieland is a novel of twenty-seven chapters, most of which are narrated by Clara Wieland, the novel's Gothic heroine, but with three chapters narrated by each of three other characters. Carwin is an unfinished novel, the first ten chapters of which were published serially. Carwin, the Gothic villain of Wieland, narrates one chapter of that novel and the whole of Carwin. The question with which this study began was whether Brown had created in Carwin a character and narrator with a distinctive voice, whether what we call the character Carwin is a distinct literary or linguistic entity. If so, the evidence would seem to suggest greater precision in the creation of narrative voice than has been traditionally recognized. To determine differences or similarities in narrative voice, the study utilized a variation on what is sometimes called the Burrow's technique, using multivariate analysis to discover patterns in the occurrence rate of the text's most common function words. However, along with the use of the fifty most common words, this analysis included as variables the occurrence rate of punctuation marks (periods, commas, question marks, exclamation points, semicolons, colons, and ellipses) and of other stylistic markers such as sentence and paragraph length. These variables included the rate of short and long sentences (defined as five or fewer words and twenty-five or more words), short and long paragraphs (defined as fifty or fewer words and one hundred and twenty-five or more words), average words per sentence, average sentences per paragraph, and percentage of unique words. The latter variables were added because in the analysis of a sample of fifteen late-eighteenth- and early-nineteenth-century novels, the addition of these variables appeared substantially to increase the sensitivity of the analysis. The presentation will discuss the variables which were most effective in making discriminations. With the addition of these variables, a principle components analysis was made for each chapter in Wieland and Carwin, and the results of the two most significant factors were displayed on a scatter graph. These results show a clear distinction between most of the chapters in Wieland and those in Carwin. (See Figure 1.)  With the addition of these variables, a principle components analysis was made for each chapter in Wieland and Carwin, and the results of the two most significant factors were displayed on a scatter graph. These results show a clear distinction between most of the chapters in Wieland and those in Carwin. (See Figure 1.) However, chapter twenty-three of Wieland, the chapter narrated by the character Carwin, is clearly situated among the chapters from Carwin. Thus, it would seem that the narrative voice of Carwin is distinctive, that Brown has created a character whose voice in one novel is recognizable as his voice in the other. Those results might have concluded the study were it not for the fact that two other chapters from Wieland are situated in the constellation of Carwin chapters, chapter twenty-seven, the final chapter of the novel, and chapter thirteen, a chapter narrated by another character, Pleyel, a romantic interest of the protagonist, Clara Wieland. It is when considering and attempting to explain the location of these two chapters among those from Carwin that the strongest case can be made for the joint use of quantitative and traditional literary analysis. It is not just that quantitative analysis may support and supply evidence for traditional approaches but that traditional analysis may help us understand what we are seeing in the results of quantitative analysis. Upon reflection, the analyst should probably not be surprised to find the narration of Clara in the final chapter to have much in common with the narrative voice of Carwin. Critics have long noted that Clara's voice in the final chapter undergoes a transformation, and some have even suggested that Clara has come ultimately under the influence of Carwin, whose villainy throughout has been associated with his powers of ventriloquism. That he has thrown his voice a final time and taken over the voice of the dominant narrator has been at least hinted by critics. Quantitative analysis supports such an interpretation, and the interpretation helps the quantitative analyst to understand his or her results. The initially more problematic finding is the location of chapter thirteen of Wieland among the chapters from Carwin. Since it is a chapter narrated by Pleyel, one might speculate that the narrator Clara has one voice and all other narrators have another. However, such an explanation does not account for chapter nineteen, narrated by Theodore Wieland, which differs both from the chapters narrated by Carwin and from those narrated by Clara. The more likely explanation comes from Steven Watts, who, in his book The Romance of Real Life: Charles Brockden Brown and the Origins of American Culture, comments on the usually unrecognized similarities of Pleyel and Carwin and, in fact, finds Carwin to be the alter ego of Pleyel. Although this interpretation has not gained wide currency, the similarities in voice indicated by the statistical analysis suggest that it might be reconsidered. These findings seem to support what J. F. Burrows points out: \"that exact evidence, often couched in the unfamiliar language of statistics, does have a distinct bearing on questions of importance in the territory of literary interpretation and judgement\" (2). However, the findings also suggest that traditional critical interpretation has a real bearing on how we understand the meaning of those statistics.  ",
       "article_title":"Charles Brockden Brown: Quantitative Analysis and Narrative Voice",
       "authors":[
          {
             "given":"Larry",
             "family":"Stewart",
             "affiliation":[
                {
                   "original_name":" The College of Wooster",
                   "normalized_name":"College of Wooster",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/029zqs055",
                      "GRID":"grid.254509.f"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   1. Introduction Human-machine communication by voice is one of the most dynamically developing fields in information technology. Since human speech in its oral form is the most natural and compact means for communication, it should be used for information exchange in any technical innovation, new technology, as widely as possible. Speech technology is inevitably gaining greater and greater importance in computing. Speech technology is a relatively new field in applied science, and it is strongly interdisciplinary. It integrates both human and engineering sciences such as theoretical and applied linguistics, acoustic phonetics, physiology, mathematics, software engineering, database handling. Although for several years it has been widely recognized, that for success, it is essential to combine these disciplines in education, it is still hardly reflected in higher education curricula [1]. The reason is obvious: it is an essential but a painfully hard task to build the bridges between the various disciplines involved in speech technology, to show its basis in its all complexity. In support of, or, instead of interdisciplinary curricula, special subject oriented courseware should be offered to the students. Multimedia tutorials are, undoubtedly, the most suitable teaching aids in speech communication sciences and for several reasons. Multimedia in this case does not simple offer novel ways of presenting unfamiliar material but it suits best both the nature of human speech itself and the technical requirements of the subject; it provides tools for experiments, creative, problem-solving learning; for the development of necessary skills in e.g. labeling speech, reading spectrograms and it offers extensive resources for ear- training. For the past few years a number of computer assisted learning systems have been developed in the field, the variety of which demonstrates the range of the topics covered, the anticipated knowledge and utilization of interactive elements [1, 2]. In this paper we will introduce a new multimedia tutorial \"Speech technology for Hungarian\" and highlight some of the methodological and technical problems associated with the development of multimedia applications in speech communication sciences.   2. Outlines of the content The multimedia tutorial introduces main features of speech technology. \"Speech technology for Hungarian\" summaries a kind of knowledge that is not taught as a complex course in Hungarian higher education institutions and presents it in a form suited for independent self- instruction by students. The objective of the tutorial is to provide a starting point for improving the training of future Hungarian linguists and speech technologists. For linguistics students it can help to turn from traditional articulatory phonetics towards state-of-the-art speech science and prepare them to solve important issues in the practical application of speech. For engineers it provides the linguistic basis for speech science, the lack of which explains the low quality of some existing applications. The tutorial consists of seven units: 1. Anatomy and physiology of speech production and hearing; 2. Physics of sound; 3. Speech acoustics; 4. The Hungarian speech; 5. Speech synthesis; 6. Speech recognition by machine; 7. Digital speech processing, and provides a glossary of terms and an annotated reference list of relevant literature for the past five years. To tailor the content material to the target users we decided not to deal in detail with the underlying mathematics of signal processing, Unit 7 outlines only the main and most widely used digital speech processing techniques such as FFT, LPC, PSOLA, HMM. At the same time, the tutorial pays special attention to language specific issues. So, for example, Unit 4 offers a collection of extensive articulatory and acoustic data on the segmental and suprasegmental building elements as well as thorough description of rules for the Hungarian language; in Unit 6 separate parts deal with the definition of language dependent rule sets for duration modelling, for the adjustment of correct intensity values in the case of each soun and for the modelling of the structure of the fundamental frequency changes in time (sound, syllable, word, phrase, sentence and text level). Each unit ends with exercises for self-assessment.    3. Technical Issues One of the first main questions that arises is the selection of the developing tool. In spite of the fact that the audio capabilities of early versions Java were quite restricted, Web-browser executable Java softwares have rapidly gain ground in speech science CAL-systems for their platform independence and computational capabilities. Before Java 1.3 one did not have speech input capability; audio playback was restricted to a-low or mu-low coded 8 bit quality, what is certainly unsuitable for a course about the understanding of the real nature of the sound. With sound handling becoming much more versatile in Java 1.3, we decided to implement the tutorial using relatively cost-saving and platform-independent tools: standard HTML including Java 1.3 and other plug-ins - Flash 5.0 and QuickTime 5.0 for animations and video elements.   4. An Example For demonstration, let us quote the tutorial material on speeding up and slowing down the tempo of speech signal. To fit into the time-limits of a TV or radio commercial or for psychological experiments the artificial manipulation of speech tempo has to be implemented without changing pitch and distorting the quality of the sound. The basic technical issues in speech processing are the determination of cutting or pasting periods, short segments of sounds. The question is what stretches of the signal to choose to minimize the distortion. There are different solutions to this problem, most of them based on mathematical methods applied uniformly to the speech signal. The tutorial material leads the student through the steps of working out an algorithm that accounts for the acoustic structure of different sets of sounds and the contextual changes in the formant frequencies [3]. This algorithm fits the best the main aim of the courseware: to show how the understanding of the nature of the sounds can be utilized in practical applications. The section is based upon with interactive elements by the help of which students themselves can manipulate the tempo of the speech examples.   ",
       "article_title":"Human-machine communication by voice (A multimedia tutorial for linguists and engineers)",
       "authors":[
          {
             "given":"Magdolna",
             "family":"Kovács",
             "affiliation":[
                {
                   "original_name":"Department of General and Applied Liniguistics, Debrecen University ",
                   "normalized_name":"University of Debrecen",
                   "country":"Hungary",
                   "identifiers":{
                      "ror":"https://ror.org/02xf66n48",
                      "GRID":"grid.7122.6"
                   }
                }
             ]
          },
          {
             "given":"Péter",
             "family":"Nikléczy",
             "affiliation":[
                {
                   "original_name":" Kempelen Farkas Speech Research Laboratory",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Gábor",
             "family":"Olaszy",
             "affiliation":[
                {
                   "original_name":" Kempelen Farkas Speech Research Laboratory",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Institutional environment In the mid-1980s the University of Exeter (Project Pallas) and the University of Würzburg ('Linguistic Information and Text Processing') initiated courses in Humanities Computing for undergraduate and postgraduate students. Student and staff exchange programmes within the Erasmus/Socrates framework of the European Union followed soon afterwards. By 1995 Exeter had launched its ReLaTe project, to investigate the use of multimedia internet conferencing for 'Remote Language Teaching (Buckett and Stringer 1997) together with University College, London. Following on from this co-operation, trials with the new medium were extended to investigate the feasibility of delivering inter-university academic courses across national borders. A report on this was published by the Würzburg Computing Centre in 1999 (Fahrner and Plehn 1999) and the results demonstrated at the CALL - Challenge of Change conference at Exeter (2001). Here the authors presented a scenario for providing and managing conversation classes designed to allow students to practise foreign language skills with native speakers across Europe using freely available videoconferencing tools (Buckett et al. 2001). The study was based on work with first year students of German at Exeter and postgraduate students of Humanities computing at Würzburg learning English for special purposes. Building on this experience we now outline the next stage in creating virtual classrooms based on videoconferencing software: There are two aims. One is to extend the conversation class setting to one in which students of English/German are studying the language of IT and technical communication; this will enable an investigation of the problems associated with the restrictions imposed by the subject area in the context of 'live' videoconferencing. The second aim is to record authentic class communication in order to build up an archive illustrating the variety of interactions that may be exploited to teach English/German for special purposes in the field of IT.   Impact of the new media: hardware, netware and software Using a PC to see and talk to someone else working at a remote location requires specific hardware (camera, graphics and audio cards, microphone and headset) as well as software and netware tools to create and manage the connection. Linking more than one PC at each location normally generates a considerable load on the network. Using software for the Mbone configured to handle multicast traffic, however, all participants can communicate with each other efficiently and on equal terms (for details see Kumar 1996). Online conversation with native language speakers creates a completely different environment for language learning, compared with the standard 'schoolroom' setting of conversation classes. Normally foreign language students, who outside class would communicate in their native language, switch to a foreign language in order to discuss problems with their teachers, language assistant or tutor. Using internet mediated communication these students are able to practise naturally with native speakers, with all the advantages which that entails: they learn to cope with regional differences in speech, develop authentic communication strategies in the foreign language, and become sensitive to the effect of subtle changes in voice pitch, the function of pauses and to the vital area of non-verbal communication. The very act of talking to native speakers (instead of each other) is likely to stimulate the use of a quite different set of words and syntactic structures. In a normal face-to-face conversation class all the participants are in one room, know each other and can draw on the physical/spatial dimensions of speech events: sensing the direction of the sound, they can detect who is speaking, can face him/her and use cues (such as the length of a pause) to judge when he/she has finished in order to take a turn. Communicating via the virtual internet, however, alters the spatial and perceptual context appreciably: the participants are not in the same room; they do not know all the other participants; they can hear in their headphone that some one is speaking, although there is no direction of sound that could help them to identify the speaker; and the audio signal in their headphones lags perceptibly behind the video signal on screen, so that they may misunderstand a slight pause as a signal to take over, prompting them to interrupt the speaker. This lack of conversational coordination leads to a kind of stop-and-go interaction in which more than one speaker starts to take a turn, realizes that others have done the same, stops, and pauses for quite a long time to establish who will finally dare to continue. There is also a direct spatial relationship between the physical speech and the visual information displayed on the computer screen. The size of the person taking part in a face-to-face conversation class is greatly reduced in the virtual, on-screen internet environment: here the size varies from thumbnail (in the list of displayed participants) up to a maximum of a CIF image. For the internet participants, who are not in the same room and do not necessarily know each other, it is therefore indispensable to customize the screen layout by captioning each window with the name of the speaker, so that all the members of the conference know who he/she is. The above factors have implications for positioning the video camera: this should be as close to the screen the student is looking at as possible, so that the listeners feel addressed by what the speaker is saying and the speaker gets the impression that the others are attending to what he is explaining (the so-called Casablanca Effect or 'Here's-looking-at-you'-principle). Ideally the camera image needs to be large enough for the viewers to be able to see the speakers' facial expressions (and possibly even to watch lip movements), whilst at the same time having a sufficiently large field of vision that they can also see gestures and general body language. The size of the conversation windows and the screen hardware used (17 inches minimum) define the maximum number of participants that can be handled effectively. We tested settings between three and eight participants: with eight participants the conversation was most lively, but not all participants took an active part in it; with three or four participants all were really active. We measured the degree of activity by comparing the number of sound packages sent by each participant. We illustrate these problems and related questions by video clips from our initial corpus, which are discussed below.  (1) An impact factor in the new medium is the relationship between the audio and the visual dimension for the participants. Issues here include: the size of the visual image of a speaker on the screen; the role of direct eye contact; the ability to see more than the face (or mouth) and the question how far the medium does intrude into the interaction or otherwise inhibit it. Is there a relationship between the language level (or even the language itself) and the influence of the medium? (2) Our experience so far suggests that the management of 'turns' or hand-overs in the discourse is of particular interest. For instance, how does a participant gain the attention of the group? How does one interject? How is the participation of listeners registered if their visual presence is reduced? Does a group need a designated manager of the discourse or is interaction completely open? Should the group be aware of 'rules' or procedures for managing interaction? What form should these take? (3) We show representative scenes that we intend to include in our database of videoclips illustrating typical communicative interactions. These clips will be transcribed using standard transcription systems for discourse analysis (Edwards and Lampert 1993, Selting et al. 1998). The materials will be used to introduce and prepare future students for technical communication in a foreign language. In addition we plan to use them for further research on how to enhance learning opportunities and develop the communicative skills of students (studying, for instance, technical translation) in an environment which offers multimedia support.  We are aware that much research and activity are being undertaken in this field and we will compare our experiences with the results of broader and more general approaches to using the internet for teaching. Such approaches may not necessarily include the use of live videocommunication in class (cf. Warschauer 1999 and Warschauer, Shetzer and Meloni 2000). In conclusion, our experience suggests that it is possible to enhance interactive communication skills in the virtual internet medium but that more research is needed to identify the optimum parameters for co-ordinating the needs of learners in groups with the medium itself. In this paper we focus on German-English interactions for the purposes of general communication, with emphasis on initiating conversation, greetings, turn-taking, eliciting information, and overcoming communication obstacles. The corpus material will provide a searchable textbase that can be compared with instances of classroom interaction and exploited for pedagogical purposes.   ",
       "article_title":"The virtual classroom: Videoconferencing for foreign language learners",
       "authors":[
          {
             "given":"Werner",
             "family":"Wegstein",
             "affiliation":[
                {
                   "original_name":" Universität Würzburg",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Derek",
             "family":"Lewis",
             "affiliation":[
                {
                   "original_name":" University of Exeter",
                   "normalized_name":"University of Exeter",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/03yghzc09",
                      "GRID":"grid.8391.3"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The advent of the Internet has introduced a number of communication technologies to higher education that have had a significant impact on the ways students and teachers interact. By now most students use email to communicate with their professors on a regular basis, and it is common for professors to use online discussion boards and chat software to reach out to their students beyond the classroom. Even though these technologies create more opportunities for contact between students and instructors, they tend to promote uneven interaction outside the classroom: some students will take advantage of the media, often dominating the discussion, while other students will feel reluctant to participate or will allow other students to carry the discussion. Many educational software companies such as Blackboard offer integrated packages with assessment tools for class surveys and online tests. As someone who has experimented with these tools in my courses in language, literature, and culture, I have searched for a way to interact individually with each student, addressing his or her particular strengths and weaknesses with content while at the same time assessing the entire class to identify those areas where they understand the material and where more guidance on my part is required. The dissatisfaction with existing courseware prompted me to write my own web-based program, the Web Cahier. In this paper I will explain the pedagogical philosophy of the Web Cahier and report on student feedback during the first year of its implementation at Hartwick College. (I will explain the design of the Web Cahier and make it available to other scholars as open source software in a separate poster session.) The pedagogy which informs the design of the Web Cahier is inspired by Just-in-Time-Teaching, or JiTT, a teaching strategy developed by Gregor M. Novak at Indiana University-Purdue University Indianapolis and Evelyn T. Patterson at the United States Air Force Academy for introductory physics classes. JiTT makes use of web technologies to present warm-up activities that students complete online shortly before coming to class. Once the deadline for completing the warm-up has passed, the instructor reviews student answers to focus on the areas where they need help. Novak and Patterson explain that \"[t]hese answers are used as talking points for the instructor later that morning, while the issue is still fresh in the students' minds. By doing this, the instructor \"individualizes\" the lecture. Students in the classroom recognize their own wording, both correct and incorrect, and thus become engaged as part of the feedback loop. It is quite common for the classroom discussion to continue via email between the instructor and particular students. Paradoxically, technology used this way encourages a more personal and intimate bond between instructors and students. It is clear from course evaluations that students feel part of a team working on a common project.\"  By surveying student performance before meeting them in the classroom, the instructor has an idea of how well each student is learning and can prepare a targeted lesson plan for the entire class. It is the balance between individual attention and group dynamics that makes JiTT a promising component to electronic pedagogy, particularly in the Humanities where students must learn how to engage texts and ask their own questions. Through my use of the Web Cahier I have adapted JiTT to courses I teach in French language, literature, and culture at Hartwick College. The Web Cahier leverages the ease of accessibility offered by computer networks and holds each student accountable for completing assignments. In my language courses, students watch digitized audio and video clips on their laptops as they complete their homework online. In my literature and culture courses, I give students warm-up questions to guide their thinking as they make their way through assigned readings. In all the courses where I use the Web Cahier, every student must complete an assignment one or two hours before class. He or she can work incrementally on the assignment, entering text into a web form, saving it, and returning to edit what they have typed as often as they wish before the assignment deadline. After a deadline has passed, students are unable to modify what they have done but they can review their work and read instructor comments. When I evaluate a student's work in the Web Cahier, I can give specific feedback for each question. As a result, I come to class with a lesson plan adapted to the specific needs of the students who I know have already engaged the material. Preliminary evaluations from students indicate that the Web Cahier has helped improve their learning by encouraging timely completion of assignments which are assessed before they meet with their instructor. Students feel they have a greater stake in what goes on in the classroom because they know that the instructor will use their work on the Web cahier to address their interests and difficulties. Students, however, feel isolated when working with the Web Cahier: they prefer the interaction that email, discussion boards, and chat rooms afford. Some complain they are overworked in completing assignments, and they resent the automatically enforced deadline which often causes them to feel anxious about their work. As for the instructor, the amount of labor required to prepare a lesson increases geometrically with every question posed to students. The instructor may also feel overwhelmed in reviewing student work within a narrow timeframe before class. The precision the Web Cahier offers in assessing individual performance comes at the cost of a more directive and labor-intensive pedagogy. In the paper I will present more definitive results from surveys of students and other faculty who have used the Web Cahier. At this time it seems evident that the Web Cahier can only provide one way to interact with students outside the classroom. While it is important to guide and assess how students are learning in a humanities course, there must be opportunities for discovery and free play beyond the control of the instructor.  ",
       "article_title":"Just-in-Time-Teaching in the Humanities: Lessons Learned from the Web Cahier",
       "authors":[
          {
             "given":"Mark",
             "family":"Wolff",
             "affiliation":[
                {
                   "original_name":" Hartwick College",
                   "normalized_name":"Hartwick College",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0288qta63",
                      "GRID":"grid.418410.8"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This presentation will describe an investigation that compares the relative effectiveness and accuracy of multivariate analysis (cluster analysis) of the frequencies of very frequent words and the frequencies of very frequent word sequences in correctly attributing texts to authors. Cluster analyses based on the most frequent words are fairly accurate for corpora of texts by known authors, whether the texts are 30,000- or 10,000-word sections of modern British and American novels, or 4,000-word sections of contemporary literary critical texts. They are, however, only rarely completely accurate; furthermore, when small groups of problematic texts taken from the corpora are used in simulated authorship studies, analyses based on frequent words rather consistently fail to cluster them correctly. But when frequent word sequences are used rather than frequent words or in addition to them, the analyses often improve in accuracy, sometimes quite significantly, suggesting that analyses based on frequent word sequences constitute improved tools for authorship attribution and statistical stylistic studies. One of the most popular places to search for a \"wordprint\" that can characterize the style of an author has been among the frequencies of the most frequent words of the language. In his seminal work on Jane Austen (1987), John F. Burrows demonstrated fairly convincingly that the frequencies of extremely frequent words like the, and, of, a, and to, can often be used to distinguish different authors, novels, and even different characters within a single novel. In spite of their intuitively insignificant nature, such words can even have interesting and potentially significant stylistic effects. This seems surprising when we remember that the five words above normally constitute roughly 20% of the word tokens in a novel. Yet their high frequency and the extreme unlikelihood that authors can or even wish to consciously control them suggests habitual or routinized use that may reflect an author's style across all his or her texts, in spite of differing subjects, themes, and points of view. Because of this, and because their frequencies often vary significantly among different authors, texts, and characters, in spite of their uniformly high frequencies (see Burrows, 1987: 3-4), frequent words have been popular targets for various kinds of multivariate analysis (see also, Burrows and Hassall, 1988, and Burrows, 1992). Much recent work with multivariate analysis of the frequencies of frequent words has produced interesting and significant results, especially in the field of authorship attribution (see Holmes, 1992, Holmes and Forsyth, 1995; Tweedie, Holmes, and Corns, 1998). As I have shown in a recent study (Hoover, 2001), however, cluster analysis of the frequencies of the most frequent words is very often not completely accurate in attributing texts to their authors when performed on a corpus of texts by known authors. As interesting as authorship attribution is, multivariate analysis is, I would argue, of potentially more interest in statistical stylistics and corpus stylistics. If techniques can be found that can accurately distinguish authors from each other, those techniques should be able to tell us something significant about the styles of those authors. To further the search for more accurate analytic methods, I have been evaluating cluster analyses based on frequent word sequences (defined simply as groups of contiguous words) rather than or combined with frequent words. (The idea for this project came out of a discussion with Gary Shawver of the Humanities Computing group at New York University about the possibility of looking at frequent collocations.) One reason that sequences are attractive is that the order of words within them provides information that cannot be retrieved from the frequencies of the constitutive elements alone. My investigation has shown that analyses involving frequent word sequences are often superior to analyses of the frequencies of frequent words in attributing known texts to known author. Some analyses using frequent sequences produce results that are completely accurate where frequent words alone fail, and some analyses using combinations of frequent sequences and frequent words are more effective than either by themselves, again sometimes producing completely accurate attributions in relatively intractable cases. I begin with an initial corpus of twenty-nine 30,000-word sections of Modern British and American novels by fourteen authors. I go on to analyze a subset of this corpus, consisting of twenty of novels by eight authors, limit the analysis still further to a corpus consisting of the sixteen third-person novels extracted from among the twenty novels, and then to the pure narrative of the same sixteen novels. I turn next to a very different kind of corpus, analyzing twenty-five contemporary articles of literary criticism, to test whether frequent collocations produce improved results for other genres. Finally, extracting the texts of two problematic authors from the sixteen pure narratives and two more from among the literary criticism analysis, I test the effectiveness of the analysis of frequent sequences under circumstances that more closely resemble traditional authorship problems. Although analyses based on frequent sequences or combinations of frequent sequences and frequent words are not universally more effective than those based on frequent words alone, and still fail to achieve completely correct results in some cases, they do seem promising as an additional tool in authorship attribution, and, potentially, in stylistic studies as well. Figures 1 and 2 show the improvement in two analyses when frequent sequences are used instead of, or in combination with, frequent words. Figure 3 shows that, in some cases, frequent sequences give correct results, even when frequent words uniformly fail.     ",
       "article_title":"New Directions in Statistical Stylistics and Authorship Attribution",
       "authors":[
          {
             "given":"David",
             "family":"Hoover",
             "affiliation":[
                {
                   "original_name":" New York University",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"      Anything a person writes contains the code of his intellectual DNA, or whatever you want to call it. Webb 1994     The greater the number of features and the more the features belong to different categories (e.g., syntactic structures, type of grammatical subject, inflexions, vocabulary, spelling, and so on) the stronger the case for shared authorship.  Eagleson 1989     INTRODUCTION: For many years it has been obvious from the literature that most non-traditional authorship attribution studies using one or some other small number of style markers do not carry the weight of scientific validity with either the majority of other authorship attribution practitioners, the specialists in the field of the study, or the general public. (In addition to Eagleson, see Banks and Rudman -- also, Rudman 1998) During a talk on the \"Style-Marker Mapping Project\" at the ALLC-ACH 2000 conference in Glasgow, I mentioned, in passing, an attribution model based on a \"DNA\" concept. (Rudman 2000) It was illustrative and not \"on topic.\" However, the audience picked up on this and some of the ensuing questioning and discussion kept trying to move away from the Style-marker Mapping Project. This paper presents a non-traditional authorship attribution model based on a \"DNA\" analogy. This paper emphasizes that it is only an analogy -- a framework to explain the techniques of the \"Inclusive Model\" -- there are obvious fundamental differences between DNA and style. Because some of the terms in this paper could be unfamiliar to the expected audience, a clear and concise definition is given the first time each such term is used.   I. BACKGROUND AND DEFINITIONS If we look at style as a living organism, style-markers are its genetic material -- making the Style-Marker Mapping Project (Rudman, 2000) analogous to the human genome project. I would like to extend this biology analogy: The Inclusive Authorship Attribution Model being analogous to the DNA analysis. The earliest reference to DNA and style that I have seen is Bailey's comparison of the tools used to decode the underlying makeup of the two -- X-ray diffraction for DNA, the computer for style. Bailey does not move towards a DNA model for stylistics. (Bailey) The lead quote by Webb also is quoted in Forsyth's dissertation. Yet Forsyth does not use the intent of the quote to move into a DNA model. (Forsyth) I have been leaning towards a more inclusive attribution model that would utilize a large number of style-markers since the mid 1980's. Other researchers also have recognized the need to expand the number of style markers in attribution studies. As the DNA structure became decoded and the comparison methods refined, it became the analogous model of choice. I first mentioned the model at the ALLC-ACH Oxford conference in 1992. (Banks and Rudman) The thrust of that presentation was towards a statistical method of combining the results of different statistical results on various style-markers. This section briefly traces the evolution of the DNA model through various publications and presentations. Clear and concise definitions of the DNA autoradiogram are given. (Kirby) A brief explanation of why this model is necessary closes this section. (Willing)   II. THE MODEL  Outline a method of analysis which will allow organization of these features [the entire range of linguistic features] so as to facilitate comparison of any one use of language with any other (Carter, Crystal and Davy, and Darbyshire). McMenamin 1993   A) How the Inclusive Model differs from other models (e.g. multivariate models and Burrows' Delta Project). (Holmes, Burrows)   B) The DNA Analogy is Explicated.It is shown how each locus of the autoradiogram is equivalent to a different style-marker. The determination of each style-marker locus is discussed.Forsyth's suggestion at the Glasgow conference that a list of \"proven\" style-markers should be provided and used is discussed. C) Visual RepresentationA Method of visual representation of the results of the model is shown. D) The following two statistical methods of combining each style-marker locus into a final answer are presented and discussed: (1) If the style-markers that are used can be shown to be independent of one another (e.g. word length distribution, percentage of nouns starting sentences, type/token ratio) a procedure based on Fisher's method for combining significance probabilities from independent statistical tests can be used. (Fisher) (2) If the style-markers that are used are not independent of each other (e.g. word length distribution, word length correlation, percentage of latinate words) the statistical method employed by DNA researchers can be used.      CONCLUSION The method of determining the DNA loci and style-marker loci are different. A single technique is employed to determine all of the DNI loci. Each style-marker locus is determined, for the most part, by different experimental techniques. And some of the style-marker loci are actually the result of multivariate statistical analysis. The Inclusive Authorship Attribution Model promises a degree of acceptability not seen in most non-traditional attribution studies -- especially in types of studies such as McMenamin's, \"`Population Model' where there are no obvious authorship candidates, and texts from an entire population of possible authors are considered against texts by one suspected author.\" (McMenamin)   ",
       "article_title":"'DNA' and Non-traditional Authorship Attribution: An Inclusive Model",
       "authors":[
          {
             "given":"Joseph",
             "family":"Rudman",
             "affiliation":[
                {
                   "original_name":" Carnegie Mellon University",
                   "normalized_name":"Carnegie Mellon University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05x2bcf33",
                      "GRID":"grid.147455.6"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction Stylometric analysis may be defined as the quantitative analysis of the recurrence of particular features of style for the purpose of deducing authorship and/or chronology of texts. Methods of sylometric analysis may be broadly subdivided up into lexical and non-lexical approaches (Binongo 2000). The approach to be taken in this study is to use function words using discriminant analysis as a method of analysis. The use of function words has often been criticised by stylistic experts as lacking in scientific validity, for example, see (Waith 1984). An ultimate aim of this work is to establish scientific foundations for the use of function words in stylometric analysis. One promising line of enquiry is in the study of aphasic patients where it has been suggested that function words may be processed by the brain separately to lexical words (Garrett 1982), perhaps we may have less conscious control over our use of them. Three plausible variables for stylometric analysis to consider are: chronology, genre and author. A writer's stylistic tendencies cannot be assumed to span an entire writing career, hence when the work was written is just as important as who wrote it. Similarly, it is unreasonable to assume that style will be fixed in different genres. One aim of this study is to fix two of the three variables as closely as possible while studying the effect of the other variable.   The Basis for this Study The primary basis for this study is to examine the effectiveness of discriminant analysis using function words as a means of distinguishing author while examining the effect of genre and time. The Sherlock Holmes stories of Arthur Conan-Doyle were chosen as subjects for study because they were freely available in machine-readable form and there were several comparable texts written at the same time in the same basic genre. Arthur Conan Doyle was also a prolific writer who has written different types of fiction very closely in time. The Sherlock Holmes stories were originally serialised in The Strand Magazine. However, when \"The Hound of the Baskervilles\" appeared it was plagued with controversy. Arthur Conan Doyle had insisted that his name appear jointly with a Mr. Fletcher Robinson. It has been suggested that Fletcher Robinson had written part of the story, in which case stylometric analysis might be able to throw light on this mystery. However, if Conan-Doyle was just given an idea for a story then there is not much that can be discovered by this technique. Nine texts were chosen for analysis, which can be divided into three equal groups:  Sherlock Holmes stories, including The Hound of the Baskervilles, as close in time as possible to it. Other works written by Conan Doyle at the same time as the three Sherlock Holmes stories. Other works by different writers in the same or similar genre (including some published in The Strand Magazine).  The sets of Sherlock Holmes stories that were written closest in time were chosen as comparand texts (note that all stories were serialised and published monthly). Thus the three Sherlock Holmes stories were: The Memoirs of Sherlock Holmes (1892/3), The Hound of the Baskervilles (1902) and The Return of Sherlock Holmes (1904) (Conan-Doyle 1986). The three comparand texts written by Conan-Doyle around the same time were: The Parasite. (1894) The Adventures of Gerard (1903) and Sir Nigel (1906).The three texts written in a similar genre around the same time but by different authors were: The Ponsonby Diamonds (Meale and Halifax 1894 - published in The Strand Magazine). The Old Man in the Corner (Baroness Orczy 1901)and The Scarlet Pimpernel (Baroness Orczy 1905). The texts were prepared in a manner that follow the technique employed by (Smith 1993) very closely. The top 20 most commonly occurring function words from \"The Hound of the Baskervilles\" were chosen. A discriminant analysis was then run using SPSS 10.00 with the three texts forming three groups. This produced a \"nearness\" metric for the texts. A series of tests will be presented that provide a strong basis for the use of discriminant analysis. The tests demonstrate the ability of this technique to separate texts by author, by genre (when author is fixed), or even by time. Tests were also carried out to ensure that the tests were not just arbitrary, but showed a real variation in texts.   Discussion and Further Work The starting point for this research was to investigate whether there is any evidence to support the thesis that Conan-Doyle may not have written all of \"The Hound of the Baskervilles\". There is absolutely no support for this in the results. The technique attempted to test for authorship by attempting to control two other major variables: time and genre. It was capable of distinguishing texts by author consistently. It also appeared to have the capability for separation of texts by genre and separated Conan-Doyle's works by time as well (The Hound of the Baskervilles was closer to The Return of Sherlock Holmes, written only two years later, than it was to The Memoirs of Sherlock Holmes, written some 9 years earlier.) One possible reason why the Sherlock Holmes stories differ from, the story used as a comparand text in a different genre might be because they employ a considerable amount of spoken dialogue, whereas the other story chosen contains far more narrative text. This has yet to be investigated. Principal Components Analysis (PCA) has been successfully used in stylometric analysis, (Burrows 1987, Binongo 2000, Binongo and Smith 1999). (Binongo 2000) seems overly pessimistic in his dismissal of discriminatory analysis, arguing against its use because of the assumption of multivariate normality. However his work reveals several worrying aspects of the use of principal components analysis. Firstly: the most frequent words tend to have the least discriminatory power and the first principal component may not be able to reveal authorship accurately. If the frequencies of function words are standardised, this in turn may lead more frequent words to be swamped by less frequent words. (Binongo and Smith 1999) demonstrated that PCA was capable of distinguishing difference in genres in a comparison between the essays and plays of Oscar Wilde. In a later study (Binongo and Smith 1999) they also demonstrated the success of this technique on a comparison of the works of two contemporaneous American authors Nathaniel Hawthorne and Herman Melville using 25 function words. Principal Components Analysis was also employed on the texts used in this study and although it appeared to reliably differentiate between three different authors, the principal components appeared to be less reliable in different genre tests and especially where two groups were from the same text. In this case the Kaiser-Meyer-Olkin statistic, measuring sampling adequacy (Kaiser 1970) indicated that the extracted components might be unreliable. Kaiser (1974) recommends accepting values greater than 0.5 and even values between 0.5 and 0.7 are considered mediocre (see also Field 2000). When PCA was applied to the same text split into two groups, KMO scores of between 0.4 and 0.55 were observed. Experiments with the numbers of function words were also tried. It was found that increasing the set of function words from 20 to 25 or 30 made only a marginal difference. Running a MANOVA test on the function word data allowed us to identify function words that were unreliable and re-running a discriminant analysis with these words removed produced a slight improvement. As the number of function words was decreased progressively the sensitivity of the test was diminished. The drawback with this approach is that it requires large amounts of text to produce reliable results (something approaching the size of a novella or short novel as minimum). The test will not be so sensitive to the insertion or interleaving of texts by different authors. If a form of dimension reduction can be established, then it might be possible to treat a text as a time series and use a window to drag over the text to look for anomalous sections that might indicate a change of author. A further way in which this work can be developed is to examine the function words themselves and examine why each author varies their usage. Some function words are used as what Schiffrin (1987) calls discourse markers and as higher-level indicators of structure, their use may well vary from one writer to the next. A linguistic basis for the variation in function words needs to be established if only to demonstrate the scientific credentials for stylometric analysis.        ",
       "article_title":"Stylometric Analysis Using Discriminant Analysis: A Study of Sherlock Holmes Stories",
       "authors":[
          {
             "given":"Peter",
             "family":"Smith",
             "affiliation":[
                {
                   "original_name":" The City University London",
                   "normalized_name":"City, University of London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04489at23",
                      "GRID":"grid.28577.3f"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In hypertext poetics, great importance is assigned to the notion of a decentralization of the authorial voice [7]: the author is said to gradually loose her power and authority within the hypertextual work, whereas the reader gains more centrality and control over the text and over the ways to navigate it and ultimately to construct the narrative. By introducing a distinction between shallow text (i.e., what appears on the screen, and therefore what the reader actually reads) and deep text (the encoded text that generates what will appear on the screen), Lughi [8] ultimately reduces the space of applicability of the above-mentioned construct: the reader's freedom is heavily conditioned and determined by the author, whose space of activity lies at the level of the deep text. So the author, by acting on and controlling the deep text, eventually limits the reading possibilities offered to the reader. This is possible because (and I may add, if) the author directly acts on the code behind the shallow text. This intervention may take different forms and open up a wide space for the author's experimental efforts and creativity. In this open space, adaptive hypermedia (AH) may play a significant role. So far, adaptive hypermedia have mainly been used as a didactic tool, i.e., as a tool to develop on-line educational systems, despite the potentially wider spectrum of systems they could be applied to (see in [1]). A restriction that adaptive educational systems soon showed was their application domain: most systems were used to teach computer science related or anyway scientific disciplines. Attempts at extending this limitation have been made, for instance by using adaptive hypermedia to teach foreign languages [2]. But the results obtained were not always so promising for the intrinsic difficulty at modeling such a domain: at a linguistic level (syntax and semantics), at a domain-related level (in the perspective of a situated learning approach to language learning), and at a level that was focusing on the intersection between the previous two. What has been neglected so far is however the possibility of using AH for the humanities, again not just as a didactic device, rather as a medium to promote advances in or to be applied to humanities disciplines. Although, recently, a research trend has emerged which has pointed out the potential advantages of exploiting adaptive methodologies to the delivery of cultural information, which have a direct influence on the way in which cultural heritage information is approached, accessed and fruited, not as much has been devoted to promote the production of artistic artifacts by means of AH. At a closer look, the contamination of art and technology is historically older than what the last years of Web explosion may seem to suggest: at the end of the nineteenth century, for instance, Rimbaud's fascination for photography was influencing his poetical style in determining which words to select, how to construct sentences, and how to juxtapose them in a visually effective way (see, for instance, in [3]). In this sense, AH should be considered as the canvas whose characteristics can influence the final artistic result in a peculiar way. A first step in this direction has however already been made: it is the system developed by Kendall and Réty [6], the Connection System. This system allows to write literature, both poetry and fiction, adaptively. On the basis of the previous discussion, this paper intends to pursue a dual goal: on the one hand, it intends to analyze a number of existing digital hyperfictions to extrapolate their characteristics, to investigate the amount of experimental composition and the kind of creativity that they allow to authors, to stretch and explore the limits both in the language and in the tool (the hypertext), in order to elaborate ultimately a rhetoric of hypertext. On the other hand, on the basis of the results obtained by this analysis, the paper will present an extension of the AHA! system [5], as augmented to include these derived elements. AHA! is an application developed at the Eindhoven University of Technology to deliver adaptive Web-based courses. Up to now, it has been used to design and develop educational courses (mainly in the field of Computer Science, but also in the Humanities [3] and an experimental course in CALL) and only recently for e-commerce applications.  ",
       "article_title":"Towards a Rhetoric of Hypertext: A Case Study in Adaptive Literature",
       "authors":[
          {
             "given":"Licia",
             "family":"Calvi",
             "affiliation":[
                {
                   "original_name":" Technical University of Eindhoven",
                   "normalized_name":"Eindhoven University of Technology",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/02c2kyt77",
                      "GRID":"grid.6852.9"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   The Research Context of the Problems In 1823, in the Public Record Office in London, a Latin document was discovered and catalogued as SP 9/61. It was identified as De Doctrina Christiana, Milton’s lost theological treatise mentioned by some of his early biographers. The evident heterodoxy of its arguments challenged Milton’s current status as the iconic poet of English Protestantism, and ever since the document has posed interpretative possibilities and problems for critics of Milton’s poetry, and especially of Paradise Lost. While some critics have treated the manuscript as a gloss on his other works, others have been perplexed by the apparent discrepancies between the treatise and his undisputed canonical works. Those discrepancies have usually been accounted for in terms of genre differences, but since 1991 a body of opinion within the Miltonist scholarly community has argued instead that SP 9/61 is not of a Miltonic provenance. In the mid-1990s an interdisciplinary group (all the current group plus David Holmes, who was then supervising Fiona Tweedie’s doctoral work) attempted to extract the key questions to resolve the problem. After several interim reports at conferences, the group issued its final report as a paper to the British Milton Seminar, as a web-document (still available as ), and as a major article (The Provenance of De Doctrina Christiana, Milton Quarterly 31.3 (1997): 67-117). The report unequivocally tied the document to a Miltonic provenance, while indicating its unfinished status and its stylometric inconsistency. It argued that the work was primarily produced in the 1650s; that Milton in part was incorporating and revising material from other Protestant exegetes; and that the manuscript has two principal strata, an ur-text and a transformation of that text effected by a process of revision which primarily consisted of the accretion of material by Milton. As a quick resolution of the status and pertinence of the manuscript in the interpretation of Milton’s later poems, the group had found some answers: indeed, the manuscript has been worked on by Milton, but that work was suspended and incomplete and probably belonged to a period significantly earlier than the publication of Paradise Lost. However, we did not contextualise it closely in the Protestant exegetical tradition; we did not disclose the lower layers of the postulated palimpsest; we did not systematically engage with its Latinity, and -- most significantly for this paper -- we did not address the important issue of its multiple scribal hands. The group has been awarded a major grant (£74k) by the Arts and Humanities Research Board to return to these concerns and to widen the investigation to a larger consideration of the place of the document in radical theology of the early-modern period (MRG-AN1763/APN11001). The debate about SP 9/61 is probably the hottest current controversy in Milton studies, and on its outcome depend not only the determination of the Milton canon but also the validity of interpretative strategies that have been and continue to be used in approaches to his major poetry.   Computer-Assisted Research from Multiple Perspectives Much of the award has been committed to the preparation of electronic texts. We had already an electronic version of the current transcription of the manuscript. To this we are adding electronic versions of the major early-modern Protestant exegetical tracts. (All are in Latin.) These will allow a much more fine-grained reexamination of the stylometry, using much larger samples of situationally analogous controls; they will allow a careful searching for borrowing and analogues in Milton’s text; and they will support the ready comparison of Milton’s use of biblical citations with the practice of significant precursors. The complex relationship between the many variables relating to the diverse physical appearance of the individual pages of the manuscript -- line density, scribes, corrections, page size, watermarks, etc. -- is being investigated using SPSS Data analysis software. Very helpfully, the Public Record Office reprographic department prepared for us a high-resolution imaged version of the document. Most straightforwardly, it allows a research group spread from Edinburgh and Bangor to Dunedin to work on a document lodged in a remote suburb of west London, with for most purposes as much facility as if the document were immediately present. But it also allows the highly vexed question of Milton’s use of amanuenses to be engaged with a new and innovative precision.   Blind Milton and his Scribes Milton was totally blind by about 1651, and for subsequent work he relied wholly on the assistance of amanuenses. SP 9/61, the most considerable extant manuscript of Miltonic provenance, consists primarily of a transcription by the scribe Jeremy Picard of an early draft that is no longer extant. The opening section of that document was copied again by Daniel Skinner, who prepared the manuscript for the press after Milton’s death. The Picard section is evidently a working manuscript, in that sections have been recopied and substituted by Picard, and there are numerous marginal and interlinear additions. These for the most part are in Picard’s hand, though the most thorough account to date has identified another seven amanuenses responsible for multiple changes, and several other hands -- perhaps as many as eleven further scribes. Here our project has several aspects. First, we want to identify how many scribes were actually involved. Second, we want to determine the order of their contribution to the developing document. Finally, we want to associate the hands we have distinguished with hands found in other Miltonic documents and elsewhere, and we should like to identify the scribes wherever possible. The imaged version of the text is immensely useful. Earlier researchers have worked -- necessarily in a somewhat inhibited manner, given its uniqueness and its fragility -- on the document itself or on photographic reproductions of the document. Using advanced image-editing software (Adobe Photoshop) we can freely cut and paste letter forms and word forms closely to compare diverse examples and to form a composite repertoire of each hand. We can with facility juxtapose examples from SP 9/61 with imaged versions of other pertinent manuscripts. Using the electronic transcription of the document, we can identify throughout the manuscript other occurrences of the word and letter forms found in the scribal additions and form judgments about their similarities and differences. This paper will present a number of investigations that are components in this whole process, from a comparison of a single marginal addition with another late manuscript of Miltonic provenance to more complex accounts of the variety of forms to be found within the practice of Picard and the implications of that for understanding the development of the manuscript into its present form.   ",
       "article_title":"Imaging and amanuenses: understanding the Manuscript of De Doctrina Christiana, attributed to John Milton",
       "authors":[
          {
             "given":"Thomas",
             "family":"Corns",
             "affiliation":[
                {
                   "original_name":" University of Wales, Bangor",
                   "normalized_name":"University of Wales",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01se4f844",
                      "GRID":"grid.8155.9"
                   }
                }
             ]
          },
          {
             "given":"Gordon",
             "family":"Campbell",
             "affiliation":[
                {
                   "original_name":" Leicester University",
                   "normalized_name":"University of Leicester",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/04h699437",
                      "GRID":"grid.9918.9"
                   }
                }
             ]
          },
          {
             "given":"John",
             "family":"Hale",
             "affiliation":[
                {
                   "original_name":" University of Otago",
                   "normalized_name":"University of Otago",
                   "country":"New Zealand",
                   "identifiers":{
                      "ror":"https://ror.org/01jmxt844",
                      "GRID":"grid.29980.3a"
                   }
                }
             ]
          },
          {
             "given":"Fiona",
             "family":"Tweedie",
             "affiliation":[
                {
                   "original_name":" University of Edinburgh",
                   "normalized_name":"University of Edinburgh",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01nrxwf90",
                      "GRID":"grid.4305.2"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In recent years the use of large corpora has revolutionised the way we study language. There are now numerous well established corpus projects which have set the standard for future corpus based research. As more and more corpora are developed and technology continues to offer greater and greater scope, the emphasis has shifted from corpus size to establishing norms of good practice. There is also an increasingly critical appreciation of the crucial role played by corpus design. Corpus design can, however, present peculiar problems for particular types of source material, and the development of the Scottish Corpus of Texts and Speech illustrates the problems which may be encountered when dealing with a complicated linguistic situation such as exists in Scotland. The Scottish Corpus of Texts and Speech is the first large-scale corpus project specifically dedicated to the languages of Scotland, and therefore it faces many unanswered questions, such as those outlined below, which will have a direct impact on the corpus design. The project is a joint venture by the Department of English Language and STELLA project at the University of Glasgow, and the Language Technology Group at the University of Edinburgh, and is funded by the Engineering and Physical Sciences Research Council. The project seeks to address the current gap in knowledge about the languages of Scotland by building a publicly available electronic corpus of written and spoken texts mounted on the Internet. The linguistic situation in Scotland is complex, with Scottish English, Scots, Gaelic and numerous non-indigenous community languages all playing a role. However, surprisingly little reliable information is available on a variety of issues such as the survival of Scots, the distinguishing characteristics of Scottish English, the use of non-indigenous languages, or the way they have developed in Scotland. The first phase of the corpus is focusing on the collection of Scots and Scottish English texts. However, the language varieties Scots and Scottish English are themselves difficult to describe, and between these two extremes lie multifarious other language varieties which defy rigid categorisation. Established practice norms to ensure corpus representativeness cannot be easily applied, as these Scottish language varieties have disparate and shifting functional roles. Scottish English is generally accepted in a wider variety of formal contexts than Scots, but Scots has stronger local and community ties which may also exert a pressure. Social class and education also influence when and where each language variety may be used. Indeed the labels 'Scots' and 'Scottish English' are themselves problematic, as written and spoken varieties of Scots and Scottish English are not as closely linked as might be assumed. There are numerous different local varieties, and so there is a strong regional dimension to be considered. Native Scots themselves often disagree about what is and is not 'Scots', before they even reach considerations of where its use is and is not considered to be appropriate. The perceived status of Scots thus has important implications for the text types and modes in which it is used. To date there has been no large scale study to identify where each of these language varieties is deemed acceptable usage by native Scots. Indeed, the native Scots themselves have ambivalent and wide-ranging opinions on these language varieties, and there are unspoken but nevertheless tangible rules which impact on where and how and when they are used. Present-day Scots also has no agreed standard spelling system, which presents problems when developing search tools for the corpus. A balanced corpus which seeks to reflect the true linguistic situation in Scotland must be sensitive to these problems and anomalies. It must reflect the variety and breadth of possible linguistic options without skewing the data along preconceived notions of what is and is not Scots or Scottish English. It must also gather its texts from a discourse community which has very ambivalent views about the range of language varieties it encompasses. This paper considers the problems presented for corpus design in view of the complex linguistic situation that exists in Scotland. It considers questions such as how to decide what should be included, how to choose, and in what proportions relative to the corpus as a whole and to the range of possible language varieties. It examines the problematic issue of how to construct a well balanced and representative corpus in what is largely uncharted linguistic territory. The paper will also consider points of comparison with other corpora.  ",
       "article_title":"The Scottish Corpus of Texts and Speech: problems of corpus design",
       "authors":[
          {
             "given":"Fiona",
             "family":"Douglas",
             "affiliation":[
                {
                   "original_name":" University of Glasgow",
                   "normalized_name":"University of Glasgow",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/00vtgdb53",
                      "GRID":"grid.8756.c"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   In this paper we will present a lexicographic database tool called The Meta Dictionary. It is designed for systematising word and phrase information from field datasets for weakly normalised languages and dialects. The Meta Dictionary has been developed to function as a pivot in the revitalisation of a large traditional national dictionary project in Norway. In addition to completing the traditional dictionary manuscript, this revitalisation project has the objective of creating a modern lexical database in a way similar to what is now being done for The Oxford English Dictionary [10] or the The Cobuild Word Bank [4]. The Norwegian dictionary project was started in 1930, and the editors are now at the end of the letter H. According to the new plan this will be finished in 2014. The Meta Dictionary system is meant to be a general tool for working with lexicographic raw material and is not limited to the Norwegian dictionary project. The plan is to use it in a project on the minority languages in Zimbabwe in (2002-2205). However, it can also be adapted to implement semantical database based on different ontologies (semantic models) like the one used in WordNet [12] or in The Simple Project [11 ]. he Meta Dictionary and the other lexicographical tools described in this paper are developed by The Museum Project (MusPro). The MusPro was established in the spring of 1998 as a national collaborative project involving all four Norwegian universities. The objective is to develop common database systems for the meta data and the management of collections, accessible for all Norwegian University museums. The project organisation has taken over the responsibility for maintaining and developing the database systems for the department of lexicography (old Norse and modern Norwegian) and place name studies, and is a direct continuation of the systems development group in The Documentation Project a major digitalisation project in the 1990-ies.   Lexicographical background - the hetrogenousity of a language In Norway several linguistically oriented research groups participated in the development of electronic corpora from the beginning of the 1970's, (LOB, ICAME corpus projects, see [6]), but for unknown reasons the development of corpora consisting of Norwegian texts did not start before the end of the 1990's. As a parallel - there is still no complete and comprehensive national dictionary for Norwegian. Both cases have connections to the fluctuating language situation in Norway and the two closely related written Norweigan languages. On a world basis two written languages in a country is neither unusual nor impressive. What makes the Norwegian case unusual is the fact that two languages are completely comprehensible for all Norwegian speaking persons, that there are no clear- cut boundaries between the two, a rich flora of accepted alternative spellings and inflection schemas and finally frequent spelling reforms (although the period between these is increased to 4 years). The two written standards derived from written Danish (the only official written language in Norway from the sixteenth century until 1870) and on a synthesis of the dialects in the western parts of Norway. The latter was created by the linguist Ivar Aasen (see Norwegian Dictionary, 1872 [7]). During the last 130 years the standards have gradually become much closer. However, in non-official written communication many archaic forms are still in use. In private written communication people often mix the two standards. The language situation introduces many problems for the language technology industry and makes good solutions complex and expensive to develop. However, the relatively weak normalisation and the focus on dialects have a democratic effect, e.g. it is possible to use a written language pretty close to one's own spoken language. In most European countries normally thought of as a homogeneous language areas with a single official written language, there are in fact many distinct dialects or even related languages. The German language area (Germany, Austria and parts of Switzerland) is one example. Thus it can be argued that the language situation we find in Norway is the norm. However, the Norwegian language policy is more complex than may be expected. In recent years initiatives have been taken to at least complete the national dictionaries in Norway. It now looks as if there will be two printed dictionaries in respectively 7 and 12 volumes. The former will concentrate on the originally Danish based written language (used by most of the population) and will be based on an already existing dictionary in six volumes. The latter will describe the second written language and the Norwegian dialects although this is not planned as a dialect dictionary in the purest sense of the term. The work on this dictionary has already been running for 60 years. The editors are now working at the end of the letter H and at the current pace the dictionary will be finished in 2040. The Ministry of Culture has, pending the reorganisation of the team, and its completion by the year 2014, rather generously refunded the work. The new project is called Norwegian Dictionary 2014 (ND2014). From 2002 the editorial team will be increased from 6 to 28 full time editors. The main focus is on a method and tool for systematising source material and preparing the semantic skeletons of the entries. To explain the idea and the specific use of The Meta Dictionary we will briefly discuss our interdisciplinary database framework and the heterogeneous lexicographical source material.   Creating a multidisciplinary object oriented, database framework To make database systems for the large number of disciplines is a challenge for a small group. An additional challenge is the requirement of providing for interdisciplinary searches. The number of databases and the aspect of inter-disciplinarity has forced us to try to make as generic database solutions as possible. The design and implementation of the common systems and interfaces are now completed in the first version. The new information system replaces older, mostly independent database applications. This is fortunate since it is easier to create new interconnecting systems instead of connecting old ones, although technologies like the Z39.50 standard have opened for the relatively easy interconnection of databases. The system group attempts to think generically along two axes:  Common interface tools and database functionality Common database solutions for common object types like geographical data, bibliographical data, data concerning individuals and legal bodies, classification systems in cultural and natural history, dictionary entries and so on.  The databases are event oriented. The information is considered to be the result of observations and all major changes result from a well-defined event. The event -based object oriented model is also used in the tools now designed for the dictionary project ND2014. The system consists of three parts: 1) The databases containing the background material, that is, databases containing the old slip collections, dictionary manuscripts, bibliography etc., editors notes and the corpora of older literary text and modern Norwegian. 2) The dictionary editing system 3) the backbone of the system, The Meta Dictionary itself.   The Lexicographic source material In the late eighteenth century, paper slip collections were introduced in dictionary making. A single word form with an example of actual usage was written on each paper slip. The slip collections were stored in alphabetic order in large filing cabinets. This was a major achievement and introduced scientific methods into lexicography. From the end of the 18th century most large-scale dictionary projects, e.g. The Oxford English Dictionary (OED, first edition completed in 1930), have been based on slip collections such as this one. At the outset of 1960 one started to use computers to create concordances from electronically stored texts and the use of slip collections was regarded increasingly as being old fashioned. Over the last 10 years the number of corpora has exploded: there are now text corpora for large number of languages and social groups. It is almost impossible to imagine that someone should start building a paper slip collection today. However, many of the large dictionary projects initiated in the first half of the 20th century and even earlier are not completed and still rely on their slip collections (The Afrikaans Dictionary, WAT, The Dictionary of the Swedish Academy, Dutch national dictionary, to mention a few). The two major Norwegian dictionaries were planned in the tradition of the large historically oriented, national dictionaries like OED and during the years slips were collected (numbering more than 6 millions today). In the 1990 most of the old collections were made electronically available either as indexed electronic facsimiles (3.2 millions) or replaced by collections of SGML tagged electronic texts. A series of older dictionaries and historical word lists from the end of the seventeenth century and onwards was also made electronically. This was done by the forerunner of The Museum Project, The Documentation Project, and has been described at ALLC/ACH 1995 and 1996 ([5][9]).   The editing system A standard electronic editing system for a dictionary manuscript is based on a database in which the entries and parts of the entries are stored. Each entry can be stored in one or many tables, e.g. separate tables for the headword parts, the definitions and the citations, or the entire manuscript can be stored in a XML based free text system. A typical editing system usually supports cross-referencing, searching and the production of camera-ready page lay out. MusPro has made several such freestanding tools for a national language project in Zimbabwe (Shona and Ndbele)[2] and for one volumes dictionary in Norwegian. The latter are both free standing and connected to the common framework and the Meta Dictionary.   The Meta Dictionary The source databases/datasets (described above) contains samples from different dialects and times. The most important lexicographical task is to systematise the material into groups, each describing a descriptive word. This process is called the lemma/headword selection. For strongly normalised written languages like Standard English this is normally not considered a difficult task. For field lexicographers working on not so normalised languages, this is well known as difficult task. The question \"what is a word \"and how to select the right candidate among a set of variants is not a trivial task. For most existing languages the standardisation has been mostly based on socio-political circumstances. The Meta Dictionary system has no features for (semi)automatic lema selection but is designed to assist the lexicographers in the process, to make it easier for others to check the lexicographers' decisions and open for more than one normalisation based on the same material. In this process we have tried to exploit the interdisciplinary nature of our organisation to make a tool for preparing the entries in the Norwegian Dictionary to create a general tool for systematising information from weakly normalised languages and /or dialect material into a lexical database.  The Meta Dictionary is based on the general system for the users' electronic notebooks or folders used in all our database-applications. In general all our users can store parts of the result from a query for later use in their private notebooks in the system. The notebooks are multi-typed and can store information of all the object types in the databases. Traditionally, a working lexicographer usually sorts the available material first under each headword and then according to the meanings and use before formulating the definitions. In The Meta Dictionary an entry is a folder containing samples use taken from the source databases and corpus system connected to it. The user uses the drag and drop mechanism to move pieces of source information into the folder. Each entry/folder is labelled by the normalised word(s) and word class information and information about the actual orthographically standard used. The database used by the editorial team of the Collins Cobuild dictionary in the lemma selection of from the corpus is an early example of such a database. Several standards can be used simultaneously in The Meta Dictionary. This is useful in the Norwegian context. One possible application could for example be a Meta Dictionary for German dialects having a Low German and a High German normalisation. A more realistic application of The Meta Dictionary system is to use it as a descriptive tool for old written languages like Old Norse. It is planned to be used in systematising the data from the fieldwork on minority languages in Zimbabwe in the phase 3 of the The ALLEX Project (2002-2005) [2]. In the Norwegian dictionary (ND2014) project The Meta Dictionary system has been used during the last 2 years and more than 150 000 entries out of a total estimated to 450 000 have been created. The Meta Dictionary can be seen as an amalgamation of a lexical database, an electronic slip collection, a set of older dictionaries and a editing tool or database with a growing manuscript for ND2014. The backbone of The Meta Dictionary is the list of all the headwords from A to Å (the last letter in the Norwegian alphabet). The other information is linked to this list. The source materials in the entries/folders can be view in their original layout. That is, all the samples can be displayed \"inline\" as they are in their separate databases.   Connecting the editing of the dictionary and The Meta Dictionary The basic Meta Dictionary is a tool for group the background information (samples) to (head)words. In a (monolingual) dictionary the defining parts of the entries usually are divided in a tree like structure reflecting the semantics of the words. Thus The Meta Dictionary used in the NO2014-project is linked to the editing system by a drag and drop based interface for sorting the information in The Meta Dictionary entry/folder into a tree-like hierarchy which in turn can be view by the editor in the editing system as the skeleton of an ordinary entry in ND2014. By these means it is possible to survey back and forth between the collected source material and the dictionary manuscript. These links will be kept after the editing of an entry is finished. Furthermore, the entry for a given word will automatically be placed in The Meta Dictionary entry/folder. The NO2014-entry will \"live\" together with its sources and in fact be just another information piece describing that word. In a corresponding way it is relatively easy to connect/extend The Meta Dictionary to WORDNET-like descriptions or any other ontology-based semantic structures. The users of The Meta Dictionary connected to ND2014 do not need to have a ND2014 perspective on the word information, but may freely chose their focus. The information given in ND2014 is simply one out of many seen in a time perspective. We believe that the The Meta Dictionary also is well suited to create a general tool for systematising information from weakly normalised languages and /or dialect material into a lexical database.   ",
       "article_title":"The Meta Dictionary",
       "authors":[
          {
             "given":"Christian-Emil",
             "family":"Ore",
             "affiliation":[
                {
                   "original_name":" University of Oslo",
                   "normalized_name":"University of Oslo",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/01xtthb56",
                      "GRID":"grid.5510.1"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   1. Introduction A current trend in humanities computing is the explosion of digital resources and tools. It is clear that the new electronic media in conjunction with distribution via the World-Wide Web offer a degree of access to resources that is unparalleled in history. But there is a gap between what users want and what they can achieve today. For instance, potential users cannot necessarily find the material they are interested in, different data providers use different formats and conventions, the average researcher has no idea how to prepare materials for publication via this medium, to name just a few of the problems. A new direction for humanities computing would be for the community to organize its efforts so as to bridge this gap. This paper describes what one subcommunity, namely, those working with language-related resources, is doing in pursuit of this goal. The Open Language Archives Community was founded in December of 2000 with the following purpose statement: OLAC, the Open Language Archives Community, is an international partnership of institutions and individuals who are creating a worldwide virtual library of language resources by: (1) developing consensus on best current practice for the digital archiving of language resources, and (2) developing a network of interoperating repositories and services for housing and accessing such resources.  This community involves both people and machines in cooperation. This paper describes the infrastructure that has been developed in order to support this cooperation. There are three aspects of infrastructure which are explained in the remaining sections: a technical infrastructure that defines how participating machines interact with other participating machines, a governance infrastructure that defines how participating people interact with other participating people, and a usage infrastructure that defines how participating people interact with participating machines.    2. Technical infrastructure The technical infrastructure for OLAC is built on an infrastructure developed within the digital libraries community by the Open Archives Initiative [OAI][2]. This infrastructure has two components: a metadata standard [OAI-DC] [3]and a metadata harvesting protocol [OAI-MHP] [4]. The OLAC versions of these are specializations that address the particular needs of language archiving. The two components of technical infrastructure are: The OLAC Metadata Set [ OLAC-MS] [6] defines the elements to be used in metadata descriptions of archive holdings and how such descriptions are to be represented in XML for publication to the community. The OLAC metadata set contains the 15 elements of the Dublin Core metadata set [DC-MS] [1], plus 8 refined elements that capture information of special interest to the language resources community. In order to improve recall and precision when searching for resources, the standard also defines a number of controlled vocabularies for descriptor terms. The most important of these is a standard for identifying languages [ Simons, 2000] [8]. The OLAC Metadata Harvesting Protocol [ OLAC-MHP] [5] defines the protocol by which machines that are \"service providers\" query the machines that are \"data providers\" in order to harvest the metadata records they publish. In this model, every participating archive is a data provider. Any other site may use the protocol to collect metadata records in order to provide a service, such as offering a union catalog of all archives or a specialized search service pertaining to a particular topic. The protocol is based on HTTP. Requests to data providers are issued via URLs with parameters; responses are returned as XML documents.     3. Governance infrastructure The infrastructure that supports the interaction among the human participants of the Open Language Archives Community is defined in the OLAC process document [ OLAC-Process] [7]. It defines three things: The governing ideas of OLAC. These are defined through a summary statement of its purpose, vision, and core values. The organization of OLAC. This is defined in terms of the groups of participants that play key roles: advisory board, participating archives and services, prospective participants, working groups, participating individuals, and coordinators. The operation of OLAC. It is through documents (such as standards and best practice recommendations) that OLAC defines itself and the practices that it promotes. The document process defines how documents are generated and how they progress from one status to the next along the five-phase life cycle of development, proposal, testing, adoption, and retirement.    4. Usage infrastructure The infrastructure that has been built to allow the community of users interested in language resources to interact with the machines that provide services for this community has four major components: A community gateway (hosted by the Linguistic Data Consortium at ) provides access to all aspects of the community: news, documents, a directory of participants, links to service providers, resources for implementing data providers, and so on. A union catalog (to be hosted by Linguist List at ) of all records harvested from all OLAC data providers along with a full search engine. A language identification server ( hosted by SIL International at ) that documents the standard OLAC follows in identifying the 6,800+ living languages of the world. A proxied data provider (also hosted by the Linguistic Data Consortium) that gives individual researchers and small institutions that do not have the capacity to implement their own data provider an alternative way to publish their data and metadata as an OLAC archive.    5. Conclusion During its first year of operation, 2001, the basic infrastructure for OLAC has been developed. By the end of that year, twelve institutions have become full participants and implemented data providers that publish a total of 18,000 metadata records. During the second year, 2002, the focus will be on enlarging the community of participating archives. The technical standards will be frozen in candidate status during that year so that archives need not worry about a moving target as they implement an OLAC data provider. Based on the experiences of the archives that participate in the first two years, the standards will be refined and formally adopted by the community during the third year, 2003. All individuals and institutions who have language-related resources to share are enthusiastically invited to take part in this new direction for humanities computing to build a distributed virtual library of digital resources for the documentation and study of human languages.   ",
       "article_title":"The Open Language Archives Community: An Infrastructure for Distributed Archiving of Language Resources",
       "authors":[
          {
             "given":"Gary",
             "family":"Simons",
             "affiliation":[
                {
                   "original_name":" SIL International ",
                   "normalized_name":"SIL International",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00d57b563",
                      "GRID":"grid.481394.3"
                   }
                }
             ]
          },
          {
             "given":"Steven",
             "family":"Bird",
             "affiliation":[
                {
                   "original_name":" University of Pennsylvania",
                   "normalized_name":"University of Pennsylvania",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00b30xv10",
                      "GRID":"grid.25879.31"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Abstract: The aim of the talk is to demonstrate how useful humanities computing can prove to be in solving issues of a seemingly as distant field as forensic science. We will present a case study of an actual forensic linguistic assignment whose aim it was to determine if a digitalized recording of a conversation had been tempered. The task, highly challenging due to its novelty both in applied linguistics and forensic practice, was carried out by investigating three independent aspects of the issue: those of experimental phonetics, situational semantics and computation. The results of the three approaches were synthesized to give a comprehensive basis for an answer to the initial question.   Introduction: To find a proof for or against the assumption that a given document has been tempered with is one of the important tasks of forensic science. With the introduction of various voice-recording techniques it became especially important to decide whether or not such a recording can authentically represent a certain event of reference. The case of magnetic tape recordings is relatively simple, since any (either electronic or mechanical) modification of the tape leaves a trace behind, characteristic even of the kind of manipulation (cf. Gruber et al. 1995, Gruber et al. 1993, Poza 1979). However, with digital recordings gaining more and more popularity in our days, on the one hand, and digital manipulation becoming technically possible and easy, on the other, one might believe that the discovery of such a manipulation is highly unlikely. The novelty of the issue in scientific literature just adds to this challenge. In order to carry out the task, we had two assumptions: a. due to human voice having a highly complex structure, even digital tempering by a person might leave a significant trace, and b. due to conversations also having their strict internal structure, the removal of a segment of the given conversation might also be noticeable.   Discussion: The findings of the three approaches were as follows:  1. A detailed spectrographic analysis of digitalized voice recordings found a characteristic pattern with a duration of 8-10 milliseconds at the place where a segment had been digitally removed. Although this pattern varied according to the actual immediate environment, its characteristic features could be established (a symmetrical increase of intensity as well as a 500 Hz increase of frequency). This spectrographic pattern was only manifested in the exact location of recordings with a digital cut. Applying the methodology to the given task, we found that no spectrographic trace of this kind of manipulation could be identified. 2. The aim of the situational-semantic study was to find out if an eventual cutting of a portion of the text could have possibly left a trace that can be identified as semantically significant. Our attention was directed to the analysis of the appropriateness of pieces of linguistic material with a referential function, including pronouns, determiners and names (cf. Brown et al. 1983, Kamp 1981). The analysis pointed at some places where the reference was not unambiguously computable from the immediate linguistic environment, but since in running conversation such immediate turns often happen, they could not be decided on the basis of linguistic content alone. These locations became thus subject to spectrographic analysis, and the latter concluded that no tempering could be identified there. 3. In our work we included a separate computational task to find out if segments of an eventual cut in the voice recording were still recoverable from the hard disk originally used for digitizing. Since it turned out that the hard disk had been completely reformatted, we could not complete this task. However, we elaborated a methodology for similar tasks and applied it in model situations. The applied statistical method of zero crossing proved to yield significant interpretable results in the differentiation of headerless segments of certain types of files. This method showed a significant difference between .bmp and .txt files, and .wav files also had a characteristic value for zero-crossing. Thus, we suggest that this methodology may be useful for the possible differentiation of at least a few types of files in future work.    Summary: This case study showed us that humanities computing can have a significant contribution to forensic science, especially in the form of a combination of theoretical and applied linguistics as well as statistics and computing. This assignment was a real challenge for us, and it resulted in the elaboration of a new methodology both in experimental phonetics and computation. Thus proving the inspiring force of humanities computing across various fields of science.   ",
       "article_title":"Forensic linguistics: the contribution of humanities computing",
       "authors":[
          {
             "given":"László",
             "family":"Hunyadi",
             "affiliation":[
                {
                   "original_name":" University of Debrecen",
                   "normalized_name":"University of Debrecen",
                   "country":"Hungary",
                   "identifiers":{
                      "ror":"https://ror.org/02xf66n48",
                      "GRID":"grid.7122.6"
                   }
                }
             ]
          },
          {
             "given":"Enikő",
             "family":"Tóth",
             "affiliation":[
                {
                   "original_name":" University of Debrecen",
                   "normalized_name":"University of Debrecen",
                   "country":"Hungary",
                   "identifiers":{
                      "ror":"https://ror.org/02xf66n48",
                      "GRID":"grid.7122.6"
                   }
                }
             ]
          },
          {
             "given":"Kálmán",
             "family":"Abari",
             "affiliation":[
                {
                   "original_name":" University of Debrecen",
                   "normalized_name":"University of Debrecen",
                   "country":"Hungary",
                   "identifiers":{
                      "ror":"https://ror.org/02xf66n48",
                      "GRID":"grid.7122.6"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   The structure of the frequency of occurrence of consonants in the speech sound chain can be a good indicator of the typological closeness of languages from the phonetic point of view. A human being can realise that this or that language sounds closer to his own native language without understanding the meaning. At the stage when it is hard to teach computer to understand a human language, it is quite possible to make it recognise the sound closeness of a language to this or that language on the basis of the analysis of its sound speech chain. We have computed the frequency of phonemic occurrence of 112 world languages as a teaching sample for the computer. Then we took Japanese as a token language. The computer had to analyse the sound chain of a language and then to put it closer to some languages and far away from the others, basing on the frequency of occurrence of certain consonantal groups. We have chosen Japanese because it is not categorically assigned to any language family. It is still considered a genetically isolated language. Therefore, it is useful to have some additional information about it in any aspect. In this project we have used the procedures which are usually used in pattern recognition. Japanese, as any other human language, has a specific structure of the speech sound chain. It can be distinguished by its structure from any other language. Every language has a unique structure of distributions of speech sounds in its phonemic chain. The distribution of Japanese vowels will not be considered till the second stage of the investigation. Let's point out that consonants bear the semantic load in the word, not vowels. Therefore, it is more possible to understand the meaning of the message by consonants, rather by vowels. However, if we fail to recognise and distinguish two languages, then we resort to the structure of occurrence of vowels in the speech sound chain. While comparing languages, it is necessary to keep to the principle of commensurability. Having it in mind, it is not possible to compare languages on the basis of the frequency of occurrence of separate phonemes, because the sets of phonemes in languages are usually different. The articulartory features may serve as the basic features in phono-typological reasoning. First of all, it is the classification of consonants according to the work of the active organ of speech or place of articulation (4 features). Secondly, it is the classification from the point of view of the manner of articulation or the type of the obstruction (3 features). Thirdly, it is the classification according to the work of the vocal cords (1 feature). In this way, 8 basic features are obtained: 1) labial; 2) front; 3) mediolingual or palatal; 4) back or velar; 5) sonorant; 6) occlusive; 7) fricative; and 8) voiced consonants. One should take the values of the frequency of occurrence of these 8 features in the speech chain of Japanese and compare them to those of the other languages. On the basis of the \"chi-square\" test and Euclidean distance, we have developed our own method of measuring the phono-typological distances between languages (Tambovtsev, 1994-a; 1994-b; 2001-a; 2001-b). It takes into account the frequency of occurrence of the 8 consonantal groups mentioned above and builds up the overwhelming mosaic of the language sound picture. Having compared Japanese to some languages, we received the following phono-typological distances: Japanese - Ujgur (6.77); Japanese - Nanaj (8.12); Japanese - Jakut (8.26); Japanese - See Dajak (8.86); Japanese - Kazah (9.02); Japanese - Turkish (9.05); Japanese - Ket (9.52); Japanese - Baraba Tatar (9.76); Japanese - Uzbek (10.63); Japanese - Hausa (10.98); Japanese - Georgean (11.05); Japanese - Kazan Tatar (11.07) and so on. One can see, that Ujgur, Jakut, Kazah, Turkish, Baraba Tatar, Uzbek and Kazan Tatar are Turkic languages. Nanaj is a Tungus-Manchurian language. Therefore, one can notice that Japanese is closer to the so-called Altaic languages which include Turkic, Mongolian and Tungus-Manchurian languages. All in all 112 languages were compared to Japanese. We can't show all the distances measured here for the lack of space. However, the maximum distances were found for Japanese - German (22,24); Japanese - English (19.83); Japanese - Rumanian (15,08) and Japanese - Swedish (17.03). Thus, one can see that the consonantal distribution pattern in Japanese and Germanic languages is rather different. As a conclusion, we can state that speech sound picture of Japanese is also far away from the languages which are geographically close: Chinese, Nivh, Itelmen or Indonesian. It was a surprise to us. Our data state that the speech sound pattern of Japanese resembles that of Ujgur - one of the Turkic languages spoken in the Middle Asia. The Ujgur people are often linked to the Old Turkic tribes, who used to live in the stepps of Southern Russia before the Tatar-Mongols captured them in the IXth century A.D. We must point out that it is not a coincidence since the other native Altaic people have a very similar data of closeness to Japanese. Turkic and Tungus-Manchurian tribes may have had a sort of common origin with Japanese. It may verify the Altaic hypothesis of Japanese origin. It is especially vivid, when the Austro-Oceanic and other languages do not show such a closeness.  ",
       "article_title":"Structure of the Frequency of Occurrence of Consonants in the Speech Sound Chain as an Indicator of the Phono-Typological Closeness of Languages",
       "authors":[
          {
             "given":"Yuri",
             "family":"Tambovtsev",
             "affiliation":[
                {
                   "original_name":" Novosibirsk University",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction The problem of wordclass tagging nowdays gains more and more importancecf. Hans van Halteren, Syntactic Wordclass Tagging, Dordecht / Boston / London 1999. It is required for syntactic analyses in dialog systems, for prosody prediction in speech synthesis and for syntactic annotations of large text corpora. There seems to be a need for systems which can be implemented as modules in other NLP-systems. The system presented here is based on the system LEMMA2e.g. Gerd Willée (1982): Das Programmsystem LEMMA2 - eine Weiterentwicklung von LEMMA. IKP-Arbeitsbericht (Abt. LDV) Nr. 2, Bonn 1982 (als Manuskript veröffentlicht and Gerd Willée (1987): Lemmatisierungsprobleme am Beispiel des Deutschen. Vortrag, gehalten auf dem 35. Kolloquium über die Anwendung der EDV in den Geisteswissenschaften in Tübingen written in PL/1 for the use on main frames. Even if the results and the speed of LEMMA2 were fairly good in those times, because of the meanwhile mostly outdated programming language PL/I we decided to rebuild the system and to redesign it at a large scale. This redesign of the LEMMA2 algorithm resulted in a much faster system, which can be used in speech synthesis systems at nearly real time, with a very efficient deflection-modul using an extendable deflection list, a disambiguation facility for homographs within the closed word classes, and a mostly stringent separation of the data and the algorithms. C++ was chosen as programming language because of its easy portability to many other platforms.   Functionality LEMMA3 is a word class tagger and lemmatizer of unrestricted German text, i.e., to each text word form the (or one possible) word class indication, the base form, and - for the simple tenses of the verbs - the indication of the inflection is added. he program processes text in three steps: Dictionary Lookup: Every text word form is compared with a dictionary containing the -- at this stage still partly homographic - members of the closed word classes Deflection: Word forms belonging to the open, inflected word classes are processed by the modul MORPH Disambiguation: The homographs -- the ones from the closed word classes and the not yet completely resolved inflection indications of verb forms -- are disambiguated by the modul UMGEBUNG  LEMMA3 detects the following word classesThe system of word classes used in LEMMA3 corresponds to the classic system, completed by the (quasi-)word class 'Verbzusatz' (under special conditions separable prefixes of German verbs, e.g. 'um-', 'ab-', 'wieder-', cf. 'er kommt wieder' vs. 'wiederkommend'). verb verbzusatzsee the endnote above noun adjective adverb pronoun (incl. article) conjunction preposition postposition numeral interjection     Structure The algorithm is mainly word form based, execpt for the module UMGEBUNG, which uses parts of the sentence context in order to disambiguate homographs. The first step after the tokenization is the dictionary look-up. The dictionary used is a full form dictionary containing the elements of the closed word classes. The information assigned to a match consists of the appropriate base form and word class indication, the latter possibly if necessary a homograph class, which will be disambiguated later on. Each dictionary entry has the following structure:keyword -- base form - <word class> Some sample entries from the dictionary: denn denn <aK>  in in <PP> seitlich seitlich <p9> zu zu <PX> zugute zugute <VZ>  The homograph class 'aK' (homograph between a conjunction and an adverb) is assigned to 'denn' and is to be disambiguated in UMGEBUNG. The wordclass 'PP' (preposition) is assigned to 'in'. The homograph class 'p8' (homograph between an adverb and a preposition) is assigned to 'seitlich' and is to be disambiguated in UMGEBUNG. The homograph class 'PX' (homograph between a verbzusatz and a preposition) is assigned to 'zu' and is to be disambiguated in UMGEBUNG. The wordclass 'VZ' (verbzusatz) is assigned to 'zugute'. The deflection for verbs, adjectives, and nouns is made by the modul MORPH, i.e. it determines the word class and generates the proper base form to a given word form. This is done by the aid of the deflection list, which contains word stems or parts of them together with the allowed endings for a given base form and its word class. The latter information can be taken only, if the tested word form ends in a valid combination of stem and ending. For verb forms the (at this point often still provisional) inflection codes are supplied as well. The entries for nouns can be searched only, if a word form has an initial capital. The distinction between adjectives and verbs (e.g. 'lieb' vs. 'lieben') can only be done, if the endings are different; otherwose this distinction has to be done by the modul UMGEBUNG. The structure of the deflection list is as follows: keystring (stem) -- word class -- base form -- list of endings ('$' means 'no ending'), the endings for verb forms include numeric codes for the inflection charcteristics. Some sample entries from the morphlis-dataset: em su em en, e, s, $, einzig ad einzig e, em, en, es, er, $, geh v gehen e,1 $,11 st,3 est,12 (...) lieb ad lieb (...) es, er, em, $, lieb v lieben (...) tet,20 st,3 t,5 (...) liebe su liebe $,  If the final string of the courrent word form matches a list entry concatenated with one of the allowed endings given, then the word class is taken from the entry (in case of nouns after a successful check of an initial capital of the word form), the base form of the current word is formed by subsituting the matching string by the given base form, in case of verbs the appropriate inflection codes are evaluated.  word form in question: 'Systems'  word class added: 'su' (noun)  generated base form: 'System'  word form in question: 'einziger'  word class added: 'ad' (adjective)  generated base form: 'einzig'  word form in question: 'vergehst'  word class added: 'v' (verb)  generated base form: 'vergehen'  indication of inflection: 'G2' (see list below)  word form in question: 'liebem'  word class added: 'ad'  generated base form: 'lieb'  word form in question: 'verliebtet'   word class added: 'v'  generated base form: 'verlieben'  indication of inflection: 'V5' (see list below)    word form in question: 'Liebe'  word class added: 'su'  generated base form: 'Liebe'  Explanation of some inflection codes for verbs: 1 G0#: provisinal, homograph between 1.sg.pres.ind.act. and 3.sg.pres.conj.act.  3 G2: 2.sg.pres.ind.act.  11 IS: imp.sg.  12 K2: 2.sg.conj.[I+II]act.  20 V5: 2.pl.pret.ind.act.  The modul UMGEBUNG is the final component of LEMMA3 operating on whole sentences. In a first step all word class assignements not yet done by the modul MORPH are added by rules, according to the word classes already determined for the surrounding word forms. E.g., if an undetermined word form ending in '-e' has a pronoun as predecessor and is followed by a noun, then the word class is 'adjective', the base form is generated by reducing the final '-e' from the word form. if the predecessor is the word 'ich', then the word class is verb, the base form is generated by adding a final '-n' and the inflection is determined as 1.sg.pres.ind.act. As example for the disambiguation of the homographs found in the dictionary follows the rule for the homograph class 'p9'(cf. the example form the dictionary): 'If the word following the word form in question is a pronoun, an article or a preposition, then it is a preposition; else it is an adverb.'  The not yet completely resolved indications of the verbal inflection forms are disambiguated by context rules, which use the fact, that the 1. and 2. person of verbforms always must be accompanied by the corresponding personal pronoun. e.g.: The provisional indicator 'G0^#' stands for present stems ending in '-e'. If an 'ich' is found in a defined tontext of the verb, 1.sg.pres.ind.act. is assumed, otherwise 3.sg.pres.conj.act.    Limitations The focus for the new conception of LEMMA3 had been laid on a clear manageable algorithm, the analysis data of which easily can be extended, more than on a system as linguistically sophisticated as possible. This is reflected in the choice of the word classes and in the algorithm in general. Some incorrect word class tages are produced systematically: Word forms with an initial capital, which are not nouns, (at the beginning of a sentence), but which resemble nouns, are treated as nouns. Without further semantic and contextual information it is not possible to distinguish between word forms like 'tränke' (conjunctive II form from the verb 'trinken') and 'tränke' (present tense form from the verb 'tränken') on the base of the pure word form given. In these cases one definite choice had to be made in favour of the (probabely) more frequent occurrence. Word sequences like 'hin- und hergehen' cannot be processed correctly by LEMMA3. Forms of the compound tenses (like 'ich werde gehen') can be analysed only as seperated parts ('werde' and 'gehen'). The default rule for word forms with no other classification normally determines the word class 'noun', which sometimes may be wrong, especially if the word form has no initial capital.   Evaluation The evaluation still is in progress. First estimates on a medium sized test set show that - as for the word classes - LEMMA3 tags more than 95% correctly. A more detailed evaluation will be presented at the conference.",
       "article_title":" LEMMA3 -- a Primarily Wordform Based Wordclass Tagger and Lemmatizer for Unrestricted German Texts",
       "authors":[
          {
             "given":"Gerd",
             "family":"Willée",
             "affiliation":[
                {
                   "original_name":" IKP - University of Bonn",
                   "normalized_name":"University of Bonn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/041nas322",
                      "GRID":"grid.10388.32"
                   }
                }
             ]
          },
          {
             "given":"Karlheinz",
             "family":"Stöber",
             "affiliation":[
                {
                   "original_name":" IKP - University of Bonn",
                   "normalized_name":"University of Bonn",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/041nas322",
                      "GRID":"grid.10388.32"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Walt Whitman's manuscripts as a whole are poorly understood and, curiously, the most important group of them, the poetry manuscripts, have never been collected and edited, despite his foundational role in American culture. Whitman's poetry manuscripts are found in approximately thirty different repositories, and the quality and depth of description varies across these repositories. Because the materials are dispersed and irregularly documented, scholars interested in the development of Whitman's poetry-through multiple drafts to finished work-cannot locate and examine the relevant documents without great expense of time and money. Our paper reports on our early successes and ongoing efforts to create a unified, item-level guide to all of Whitman's poetry manuscripts held in the US and the UK. Our work is an unprecedented use of Encoded Archival Description (EAD), a standard supported by the Society of American Archivists and the Library of Congress. The depth of our item-level description and our ability to pull together disparate collections to create a single virtual finding aid make this project distinctive. This guide, or Virtual Archive, will soon be incorporated into, and will be maintained by, the Walt Whitman Archive (), an NEH-supported project organized and affiliated with the Institute for Advanced Technology at the University of Virginia (IATH). Thanks to a grant of $10,000 given to the Whitman project from the Gladys Krieble Delmas Foundation, we have established the framework for this project at the University of Nebraska-Lincoln. The Delmas Foundation funding has supported the development of an encoded finding aid to poetry manuscripts in the Charles Feinberg Collection of Walt Whitman at the Library of Congress. The next steps for our project will be, first, to link and coordinate our EAD finding aid to ones developed by the New York Public Library and Duke University, and, second, to collect, organize, and present records from various smaller institutions that currently have only paper-based or HTML records of their Whitman holdings. The Whitman Virtual Archive is conceived as an attempt to provide universal access from the collection to the item level for all Whitman manuscript resources distributed in over sixty repositories. It will provide a centralized online finding aid that will pull together and regularize information that now exists in these different institutions in partial and inconsistent forms, and it will provide a search interface adapted to the particular nature of the Whitman manuscripts and the special needs of their users. This work is crucial because Whitman frequently left his manuscripts untitled, and when he did title them, he often used a title different from that employed in any of the six distinct editions of Leaves of Grass. It is thus difficult for anyone but a specialist to correctly identify and categorize Whitman's manuscripts, a difficulty compounded by the fact that Whitman's poems sometimes began as prose jottings and only gradually evolved into verse. In such cases, specialists can help identify manuscripts that are in fact the working papers contributing to poems. That identification can, in turn, be encoded into the EAD finding aid, enriching access and understanding for a wide network of archivists, scholars, and students. Our project builds upon work done by the American Heritage Virtual Archive and the Research Libraries Group. One of the goals of the American Heritage Virtual Archive (funded by NEH) was to integrate \"collections that have been dispersed among two or more institutions (such as the Mark Twain collections at Virginia and Berkeley)\" and to experiment \"with cooperatively creating a single finding aid, in which separate components are used to describe each of the separate collections held at separate repositories\" (). However, because of its broad scope-American heritage materials of all sorts-the goal of creating a single, integrated finding aid was not reached. This remains an important research objective, and we believe that a project involving scholars with a focused interest in a more limited subject area (Whitman, rather than all of American Heritage) can attain it. For Whitman studies, what is needed is a single index in which one can find all the various manuscript drafts and notebook versions of any single poem. Our paper will detail the processes and struggles of creating such an index. From collecting finding aids and creating partnerships with other institutions, to developing a proper encoding standard and establishing good cross-department working relations, our project has embodied many of the benefits and challenges of integrating computing into the humanities. By identifying our procedures, and by laying out our future hurdles, we will contribute to a broader discussion of how scholars and archivists can collaborate effectively and of how the potential of EAD can be realized.  ",
       "article_title":"Ordering Chaos: A Virtual Archive of Whitman Poetry Manuscripts",
       "authors":[
          {
             "given":"Mary",
             "family":"Ducey",
             "affiliation":[
                {
                   "original_name":" University of Nebraska-Lincoln",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Andrew",
             "family":"Jewell",
             "affiliation":[
                {
                   "original_name":" University of Nebraska-Lincoln",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          },
          {
             "given":"Kenneth",
             "family":"Price",
             "affiliation":[
                {
                   "original_name":" University of Nebraska-Lincoln",
                   "normalized_name":"University of Nebraska–Lincoln",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/043mer456",
                      "GRID":"grid.24434.35"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   A jigsaw puzzle problem arises whenever an archaeologist finds an old manuscript broken into fragments and wishes to reconstruct it. The best known examples are the Dead Sea Scrolls, reconstructed by hand during the middle of the 20th century. In fact, there are two different types of problem, each requiring a different strategy. If the fragments come from a familiar text, the manuscript can be reconstructed by locating each fragment in an existing copy without reference to other pieces of the puzzle. If the text is unfamiliar, we must compare the shapes of fragments to one another to discover good matches with reasonable letter sequences formed across the boundaries. We haved called the former Jigsaw Problem A, the latter Problem B. The purpose is to find computer techniques which substantially reduce the human work involved in reconstruction.   Previous Work Very little has been published on this topic since it was brought to the first author's attention 35 years ago. A previous paper ( Levison and Wu, 1999) reported on some experiments involving artificial fragments derived from a paragraph of Alice in Wonderland. This work essentially provided a computer solution for Problem A, predicting whether a single site is likely for a given fragment based on letter-sequence frequencies and, for our fragments, finding such a unique location in Alice whenever it was predicted. Experiments on problem B proved less successful. A program matched each of the 53 fragments to each of the others with all possible vertical shifts, computing a score for each juxtaposition. The \"best\" matches were listed for each fragment, different scoring formulae being tried without materially affecting the outcome. The correct match often occurred among the best five, but only a few times in top or second place. In effect, two or three of 52 competitors often scored better than the correct one by chance alone. Extrapolation suggests that, given a set of a few thousand fragments, a human might have to examine a few hundred matches per fragment to determine the correct one -- a ten-fold improvement over a human-only approach, but still very time-consuming. For a satisfactory outcome, we want to narrow the field, allowing a human researcher to inspect only a few matches per fragment. Incidentally, these experiments did suggest that letter-sequences play only a secondary role in the scoring, shape being the most important feature. This is perhaps not surprising since letter-sequences can be assessed only on lines which fit together.   Refinement In Levison and Wu (1999), the left- and right-profiles of each fragment are represented by a sequence of measurements from an arbitrary vertical datum line. At first sight, it is the step-like nature of the edges which causes the problem, allowing too many good fits between unrelated fragments. In fact, however, any shape represented by a finite sequence of measurements can be viewed as step-like. The true problem is the granularity of the measurements: a single measure per line of text, with a (fixed) character width, about 2.5 mm, as the horizontal unit. In new experiments, a further paragraph of Alice was fragmented and three measurements were made for each line of text to the nearest 0.5 mm. Once again each fragment was compared with every other in all different vertical positions. The scoring algorithm measured the total gap between lines when two fragments were \"just touching\". This initial measure was adjusted according to the number of lines which contributed to it, and the final score biased to reward juxtapositions where several lines fitted closely but not exactly. In the set of 61 new fragments from the paragraph, 41 correct matches occurred in first place, with five each in second and third, a major improvement on the earlier results.   Tear and Wear Do these improvements persist if the number of fragments is much higher? With many correct fits now occurring in first place, the rough extrapolation used earlier no longer applies. Although few good shape matches may appear by chance among 61 fragments, they may well arise among thousands. To answer the question we need to experiment with much larger sets. Unfortunately no such sets are available, while tearing and measuring thousands of artificial fragments is not an enticing prospect. We have therefore created sets of \"virtual fragments\". The comparison process uses only the left- and right-profiles of each fragment. We therefore generate vertical \"tears\" which form the right- profiles of one group of fragments and the left-profiles of an adjacent one. A tear is simply a sequence of measurements, each randomly displaced from the one above. The displacements are chosen from a non- uniform distribution, small changes being common, larger ones less frequent. In fact, the actual distribution determines how rough or smooth the edges of the virtual fragments are. In practice, the shapes of these virtual fragments may be too perfect. Since the right-profile of a fragment and the left-profile of its neighbour come from the same tear, the correct match will involve no gap at all, helping to ensure that it is always the best. We therefore submit the virtual fragments to a process of \"wear\". Each profile measurement is altered by a random amount to reduce the quality of the fit between correctly matching profiles.   Results Many sets of fragments ranging from about 60 to 5400 in number were simulated and compared using the matching process. Typical results are shown in Table 1. For sets of around 60 fragments, the average number of correct matches occurring in the first three places is around 88%, very close to the results obtained for the paragraph from Alice. As expected, this percentage diminishes as the size of the set increases. For 3600 fragments, it is still about 66%. In other words, if we have a set of 3600 fragments whose profiles do not deviate substantially from our virtual sets, we expect to find the correct match among our three top choices about two-thirds of the time. This meets the objective set earlier. In fact, we can expect even better results if we apply the process in both directions, and use it interactively, so that the human scholar confirms some matches and thereby eliminates some profiles from later comparisons. The comparison program is written in C, and was run on a 500Mhz PC. Table 2 shows the time taken to carry out the comparisons for sets of different sizes. In principle, the time should be proportional to the square of the number of fragments, and this is closely borne out by the last three observations. (The processing portion of the two smaller cases is swamped by the initialization and output portions of the program.) Even for very large sets the process is feasible on current computers.  Table 1. Typical results of the comparison process.   number offragments number ofcorrect matchesamong top three percentage    62 50 80.7%    62 56 90.3%    58 54 93.1%   average   87.9%    199 152 76.4%    181 157 86.7%    193 146 75.6%   average   79.4%    406 303 74.6%    407 284 69.8%    383 297 77.5%   average   73.9%    3636 2443 67.2%    3667 2448 66.8%    3627 2412 66.5%   average   66.8%     Table 2. Timings for sets of different sizes  fragments time (seconds)   60 1   111 1.5   272 4   1084 48   5381 1100 (= 18 minutes 20 seconds)     Further Stages The comparison process is the central component of the reconstruction task. It permits many of the fragments to be combined into pairs and, by extension, into horizontal bands, from which scholars can easily complete the reconstruction. A complete program suite might go further, making use of digital photographs to display best fits for human judgement, applying the comparison process in the vertical direction to the horizontal bands, and so on. One time-consuming activity remains -- that of accurately measuring fragments to obtain their profiles. We have therefore investigated the possibility of deriving measurements directly from digital photographs. Some of our artificial fragments were photographed horizontally against a bright red ground using a digital camera. For this discussion we assume that the text is black on white. Our purpose is simply to distinguish three kinds of pixel: text, \"paper\" and background. If the text is faded or the material dirty, there are well-known digital processes for \"cleaning\" the image. The horizontal rows of pixels are scanned, and the numbers of black pixels in each row are counted. In principle, the numbers will be near zero for the inter-line gaps, higher for the bodies of text lines, and intermediate for rows corresponding to risers and descenders if any. This lets us locate the pixel rows which delimit the body of each text line; after which, scanning the top, middle and bottom rows of each line to find where red pixels stop and start again allows us to compute the desired measurements. Experiments confirm that this holds true in practice. In essence, then, the processes described here provide a computer solution to Problem B.   ",
       "article_title":"Jigsaw Puzzles: Problem B",
       "authors":[
          {
             "given":"Michael",
             "family":"Levison",
             "affiliation":[
                {
                   "original_name":" Queen's University, Ontario ",
                   "normalized_name":"Queen's University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02y72wh86",
                      "GRID":"grid.410356.5"
                   }
                }
             ]
          },
          {
             "given":"Craig",
             "family":"Thomas",
             "affiliation":[
                {
                   "original_name":" Queen's University, Ontario ",
                   "normalized_name":"Queen's University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02y72wh86",
                      "GRID":"grid.410356.5"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   1. Introduction The character-glyph model, which essentially describes a class-instance relationship between a character and the class of glyphs that can be used to render it, has been the underlying model that made modern character encoding standards possible. While this model has been successfully applied to many scripts, its application to East Asian logographic writing systems has not been satisfying. One of the reasons for this fact is that the model implicitly neglects the importance of differences on the glyph level and does not allow for the encoding of such differences. In East Asia, however, there are a variety of orthographic conventions in place, that do require that specific glyphs can be selected. Since the model did not allow for this, in many cases such glyphs have been encoded as separate characters that would have to be considered merely glyph variants according to the character-glyph model. This fact has largely contributed to the bloat of East Asian character encodings and has severely hampered information processing in this cultural sphere. In this paper, I am presenting research that is designed to solve this problem. It does this by storing information about the relationship of glyph variants, characters and a considerable number of other attributes in a separate database in form of a topic map. Encoded documents are normalized to use only codepoint to represent one character. Information about the intended glyph is separated out and represented with markup constructs. Upon rendering of the document, this can be used to draw the glyphs that where present in the original document. Additionally, this approach allows also alternative renderings according to various requirements. Since information about the usage of these glyphs is stored in the topic map, it can be used to render with the respective glyphs expected by audiences in modern Japan, Taiwan or mainland China, where the typographic and orthographic conventions have widely diverged in the last decades. As a proof of concept, a fragment of a topic map for East Asian logographs has been implemented and used with a small sample of texts to produce output in the described manner. Some details of this process are given below.   2. Some considerations In a series of experiments, some different rendering processes have been tried. They vary in how the information needed for rendering is deduced and how explicit the markup for this has to be. The two most extreme cases are: No explicit information about glyphs that may have variant rendering forms is stored in the texts. All information has to be deduced from contextual information (e.g. the date and place of the origin of a text and the information associated with this in the database) or explicit information about these occurrences in the database. All information for rendering the glyphs is stored in the texts. In this case, no external database is needed.  The main purpose of the experiments has been to determine a good compromise between these two extremes. What can be considered a good compromise itself, however depends on a range of different factors and will only be possibly established independently for every project, no global recommendations are possible. Any encoding scheme, that attempts to deal with these complexities, will have to take this into account and be better adaptable to the needs of its intended users. In this experiment, the texts are encoded according to the TEI Guidelines (P3/P4) and the database of characters and glyphs is encoded in the topic map XML format, a XML implementation of the ISO 13250 Topic Maps. Both of these are widely used standards in their respective areas and provide the flexibility and expressive power that was needed here.   3. A topic map of East Asian logographs The topic map of East Asian logographs contains information in the following dimensions: abstract character character instances mappings to coded character sets structure of glyph glyph variants time scope location scope meanings equivalent characters  This list is not exhaustive, since one of the characteristics of the topic map paradigm is that it is always possible to add basic data types, but reflects which particular character properties proved necessary for the problem at hand. This list will have to be modified as other application domains and areas are explored. It should also be noted, that this list is not flat, but the data are organized in super-class/sub-class and class/instance hierarchies. This will be made more explicit in the presentation.   4. The rendering process The rendering process requires a two pass processing. In a first pass, the rendering process will read the text and determine which characters require special treatment. These might be marked as such in the text, or this information might depend on parameters passed to the rendering process. After information about which characters occur in the text is available In a first step, the rendering process has to read the topic map and store the information contained in there in tables that will be accessed in the later stages of the rendering process. What information is extracted from the topic map depends on the desired form of rendering.   5. Conclusions  Since the character encoding lacked the necessary differentiation of characters and glyphs and appropriate mechanisms to access them, an additional layer of information has been created in form of a topic map that mediates between the characters that are encoded in the document and the rendering of these characters with instances of glyphs. This allows for the additional feature of rendering according to various user requirements, that otherwise would involve costly and error prone conversions on the character encoding level. It thus opens a new prospect for the information processing of East Asian texts. It should be noted however, that this process introduces the abstraction from the concrete glyph instance to the underlying character, which is an act of interpretation that might introduce errors. The experiments reported here might also provide valuable input for a revision of the Writing System Declaration (WSD) in TEI, which currently does not support such fine-grained and flexible encoding of glyph differences.   ",
       "article_title":" Encoding of glyph variants -- some preliminary experiments",
       "authors":[
          {
             "given":"Christian",
             "family":"Wittern",
             "affiliation":[
                {
                   "original_name":" Kyoto University, Institute for Research in Humanities ",
                   "normalized_name":"Kyoto University",
                   "country":"Japan",
                   "identifiers":{
                      "ror":"https://ror.org/02kpeqv85",
                      "GRID":"grid.258799.8"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The paper describes a collaborative project, funded by the UK Arts and Humanities Research Board, between the School of Library, Archive & Information Studies, University College London and two major digital resource gateways, Arts & Humanities Data Service, and the Humbul Humanities Hub. AHDS () and Humbul () are on-going government funded projects for the identification, evaluation and organization of quality digital resources in the humanities, primarily for the use of the higher education community. AHDS' remit includes visual and performing arts and archaeology in addition to traditional humanities disciplines; Humbul covers a slightly narrower humanities field including history, archaeology, literature, theology and philosophy. The two are developing a single humanities portal () which will become operational in 2002. The new portal will draw in resources from the wider Web in addition to the managed material already available. An important consideration is the choice of a tool to manage the subject content of the new site. A digital library on the AHDS model has much in common with the conventional library in terms of structuring the semantic content of the resource; it may benefit from the knowledge organization theory that has been developed over the last fifty years within the library sector for the creation of tools for vocabulary management and semantic organization of document content. Systems such as faceted classifications, structured subject headings, thesauri, and other controlled vocabularies provide a scientifically based approach to the analysis of 'document' content, and to the creation of indexes, descriptors, visible taxonomies and hierarchies, as well as linear ordering schemes (i.e. rules for filing order and sequencing) for the physical management of materials with respect to intellectual content. These have been tested over managed bibliographic databases as well as print-based materials, and the theory is at a high level of sophistication. Existing means of subject organization at AHDS and Humbul are the Library of Congress Subject Headings (LCSH) and the Dewey Decimal Classification (DDC), both designed for organization of print-based material in a traditional library. While these offer management advantages (e.g. an established system with institutional support, regular maintenance and revision, and centralised bibliographic services) they are not particularly useful within a digital environment. They display little sophistication in the structure, cannot handle complex objects well, and can do little to expose the complex interrelationships and multidimensional links within the structure of the digital collection. The new humanities portal requires a system that performs several functions; accurate description, for retrieval purposes, of complex digital documents/objects with a range of attributes, both of intellectual content and format provision of a systematic structure for the organization of the front-end in a directory format, using hypertext techniques to expose deeper layers of the network generation of structured subject headings for specific objects manipulation of these to create browsable alphabetical subject indexes capability of conversion to a thesaural structure to provide a controlled vocabulary of keywords and concepts.  Ideally, the system should also display; potential for multiple access points to the structure to enable resource discovery by various routes or search strategies; potential for incorporation into search software as a device in negotiating the wider Web.  The School of Library, Archive & Information Studies (SLAIS) at University College London has a particularly strong history in education and research in classification and indexing. It is one of only a few British schools offering teaching in this area, and its staff are actively involved in the management of several systems of bibliographic classification, and research into the development and use of faceted schemes. We are investigating a structure of this kind for the organization of the new humanities portal. Classifications built on the facet analytical model provide effective tools for analysing and organizing documents on the basis of their subject content, and consequently for retrieving those documents from a managed store. They work on different principles from older enumerative schemes such as Dewey and the Library of Congress classifications which simply provide long lists, or enumerations, of classes for the accommodation of documents. Facet analysis was conceived by S. R. Ranganathan, a mathematician by training, and a student at SLAIS in the 1920s. He proposed a system for the description and organization of documents with complex subject content, based on identification and analysis of constituent parts of the subject, rather than by creation of lists or enumerations of compound classes into which specific documents must be fitted. Documents were analysed, the content encoded, and the codes synthesised into an appropriate classmark which was used for filing and which was expressive of the subject content. Ranganathan's system was ground-breaking, but relatively unsophisticated. It continues to be developed in India, but is virtually unused outside that country. In the UK the Classification Research Group, formed in the 1950s, further developed facet analytical theory. The internal logic of a faceted system of the CRG type is based on rigorous analysis of the terminology of a subject, whereby terms are sorted into standard sets of functional categories. Within these categories a range of semantic relations are acknowledged, and problems of vocabulary control (such as synonymy, partial synonymy and variations in word forms) are addressed. A sophisticated system syntax provides for arrangement and combination of terms both intra- and inter-category, and for the management of syntactic relations. This improves performance in the accommodation of complex subjects, the predictability of location, and in the effectiveness of retrieval. A faceted classification is, in its simplest form, a structured set of simple terms or concepts with rules for the combination of these into compound concepts such as occur in the content of documents. These compounds are placed precisely in the base structure by the application of the system syntax. When these classes are populated by the 'real' subjects of documents (or other objects with semantic content) a more complex structure grows in accordance with the internal logic of the system. A faceted classification, when applied to a large collection of documents, can generate a very complex knowledge structure of n-dimensionality and great logical regularity, with deep levels of hierarchy. The resultant structure can be utilised in a number of ways; as an ordering device, as a source of index terms and subject headings, and can also be converted to a thesaurus. Hypertext can be utilised to expand the levels of hierarchy, or to make links between distributed elements. An example of a small classification for religion demonstrates how the structure can be applied: Judaism (Form subdivisions) Bibliography of Judaism Encyclopaedia of Judaism (Place subdivisions) Judaism in Europe (Period subdivisions) Judaism in the Middle Ages Judaism in the Nineteenth Century Judaism in Nineteenth century Europe (Philosophy and theory of religion) Religious philosophy of Judaism (Sacred texts) Hebrew Bible Mediaeval Hebrew Bible (Worship) Jewish festivals (Organization of the religion) Jewish religious law (Sacred texts) The Hebrew Bible in Jewish religious law  This can be represented in the form of subject headings as: Judaism Judaism - Bibliography Judaism - Encyclopaedias Judaism - Europe Judaism - Middle ages Judaism - Nineteenth century Judaism - Nineteenth century - Europe Judaism - Religious philosophy Judaism - Bible Judaism - Bible - Middle Ages Judaism - Festivals Judaism - Religious law Judaism - Religious law - Bible  These can be left in this order, to represent the systematic structure, or they can be alphabetized: Judaism Judaism - Bible Judaism - Bible - Middle Ages Judaism - Bibliography Judaism - Encyclopaedias Judaism - Europe Judaism - Festivals Judaism - Middle ages Judaism - Nineteenth century Judaism - Nineteenth century - Europe Judaism - Religious law Judaism - Religious law - Bible Judaism - Religious philosophy  From a small base vocabulary of 30-40 terms like this one, hundreds of multi-term subject headings can be generated. The subject headings can be inverted to form a browsable index in which distributed relatives are collocated.  Bible - Judaism Bible - Religious law - Judaism Bibliographies - Judaism Encyclopaedias - Judaism Europe - Judaism Europe - Nineteenth century - Judaism Festivals - Judaism Judaism Middle Ages - Bible - Judaism Middle Ages - Judaism Nineteenth century - Judaism Religious law - Judaism Religious philosophy - Judaism  The regularity of the system and its rules of syntax suggests that much of the routine work of managing documents could be carried out automatically, once the initial intellectual analysis has been made. In a testbed implementation for the research, AHDS and Humbul are applying the knowledge structure to the Portal's planned metadata repository for all the digital objects in their collection; it is likely that XML will prove to be the best tool for the implementation of the structure. They will also experiment with its use in cross-disciplinary browsing and retrieval of digital resources which are held elsewhere.  ",
       "article_title":"Facet analytical theory as a basis for a subject organization tool in a humanities portal",
       "authors":[
          {
             "given":"Vanda",
             "family":"Broughton",
             "affiliation":[
                {
                   "original_name":" University College London ",
                   "normalized_name":"University College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/02jx3x895",
                      "GRID":"grid.83440.3b"
                   }
                }
             ]
          },
          {
             "given":"Michael",
             "family":"Fraser",
             "affiliation":[
                {
                   "original_name":" Humbul Humanities Hub ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Sheila",
             "family":"Anderson",
             "affiliation":[
                {
                   "original_name":" Arts and Humanities Data Service ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction The Documentation Project and the Museum Project are cooperative projects between the four universities in Norway. Since 1991, these projects have performed retro-conversion and digitization of analogue archives, books, images and other media types. During the building up of our diverse collections, the need for a model expressing relationships between place names has been strongly felt, both in the modelling and the presentation of each collection and in the present work towards deeper integration between the various collections. In our paper, we will present an object-oriented model for expressing relations between geographical objects applied on Norwegian geographical information taken from our digital collections, followed by a discussion about further development and implementation of the model.   Previous efforts In connection with geographical reference systems for digital libraries we commonly find the use of UTM coordinates in the specification of geographical entities, as seen e.g. in the Alexandria project (Beard 1997). Whereas this method gives many possibilities both for use in single collections and in the interconnection of heterogeneous collections of information object, this method is not suitable for collections that contains a large number of references to geographical entities for which it is impossible or unfeasible to enter this type of coordinates. Whereas the use of UTM coordinates will be an important part of our system, we have to design a system which does not depend on this type of information. As the basic structure in the organization of the geographical information objects, we use the political geography of Norway. At any single point in history, this structure takes the form of a tree, as seen in fig. 1. However, this tree structured model is too restricted to cover collections spanning a time period. It have to be expanded, as we will see below.  Vegard Elvestrand has published a standard for geographical classification (Elvestrand 1977) with municipalities as the smallest unit. The number of municipalities in Norway has changed a great deal through splits and unifications. His model is effective for the classification of literature; one of our collections is classified according to his standard (Eide, 1998 p. 287-288), but the underlying model is also too restricted for our use. His model contains classification symbols for every municipality that has existed, but it has no linking showing the actual changes. Norsk samfunnsvitenskapelig datatjeneste (Norwegian Social Science Data Services, NSD) has designed various databases based on the administrative structure of Norway, directed towards the needs of the social sciences to present data on present and historical statistics (NSD 1997, chapter 3). Their model is effective for historical models of municipalities, but it does not allow us to include the multiple systems of geographical information needed for cultural and natural historical information systems, as we will see below. The model described here is conform with the CIDOC object-oriented Conceptual Reference Model (CIDOC 1999), and we will follow the further development of the CIDOC model closely as we develop and implement our model of geographical information.   Places vs. place names There are several other structures interconnected with the simple structure of fig. 1, e.g.: Other administrative regions, e.g. police districts, clerical districts Named geographical formations, e.g. lakes, mountains, rivers Property structures, e.g. farms, common land  In the geographical model in fig. 1, there is no distinction between a place and a place name. The difference between a geographical unity in the real world and the name we use to refer to this unity must be expressed in any geographical model. There are two main reasons for this: The same word often refer to several distinct places. One example of this is a municipality called Våler, which is found in the county of Hedmark as well as in the county of Østfold. Political structures may change. The area of the town of Hamar is larger today than it was ten years ago, whereas the municipality of Vang no longer exists (in a bureaucratic sense of the word exist), as it is included in the expanded town of Hamar.  In fig. 2, our basic geographic model is sketched, populated with several geographical objects. The idea behind the model is not to represent all potential information about geographical entities and their relations, but to include enough information to perform computer search, browse and interconnection between objects in an effective way.  Each object in our model may have an attribute showing the time span that the geographical entity was known in that form. This does not mean that the object is placed in a time continuum, as it has a distinct, everlasting geographical location, which may be expressed in geographical coordinates. The time span is just an attribute showing which time period this specific geographical object was referred to. This is especially important for legal entities such as municipalities. Municipalities described as old or new in the figure have been split up or united. The direction of the graph is always from the bigger unit to the smaller unit: a unit pointed to is a part of the unit pointing from. To make this possible, we have split geographical entities covering parts of several other entities into parts shown on the figure as hp for hill parts and sp for sea parts. Each such part is part of both the sea as a whole and the municipality that this part is located in. The known place object contains a name, an optional time span, and references to other objects forming the graph. As some of our collections contain UTM data, and as we will be able to import such data for all our municipalities from e.g. NSD, the object also contains an optional UTM attribute.   Expansion into the unknown We define each actual mentioning of names in texts and other media as an observation. In this context, observations are all references to geographical entities in information objects. Their relation to the model is seen in fig. 3. These observations are external to the model; whereas the model shows the objective reality (in the meaning agreed upon, checked), observations are subjective references made by a human being at a specific time, although this time may be uncertain or unknown.  An observation will never refer to more than one place-name by definition. If there is more than one link from an observation to known facts, such a split expresses an insecure fact, in which case an observation may relate to one out of several possible known places, but we do not know which one. There is no limitations as to what kind of entities may be included in an implementation of this model, but we will have to decide on standards when we use the model in actual work. The development of such standards will be be discussed in our paper, together with search facilities and matters of implementation.    ",
       "article_title":"The name of the place : towards a model for interconnection of geographical entities",
       "authors":[
          {
             "given":"Øyvind",
             "family":"Eide",
             "affiliation":[
                {
                   "original_name":"The Museum Project,  University of Oslo ",
                   "normalized_name":"University of Oslo",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/01xtthb56",
                      "GRID":"grid.5510.1"
                   }
                }
             ]
          },
          {
             "given":"Lars-Jørgen",
             "family":"Tvedt",
             "affiliation":[
                {
                   "original_name":"The Documentation Project,  University of Oslo l.j.",
                   "normalized_name":"University of Oslo",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/01xtthb56",
                      "GRID":"grid.5510.1"
                   }
                }
             ]
          },
          {
             "given":"Jon",
             "family":"Holmen",
             "affiliation":[
                {
                   "original_name":"The Museum Project,  University of Oslo ",
                   "normalized_name":"University of Oslo",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/01xtthb56",
                      "GRID":"grid.5510.1"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Still under development, the University of Minnesota's Early 19th Century Russian Readership & Culture Project () is one of relatively few comprehensive digital archives for the study of Imperial Russian history. Based on materials gathered over two decades of research that culminated in a dissertation on The Expansion of Russian Reading Audiences, 1828-1848, the project is distinctive in a number of ways. First, it pulls together a variety of primary research materials into a single archive. (To some extent this is not uncommon, but the ENCRRC archive includes not only texts, images, and scholarly reference materials, but also a large statistical database of 12,000 subscription records which provide much additional research data). Second, the search mechanism has been customized to provide simultaneous access to both English and Cyrillic TEI-encoded texts without the need for a Cyrillic keyboard--and this in itself is a technical feat. But the project's most distinctive feature is its use of SGML <interp> tags to enrich the texts by encapsulating a number of pre-selected analytical categories. This means that users of the archive are given not one but two modes of access to the content of the four main text groups (fiction, journals, memoirs and travel accounts). How is this achieved? First, researchers are presented with a comprehensive full-text search option, since the project uses Enigma Corporation's powerful SGML-based DynaWeb software to deliver the texts to the web (). Second, the project's adoption of SGML-based analytical tagging means that users have an additional avenue of access to thematic categories. To enable this approach the interface presents a roster of 10 main categories of analysis, each divided into small groups of differing sizes for a total of 60 subcategories. Entries are scripted so that the software can retrieve and display passages that contain superimposed SGML ID references even though the texts are converted to HTML for delivery over the web. Space limitations prevent enumeration of all the subcategories, but a listing of the main themes will give some idea of the research potential involved: Publishing, Print Categories, Novels, Journals, Newspapers, Booktrade, Text Access (Bookstores, Coffeehouses, etc), Reading Publics, Social Groups, and Job Titles. The provision of these categories works well for researchers in differing fields of Russian culture. A literary scholar may use the archive to trace references in the various groups of texts to the distribution of original Russian novels versus translations of foreign compositions; he or she may then enhance these findings by searching the database of subscription records from the period 1825-1846, and reviewing biographic and geographic data about the subscribers connected with both native and translated novels. A history scholar, on the other hand, may use the materials to trace levels of access to print materials among less privileged groups (such as lower-level bueaucrats and merchants) not normally considered part of the contemporary cultural milieu. A women's studies scholar may use the archive to piece together hard-to-find references to women's reading, and the mechanisms women used to gain access to texts in a clearly defined patriarchal society. It should therefore be clear that employment of a relatively simple SGML-based analysis option has enabled substantial enrichment of a carefully-selected core of texts such that they are immediately serviceable for multiple purposes. In addition to the project's value for different fields of study, another important benefit resides, as noted, in its presentation of a rich variety of sources that include encoded images and historical records as well as primary texts. All these resources are related, moreover, by topic. But the linkage between them is not always straightforward, and so it has seemed important to take note of new concepts like SGML Topic Maps (ISO standard 13250, Geneva, December 2000)--which promises to facilitate the linkage of similar elements in different types of research data. As Christian Wittern has suggested (\"TEI and Topic Maps,\" ACH/ALLC 2001), topic maps provide an \"architecture for the semantic structuring of information networks…[that] has the potential to provide a bridge between… texts encoded with schemes like the TEI [and] other information resources.\" More explicitly, Bill Trippe noted recently in an article published in EContent (August 2001, v. 24, issue 6, p. 45ff), \"For proponents, topic maps are the ideal solution for helping users find information about a topic across a variety of documents.\" Wittern also suggests that topic maps enable an archive to present abstract as well as concrete representations of knowledge. Until recently the categories used in the ENCRRC archive were more exclusively objective than the mix of categories used in our better-known sister project, Women's Travel Writing, 1830-1930 (), and as such, have been easier to apply. But WTW's two subjective categories--gender and ethnicity--are strongly championed by faculty advisers to the project as essential material for the pursuit of current scholarly trends in women's studies (though certainly more challenging to the encoder). It thus seemed advisable to problematize more fully the analytical tools supplied for the ENCRRC archive. With this in mind, we are redesigning and testing out certain revamped portions of the ENCRRC archive. Drawing on the work of Hans Holger Rath and Steve Pepper (including their \"Navigating Haystacks, Discovering Needles,\" Markup Languages, 1999), we hope to achieve a partial implementation of the topic map standard. We are partially motivated by the hope that this will facilitate an overlay of more provocative, abstract linkages superimposed on our current, largely objective interpretive network. A second goal is to determine whether the presentation of disparate early 19th century Russian history materials in the form of topic maps will make their analysis more convenient and productive for the researcher than their current presentation in separate, though proximate, data groups. But as noted by Trippe, \"the proposed standard itself is relatively new… and the commercial technology supporting the standard is still in its early stages.\" Our second goal is therefore our major concern: to explore how well this concept works in a multi-type archive.  ",
       "article_title":"Facilitating Text Analysis in Russian Culture: TEI or Topic Maps?",
       "authors":[
          {
             "given":"Miranda",
             "family":"Remnek",
             "affiliation":[
                {
                   "original_name":" University of Minnesota ",
                   "normalized_name":"University of Minnesota",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/017zqws13",
                      "GRID":"grid.17635.36"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  In this paper we will describe the aims, the main research objectives and the crucial computational aspects in establishing a large electronic text corpus. In the first part the concept of our corpus approach will be given and its backgroud as well as its consequences discussed. In the second part of the paper noteable features of the digitization processes based upon the application of XML-Schemas will be discussed. The Austrian Academy Corpus (AAC) is a newly founded institution based within the Austrian Academy of Sciences in Vienna. Its aim is to set up a text corpus and also to conduct research in the field of electronic text corpora. Electronic text collections to date have been generally focused on linguistic studies and lexicography,and designed and set up for language-orientated research. Recently, the perspective has changed towards providing resources for scholars from various fields within the humanities. The AAC is attempting to establish a corpus that meets the needs of textual studies and conveys essential information about language and history. The AAC functions as an example of an experimental corpus that is predominantly designed for textual studies. It will be a complexly structured text collection in which sources from a variety of fields will be included. The AAC also aims to include a wide range of significant texts from various cultural domains, which will be carefully selected as being of key historical and cultural significance and relevance. The AAC will create, structure, provide and analyse selected text sources from the past two centuries, taking advantage of the latest standards and techniques in electronic text processing. The AAC intends to digitally store a wide selection of different sources of scholarly, journalistic and political texts which were of considerable influence in the period between 1848 and 1989. It has started the digitisation and structured integration of texts, amongst which are for example several influential and notable literary and political journals, such as “Die Weltbühne” or “Die Aktion”, published in Berlin in the first decades of the last century, and the Austrian journal “Der Brenner”, published in Innsbruck, as well as many other sources. The famous satirical magazine “Die Fackel”, published by Karl Kraus in Vienna, will constitute the core of the AAC and will be a starting point for future selections of texts. Images and manuscripts will be included in the corpus, where necessary, because the original graphical and typographical information is important for the meaning and interpretation of digitised texts. This is particularly the case with complex text structures such as newspapers or literary journals which comprise a whole variety of functionally different text types within their structure. Digital resources in the form of electronic text corpora should be regarded as structures for representing complex information. The electronic text collections established at the AAC so far and its future projects will focus on electronic representations not only of literary texts, literary magazines, journals and newspapers but also on a carefully considered selection of texts from several other cultural and social domains. Special emphasis will be placed on areas that have been rather neglected in humanities computing to date. Journals and newspapers pose an especially difficult task when it comes to their representation in digital form. An equally difficult task is the analysis and description of the media’s decisive historical influences and contexts. The study and detailed investigation of texts has always been crucial for our understanding of historical processes. The knowledge of texts and the accessibility of textual knowledge can be furthered by means of large text corpora like the AAC. The text selection for the AAC, which will take place at the same time as the corpus work, will be guided by thematic and empirical criteria, as well as factors specifically related to the type of text. The specificity of text type is therefore, amongst others, a decisive factor not only for the selection of texts but also for their categorisation in a corpus: letters by Oskar Kokoschka, anecdotes by Max Liebermann, writings of Adolf Loos, narrations by Adalbert Stifter, feature articles by Daniel Spitzer, funeral sermons, electoral speeches, propaganda and advertising slogans, pop song lyrics, political speeches, comic books, instructions, travel guides, TV programmes, mailing catalogues, and so on.. In recent years, the establishment of large German language corpora has been restricted to the field of linguistic and lexicographic studies. So far, there have not been any large-scale initiatives in the area of text-centred studies. Although more and more literary texts are becoming available, many of these came into existence as by-products of efforts to amass data for lexicographic research. Generally speaking, the historical period on which the Austrian Academy Corpus is working is poorly documented in terms of digital literary texts. This applies even more when it comes to collective text ‘carriers’ such as magazines, papers, yearbooks, commemorative volumes and similar materials. Among the sources being digitised for the AAC are a considerable number of historical literary magazines of major importance. One example is the journal “Der Brenner”, which was published by Ludwig Ficker in Innsbruck from 1910 until 1954. Among its contributors are figures as renowned as Carl Dallago, Theodor Haecker, Else Lasker-Schüler, Adolf Loos, and Georg Trakl. Other sources on which the AAC is working were published in Berlin, for instance, the journal “Die Aktion” edited by Franz Pfemfert between 1911 and 1932. Among its contributors were Peter Altenberg, Hermann Bahr, Walter Benjamin, Max Brod, Richard Dehmel, Salomo Friedlaender, Georg Heym, Kurt Hiller, Max Oppenheimer, Egon Schiele and August Strindberg. Another journal to be mentioned here and perhaps the most important one in the pipeline is the weekly Berlin journal “Die Schaubühne” (1905 - 18), later renamed “Die Weltbühne” (1918 - 33,) which was edited by Siegfried Jacobsohn, Kurt Tucholsky and Carl von Ossietzky. Among the writers who contributed to “Die Weltbühne” were Henry Barbusse, Bertolt Brecht, Alfred Döblin, Lion Feuchtwanger, Arthur Koestler, Heinrich Mann, Alfred Polgar, Romain Rolland, and Leon Trotsky. To produce a digital version of, for example, the literary journals “Der Brenner” or “Die Weltbühne”, the original text has to undergo the usual stages of electronic processing. After being scanned, the text is made readable by means of OCR. The structure of the text (pages, paragraphs and lines) is identified by automatic routines. Application of markup is the last step in this process. Tags encoding content are carefully inserted by literary scholars especially trained for this task. This process, which takes several runs, is accompanied by proofreading against the original and constant checking and validating of the achieved results. Literary encoding projects in the past have employed SGML, very often following the TEI Guidelines (P3). The AAC makes extensive use of XML’s modular system of specifications. Aside from the basic XML specification, several other specifications exist, all of them having more or less defined place within the overall framework provided by XML. The exact nature of some of these specifications is not yet clear (XLink, XML Query), as development work still continues apace. Those that are classified as recommendations are XSLT (Extensible Stylesheet Transformations) and XPath (a language for addressing parts of an XML document). The implications of others such as XLink (Extensible Linking Language), XPointer (an abstract language that specifies locations), and XQL (Extensible Query Language) for literary computing will have to be considered in due course. As XML comes of age, the issue of a standard way of defining the structure of documents becomes more and more important. Both traditional DTDs (document type definitions) and XML Schemas are formats which model document structures. Whereas DTDs in the traditional sense have been around for some time and are widely accepted in the field of SGML-based text-encoding, XML Schemas must be regarded as a fledgling technology that still has to win its spurs. XML Schemas are commonly regarded as an attempt at an XML answer to the problem of defining the structure, content and semantics of documents. There are several arguments in favour of XML Schemas, among which are XML syntax, object orientation, inheritance, polymorphism and datatyping. Firstly, XML Schemas follow XML syntax rules, which makes it possible to parse them with XML tools. Nowadays, authors of XML documents often regard traditional DTDs as unwieldy and inconsistent with the structure of XML. With XML Schemas, validating parsers can be built on the basis of XML syntax. Secondly, XML Schemas may include explicit restrictions on the data types an element may hold. They allow the text programmer to attribute data types such as strings, numbers (integer, floating point), date and time formats, boolean and others to elements constituting an XML document. In addition, XML Schemas are also intended to allow the definition of new data types to future refine the markup scheme being used. For the corpus holdings of the AAC such applications are investigated and implemented for the benefit of various corpus-based linguistic and textual studies.  ",
       "article_title":"The Austrian Academy Corpus - Digital Resources and Textual Studies",
       "authors":[
          {
             "given":"Hanno",
             "family":"Biber",
             "affiliation":[
                {
                   "original_name":" Austrian Academy Corpus ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Evelyn",
             "family":"Breiteneder",
             "affiliation":[
                {
                   "original_name":" Austrian Academy Corpus ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Karlheinz",
             "family":"Moerth",
             "affiliation":[
                {
                   "original_name":" Austrian Academy Corpus ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Overview: This paper will look at the use and potential of electronic publication and the usability of the markup language XML for the publication of field reports by commercial archaeological units. The field reports fall into the field of grey literature as they are produced as client reports by archaeological units as part of the planning process and do not receive official publication or widespread dissemination. It has been recognised, however, that the reports contain valuable information that would be useful for research purposes as well as for other commercial units and the public and that suitable means should be found in order to disseminate that information. The basis for the project is a case study undertaken by the author, who was part of the Department of Information Studies at the University of Sheffield, in conjunction with a small commercial unit, Archaeological Research and Consultancy at the University of Sheffield (ARCUS). Current publication practices within academic and commercial archaeology were reviewed and the ARCUS field reports were used in order to test the usability of XML for the markup and electronic publication of those reports.   Background: The dissemination of archaeological information and data is still mainly undertaken in the format of the traditional paper-based excavation report. However, archaeological data is now increasingly being collected and stored in a digital format and the advent of the Internet has also raised the issues of electronic publication and dissemination of data and site reports via this medium. Electronic publication is taking place, but it is still few and far between and mainly restricted to academic research projects. In Britain, archaeological practice outside the university research environment is restricted to interventions undertaken as part of the planning process and is funded entirely by the “polluter pays” principle. This has emphasised a dilemma archaeological practice was increasingly faced with. On the one hand, it has become just a small part of the planning process and is governed by the same commercial principles as any other development contractor. On the other, the results it produces are still seen as “academic” and a basis for research. There is an innate duty or ethic within archaeology to disseminate results and to produce syntheses of material. However, the funds provided usually do not allow more than the implementation of an immediate mitigation strategy for development and the production of a minimal “developer’s report”. A recent survey of user needs concerning publication and dissemination of data within archaeology (“PUNS survey”, Jones et al 2001) undertaken by the Council of British Archaeology (CBA) identified that even though fieldwork reports are one of the most frequently consulted types of archaeological literature, grey reports have a limited audience beyond the commercial domain due to the difficulty in access and lack of awareness of that literature. It also stressed that many archaeologists are dissatisfied with this situation as they feel that information of relevance to their work is being produced of which they are unaware.   Proposed paper: This paper addresses some of the problems of dissemination raised by the PUNS survey and also other issues inherent in the publication an dissemination of archaeological data like the provision of adequate metadata and the searchability of archaeological information on-line. It describes the use of XML and a modified version of the TEI lite DTD of the Oxford Text Encoding Initiative for the mark up of a sample of ARCUS excavation reports. It also looks at the possibility of incorporating controlled archaeological vocabulary into the DTD. It raises questions about current practices of electronic publication within archaeology and the availability and searchability of data provided by the discipline for research purposes.   Results: It was found that the electronic publication of grey reports would be very useful for as it would allow a quicker response time and a rapid dissemination of information within the fast moving and changing environment of commercial archaeology. XML would be a useful and economic tool for the publication of field reports as it would allow users to selectively download separate sections of field reports which are of particular importance to them. Archaeology-specific elements would also allow a better context specific searchability of reports on the web. However, it is acknowledged that XML is still a technology in its infancy and also that it is beyond the financial capabilities of many small commercial units to implement a system of electronic publication. An important point that needs to be raised is that national archaeological institutions will have to accept electronic versions of field reports in order for them to be formally recognised and be able to be built into the financial framework of a commercial project design.   Relevance to ALLC-ACH themes: The paper is of particular relevance to the themes of the ALLC-ACH conference as it addresses the issues of the development of new methodologies in one discipline that will have a considerable impact on others. The electronic publication of commercial field reports will provide indispensable information not only for other commercial units but also for academic research, thus helping to bring together two increasingly diverging fields. Archaeological data is also used by other disciplines within the Humanities and Sciences, like anthropology, sociology, earth sciences and others. Archaeological site reports also consist not only of text but of a wide variety of images and the electronic medium could provide increasing resources such as 3D reconstruction or even audio and video data. The quicker response time of the electronic medium and the increasing availability of primary archaeological data should also allow users to examine the assumptions upon which previous interpretations are presented and to facilitate the development of their own models, thus fundamentally changing the nature of discourse within archaeology and the humanities in general.   ",
       "article_title":"The electronic publication of archaeological field reports using XML",
       "authors":[
          {
             "given":"Christiane",
             "family":"Meckseper",
             "affiliation":[
                {
                   "original_name":" Humanities Research Institute, University of Sheffield ",
                   "normalized_name":"University of Sheffield",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/05krs5044",
                      "GRID":"grid.11835.3e"
                   }
                }
             ]
          },
          {
             "given":"Claire",
             "family":"Warwick",
             "affiliation":[
                {
                   "original_name":"Department of Information Studies University of Sheffield c.",
                   "normalized_name":"University of Sheffield",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/05krs5044",
                      "GRID":"grid.11835.3e"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   The Trésor de la Langue Française (TLF) corpus () was set up almost half a century ago. When one reads the description of how this was done, the distance becomes evident. Professor Imbs quite openly admits that the goal is to reflect \"elite\" usage of the French language; texts were chosen after consultation of histories of literature, some of which were quite dated even then (Imbs 1971, I, xv-xl). Considerations of inclusiveness, of representativity, as discussed in Scholes (1992) or von Hallberg (1984), do not seem to have concerned the committee which finalized the corpus. One is entitled to wonder to what extent this corpus represents the interests of scholars of French literature a half century later.   Purpose It is legitimate to evaluate the extent to which the texts included in the TLF database do represent important trends in French literature, as judged by what interested scholars at the time it was constituted, and as reflected by what has interested scholars of the present. More specifically, it is possible to see whether the choices embodied in the TLF reflect what scholars of the time judged important by comparing the choices of texts in a given genre - the novel - to the number of lines dedicated to the authors chosen for the TLF found in the Oxford Companion to French Literature (Harvey & Heseltine 1959). Similarly, the MLA Bibliography () provides online data showing the number of publications in the modern languages and literatures for the periods 1963-90 and 1991 to the present. A comparison between the number of publications mentioning a novelist found in this bibliography and the number of texts by the same novelist in the TLF will show the extent to which choices made by the TLF group have been confirmed by the interest of later scholars. Given the volume of data involved these questions must be dealt with using statistics.   Data A subset of the TLF database was chosen for analysis: novels published between 1789 and 1954 (See Table 1). The name of the novelist (Author) and the number of novel texts included in the database for each writer (Texts) was recorded, along with the publication date of the text included in the database (Pub Date). When more than one novel by a given author is in the TLF Pub Date records the date of the earliest one published. In cases where authors were better known for other genres rather than prose fiction, they were removed from the test data, because they would be a source of ambiguity. These numbers were compared to three series of test data. The column OxC in Table 1 records the number of lines devoted to the novelist and to the included novels by that author which are found in the Oxford Companion to French Literature (Harvey & Heseltine 1959), a volume contemporary with the formation of the TLF database. Columns MLA 1 and MLA 2 record the number of articles mentioning the novelist or work(s) found in the MLA online bibliography of learned articles dealing with language and literature. MLA 1 covers the period 1963-1990 and MLA 2, 1991-2000. For analysis the entire set of 128 frequencies concerning novels was used. Subsequently subsets of roughly equal numbers of authors were generated, covering the periods 1789-1859 (33), 1860-1907 (35), 1908-23 (25), and 1925-54 (35).   Author Pub Date Texts OxC MLA 1 MLA 2   Abellio 1946 1 0 9 0   About 1857 2 14 1 0   Adam 1902 1 25 1 4   Alain-Fournier 1913 1 93 29 4   Ambriere 1946 1 0 1 0   Aragon 1936 1 25 445 305   Arland 1929 1 0 37 4   Ayme 1933 1 7 38 9   Baillon 1927 1 0 3 6   Balzac 1824 16 577 1986 781   Barbusse 1916 1 16 52 13   Barres 1888 5 87 93 72     Method A glance at the frequencies of the texts recorded for individual authors shows a large number of authors with one text, and a very small number of authors with ten or more, a distribution pattern quite familiar to people who work with word frequencies in natural languages. These data do not form the familiar bell-shaped curve typical of the Gaussian or normal distribution. Since the data are not normally distributed, Pearson's product-moment correlation analysis cannot legitimately be used on them. Similarly these data would produce a very high proportion of predicted values smaller than 5 in a contingency table for a chi-squared analysis, so this method cannot be employed. The usual way of handling such a problem (grouping the data) is not appropriate, since it is the treatment of individual authors which is of interest. Spearman's rank correlation analysis does not require normally distributed data nor predicted frequencies greater than five; it has been chosen as the primary analytic technique and applied in pairwise fashion to the data, and to the four subsets of the data. At the same time, jackknifed outlier analysis provided by JMP-IN (Sall & Lehman 1996) has been used to identify authors whose distribution varies the most from the trends in the data.   Results Taken as a whole, the data show a high degree of correlation among the number of texts in the TLF database, the number of lines in the Oxford Companion, and the two sets of MLA Bibliographic data (See Table 2). There is no measurable probability that these correlations be the result of chance alone.  Table 2: Nonparametric Measure of Association  Variable by Variable Spearman Rho Prob>|Rho|   OxC Texts 0.5528 <.0001    MLA_1 Texts 0.4475 <.0001    MLA_1 OxC 0.6101  <.0001    MLA_2 Texts 0.4047 <.0001    MLA_2 OxC 0.5918 <.0001   MLA_2 MLA_1 0.9084  <.0001   The data divided into four sections show a higher correlation in the earlier period than in the later, and outliers in the earlier two periods tend to be the greats of French literature, like Balzac, Stendhal and Zola, whereas in the later periods they tend frequently to be novelists whose literary fortunes are less obvious, like Simenon or Giono.   Conclusion The analysis carried out on the number of novel texts included in the TLF database shows that the texts included tend to be about the same as what might have been included if a different team of scholars had drawn it up in the late 1950s. Similarly the works included do correspond - particularly for the period up to 1908 - to what scholars of our day find sufficiently interesting to be included in their published studies. It is thus reasonable to conclude that the TLF database is a valid representation of important French literary texts for the period from 1789 to 1954. As more and more databases become commercially available, the method presented here for validating the representativity of a database using readily-available online bibliographical information would seem to have a significance which goes beyond modern French literature.   Acknowledgements The research reported here has been supported by the Social Sciences and Humanities Research Council of Canada (SSHRCC) under grant number 410-98-1348.   ",
       "article_title":"That Was Then: Canonicity in the Trésor",
       "authors":[
          {
             "given":"Susy",
             "family":"Santos",
             "affiliation":[
                {
                   "original_name":" University of Manitoba ",
                   "normalized_name":"University of Manitoba",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02gfys938",
                      "GRID":"grid.21613.37"
                   }
                }
             ]
          },
          {
             "given":"Paul",
             "family":"Fortier",
             "affiliation":[
                {
                   "original_name":"Centre on Aging, University of Manitoba ",
                   "normalized_name":"University of Manitoba",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02gfys938",
                      "GRID":"grid.21613.37"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  *Considerations on a framework for electronic editions*  Overview A replacement for tools like Tact and WordCruncher is very much needed, because the existence of reliable and accepted standards for text encoding like TEI stimulate the creation of electronic editions, but there is no free tool available to publish them. The design of such a reading program has to be abstract enough to make implementations in different programming languages possible, therefore it would be advisable not to write a new program but creating a framework for philological components. The proposed framework should help small to medium sized projects planning electronic editions to get started with as little additional time and money investment as possible. At the same time the framework should scale so well, that it allows its use in very large projects or in projects with a lot of additional features. As has been pointed out before no philological program is able to fulfil all user wishes, so extensibility is an important design factor for the proposed framework. In this paper I will discuss some typical needs of such a framework from three different perspectives: 1. The end user reading an edition or searching it 2. The editor building an edition 3. The programmer customizing and extending the framework  In the fourth section I point out some design options and problems for the architecture of such a framework.   end user The end user should be able to use the edition in two modes: first without any learning curve; at least basic feature like reading and simple searches should be accessible in this mode. Therefore the end user interface should be imitating established practices like browsing the net and using an Internet search engine. The second mode for more advanced users has a command line interface, which allows the formulation (storing, commenting etc.) of more complex retrieval requests. Three typical uses of an electronic edition should be possible: Reading / browsing. Next to the usual navigating along the structural information of the text, the reader should be able to choose between different views on the text. Three views are probably enough for a start: a) simple reading version, where all additional information is hidden; b) scholarly reading text with all additional information presented in a typographical form; c) the raw xml text. In the case of critical editions it should be possible to choose between different base texts. Text retrieval. The user can choose to have all 'hits' either in the sequence they are in the repository or ranked according to the quality of the 'hit' as is the practice in Internet search engines. - The scope of the search can be limited to certain parts of the corpus or extended to include other corpora maybe living on other servers. Retrieval of simple statistical information like type/token ratio, collocation etc. (as in WordCruncher or Tact).  To have some solid ground for any assumptions on the user needs, this section will be based on the analysis of some electronic editions and a survey distributed to editors on the next meeting of German editors.   editor The editor wants to make the material she or he edited accessible to the end user without much additional work and without loosing to much information. These are contradictory aims, because necessarily it needs some configuration to make all specific features of an edition accessible and a standard interface can only make those features accessible, which most editions probably share. But the configuration should be made as easy as possible and in the normal case should not need any programming knowledge. Especially for small to medium sized projects there is a need to have some publishing environment to test the markup during production and to publish the text because they cannot afford expensive publishing software. Existence and easy use of such a framework could also stimulate the creation of small editions because editors would find help with all basic steps. To get a clearer picture of the needs of editors the survey which has already been mentioned above will also include questions on what editors expect from their electronic tools.   programmer The programmer wants to be able to extend the framework to fulfil the needs of a special edition. Doing this should be possible by using standards like XML, TEI, SOAP etc., which he or she probably would know about anyway, and by working with a well-defined and well-documented API. Having working examples to build on should further the process of creating new components. It should be possible to build new components, which enable new ways of visualization, searching or using (e.g. statistics) the edition. Ideally these components can be added to the basic software without recompilation of the other software or reindexing the text, just by registering them with the framework. The framework shouldn't be tied to a special platform.   The framework The many different needs, which define the framework, shouldn't lead to a monolithic program doing it all. They can better be solved with an ensemble of small components each of them solving only one task. Unix tools are good model for this, but in contrast to these tools the input and output needs to be more strictly defined and a graphical user interface, at least for the group \"end user\" and the group \"editor\", should make the tool collection more accessible. The main task of designing the framework is the definition of a basic set of such components and the communication between them. The basic framework doesn't need to have already all requested features users wish, but should exclude their integration at a later moment. The following components would be part of such a basic structure: A web browser based interface to the end user A component, which receives the cgi requests from the web browser. It wraps the request information into an xml based communication format. A component, which has a table matching, requests to some request handler. It receives the request, looks up the appropriate handler and calls it. A handler, which uses the next two components to send the user query to the backend. It manages state information etc. A component, which converts the information from the user form to some query format using XPath, XML Query etc. See below for a more detailed discussion of this aspect. This is used by all components. A component, which converts the query format to some format the backend supports (if the query format is supported by the backend, it is even better). A backend consisting of some storage for the edited material, e.g. a relational database or an xml database. A component, which wraps the results - usually a node set - return by the backend into some standardized wrapper format.[1] A component, which allows the application of xsl:t stylesheets to the query result to convert it into html, which is sent back to the web browser.  It should be possible for a programmer to insert pre- or postprocessor components into this basic structure, so the components shouldn't be integrated too tightly. The paper will discuss possibilities to achieve this by using newer web service protocols like XML-RPC or SOAP. The framework has to define standards for at least to interfaces in this data flow: 1) A wrapper format into which the cgi request is stored; 2) a standardized way to match user requests and handlers; 3) the query language, 4) the wrapper. The paper will discuss possible solutions, especially contentrating on the problem of a suitable query language. The reason for the paper is not so much the proposal of adefined framework but the hope to start a new discussion in a time where the means for collaborative work are easily accessible. Not the least aspect of it is its aim to offer a sound basis for such project by defining the user needs.   ",
       "article_title":"Some considerations on a framework for electronic editions",
       "authors":[
          {
             "given":"Fotis",
             "family":"Jannidis",
             "affiliation":[
                {
                   "original_name":" Institut fuer Deutsche Philologie LMU Muenchen ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Summary We present a Web edition of the three writings on electoral systems by Ramon Llull (1232-1316). The first of these, Artifitium electionis personarum, which has come down to us only as a fifteenth century manuscript, is exhibited here for the first time. We discuss the goals of providing our Web edition, and outline the implementation we have chosen.   Zusammenfassung Wir präsentieren eine Web-Edition der drei Wahlsystemschriften des Raimund Lull (1232-1316). Deren erste, Artifitium electionis personarum, ist einzig in einer Handschrift des fünfzehnten Jahrhunderts überliefert und wird hier erstmals vorgestellt. Wir diskutieren die Ziele unserer Web-Edition und skizzieren die softwaretechnische Umsetzung, für die wir uns entschieden haben.   Contents   1. Introduction 2. Llull's electoral writings 3. Goals for the Web edition 1. Global instant access 2. Expert discussion forum 3. Computer supported editing  4. Software implementation 1. Document object model and browser orientation 2. JavaScript-based dynamic HTML 3. Text fragmentation and linked highlighting 4. Future work  5. References Univ.Augsburg, Inst.f.Mathematik, Report 439 (10pp., 8 May 2001)      ",
       "article_title":"A rediscovered Llull tract and the Augsburg Web edition of Llull's electoral writings",
       "authors":[
          {
             "given":"Friedrich",
             "family":"Pukelsheim",
             "affiliation":[
                {
                   "original_name":" University of Augsburg ",
                   "normalized_name":"University of Augsburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/03p14d497",
                      "GRID":"grid.7307.3"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Wolfram von Eschenbach's Parzival ranks as one of the most significant narrative works to emerge from medieval Europe. Composed between 1200 and 1210, it combines the Arthurian material of Celtic origin with the religious subject- matter of the Holy Grail. The central question that hereby emerges is how a world torn apart by contradictions and conflicts can again be rendered whole. Within the fictitious garb of the Parzival-romance Wolfram confers upon this question a shape that transcends time, which gave rise to intense interest on the part of listeners and readers. The sheer number of medieval manuscripts preserving Parzival today speaks for itself (16 manuscripts that have preserved the entire text, 70 fragments, and a print dating from 1477). Ever since the late eighteenth-century revival of interest in the vernacular poetry of the Middle Ages, modern literary scholarship has concerned itself with Wolfram's Grail romance. The interpretations that have been arrived at are as varied as they are controversial. Exegesis has, however, been based upon an edition which, although a masterpiece of its time, can no longer meet today's expectations. Karl Lachmann's Parzival edition of 1833 formed the standard basis for interpretation for generations of Germanists, but recent scholarship is agreed upon the necessity for a new edition, and has become increasingly discontented with working with a text that is generally acknowledged to be in need of revision. The challenge presented to the editor of Parzival also affects central problems in the theory of medieval philology today. Worthy of note in this context are phenomena such as the relationship between oral performance and its literary codification, the ensuing variability of medieval texts, as well as concepts of authorship and transmission, and their effects upon the way in which a text is presented. To put it in its simplest terms, scholarly debate hinges upon two pivotal positions, which may be denoted by the keywords New Philology and New Phylogeny: New Philology emphasises the variety in transmission and the ensuing instability of medieval texts. Its tendency is to undermine the hierarchy of individual manuscript sources in the interest of the fundamentally variable, unstable status of medieval manuscript culture. New phylogeny, by contrast, clings to manuscript interrelations and groupings as the basis for the critical determination of the text. The concept of \"phylogeny\", which derives from evolutionary biology, denotes the race-history of breeds. Recently it has been applied to questions of manuscript interrelations. Research on Chaucer, for example, has attempted, in an article published in the magazine 'Nature' which attracted great attention, to establish the 'Phylogeny of the CanterburyTales'. A new critical edition of Parzival will have to come to terms with the abundance of variant readings and the not inconsiderable problems of establishing a text against the methodological background of the polarity of New Philology and New Phylogeny. A challenge voiced in the Parzival scholarship of the 1960s now seems more relevant than ever before. It was then argued that it was necessary \"to publish all the material that was collected for critical assessment before the question of manuscript interrelation could be clarified\" (E. Nellmann). Perhaps the idea, when it was voiced in 1968, had a Utopian ring. Today, however, it can be put into practice, step by step, with the aid of computer technology, and at reasonable expense. A critical electronic edition of the manuscript sources would constitute a work-base that would be an indispensable prerequisite for any new edition of Parzival. The possibilities offered by the synoptic representation of the manuscript sources on screen can be illustrated by reference to a short extract from the Parzival prologue (see illustration below). The screen presentation created by an internet browser shows above, in the left window, a normalised text, based on the main manuscript D. In the window on the left below is the apparatus of variants relating to this text. The windows on the right contain the transcriptions and facsimiles of the various manuscript sources. All the windows are internetted by hypertext-links and permit users an interactive interchange between base-text, apparatus of variants, transcriptions and facsimiles.  There is no doubt that, on the screen, the variability postulated by New Philology can be presented in much more lucid, visual terms than in conventional editions of texts. The critical apparata of the traditional kind generally only present readings in punctual fashion, reproducing word-for-word variants. On the screen, however, the variety of readings in the manuscripts, in context, can be encompassed. The second important advantage of electronic display lies, however, in the presentation of manuscript groupings advocated by New Phylogeny. In this context, computer programmes open new fields of experiment and accelerate analytical processes. They facilitate the flexible disposition of manuscript groupings and enable the rapid revision of philological judgements concerning base manuscripts and stemmatological interrelations. Thus electronic display enables a synthesis of philological positions, which at first sight appear contradictory. Such a synthesis offers a work-tool, and an indispensable prerequisite for any future critical edition of Parzival. At the same time, the electronic display amounts to a form of edition which has its own peculiar nature and justification. Its concept results from the discussion concerning New Philology in the last decade, and leads this discussion towards a pragmatic editorial solution. From this a new Parzival edition can emerge, which, up to a point, enables its users to participate in the editorial process, and leaves them the freedom to decide between different textual variants and the form in which they are transmitted in the manuscripts. The manuscript data produced by this process would be of interest to both literary and linguistic historians. In employing this electronic medium, users are embedded in a century-old process of transmission - from the post-Gutenberg era they go back to the age before Gutenberg. Here the cultural and scholarly relevance of electronic editions of medieval texts becomes evident: they merge with a development in historical scholarship which is increasingly concerned with the mediality of manuscript transmission, as well as with questions of discourse analysis and anthropology. Political historiography, concerned with the great events of history, and social history, defined in relation to human labour, has yielded place to aspects of mediation, transmission and the preservation of historical data. The 'homo laborans' thus yields his place to the 'homo tradens' of historical anthropology. This new trend may in turn favour a culturally based 'rephilologisation' of linguistic and literary scholarship.  ",
       "article_title":"New Philology and New Phylogeny. Aspects of a critical electronic edition of Wolfram's 'Parzival'",
       "authors":[
          {
             "given":"Michael",
             "family":"Stolz",
             "affiliation":[
                {
                   "original_name":" University of Basle ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction This paper illustrates how humanists' XML documents from around the web can be shared and integrated using careful schema design and simple XSLT programs. In the web publishing world, resource sharing and syndication are all the rage. [1] The latest RSS specification, composed in RDF, is documented at , and a portion of the O'Reilly network is devoted to the topic. Using XML-based methods like the RDF Site Summary (RSS) or Channel Definition Format (CDF), sites can reuse and share their headlines and stories across the web. oreillynet.com's Meerkat is an impressive example of the power of this technology. Computerists in the humanities have applied less effort to syndication and document sharing, but as scholarly markup begins to describe realms of knowledge such as archaeology [Schloen] Schloen, J. David. \"Archaeological Data Models and Web Publication Using XML\". Computers and the Humanities 35. 2001., anthropology , geography or history, whose content is impossible to confine, and as XML namspaces make it possible to blend and reuse documents one within another [Bosak] Bosak, John. \"XML Ubiquity and the Scholarly Community\". Computers and the Humanities 33. 1999., it is clear that we need to build into our markup endeavours the means of integrating resources from around the web. As briefly illustrated in a paper at ACH/ALLC 2001, this has been a facet of the research of the Historical Event Markup and Linking Project (Heml) from its inception two years ago. Heml aims to provide the means to coordinate historical information on the web though a markup language that describes historical events and through transformations of conforming documents into historical maps, timelines and tables. After defining its markup language in XML schemas (version 2001 07 02) and producing conforming documents outlining Roman Republican history, Heml has turns its attention to exploring the problems and opportunities offered by a semantic web of disparate documents across the web [Berners-Lee] Berners-Lee, Tim. \"The Semantic Web\". Scientific American. May, 2000.. Two conclusions have been drawn from this work: first, rich scholarly markup like Heml has requirements in content integration that are more complex than the problems which the current syndication techniques aim to solve; and that, secondly, these requirements can be met using simple and ubiquitous computational tools.    The Difficulties of Content Integration The more complex content integration requirements of Heml and similar schemes can be illustrated with the example of collecting recipes. Syndication schemes in common use today gather XML content like recipes in a file-box: documents or links are placed alongside each other creating a larger document seriatim. However, as illustrated in Figure 1, this process fails to recognize the overlapping identities between documents of richly marked-up content: the id attributes flour and apples appear twice, confusing matters and possibly causing the document to no longer be valid.   Figure 1. Gathering Documents Seriatim  Figure 2 illustrates the preferred outcome. This process gathers the recipes as they are in a cookbook, where ingredients are identified properly with each other and so would, for instance, appear only once in an index.  Figure 2. Integrating Content  Content integration of Heml documents has to be of the second, more complex sort because its schema is built largely upon the identification of entities through id and idref attributes. In brief, Heml markup comprises a series of event elements; each of these includes information about participants, chronology, locations, keywords and supporting documents. Example 1 is a Heml fragment that identifies the location `Rome.' Once defined in this way, subsequent references to this same location within the document use an XML reference element , thus: <LocationRef idref=\"Rome\">. These corresponding  Concept  and  ConceptRef  elements are a design feature of Heml markup: no elements are defined with names ending in the string Ref, except those that have idrefs which are meant to refer to elements with the corresponding name.  Example 1. heml:Location element  <Location id=\"Rome\">  <LocationLabelSet> <Label xml:lang=\"en\">Rome</Label> <Label xml:lang=\"la\">Roma</Label> <Label xml:lang=\"el\">Ῥώμη</Label> </LocationLabelSet>  <Latitude> <GeographicalHourLatitude>41</GeographicalHourLatitude> <GeographicalMinute>49</GeographicalMinute> <GeographicalSecond>2</GeographicalSecond> </Latitude>  <Longitude> <GeographicalHourLongitude>12</GeographicalHourLongitude> <GeographicalMinute>19</GeographicalMinute> <GeographicalSecond>8</GeographicalSecond> </Longitude> </Location>    Design Goals There are many ways to approach the problem of integrating such materials; some approaches have the virtue of not requiring metadata at all. The following design goals dictated the Heml project's solution: 1. Each constituent document and the resulting integrated document should be intelligible and valid. 2. It should be possible to refer to constituent documents through URLs. 3. A third party (that is, someone who is not the creator of any of the documents and has no influence in their design) should be able to integrate documents as satisfactorily as their authors. 4. The integrating code should be as portable as possible, ideally running on clients or servers. 5. The integration process should be recursive so that its results can be the input of a further integration process.    Solution The Heml projects has fulfilled these goals through a metadata file and an algorithm implemented in the XSLT XML document transformation language. The meta-documents that control this process have been named 'Jackdaw' documents, since like their namesakes they gather and put to use disparate materials. [2] A schema has not been prepared for the Jackdaw documents because they are still experimental. I imagine eventually generalizing the concept and expressing it in RDF Example 2 is a jackdaw file that controls the integration of documents relating to the history of Rome down to 201 BCE. Reading from the bottom of the document up, the <filelist> element collects URLs that refer to documents whose content this Jackdaw integrates. Further up, <IdEquivalence> elements identify a <Master> element document with one or more <Duplicate> document elements. In our example the element identified as 'Rome' in the second punic war.xml document is listed as the master of similarly named elements in the other two. The XSLT code that operates on the Jackdaws can be obtained[3] This URL corresponds to the directory path of the file; if the XSLT code is reorganized, the URL will fail. In that case, follow links from the Heml homepage to the CVS viewer and the getDocument XSLT file. on the Heml CVS server. Though the Heml project presently organizes its XSLT transformations using the server-side Cocoon2 engine from the Apache group, advanced web browsers -- Microsoft Explorer 5.5 and greater and most builds of Mozilla 0.9.5 and greater -- perform XSLT transformations on documents that include the proper XML processing instruction tags. The integrating algorithm is simple and based on the assumption that document URLs are unique and that ids are always unique within their document. A function that concatenates an input id with its document's URL is therefore also assumed to be unique in the integrated output document.  The integrating algorithm blindly copies all <Event> elements and their children from every document addressed in the <file> elements, except that it generates new id or idref attributes based on the URL and id of the old ones. Furthermore, if an idref points to an element whose id is among those listed in as an <IdEquivalence><Duplicate>, the new id of the corresponding <IdEquivalence><Master> is output instead. If an id is among those listed as an <IdEquivalence><Duplicate>, the XSLT generates the appropriate  ConceptRef  element and gives it the idref attribute generated from the corresponding<IdEquivalence><Master>.  Example 2. `Jackdaw' Metadata file  <Jackdaw>  <IdEquivalences> <IdEquivalence>  <Master id=\"http://localhost:8080/heml-cocoon/source/second_punic_war.xml#Rome\"/>  <Duplicate id=\"http://www.java.utoronto.ca/~brucerob/early_history.xml#Rome\"/>  <Duplicate id=\"http://heml.mta.ca/~brucerob/first_punic_war.xml#Rome\"/> </IdEquivalence> <IdEquivalence> <Master  id=\"http://localhost:8080/heml-cocoon/source/second_punic_war.xml#Carthage\"/>  <Duplicate id=\"http://heml.mta.ca/~brucerob/first_punic_war.xml#Carthage\"/> </IdEquivalence>  </IdEquivalences> <filelist> <file>http://localhost:8080/heml-cocoon/source/second_punic_war.xml</file> <file>http://heml.mta.ca/~brucerob/first_punic_war.xml</file> <file>http://www.java.utoronto.ca/~brucerob/early_history.xml</file>  </filelist> </Jackdaw>   This process is reasonably speedy. As a functioning example for this paper I ran the jackdaw file in Example 2 using Apache's Xalan 2.0 XSLT engine on a Celeron 400-class machine running RedHat Linux 7.2 and the IBM 1.3-8.0 Java2 SDK. With all caching switched off, it took under ten seconds to gather the resulting 35k document; and three of these seconds appear to be overhead required just to start the java processes. ( xsltproc, an XSLT engine written in C, completes the same task in 1.7 seconds!) Network access seems to be the limiting factor for these reasonably small files. Of course, the resulting integrated XML document is usually invisible to the user, who navigates one or more further transformations of that document into HTML or images. Among the transformations available for Heml documents is a dynamic SVG map. (A browser plugin, available from Adobe for Windows and Macintosh, is required to view this image.) Passing the cursor over a dot on the map will bring up the name of the location and a list of events that took place at that location. It can be seen that the lists for Rome and Carthage include events from all periods, as instructed in the Jackdaw file in Example 2.    ",
       "article_title":"XML Content Integration: An Example from the Heml Project",
       "authors":[
          {
             "given":"Bruce",
             "family":"Robertson",
             "affiliation":[
                {
                   "original_name":" Mount Allison University, Canada ",
                   "normalized_name":"Mount Allison University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/03grc6f14",
                      "GRID":"grid.260288.6"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Our immediate area of concern is the problem of providing a clear, explicit account of the meaning and interpretation of markup. Scores of projects in humanities computing and elsewhere assume implicitly that markup is meaningful, and use its meaning to govern the processing of the data. A number of authors have described systems for exploiting information about the meaning or interpretation of markup; among those most relevant to the work described here are Simons (1997 and 1999), Welty and Ide (1999), Ramalho et al. (1999), and Sperberg-McQueen et al. (2000 and 2001). While a complete account of the \"meaning of markup\" may seem daunting, at least part of this project appears manageable: explaining how to determine the set of inferences about a document which are \"licensed\", implicitly or explicitly, by its markup. This may or may not be all there is to an account of the meaning of markup, but it is certainly a significant part of any account -- and doesn't seem, on its face at least, to be particularly difficult to provide. However, it proves remarkably difficult to find, in the literature, any straightforward account of how one can go about interpreting markup in such a way as to draw all and only the correct inferences from it. For the most part, the papers cited have limited themselves to describing how such a system would work in theory, rather than describing a practical instantiation of the idea. Designers and users of markup systems, meanwhile, have relied on human understanding rather than on automated systems to interpret the markup in documents. Sperberg-McQueen et al. (2000) describe at some length a 'straw man' proposal for defining the proper interpretation of markup in a given markup language. In particular, they identify the meaning of markup in a document as the set of inferences authorized by it, and propose that it ought to be possible to generate those inferences from the document mechanically, following certain simple rules. Having set the strawman up, they then proceed to dismantle it, noting a number of problems in the rules they have proposed for generating the inferences. They sketch in general terms how a better account of meaning and interpretation in markup could be constructed, but leave the actual construction as an exercise for the reader. This paper describes a concrete realization of one part of the 'framework' model proposed in Sperberg-McQueen et al. (2000), and outlines some of the problems encountered in specifying the inferences licensed by commonly used DTDs. (Other parts of the framework model are also being worked on, but the description of that work is out of scope for this paper.) We focus here on the development of a notation (specifically, an SGML/XML DTD) for expressing what Sperberg-McQueen et al. (2000 and 2001) call \"sentence skeletons\", or \"skeleton sentences\". These are sentences, either in English or some other natural language or in some formal notation, for expressing the meaning of constructs in a markup language. They are called sentence skeletons, rather than full sentences, because they have blanks at various key locations; a system for automatic interpretation of marked up documents will generate actual sentences by filling in the blanks in the sentence skeletons with appropriate values from the documents themselves. In existing markup systems, the appropriate values are often to be taken from whatever occurs in the document at a specific location. In the TEI DTD, for example, the current page number for the default pagination is given by the value of the 'n' attribute on the most recent 'pb' element, while the identifier for the language of the text is given by the value of the 'lang' attribute on the smallest enclosing element which actually has a value for the 'lang' attribute; the language itself is described by whatever 'language' element in the TEI header has that identifier as the value of its 'id' attribute. In the sentence skeleton, therefore, we need to label each blank with some expression which describes how to derive the appropriate value to be used in the sentences constructed from this skeleton. Since these expressions typically \"point\" to other nearby markup structures, Sperberg-McQueen et al. refer to them as \"deictic expressions\"; they express notions like \"the contents of this element\" or \"the value of the 'lang' attribute on the nearest ancestor which has such a value\" or \"the value of the 'type' attribute on the nearest ancestor of type 'div'\". We will describe theoretical and practical problems arising in using sentence skeletons to say what the markup in some commonly used DTDs actually means, in a way that allows software to generate the correct inferences from the markup and to exploit the information. These include the following questions among others. What formalism should be used to write the sentence skeletons? We can easily adopt some existing formalism, e.g. that of Prolog or whatever inference engine we choose to use; can we devise a formalism that will not commit us to a particular inference engine? There appears to be a significant difference among (a) sentence skeletons which serve to formulate in some formal notation the specific facts expressed by the markup in the document, (b) sentences or sentence skeletons which express invariant rules about specific properties captured by the markup (e.g. \"The value of the 'lang' attribute is inherited; the value of the 'n' attribute is not inherited\") and (c) sentences or sentence skeletons which express invariant rules about textual and other constructs (e.g. \"The author of a letter is physically located at the place given in the place-date line, on the date given in the place-date line, unless the letter is falsified or forged in some way\"). Sentences in group (b) serve to capture useful generalizations about the way markup constructs in a given DTD behave; sentences in group (c) are important for certain kinds of inferences, but appear to have relatively little to do with the markup itself. What is the best way to reflect these differences in function among the sentences and sentence skeletons of a markup system? What is the most convenient way to generate the inferencs licensed by markup? Possibilities include ad hoc software, XSLT transformation sheets, and Prolog or other inference tools. Our experience suggests that it may be worthwhile to distinguish different classes of inferences, and to use different software to derive the different classes. In sentence skeletons like \"[this element] is in English\", what do the deictic expressions like \"this element\" actually refer to? In some cases the inferences appear to relate to the linguistic components of the text itself (\"This document is written in English\"), and in some cases to the text's formal properties (\"Augustine's Confessions is divided into 13 books\"). In some cases, the markup appears to license inferences about some object or entity in the real world (\"Henry Laurens was in Charleston on 18 August 1775\"), but sometimes the entities referred to are not at all in the real world (\"Harry Potter missed the Hogwarts Express on 1 September\"). In still other cases, the inferences appear to apply to the electronic encoding of the text itself (\"This metadata was last revised on 20 July 1998\") or to some other witness to the same text (\"The recipient's copy of this letter is preserved in [some particular archival collection, with some particular call number]\"). Attempting to disentangle these lands the would-be formulator of sentence skeletons promptly in a thicket of ontological questions which have not yet received adequate attention. The ontological questions become even more thorny in connection with markup systems like that of the Text Encoding Initiative, which are intended for use by a wide variety of projects which are expected to have widely different views about the ontological commitments to be associated with the TEI markup. Do statements about Augustine's Confessions, for example, relate to some abstract text distinct from each physical copy of the text, or is the phrase \"Augustine's Confessions\" merely a convenient shorthand for \"all the physical documents which witness Augustine's Confessions\"? It would appear essential to decide this question in order to formulate sentence skeletons for markup languages like the TEI, but the TEI itself is intentionally coy about the issue, in order to ensure that textual Platonists and textual constructivists can both use TEI markup. It is a challenge to build a similar ambiguity or vagueness into the set of sentence skeletons which document the prescribed interpretation of TEI markup.  ",
       "article_title":"Skeletons in the closet: Saying what markup means",
       "authors":[
          {
             "given":"Michael",
             "family":"Sperberg-McQueen",
             "affiliation":[
                {
                   "original_name":" World Wide Web Consortium ",
                   "normalized_name":"World Wide Web Consortium",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0059y1582",
                      "GRID":"grid.507688.4"
                   }
                }
             ]
          },
          {
             "given":"Allen",
             "family":"Renear",
             "affiliation":[
                {
                   "original_name":" University of Illinois at Champaign-Urbana ",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Claus",
             "family":"Huitfeldt",
             "affiliation":[
                {
                   "original_name":" University of Bergen ",
                   "normalized_name":"University of Bergen",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/03zga2b32",
                      "GRID":"grid.7914.b"
                   }
                }
             ]
          },
          {
             "given":"David",
             "family":"Dubin",
             "affiliation":[
                {
                   "original_name":" University of Illinois at Champaign-Urbana  ",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction The difficulty of annotating not hierarchically structured text with SGML-based mark-up languages is a problem that has often been addressed. Renear et al. (1996) discuss one of the basic assumptions about the structure of text data: the \"OHCO-thesis\", which states that text consists of an ordered hierarchy of content objects, and show that this assumption cannot be upheld consistently: A number of texts contradict this OHCO-thesis. The options to represent data which correspond to different structural hierarchies are discussed in detail by the TEI (see Barnard et al. 1995). One type of annotation mentioned by the TEI-guidelines consists of separately annotating the original data according to different document grammars. \"The advantages of this method of markup are that each way of looking at the information is explicitly represented in the data, and may be processed in straightforward ways, without requiring complex methods of disentangling information relevant to one view from information relevant only to other views.\" (Sperberg-McQueen & Burnard 1994, p.775f.) The problem, however, is that separate annotations do not allow for establishing relations between the annotation tiers. It will be shown that a model for the meaning and interpretation of documents, marked-up once, can be extended to documents concurrently marked-up with several annotations.The work presented here is described in more detail in Witt (2002).   Model of representation The model of representing complex structured text is itself based on the data to be structured. This data can be annotated according to specific document grammars. For a simple case there is one document grammar and one annotation. This means all SGML documents and all valid XML documents are represented by this model. In addition to this standard representation every document can be structured according to further document grammars. The maximal number of annotations is not restricted, so that the model permits a steady expansion of the annotations which are used. Therefore, the model can be viewed as an open model. As a result the annotated text data fuses different levels of annotation together. As a prerequisite the preparation of the data must be done in a way that a separate annotation of relevant phenomena is possible. The data which will be annotated is categorized as having a status of primary data. The primary data consists of the yet to be annotated text. Accordingly, the mark-up will be categorized as secondary data or meta-data.The terminus \"meta-data\" is preferred by the author, because \"secondary data\" presupposes a characterization as less important data. However, \"meta-data\" is far from being the ideal term: it causes an ambiguity, since it is also used for the information typically contained in the headers of annotated documents. The primary data is used in two ways: On the one hand, the primary data is the subject of several (possible) annotations, on the other hand it forms a link between the levels of annotation. It is important to note that the second way of using the primary data allows for linking the independent annotations without introducing explicit links. After the stipulation of the primary data individual document grammars can be developed and applied which each pertain to a single level of annotation. For the annotation of linguistic data this could be e.g. the level of morphology, the level of syllable structures, a level of syntactic categories (e.g. noun, verb), a level of syntactic functions (e.g. subject, object), or a level of semantic roles (e.g. agent, instrument). As already mentioned this annotation technique data allows for the introduction of an unlimited number of concurrent annotations, but there exists a constraint: when designing the document grammar it is necessary to consider that the primary data is the link between all layers of annotation. This has a direct consequence for the modelling process: Even if parts of the primary data are irrelevant for one of the tiers of description, the data must exist as primary data in the annotation. This contradiction can be solved by introducing a special mark for this irrelevant primary data in the document grammar according to the phenomena. This mark allows one to represent the corresponding passages as primary data in a technical way. Such a mark can be done for example by using the element <ignore>. While interpreting the distinguished data these marked sections will be filtered out. Durand et al. (1996) already discussed a similar solution where a classification on a more abstract level of representation should happen. The compilation of document grammars which are used for different annotations, can be seen as a pool of individual units or as a collectively structured source of knowledge. Not only the structure of document grammars, but also the schema language used (e.g. DTD, XSchema) are irrelevant to the process of annotations.   Knowledge representation of annotated text In general, annotated text consists of content and annotations. Annotations are used on a syntactical level. Therefore they are used for assigning a meaning to (parts of) a document. While developing a document grammar the focus should be centred on the content. This point of view is expressed by Sperberg-McQueen, Huitfeldt and Renear (2000). They show how knowledge which is syntactically coded into text by annotations can be extracted by knowledge inference. After summarizing this approach, it will be shown, how this technique can be expanded so that it can be used for inferences of separately annotated and implicitly linked documents - documents marked-up according to different document grammars.   Documents Sperberg-McQueen et al. (2000) regard annotated documents as a compilation of knowledge which can be represented and can be used for inference. Illustrating their approach they use the programming language \"Prolog\" to represent this knowledge. A XML-Document without cross relations can be represented as a tree. A representation of annotated text in Prolog looks like the following: node([x, y, z], element(gi)).  attr([x, y, z],attr-name, 'att-value'). A predicate with two arguments, called node (or short notation in Prolog node/2), is used. The first argument is a list of digits to identify each node of the tree representation of documents. The second argument is more complex. It consists of the functor element or pcdata and an argument for the name of the element or the textual content. Furthermore, a predicate of three arguments called attr/3 is used for representing attributes. Attributes are also related to nodes (argument 1). They have a name (argument 2) and a value (argument 3). Such a representation of a document is a very good basis for automatic inference of relations which exist between the nodes. If e.g. infer/2 is called with an address, all active features can be printed out: infer(Property,[x, y, z]).  Property = a;  If infer/2 is called with a concrete feature and a variable at the place of the address, all locations are printed out which apply to these features. Other frameworks for knowledge representation of annotated documents also exist. Welty and Ide (1999) introduced an approach, where the knowledge of documents is pasted into a knowledge representation system. However, so far it has not been shown how to use differently annotated documents as a base for inferences.   Relations between separate annotations The described model of knowledge representation can only be used for single documents. However, it will be shown, that this model can easily be expanded, so that it is applicable for the inference of relations between several separately annotated XML-documents with the same primary data. In order to use the model for several different annotations the representation of an absolute system of references must be introduced. This absolute reference system is already given in the data: the parts of text which are saved as \"PCDATA\", i.e. the primary data as defined above, is ideally suited for this task. This means that the string which is identical in all documents serves as the absolute system of references. The primary data forms a string of a fixed length. The representation of the annotation tiers is an absolute basis of relations. This basis must renounce the predicates node/2 and attr/3 in favour of the predicates node/5 and attr/6. The three additional arguments are used for the reference on the level of annotation (comparable to the concept of namespaces in XML), for marking the start and the end of the annotated textual content. The extension of the original model in this way allows for inferences of relations between different concurrent annotations, e.g. regarding the separate annotation of morphemes and syllables might show that these units are not compatible.Other relations of concurrent markup can be, for example, compatibility, identity, and inclusion. Since the arguments originally introduced are reused, all the inferences of the original model are still possible.   Advantages and Perspectives The outlined architecture has many advantages. The model allows for structuring text according to multiple concurrent document grammars without workarounds. Furthermore additional annotations can be subsequently included, without changing already established annotations. The annotations are on the one hand independent of each other, on the other hand they are interrelated via the text, allowing for the inference of relations between different levels of annotation. The final advantage to be mentioned is that the compatibility of several or all annotations used can be proven automatically. This can be done using a technique originally developed within linguistics, namely unification.   ",
       "article_title":"Meaning and interpretation of concurrent markup",
       "authors":[
          {
             "given":"Andreas",
             "family":"Witt",
             "affiliation":[
                {
                   "original_name":" Universität Bielefeld ",
                   "normalized_name":"Bielefeld University",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/02hpadn98",
                      "GRID":"grid.7491.b"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Recently Sperberg-McQueen and others have argued that markup functions by licensing inferences about a text. They remark, however, that the information warranting such inferences may not be entirely explicit in the syntax of the markup language used to encode the text. (Sperberg-McQueen et al., 2001) For example, a language defined in SGML or XML may include an attribute (such as 'lang') that an encoder may apply to an element with the generic identifier 'QUOT.' One might then infer that the QUOT element marks an identifiable component of the document (called a quotation) and that the quotation has the property of being in a particular language (as indicated by the 'lang' attribute). It may also be valid to infer that children of the 'QUOT' element share the property of being in that language, unless overridden with a language attribute of their own. On the other hand, there may not be such a simple one-to-one mapping between components and elements: for example, a single quotation may be broken across two or more 'QUOT' elements. There are a number of other inferences that are typically assumed by tag set designers and application designers alike, but which cannot be formally expressed in the DTD, and may or may not be informally expressed in the tag set documentation. In order to adequately represent such inferences (the \"meaning of markup\") the Sperberg-McQueen group developed techniques for expressing in predicate logic, (i) the facts signalled by the encoding of a particular document instance and (ii) the logical relationships commonly understood to exist and license further inferences. A Prolog database was used to demonstrate the effectiveness of this approach. The present paper builds directly on this previous work, and reflects new results which provide more rigorous and explanatory layers of abstraction and progress in understanding problems with \"deictic\" expressions and domains of variables, etc. But the fundmental new result presented is the completion of a complete integrated working system with an entirely new and substantially redesigned Prolog database at its core. This Prolog database has been redesigned to improve functionally, better reflect the theoretical results, and increase functionality, flexibility, and performance. The system permits an analyst to specify facts about the markup syntax (e.g., generic identifiers and attribute values) separately from facts and rules of inference about semantic entities and properties. The system provides a level of abstraction at which the performative or interpretive meaning of the markup can be explicitly represented in machine-readable and executable form. Inferences can then be drawn regarding document components, including problematic structures, such as those participating in overlapping hierarchies. The new Prolog database is integrated with an SGML/XML parser so that SGML and XML instances can be input and output. Facts and rules of inference concerning the document are expressed in Prolog's standard declarative syntax. We have developed a collection of predicates that emulate a subset of the W3C's Document Object Model methods for navigating the hierarchical structure of nodes, and retrieving attribute values and information from the document type definition. These predicates afford a clear separation of the syntactic information captured by the parser and the document semantics expressed by the analyst. Another collection of predicates support deictic expression resolution. These allow rules of inference to include location-relative pointing from one part of the document to another. For example, we have predicates for resolving an element's closest ancestor having a particular generic identifier, attribute, or attribute value pair. Another set of predicates resolves the identity of an element of a particular type occurring most closely in terms of the linear structure of the document (rather than the closest in the hierarchy). A third set of predicates supports the tracing of connections across elements, such as those linked by ID and IDREF attribute pairs. Rules (axioms) represent the further logical relationships mentioned above, such as for defeasible inheritance, distribution of distributive properties, etc. In developing the architecture of this system, we have adopted an object-oriented strategy: each node identified by the parser and semantic entity instantiated via a rule of inference has a unique identifier assigned by the system. Predicates for retrieving or manipulating that information are written with the aim of hiding the underlying data structure. The system architecture can therefore be understood to have several distinct layers of representation: 1. A parser that handles the serialized document instance. 2. Predicates for processing the output of the parser. 3. Predicates for storing a representation of the parse tree in the Prolog database. 4. Predicates for emulating DOM methods, deictic expression resolution, object instantiation, and general characteristics of properties (such as their inheritance and distribution). 5. Facts and rules of inference expressing the document semantics.  The first two layers are implementation-dependent, and are designed to be modular, allowing experimentation to improve robustness and scalability. We intend the upper layers to be consistent across different implementations of the system. Currently the interface between the lower and upper layers in written entirely in Prolog, but we may employ other technologies (such as XSLT) in the future. This system is being developed with the aim of advancing several interrelated research goals. We will develop applications that provide a complete formal account of the semantics of particular document classes. The system will also provide an environment for experimenting with proposed or conjectured semantics with the goal of improving document retrieval and conversion applications. We also propose to build applications that draw inferences based on both document semantics and domain or world knowledge. At this time the new Prolog database has been completed and tested and the entire working system can be demonstrated on small fragments of TEI. By the conference we hope to have completed the representation of the markup semantics of two XML systems (XHTML and TEI-lite), which will allow us to be able to present a substantial demonstration of the practical advantages of representing markup semantics, as well as the theoretical soundness of this approach to the meaning of markup.  ",
       "article_title":"A logic programming environment for document semantics and inference",
       "authors":[
          {
             "given":"David",
             "family":"Dubin",
             "affiliation":[
                {
                   "original_name":" University of Illinois at Champaign-Urbana ",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Michael",
             "family":"Sperberg-McQueen",
             "affiliation":[
                {
                   "original_name":" World Wide Web Consortium, USA ",
                   "normalized_name":"World Wide Web Consortium",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0059y1582",
                      "GRID":"grid.507688.4"
                   }
                }
             ]
          },
          {
             "given":"Allen",
             "family":"Renear",
             "affiliation":[
                {
                   "original_name":" University of Illinois at Champaign-Urbana ",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          },
          {
             "given":"Claus",
             "family":"Huitfeldt",
             "affiliation":[
                {
                   "original_name":" University of Bergen, Norway ",
                   "normalized_name":"University of Bergen",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/03zga2b32",
                      "GRID":"grid.7914.b"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The impact of SGML and XML on computing in the humanities is wellknown. Markup languages have been used on such diverse texts as Nordic runes (Driscoll Driscoll, M. J., Encoding Old Norse-Icelandic primary sources using TEI-conformant SGML/XML: A handbook ()), to the Oxford English Dictionary(New OED and Text Research UW Centre for the New OED and Text Research (ptr target=\"http://db.uwaterloo.ca/OED/\" />). Substantial experience has been gained in the application of markup languages to diverse texts with corresponding gains in the usefulness of such texts for computing humanists. The default use of SGML for the representation of only one hierarchy, and the explicit prohibition of overlapping elements in XML, has posed considerable limitations to humanists attempting to record their observations of multiple overlapping hierarchies in textual materials. Several approaches have addressed these limitations, ranging from milestone elements(TEI), stand-off markup to record hierarchies (Thomspon 1997Thompson, H.S., and McKelvie, D, Hyperlink semantics for standoff markup of read-only documents, Language Technology Group, HCRC, University of Edinburgh()), fragmentation of a base text (TEI), to the development of non-SGML/XML markup languages which will require special processing software TexMECS: New meta-language for recording multiple hierarchies (Sperberg-McQueen and Huitfeldt 2001Huitfeldt, C., and Sperberg-McQueen, C.M., TexMECS: An experimental markup meta-language for complex documents ()).All of these approaches suffer from a variety of limitations and problems. The most notable problem is a lack of commonly available software to create and make the overlapping hierarchies available to scholarly investigation. Durusau and O'Donnell (2001Durusau, P., and O'Donnell, M.B., Implementing Concurrent Markup in XML, Extreme Markup 2001. Paper submitted to Markup Languages: Theory and Practice. Presentation materials andworking examples available at .) present a method for recording overlapping hierarchies using XML standards, particularly XPath and XSLT. The essential insight is that the key piece of information in encoding a complex text is membership of a particular piece of text (PCDATA in SGML/XML terminology) in a given hierarchy, and not its surface representation through markup. Whether this membership information is represented in a conventional SGML/XML \"tree\" or by some other means, it is the information that is of interest to the computing humanist. This technique relies upon the use of XPath syntax to record the hierarchy information for atoms of text in a larger work. For example: <w id=\"w1\" sn:clauses=\"/clauses/clause[1][@id='c1']/s[1]/w[1]\" tx:text=\"/text/para[1][@id='p1']/w[1]\" pg:pages=\"/pages/page[1][@id='p1']/line[1][@id='l1']/w[1]\" >This</w> Durusau and O'Donnell demonstrated the use XSLT to construct and then query overlapping hierarchies of XML markup on a base text.The theory and practice of querying of XML documents has been under rapid development at the World Wide Web Consortium (XQuerySee ). Despite that pace of development, there has been no systematic investigation of querying texts across multiple hierarchies. However, most of the techniques currently under development recognize the importance of XPath syntax on the effective formulation of an XML query language. To develop effective querying of XML documents across multiple hierarchies, Durusau and O'Donnell are developing techniques to visually present overlapping hierarchies to allow \"discovery\" of fruitful lines of investigation for particular texts. XSLT andSVG are used for this purpose. Overlapping hierarchies may well exist for a text that are of no interest to one researcher that are the focus of research for another. The ability to effectively choose which hierarchies are of interest will be crucial to effective use of overlapping hierarchies recorded for a particular text. In addition to allowing a separation of hierarchies of interest from those that are not, the visual presentation of overlapping hierarchies is also the first step towards addressing the need for intimate knowledge of the underlying markup to formulate a query for the multiple hierarchies recorded using this technique. Effective queries now require mastery of the various hierarchies and a working knowledge of their relationships, one to the other, in order to obtain any predictable results. With an entirely visual presentation of the overlapping hierarchies, scholars will be able to choose the hierarchies by display and not markup syntax, as being the subject(s) of further analysis. This paper demonstrates the use of XSLT and SVG to produce visual representations of documents with overlapping hierarchies and the linking of these visualizations to XPath and XQuery statements. Multiple editions, annotations and analyses of Book 1 of Milton's Paradise Lost form the test case for these techniques. The more detailed the analysis the greater the need for detailed knowledge of the syntax but that will be only after the scholar has found information of interest and not a general exercise in mastering several divergent markup regimes. This will be of particular importance with many texts such as biblical texts that have complex syntax, grammatical, literary and transmission hierarchies that overlap and combine in complex ways.  ",
       "article_title":"Visualizing Overlapping Hierarchies in Textual Markup",
       "authors":[
          {
             "given":"Patrick",
             "family":"Durusau",
             "affiliation":[
                {
                   "original_name":" Society of Biblical Literature ",
                   "normalized_name":"Society of Biblical Literature",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0029qfe93",
                      "GRID":"grid.447552.5"
                   }
                }
             ]
          },
          {
             "given":"Matthew",
             "family":"O'Donnell",
             "affiliation":[
                {
                   "original_name":" OpenText.org ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   1 Introduction  SGML, and increasingly XML are the prevailing standards for text encoding today. XML, a simplified subset of SGML, is commonly believed to become the new standard for web publishing. One of the strengths of SGML/XML is that it combines a labelled bracketed notation with an intuitive document structure and a powerful formalism for constraining that structure. All SGML/XML like systems, however, have problems representing non-hierarchical textual features, because they require elements to nest. In practice, various techniques for encoding non- hierarchical features have been developed, including the use of milestone elements, fragmentation of elements, the SGML feature CONCUR, and various other techniques for recreating 'virtual elements' from fragments of the text. The problem with all of these approaches however, is that they require application- specific logic to retrieve the intended structure. SGML/XML processors are not sufficient. MECS is a text encoding system designed by Claus Huitfeldt at the Wittgenstein Archive in Bergen that was specifically designed to enable elements to overlap. [3] MECS, on the other hand, has no defined data structure. The MLCD (Markup Language for Complex Documents) project was started in February 2001, and is hosted by the Center for Humanities Information Technology at the University of Bergen. It aims at developing a text encoding system combining the best of SGML/XML and MECS. This work has resulted in a notation for a markup meta-language for complex documents, TexMECS [2], and a data structure for complex documents, GODDAG. This paper will present the data structure and its implementation.   2 A computational model for MLCD In [1], Huitfeldt and Sperberg-McQueen describe a data structure for overlapping hierarchies. The GODDAG, or Generalised Ordered-Descendant Directed Acyclic Graph, is defined as a labelled directed acyclic graph. As for the tree structures of SGML/XML documents, leaf nodes represent the character data content of the document, non-terminal nodes represent elements and are labelled with the generic identifier. The set of leaf nodes is referred to as the graph's frontier. An arc indicates a parent-child relationship, modelling containment. A node is said to dominate all nodes reachable from it. There is no requirement that the graph be rooted. MECS, and TexMECS allow elements to overlap arbitrarily. Overlapping elements are represented in the GODDAG as nodes with shared children. All elments will dominate unique and contiguous sequences of leaf nodes. TexMECS has a specific syntax for encoding elements that are interrupted by other material. In the TEI a part attribute is sometimes used to indicate that the linear form is in some way incomplete, and that more than one element constitutes the entire element. It would be useful if these elements could be treated as one element on the GODDAG level, for instance to facilitate proximity search. In the GODDAG the whole element is represented by one node, which is the parent of the fragment elements. <sp who=\"AASE\"><l part=\"i\">Peer, you're lying! <l><sp> <sp who=\"PEER GYNT\"> <stage>without stopping<stage> <l part=\"f\">No, I'm not!<l><sp>  See Figure 1 for the GODDAG structure.  Figure 1  In the TEI there are several examples of elements used to recreate 'virtual elements' from fragments of the text. (C.f.<join>, <span>,<fs>.) TexMECS also includes a specific syntax for so-called virtual elements, elements that are created by fragments of the text that may be discontiguous or out of order. A virtual element in TexMECS has an element identifier and an id reference. In the GODDAG, the virtual element is labelled with its generic identifier, and is the parent of all children of the referenced element. This means that applications can access and process virtual elements like any other elements. Below is an example, in TexMECS syntax, that shows a simple reordering of lines, in one view representing a dialogue between Hughie, Luis and Dewey, in another representing the haiku they are trying to remember. {sp who=\"HUGHIE\"{{p{How did that translation go?}p} {lg type=\"haiku\"{{l{da de dum de dum,}l} {l=frog{gets a new frog,}l} {l{...}l}}lg} }sp} {sp who=\"LOUIS\"{{p{Er ...}p} {lg{{l=new{it's a new pond.}l}}lg} }sp} {sp who=\"DEWEY\"{{p{Ah ...}p} {lg{{l=pond{When the old pond}l}}lg} {p{Right. That's it.}p}}sp} {lg{{^l^pond}{^l^frog}{^l^new}}lg}  In the GODDAG generated from this example (Figure 2), both the dialogue and the haiku are structured as part of the document, and none of the views is dominant.  Figure 2    3 Implementation A computational model of the above structure has been implemented in Java and CommonLisp, along with loaders and linearisers for TexMECS, MECS and XML. Using these, the GODDAG can function as a simple means of translating between the various encoding schemes.   4 Serialization: some issues and problems There is an obvious 1-1 relationship between the set of valid XML documents and their tree representations. In the case of TexMECS, things are more complicated. A given GODDAG structure can be serialized in different ways, depending on whether nodes with multiple parents are serialized as virtual elements or as overlap. On the other hand, there is a small class of TexMECS documents with discontinuous elements which do not seem to correspond to any GODDAG structure, if one tries to map discontinuous elements to a single node. We try to formalize the relationship between GODDAG structures and serializations and propose a solution for the aforementioned problem.   5 Conclusion The work presented in this paper shows how to implement the data structures and formalize some of the ideas presented in [1].   ",
       "article_title":"A computational model for MLCD",
       "authors":[
          {
             "given":"Paul",
             "family":"Meurer",
             "affiliation":[
                {
                   "original_name":"HIT-senteret,  University of Bergen ",
                   "normalized_name":"University of Bergen",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/03zga2b32",
                      "GRID":"grid.7914.b"
                   }
                }
             ]
          },
          {
             "given":"Kjersti",
             "family":"Bjørnestad Berg",
             "affiliation":[
                {
                   "original_name":"Department of Information Science,  University of Bergen ",
                   "normalized_name":"University of Bergen",
                   "country":"Norway",
                   "identifiers":{
                      "ror":"https://ror.org/03zga2b32",
                      "GRID":"grid.7914.b"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Background Music information retrieval (MIR) research and development strives to afford the same level of access to the world's corpus of music as is afforded to text. Recently concluded international symposia devoted to MIR research and development bear witness to the promise of powerful, robust, large-scale, and flexible search and retrieval tools designed to be used by scholars and novices alike (ISMIR 2000; ISMIR 2001). Like many other humanities computing endeavours, it has attracted a multi-disciplinary group of researchers who are applying their domain-specific expertise to the wide range of challenges inherent in MIR development. MIR researchers come from library science, musicology, music theory, music psychology, computer science, digital and traditional librarianship, audio engineering, information science, information retrieval, publishing, media studies, broadcasting, law, and business, to name only a small handful of the disciplines represented. MIR research is all the stronger for the cross-pollination of disciplinary paradigms, research methods, and technological approaches. Effective communications across disciplinary boundaries, however, are impeding the coalescence of MIR research into a coherent discipline in its own right. The nascent MIR community is plagued with two forms of communications breakdown: 1. The scattering of the MIR literature caused by a lack of bibliographic control; and, 2. The confusion brought about by the use of discipline-specific assumptions, techniques, and language throughout its literature.  My paper reports upon the background, framework, goals and ongoing development of the Music Information Retrieval Annotated Bibliography Project (). It extends and augments the preliminary explication presented in Downie (2001). This project is being undertaken to specifically address and overcome the bibliographic control and communications issues plaguing the MIR research community. However, since most humanities computing projects also draw upon multi-disciplinary groups of researchers, I to hope provide and delineate a model that should assist others in facilitating efficient and effective communications among their particular research communities.   Problem Overview MIR research has no disciplinary home. Because of this, researchers have been publishing their findings within the contexts of their own disciplines. Important MIR papers are thus scattered, for example, across the computer science, library science, musicology, humanities, and audio engineering literatures. There is no comprehensive indexing tool that successfully gathers up these papers. For example, significant musicology-based advances can be located through various music, arts and humanities indexes but not through the engineering and computer science indexes. Advances in audio engineering techniques are similarly absent from the music, arts and humanities indexes. Since researchers are generally unaware of the differences in scope of the various discipline-based indexes, they tend to focus upon those with which they are most familiar and thus overlook the contributions of those based in other disciplines. Unfamiliarity with the wide range of vocabularies used by the various disciplines further compounds the communication difficulties by making it problematic for MIR investigators to conduct thorough and comprehensive searches for MIR materials. Until these issues are addressed, MIR will never be in a position to fully realize the benefits that a multi-disciplinary research and development community offers, nor will it be able to develop into a discipline in its own right.   Solution  Our solution to the issues outlined above centers about the creation of a Web-based, two-level, collection of annotated bibliographies (Fig. 1). The first level, or core bibliography, will bring together those items identified as being germane to MIR as a nascent discipline. Thus, the core bibliography comprises only those papers dealing specifically with some aspect of the MIR problem, such as MIR system development, experimentation, and evaluation, etc. The second level, or periphery bibliographies, comprise a set of discipline-specific bibliographies. Each discipline-specific bibliography in the set will provide access to the discipline-specific background materials necessary for non-expert members of the other disciplines to comprehend and evaluate the papers from each participating discipline. For example, an audio engineering bibliography could be used by music librarians and others to understand the basics of signal processing (e.g., Fast Fourier Transforms, etc.). Another example would be a musicology bibliography that computer scientists could draw upon in an effort to understand the strengths and weaknesses of the various music encoding schemes, and so on. Thus, taken together, the two-levels of the MIR bibliography will provide: 1. The much needed bibliographic control to the MIR literature; and, 2. An important a mechanism for members of each discipline to comprehend the contributions of the other disciplines.   An important operating principle of the project is the use of non-proprietary formats and software. We are committed to the ideals of the Open Source Initiative (OSI, 2001) and the GNU General Public License (GNU Project, 2001) and thus intend to make our innovations freely available to others. In keeping with this commitment, we have chosen the Greenstone Digital Library Software (GSDL) package (New Zealand Digital Library Project, 2001), the Apache HTTP server (Apache Software Foundation, 2001), the PERL scripting language (PERL Mongers, 2001) and the Linux operating system (Linux Online, 2001) to create the basic technological foundation of the project. We have purchased copies of the commercial bibliographic software package, ProCite (ISI ResearchSoft, 2001) for initial, in-house, data-entry. ProCite also provides us with a representative instance of commercially available software that many end-users might utilize in manipulating the records they retrieve from our bibliography. At present, there are two central components of project undergoing development and alpha testing: 1. The bibliographic search and retrieval interface using the GSDL package; and, 2. The Web-based end-user data entry system.  For both of these, the goal is to create a system that will permit ongoing viability of the bibliography by minimizing-but not necessarily eliminating-the amount of human editorial intervention required. Item ^#1 issues being addressed include modifications to the basic GSDL system to permit the importation of specially structured bibliographic records and their subsequent access through a variety of field selection options. Item ^#2 is a CGI-based input system that guides end-users through the process of constructing well-structured bibliographic records through a series of step-by-step interactions and the on-the-fly generation of input forms specifically designed to provide the appropriate fields for the various types of bibliographic source materials (i.e., journal articles, conference papers, theses, etc.). Now that the general framework for the core bibliography has been laid, we are moving forward on the acquisition of the supplementary and explanatory materials. For these we are drawing upon the expert advice of those that have graciously signed on as advisors to the project. These advisors are not only lending their disciplinary expertise but are also affording us a very important multinational perspective on the potential uses and growth of the bibliography.   Acknowledgements I would like to thank Dr. Suzanne Lodato for helping the project obtain its principal financial support from the Andrew W. Mellon Foundation. I would also like to thank Dr. Radha Nandkumar and the National Center for Supercomputing Applications Faculty Fellows Programme for their support. The hard work and insightful advice of Karen Medina, Joe Futrelle, Dr. David Dubin, and the other members of the Information Science Research Laboratory at the Graduate School of Library and Information Science, UIUC, is gratefully appreciated. Those members of the MIR research community who have volunteered to act as project advisors are also thanked.   ",
       "article_title":"Bridging disciplinary boundaries: A case study of the Music Information Retrieval Annotated Bibliography Project",
       "authors":[
          {
             "given":"J.",
             "family":"Downie",
             "affiliation":[
                {
                   "original_name":" Graduate School of Library and Information Science, University of Illinois at Urbana-Champaign ",
                   "normalized_name":"University of Illinois at Urbana-Champaign",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/047426m28",
                      "GRID":"grid.35403.31"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Over the past year, the University of Virginia Library's Japanese Text Initiative (JTI), in partnership with the Electronic Text Center (Etext), has been developing an online multimedia dictionary of haiku. In this paper, I will describe the scope and content of the project, and how it combines text and multimedia materials in unique ways. I will also discuss the project from a technical perspective, describing its encoding, interface, and search and delivery mechanisms, with an emphasis on new problems encountered in the course of development. Haiku is a well-known and increasingly ubiquitous poetic form in the West and around the world. But while many non-Japanese are familiar with the metrical structure of haiku poetry, few have a sophisticated knowledge of its traditional themes, which are based upon deeply nuanced kidai and kigo (season-words). At the heart of the JTI project is an English translation of portions of the Nyumon Saijiki, a standard reference of kidai and kigo produced by the Museum of Haiku Literature in Tokyo. The Nyumon Saijiki contains entries for the people, places, and things found in haiku, with examples of modern and classical haiku under each entry. Our translation of this work, completed by world-renowned haiku expert William Higginson, consists of kanji, hiragana, romaji, and English renderings for each of the 3,000 kidai and kigo, along with a definition and brief explication. For the 100 most important terms, Mr. Higginson has translated the full entry, which includes a discussion of proper usage and several example poems. In addition, we have supplemented these full entries with image and sound files harvested from online archives. Audio and visual illustrations of these recurrent themes (the cries of autumnal insects, the hazy moon of a spring night, and so forth) will greatly enhance the user's understanding of poetry that is so deeply rooted in sensory perceptions of nature; this is doubly true for Western audiences who live half a world away from the particular plants, animals, rituals, and seasonal phenomena discussed in the poems. Although Etext has produced and maintained full-text SGML/XML collections for over a decade, and the JTI's work with Japanese language materials dates back to 1997, the current project has presented us with many new challenges. In the past, the use of images in our collections was limited primarily to page images and book illustrations; sound files were all but non-existent. The project's web designer has strived to integrate the multimedia components into the project in a way that supplements but does not overwhelm the textual content. The potential audience for this project is incredibly broad; we anticipate that it will be used around the world by everyone from grade-school students to professional scholars of Japanese literature. Different users will require different views of the data and different tools for manipulating it. A K-12 audience might want to see just the English translations of the poems, accompanied by the relevant images, while someone learning the Japanese language would be better served by a poem in its kanji and romanized versions, along with an audio recitation and highlighted links from the keywords in the poem to their full dictionary entries. More expert users might want to run sophisticated search queries on the dictionary, and a student of comparative literature might want to cross-query the dictionary against the English Poetry Database or a similar collection. In order to present these multiple views of the project, we have encoded the data in XML-compliant TEI and have generated much of the output with XSL stylesheets. This represents a major transition for the Etext Center, which since 1992 has stored data in SGML and converted it \"on the fly\" to HTML with CGI scripts. The nature of the saijiki data, with its deeply nested hierarchies and the database-like entry structure, combined with our need to sort, recombine and re-present this data very nimbly, led us to an XML/XSL workflow. However, we made some sacrifices with this approach. Our on-the-fly delivery of the data through XSL stylesheets has been considerably slower than a comparable CGI transformation. For this reason, we currently deliver large chunks of data through CGI or as XSL-generated static HTML pages while we work to reduce the delivery time. A further complication has arisen from our need to use UTF-8 encoding for this project, instead of the EUC encoding we have used in the rest of the JTI collection. Unicode allows us to easily represent kana and hiragana alongside Western characters, including macron vowels that aren't part of the EUC character set. However, our OpenText search tool does not support Unicode; until current work on Unicode compatibility for OpenText is completed, we need to convert the Unicode to EUC prior to indexing and essentially maintain two parallel versions of the same data. In the coming year, we look forward to translating the remaining portions of the Nyumon Saijiki, adding to the supplementary multimedia materials, and integrating this haiku collection both \"vertically\" (with our other Japanese-language materials) and \"horizontally\" (with other poetry collections and dictionary projects across our holdings).  ",
       "article_title":"Building a multimedia haiku dictionary",
       "authors":[
          {
             "given":"Christine",
             "family":"Ruotolo",
             "affiliation":[
                {
                   "original_name":" Electronic Text Center University of Virginia Library",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  From the notorious HTML <blink> tag and simple animated GIFs to elaborate cinematic presentations produced with Java, DHTML, or authoring systems such as Macromedia Flash, Macromedia Shockwave, and Adobe LiveObject, moving images and texts have become a ubiquitous feature of the World Wide Web. As computer animation technologies have become more robust and accessible, animations of all kinds have become more prevalent in electronic art and literature, displays of information, and pedagogical materials. For scholars working to develop critical methodologies for the analysis of electronic media, computer animations--whether they take the form of word-and-image poetry, film-like narratives, or diagrammatic representations of philosophical concepts--offer challenging moving targets. Animation requires an interpretive approach that can account not only for the role of spatial and temporal dimensions in the production of the work's meaning, but also for the technical, code-based operations that create the specific animated elements in the work. As this presentation will argue, engaging the complex issues involved in a hermeneutics of animation propels us toward a recognition of the potential of animation as a medium of hermeneutic reflection in its own right. Extending to computer animation the same consideration that has recently been given to scholarly hypertext [2, 5] leads us to a view of animation not simply as the object of hermeneutic inquiry, but as a way of \"doing hermeneutics\" in the strong sense of philosophizing about meaning and interpretation. Focusing on a small set of concrete examples, this presentation briefly outlines a set of questions confronting the interpretation of computer animation: 1. What is the semiotic function of the movement of elements in an animation? This question elicits more or less straightforward answers only when we are discussing \"representational\" animations that strive for versimilitude of movement in animated figures (realistic gaits in humans and other creatures, for example--the sum of the animated movements means \"walking\"). When the movement is essentially \"non-mimetic\" (the appearance and disappearance of text, for example), how do we understand the contribution of the motion to the \"whole\" meaning of the work? 2. What is the relationship of the precise chronometic time and geometric space assigned to elements in the code of the animation (the values of a setTimeout method in JavaScript, for example) to the phenomenological experience of time and space produced in the work for a reader [10] (such as the perception of an object crossing the screen \"slowly\")? As much as the programming that underlies traditional hypertext systems, the technical substrata of computer animation suggests a need for a comparative method that reads the text of the code alongside the manifest text of the work on the screen, viewing the finished animation as a dialogic hybrid of (at least) two distinct languages. 3. Is it possible (or useful) to distinguish broad genres of animation that correspond to narrative, lyric, and drama? How does the sequencing of elements in an animation intersect with the rhetorical conventions of these traditional genres, and in what ways can the movement of animation disrupt and complicate these forms? Can we imagine, following Kolb's work on hypertext writing in philosophy [6], a dialectical, argumentative mode of animation? 4. How do animated elements contribute to whatever forms of interactivity a specific electronic work invites? 5. How do animations intersect with other communication technologies, especially the hypertext systems in which they are often embedded?   Each of the examples I discuss illustrate these challenges. in addition, each indicates the potential of animation as a means of visualizing theoretical concepts and hermeneutic procedures. Josh Santangelo's DHTML poem \"Iris\" cites and interprets lines of D. H. Lawrence [9]; the collaborative Flash project \"Im Zeitalter der Konversationseuphorie\" by Merkel, R. et al. presents an elaborate meditation on the possibilities and impossibilities of human communication [11]; and Charles Heinemann's \"Jacques Lacan's Imaginary Prisoner Game\" offers a Java-driven dramatization of one of Lacan's models of temporality in the psyche [4]. To reinforce my claim that animation provides a powerful medium for critical and metacritical discourse in the fields of philosophy and literary criticism, I will conclude my presentation with a few examples of work from a graduate seminar in aesthetics in which students produced web-based diagrams of concepts as a way of investigating their developing theoretical frameworks. Students drew on the approach to modeling concepts introduced by Gilles Deleuze and Felix Guattari [3] and W.J.T. Mitchell's critical iconology, especially the formulation of \"ut pictora theoria\" [8]. Their diagrams presented both single concepts such as \"intertextuality\" and \"power\" as well as relationships among concepts, such as the intersection of \"authenticity,\" \"justice,\" \"representation,\" and \"community.\" Working together to produce protocols for animating these diagrams in Macromedia Flash led the seminar participants into dynamic discussions of the processes, relations, hierarchies, and even the contradictions that structure--and animate--our thinking about literature and culture. Computer animation not only invites us to shuttle among different fields, shifting, for example, from theories of editing effects in the cinema to narratological accounts of sequence and perspective to phenomenological reflections on the cognition of movement; it also expands the range of disciplines that can help us understand how movement can produce, problematize, and impede meaning. My paper suggests that in order for critics of electronic media to account more fully for the phenomenon of online animation, we will at the same time have to take fuller advantage of the medium for the expression of our own critical ideas. The natural sciences frequently employ digital animation techniques for the purpose of visualizing structures, processes, and relationships within a wide range of data [1]. Envisioning a philosophical writing practice in the humanities (including, of course, intellectual work such as literary and cultural criticism) in which animation serves more than an ancillary, illustrative role in an occasional diagram is one way to meet the challenge of what Adrian Miles has called the \"riskful writing\" that acknowledges hypertext's debt to cinematic forms. As I hope to demonstrate, however, even these simple diagrams and modest literary productions are in a sense rehearsals for a lively future discourse in--and not merely about--electronic media.  ",
       "article_title":"Interpreting Animation and Vice Versa: Can We Philosophize in Flash?",
       "authors":[
          {
             "given":"John",
             "family":"Zuern",
             "affiliation":[
                {
                   "original_name":" University of Hawaii-Manoa",
                   "normalized_name":"University of Hawaii System",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/03tzaeb71",
                      "GRID":"grid.162346.4"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   Introduction The growing availability on the Web of literary heritage is going to make easier humanistic researches, on the one hand facilitating access to information sources and documents and on the other hand providing a knowledge representation of texts, enabling its sharing and reuse. One of the major problems to face in knowledge representation is the formalization of literary data. The main difficulty is to capture the richness of word meanings into an established form, which allows automatic data treatment, preserving the essence of a thing anyway. This challenge is related to the different nature of Computer Science and of the Humanities. The former has its foundation in establishing a formal representation of what exists (formal languages and modeling of reality); the latter is based on interpretation, whose subjectivity escapes from classification or rules. It is recognized that accuracy in literary analysis is related to cultural background and literary sensibility, but the underlying ambiguity of natural languages poses to researchers further difficulties: a specific term may have different or contradictory meanings and intepretations; authors frequently use different words or expressions to refer to the same meaning By developing common formalisms, Computer Science tools aim at reaching a sharable agreement on world representation. Similarly, in order to give an objective basis to concepts (starting point of the analysis), an application of this formal approach in the literary domain may allow experts to define and share a common vocabulary, to reach an agreement on word senses, thus reducing ambiguity. In the hypothesis proposed in this paper, the use of a reference tool (such as an ontology»An ontology is a specification of a conceptualization (…)That is, an ontology is a description (like a formal specification of a program) of the concepts and relationships that can exist for an agent or a community of agents« in T. Gruber. »What is an ontology?« URL  (T. R. Gruber. A translation approach to portable ontologies. Knowledge Acquisition, 5(2):199-220, 1993)) seems to offer a means to face this challenging task with success: by keeping from misunderstanding in reading texts and by limiting subjectivity in their analysis, the first expected result is a better comprehension of literary phenomena; by improving knowledge representation of a literary text, the second effect of formalization is the retrieval of more relevant texts for research purposes.   Application and Results In the analysis of a literary phenomenon, some of the aspects to be considered are: the ambiguity of natural languages, that poses to experts problems in order to limit subjectivity in interpreting texts; and the heterogeneity of information sources to select (historical, cultural, geo-political), that determines the need of retrieving relevant documents for the analysis.  Identifying criteria able to deepen the study of a literary phenomenon and to extract interesting documents on that subject, would be of great utility. The adoption of a linguistic resources (namely the ontology of WordNet [11]) as reference tool, seems to be a viable idea in order to reach both goals. In order to test this approach in humanistic research, the \"Dualism Truth vs. Propaganda\" [2] in Karl Kraus has been investigated, using WordNet, the on-line reference system designed at the Cognitive Science Laboratory of the University of Princeton, to model lexical memory. Kraus was an Austrian intellectual and one of the bitterest satirists of fin-de-siècle Vienna, to be compared with Jonathan Swift for his satiric vision and command of language. He was a critic, a playwright, a poet, a journalist and the editor of the magazine \"The Torch\" - Die Fackel [8]) - for about 36 years. Strongly believing in a language as a medium to express the truth, one of his major concerns was the German language and its misuse by the press. As a journalist he believed in informing the public rather than overwhelming it with propaganda: his main goal was to report facts, instead of interpreting them. Referring to this informative function of journalism, he wrote: \"My duty is to say the Truth to Mankind\" \" Mein Pflicht ist es, den Menschen die Wahrheit zu sagen\", Kraus K.: Die Fackel, Band 11, no. 852-856 (May 1931), p. 95 Basing on Kraus' writings, the literary phenomenon under analysis has been synthesized into four keywords: \"Language\", \"Truth\", \"Journalism\", \"Propaganda\". The meanings of these selected terms have been defined using WordNet concept disambiguation. Because in this lexical database English nouns, verbs, adjectives and adverbs are organized into synonym sets called synsets (each representing one underlying lexical concept), disambiguation is based on lexical and semantic relationsLexical relationships: synonimy, antonimy, polisemy. Semantic relationship: hyponymy, hyperonimy. with other concepts. Examination of WordNet definitions has led to: the exploration of keywords meanings; the delimitation of their semantic fields; and the finding of other related couples of opposing concepts such as: Truth vs. Verisimilitude, Language vs. Paralanguage, Journalism vs. Propaganda. The application of this ontology-based approach has been able to improve the comprehension of the \"Dualism Truth vs. Propaganda\" in Karl Kraus (1874-1936). As main consequence, by using WordNet it has been possible to study the literary phenomenon under analysis, confirming the validity of Kraus' position towards information problems and finding the core of the antagonism between \"Propaganda and Truth\". As far as the second goal of this research is concerned (that is to find more relevant text for analysis), in order to apply the proposed approach, two sets of Kraus’ aphorisms (Kraus, 1955) - »Writing and Reading« and »By Night«[4] \"Writing and Reading\" and \"By Night\" have been extracted from \"Dicta and Contradicta\" (Sprueche und Widersprueche), a selection of aphorisms appeared in \"The Torch\" and published in 1909. - have been digitized. Then, by a human indexing operation performed using the ontology contained in WordNet, it has been assigned to each aphorism a category, based on semantic fields. The above selected keywords (»Language«, »Truth«, »Journalism«) have been adopted as indicator of semantic fields. Each aphorism has been labelled by the presence/absence of these fields. Despite the fact that »By Night« has no occurrences of the keyword »Journalism«, human analysis shows that it contains two relevant aphorisms\"Wort und Wesen: das ist die einzige Verbindung, die ich je im Leben angestrebt habe\" Kraus K. Beim Wort genommen, p. 431; Detti e Contraddetti, p. 352; \"Zensur und Zeitung - wie sollte ich nicht zugunsten jener entscheiden? Die Zensur kann die Wahrheit auf eine Zeit unterdruecken, indem sie ihr das Wort nimmt. Die Zeitung unterdrueckt die Wahrheit auf die Dauer, indem sie ihr Worte gibt. Die Zensur schadet weder der Wahrheit noch dem Wort; die Zeitung beiden\", Kraus K. Beim Wort genommen, p. 443; Detti e Contraddetti, p. 358 for the comprehension of the »Dualism Truth vs. Propaganda« in Karl Kraus. In »By Night« the keyword »Journalism« is absent, but it is present the word »Zeitung« = newspaper, an implicit form, but semantically related to the keyword »Journalism«. If the goal of the search were to find all sets of aphorisms where Language and Truth and Journalism occur, probably this set of aphorisms would have been ignored, because not pertinent with the query. By defining semantic fields and categorizing aphorisms using them, the proposed approach has made possible to select »By Night« as a relevant document.   Conclusions The achieved results show that literary data formalization based on ontologies is able to improve the accuracy of literary research. By including definitions of basic concepts in the domain (also in a machine-interpretable form), by identifying relations among them and by defining semantic fields, WordNet allows experts to share information in a domain, to provide critical notes and comments on texts, and to interpret them. Furthermore, from this study emerges that defining the semantic field of words (by applying definitions provided by an ontology) and indexing documents by adopting a semantic categorization is an effective way of representing the content of a text: the faculty to bring to light word meanings, hidden in texts in an implicit form, improves the retrieval of more relevant documents, matching humanistic research needs.   ",
       "article_title":"An hypothesis of formalization of literary data for text analysis: a case study on Karl Kraus' writings",
       "authors":[
          {
             "given":"Daniela",
             "family":"Alderuccio",
             "affiliation":[
                {
                   "original_name":" ENEA/UDA (Italy)",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  A series of studies has built up a set of 178 samples of Greek texts in a format which now provides a dataset comprising 178 rows and 35 columns. The previous studies by this author and relevant works by others are listed in Mealand (1999). There are 30 effective linguistic features being tested for each sample, and to these we need to add classification categories such as author, genre, subgroup, text and sample. The 178 samples and 30 linguistic variables alone provide 5340 observations. We are now in a position to make report on detailed stylistic comparisons of a wide range of texts from 300 BCE to 200 CE but focused on texts from 100 BCE to 100 CE. Some manual intervention and a set of programmes eventually turns complexities of the Greek texts from the TLG into thousand row columns of single words, and simpler routines then allow a pentium to make much shorter work of the word counts. The texts include a range of Greek historical writers: Polybius, Dionysius of Halicarnassus, Plutarch; also a set of Greek technical writers, some samples of letters and other documents preserved on papyri, and a range of biblical and other Jewish and Christian writings of the period. The latter include samples from the Septuagint (4 Kingdoms, Deuteronomy, Isaiah, 2 & 4 Maccabees and the like), most of the New Testament books, Philo, Josephus, Clement of Rome, Ignatius of Antioch and Polycarp. The variables include a set of stylistic features which are mostly function words: a set of sentence connectives equivalent to but, for, and, therefore, on the one hand; a set of conjunctions which introduce clauses such as if, while, in order that, where, whenever, and the like; a large set of prepositions such as away from, into, out of, in, on account of, according to, after, with, about, on, towards; and a set of (genitive) participle endings. These variables were chosen as a set of function words suitable for inspecting the style of samples of 1000 words. (It would be possible to extend the range of the linguistic features by choosing much larger samples of those texts which have the necessary bulk. Some, but not all, of the works are extensive.) Variables were selected in the light of recent studies showing the effectiveness of high frequency words and or features linked to syntax. The statistical method used was Correspondence Analysis as this plots both samples and variables and allows one to see not only relations between texts, but also those linguistic features which associate or separate the texts. CA was run on a Sun machine shared by scores of other users. Correspondence Analysis also differs from some other multivariate analysis in not requiring the data to be normally distributed. This issue is a problem when samples from large numbers of different authors are involved. Many stylometric studies limit themselves to a handful of authors or even just two or three. This study is attempting to make comparisons across a wide range of ancient literature. Using all the data we obtain a complete plot (OHP Cell 1) showing the location of 178 samples and 30 variables. This was the plot to which much careful attention was given. But it is not an easy plot to interpret and for display purposes a supplementary strategy provided an overview plot of cumulated samples - mostly of 5000 words from 5 samples from each author combined together. This preserves the broad shape and outline of the more complex plot. Each author still appears in the same general segment of the plot as in the original. We can, however, more easily see how the authors relate to each other and to the linguistic variables. We can also interestingly see how the original plot reveals the wider spread of 1000 word samples compared with the cumulated 5000 word samples. This gives added insight into variation within the work or works of each author The main interpretation of the plots is that the more fluent Greek writers (i.e. Polybius, Dionysius, Plutarch and their associates) lie to one side (right) of the plot while the Septuagint translations of biblical texts lie to the other side. (I use the term fluent to capture the more complex observation that the writers who appear on the right of the plot are those whose work would be regarded as more acceptable in style by an educated Graeco-Roman public. Jerome is on record as being embarrassed by some features of biblical Greek.) As the horizontal axis represents Dimension 1 this captures the largest single component of the inertia. The vertical axis seems to separate narrative texts lower down the plot from speech, reported speech, treatises, and ordinary letters in the centre of the plot. Paul's letters are even higher up the plot though neither to the extreme left nor right. Paul's style is therefore between that of the Septuagint and that of Dionysius on the one dimension, and much higher than the others on the vertical dimension. This may reflect the very argumentative style of Paul's dictation in his letters. The plots from Correspondence Analysis show the location of the variables in relation to the location of the samples and from this we can draw inferences which we can confirm by inspecting boxplots of means and other statistics. This reveals the relative weight of usage of our main criteria in the different writers. So for example we can see that Paul makes heavier use of alla, gar, ei, ou, mh, hina (but, for, if, not, not, in order that) and to some extent dia and eis (into), while making moderate use of de, oun, and men (but, therefore, on the one hand). He has low usage of kai, meta and peri (and, after/with, about). Much work on Paul has focused on internal differences of style. This study shows more clearly than before just how Paul differs widely from most, but not all, of the other authors selected. The Septuagint passages from Biblical texts have higher use of kai, apo and epi (and, from, on)and to some extent hews (while/until), moderate use of ou (not), and low usage of alla, gar, de, men, oun, ei, mh, kata and peri (but, for, but, on the one hand, therefore, if, not, according to/against, about). But the Septuagint texts themselves differ and I conclude that the differences are not just due to the style of samples from 4 Kingdoms, nor just to differences between translated texts and those composed in Greek. The samples from Genesis and Proverbs lie between these other Septuagintal groups. The more fluent Greek writers have higher use of de, men, kata, peri (but, on the one hand, according to/against, about) and the genitive participle endings, moderate use of alla, gar, ou, kai, and oun (but, for, not, and, therefore), and low usage of hina and mh (in order that, not). Here it is worth noting that despite his diffidence Josephus is close on stylometric grounds to Dionysius and Plutarch. It is also worth noting that the few selected samples from non-literary papyri are not as far distant from Dionysius as some might expect. Finally we should note that the Greek technical writers do not form a coherent group, as Alexander and others tend to assume, but lie in several different directions around the group including Dionysius and Plutarch. All these provisional conclusions suggest lines for further investigation. Therefore (oun) is more heavily used in some NT texts esp. some Johannine texts. The conjunction hopws (in order that) is mainly used by the more fluent Greeks. The preposition ek (out of) is more heavily used in Revelation, moderately in the Septuagint. The word en (in/by) is used more heavily in Colossians and Ephesians and in some of the Johannine texts, and moderately high usage is also found in the Septuagint and Paul. Philo and Josephus often side with the more fluent Greeks on most criteria, but do have a raised usage of pros (towards). The results reported here make use of a larger selection of samples than any of the previous work I have published on the stylometry of 1st Century Greek texts. We can see not only that the writers fall into different groupings, but also which literary criteria tend to distinguish their styles one from another. Some of the authors used could not provide further samples as their works are relatively short. Others could provide massive amounts of further text for analysis. That leaves plenty of scope for further researchers to explore the more voluminous authors further, and perhaps, by taking much larger samples from them, to scrutinize stylistic features of their work which appear slightly less frequently than the highest frequency words and features which are the main criteria used here.  ",
       "article_title":"Mapping Differences in 1st Century Greek Style",
       "authors":[
          {
             "given":"David",
             "family":"Mealand",
             "affiliation":[
                {
                   "original_name":" University of Edinburgh",
                   "normalized_name":"University of Edinburgh",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/01nrxwf90",
                      "GRID":"grid.4305.2"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper discusses the problem of the theoretical-methodological status of Humanities Computing (HC). More particularly it focuses on Computer Philology (CP), i.e. the computational concepts and techniques dedicated to aspects of a philological analysis of literary texts. The question guiding my deliberations is whether CP is a proper ‘Wissenschaft’ in the sense of an emerging fully-fledged discipline, or rather an ancillary discipline, a ‘Hilfswissenschaft’ similar to Statistics or Library Science? One can safely say that the practice of HC has begun to establish itself as an integral part of research and teaching in various disciplines in the Arts and Humanities. But one must also concede that a consensus regarding the methodological-theoretical calibre of our undertakings seems to have evaded us thus far. As a result our own debate seems to retreat more and more onto the safe ground of pragmatism and technology, or in other words: who cares about the methodological and philosophical appraisal of HC - as long as it works? I believe that this is a dangerousely narrow focus - if nothing else, it is at least strategically detrimental to the aim of furthering the institutionalization of Humanities Computing (HC) at universities. The fact that Computer Philology (CP) concerns a more narrowly defined and homogenous array of disciplines than HC makes matters even worse. Any attempt to introduce CP vis à vis the well defined philological disciplines only accentuates the problem of the as yet vague methodological status of this ‘newcomer’. For example, at Hamburg University we recently (October 2001) managed to formally institute a joint ‘Arbeitsgruppe Computerphilologie’ in the Faculties of Computer Science and the Faculty of Languages. We are clear as to our brief: introduce modularized course components in CP into the language and literature curricula. Pragmatic considerations, such as, the aim to equip students with new skills, as well as the current ‘sexyness’ of computer technology work in our favor. But we are nevertheless divided as to the more profound philosophical and methodological arguments on which to ‘sell’ this initiative to colleagues and students. And or differences do indeed boil down to the very question: Is CP a proper ‘Wissenschaft’ - or is it rather a ‘Hilfswissenschaft’? Proponents of the latter position hold that CP cannot be a true discipline as it is neither defined by an exclusive subject matter, or a particular perspective onto such matter, nor ─ and this would seem the more difficult verdict to counter - has it as yet produced a new theory of literature, not to mention a meta-theory reflecting its own axioms and practices. This, for example, is the opinion of my colleagueWalther von Hahn who approaches the question from the perspective of the Linguist and Computer Scientist. Von Hahn illustrates his argument with the following process model: Figure 1:  The positioning of the CP-block in the bottom left rectangle, and in particular the orientation of the unilinear arrows in this diagramm clearly demonstrate an hierarchical organisation: CP is primarily conceived of by von Hahn as a research practice that is governed by the preceeding formulation of philological desiderata. These desiderata are identified in the course of the inspection of textual data (1) which leads to a formulation of specific research questions and interests (2). Only then can the selection of particular CP-tools (= practices) take place (3). These are now applied to the data (4) and generate a new representation of the textual data which in turn are passed back to the philologist (5) for their eventual interpretation. This interpretation then feeds back into philological theory and methodology. I would now like to juxtapose the underlying hypothesis - e.g., CP as a ‘Hilfswissenschaft’ - with conclusions drawn from my own research into computer based analysis of narrated action, a 7-year project concluded in early 2001 (components of this project were presented at the ALLC conferences Paris 1994 / Virginia 1999.) I initially set out to analyse the structural features of narrated action, with a view to formulating a model of ‘minimal action structures’, that is, the least complex yet logically coherent and context-idenpendent sequence of narrated events. I had found that there was no hard and fast theory on what actually constitutes a coherent piece of narrated action ─ the problem was either being discussed in terms of the conditions for achieving narrative ‘closure’, or in an altogether impressionistic and intuitive way. The only models and partial definitions useful to me were those developed in Formalism and Structuralism, schools of thought to whom the graphic respresentation of narrated action in the form of tree-and-nodes diagrams is fairly common (and owed to generative models imported from structural linguistics). Then I happened to stumble upon an article by Alain Colmerauer which gave a brief description of the AI-language PROLOG and illustrated it by way of a tree diagram. The visual analogy between an action- and a PROLOG-tree pointed me to the conceptual analogy, and the possibility to use a computer to model a minimal coherent action in a narrative, i.e., an EPISODE in the logical sense. Up to this point my project had thus proceeded more or less along steps 1 to 3 of van Hahn’s process model. The real problem ─ and challenge! ─ however was that the two blocks in the bottom left box turned out to be - empty containers: there was no developed theory or model dealing with my particular research problem available in HC/CP; there were also no established practices to apply. The bottom line was that I had to design and program a mark-up tool before I could even begin to model action structures by running a combinatory PROLOG-algorithm on the meta-data. In more abstract terms, I had found myself confronted with what I now regard as the most important methodological principle to be elaborated upon in the methodological-theoretical appraisal of CP. The type of questions considered relevant in the philologies normally require a semantic mark-up of the ‘raw’ textual data; the mere digital representation of non-numeric data (unless it is confined to a purely statistical distribution analysis) does not provide access to semantic phenomena. However, the process of data mark-up, subsequent analysis and modeling of meta-data, and eventual evaluation of the model in itself also presupposes that the philologist has identified not just a philological, but also a ‘computational’ frame of reference - in other words, the computer philologist is not just picking tools from a box and applying these to old questions. He or she must rather reconceptualize the research problem in a new light before the tool’s aspect even comes into play. As for my own project, this is how I would therefore schematize its methodological architecture: Figure 2:  In comparing von Hahn’s to my own diagram I come to the following conclusions: The interdependency between the tools and practices of CP on the one hand, and the textual data on the other involves a hidden ‘third party’ - a conceptual frame of reference novel to the philologies. The two-step representational transformation from textual data to meta-data, and then from meta-data to philological interpretation, therefore cannot be thought of as governed exclusively by conceptual models drawn from philological theory and methodology. The import of the ‘foreign’ conceptual model informs the entire research architecture throughout; it is as essential to the design of the transformational processes as it is to the eventual interpretation of the transforms generated. The key intellectual prospect of the development and integration of CP into the methodological ambit of the philologies proper is not the fact that, because of deploying computer technology, one can in certain cases analyze and model textual data faster, more coherently, or on the basis of more explicit (and thus transparent) interpretive and/or cognitive algorithms. What seems far more important is that we are offered new frames of reference as to how to conceptualize and model text-based phenomena. Computer Philology undisputedly ‘has’ tools and practices - but it ‘is’ more than these: neither a proper ‘Wissenschaft’, nor an a-theroretical ‘Hilfswissenschaft’, its methodological status is that of a new philological heuristics.  Part of the problem of how to adequately define the methodological status of HC and CP might, in fact, stem from the tendency to confound two meanings of the term ‘computer’. When we focus on tools and practices in CP we refer to the machine ‘computer’ in a very literal sense - hence the pragmatists’ attempt to define CP per se in this vein. But ‘computer’ is also a metaphor, like ‘book’ or ‘image’, that stands for a particular mode and method of symbolic representation. In the philologist’s case it is a metaphor for a very specific reconceptualization of the phenomenon of ‘text based meaning’: one that aims at bridging the gap between the qualifiable, and the quantifiable. Whether that ‘metaphor’ is run on a Cray or on paper and pencil is not really the key question.  ",
       "article_title":"Computer Philology: 'Wissenschaft' or 'Hilfswissenschaft'?",
       "authors":[
          {
             "given":"Jan",
             "family":"Meister",
             "affiliation":[
                {
                   "original_name":" University of Hamburg",
                   "normalized_name":"Universität Hamburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00g30e956",
                      "GRID":"grid.9026.d"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The year 1960 marked the beginning of several strands of the speaker's activities, strands which have remained fruitfully entwined until the present day. In January 1960 he was appointed Honorary Secretary of the Modern Humanities Research Association (MHRA), an influential learned society, established in 1918, becoming its Honorary Treasurer in July 1963, a post he held until March 2001. Following this he was elected as the Association's President for 2003. Quite separately, and in a direction more obviously germane to the present conference, he embarked in June 1960 on his first applications of the computer to medieval literary texts, after a year or two of growing awareness, above all through press reports concerning the work of Roberto Busa, that scholarship had gained a powerful new tool for the compilation and analysis of natural language. His first paper on the subject was presented in November 1960 to humanities scholars at the University of Cambridge, and published in The Modern Language Review, an MHRA periodical, in April 1962. Also in November 1960, he attended and contributed a paper to the colloquium on the mechanization of literary analysis and lexicography organized by the Tübingen Computing Centre in conjunction with IBM Germany and the Centre (CAAL) which Pater (later Monsignor) Busa had established in 1956 at the Aloisianum in Gallarate. Also in the autumn of 1960, the speaker had submitted proposals to the University of Cambridge for what was eventually called the Literary and Linguistic Research Centre, the doors of which opened in October 1964. Its work gave rise in March 1970 to an international symposium on 'The Computer in Literary and Linguistic Research' (published C.U.P., 1971) the first of its kind in Great Britain, a forerunner, and one of the progenitors of the present conference series. There was a direct link between the symposium and the establishment in April 1973 of the Association for Literary and Linguistic Computing. Its inaugural meeting at King's College London was under the speaker's chairmanship, with Mrs Joan M. Smith as founder Secretary. Within a few years the Association was fostering international co-operation in humanities computing on a considerable scale. The range of experience represented by these early developments offers an opportunity to examine examples of how far the aims of projects undertaken during this period had to be tailored to what was technologically possible at the time, and the extent to which that generation's expectations for the future were realistic, or fell short of what would actually be achieved. Another question is how far subsequent research has been shaped by events which were not foreseeable in 1960, for instance the present scale of access to personal computing, together with its low cost, and the associated ubiquitousness of e-mail and the web. In seeking new directions for humanities computing it is sensible to reckon with some developments, such as the availability of inconceivably large storage capacities, and the processor speed necessary to harness them for every aspect of the worldwide electronic library. Among the lessons the past can teach us, on the other hand, is the realisation that we have little prospect of predicting future technologies, and the radical changes they will bring about. In choosing illustrations from the present-day situation in humanities computing, the speaker draws inter alia on experience he has gained during the past decade, during which he has played a guiding role in giving electronic access to the very large archive of MHRA periodicals, bibliographies, monographs, and other book publications. This challenging task, which finally and fully brought together various strands of his activities, as outlined above, represented a striking blend of processes such as the digitization, computer typesetting, and electronic storage of periodicals, which it was possible to envisage in 1960, although hardly on the scale required, with advanced features like mass accessibility for full text searching and article delivery through the internet. Reference is made also to non-electronic obstacles in the way of such undertakings, among them legal matters, for instance intellectual property rights, and commercial considerations like financial viability. Such concerns are of particular importance to learned societies which, like the MHRA, see their charitable function as the provision of a stable, long-term environment for core bibliographies and periodicals, as well as the generation of funds for future initiatives, above all those which cannot hope to be self-sustaining at the outset. Recent electronic developments have ensured that the work of past contributors to MHRA publications is revivified and that it remains current indefinitely, while helping to fund new directions of scholarship.  ",
       "article_title":"Some Thoughts on Forty Years of Humanities Computing - A Signpost to New Directions?",
       "authors":[
          {
             "given":"Roy",
             "family":"Wisbey",
             "affiliation":[
                {
                   "original_name":" King's College London  ",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       "keywords":[
          "invited paper"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The established present use of the computer in the humanities is to enhance the properties and quality of the book. With the book electronically stored, book contents and book knowledge can be accessed fast and very flexibly; while the physical book, widely still the end product of computer deployment, can be manufactured more accurately, perhaps even more economically, than under the conventional conditions of the composition room and press. Beyond the token book in electronic storage, and the real book generated by electronic means to habitually palpable materiality, paperwork and the book provide significant metaphors for computer-related discourse, with the file or the home page heading the terminology. This is a specific indication of a patterning of thought according to inherited figurations that is as tenacious as it is universal. In the face of the forces of habit, the question arises how clearly the book-conditioned and book-trained humanities scholar and researcher is capable of discerning the unique otherness of the electronic medium and both explore and exploit its potential. I suggest that the grasp hitherto, in humanities scholarship at least, of the essential virtuality of the electronic medium does not securely reach further than to a simulation of the materialities of paper and print. Hence an orientation towards the future requires reflecting on how the virtuality of the electronic medium uniquely might put us in a position to deconventionalize our scholarly pursuits and innovate them. Such reflections should not remain abstract and arcane. Examples will help to flesh them out. A main area for applying the computer is that of the organisation of knowledge. Handbooks, dictionaries and the like are already preferred objects of conversion into electronic multi-mediality. A distinct qualitative step might however be taken if, from its very conception, the organisation of knowledge is not projected as going linearly (with cross-reference props) into book form and bookish formats, but relationally into the virtuality of the computer. The example of an incipient project on such terms will be cited. A specific sub-area, furthermore, of the organisation of knowledge is the commentary on a given text, or body of texts. The thrust of the arrangement of commentary in book-printing practice is currently linear - it having been largely forgotten how early book typography was already capable of suggesting relationalities (and so was properly 'hypertextual' avant la lettre). But the relationality of the electronic medium will encourage a re-transforming of commentary into something akin to that early understanding of the essential relationality of the represented matter itself. Again, an example will illustrate the issues as indicating some first principles in need of developing towards the systematisation and organising of the computer commentary. This should lead to a brief glance at a project under way exemplifying an ambition to set up the scholar's and critic's working desk virtually by assembling all requisite primary and secondary materials relative to a pre-defined research interest on one internet website globally accessible. The Italo-Franco-German project HyperNietzsche currently under development from its German base in Munich is making the bid for such comprehensiveness. Against its heterogeneity, one may hold that it is textual editing that remains a pivotal - and perhaps the central - area of humanities scholarship. It has the longest, and intrinsically most bookish, tradition. It would seem particularly necessary, therefore, that the tradition's centeredness on the book not remain unreflected. Textual scholarship's critical self-distancing is aided by literary and textual theory that focuses the process nature of texts in reading as in composition. To such general theory, the counterpart, in theorizings of the electronic medium, are current notions of hypertext. The concept of hypertext, however, does not by itself guarantee an electronic text edition properly so called, exploiting, if worthy the definition, the medium's virtuality on its own terms. Considerable rethinking of procedures and presentational modes is required to develop, organise and realise electronic editions as medium-specific alternatives to scholarly editions in book form. An example, again, will serve to indicate directions for development. Finally, in this paper, I wish to introduce (and demonstrate) a distinction between electronic text editions and the electronic editing of processes of manuscript writing. With the critically editorial exploration of manuscripts as sites of writing and of writing processes as the successive, yet random, filling of writing space, the editorial branch of humanities scholarship cuts the moorings to the book. Such exploration is possible only in the virtuality of the electronic medium and its evanescent screen images.  ",
       "article_title":"There is Virtue in Virtuality. Future potentials of electronic humanities scholarship",
       "authors":[
          {
             "given":"Hans",
             "family":"Gabler",
             "affiliation":[
                {
                   "original_name":" Ludwig-Maximilans-Universität München",
                   "normalized_name":"Ludwig-Maximilians-Universität München",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/05591te55",
                      "GRID":"grid.5252.0"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Tübingen ",
       "date":"2002",
       "keywords":[
          "invited paper"
       ],
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper describes an authorship, and more generally document classification, experiment on a difficult Dutch corpus of university writings. By measuring linguistic distances using a cross-entropy technique, a technique sensitive not only to the distributions of language features, but also to their relative intersequencing, classification judgments can be made with great sensitivity, significance, confidence, and accuracy. In particular, despite the designed difficulty of the Dutch corpus used, the technique was still able to reliably detect not only authorship, but also subtle features of register, topic, and even the educational attainments of the author. Authorship attribution has received significant attention in recent years as a testbed and touchstone for new theories of authorial markers and linguistic statistics; (Holmes, 1998) presents a brief historical outline of some major theories. One of the current front-runners, proposed in (Burrows, 1989), suggests that a strong cue to authorship can be gleaned from a principle components’ analysis (PCA) of the most common function words in a document. Like most statistical techniques, a scholar’s ability to apply this technique is limited by the usual features of sample size, sample representativeness, and test power. Another popular technique, linear discriminant analysis (LDA), may be able to distinguish among previously chosen classes, but as a supervised algorithm, it has so many degrees of freedom that the discriminants it infers may not be “clinically” significant. An alternative technique using measurements of cross-entropy (Wyner, 1996; Juola, 1997, 2003) might be a better tool under difficult circumstances because it is capable of extracting more information (and thus distinguish more readily) along perceptually salient lines from a given data set. The corpus studied by Baayen, Van Halteren, Neijt, and Tweedie (Baayen, et al. 2002) has been specifically developed to be a difficult problem in authorship attribution. This corpus consists of small writing samples taken from undergraduate students at the University of Nijmegen on carefully and closely controlled topics. Eight students, four first-year and four fourth-year, were asked to write nine essays (for a total of 72) on the same set of closely-specified topics. These topics included three samples of fiction (for example, a retelling of the fairy tale of “Roodkapje,” the Dutch equivalent of “Little Red Riding Hood”), three of argumentative writing, and three of descriptive writing. Document sizes varied from approximately 3600 to 7600 words. These documents were analyzed using cross-entropy and the results compared to analysis via PCA and linear discriminant analysis (LDA) (Baayen et al., 2001) to see both what the appropriate dimensions of variation were, and whether or not authorship attribution was practical in this tightly controlled a setting. Analysis using function word PCA showed a marked lack of significant and useful results. Baayen et all found “no authorial structure,” although education level and to a lesser extent genre were separated. LDA also could not reliably identify author, and in most experiments performed at chance level. First, as expected, the overall cross-entropy distances showed an extremely strong ability to sort documents by topic (grouping all “Roodkapje” stories together, for instance). Statistical analysis, however, shows a strong (and significant) tendency to within-group homogeneity across all dimensions of interest : topic, genre, author, and even year in school. In particular, the average within-group distance, excluding identity measurements, is significantly less than the average without-group measurement. This holds true, irrespective of whether groups are defined by authorship (p < 0.000001), register (p < 10-15), or even education level (p < 0.0084). This suggests that cross-entropy can be usefully deployed to author identification in this corpus.  A further test revealed that, for every document in the corpus and for every pair of authors, a potential authorship dispute can be resolved approximately 73% of the time using cross-entropy. Furthermore, applying a fractionation technique to restrict attention to only the function words (Juola, 1998; Juola and Baayen, in preparation) could improve these results to 86% correct identification. This can be compared to a best published result of 82% obtained by Baayen et al. using an enhanced LDA model. Although the enhanced version of LDA performed substantially better, it should be noted, however, that LDA is a supervised technique, and itself makes strong assumptions about the nature of the authorial problem that make it unsuitable to exploratory, unsupervised study. Even when one restricts the samples to a mere 500 words each, analysis still yields 63% of the pairwise comparisons correctly made, better than any result using PCA or standard (unenhanced) LDA. These results improve upon principle components analysis and standard linear discriminant analysis, with remarkable economy of calculation. The improved performance can be attributed to information used by cross-entropy but not by word-frequency histograms, specifically in ordering and sequencing of (function) words. Function word PCA performs well by using the relative presence or absence of function words as a cue and/or proxy for idiosyncratic syntactic patterns. Cross-entropy can distinguish not only a difference in presence/absence, but also a difference in relative sequencing. This additional source of information can allow more reliable inferences to be drawn from corpora and more subtle distinctions to be made. In theory, the idea that sequencing is more distinguishing than “mere” presence or absence could be applied to almost any form of linguistic data and any task. Irrespective of what feature sets are eventually found to be informative for authorship attribution, it is to be expected that sequences of features will probably outperform unordered bags of features in making fine distinctions.  ",
       "article_title":"A Controlled-Corpus Experiment in Authorship Identification by Cross-Entropy",
       "authors":[
          {
             "given":"Patrick",
             "family":"Juola",
             "affiliation":[
                {
                   "original_name":" Duquesne University",
                   "normalized_name":"Duquesne University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02336z538",
                      "GRID":"grid.255272.5"
                   }
                }
             ]
          },
          {
             "given":"Harald",
             "family":"Baayen",
             "affiliation":[
                {
                   "original_name":" Max Planck Institute for Psycholinguistics",
                   "normalized_name":"Max Planck Institute for Psycholinguistics",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/00671me87",
                      "GRID":"grid.419550.c"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   INTRODUCTION:    The work’s material history since its inception, the vast and largely uncharted alterations imposed by the history and by the mediation of generation upon generation of printers, editors, publishers—this is a relativism we are prone to ignore, but ignore at our peril.  (Marcus 1996)      The literary texts often are not homogenous since they may comprise dialogues, narrative parts, etc. An integrated approach, therefore, would require the development of text sampling tools for selecting the parts of the text that best illustrate an author’s style.  (Stamatatos et al. 2001)   Most non-traditional authorship attribution studies place too much emphasis on statistics, stylistics, and the computer and not enough focus is given to the integrity and validity of the primary data— the text itself. It is intuitively obvious and easily shown empirically that if you are conducting a study of the patterns of an author’s stylistic usage (e.g. Daniel Defoe), the study will be systematically denigrated by each interpolation of non-Defoe text and even by each interpolation of Defoe text of a different genre or significantly different time period. The crux of this paper is about one important element in the empirical methodology of a valid non-traditional authorship attribution study—the preparation of the text for stylistic and statistical analysis: unediting, de-editing, and editing. The general emphasis of this presentation is on prose analysis with some peripheral treatment of drama and poetry.  I. BACKGROUND AND DEFINITIONS A. Why a valid text is necessary should not even be asked. No valid experiment can be done if the input data is flawed—garbage in, garbage out!Too many practitioners simply grab a text from any available source—without any thought to its pedigree. (e.g. Khmelev and Tweedie’s “Using Markov Chains for the Identification of Writers.”)Are undertakings such as Project Gutenberg or the Oxford Text Archive with their easily available machine readable texts a boon or a bane to non-traditional authorship atudies? This question is explored in some detail. B. Selecting a starting textThe validity of using texts from the oral tradition and the scribal tradition is discussed.Before any manipulation and analysis of a text is carried out, a valid starting text must be acquired that fulfills many necessary requirements. This selection is primarily bibliographically driven. If a practitioner is not savvy in the bibliographical arts, a collaborator who is should be recruited.Examples of bad starting texts causing problems are given (e.g. Peng and Hengartner’s “Quantitative Analysis of Literary Styles.”)If you cannot obtain a valid text, do not do the study. C. Unediting—getting back to the state of “not yet edited”De-editing—removing selected text Editing—changing (preparing) a text for statistical analysis   II. EXPLICATIONThe statement, “each age, each author, each study demands a different mixture of the following particulars,” is discussed. A. UneditingAs a rule, the closest text to the holograph should be found and used. 1. Editorial interpolation a. Filled in lacunae b. Marginal notation c. ‘Changes’ in the text d. Critical editions  2. Printer interpolation For the Printer is a beast, and understands nothing I can say to him of correcting the press. Dryden (Ward p. 97) a. Catchwords (the first word of the next leaf or gathering) b. Signatures (combinations of letters and numerals used something like catchwords) c. Removing obvious typesetting mistakes (a slippery slope) i. ‘f’ for the long ‘s’ ii. Double words (e.g. ‘the the’ ‘was was’    B. De-editing 1. Quotes a. Factual, unattributed b. Factual, attributed c. Self quotes from earlier writings  2. Plagiarism a. Direct copy b. Paraphrasing c. Imitation  3. Collaboration a. Sectional b. Phrasal c. Word level d. Ghostwriting  4. Genre a. Poetry, prose, drama, letters, etc. b. Mixture (e.g. verse drama)  5. Graphs and Numbers a. Tables b. Lists c. Arabic and Roman numerals  6. Guide words a. Titles—chapter headings—the end word ‘Finis’ b. Marginal annotation  7. Foreign Languages a. Sentence level and greater b. Phrase or word level  8. Translations a. Verbatim b. Concepts  9. Examples of items de-edited (or not de-edited) incorrectly by practitioners a. Biblical quotes b. Titles in direct apposition c. Numbers that are spelled out d. Words with an initial capital   C. Editing 1. Encoding the text a. Why (e.g. homographic forms) b. TEI  2. Regularizing a. Spelling b. Contracted forms (simple, compound) c. Hyphenation d. Masked words (e.g. ‘D_ _ _ e’ for ‘Defoe’)   3. Lemmatizing a. Pro b. Con   D. Special Problems in Drama and Poetry 1. Stage directions 2. The ‘age’ dependency of transmission and technique.    III. SOME EXAMPLESStudies that are compromised by mistakes of commission and/or omission in editing, unediting, or de-editing. A. Historia Augusta 1. Twelve individual studies  B. Shakespeare 1. Eliott and Valenza 2. Foster 3. Horton  C. Defoe 1. Hargevik 2. Rothman   IV. CONCLUSION 1. Some items that are de-edited are valid style markers in their own right (e.g. latin phrases, different genre) and should be treated as such in a parallel study. 2. No matter which text is selected, the practitioner must disclose which text was used and everything that was done to it. 3. The same care must be taken with every text in the study—the anonymous text, the suspected author’s text, and all of the control texts. 4. If valid texts cannot be located and correctly edited, unedited, and de-edited, do not do the study 5. A valid text does not guarantee a valid study. However, a non-valid text guarantees a non-valid study.     ",
       "article_title":"On Determining a Valid Text for Non-Traditional Authorship Attribution Studies: Editing, Unediting, and De-Editing",
       "authors":[
          {
             "given":"Joseph",
             "family":"Rudman",
             "affiliation":[
                {
                   "original_name":" Carnegie Mellon",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Authorship attribution typically seeks a small number of textual characteristics that distinguish the texts of authors effectively from each other (see Morton, 1978 for a classic discussion). With small groups of texts, these features can be found by examining frequency lists manually, but statistical tests such as the t-test can also be used (see Binongo and Smith, 1999). For the purposes of authorship attribution, a few items occurring at consistent and consistently different frequencies in all of the known texts by all of the claimants may be sufficient for confident attribution. Most multivariate authorship work focuses on frequent words, following the lead of Burrows (1987, 1988, 1989, 1992a, 1992b, 1994). Much persuasive recent work continues this tradition (Craig, 1999a, 1999b, 1999c, 1999d; Forsyth et al., 1999; Holmes et al., 2001a, 2001b; McKenna and Antonia, 2001; Tweedie et al., 1998). In two recent studies, however, I have shown that cluster analyses based on frequent words often fail to attribute known texts to their authors, and that analyses based on word sequences are sometimes more effective (Hoover, 2001, 2002). Continuing along these lines, I will test the accuracy of analyses based on collocations, while simultaneously examining the effects of using much larger numbers of items than are typically used. Large numbers of words, sequences, and collocations provide more information for potential stylistic analyses, assure that the results take into account a large proportion of the texts under consideration, and, as we will see, usually produce more accurate results. The results of my investigation also show that analyses based on collocations are often more accurate than those based on frequent words or sequences. For this investigation I will define collocations simply as any two words that appear repeatedly within a certain span of words. Preliminary tests show that, perhaps contrary to intuition, meaningful collocations like house...yard, or car...highway, are not very effective for authorship attribution. They do not occur very frequently, and their occurrence depends too much on the content of the text. Many multivariate analyses have been based on function words alone, in the belief that such frequent and relatively insignificant words are most likely to reflect unconscious and regular authorial habits. This suggests the use of collocations of function words, but preliminary tests show that these are also not very effective. The most effective collocations are simply those that occur at the highest frequencies, with the exception of collocations of personal pronouns, which, like collocations of meaningful words, seem too much conditioned by content (especially the characters) of the texts. I omit personal pronouns and any items for which a single text provides more than 80% of the occurrences (typically proper names). To test the effectiveness of collocations in authorship, it seems best to begin with a corpus of texts by known authors, so that various spans, numbers of collocations, and statistical methods can be tested for effectiveness before trying the method on real authorship questions. I begin with a corpus of 10,000 words of pure narrative from fourteen third-person novels by six authors from about 1900, and, as a baseline, test the effectiveness of frequent words and sequences. For the restriction to narrative and to third- person, see Burrows (1987, 1992) and Hoover, (2001). The best results cluster the texts of five of the six authors. Although analysts usually select a small number of items (e.g., the 50 most frequent function words), much larger numbers of frequent words are often more effective. I test the 50, 100, 200, 300, 400, 500, 600, 700, and 800 most frequent items except where fewer items than 800 occur frequently enough to be included. (For this corpus, the best results for frequent words are based on the 300-800 most frequent.) When collocations are tested, various spans and linkages give various results, but several analyses correctly cluster the texts of all six authors, as Fig. 1 shows. A representative completely correct cluster analysis is shown in Fig. 2.  Fig. 1   Fig. 2  It seems useful to test the methods on another genre, as I did in previous work (Hoover, 2001), so my next corpus consists of the first 4,000 words of twenty-one contemporary literary critical articles by ten authors. Here, analyses based on frequent words and sequences each correctly cluster all of the texts once. Analyses based on collocations with spans of two, five, and ten words also succeed. Analyses based on collocations seem to be quite effective in attributing texts to their authors in cases of known authorship, and can now be tested in an authorship simulation to see how well they work under conditions that more closely resemble true attribution problems. The simulation includes the fourteen narratives by six authors discussed above, adds four novels by two new authors, and then two “anonymous” novels, each known to be by one of the eight authors. Frequent sequences succeed for only six of the authors. Frequent words still fail to cluster Kipling’s texts correctly, but they do successfully cluster the four texts of the two new authors. They also consistently cluster one of the anonymous texts with Cather’s texts and the other with London’s. Analyses based on collocations with a span of four words are extremely effective and consistent: the 400, 500, 600, 700, and 800 most frequent correctly cluster all of the known texts, even when the graphs are strictly interpreted, as Fig. 3 shows. Like analyses based on frequent words, these also consistently cluster the anonymous texts with those of Cather and London. These identifications are correct. What makes these results even more impressive is the fact that four of the six added texts, including the two anonymous ones, are first-person rather than third-person narratives.  Fig. 3  The results of my study confirm what many researchers have found: analyses based on the frequencies of frequent words are quite effective in attributing texts to their authors. Analyses based on frequent sequences of words are also often effective, and are more effective under certain conditions, as I have showed elsewhere (Hoover, 2002). Frequent collocations, however, are often more effective than either words or sequences, producing the only completely correct attributions in some cases and producing more consistently correct attributions in others. The frequencies of frequent collocations clearly reflect important aspects of authorial style. Analyses based on them constitute a promising method of authorship attribution and may also prove useful in stylistic studies.  ",
       "article_title":"Collocations, Authorship Attribution, and Authorial Style",
       "authors":[
          {
             "given":"David",
             "family":"Hoover",
             "affiliation":[
                {
                   "original_name":" New York University",
                   "normalized_name":"New York University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0190ak572",
                      "GRID":"grid.137628.9"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Temporal Modelling is an interface designed specifically to meet the needs of humanities scholars wanting to interpret or analyse the subjective experience of temporality in historical documents or imaginative artifacts. Since its inception, Temporal Modelling has had two goals: 1) to provide a responsive interface for visualizing complex temporal relations in humanities data and 2) to experiment with an alternative approach to content modelling in humanities computing. Two years into the project, we think we can demonstrate progress on both fronts. We’ve designed a working prototype suited to humanities scholars that generates XML output concurrent with a user's graphical modelling. Our session describes the interwoven conceptual and technical development of this project (research, visual design development, conceptualization process, and prototype production), and also aims to make it known to a community who we hope will be among its primary users and first adopters. Temporal Modelling demonstrates an alternative approach to content modelling for humanities computing. In the usual sequence of humanities computing events, a content model is developed and then used to create a richly marked data set. Visualizations or graphical displays follow as a result, not as a point of input. By contrast, we’ve created a tool that allows visualization to act as a procedure, not an outcome, of interpretation. Graphical sketching integrates interpretation into digitization concretely, creating a content model. Temporal Modelling consists of two parts. The first we call a composition space or PlaySpace: an interface with a set of visual, graphical elements for making models of temporal relations. The mechanisms and processes of the composition space focus on: the positioning of temporal objects (such as events, intervals, and points in time) on the axis of a timeline; the labelling of those objects using text, color, size, and quality; the relation of objects to specific temporal granularities (the standards by which we mark hours, seasons, aeons); and, in complex interaction, the relation of objects to each other. These can be further modified by a process we call “inflection”—the assignment of attributes with either semantic or syntactic values. For example, a humanities scholar attempting to chart a sequence of events traced in family letters, in which many temporal events are referenced relationally, rather than by date, could use such a system in a preliminary stage of analysis, creating a visual scheme to represent events or references. In addition, however, Temporal Modelling contains tools of interpretation and analysis to characterize events through such inflections as anticipation, regret, foreshadowing or causality. A user’s interpretation is captured and formalized into a structured data scheme that develops concurrently, as the visualization proceeds. User gestures and image renderings are translated into an XML schema which can be exported, used to design a DTD, or transformed through use of XSLT or other manipulations. In the second part of our project, a complementary DisplaySpace will use the same graphical elements to display “published” models from the PlaySpace, as well as supporting display from imported structured data. DisplaySpace models can be manipulated for contrast and comparison in their “published” form or reloaded into the PlaySpace for further refinement. The composition space enables understanding through iterative visual construction in an editing environment that implies infinite visual breadth and depth. In contrast, the display space channels energy into iterative visual reflection by providing a responsive, richly-layered surface in which subjectivity and inflection in temporal relations are not fashioned but may be reconfigured. The objects, actions, and relations defined by our schemata and programming are not married inextricably with specific graphics and on-screen animations or display modes. Just as we have provided tools for captioning and coloring (and the ability to regularize custom-made systems with legends and labels), we have also made possible the upload and substitution of user-made standard vector (SVG) graphics for the generic notation systems we've devised. This is more than mere window-dressing. Our intense methodological emphasis on the importance of visual understanding allows the substitution of a single set of graphics (representing inflections for, say, mood or foreshadowing) to alter radically the statements made possible by Temporal Modelling’s loose grammar. Users are invited to intervene in the interpretive processes enabled by our tool almost at its root level. Temporal Modelling can be used to model date-stamped, or empirical data, but its strength is in its ability to structure the representation of subjective experience. Several of the design features of our project embody this conviction, most notably the “now slider”. This element embeds an individual point of view (or several, depending on the project) into the graphical model and allows the interpretation to be played forward or backward (“progressed” or “regressed”). The model of events changes depending on the position of these individual now-sliders. An event once anticipated may give rise to an unforeseen set of outcomes and be transformed into an interval charged with regret or melancholy. Temporal Modelling attempts to embody the subjectivity of an interpretation, not merely depict a subjective approach. Our initial conceptualization was informed by reading across a range of disciplines and fields. We were interested in a historical, trans-cultural inventory of ways time and temporality have been conceptualized. We grounded our fundamental distinction between time as an a priori category and temporality as a relational conception. Our readings were drawn from logic, religion, anthropology, and philosophy, as well other humanities and social sciences (see References). In addition, we made an inventory of visualizations for showing and analyzing data that have a temporal dimension (). Because we are intent on creating both a composition space and a display space that can utilize the same set of visual elements, we wanted to consider conventions for presenting date-stamped information while concentrating our design on a system suited to humanities documents whose temporal references and/or relations do not conform to empirical models. After this research survey, we created a basic conceptual schema for designing the Temporal Modelling project. This involved several steps 1) defining temporal primitives (entities, actions, and relations); 2) developing a graphical vocabulary for their presentation; 3) developing a labelling and annotation system that allows for customization by individual users. Our assertion is that representations based on empirical approaches assume an objective, homogeneous, continuous, and uni-directional notion of time. We wanted to design a system grounded in subjective, heterogeneous, dis-continuous, and multi-directional temporalities. To accomplish this, we conceived of a number of unique design features in addition to the now slider. These include “stretchy” timelines with variable scales and granularities, branching and alternative narratives of temporal events according to catastrophic (event-driven) and continuous (unfolding) models. Designs for ruptured and discontinuous time are also in the plans. The technical implementation of this design involved creating a tightly integrated relation between visualization and digitization of information. In addition, at each instance, the design of specific elements raises interesting intellectual problems at the intersection of philosophical and computational concerns—for instance, is “regret” a semantic attribute of an event, or a syntactic one, engaged in relational effects? Such issues are central to the intellectual and programming structure of this project. Temporal Modelling is an experiment in using speculative methods— approaches to conceptualization and design of computational tools or projects with uncertain outcomes. The Speculative Computing Lab at the University of Virginia defines its mission as undertaking projects that have a risk of failure. These projects are not necessarily grounded in discipline-specific research and have a tool-based and interpretive aim rather than a collections or archive development goal. Speculative projects are motivated by the desire to foreground these interpretive practices, particularly the subjective practices that are central to traditional humanities. Temporal Modelling is an Intel-sponsored project of the Speculative Computing Lab in the Media Studies Program at the University of Virginia.  ",
       "article_title":"Temporal Modelling",
       "authors":[
          {
             "given":"Johanna",
             "family":"Drucker",
             "affiliation":[
                {
                   "original_name":"Media Studies, University of Virginia ",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          },
          {
             "given":"Bethany",
             "family":"Nowviskie",
             "affiliation":[
                {
                   "original_name":"SpecLab, University of Virginia ",
                   "normalized_name":"University of Virginia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/0153tk833",
                      "GRID":"grid.27755.32"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   THE PROBLEM: REPRESENTING AND ARCHIVING LIVE PERFORMACE Manuscripts, paintings, sculptures, films, and recordings are artifacts that can be preserved and archived for subsequent generations to appreciate and analyze. Live theatre, however, is ephemeral. Is it possible to archive a live performance? One can use film or videotape to document a present-day performance, and, with some creative interpretation and speculation, to recreate a performance from the past. But films and videotapes are incapable of conveying the experience of attending a live performance. A filmed performance offers only a single perspective on the action: the camera decides exactly where to look at each moment. Spectators at a live event, by contrast, act as their own camera operators, selecting their own point of focus—which may not even be on stage. Films omits a vital dimension of live performance: the viewer’s immersion in the world of the theater, and the crucial role that the community of spectators plays in constituting a performance event. The underlying problem here extends beyond the theatrical performance. Precisely the same challenges arise with any kind of performative event, such as dance performances, rituals, political congresses, coronations, parades, festivals, battles, riots, etc. One strategy to address this problem has been to build a physical reconstruction of an historic structure and to stage performances in it, as has been done, for example, with the Globe Theatre in London and with numerous structures in Colonial Williamsburg in Virginia. This solution requires an extraordinary, continuing investment of money and land, and so is feasible only in a very limited number of cases. Moreover, such physical reconstructions are available only to people at one geographic location and implement only one interpretation, and so they cannot be used to evaluate conflicting scholarly interpretations of the historical evidence. Perhaps the deepest problem with such historical constructions is that, while painstaking efforts may be undertaken to achieve historical accuracy in the physical environment, performers and perhaps even the support personnel, the audience itself—and so, ultimately, the context of reception—remains resolutely contemporary.   OUR SOLUTION: THE LIVE PERFORMACE SIMULATION SYSTEM In January of 2002, I began work as Principle Investigator on a project designed to address this problem: “A Live Performance Simulation System: Virtual Vaudeville.” Our strategy is to recreate historical performances in a virtual reality environment. Virtual Vaudeville is, in effect, a single-user 3D computer game that allows users to enter a virtual theatre to watch a simulated performance. The objective is to reproduce a feeling of “liveness” in this environment: the sensation of being surrounded by human activity onstage, in the audience and backstage, and the ability to choose where to look at any given time (onstage or off) and to move within the environment. A vital concern is to find a way to bring the nuances of great stage performances into this virtual environment. To this end, we are using optical motion and facial capture technology to capture real-world performances by professional, highly skilled actors, singers, dancers, acrobats and musicians. This three-year project is supported by a $900,000 grant from the National Science Foundation, supplemented by an additional $110,000 from the State of Georgia. I am leading a team of researchers from seven universities, including the University of Georgia, the University of Pittsburgh, Georgia Tech and the Naval Postgraduate School in Monterey, that includes historians specializing in nineteenth century American theatre, music and culture, computer scientists specializing in high-performance 3D game design, and theatre practitioners. Our long-term goal to develop a flexible set of techniques and technologies that scholars and theatre practitioners can use to simulate a wide range of performance traditions, from Classical Greece to Japanese Noh. Our short-term objective is to complete a fully-functional simulation of nineteenth century American vaudeville theatre.   VAUDEVILLE American vaudeville an especially apt test case for Live Performance Simulation. Vaudeville was the most popular form of entertainment in the United States from the 1880s through the 1920s, functioning in its day much as television does today. Many vaudeville acts both reflected and helped to constitute the enthusiasms and anxieties of their time, especially those concerning the integration of new immigrant groups into mainstream American culture. Consequently a rich simulation of a vaudeville performance will be a useful resource, not just for those interested in theatre history, but for scholars and students of American history generally. A vaudeville performance was divided into many short, self-contained segments. A typical vaudeville bill encompassed a wide variety of acts— contortionist performances, dance numbers, juggling acts, singing groups, comic monologues, blackface comedy, condenses versions of full-length plays—with particular acts in the lineup appealing differently to different groups in the audience. Consequently, simulating different acts of a vaudeville show and exploring the likely responses of different groups of spectators opens up for historical investigation a wide range of ethnic, gender, class, and racialized interactions during America's industrial age. Our simulated performance takes place in B.F. Keith’s Union Square Theatre, a typical Vaudeville house seating approximately 2000 spectators, in the year 1895, fifteen years after the first Vaudeville theatre opened in New York. We are recreating four of the most popular and representative acts on the vaudeville circuit during that time: (1) the strongman Sandow the Magnificant; (2) the Irish singer Maggie Cline; (3) the comic “stage Jew” Frank Bush; and (4) the sketch comedy of the four Cohans, whose youngest member, George M. Cohan, went on to become one of the great stars of early twentieth century Broadway. As we approach the end of our first year of work on the project, have completed archival research into all of these acts and are creating the models and motion-capturing the performances. Our presentation will feature a demonstration of significant portions of the Sandow act that as of this writing are complete and fully-functional.   DESIGN Virtual Vaudeville allows the user to switch between two very different ways of experiencing the simulated performances. In what we call “invisible camera” mode, viewers fly through the 3D space to observe the performance from any position in the theatre and zoom in as close to the performers as they please. Alternatively, the viewer can adopt an embodied perspective, watching the performance through the eyes of a particular member of the audience. Virtual Vaudeville allows the viewer to select one of four spectators, each representing a different socio-economic group in 19th century America: (1) Mrs. Dorothy Shopper, a wealthy socialite attending the performance with her young daughter; (2) Mr. Luigi Calzilaio, an Italian immigrant fresh off the boat, attending the performance with his more Americanized brother; (3) Mr. Jake Spender, a young “sport” sitting next to a Chorus Girl (with him he may or may not strike up a relationship, depending on the viewer’s choices); and (4) Miss Lucy Teacher, an African American schoolteacher watching the performance with her boyfriend from the second balcony, where she is confined by the theatre’s segregation policy. Viewers can switch between any of the avatars at any time, and can move the avatar’s head to focus on different areas of the stage or auditorium, and can trigger a limited set of avatar responses, for example applauding or laughing. Some of these responses are verbal, such as cracking a joke or heckling the performance. In these cases, the viewer selects only the generic response type, and the system produces a specific response appropriate what is happening onstage and off, taking into account the viewer’s previous interactions with other members of the virtual audience. Because the surrounding spectators respond interactively to the viewer's avatar, each viewer has a different experience of the performance event.    SIGNIFICANCE Virtual Vaudeville offers scholars in all disciplines in the humanities a model for a new kind of “critical edition.” A conventional published monograph can pick and choose details to examine, and so lacuna and even contradictions in the historical analysis are easy to overlook. The imperative of precisely recreating both on-stage and off-stage events will demand an unprecedented degree of scholarly thoroughness and rigor.  Key to our project is the depth of the collaboration between technology, scholarship, pedagogy and art. This project is conceived to make a significant contribution to all four domains simultaneously, rather than merely using any one in the service of the others. The end result, we, hope, will represent an important advance in the design and implementation of virtual environments, building on recent successes in creating photo-realistic simulations of real 3D environments by introducing a large quantity of complex human performance data. It will constitute an invaluable work of applied scholarship, an unprecedented resource for visualizing past performances and testing hypotheses about historical performance practices. It will provide an unprecedented resource for students to engage with historical performance traditions as performance (and not as literature or film). Finally, from an artistic perspective, the Virtual Vaudeville project will test the potential of virtual reality technology to provide truly nuanced and engaging theater experiences.   ",
       "article_title":"Virtual Vaudeville: A Live Performance Simulation System",
       "authors":[
          {
             "given":"David",
             "family":"Saltz",
             "affiliation":[
                {
                   "original_name":" University of Georgia",
                   "normalized_name":"University of Georgia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00te3t702",
                      "GRID":"grid.213876.9"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper presents work in progress from a current research project at Hamburg University that employs Humanities Computing methodology for developing and testing a new theory and model of “narrative time”. Our premise is that narrative time should be defined in functional and not in essential or categorical terms: time is not an objective phenomenon, but a cognitive construct and can thus best be modeled in terms of a 'temporality effect'. This effect -- that is, the impression of temporal order in narrative, both on the level of fictional reality and narrative discourse – is to be explained and analyzed in terms of the distribution of empirical 'notions' (representations of objects) and 'temporal operators' throughout a representational medium, in our case: a narrative text. Humanities Computing methodology plays a central role with regard to both the description (markup) and the subsequent combinatory analysis of relevant textual elements. However, adhering to a TEI compliant tagging approach proves unacceptably complicated. The paper therefore argues for a quick and dirty approach to time tagging based on feature structure tags that are defined in the form of PROLOG clauses.   THEORY To date most theories of narrative—in particular those focusing on the domain of literary narratives—conceptualize of ‘time’ in terms of a dichotomy of narrated time vs. time of narration or, as Günter Müller’s classic formulation goes, of Erzählzeit vs. erzählte Zeit. This is essentially an ontological distinction that attempts to set apart two ‘worlds’, each of with is seen to have its ‘own time’. However, this distinction immediately becomes problematic when dealing with non-fictional representations of events which, irrespective of chronological proximity, are by definition situated on a singular objective time line. Our approach is therefore based not on the traditional narratological concept, but rather on the unitary model of time originally proposed by McTavern who distinguished between two perspectives onto time: namely, that of events in an objective before-after relationship (the so-called B-series of time), and that of events as occurring in the subjective cognitive order of future-present-past (A-series).    COMPUTER-BASED IMPLEMENTATION As far as tools are concerned, the implementation of this theoretical model in a Humanities Computing orientated project has necessitated the development of two programs: TempusMarker -- a software tool providing automatic and semi-automatic markup routines for the tagging of temporal expressions in natural language texts. A prototype of TempusMarker has already been programmed.Available for download at . A detailed description of the tool (in German) is included. TempusParser -- an analytical tool that generates a version (or versions, as the case may be) of the base text in which all the sequences that form a complex narrative discourse are organized in strict chronological order. This (re)construction is the result of an algorithm driven process of analysis and recombination of textual segments during which the ‘time stamp’ of each segment as indicated by the temporal tags is interpreted.A schematic representation of the TempusParser architecture and workflow is available at   The computational implementation of McTavern’s model for the purpose of concrete analyses of A-series governed textual discourse and its eventual reconstruction in terms of B-series ordered event sequences offers an interesting example for the difficulties faced by the computing Humanist who tries to tackle even modestly ‘intelligent’ hermeneutic problems. In the project to be presented here an added problem stems from its empirical orientation: rather than having experts (Literary Scholars) tag the texts we are using student groups in order to simulate as closely as possible the ‘naïve’ reader’s processing habits. A demonstration of the actual tagging process, including the use of the TempusMarker prototype, will form part of the presentation. The computational implementation of McTavern’s model for the purpose of concrete analyses of A-series governed textual discourse and its eventual reconstruction in terms of B-series ordered event sequences offers an interesting example for the difficulties faced by the computing Humanist who tries to tackle even modestly ‘intelligent’ hermeneutic problems. In the project to be presented here an added problem stems from its empirical orientation: rather than having experts (Literary Scholars) tag the texts we are using student groups in order to simulate as closely as possible the ‘naïve’ reader’s processing habits. A demonstration of the actual tagging process, including the use of the TempusMarker prototype, will form part of the presentation. Whereas the temporal value of the respective natural language expressions -- be they denotative or deictic -- is comparatively easy to establish either contextually, or from a dictionary, their explication in the form of standardized TEI markup (core tag set + additional tag sets for dates and time, ref. chapter 20.4. of TEI guidelines) has proven rather unwieldy and often contra-intuitive to our readers. Defining more readily comprehensible feature structure tags would seem to be the alternative of choice; however, this raises the methodological question of how to design a sufficiently fine-grained feature structure that does not automatically become completely idiosyncratic to the particular research problem at hand, thus inadvertently restricting the uses of the tagged corpus at a later stage. Against this background the paper advocates and will demonstrate a calculated quick and dirty approach to designing temporal feature structure tags. In particular, it will be shown how the PROLOG predicate structure can facilitate rapid prototyping of feature structure tags.   CONCLUSION Expressing relatively complex hermeneutical problems and models in terms of Humanities Computing methodology and standards should be conceived of as a process of translation, rather than one of mere re-presentation. From a hermeneutical point of view semantic tags are not just descriptors, but rather predicates of a prepositional clause in which the tagged string itself is one argument, and its feature values the subsequent arguments. Capturing experimental temporal feature structure tags in the form of PROLOG predicates therefore holds two advantages: first, it offers a more intuitive approach to semantic tagging. Second, it facilitates automatic conversion of feature structures into composite TEI tags at a later stage, thus turning the quick and dirty into the beautifully intricate -- and fast at that.  ",
       "article_title":"Tagging Time in PROLOG: from quick and dirty to TEI ",
       "authors":[
          {
             "given":"Jan",
             "family":"Meister",
             "affiliation":[
                {
                   "original_name":" University of Hamburg",
                   "normalized_name":"Universität Hamburg",
                   "country":"Germany",
                   "identifiers":{
                      "ror":"https://ror.org/00g30e956",
                      "GRID":"grid.9026.d"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  We describe here the web-based Virtual Center for the Study of Language Acquisition (VCSLA) now under development at Cornell University as a close collaboration between the Cornell Language Acquisition Laboratory (CLAL), Cornell's Albert Mann Library and a set of national and international partners. The purpose of this center is to foster and facilitate active and continuing interactive research on shared data involving many different languages. Taking advantage of the potentialities of the web, we bring together in a truly interactive way the expertise of researchers at CLAL and that of others at a number of scattered institutions, with the experience and capabilities of the library in information technology applied to storage of and access to shared data This first stage of development, financed by an NSF Planning Grant, has a number of crucial features, described here, that in assembly make it an innovative creation that may also serve, as both an ongoing enterprise in language acquisition research and a model for other fields as well.  1. While centered in CLAL, the VCSLA involves the active participation of national and international researchers at other institutions, incorporating a multi-directional flow of information and a perpetual linkage to the library. 2. It incorporates the Virtual Linguistic Lab (VLL), a new web-based interface for data transcription, analysis and access. This interface serves as a common but flexible framework for data entry and access in an interactive manner. This is well under development, and designed to be applicable to both research and teaching. 3. The enterprise embodied is cross-linguistic, including data and research on a widely spread set of languages that will be constantly augmented. VLL furnishes a detailed framework, assuring cross-language comparability, and facilitating both entry and access. It will include the considerable resources amassed in CLAL, and that provided by the participants physically located elsewhere. Thus it will necessarily be under constant revision as information flows in both directions. 4. The presence of the Mann Library in VCSLA is a special and unique element of its design. Mann Library brings its considerable expertise in the areas of data preservation, data archiving, and metadata management into force on the goals of the enterprise. This effort is consistent with the Cornell Library’s long standing active commitment to outreach activities that more firmly engage the library into faculty research and instruction in non-traditional ways, including several of its current digital and Virtual Library initiatives (See for example, “Reinventing the Humanities: Cornell Librarians and Faculty Members Create Electronic Collaborations”, Cornell Library READ!, Winter 2002). 5. The cooperative effort with the library is crucial to the functioning of VCSLA. Among other things, it links the VCSLA to the Open Language Archives Community (OLAC), an international partnership of institutions and individuals who are creating a worldwide virtual library of language resources, by agreeing to use The OLAC Metadata Set (OLACMS), a set of metadata elements for describing language resources. This component thus extends the informational and accessibility resources of the VCSLA.The OLACMS Standard uses XML to represent metadata descriptions, and metadata librarians at Mann are engaged in making the Cornell Language Acquisition Lab and associated VCSLA a metadata provider as well as a service provider within OLAC. That is, the library has built the infrastructure that allows the CLAL to share its metadata with other OLAC participants and it also has provided the interface that allows CLAL to harvest the metadata from other OLAC participants. 6. There are in existence other forms of electronic data sharing for child language research, notably CHILDES (Carnegie Mellon). VCSLA differs from and is complementary to these in both scope and design, most notably in the incorporation of the VLL format for comparability of data entry and access, and in its essential interactive component. That is, it does not represent simply a databank that allows researchers to access and enter data, but a facility that allows interactive and cooperative communication and research by active scholars who are physically separated by location. It is also designed to be a useful element in the training of students, especially potential researchers in language acquisition Also, as an International Web-Based Interactive Infrastructure, it is designed to incorporate new participants and laboratories worldwide, including areas and countries where resources for training and research in the field are limited or nonexistent.   In our presentation we will sketch each of the above components of the VCSLA and its progress to date. We present concrete examples of its applications to both research and teaching in the language sciences.  ",
       "article_title":"Creating a Virtual Center as an International Web-Based Interactive Infrastructure for Research and Teaching in the Language Sciences: A new Research and Library collaboration.",
       "authors":[
          {
             "given":"María",
             "family":"Blume",
             "affiliation":[
                {
                   "original_name":" Cornell University",
                   "normalized_name":"Cornell University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05bnh6r87",
                      "GRID":"grid.5386.8"
                   }
                }
             ]
          },
          {
             "given":"Elaine",
             "family":"Westbrooks",
             "affiliation":[
                {
                   "original_name":" Cornell University",
                   "normalized_name":"Cornell University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05bnh6r87",
                      "GRID":"grid.5386.8"
                   }
                }
             ]
          },
          {
             "given":"Cliff",
             "family":"Crawford",
             "affiliation":[
                {
                   "original_name":" Cornell University",
                   "normalized_name":"Cornell University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05bnh6r87",
                      "GRID":"grid.5386.8"
                   }
                }
             ]
          },
          {
             "given":"James",
             "family":"Gair",
             "affiliation":[
                {
                   "original_name":" Cornell University",
                   "normalized_name":"Cornell University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05bnh6r87",
                      "GRID":"grid.5386.8"
                   }
                }
             ]
          },
          {
             "given":"Tina",
             "family":"Ogden",
             "affiliation":[
                {
                   "original_name":" The Ogden Consulting Group",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Barbara",
             "family":"Lust",
             "affiliation":[
                {
                   "original_name":" Cornell University",
                   "normalized_name":"Cornell University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05bnh6r87",
                      "GRID":"grid.5386.8"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   INTRODUCTION With the development of computing technology, many Chinese museums and museums with significant Chinese collections have digitized and provided images of their collections on Web sites. These online resources offer users around the world access to valuable treasures to learn about Chinese culture. However, there are obstacles that must be overcome to achieve the goals of promoting the Chinese heritage and educating new generations. The purpose of this project is to study how museum practitioners use current image indexing practices and services to retrieve the images of the Chinese collections. Several issues, including image needs, information-seeking strategies, information queries, search functions, display formats and human-computer interaction are examined in this study. This paper focuses specifically on the current practices of image management. The following questions are addressed: What kind of images do the museums index? How do the museums index their image collections? What kind of indexing tools do the museums use?    BACKGROUND OF THE PROBLEM  CURRENT IMAGE MANAGEMENT AT MUSEUMS AND ART LIBRARIES Graham (1999) surveyed 60 art libraries in the U.K. The survey included the important issues of image collections, cataloging and indexing practices, content-based image retrieval (CBIR) systems, and the use of images. Graham’s study reports on the current management of image collections and techniques for image and video retrieval in the U.K. Eakins and Graham (1999) study the current state of the art in CBIR systems within the U.K. and submit several suggestions to U.K. governmental agencies, users and managers of image collections, and CBIR software developers. The Visual Image User Study (VIUS) project at Penn State University is conducting an extensive and systematic assessment of its needs for digital image delivery (Pisciotta, et al, 2001). The VIUS project is working to develop digital picture libraries to serve new uses of digital images for teaching and research.   IMPORTANCE OF INFORMATION NEEDS AND INFORMATION-SEEKING BEHAVIOR TO SYSTEM DESIGN Stephenson (1999) examined several cultural heritage image databases and identified key issues for future improvements. She pointed out that, in addition to technological challenges, the areas of audience, user behavior, and use should be addressed as well. According to Stephenson, museums should examine: For whom are museums building an image database? What image databases are museums building? What are those users’ purposes in using the image database? What functionality do the users need to use the database?  These questions reflect a user-centered design philosophy which assists database designers understand their users thoroughly.   Defining the primary user group  One of the major goals of an online image database is to break the barriers of time differences, geographical locations, and limited physical access to materials. This brings up a critical question: who is the user? The online database is likely to serve a diverse group of users: local and remote users, experienced and naïve users, and existing and new users. Different user services are required for this diverse group of users. Museums should examine the institutional goals and purposes of their online image databases to determine the priority of their missions.    Understanding anticipated users  After determining their primary user group, the museums should study their users. The following questions should be considered: Goals of designing an online image database Differences between existing access to the image collection and the use of the new online image database Anticipated users’ behavior in different locations Selection of surrogates for image indexing and retrieval Information architecture of the online image database     Supporting discovery and retrieval  To use the online image database successfully, the users should have knowledge of information retrieval in general, subject areas, and search systems. On the other hand, the museums should investigate the following factors: Indexing standards such as metadata Query analysis Effectiveness and efficiency of indexing tools and methods Multi-dimensional indexing and retrieval methods Users’ capabilities User support and training     Supporting functionality  New functions may be created to facilitate users’ search strategies. To achieve such goals, studies on interface design, human-computer interaction, and users’ information-seeking behavior should be conducted. Different tools may be required by special users and environments.    Other challenges  In addition to the above key issues, the museums also face several challenges: lack of communication among museums; lack of indexing standards and tools; and lack of translation standards of Chinese into western languages.  Chinese museums and museums with significant Chinese collections should form a consortium to establish communication and to develop collaboration. Many western museums have begun those efforts. The Art Museum Image Consortium (AMICO) is one of those consortiums, but its image collections only have about 6,000 works from Asian cultures (AMICO, 2001). The lack of indexing standards and tools is the same challenge for all museums. Most museums either develop their own indexing standards and tools or do not have adequate professional personnel to manage their image collections (CLIR, 1999; Graham, 1999). Regarding translation standards, although American libraries started using Pinyin as the standard romanization scheme for Chinese characters on October 1, 2000 (RLG, 2000), many museums may not be aware of this change and may still use the Wade-Giles system. These challenges are important to the development of image collections.     METHODOLOGY  PARTICIPANTS Six museums were selected for this study based on the size and diversity of their Chinese collections or their image management. The six participating museums were in the states of California, Illinois, Massachusetts, New York, Ohio, and Washington, D.C.   PROCEDURE   Pre-visit questionnaire  A set of self-administered questionnaires was used to collect librarians’ views on cataloging/indexing practices, the functions of new image management systems, and the use of images. The questionnaires were distributed to librarians before an on-site visit and were collected between January and April 2002.    Follow-up phone interview  After all respondents answered the questionnaires, the investigator examined the questionnaires and conducted phone interviews with the respondents for unclear answers and in-depth information. The investigator identified several key people for observations and interviews when visiting museums.    On-site visit  Based on the knowledge gained from the questionnaires and phone interviews, on-site visits were conducted between June and August 2002. The investigator observed librarians and museum practitioners’ image seeking behavior and also interviewed those people for further understanding of their search behavior. The investigator interviewed museum administrators to obtain their expectations for digital image management in the mission of the museum.     RESULTS AND DISCUSSION According to the questionnaires, observations, and interviews, the investigator reports on the current status, problems, and future of image management: Current Status: all six museums have been digitizing the Chinese collections. Photographic prints, photographic negatives, and transparencies/slides (35 mm) are the most popular formats. All the museums have used or purchased a computer-based image management system to organize the digital images and related information. Problems: most museums did not have comprehensive records in the past, so they have spent substantial amounts of their budgets and people-power to establish basic records or re-enter data for the digital images. Their conventional cataloging and indexing practices are not suitable or transferable for the new image management system. Image management systems used by the six museums are not able to accommodate the features of the Chinese collections and their records. Most image management systems are not metadata/XML ready, which means that the expansion of the systems onto the Web may be limited. Each museum has its unique institutional structure, which hinders the workflow of image management and a lack of communication and collaboration exists among museums departments.  Future: the development of the indexing schema is critical to the management of digital images and to the museum practitioners and on-line users. Image management systems should be enhanced with the standards of metadata/XML, etc. for the Web-based environment.   Regarding the image-seeking behavior of museum practitioners, and the administration's expectations of digital images, the investigator will report the findings of these issues in the future.   ",
       "article_title":"Chinese Collections in Museums on the Web: Current Status, Problems, and Future",
       "authors":[
          {
             "given":"Hsin-Liang",
             "family":"Chen",
             "affiliation":[
                {
                   "original_name":" Graduate School of Library and Information Science, UT-Austin ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   The first ten years of the web have largely represented a triumph of interconnectedness over functionality: in the late nineteen eighties and early nineteen nineties information resources and teaching tools were being developed that were highly sophisticated and interactive. The web, for all its benefits of connectivity, actually resulted in a massive downturn in functionality, and we are only now able to recover some of those functions with newer developments. A paradigm example of this loss of functionality is in the Oxford English Dictionary: the first version of this, released in the late 1980s, with a DOS interface, represented a revolution in data access. Version 2, released in 1992 with a Windows interface added little in terms of functionality, but a great deal in terms of access. However, OED Online, released in 1999, gave connectivity and wider access at the cost of a huge loss of the functions that many users had come to rely upon—so much so that many users have never made the transition from CD. This is by no means the fault of the developers, but is a consequence of the platform that we are all now using. New resources are now being developed for data access and retrieval that take full advantage of the benefits of interconnectedness, while giving us enhanced functionality and also allowing us to integrate complex technologies into an apparently seamless whole. This paper will discuss the development of an advanced Internet resource, Forced Migration Online () that was launched in November 2002.   THE STUDY OF FORCED MIGRATION Forced migration is defined by the International Association for the Study of Forced Migration as ‘a general term that refers to the movements of refugees and internally displaced people (those displaced by conflicts) as well as people displaced by natural or environmental disasters, chemical or nuclear disasters, famine, or development projects’. Forced migration studies are essentially interdisciplinary, drawing from anthropology, history, politics, international law, sociology, psychology, and many other disciplines in the humanities and social sciences. The documentary base of the subject has grown rapidly over the last twenty years, and scholars and practitioners in the field rely for their information and studies upon a diverse body of work: conventional books and journals, but also largely ‘grey’ (unpublished or semi-published) literature. This grey literature can be difficult to get hold of, as it derives from so many different sources: government agencies, non-governmental organizations, academic sources, etc.   THE DEVELOPMENT OF FORCED MIGRATION ONLINE The development of Forced Migration Online (FMO) began in 1997 at the Refugee Studies Centre (RSC) at the University of Oxford. The RSC has the world’s largest collection of grey literature on forced migration (some 15,000 items) and the Andrew W Mellon Foundation granted funding for a portion of this to be digitized. In 2000, the Mellon Foundation and the European Union gave further funding for the development of an integrated portal to be developed on forced migration. The project to develop this portal has been led by the RSC, but with technical and content partners from around the world: the FMO team coordinates participants in some 10 institutions and is working with many more than this to develop content further. FMO now contains 100,000 pages of fully searchable grey literature, 30,000 pages of full-text journal materials, a number of specially-commissioned research guides, a web catalogue with c. 700 entries, an organizations database with c. 800 records, and a prototype image database.   CREATING INFORMATION ARCHITECTURES FOR THE DEVELOPMENT OF FMO As anyone engaged in the development of digital libraries and portals knows only too well, there is no one obvious tool or technology to implement such complex information resources, though many are currently in development. In FMO, there are a number of different technologies underlying the resource: a complexity which is well hidden from the user, for whom access is relatively simple. The full-text documents are presented using Olive Software’s Active Paper Archive, which was originally developed for presentation of historic newspaper content on the web, and which has proved an excellent choice for the grey literature and journals. FMO is the first project that has used this product in this way, and the development was a joint research project between the FMO technical teams at the RSC and at the Centre for Computing in the Humanities at King’s College London (CCH), and Olive Software. The structured information resources and catalogues are delivered using Esprit Soutron’s xdirectory content management system, and various research guides and other documents are created and presented by means of XML/XSLT. The core challenge is one of integration: integration of a wide variety of information types, drawn from geographically separated repositories capable of providing widely disparate levels of metadata; integration of materials in numerous languages in a variety of scripts; integration of the multiple technologies required to meet the differing information processing and delivery functions; integration of academic analysis and advice for practitioners, and of information and knowledge, to meet widely varying user requirements. Delivering a coherent and integrated resource in a seamless way is a non-trivial technical challenge. It involves visual design, architectural design, development of DTDs and style sheets, and the implementation of leading edge (and therefore constantly evolving) products. Managing the input from so many people and places around the world represents another layer of challenge. This paper assesses the problems of developing and integrating these complex technologies into a hybrid information environment, in particular looking a metadata, cataloguing, preservation, delivery and accessibility issues. It reports on the solutions and partial solutions developed so far, and assesses the extent to which the solutions fall short of the ideal. It also discusses a range of further challenges that the FMO team is now tackling: automatic metadata extraction from journal cross-searching tools for different products using advanced APIs; using focused crawlers as aids to cataloguing; automatic categorization of documents for the creation of regional and topical browse sets. The progress in meeting these challenges will be discussed in the context of the more established work in the project. The paper also places the project in a wider context: of past and current work in the development and delivery of scholarly resources, including a number of projects at IATH in Virginia, in CCH at King's College London, and elsewhere; and of digital library research and development, including projects such as the ‘hybrid library’ projects funded by the UK government (including the Malibu project), the DSpace initiative of MIT and others, and the Mellon-funded FEDORA project based at Virginia and Cornell.   ",
       "article_title":"New Technologies, New Strategies for Integrating Information and Knowledge: Forced Migration Online",
       "authors":[
          {
             "given":"Marilyn",
             "family":"Deegan",
             "affiliation":[
                {
                   "original_name":" University of Oxford",
                   "normalized_name":"University of Oxford",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/052gg0110",
                      "GRID":"grid.4991.5"
                   }
                }
             ]
          },
          {
             "given":"Harold",
             "family":"Short",
             "affiliation":[
                {
                   "original_name":" King's College London",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   This paper will discuss a case study of the dual direction migration of metadata describing medieval and renaissance manuscript collections between the Digital Scriptorium database schema and an XML DTD developed to encode extant manuscript descriptions. This discussion highlights the challenges and tensions that exist in trying to merge data that is captured in different data models, roughly categorized as data-centric and document-centric. The observations garnered from this study may have broader implications for other efforts to meld distinct descriptive models for unitary searching and resource discovery.   CONTEXT The author was hired by Digital Scriptorium to: 1)provide XML —> HTML XSLT transformations of an electronic version of the Huntington Library “Guide to Medieval and Renaissance Manuscripts” which has been encoded in the TEI Medieval Manuscripts Description Work Group (TEI MMSS) draft DTD , 2) migrate data from the existing Digital Scriptorium database to the the draft DTD and 3) migrate the TEI-MMSS encoded “Guide” to the Digital Scriptorium’s existing database schema. lIn the context of this work, many interesting challenges have come to light. The Digital Scriptorium database schema and the TEI MMSS draft DTD are closely allied since some of the same people worked on both data models. However, significant differences exist in nomenclature and structuring of the data, both because the TEI MMSS DTD is an extension of TEI and because the purpose of the TEI MMSS DTD is to allow for the capture of extant descriptive information. The database schema, created prior to the DTD is a more constrained data model and uses slightly different elements and nomenclature for its basis.   DESCRIPTIVE TRADITIONS Although the library community has largely settled on a particular record-based model for descriptive information (the MARC record and AACR2) about printed works, serials and selected media, many communities of practice that have need of description to facilitate resource discovery and scholarship have no such common standard. In many cases, a narrative form of description has evolved in an attempt to capture the particular descriptive needs of the curators and users of the materials. Descriptive practice in disciplines such as manuscript collection or archives has varied greatly across nations, institutions and within individual institutions over time. This evolution has largely been dependent on the interests, energies and capabilities of individual curators. Furthermore, descriptive practice varies greatly between communities of practice because of the particular characteristics of the disciplines that they support. Both the elements of description fundamental to supporting work and the nature of their expression may be unique. That these descriptive needs vary greatly has become more obvious as efforts to develop single metadata standards or standards that easily map across disciplines have had limited success. Nonetheless, these communities often face similar problems as they attempt to migrate their formerly print based description to an electronic form that can be shared across institutions. Each community must revalidate what aspect of description is important to the work of the community, 2) define a common way of expressing information about those important things and 3) develop a model that accurately captures that information. The work must be done in an environment with no uniform extant practice and differing philosophies about the nature, scope and role of description in the discipline. This paper will focus on two approaches to capturing significant descriptive data within the same discipline and examine the challenges in merging the two.   DATA-CENTRIC VS. DOCUMENT CENTRIC DATA MODELS The numerous schemes for descriptive metadata that have emerged since the advent of the web vary greatly in their scope, complexity and underlying assumptions about the nature of descriptive practice. Ronald Bourret’s article XML and Databases and his associated materials provide an interesting framework from which to view these different approaches to capturing descriptive metadata. The emerging practices might be categorized into two distinct models: Data-Centric models: These record-like models capture discrete data elements in a structure that is well represented in a flat or relational model (ie. Dublin Core, MARC). Often these flat or relational representations atomize data at the most discrete levels at which one might manipulate it. Some require or enforce the normalization of the data, institute data typing and constrain the syntactical expression in which it is represented. The Digital Scriptorium database schema follows this record-like model, though without some of the rigor of syntax that is required in a MARC record. Document-Centric models: These models capture more narrative or discursive, often complex, hierarchical descriptions. Often, they allow considerable flexibility in what data elements are required, the form in which data is expressed, and rarely (especially prior to the advent of XML Schema) utilize data typing to constrain the expression of information.  These document-centric models may further be categorized as those that provide a representation of an idealized model of descriptive practice and those that represent extant descriptive practice. Those that attempt to capture an electronic edition of an extant description are often the least rigorous in their requirements for data elements and normalization because of the need to accommodate heterogenous practice. The tension between representing an idealized model of a particular document structure—either to facilitate machine processing of the data or to enforce practice in a community—and providing a model that captures existing representations has been discussed extensively by authors such as Piez and in efforts to develop data models such as the TEI dictionaries DTD   CAPTURING EXTANT DESCRIPTION Although document-like descriptions from various institutions and time frames seem similar in format they often contain distinctly different elements of description in distinct orders and with variant nomenclature. This has presented challenges to the developers of DTDs and schemas that attempt to capture descriptive metadata in these communities of practice. DTDs such as Encoded Archival Description (EAD) and the TEI MMSS draft DTD are explicitly designed to accommodate a variety of extant descriptive practice rather than constrain practice to the schema developers’ prescribed notions of practice. The extant documents that are encoded using these models often differ greatly from their record-like cousins. Narrative and complex, they utilize indirect reference, inference from context (the description of a manuscript written in French may never explicitly state the language in which it is written, assuming that the reader can discern from the quoted text imbedded in the description), inference from relationships to siblings or parent elements, and assumptions about relationships of the parts of the description that are not explicit. Although a human reader can infer these relationships, the lack of explicitness makes it more difficult to capture characteristics of the described object in machine processable ways. To capture this information explicitly requires modifying the extant description. Data-centric models tend to capture this information in explicit ways from the start.   MIGRATING DATA Many practitioners and users of existing descriptive material find that even when description is reworked by knowledgeable humans in order to import it to a data-centric metadata representation, the records fail to capture significant information that is embedded in the narrative of these extant descriptions. In many cases, the original cataloger has specialized knowledge of either the collection being described or of the field of study. References to other significant or related work, connections between objects in the collections are often lost in their transformation to data-centric descriptions Tensions between models for capturing descriptive metadata are often amplified at the point at which migration occurs. Automated efforts to migrate encoded extant description have met with mixed success. Chris Prom’s migration of EAD encoded descriptions to Dublin Core RDF records for importation into a cultural heritage OAI database points to some of the problems inherent in automated migration of extant data. Lundberg discusses the implications for information retrieval of data encoded in XML.   THE DIGITAL SCRIPTORIUM: CASE STUDY This paper reports on the outcomes of the effort to automate the migration of data captured in two distinct models and the resulting implications for resource discovery. The paper describes: the differing data models, the process by which the migration occurred, the challenges faced both in migration from relational database to XML and in migration from XML to a relational database model, the development of a relational database model that attempts to capture both the XML encoded manuscript descriptions as well as the data in the existing database the trade-offs in these various representations and the implications for information display and resource discovery.  By highlighting points of ambiguity, strengths and weaknesses of the different approaches to metadata capture and challenges in merging the data into the same repository, it is hoped that this study will inform the work of others who face similar challenges in their efforts to provide descriptive metadata to their user communities.   ",
       "article_title":"Data or Document? Migration of Descriptive Metadata for Medieval and Renaissance Manuscripts Between Data-Centric and Document-Centric Models: A Case Study",
       "authors":[
          {
             "given":"Elizabeth",
             "family":"Shaw",
             "affiliation":[
                {
                   "original_name":" Aziza Technology Associates",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Charles W. Cushman CollectionThis project is funded by an IMLS National Leadership Grant. comprises nearly 18,000 Kodachrome slides, captured from 1938 to 1969 by Charles Weever Cushman, an accomplished amateur photographer, world traveler, and pioneer in the use of color photography. Cushman bequeathed the collection to his alma mater, Indiana University, on his death in 1972, yet its existence was virtually unknown until late 1999 when a university archivist rediscovered the suitcases in which the slides were stored, along with Cushman’s notebooks containing identifying information and dates for each slide. The images visually document in color the vernacular history of people, places, and events that have previously been seen only or primarily in black and white. To provide online access to the collection, a project team at Indiana—including staff of the Digital Library Program, the University Archives, and the University Libraries— has digitized the slides and has designed a relational database to document administrative, technical, and descriptive metadata about the images. Transcriptions of Cushman’s notebook annotations and corresponding documentation on slide mounts will be keyword searchable. In addition, we are creating descriptive metadata for each image by assigning terms for subject content, genre, physical characteristics, and geographic location. The terms are selected from standard controlled vocabularies, primarily the Library of Congress Thesauri for Graphic Materials (TGM I and TGM II), but also Library of Congress Authorities Files (LCAF) and the Getty Thesaurus of Geographic Names (TGN). When all the slides have been cataloged, the database will be mapped to an XML format—most likely Encoded Archival Description (EAD) but possibly Metadata Object Description Schema (MODS)—and will be searchable over the Web. The use of controlled vocabularies in the descriptive metadata of a large image collection has the potential to optimize visual resource discovery, but many issues must be addressed and resolved to ensure the expense of assigning the terms truly results in improved image retrieval: The Library of Congress Thesaurus for Graphic Materials I: Subject Terms (TGM I) and Thesaurus for Graphic Materials II: Genre and Physical Characteristic Terms (TGM II) include topic headings developed by the Library of Congress Prints and Photographs Division over the past fifty years to catalog graphic materials. In adherence with ANSI/NISO Z39.19-1994,ANSI/NISO Z39.19-1984 is the American National Standards Institute/National Information Standards Organization Guidelines for the Construction, Format and Management of Monolingual Thesauri. NISO recently announced plans to revise these guidelines: see . terms are expressed in natural language word order following American English spelling conventions. Although new terms are added regularly based on proposals submitted by catalogers (Alexander), these thesauri frequently lack the specificity desirable for describing individual images. In addition, regardless of the domain knowledge and cataloging expertise of personnel performing image analysis to assign subject headings, inconsistencies are inevitable. Professional catalogers of traditional media readily acknowledge that subject analysis of printed materials may vary from cataloger to cataloger as well as from day to day. In this paper, I will discuss how we have attempted to approximate consistency of image analysis by multiple catalogers, the quality control procedures we have implemented to further ensure uniformity of subject term assignment, and the “work- arounds” we have devised to deal with encountered inadequacies in the selected controlled vocabularies. The Digital Library Federation, the Getty Grant Program, and the Andrew W. Mellon Foundation in cooperation with Rice University are currently sponsoring a Visual Resources Association (VRA) review and evaluation of existing data content standards and current practice in order to compile a guide to good practice for describing cultural objects and images. The forthcoming Cataloguing Cultural Objects: A Guide to Describing Cultural Objects and their Images—the CCO Guide—will provide recommendations and examples for using data value standards tools and building authority records.See  and “Project Proposal for Guide to Good Practice: Cataloging Standards for Describing Cultural Objects and Images,” Digital Library Federation, January 12, 2001, at . Used in concert with XML data structure and communication standards, the CCO Guide will facilitate consistent creation of descriptive metadata for visual resources, enabling interactive and complex search options. Paradoxically, Open Archive Initiative Metadata Harvester (OAI-PMH) service providers require data providers map rich metadata to unqualified Dublin Core, while content aggregators like the Research Libraries Group (RLG) Cultural Materials service must currently treat all controlled vocabulary terms as keywords.See the Research Libraries Group (RLG) Cultural Materials site at . In this regard, the RLG states, “Ultimately, we hope to add powerful ‘assisted searching’ tools, to help users navigate across different vocabularies and subject schemes, but in the meantime we are simply using whatever subject terms are provided in the source data.” (See ). Although neither OAI service providers nor RLG preclude the use of multiple or complex metadata formats, content creators nevertheless face a quandary: why commit extensive and expensive personnel hours to create increasingly rich metadata when interoperable and federated access is possible when implementing simpler, less costly, metadata standards?I am, of course, playing the devil’s advocate here. The foremost concern when designing and implementing an image retrieval system should be the searching needs of the user community (Sundt). Art historians, for example, may be trained in the use of one or more controlled vocabularies specific to art images,For example, Getty Art & Architecture Thesaurus (AAT) at , ICONCLASS at http://www.iconclass.nl/, and Categories for the Description of Works of Art at . but users of vernacular photography collections may have limited experience with any controlled vocabulary. Since lack of familiarity with the underlying vocabulary tools may actually diminish rather than enhance user interaction, the search interface should ideally facilitate direct mining of the encoded metadata, directing the user to broader, narrower, related and cross-referenced subject terms. This paper will conclude with a review of our usability test findings in respect to a prototype interface that integrates TGM I and II terms into the search facilities for the Cushman Collection.   ",
       "article_title":"The Charles W. Cushman Collection: Enhancing Visual Resource Discovery Through Descriptive Metadata Based on Subjective Image Analysis",
       "authors":[
          {
             "given":"Linda",
             "family":"Cantara",
             "affiliation":[
                {
                   "original_name":" Indiana University Libraries",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  It has become a commonplace to declare that we are living through one of the great epochs of human discovery. “The digital revolution.” The Internet as a “fundamental and extensive force of change.” With the advent of the Internet as a mode of communication and information discovery, we are leading lives, and facing challenges undreamed of in previous generations except as the stuff of science fiction. But what if it’s actually true? What if we are living through a period as significant as the Renaissance or the Enlightenment? What if our current circumstances are comparable to the Depression? Or the Post–war era? How about the counter–culture revolution of the 60s? Each of these eras, great and small, are marked, not only by their social upheavals, but also by their great strides in artistic representation. The Renaissance had Michelangelo and Leonardo; the Enlightenment had David and Ingres. In the modern era, we have Frank Lloyd Wright, Piet Mondrian, Mark Rothko, Andy Warhol, Georgia O’Keeffe, le Corbusier, Picasso, Matisse, and the list goes on and on. Great artists, whose work helps us make sense of the past. Of course, we also have meaningful contemporary art—but there’s a problem. Whereas we can still look at Michelangelo’s paintings and sculptures, and in many cases, can look at preliminary studies and drawings—we’re running the risk of loosing contemporary art as soon as ten years after its creation The new media art community has recently come to the realization that the objects and ideas of their most compelling thinkers and artists run the risk of disappearing forever, because these objects are often in digital or other variable formats, which tend to rapidly become, at best, inaccessible; and at worst, irretrievably lost. Unless there is a systematic and persistent exploration of preservation and archival procedures for these significant cultural objects, they will, in all likelihood, not be accessible for future generations of artists, scholars, or the general public. One of the new media community’s most important advances towards this goal of systematic investigation is the development of the “Archiving the Avant-Garde” project, which brings together new media venues, curators, and artists to devise possible solutions to this very difficult problem of the disappearing objects of the avant-garde. There are three distinct challenges that the new media art community has to confront. They must first assess the nature and goals of new media art in general. It’s very difficult to preserve a nebulous “something,” especially if that “something” is a complex series of digital objects with variable and intricate relationships between parts and the whole, which also happens to comment and feed off of a culture in which the conservator is currently living. Unless the preserving agency has a convincing vision of what is intrinsically important to maintain, the resulting objects run the risk of being inauthentic and/or presenting an inaccurate indication of the artist’s intentions. It would also be prudent for this community to consider the current state of art conservation and preservation in related fields. It’s important to recognize the challenges and philosophical conflicts that art conservators of the last half century have faced. No matter how cutting edge and exciting these new media artifacts are, they are not being created in a vacuum, and conservators have faced similar (although not digital) problems for hundreds of years. I will specifically review methods of traditional art conservation, and how those professionals are dealing with less stable forms like conceptual and performance art. Finally, the community should recognize and reflect on the history of access and preservation from an archival point of view. The project is, after all, called “Archiving the Avant-Garde,” and there are some common tropes and methods within the archival community, particularly the ideas of diplomatics and intrinsic value, for example, that the new media art community might find helpful. Information technology professionals and humanist scholars have a responsibility to keep the past alive. The information scientists provide the infrastructure and methodologies, and the humanists provide the interpretive structure and deep understanding from which future generations will gain insight into our collective consciousness. Unless we begin working together, building systems that grow naturally out of the scholarly tradition, rather than expecting the scholarly tradition to twist itself into knots trying to fit within the current Information Infrastructure, we will never be able to transcend the prosaic world of subject headings and metadata elements. Information artists and the new media art organizations are accomplishing, in the work itself, this very feat of bringing together traditional forms of expression with new technologies. We should take a lesson from them. We need to find a way to save and preserve the richness of this experience for future generations.  ",
       "article_title":"Preservation of the New Media Arts",
       "authors":[
          {
             "given":"Megan",
             "family":"Winget",
             "affiliation":[
                {
                   "original_name":" UNC - Chapel Hill",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The structuring qualities of the computer screen play an important part in how we understand the Internet and other computer-facilitated representations but they are rarely addressed in the critical literature. In this presentation, I consider how the screen and mediation remain largely invisible because the Internet is described as a material setting that users inhabit. I provide a brief study of popular and academic narratives and focus on renderings of the computer and Internet as a window or entrance that appear in telepresence art works, webcams, and advertisements for computer screens. Close visual and textual analysis and critical considerations of photography’s referential aspects are employed. These methodologies provide a way to highlight the computer screen and encourage an alternative understanding of the computer and Internet. The tendency to engage with the computer and Internet as physical and animate has significant ramifications. Our conceptualization of the medium determines the questions that we can ask and whether the representational aspects of stereotypes can be perceived and critiqued. Judith Mayne has argued that feminist film theorists such as Laura Mulvey have attributed “the polarity of gender, of masculinity versus femininity, to the very structures of pleasure and identification in the classical cinema” (48). Computers and Internet settings employ similar devices. However, the forms of identification produced by computers and Internet settings have an even more consequential effect because users spend significant amounts of time engaging with computers; computers and networks also appear in film, television, and print advertising; dream or trance-like experiences are often part of the engagement; the user’s identification with characters and other representations can be intense; and there is an idea that people are alive and their bodies are accessible through the Internet. Considering the screen rather than the delivered representations is difficult because of the rhetoric about physical and populated Internet settings. For instance, Esther Dyson makes it appear like people live on the Internet rather than use it when she states that “the Net includes all the people, cultures, and communities that live in it” (2). Eduardo Kac uses his telepresence artwork, Teleporting an Unknown State (1994/96, 1998, and 2001), to render the “Internet as a life-supporting system” (Kac) and suggests that there is “Birth, growth, and death on the Internet” (Kac a). Such narratives about people inhabiting the setting and live interfaces are connected to early descriptions of the computer and Internet. William Gibson suggests that physical settings and screens will combine in the first sentence to Neuromancer when he states that “the sky above the port was the color of television, tuned to a dead channel” (3). Gibson starts Neuromancer, which had a significant effect on computer culture, with the screen but he displaces its constructed aspects with live programs, cyber “space,” and populated interfaces. Internet sites continue to provide “welcome” messages, invitations to enter, and depictions of spatial progressions. For instance, the webcam operator Gwen encourages the spectator to “enter the life of a college student” and Cindy coaxes the spectator to “ENTER ... with an open mind.” A variety or multi-purpose sites are referred to as “portals” and Gretchen Barbatsis, Michael Fegan, and Kenneth Hansen indicate that spectators “engage the computer screen as a gateway to another place.” Window-like effects are employed because they seem to provide views “onto” other terrain, articulate an inside and outside, and suggest that computers and the Internet deliver a continuous spatial landscape. Thomas J. Campanella suggests that “Webcams, the Web’s windows on the world, knit the Net to the physical spaces we inhabit. The accompanying illustration by Jack Desrocher supports this conception by depicting the computer monitor as a curtained window that is incorporated into the home setting. Such renderings make representations seem to be part of the lived space of the spectator. The conception that the computer and Internet are a window or portal, which provides an entrance into other places, is related to societal conceptions of photography. Photographic images are often talked about as if they provided direct access to the thing depicted—the referent—rather than being representations. This cultural conception is conveyed by Susan Sontag when she states that photography is “not only an image (as a painting is an image) an interpretation of the real; it is also a trace, something directly stenciled off the real” (154). Internet sites often use data, texts, and graphics instead of digitized photos and digital imaging technologies to make representations seem real. However, society’s usual conception of photography can still provide another example of a technology and cultural form in which the framed aspects of images are often ignored. Histories of photography and other referential media-like film and television-indicate that the physical and animate aspects attributed to the computer and Internet are not unique and that these renderings should be associated with other produced forms. Such photography theorists as Rosalind Krauss, Martha Rosler and John Tagg, also offer some significant literature to employ in engaging with the produced and framed aspects of computer and Internet renderings. A re-employment of the photographic vocabulary of cropping and rectangular formats can highlight the relationship between individual screen elements, software windows, and monitors. Noting and articulating the repetitive aspects of computer and Internet representations can also indicate that it is cultural conventions rather than people that are conveyed. For instance, Tagg argues that realism “works by the controlled and limited recall of a reservoir of similar ‘texts,’ by a constant repetition, a constant cross-echoing” (99). Such critical considerations of photography may be more productive in highlighting the produced aspects of computer and Internet settings than in helping society to engage with photography differently. Computers and Internet settings rarely employ traces of material objects in the ways this concept operates in photography. The redeployment of photography theory can also begin to render a new vocabulary for describing and conceptualizing these settings. We should continue to consider other possibilities for speaking, writing, designing, and visually rendering the Internet since these acts produce the setting.  ",
       "article_title":"The Screen or the Window: A Critical Proposal for Reading Computer Representations",
       "authors":[
          {
             "given":"Michele",
             "family":"White",
             "affiliation":[
                {
                   "original_name":" Wellesley College",
                   "normalized_name":"Wellesley College",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/01srpnj69",
                      "GRID":"grid.268091.4"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  One benefit of the development of Immersive Virtual Environments (IVEs) for education is that students in the humanities may now encounter and learn about places and cultures that which are difficult to visit or may no longer exist. As increasingly sophisticated graphical interfaces have become possible, many IVEs have decided to use this technology. Yet some might question whether the highest fidelity interface is always necessary or even desirable for the study of the humanities. This project seeks to answer that question by developing two parallel versions of an IVE that represents the Like-a-Fishhook Village and Fort Berthold area in western North Dakota as they existed in 1858, when people lived there, and in 1954, when it was being excavated. Both versions will permit users to explore the environment freely, but one version creates a complete, three-dimensional visual representation while the other is constructed mostly of text with a few carefully chosen images and sounds. To understand the story Like-a-Fishhook Village, the last great earth-lodge village in the Great Plains, and Fort Berthold, the nearby fur trading post, is to understand the history of west. The Mandans, who had befriended Lewis and Clark and the Corps of Discovery in 1804–04, and their close neighbors, the Hidatsas and Arikaras, had lived in sedentary earth-lodge communities along the Missouri River for centuries, hunting, raising corn and other crops, and trading extensively with neighboring tribes. Despite a basic similarity of economic and social life, these peoples differed remarkably in language and customs. The unification of these three tribes at Like-a-Fishhook village tells of the significant impact that the coming of European American people had upon Native American cultures. Although they had suffered several epidemics previously, a smallpox epidemic in 1837 reduced the Mandan population to fewer than 200 individuals. The Hidatsa and Arikara populations, although not as severely struck, shrank as well. Ethnohistorical evidence suggests that the first permanent residents of Like-a-Fishhook Village were Hidatsa who arrived in 1845, and that they were joined shortly thereafter by a smaller group of Mandans; the two tribes felt it was beneficial to combine their numbers in order to resist the attacks by Sioux tribes in the area. Although living together, each tribe maintained its own language and culture. P. Chouteau, Jr. and Company, an offshoot of the American Fur Company, quickly established a trading post north of the village, and European Americans were part of the community from then on. Many changes occurred in the next forty years. The fur traders were followed by missionaries, United States government representatives, and soldiers, all people who wanted the natives to change their ways. Fur trading resulted in increased competition for natural resources, which caused more friction than usual among the tribes, and the Arikara, hoping for support in the battles against the Sioux, joined the other two tribes at the village in early 1860s. In the mid 1880s, the United States government forced the residents to abandon their village life and to take up individual allotments scattered throughout the reservation. Archeological salvage excavations were carried out at the site in 1950–52 and in 1954 by the State Historical Society of North Dakota, under contracts with the National Park Service and by the River Basin Surveys of the Smithsonian. However, the rising waters from the Garrison Dam and Reservoir project ultimately inundated the site, which now rests about a mile offshore under the waters of Lake Sakakawea (Garrison Reservoir). Although many individuals were involved in that work, the final report was written by G. Hubert Smith and published by the U.S. Department of the Interior (Smith, 1972). A multidisciplinary team is creating two parallel IVEs based on the same content: both recreate the village site as it appeared in 1954, when it was being excavated, and also in 1858, when it was occupied, and visitors will be able to “time travel” from one period to the other. The 3-D, graphically rich version is modeled on other IVEs that have been created at NDSU for students to learn to do science by conducting simulated experiments and solving authentic (albeit virtual) problems. Science-based systems of this sort have demonstrated statistically significant impact on student learning in controlled studies (McClean et al., 2001). The great strength of this version is the visual stimuli including an environment that visually depicts all space and creates the sense of moving through that space. In addition, novice users will probably find it easier to learn how to interact with this IVE. The largely text-based version uses the enCore Xpress MOO interface, and while it does allow the inclusion of some visual and audio components, it requires students to use written language to interact with the virtual environment. The MOO be better able to create multiple perspectives of the multiple cultures represented in the site. In addition, students will, after a process that involves review by scholars and others, be able to enrich the MOO with their own writing, to become producers as well as consumers of content. The MOO version is platform independent and requires less bandwidth than the 3-D graphically rich version. Both types of interfaces have the ability to engage users in the environment, but the question is whether one or the other is more effective for certain types of learning. The 3-D graphically rich version of the Like-a-Fishhook IVE will be used by archeology students to learn to think like archeologists. However, both versions will be used by a multidisciplinary, writing-intensive humanities class, which will include students from Fort Berthold Community College, descendents of the village inhabitants. Students will experience the site via the IVEs: they will observe, analyze, reflect-and write-about the area, the cultural changes, the history, and the virtual environments themselves. Students from all disciplines will meet together (at least in virtual space) and discuss the conventions of their disciplines. Then they will explore the IVEs together with a set of questions that require exploration and investigation in order to find the answers. They will also write parallel assignments that are appropriate for their disciplines. For example, all students will keep journals to record their visits to the IVEs, journals that stress purposeful observation and inquiry, but each will concentrate on recording the type of information suitable for his or her discipline as they travel through the IVEs. The journals will be used as the basis for other writing assignments. For example, all students might write about an artifact found during the excavation, but the anthropology student might write an analysis of its cultural significance, the creative writing student a poem or a piece of creative non-fiction about it, and the public history student a museum script for it. Longer papers will require additional research using conventional sources as well as more innovative ones like the Digital Archive Network for Anthropology (DANA) being developed at NDSU and which includes 3-D scans of actual objects found at the Like-a-Fishhook site. The creative writer might write a piece of historical fiction, the anthropology student a scholarly article on some aspect of the cultures depicted in the IVEs, and the public history student a brochure for the site such as the ones that are produced for visitors to actual historic sites. Students from all disciplines might collaborate on the making of a short documentary on some aspect of the area or a multi-vocal web essay that combines the expertise and perspective of several disciplines. After a process of review, student writing can be incorporated into the MOO version to enrich it for future visitors. The value of each IVE as it relates to the learning in the humanities will be assessed in several ways. The journals kept by the students during their visits to the IVEs will be assessed to determine (1) the level of engagement in each version of the IVE, (2) any differences they note about the different experiences in the IVEs, (3) the type of information they notice in each IVE, and (4) how well they understand the time periods, the cultures, and the places they are observing. In addition, students will complete surveys that ask them to respond to questions that solicit similar information. These results will provide useful information about the strengths and weaknesses of both types of interfaces.  ",
       "article_title":"Visual or Verbal: Two Approaches to Creating an Immersive Virtual Environment",
       "authors":[
          {
             "given":"Eunice",
             "family":"Johnston",
             "affiliation":[
                {
                   "original_name":" North Dakota State University",
                   "normalized_name":"North Dakota State University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05h1bnb22",
                      "GRID":"grid.261055.5"
                   }
                }
             ]
          },
          {
             "given":"Jeffrey",
             "family":"Clark",
             "affiliation":[
                {
                   "original_name":null,
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Brian",
             "family":"Slator",
             "affiliation":[
                {
                   "original_name":null,
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Gary",
             "family":"Clambey",
             "affiliation":[
                {
                   "original_name":null,
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Aaron",
             "family":"Bergstrom",
             "affiliation":[
                {
                   "original_name":null,
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Shawn",
             "family":"Fisher",
             "affiliation":[
                {
                   "original_name":null,
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Justin",
             "family":"Hawley",
             "affiliation":[
                {
                   "original_name":null,
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"James",
             "family":"Landrum",
             "affiliation":[
                {
                   "original_name":null,
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"David",
             "family":"Martinson",
             "affiliation":[
                {
                   "original_name":null,
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"J.",
             "family":"Vantine",
             "affiliation":[
                {
                   "original_name":null,
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  I describe and evaluate three methods for automatically identifying in English text a frequently occurring type of multiword token, the lexicalized noun compound. The methods combine symbolic part-of-speech information with different measures of collocational strength, namely minimal frequencies of occurrence in a corpus, log likelihood of association, and a combination of these two. The results of testing the methods on two software manuals of approximately 170,000 and 210,000 words suggest that although raw frequency is the best single measure overall, the combined strategy is useful to the extent that one favors precision over recall. I also discuss the limitations of the corpora and the test and suggest additional applications of the methods. A noun compound, like stone wall or stock market, is a series of nouns that function syntactically as a single noun, inheriting the features of the head (final) noun. In some languages, noun compounds are not separated by whitespace and are thus trivially identifiable as words. For instance, the English compound departure time is Abfahrtzeit in German (Abfahrt ‘departure’ + Zeit ‘time’) and lähtöaika in Finnish (lähtö + aika). A lexicalized compound is one that has acquired a conventional or specialized meaning. The compound garbage man, for instance, could refer to a man made of garbage (cf. snow man) or a man who delivers garbage (cf. milk man), but has a more salient lexicalized meaning. In some cases, lexicalization is reflected in orthography, as in the single words fireman and policeman, and occasionally one finds both multiword and single-word versions, such as air mail and airmail (both attested in the Brown corpus). It would be useful to be able to treat compounds like garbage man as single terms on par with their orthographically unitary counterparts for purposes such as document indexing and classification. I approach the task of identifying such compounds as a secondary tokenization step. Tokenization, often unfairly regarded as an uninteresting bit of text preprocessing, requires one to make nontrivial decisions about what constitute minimal “word-like units” for further analysis (Grefenstette & Tapanainen 1994). In addition to garden-variety words, tokens include punctuation, which is important for identifying clause and sentence boundaries, and multiword units. Karttunen et al. (1996) divide these multiword units into several categories: adverbial expressions like “all of a sudden,” prepositions such as “in spite of,” date and time expressions, proper names, “and other units.” Typically, a basic tokenizer first segments the text into simple units, then one or more “multiword staplers” group tokens together again (Karttunen et al. 1996). What is unique about the approach presented here is the combination of part-of-speech information, used to identify noun compounds, and collocational measures. Although “highly collocated” and “lexicalized” are not the same thing, I suggest that the former can serve as one useful indicator of the latter. The procedure works as follows. The text is processed by a basic tokenizer, tagged for part-of-speech, and then the noun sequences are extracted. The goal is then to identify the subset of these compounds that might qualify as lexicalized terms by measuring the collocational strength of the component nouns. The simplest way to identify possible collocations is to extract all those that occur at or above a certain frequency cutoff. One might hypothesize, for example, that if a certain noun compound occurs five times in a text, then it has a lexicalized meaning. Collocational strength can also be measured by compiling all of the bigrams found in the text and comparing the rate of co-occurrence of the elements with that expected by chance. Although there are several possible measures, I use the log likelihood statistic, which has been shown to be preferable to alternatives such as chi-square and mutual information (Dunning 1993). To generate collocations of more than two words, a separate bigram merging process is performed on the corpus. If, for example “NASDAQ composite” and “composite index” are both significant collocations, then the trigram “NASDAQ composite index” will be extracted as well if it occurs in the corpus. In practice, this method can generate terms that are quite long, such as “American Stock Exchange Market Value Index,” an example extracted from a portion of the Wall Street Journal corpus. It is also possible to combine these methods, by extracting compounds that are above certain frequency and likelihood threshholds. Although the two measures generally correlate (that is, frequently occurring compounds tend to have larger likelihood scores), they are sensitive in different ways to corpus size. To evaluate the procedure, I extracted noun compounds from two software manuals and compared each list to the compounds found in each manual’s index, which would be expected to contain the significant terms. A baseline procedure using all the noun compounds averaged 0.26 precision and 0.77 recall on the texts. In other words, about a quarter of the compounds occurring in the texts were found in the indexes. Recall is less than 1.0 because some ideas or topics that do not occur in the text as compounds are reformulated as such for the index. The 0.77 score thus serves as an upper bound on recall. Assigning equal weight to precision and recall, the best performing strategy was to use a minimum frequency of 4 for the first text and 3 for the second, with an average of 0.474 precision (82.3% higher than the baseline) and 0.527 recall (31.6% lower than the upper bound). Adding a conservative log likelihood score threshold increased precision by an average of 13.6% but lowered recall by an average of 24.1%. Unless one substantially discounts recall, the likelihood score was not as useful as anticipated. The results indicate that measures of collocational strength are useful in separating lexicalized noun compounds, which are profitably viewed as multiword tokens, from nonlexicalized ones that are best analyzed as token sequences. Such methods can facilitate the automatic indexing and classification of documents for textual analysis and search and retrieval applications. Two important limitations on these results are the restriction to a particular kind of technical text and the nature of the test itself. It is by no means self-evident that an index should contain all and only the lexicalized noun compounds of a text. Such a test is at best indirect, and the project should thus be viewed as preliminary work indicating the feasibility of the approach. Future work will test the generalizability and robustness of the methods by identifying other tests, such as using a glossary rather than an index, and applying the methods to corpora of different sizes and genres.  ",
       "article_title":"Identifying Multiword Tokens Using POS Tagging and Bigram Statistics",
       "authors":[
          {
             "given":"Mark",
             "family":"Arehart",
             "affiliation":[
                {
                   "original_name":" University of Michigan",
                   "normalized_name":"University of Michigan–Ann Arbor",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00jmfr291",
                      "GRID":"grid.214458.e"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   The Newcastle Electronic Corpus of Tyneside English (NECTE) project is based on two separate corpora of recorded speech, one of them collected in the late 1960s as part of the Tyneside Linguistic Survey (TLS), and the other in 1994 by the Phonological Variation and Change in Contemporary Spoken English (PVC) project. Its aim is to combine the TLS and the PVC collections into a single corpus and to make it available to the research community in a variety of formats: digitized sound, phonetic transcription, and standard orthographic transcription, all aligned and available on the Web. We are currently developing a methodology to study NECTE from a sociolinguistic point of view, and have begun by looking at the one formulated by the TLS, which was radical at the time and remains so today: in contrast to the then-universal and even now dominant theory-driven approach, where social and linguistic factors are selected by the analyst on the basis of a predefined model, the TLS proposed a fundamentally empirical approach in which salient factors are extracted from the data itself and then serve as the basis for model construction. To this end, an electronic corpus was created from a subset of the data, and various cluster analysis algorithms were applied to it in order to derive social and linguistic classifications of the sample. Stability of classifications across different clustering methods was already a known theoretical problem. The clustering techniques available at the time, and still widely used today, are sensitive to factors such as vector distance measure, clustering algorithm, and the order in which data items are presented —different combinations of these factors typically yield different analyses of the same dataset. These effects were observed in the TLS classifications. In an experiment on artificial data sets Jones (1979) demonstrated that certain combinations of clustering algorithms are capable of imposing erroneous structure on data which was inherently unclassifiable (by design). The types of structurings derived were consistent with theory and observation; for example Ward’s method tended to ‘discover’ spherical clusters irrespective of the natural structure of the data and classifications were shown to be sensitive to input order of datapoints. These properties of clustering techniques raise at least two issues relating to validation of classifications: objectivity—to what extent does a given analysis represent the actual structure of the data, and to what extent is it an artefact of the clustering method? selection—upon what criteria does one choose among alternative analyses?  We seek an approach to classification which improves on the methods available at the time when the first analyses of the TLS data were conducted, that is, techniques as insensitive as possible to variation in the sort of parameters identified above. In this paper we consider as a candidate a method that had not been invented when the TLS was active —the self-organizing map (SOM). The discussion is in three main parts. The first part outlines the TLS methodology, the second describes self-organizing maps, and the third compares the consistency of the analytical methods used by the TLS with that of the SOM relative to the TLS phonetic data.   TLS METHODOLOGY The TLS aimed to model the overall linguistic variability of an urban community, that of Tyneside in north-east England, and more specifically to identify and exhaustively characterise the varieties of speech which co-occur in that area, and to determine the distribution of both the speech varieties and their constituent elements across the relevant social subgroups  To achieve this aim, a methodology was proposed which differed fundamentally from the one standard at the time, and which was taken to have important theoretical and practical consequences for the discipline of sociolinguistics. In place of the theory-driven approach which characterized the work of Labov and Trudgill, among others, in which a sociolinguistic model was defined and then used to select a relatively small number of social and linguistic variables for analysis, the TLS was conceived as a set of methods whereby the salient features of linguistic and social diversity were to be empirically determined by, rather than presupposed in, the model. This methodology was designed to generate multiple candidate hypotheses about the data by applying high-dimensional multivariate analysis methods to it in different ways, from which those most useful to the aims of the TLS could be selected.   SELF-ORGANIZING MAPS The self-organizing map, also known as the Kohonen net after its inventor, is a k-dimensional surface of processing units, where k is usually 2, together with a buffer into which input vectors are loaded. Associated with each unit is a set of connections from the input buffer such that, for a buffer of length n, there are n connections per unit, and each connection can take on a real-number value or ‘strength’ in some range, typically -1..1 or 0..1 (for clarity, only sample connections are shown in Figure 1): Figure 1  Because it is an artificial neural network, the SOM is not explicitly configured or ‘programmed’ to behave in some desired way like a conventional computer, but rather learns its behaviour by exposure to input data using a learning algorithm. A full explanation of SOM learning would take us too far afield; details can be found in most textbooks on artificial neural networks. In outline, though, learning takes place by repeated presentation of vectors drawn randomly from a set V, and adjustment of the connections at each presentation. The SOM is initialized by assigning random values in some range (ie, 0..1) to the connections. When the first vector vi is presented, it activates the processing units to varying degrees, depending on the differences in connection strengths between the input buffer and each unit; the most highly activated unit uj is selected, and the connections are adjusted so that, next time vi is presented, uj will be even more highly activated than before, thus associating vi ever more strongly with a specific location on the map. As learning proceeds over—usually—many thousands of presentations, each of the vectors in V is associated with a specific unit in the map. After learning is complete, the entire set V is presented once again. Each vector activates the unit with which it has learned to become associated, and the result is a pattern of activations on the map surface. That pattern is significant: the distances among activated units represent the similarity relations in the input vector space.   COMPARATIVE STUDY This comparative study confines itself to the phonetic-level representation of the TLS corpus. In order to apply cluster analysis to this data, the TLS had to represent it numerically. The method was as follows. For each of the 52 informants whose phonetic-level transcriptions had been digitized, the number of token occurrences of each of the 542 state types S defined in the transcription protocol was counted, where a ‘state’ is a discrete phonetic segment type. Each informant’s phonetic profile was thus represented as a 542-element integer-valued vector V, in which any element Vi contained the number of token occurrences of state Si. The set of informant vectors was stored in a 52 x 542 matrix which, after normalization, served as input to the various clustering algorithms used in the analysis. The present study replicates the TLS data representation and cluster analyses, and then compares the performance of a SOM on the same data, using a variety of settings for initialization of connections, sequence of input vector presentations, and map dimension.   CONCLUSIONS Preliminary results from a relatively small subset of the 52 TLS informants indicate that the SOM performs as well as the other clustering algorithms in terms of its ability to identify and represent clusters, but that it is far less affected by variation in processing parameters. Results for the full TLS dataset will be available if and when this paper is presented.   ",
       "article_title":"Cluster Analysis of the Newcastle Electronic Corpus of Tyneside English: A comparison of Methods",
       "authors":[
          {
             "given":"Herbert",
             "family":"Moisl",
             "affiliation":[
                {
                   "original_name":" University of Newcastle",
                   "normalized_name":"University of Newcastle Australia",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00eae9z71",
                      "GRID":"grid.266842.c"
                   }
                }
             ]
          },
          {
             "given":"Val",
             "family":"Jones",
             "affiliation":[
                {
                   "original_name":" University of Twente",
                   "normalized_name":"University of Twente",
                   "country":"Netherlands",
                   "identifiers":{
                      "ror":"https://ror.org/006hf6230",
                      "GRID":"grid.6214.1"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   FOREWORD  A program called VINCI from Queen's  Employs computational means To make metre and rhyme In right to left time So limericks appear on your screens.   The people attending this session We hope will have gained the impression  That these powerful tools, With appropriate rules, Can give rise to poetic expression!    BACKGROUND There is evidence that the generation of at least some classes of humour is a rule-governed activity, formalisable in terms of algorithms, and implementable computationally in order to generate actual humorous utterances. In earlier work, using the VINCI natural language generation environment, we have shown this to be the case for several subclasses of verbal humour, including Tom Swifties and several classes of riddles. (See Lessard & Levison, 1992, 1993, 1995, 1997). Others have done similar work (see for example Binsted & Ritchie, 1997). This does not mean that we suppose the specific algorithms used in generation to exist as internal mental representations in speakers, but only that the productions observed in humans admit of formalisation. It must be admitted, however, that puns and riddles form instances of what is often called verbal humour. This is typically non-narrative, based on relatively simple paradigmatic lexical relations, and usually evaluated in terms of cleverness rather than funniness. (See Lessard, Levison & Venour, 2002, for a discussion of the distinction.) Instances of verbal humour are in many respects inherently simpler than more narrative structures like jokes. Limericks, on the other hand, presuppose at least a primitive narrative structure. As such, they provide a useful proving ground for more advanced work on both language generation (since it is necessary to take account of textual coherence) and humour generation (since narrative structure is involved).   LIMERICKS Limericks may be characterised generally as five-line verses with the aabba rhyme scheme, one of a range of metrical foot structures, and some attempt at humour or at least cleverness. (For discussion, see Bibby, 1978). A typical example will illustrate the model:  There was an old man from Peru Who dreamt he was eating his shoe He awoke with a fright In the midst of the night And found it was perfectly true.  Taken as a subset of more general poetic language, limericks confront us with a number of practical problems, but also with a number of theoretical challenges when they are compared with prose-based natural language generation. For example, traditional approaches to text planning and generation tend to be driven by conceptual and syntactic rather than metrical structure. This is unsurprising when one considers that the object of most systems of the sort is to represent some encyclopedic or data-base-like description of some state of affairs in a discursive format. In such a context, the primary factors to be considered include the model of the user (see for example Paris, 1993), the overall structure of the text plan (see for example McKeown, 1985), paragraph-level phenomena (see for example Mann, 2002), and problems of reference (see for example Dale, 1992). As a consequence, planning tends to be top-down (from universe of discourse, to dialogue model, to context, to overall text, to paragraph, to sentence) and many of the phenomena to be dealt with (anaphora, for example) can be examined in a left-to-right sequence. This has meant that conceptual tools such as phrase structure grammars or their variants have provided sufficient power to deal with the problems raised. Even phenomena such as freer word order may be captured in such systems by means such as the splitting apart of linear precedence and immediate dominance as in formalisms like Generalized Phrase Structure Grammar (Gazdar et alii, 1985). The production of poetic text, on the other hand, requires that content be filtered by metrical and phonological factors such as the number of syllables, stress, and rhyme. A particularly interesting consequence of this fact is that generation must take account of the ends of lines in the production of the beginning and middle. For example, in the case of the limerick quoted above, we may represent the syllable structure as follows, where s represents an unstressed and S a stressed syllable: It can be seen that the first, second and fifth line contain an iamb followed by two anapests, while the third and fourth lines contain two anapests. In addition, rhyming lines (Peru-shoe-true) must end on the same stressed syllable nucleus and coda (to use the terminology of metrical phonology (see for example Hogg, 1987). Note that these are not necessarily lexical items: in the first, second and fifth lines, the sequences “his shoe” and “-ly true” rhyme with “Peru”.   GENERATION A full analysis of the semantic, syntactic, lexical and phonological relations found in the limerick would take more space than is available here. However, from the generative perspective, it is possible to imagine three mechanisms to satisfy this constellation of constraints: 1. exhaustive generation: in essence, if we define a set of templates, we can generate large numbers of instances of potential limericks and ask a human to select those few judged to be of good quality. (This is the monkeys typing Shakespeare model, or Borges’ library.) 2. initial generation followed by tweaking: we may also begin by producing a sequence of lines and then selectively edit these to approach the target of an acceptable limerick, by means of the addition, removal or shifting of items. This may be how human poets work: it is how we produced the limericks at the head of this abstract. (Valéry is said to have claimed that the first line of a poem was a gift of the gods and that the remainder was the product of the poet's craft and efforts.) Since the edit operations described above are well-known in the computational context, it is not impossible to imagine their implementation. 3. right-to-left generation: given the direction of the constraints in limericks, a possible computational model involves the selection of the right-most elements of a sequence of lines as a starting point and then within each line, the constraint-satisfaction generation of the intervening elements required to flesh out the entire limerick.  In fact, these right-to-left formal constraints must interact with top-down constraints which govern the semantic coherence of the text. In other words, in order for the limerick to be coherent, the narrative events which it includes must be consistent with the subject. These thematic constraints may be fairly loose, as in the Old Man from Peru poem, where there is no particular logic which attaches old men from that part of South America and the ingestion of footwear (although more generally, the poem respects the constraint that old men are humans and that humans eat food which is a subset of tangible objects, and that footwear is also a subset of tangible objects). On the other hand, they may be quite tight, as in the VINCI limericks, which capture a set of characteristics of the software (its origin, the architecture of the program). Compare as well Bibby’s attempts to produce limericks appropriate to each of the Cambridge colleges. The limerick thus represents the semantic expansion from one or more initial lexical choices, by means of the selection of traits or actions which are at least consistent with these. This presupposes a rich representation of the characteristics of each lexical item, including both high-level semantic traits such as that between humans, eating and objects, but also more encyclopedic ’information, such as the fact that VINCI is a program produced at Queen's. (See Peeters, 2000 for a discussion of some of the problems found at this ‘lexicon-encyclopedia interface’). Note also that a sufficiently rich set of traits will provide the seed for variant limericks based on the same starting point. Consider for example the following example by Edward Lear:  There was an Old Man of Peru, Who watched his wife making a stew; But once by mistake, In a stove she did bake, That unfortunate Man of Peru.  At the formal level, problems arise both between lines (rhyme) and within lines (metrical structure). Consider the latter in the context of the initial old man from Peru poem. Let us assume that we have determined to use a metrical structure based on an iamb and two anapests and that we have selected the the place-name Peru. At this point, we have used up two of the eight available syllables. We also know that the next earliest syllable must be unaccented. The solution found here is “from”. At this point, we know that we have a prepositional phrase “from Peru” which requires (among other choices) a noun phrase with the appropriate accent structure. This constraint is satisfied here by “an old man”). However, in order to verify that this noun phrase is appropriate, we must have knowledge of its metrical structure. For example, if the target were an iamb, we would need to seek, among other possibilities, a DET N structure. This application of constraints cascades right to left until the overall metrical target is met, within the semantic constraints already specified   IMPLEMENTATION In the conference presentation, we will illustrate the implementation of the approach described above using the VINCI natural language generation environment (see  for details). Briefly, VINCI allows for the initial specification (PRESELECTION) of a constellation of lexical items which may be constrained by semantics, morphological and syntactic characteristics, and phonology. Preselected items, as well as all their characteristics, are available to subsequent steps of the generation process. Among other things, this allows for multiple layers of preselection, in which a first item determines a set of rhyme constraints, as well as a set of semantically appropriate elements. Within each line of the limerick, and between lines, the VINCI mechanism of GUARDED SYNTAX allows the control of subsequent steps of the overall generation to be conditioned by the framework existing at that point. (A simple example of guarded syntax: if a noun phrase node carries the attributes ‘first person’ or ‘second person’, then its children may only be instantiated by a pronoun, whereas if the parent node carries the attribute ‘third person’, then the children may be pronouns, full noun phrases, or proper names.) Applied to the production of a limerick, and assuming right-to-left generation, if the parent node of a tree contains a prepositional phrase which sums to an anapest, then the next left-most chunk of syntax must inherit this information and construct an appropriate item from a library of possible patterns. One consequence of this model is that syntax is reduced from a single overarching tree to the sum of a number of possible micro-trees. Such an approach is comparable to the model of Tree Adjoining Grammars, in which insertion and development occurs at the level of lexical-based subtrees (see Joshi & Schabes, 1997). It is also somewhat similar to the Labelled Deductive System approach used in parsing by Kempson et alii, 2001. Another consequence is that by precluding tweaking and editing, the system itself is forced to deal with the interplay of linguistic levels and constraints, without human assistance, thus providing an acid test for both the model and the implementation. Of course, this leaves aside the question of evaluation, itself a thorny issue, since the criteria used are many, varied and probably fuzzy as well. In related work, we are examining the ability of humans to learn how to produce limericks, as well as their ability to evaluate them.   ",
       "article_title":"Computational Generation of Limericks",
       "authors":[
          {
             "given":"Greg",
             "family":"Lessard",
             "affiliation":[
                {
                   "original_name":" Queen's University",
                   "normalized_name":"Queen's University",
                   "country":"Canada",
                   "identifiers":{
                      "ror":"https://ror.org/02y72wh86",
                      "GRID":"grid.410356.5"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  At the beginning of 2001 work started on the Commedia Project, a research effort which will transcribe in full seven manuscripts of Dante’s Divine Comedy in order to collate them and present them in an electronic format. As part of the prolegomena for the Project, we had to write its transcription guidelines, a task that appeared straight forward. However, as often happens, drafting the new guidelines developed into a task with implications beyond its immediate intended use. Initially, it was agreed that the Commedia Project transcription guidelines should be based on those of the Società Dantesca for their Dante Online website (). It soon became evident that although the Società Dantesca's guidelines offered the advantage of having taken into consideration practical matters concerning spellings, punctuation, word division and the expansion of abbreviations, they did not deal with many other matters which were required for the Commedia Project. The Società Dantesca uses a form of symbolic representation—based on conventions—to convey the transcribers interpretation of what they believe to be in the manuscripts. For example, the Società Dantesca transcribes (Riccardiana 1005, Inferno, Canto I, 17): <di +i0 del> These symbols are used to represent a deletion. In this case, the deletion was carried out by the main scribe of the text—or by an indistinguishable hand—indicated by 0. The complete set of symbols is enclosed in angle brackets. The first word, in this case ‘di’ is the one which was originally in the manuscript, and the last word—‘del’—is the one which replaced it. Next to the 0—representing the main hand or one which cannot be distinguished from it—the plus symbol is used—addition—and the letter ‘i’ which indicates that the correction has been introduced between the lines, i.e. it is interlinear. The Società Dantesca guidelines allow the possibility of marginal additions—‘m’—or additions within the line—for which they do not use any symbol. In this specific case, according to the transcription produced by the Società Dantesca, the manuscript has the word ‘di’ which has been substituted by the word ‘del,’ creating the phrase ‘del pianeta’ instead of the original reading ‘di pianeta.’ A second example can be found in Riccardiana 1005, Inferno, Canto I, 94:<crede +i0 cride> Here, the original reading “crede” is followed by the identifiers for the position and the scribe, and at the end, the modified reading “cride,” again, all enclosed in angle brackets. The Società Dantesca system also permits a symbolic representation of marginal additions—[... +m1 o], an addition of the letter ‘o’ in the margin, which has been added by a second scribe (here represented by the number 1) to cover a lacuna—, interpolations or cancellations. Although these guidelines were useful as a base for the Commedia Project’s transcription system, a new encoding system was required for the encoding of the manuscripts. Other projects in which we are involved used very simple encoding systems. For example, the encoding for the Canterbury Tales Project’s publications uses [add][/add] for additions and [del][/del] for deletions. Thus, an interlinear addition in the Merchant's Tale, line 219 (CTP lineation system) was tagged:tree [add]is[/add] neydir However, the verb is not in the same line as the other words, in fact, there is a caret indicating that the word ‘is’ is an addition to the line. Although the Canterbury Tales Project tags were useful when it started, they now seem to lack the flexibility which is required to present certain aspects of a scholarly edition. Given the nature of the corrections in the Commedia manuscripts, the Project required tags that were able to handle situations more complex than those of additions and deletions. One of the main aims of this project is to produce a CD-ROM with seven witnesses of the Commedia which Federico Sanguineti has identified as textually the most important. Sanguineti has already produced a critical edition of Dante’s Commedia, and the research he has already done is still being carried out at the Canterbury Tales Project. For this reason, the Commedia Project has a clearer conviction of which things are important and should be displayed in the CD-ROMs, and what the purposes of its transcriptions are, than the Canterbury Tales Project had when it was officially started in 1993. Because of this, it was possible to develop a encoding system which allows the distinction of different scribal hands or corrections made by the same scribe at different stages. Hitherto, the encoding of projects similar to the Commedia Project, such as the Canterbury Tales Project, attempted to present simultaneously both ‘what is in the manuscript’ as a series of additions or deletion, and ‘what is in the text’, as a series of distinct readings. However, after months of discussion with Klaus Wachtel (Institute for New Testament Research, Munster) about the transcription of corrections of the manuscripts of the Greek New Testament, new ideas about how to encode different textual stages started to emerge. These discussions were the base of the encoding system developed for the Commedia Project. The main goal of this new transcription system is to present a clear distinction between what is in the manuscript and how the transcriber interprets the different stages of development of the text. The Commedia Project encoding system aims to represent the different stages of variation in the text. When a transcriber finds a ‘place of variation’ in the manuscript, he or she can use the apparatus tag—[app][/app]. (We are using Collate-style encoding in the transcriptions: before publication, these will be translated into XML encoding). The apparatus tag contains three main components: the original reading (contained in the [orig][/orig] tag), the final reading (contained in a tag which specifies which copyist produced this [c1][/c1], [c2][/c2], [c3][/c3]), and what literally is in the manuscript (contained in the [lit][/lit] tag). If there are more than two stages in a correction, for example, in the case of having more than one corrector), these stages are presented in what is likely to be their successive order. The following example is taken from Riccardiana 1005, Inferno, Canto III, 9: The Commedia Project transcription guidelines indicate that we should transcribe as follows : [app] [orig]dura[/orig] [c1]duro[/c1] [lit]dur[ud]a[/ud]o[/lit] [/app] Since the dot below the letter ‘a’ indicates deletion, the transcriber is faced with a place of variation—indicated in the transcription by the apparatus tag—[app][/app]. The first component inside the apparatus tag is the original reading—[orig]dura[/orig]. The second component is the final reading by the main hand, the reading [c1]duro[/c1]. These two components express different states of the text, but do not explain by which process the text change from one to the other. For this purpose we use the literal tag—[lit][/lit]—which indicates what literally is happening in the manuscript, in this case [lit]dur[ud]a[/ud]o[/lit], that is, literally the letters ‘d’ ‘u’ ‘r’ are present, followed by an ‘a’ which has been underdotted and an ‘o.’ In the literal tag, there is less space for interpretation and the transcriber is required to postpone judgment (for example, whether the underdotting of the ‘a’ indicates cancellation or not). In comparison with the lack of flexibility of the old encoding system of the Canterbury Tales Project, the Commedia Project's guidelines present several advantages. Firstly, the transcribers can defer interpretation of the stages of meaning, since the literal tag can be transcribed independently of the other components of the apparatus tag (this also gives the advantage of allowing the editor of a publication to make a final decision as to what happened at each individual place of variation). Secondly, the contents of the literal tag allows us to reconstruct what actually appears in a manuscript on the computer screen. Thirdly, the other components of the apparatus tag, such as original reading, final reading, and intermediate readings, can be collated separately from the rest of the text. The separate collation of multiple readings in a manuscript will be most useful when a scribe used a manuscript of different affiliation to correct his copy. In such cases, separate collation will allow the isolation of readings which originated in different manuscripts and which could hint at distinct affiliations in a single text. Separate collation might also be of help in cases in which conflation has occurred because a manuscript is corrected with another one from a different branch of a textual tradition. The encoding system of the Commedia Project has also been implemented by the Canterbury Tales Project (for publications to appear after the Miller’s Tale on CD-ROM, edited by Peter Robinson and the Nun’s Priest’s Tale on CD-ROM, edited by Paul Thomas) and, in the future, might be also adopted by other projects (notably, transcription of Greek New Testament manuscripts) for their transcriptions. This new encoding system also offers advantages when applied to authorial manuscripts, and although it was originally designed to deal with problems of corrections presented by medieval manuscripts, it should work as efficiently to distinguish different authorial versions of a particular text. This should translate into an easier reconstruction of these authorial versions and allow the distinction and separate reconstruction of different authorial versions.  ",
       "article_title":"Textual Critical Encoding",
       "authors":[
          {
             "given":"Barbara",
             "family":"Bordalejo",
             "affiliation":[
                {
                   "original_name":" De Montfort University",
                   "normalized_name":"De Montfort University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0312pnr83",
                      "GRID":"grid.48815.30"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This paper considers the purpose of annotation in scholarly editions and the methods by which annotation should be provided in the electronic environment. There is a consensus that the most important task in scholarly editing is to prepare an accurate and reliable text, according to transparent criteria. The second task is to supplement that text with apparatus to enable the modern reader to read it more adequately. While some of the more influential guides to editing procedure (Center for Editions on American Authors, Committee for Scholarly Editions) suggest that annotation will not in all cases be required, the annotative process can be high on the list of editor’s satisfactions and may therefore be accorded disproportionate effort. As Mary-Jo Kline reports, some of the volumes in such magisterial series as the Jefferson Davis Papers and the Madison Papers were criticised for the “plague” of overannotation. Any General Editor must quickly learn stealthy strategies for restraining the annotative enthusiasms of contributors. The process of footnoting or annotating in the presentation of an edited text has a different role from the one it does in works of literary critical commentary. On a critic’s page the primary end of the annotation is to provide the evidentiary basis for the commentator’s assertions and argument, or, in one lurid account, to fight turf wars about academic status with other scholars (McFarland 1991). We concentrate here, however, on the principles and practice of annotating the primary text in a scholarly edition. The purpose of annotation is usually framed with the reader in mind. It may be based idealistically on “render[ing] the author’s meaning wholly intelligible” (Battestin 1981), or it may be based on the attempt to provide the modern reader with the information that would have been possessed by a reader on first publication. In the case of unpublished works, it may translate a general reader into an essentially private world. There is often a strong injunction that annotation should not be subjective or judgmental (CEAA, Hewett 1996). In practice, a further aim may be to present the accretion of scholarly knowledge and interpretation of the text up to the present (Ricks 1989) Annotation in the print mode is arranged in a number of different ways: textual or explanatory notes at the foot of the page, notes at the end of the chapter or book, collations, glossaries, appendices, and “excursus notes” (McFarland 1991). For reasons of cost, electronic editions can offer much richer annotation than print editions, and targeted parts of this extra material can be made accessible from precise points in the text. Two important questions arise, though: how should this ancillary material be arranged, and how should it be linked, or rather, how should its availability be signaled from within the text? One system, used especially by publications like journals that have both a print and a web publication, is to follow closely the endnotes practice used in print publications by offering the notes in a single file and signaling them by a numerical footnote indice. Other editions indicate the presence of the annotative note by a marginal indice (e.g. Thomas Gray Archive), by highlighting the span of text to which the note refers, or by enabling the highlighting of the text span through a mouseover to reveal the presence of the link. Some of the most spectacular scholarly editing projects of the last decade such as the Rossetti Archive and the Blake Archive situate their texts in such a complex environment that the annotative strategy of immediate explication of individual terms in texts is abandoned in favor of more serious immersion in the contexts of the poems. Paradoxically, though, under this system precise assistance is not always available to the reader. In the electronic edition of His Natural Life developed by the Authenticated Editions Project at its JITM website, we commenced with the list of annotations in a flat file transferred as legacy data from the printed Academy Edition of this text, and highlighted the text spans which were indicated in the lead-ins to the print edition’s endnotes. But this seemed to make poor use of the amplitude and precision offered by the markup scheme of the electronic edition. Accordingly, we have analysed the content of the notes and reformatted them according to type into sub-glossaries. The result constitutes in itself a critical approach to the text. Signaling the presence of annotations has been rethought in the light of the expected readership of the electronic version, and this has led to the provision of greater density of annotation links. However, providing for multitudinous non-sequential reading patterns can easily produce an absurdly over-linked text when the names of recurrent characters, places or motifs are annotated. This latter problem can be alleviated if users have a mechanism for enabling or suppressing the link indicators themselves. We conclude that different strategies are required for the arrangement and announcement of annotative material in the electronic environment from the print one, but the major determinant of how annotations are to be arranged and offered (and perhaps still the most difficult thing to judge) is how readers will approach the edition.  ",
       "article_title":"Annotation and Electronic Scholarly Editions",
       "authors":[
          {
             "given":"Chris",
             "family":"Tiffin",
             "affiliation":[
                {
                   "original_name":" Univ of Queensland",
                   "normalized_name":"University of Queensland",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00rqy9422",
                      "GRID":"grid.1003.2"
                   }
                }
             ]
          },
          {
             "given":"Graham",
             "family":"Barwell",
             "affiliation":[
                {
                   "original_name":" Univ of Wollongong",
                   "normalized_name":"University of Wollongong",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/00jtmb277",
                      "GRID":"grid.1007.6"
                   }
                }
             ]
          },
          {
             "given":"Phill",
             "family":"Berrie",
             "affiliation":[
                {
                   "original_name":" Australian Defence Force Academy",
                   "normalized_name":"Australian Defence Force Academy",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/02j5s7g39",
                      "GRID":"grid.97008.36"
                   }
                }
             ]
          },
          {
             "given":"Paul",
             "family":"Eggert",
             "affiliation":[
                {
                   "original_name":" Australian Defence Force Academy",
                   "normalized_name":"Australian Defence Force Academy",
                   "country":"Australia",
                   "identifiers":{
                      "ror":"https://ror.org/02j5s7g39",
                      "GRID":"grid.97008.36"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The existing body of theoretical work in text encoding suffers from two interrelated problems: firstly, confusion over the specific nature of the work; and secondly, the absence of any truly critical theory. Theory’s purest signified and a fundamental predicate of progress in the natural sciences involves a representation that meets discursively bounded criteria of “truth.” According to Renear (1997) the electronic document encoding and processing community “has evolved a rich body of illuminating theory about the nature of text” (107), a claim that expands that of Renear, Durand, and Mylonas (1996) and anticipates Mylonas and Renear (1999) who assert the principal goal of the research community that develops and applies the TEI Guidelines and other text markup schemes is a greater “theoretical understanding of textual representation” (my emphasis). In making their case the authors invoke Lakatosian criteria which they confidently pronounce (some disclaimers notwithstanding) this research community’s work meets. Yet the development of OHCOs 1-through-3, for example—surely one of text encoding’s founding “theoretical” moments— demonstrably fails to qualify as a progressive problemshift in Lakatosian terms.I omit the argument of proof here for brevity. Text encoding has not generated a rich body of illuminating theory because it cannot. Asking, for example, “what is text, really?” already poses the wrong question because it assumes an undiscovered essence the encoding community can find with a progressive research program. This will not happen because no mysterious core exists whose explanation falls to home-grown text encoding theory. Renear argues that despite falsification of all OHCO variants there is nevertheless “no reason to give up the common-sense view that texts do have an objective structure independent of our methods and theories about them” (1997, 122). Structure there may be, but discovering it is not like positing the double-helix of DNA; we need not strive to understand-by-modelling because of inadequate observational technology or limited knowledge. Praxis prompts reflection which generates principle to guide subsequent praxis. Poetry, for example, shows a self-conscious praxis continually reviewing, refining, and codifying (prescriptively) its methodology,In the hyper-self-consciousness of artistic praxis prescription inevitably invites its own negation—though anti-prescription is arguably still a principle. and this is as true of its encoding as its writing. Out of “successful” text encoding praxis comes not theory but principle.See, for example, the Women Writers Project’s online guide for marking up line groups. () This is to say not that text encoding has no theoretical component but that pursuing the theory behind a principle takes us to another place: to linguistics or rhetoric or semiotics or the cognitive psychology of visual perception, and so on. Such pursuits can produce sophisticated and thought-provoking borrowings that unquestionably enrich the literature, but these are the exception, not the rule.Recent productive borrowings include Buzzetti’s from Hjelmslevian Semiotics (1999); Renear’s from Austin’s Speech Act Theory (2000); and Piez’s from Traditional Rhetoric (2001). In particular, scholarly work in text encoding rarely positions itself with respect to work in modern literary/cultural theory, and even more rarely does it use text encoding as a springboard for a sustained engagement with such theory. This seems to me a significant and unfortunate absence, a product both of text encoding’s desire to differentiate itself as a specific field of intellectual inquiry and of a positivist, utilitarian bias against the perceived negativity and self-imprisoning reflexivity of contemporary theory. Renear’s narrative of the development of text encoding theory exemplifies the latter stance with its Realist/Anti-realist distinction, associating the former with common-sense and characterizing the latter as “consistent with post-structuralist epistemologies” (122). This imitates an exclusionary move Zavarzadeh and Morton (1991) identify in literary studies: positing deconstruction as the boundary of theory beyond which it is unthinkable to go because (supposedly) deconstruction represents the limits of the thinkable, the point where theory swallows itself in absolute relativism. Ironically, Renear’s account positions itself precisely as theoretically reflexive while exposing its own refusal of genuine reflexivity. Text encoding—indeed humanities computing as a whole—can too easily think of itself as related to every humanities discipline but also marginal or even external to all of them. This licenses attitudes such as Renear’s contention that what he calls text encoding theory brings “a much needed fresh perspective on textuality” (107), as if text encoding occupied a different space from traditional disciplines. Contrarily I would argue that whatever its origins in non-academic praxes, text encoding forms itself in and of humanities disciplines. I should stress that I do not consider these disciplines stable sites: they can and should be challenged. Text encoding therefore offers a locus for work that tries to think through the tensions, contradictions, and faultlines that constitute those disciplines qua humanities disciplines. However, unless it can stand on sufficiently equal terms to enter a critical dialogue with theory of exemplary reflexivity and philosophical rigor, the text encoding community’s theoretical work will have limited significance and appeal—a fate already shared by much of the humanities computing literature (Corns 1991; Warwick 1999). My own position is that the historical materialism that comes down to us from Marx, enriched and updated by thinkers such as Lenin, Adorno, Althusser, and many others, offers us the best critical tools for a dialectical engagement with what it means to encode texts in the humanities. Althusser (1982) memorably describes the problem: Left to itself, a spontaneous (technical) practice produces only the “theory” it needs as a means to produce the ends assigned to it: this “theory” is never more than the reflection of this end, uncriticized, unknown, in its means of realization, that is, it is a by-product of the reflection of the technical practice’s end on its means. A “theory” which does not question the end whose by-product it is remains a prisoner of this end and of the “realities” which have imposed it as an end. (171, emphasis in original)  Currently a prisoner of its pragmatic roots, theoretical work on text encoding has only its chains to lose.  ",
       "article_title":"Theory in Text Encoding",
       "authors":[
          {
             "given":"Paul",
             "family":"Caton",
             "affiliation":[
                {
                   "original_name":" Scholarly Technology Group, Brown University",
                   "normalized_name":"Brown University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05gq02987",
                      "GRID":"grid.40263.33"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  While computational stylistics and text analysis seems to be in constant quest for the just right set of criteria (see e.g. David Hoover’s presentation at the 2002 ALLC/ACH Conference in Tübingen), this presentation will try to apply what has already become a standard in statistical stylistic analysis to a (relatively) novel material. Taking for granted—very unoriginally—the usefulness of John Burrows’s method that has been around since his 1987 Computation into Criticism, in discerning stylistic differences between individual characters in works by the same author, I will try to see if the same or similar patterns of similarity and difference travel across linguistic boundaries; if differences between characters’ “idiolects” are preserved in translation. In a typically Polish approach to the matter, I have chosen as my material the trilogy of historical romances by Poland’s first literary Nobel Prize winner, Henryk Sienkiewicz, written between 1882 and 1888, and its two English (or, more precisely, American) translations by Jeremiah Curtin (completed between 1890 and 1893) and W.S. Kuniczak (1991). The reasons for this choice have been manifold. First, Sienkiewicz’s three novels, With Fire and Sword, The Deluge, and Pan Michael, although set in the turmoil of 17th-century Poland, remain to this day a major classic of Polish literature and the country’s most popular reading. Second, a trilogy, the subsequent parts of which share some of their characters, seems ideal for Burrowsian analysis (a fact confirmed by the interesting coincidence of John Burrows’s undertaking of a study of Beckett’s bilingual trilogy in a much later paper). The final reason was the difference between the two translations, evident both in their being separated by an entire century and, what follows, in the entirely different approaches and results obtained by the two translators: the largely word-by-word transcoding by Sienkiewicz’s contemporary and the highly adaptative and “free” method of the modern Polish-American writer. Faithfully maintaining the original Burrows model, the study of distances between the “idiolects” of the major characters has been based on relative frequencies of the 30 most frequent words in the dialogue of each version of the trilogy. The resulting correlation matrices were then used to produce two-dimensional multidimensional scaling charts of distances between such “idiolects.” This procedure has yielded, in Sienkiewicz’s original, a very consistent influence of the personae’s social status and ethnic background, especially in the first part of the series. It is particularly visible in idiolects of ‘enemy’ (non-Polish) collective characters, usually plotted at some distance one from the other. Curtin’s translation is notable for ‘de-clustering’ idiolects of various Polish gentry characters, making their idiolects much less alike. There is also a visible tendency in Curtin to limit the distances between rival collective characters and making them markedly similar rather than divergent as in the original. Idiolects in Kuniczak are even more evenly distributed, with a general trend towards greater distances and less clustering observable in the graphs. The very high resemblance between the idiolects of two major characters, Zagloba and Wolodyjowski, in all three versions of The Deluge (almost identical in Sienkiewicz) is one of the most consistent traits of this portion of the analysis—an interesting illustration of the fact that the two personae’s function of keeping the three volumes together becomes evident in the second part of the series. Sienkiewicz's social/ethnic idiosyncrasy has been confirmed in a plot for idiolects of characters involved in the Polish-Ukrainian conflict in With Fire and Sword, a feature slightly visible in Kuniczak and almost not at all in Curtin. As perhaps the most consistent effect of all, the peripheral situation of female idiolects is a constant element in almost any configuration. The study of more detailed and thematic configurations of characters is also the source of interesting insight. Plots for female characters exhibit a tendency to group together young (and marriageable) Polish women; Helena's Ukrainian provenience is highly visible in Sienkiewicz, while the ethnic element is indiscernible in both translations. Among characters involved in each novel’s eternal triangles, both of the above aspects are clearly visible in all three versions; differences between characters in the same triangle are quite considerable. In an examination of characters that recur throughout the series, a good consistence has been observed between the three idiolects of the Polish Falstaff, Zagloba, the most inveterate talker of the series, in each version separately: best in Sienkiewicz, worst in Curtin. Another character, Wolodyjowski, is much more of a developing character, which agrees well with the evolution of the persona in the course of the series, from a humble officer to the hero and spearhead of Sienkiewicz’s ideology. This has been confirmed in a separate plotting of idiolects of those two characters. A number of joint plots for Curtin’s and Kuniczak’s translations (based on frequent words common in both versions) has been made to investigate if there is a constant pattern in their respective differences. In agreement with some “intuitive” assessments as to the decreasing differences between Curtin’s and Kuniczak’s versions (mainly due to Kuniczak’s gradual abandonment of adaptative procedures, especially on the microstructural level), the patterns become more consistent with time: fairly chaotic movement for the first part of the trilogy has become more ordered in the second and almost uniform in the third. The “stylistic drift” observed between idiolects in Curtin and in Kuniczak— divided, apart from their contrasting approaches to translation, by an even more significant difference of a whole century—is a vindication of Burrows’s ‘tiptoeing towards the infinite:’ that visible and uniform shift in the configuration of the most frequent words in English texts with time.  ",
       "article_title":"Burrowing into Translation: A Case Study",
       "authors":[
          {
             "given":"Jan",
             "family":"Rybicki",
             "affiliation":[
                {
                   "original_name":" Pedagogical University, Krakow, Poland ",
                   "normalized_name":"Pedagogical University",
                   "country":"Mozambique",
                   "identifiers":{
                      "ror":"https://ror.org/0331kj160",
                      "GRID":"grid.442441.3"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"    (Talan Memmott is a hypermedia artist/writer/editor from San Francisco, California. He is the Creative Director and Editor of the online hypermedia literary journal BeeHive (). His hypermedia work appears widely on the Internet. In 2001 he was awarded the trAce/Alt-X New Media Writing Award for his work Lexia to Perplexia, which also received honorable mention for the Electronic Literature Organization’s award in fiction. He is a tutor for the trAce Online Writing School, and has been a speaker, panelist, reader and performer at various Conferences and Universities. He is currently at Brown University as their first electronic writing graduate fellow.)    What is digital poetry? The definitions are decidedly nebulous. The term digital poetry has been applied to a variety of creative literary applications, from work developed in Flash and DHTML to MOO spaces and works that utilize Perl. From cybertext to web art, digital poetry is somewhat interchangeable with other terms used to describe what could be called creative cultural practice through applied technology. We can agree that digital poetry as hypermedia presents an expanded field of textuality that moves writing beyond the word, toward a relationship between signs and sign regimes, their integration, disintegration, and interaction one to another. But, how these relationships are established in digital poetry is as diverse and various as the practice itself. The problems of developing any general typology, let alone taxonomy, are hinted at in Espen Aarseth’s Cybertext: Perspectives on Ergodic Literature. Aarseth puts forward a number of models for the definition of various types of objects (buttons, actors, interactive, controller, layout) within creative applications—including games, interactive fiction, and hypertext. As the analysis expands, it is discovered that these typologies breakdown when any given piece is viewed as a whole. One moment a button may be a button, the next moment it may be an actor; or, any given element may carry the attributes of any number of types simultaneously. Digital poetry does not properly define any specific type of expressive object. Because of this there are many problems that emerge for the reader/users of digital poetry and for those that deal critically with such work. Lacking a definite object of study, we must begin to move away from the idea of digital poetry as a genre toward an observation of applied poetics within the digital environment—a poetics that is based in an individual author’s engagement with media technologies, as scripted, programmed and applied within a particular work. Using Artaud’s The Theater and its Double as a guide, this paper explores the mise en scène (or mise en screen) as a potential model for the close reading of digital poetry. The paper looks at a number of web-based digital poetry works that utilize a variety of technologies to demonstrate how the network and its technologies play into artist/writer intent to develop an applied poetics rather than poetry proper. Katherine Parrish’s Oulipo inspire web project MOOlipo is examined for its creative use of MOO technology to create a particpatory poetical space. In this work the user particpates by inputting text, which is parsed and filtered under certain rules to affect the output. The various rooms of the MOO have different produce different effects. In one room a mesotic is created from user input, in another room the word order of the input text is reversed. Two other works that require user participation for the construction of content will be examined. Lisa Jevbratt’s Syncro Mail, a web-based mail service, requires that a user input the email address of a second (perhaps unknowing) ‘user’. Through this process, the second ‘user’ receives a random image and a random word in their email. This piece uses perl scripting for its functionality and presents a unique, if not mysterious method of poetic emergence. In the delivered email there is no explanation as to the relevance or connection of image to word, nor any indication of how, or where the mail originated. The connections, the poetry must be made by this second ‘user’, independent of any knowledge of the process. Another project with much more immediate participatory poetic results is You and We, a collaboration of Seb Chevrel and Gabe Kean. You and We allows users to upload images and short texts. Using a combination of Flash and Macromedia Generator, the images and texts are randomly compiled in a somewhat cinematic, MTV-like display complete with music. Within seconds of uploading an image or text, it is incorporated into the collection. As of November 10th, 2002 there had been over 1,500 images uploaded and nearly 5,000 texts. Additional works to be examined include; Brian Kim Stefans’ The Dreamlife of Letters for its use of letterist animation, and a couple of “codeworks” by Ted Warnell—VIRU2 and BERLIOZ—for their elegance and simplicity of interface as well as their transparency. Warnell’s work allows the functions of code to play into the applied poetics of his work, at the surface. In VIRU2 the actual code that drives the piece is made viewable as screenal text. The exposure of code at the surface and the integration of functionality, aesthetics and poetics in Warnell’s work emphasizes the role of technology and an individuated ecounter with media in establishing what is inferred by the term digital poetry as well as applied poetics. Rather than work from a model that hopes for a close reading through the abstraction of words from their media-rich environment, this paper proposes that critics and readers take a more choragraphic (to borrow a term from Gregory Ulmer) approach to reading that observes the entirety of a work—from interface design to interactivity, the written word and code—as something of a micro-cultural statement. By examining digital poetry objects as a whole we may begin to recognize how each work presents an individuated applied poetics and move away from overreaching taxonomic designations. This paper also proposes that more critical work be developed in hypermedia environments as a way of diminishing critical/theoretical detachment from the realities of creative digital practice.   ",
       "article_title":"Beyond Taxonomy: Digital Poetics and the Problem of Reading",
       "authors":[
          {
             "given":"Talan",
             "family":"Memmott",
             "affiliation":[
                {
                   "original_name":" Brown University",
                   "normalized_name":"Brown University",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/05gq02987",
                      "GRID":"grid.40263.33"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"   SUMMARY For over ten years there have been several carefully argued critiques of the descriptive markup approach to text representation that have for the most part not been adequately answered. Recently an integrated development of some of these criticisms has been presented at length, and with considerable ingenuity and extension, in a widely read article by Dino Buzzetti, published in New Literary History (2002, 33: 61–88). In our paper we analyze some of Buzzetti’s criticisms, accepting some but resisting others.   BACKGROUND When the SGML or “descriptive markup” approach to text began to be actively promoted in the mid-1980s, quite a number of criticisms were developed in response to this approach, or, more exactly, to the accompanying theoretical claims, express or implied. In retrospect, it appears today that some of these criticisms have been answered by the proponents of SGML descriptive markup, others have simply faded in apparent significance, and still others, while live issues to some scholars, seem neverless to generate only unproductive repetitive discussions. However there is a particular family of related criticisms that upon careful re-consideration still seem fresh, deep, and, at least given their positive reception, plausible—and yet these criticisms have also been, strangely, largely ignored by the markup community. Some of these arguments were first presented in the late 1980s in a paper titled “Markup Considered Harmful” privately circulated by Darryll Raymond in response to Coombs et al 1987. They were later presented again, in more detail in 1992 in “Markup Reconsidered” (Raymond, Tompa, and Wood 1995); and then were further refined in “From Data Representation to Data Model: Meta-Semantic Issues in the Evolution of SGML” (Raymond, Tompa, Wood 1996). In brief these papers argue that “Markup is not a data model” but rather a “data representation” (Raymond et al 1992), and that while “SGML provides standard representations for documents” a “standard semantics” is required as well (Raymond et al 1996). And then, generalizing from this conclusion, the authors go on to suggest that many of the expectations for SGML-based markup systems, such as TEI, are misplaced. Despite the fact that these were pointed and clearly argued positions, the response from the TEI community, and perhaps more importantly from the theorizing researchers whose ambitious claims were particularly targeted (Coombs et al 1987, DeRose et al 1990, Sperberg-McQueen 1990), appears to have been largely to simply ignore these criticisms. Recently Dino Buzzetti has been re-presenting these arguments, supplementing them with further theoretical extensions, and integrating them with other criticisms of descriptive markup approach, in particular those of Jerome McGann, most recently presented and extended in a new book Radiant Textuality: Literature Since the World Wide Web (New York 2001)—which Buzzetti takes as supporting Raymond’s and his own criticisms. Buzzetti has in fact presented his concerns in several venues over the last 5 years (e.g. Buzzetti 1999a, Buzzetti 1999b), but it is in “Digital Representation and the Text Model” (New Literary History, 2002 33) that he presents these criticisms in a form, and a forum, and a language, in which they can no longer be ignored. Interestingly, this article comes precisely at a moment when some of the theorizing descriptive markup proponents are themselves very actively working in a similar vein. Sperberg-McQueen, who has called for the development of a SGML/XML semantics since at least 1992 (Sperberg-McQueen 1992), is leading a project to develop such a semantics now, and Renear, who is member of this project, has also recently claimed, in language similar to that of Buzzetti and Raymond et al, that an SGML document instance per se provides “only a data structure, not a theory” (Renear 2001).   THE CRITICISM Raymond et al argue, as described above, that markup is not a “data model”, but only a data representation that is “fully dependent on external information for meaning”. They note several characteristics that they say confirm that markup is not a data model in the sense that that expression is used in database theory. These include inadequate notions of equivalence, lack of redundancy control, and lack of defined algebraic operators. This claim should not of course be taken simply as a narrow assertion that SGML markup does not meet criteria for being a certain sort of thing as defined in some field or other—the larger point, and in the context of the discussion a plausible one, is that SGML markup will be inadequate for the kinds of theoretical tasks it is being assigned in virtue of failing to have these characteristics. Raymond et al (1995) state that this failure can be remedied either by adding a “formal structure on top of markup systems” or by defining a new abstraction from scratch. Raymond et al prefer the latter because they believe that markup systems are not only a representation rather than a data model, but they are in fact a relatively poor technique for representation. Raymond et al 1996 characterizes this addition of a formal structure as adding “semantics” to SGML. Buzzetti accepts Raymond et al’s criticism of SGML markup. But he goes much further, both in the extent of his theorizing and in the specific nature of his criticisms. With respect to the latter Buzzetti argues that descriptive markup theorists are actually confused, and profoundly so: they conflate the “structure of the representation” with the “structure of the object represented”, or, alternatively, they confuse the “expression” with the “content” of that expression. Supplementing Raymond’s criticism from the perspective of database theory with an opposition from Saussure Buzzetti argues that descriptive markup is an expression whose form is a data structure. But the form of the content of that expression is a “data model”. Neither the expression itself nor its form however, is a data model—as mistakenly believed by the promoters of SGML descriptive markup and the defenders of the OHCO model of text. (Buzzetti goes on to make this point with juxtapositions of other theoretical terms: “format” vs. “formalism”, “syntax” vs. “interpretation”). As examples of this confusion Buzzetti singles out in particular the TEI Guidelines, Coombs et al 1987, and DeRose et al 1990, and Renear et al 1996. “In summary,” Buzzetti says, “we may assert that strongly embedded markup systems [are] inadequate in reference to both the exhaustivity and the functionality of the text representation and model.”   OUR RESPONSE We believe that that there is some truth to both Raymond, Tompa, and Wood’s criticisms, and to Buzzetti’s application of the criticisms. However we believe that both Raymond et al and Buzzetti substantially overstate the case, Raymond somewhat and Buzzetti much more. Such a response may seem too mild to deserve public airing, but it is for two reasons much more important than it sounds: first because of the radical form the criticism takes in Buzzetti, and second because our response might clarify some long confused issues in text encoding. There can be little doubt that many SGML markup enthusiasts have been now and then confused, and, even more frequently, confusing, about the representational nature of SGML document instances. This is mostly because representation itself is conceptually difficult, and subject to a kind of semiotic oscillation between expressing, referring, and exhibiting. And it may also be partly because of carelessness and genuine lapses of clarity and understanding. But Buzzetti makes too much of the fact that we are sometimes awkward about what document instances are, representationally, and exactly how markup does what it does. Notice that for the most part everyone manages the ambiguities quite well, and gets on, successfully more often than not, with various tasks and projects. How could this be the case if there were such a consistent systematic widespread conflation of expression and content? And, in fact, a close examination shows that Buzzetti does not produce evidence for such a systematic conflation, other than the various awkward or unfortunate characterizations mentioned above—and those, while admittedly signs of some failure to fully conceptualize key notions, are far from convincing evidence for systematic conflation. Moreover, there is no evidence that this confusion, such as it is, is connected, as either cause or effect, with a radical unsuitability of SGML markup to express theories about text. (According to Buzzetti “SGML is a data structure representation language” and does not provide a data model; and he echoes the views of Raymond et al that it is, moreover, a bad data structure representation language.) Part of the problem is that Raymond and Buzzetti rely too much in their argument on the specific notion of a “data model” from database theory, and therefore over-react to the failure of SGML to have precisely that sort of data model, or to have a good (i.e. completely or exactly defined) one. Part of the problem is that they demand formalization before admitting that there has been sucessful representation. It cannot be denied that we use SGML document instances to express express facts and theories about texts rather effectively: that is simply an empirical fact. Part of the problem is that in claiming that SGML and XML lack semantics and a defined set of operators, Raymond et al and Buzzetti ignore the fact that both SGML and XML define the semantics of the markup vocabulary in use as an intrinsic part of the application's document type definition. Unlike Raymond et al and Buzzetti, the SGML standard does not assume that the full semantics of an SGML application are captured in full by the SGML formalisms used to declare elements and attributes. Nor does a demand for an exhaustive set of algebraic or other operations seem a promising start for a critique of a markup discipline intended to encourage the reuse of data and the separation of data representation from processing concerns. In this connection Buzzetti’s criticism of the ordered-hierarchy of content objects (OHCO) hypothesis is particularly illuminating, because strikingly unconvincing. Unlike SGML markup, OHCO is obviously not intended as a representation syntax, but rather an abstraction claimed to be, structurally, the general form taken by all texts. As such it actually appears to be roughly the kind of thing that Buzzetti wants for a data model, though without the specific features characteristic of data models in database theory. Buzzetti’s interpretation of OHCO however has it as “not a model of the text, but a possible model of its expression”, just as SGML on Buzzetti’s account, describes not the text, but “the structure of the text’s expression”, consistent with its distinctive role as “a data structure representation language”. But whatever the failures of OCHO as a data model, this is no more than a tortured effort to make things seem worse then they are. The data structures in question have untyped purely logical parent/child relationships, whereas “hierarchy” in OHCO clearly refers to containment—and that is not a purely data structural relationship, despite the obvious formal similarities (asymmetry, transitivity, etc.) that makes tree data structures convenient for representing hierachical containment. It is true that additional formalization of the semantics of SGML markup is a good thing. Formalization will assist in precision, clarity, and computation. It is for that reason that Sperberg-McQueen and others are working to explicate the semantics of XML markup. But while we may regret that such work has not made more progress, or that not everyone understands the need for improved formalization of semantics and “data models”, or that some of our colleagues are sometimes confused about subtle matters of representation, or that we ourselves are now and then confused, or even often confused, we should not conclude that things are worse than they are. Scholars working in SGML descriptive markup are working with languages that do have semantics, and do have “data models”, even if inadequately formalized, or not quite the sort of thing database researchers prefer. And these scholars are at least as often as not quite clear on the difference between expression and content.   CONCLUSION The criticisms of inadequate formalization that began with Raymond et al and that have recently been so intriguingly developed and extended by Buzzetti are important ones that have been mysteriously neglected by the markup community. And the more ambitious criticisms made by Buzzetti of our failure to fully theorize, or at least attempt to clarify, the complexities of representation are perhaps even deeper and more important, and also unfortunately neglected. But the claim that in these failures we see the signs of either a systematic category mistake endemic in the text encoding community, or evidence of a disastrous inadequacy in current techniques, simply has not yet been proven. On the contrary, as we have suggested here, that claim itself seems to be based upon the mistaken notion, ironically reminiscent of the third man argument, that one more formal system will finally bridge the gap between thought and object. There may or may not be a gap, but if there is it will not be closed simply by another formal system, however useful that system may be.   ",
       "article_title":"Text Markup—Data Structure vs. Data Model",
       "authors":[
          {
             "given":"Allen",
             "family":"Renear",
             "affiliation":[
                {
                   "original_name":" GSLIS/University of Illinois",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Over the last decade, many humanities scholars have been persuaded by the promise and the power of encoding schemes for electronic texts to create texts, sometimes very large and complex, encoded using these schemes. This is specially true of SGML/XML based encodings, with the implementation of the Text Encoding Initiative being particularly influential in the community. However, scholars who have made such texts have typically discovered that software to publish them is too expensive for their limited budgets, or too difficult to use, or lacking essential facilities, or all three of these. The Anastasia electronic publishing system, developed in the last five years in a partnership between the Centre for Technology and the Arts, De Montfort University, and a new electronic publishing company, Scholarly Digital Editions (SDE), attempts to supply this deficiency. Anastasia stands for ‘Analytic system tools and SGML integration application’. As this implies, it is able to handle all valid SGML and XML documents, with no limits on their complexity. Particularly, Anastasia has been designed to meet the needs of humanists, and especially textual scholars. It is a common complaint of humanists that SGML/XML systems constrain a single hierarchical view of a document, while humanities texts can be seen as containing many overlapping and competing hierarchies. SGML/XML publishing systems usually cannot support facilities which cut across the primary document hierarchy, and so cannot satisify even such simple needs as display of a single page of transcription, or display of a tabular list of key word in context search results with formatting of all returned search strings according to the embedded encoding. Anastasia seeks to escape these limitations by adopting a document processing model that sees the document as made up of a series of events which are defined not only by their hierarchical relation, but also by their left to right relation in the document stream. As a result, Anastasia provides tools which allow the document to be manipulated according to alternative hierarchies implicit in the element relations. Thus, one can very easily extract views of the text by column or page, or indeed start a display at any point in any element and continue to any point in any other element. A KWIC display, for example, requires that we display an arbitrary number of characters before a hit, then display the characters in the hit themselves, and then display an arbitrary number of characters after the hit, all with complete awareness of the document encoding within those spans of characters: Anastasia can do this. Then, one should be able to click on a link from the KWIC display to the document itself, and see the hits highlight in the full-text context: once more, Anastasia has been designed to make this easy. One can also manufacture virtual texts by extracting and combining multiple and even overlapping segments. Anastasia is also designed to fill another need: for a mode of publication which is identical on both CD-ROM and the internet, on the major Windows and Macintosh systems. Typically, the scholar will prepare a body of SGML/XML documents for publication using the Anastasia GroveMaker application, which compiles the documents into a binary database. The Anastasia Reader then serves the documents to an internet browser, either over a network or from a CD-ROM. Control of all aspects of the publication's display and behaviour (including fully SGML/XML aware searching) is achieved through a series of Tcl script files. A key factor in the development of Anastasia has been the desire to achieve publication without compromise. That is: if it is possible to achieve a certain kind of computer display effect, then Anastasia will allow this. For example, we might want to use some of the advanced dynamic HTML features permitted by Javascript: pop-up menus, text which changes colour as the mouse passes over some other part of the document (for instance, to show that a word or phrase in one window is a translation of, or is otherwise related to, a word or phrase another window), synchronous scrolling or separate windows, and more. Practically, this means that we should be able to generate streams for display in any format whatever, directly from the XML: in pdf, SVG, rtf, any variant of HTML and XML, and send it directly to the display engine. We have concentrated on using Anastasia to generate HTML with Javascript: an example of the effects possible through this can be seen in the work on the digital 28th edition of the Nestle-Aland Greek New Testament, accessible through nestlealand.uni-muester.de. Other instances can be seen from the SDE website, . Anastasia is designed to work as a Apache webserver module. It also requires C-language support, and the Tcl (Tool Control Language) libraries. In theory at least, this means Anastasia can operate whereever Apache operates: our main development is on Macintosh OS X and Windows machines; there is also a Linux port. The search systems in Anastasia are based on SGREP, written by Jani Jaakkola and Pekka Kilpelainen of the University of Helsinki: we have heavily customized the SGREP code to improve its performance with large texts. Perhaps one of the most distinctive (if not controversial) features of Anastasia is that the style sheets we use to control exactly how the source XML is sent to the browser are written in Tcl, and not in any of the various XML-based systems which have appeared in the last years (such as XSLT, XPATH, and others). In part this is historical: the roots of Anastasia lie some distance back, as far as the first work done by myself on the Canterbury Tales Project with Elizabeth Solopova and Norman Blake) in 1993, long before even XML made an appearance. In part, it is because those systems themselves remain in a start of flux. But it is also because there is room for argument about the efficiency of such schemes. There is no doubt that XML is superb at representing textual structure. But this does not mean it is suitable for use as a programming language, requiring ease of use, rapid development, efficient maintenance, and widespread support across many different computer systems. Tcl does offer all these. Anastasia is not intended to be the tool of choice for everyone who works with XML. It is designed for situations where the very best possible presentation is required of highly complex XML. A single screen of the digital Nestle-Aland, for example, may draw XML from hundreds of different places within the source, reformat into HTML interwoven with Javascript commands, and spread this across a series of frames nested within the browser display Ð all in a fraction of a second, in response to a request from the reader. It is also designed to run identically on CD-ROM and over the internet. Reports of the death of CD-ROM appear rather exaggerated: indeed, the availability of cheaply priced publication tools such as Anastasia may make it possible for high-quality CD-ROMs to be made available at much lower prices than hitherto, and so create a market which has been previously elusive. Finally, my hope when designing Anastasia was that a single scholar, with reasonable dedication, good knowledge of XML and with no more computer resources and support than are commonly available within university departments, would be able to use it to make high-quality XML based publications. There have been some encouraging signs that Anastasia can indeed be used in this manner. In the same context, it should also be appropriate for use by smaller academic publishers. This is the first conference presentation of Anastasia as a mature publication system. There has been one previous conference presentation of the system, at the DRRH conference in Sydney in September 2001, when only a preliminary version of the software was available.  ",
       "article_title":"Anastasia: A New XML Publication System",
       "authors":[
          {
             "given":"Peter",
             "family":"Robinson",
             "affiliation":[
                {
                   "original_name":" De Montfort University",
                   "normalized_name":"De Montfort University",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0312pnr83",
                      "GRID":"grid.48815.30"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  This year, the Dickinson Electronic Archives (DEA) began work on an edition of Emily Dickinson’s Correspondences (EDC), a multi-volume TEI-conformant XML scholarly edition of individual collections of letters and poems sent to and from Dickinson to her various correspondents that will include manuscript images, transcriptions, and critical annotation. Overseen by general editors Martha Nell Smith, Ellen Louise Hart, Lara Vetter, and Marta Werner, and coordinated by Vetter, EDC volumes are guest-edited correspondence-by-correspondence by Dickinson scholars across the country. The project is ambitious in scope; Dickinson sent over 1800 letters and poems to about 100 friends, family, and acquaintances, and conceivably many individual guest editors could be working simultaneously on various sets of documents. The DEA faces several challenges in managing the concurrent editing of the voluminous correspondence, not the least of which is geographical. All four general editors are dispersed geographically: Smith in Maryland, Hart in California, Vetter in Missouri, and Werner in New York. The DEA project manager, Jarom McDonald, and encoding staff are located in Maryland at the Maryland Institute for Technology in the Humanities, while guest editors live and work all over the country. The documents are located primarily in two different libraries in Massachusetts with scattered manuscripts elsewhere, and the encoded data resides on a server at Institute for Advanced Technology in the Humanities in Virginia. Clearly, we need a system by which the geographically remote project coordinator can track the status of multiple documents in multiple correspondences at various stages of editing and encoding, and work with a staff and editors at a distance. We also face the utter impracticality of training innumerable guest editors in advanced TEI and XML, particularly given the encoding difficulties posed by the experimental nature of Dickinson’s page, yet we need to maintain consistency in encoding and editing across sets of correspondence, all the while preserving the integrity of the individual editor’s work. Originally conceived by Lara Vetter and implemented by Jarom McDonald, the Dickinson Electronic Archives Manuscript Project File Management System is designed to allow editors and staff working from different locations across the country to collaborate on the production of XML files for web delivery. The system will accommodate multiple editors and staff working on different subsets of documents. The DEA Manuscript Project File Management System facilitates the following processes: 1. For each manuscript, the editor completes a php-driven submission form collecting relevant information about that manuscript. When the form is submitted, it writes an XML file that parses against the DEA DTD, pulling additional data from SQL regularization databases, and deposits the file into the directory for newly submitted documents; whatever data cannot be automatically tagged is placed in a comment tag to provide instruction to the encoder. When the form is deposited into the directory, the editor receives a copy via email, and the project coordinator is notified via email that new manuscript data has been submitted. Additionally, an entry in the SQL-driven management submission database is generated that contains the filename, the editor’s name, and the date the file was submitted. Finally, a <revisionDesc> statement is generated and placed automatically into the XML file, containing information about the nature of the action, the person who performed it, and the date. 2. The project coordinator logs into the administrative section of the system and is presented with a complete list of all submitted documents pending assignment to encoders. The coordinator can assign each document individually to any of the current DEA encoders via a drop-down menu; doing so will automatically generate and send an email to that encoder, notifying him/her of the new assignment. The corresponding entry in the submission log will be updated to reflect the encoder assigned to the project. 3. The encoder logs into the “Document Check-in/Check-out” section of the system and is presented with a complete list of all documents pending to be downloaded and those which have previously been checked-out for encoding but not yet uploaded. The encoder clicks on the file to be downloaded and is taken to the download page, where he/she can save the file locally for encoding. This step updates the submission log entry, noting the date that the file was downloaded for encoding, and moves the particular file from the “pending check-out” section to the “pending check-in” section for the given encoder. This step also automatically updates the <resp> attribute in the header that signifies the encoder. The encoder’s job is to perform post-processing on the XML file by utilizing the editor’s notes, comparing the XML file to an electronic image of the manuscript, and tagging everything that cannot be automatically generated by the form. 4. When the encoder has finished with the file locally, he/she can log back into the system and click on the entry to “check-in.” The encoder is then taken to the upload page and deposits the newly encoded file in a directory separate from that which the editor-submitted file is located. Checking a file in through the system will update the submission log to reflect the date of encoder uploading, as well as generating another <revisionDesc> entry into the document itself. The project coordinator/proofreader is also notified via automatic email that the encoded file has been uploaded. 5. Upon logging back into the system, the project coordinator is presented with a list of files that need to be proofread, in addition to seeing which files need assignment to encoders (see item 2 above). These files can be checked-out and checked-in for proofreading in the same manner as described above for encoders; each action generates the relevant entry in the submission log. Downloading modifies the <resp> element that signifies the proofreader’s name, and when the final proofed file has been uploaded, it is deposited in a third directory; hence, all archival copies of the document are preserved. Once the proofreader has uploaded the file, a third <revisionDesc> entry is generated and written into the file and the coordinator is notified via e-mail. 6. Finally, when the proofreader is done, an automatically generated e-mail is sent to the editor(s) with a URL for a transformed version of the file. The editor then visits the URL indicated to perform a final proofing of the document; the HTML page gives the editor the option to accept or modify the file, and the coordinator is notified of the status of the document. If the file is accepted, it is copied to the public directory and becomes available online; if it is modified, the editor’s modifications (again, submitted via a php-form) are routed back to the proofreader who can make the final emendations and subsequently upload the document into the public directory.  Additionally, the project coordinator has access to a report feature, which returns the entire SQL database of the submission log. This allows her to monitor the status of all ongoing projects and documents and to track what documents individual editors, encoders, and proofreaders are currently working on. Though there will, undoubtedly, be issues with certain edited documents that cannot be addressed through the type of automation described above, the document management system will do much to streamline the editing and encoding process and increase the quantity and quality of work done by the dispersed staff. Decisions about what to tag and how to tag complex features of Dickinson’s manuscripts are made by the general editors and enforced by the editorial submission form, so documents are encoded consistently across the various sets of correspondence. Editors can focus on editing, without having to learn advanced XML and TEI (although the system, as designed, will accept XML tags as part of submitted editorial data for those editors who are familiar with the TEI and the Dickinson project DTD). Encoders can focus on encoding, without being called upon to make difficult editorial choices. Ultimately, the entire process will facilitate integrity in editing, quality control, and institutional memory.  ",
       "article_title":"Confronting the Challenges in Collaborative Editing Projects: The Dickinson Electronic Archives File Management System",
       "authors":[
          {
             "given":"Lara",
             "family":"Vetter",
             "affiliation":[
                {
                   "original_name":" Maryland Institute for Technology in the Humanities, University of Maryland, College Park ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          },
          {
             "given":"Jarom",
             "family":"McDonald",
             "affiliation":[
                {
                   "original_name":" Maryland Institute for Technology in the Humanities, University of Maryland, College Park ",
                   "normalized_name":null,
                   "country":null,
                   "identifiers":{
                      "ror":null,
                      "GRID":null
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  The Centre for Computing in the Humanities at King’s College London is involved in three broadly Prosopographical projects: the Prosopography of the Byzantine Empire (PBE) (recently renamed Prosopography of the Byzantine World—PBW), the Prosopography of Anglo-Saxon England (PASE), and the Clergy of the Church of England Database (CCE). (All are funded by the UK’s Arts and Humanities Research Board.) The goals of these three projects at King’s are ambitious. PBE’s goal is “to record in a computerised relational database all surviving information about every individual mentioned in Byzantine sources during the period from 641 to 1261, and every individual mentioned in non-Byzantine sources during the same period who is ‘relevant’ (on a generous interpretation) to Byzantine affairs.” (from website, see references). PASE’s aim is “to provide a comprehensive biographical register of recorded inhabitants of Anglo-Saxon England (c. 450-1066).” (from website). CCE intends to create a “database of clergymen of the Church of England between 1540 and 1835.” (from website). The sources of information for all three projects are surviving manuscript records of many kinds. The central computing tool that all three projects employ is the relational database. Obviously, there is a significant issue involved in taking the textual source material, often presented discursively, and presenting it in the structured form that a database requires. Central, then, to the design of the database, and to the continuing process of putting data into it, is ensuring that the scholarly interpretation essential to this transformation is properly accommodated. In traditional Prosopography (see, for example, the well-known Prosopography of the Late Roman Empire, or the more recent Prosopographie der mittelbyzantinischen Zeit—PmbZ(Lille et al 1998-2002)), the central organising principle is the person. The information about the person is formed into an article by the scholar, and the articles themselves are organised by the person’s name. There is often some degree of more structured information attached (in PmbZ, for example, there is, among other lists, a formal list of sources in which information about the person could be gleaned). However, the primary source of information for the user is the article. The article is presented as a narrative in which we find a complex blending of quotation from sources and scholarly interpretation. Some of the assertions are made without further justification, in some cases an argument is provided to support why an assertion has been made, in some cases an issue is left unresolved and only alternatives are outlined. The scholar's task is to take the evidence provided by the sources s/he has read and to represent in the article the shades of certainty about any of them. All three of our Prosopographical projects take a radically different approach.Those familiar with the CD publication of PBE I may have observed that the approach taken there was actually a transitional one—somewhere between the article-oriented and factoid-oriented approach described here. The current phase of PBE work has fully taken on the approach described below The final publication will not be a set of volumes containing articles, but an online database. Furthermore, all three projects agree that there will be none or very few articles about persons in their database, and they will be written after the data collection process is complete, rather than being central to it. Instead, the evidence data will be recorded as a series of factoids—assertions made by the project team that a source “S” at location “L” states something (”F”) about person “P”. Factoid was first applied to this kind of information by Dion Smythe and Gorden Gallacher, and is not a statement of fact about a person; it is an assertion that a source says something about him/her. In Figure 1 you can see some sample output from the PBE database, showing factoids derived from the source Skylitzes Continuatus; for Emperor Alexios I Komnenos (identified in the DB as Alexios 001) in 1078. As the illustration suggests, there are several different kinds of factoids provided. In PBE, factoid data is collected for things such as activities or events in which the person took part; physical, spiritual or physiological descriptions applied to them; dignities or offices they held; ethnic group to which they belonged; kinship relationships with other people; locations with which they were associated; occupations they took up; possessions they owned; and religion they professed.  Figure 1: A Factoid List for Alexios I Komnenos (PBE)  Factoids are modeled as entities in the prosopographical database, and each factoid type contains both an explicit and implicit structure. The explicit structure can be relatively complex. Figure 2 shows the data capture screen displaying one of PASE's event factoids—the event being the tonsoring of Guthlac by Aelfthryth (as recorded in the Vita Sancti Guthlaci). Not only is there a description field that contains information about the act itself (here only the beginning of the full text recorded in the field is visible), but the event is: categorized in the Term field, attached to a place (the place described using the word found in the original text, the type of place it is and its modern day location), and linked to the two people involved, one identified as the recipient and the other as the agent.  Furthermore, the textual source for the event, and location in the text where the act is recorded is entered in the database, and there is space for recording a scholarly date recording when the event is thought to have occurred, and space to record whatever dating information is given in the source (only partially visible in this figure).  Finally, there is a place in the “Notes” and “Problems” field where the researcher can record in free text some commentary on the factoid that s/he considers important but does not fit the structured fields associated with the factoid itself.  Figure 2: An Event Factoid in PASE  There are, in addition, elements of implicit structure that must be recorded in the textual elements—a reference to another person in the database in the description of an event would be an example of this. Textual fields in our projects are, therefore, often structured as mini-XML documents, with XML being used to handle the structure that they contain and making it available for further machine handling. A relational database is most useful when the data it contains is highly structured. The capability of the relational model is often underestimated in scholarly circles, and both Greenstein (Greenstein 1994), and Townsend et al (in the AHDS Guides to Good Practice: Digitising History) begin their discussion of databases by acknowledging that a single table in the relational database as often too limited for large scale historical use (it is described as the “matrix straitjacket” in Townsend). Greenstein, however, recognises that the relational aspect of the relational model—which allows material from more than one table to be linked into a single logical entity —allows for richer collections of information to be formed. Once an entry (say, for the Person) in a table is changed from being a text string containing the person’s name to a link to a row in another “person information” table it is possible to record a great deal of richer information about that person. Thus, PBE, PASE and CCE databases contain not only structured data in the form of factoids, but they also contain complex structures spread over more than one table each that represent other important “objects” in the database related to the factoids such as persons, geographic locations and possessions. The challenges associated with our Prosopographical databases are many. First, there is a constant struggle to be sure that, for each field one enters, one is clear about to what extent the field represents simply what is in the source, and to what extent it is actually represents a scholarly interpretation of that source. Even the “original source” fields holds text out of context, making it a matter of scholarly decision about exactly what fragment should be included.Linking of our databases to full-text representations of the source texts have been considered by all projects. In all three projects, however, this has been rejected for various reasons. There was no funding to take on the preparation of a fresh electronic edition, and, in general, no scholarly reputable electronic editions of the texts were available elsewhere. PASE might yet explore some options in this area, however, for recording data from charters. Furthermore, for all three projects data is collected on a source-by-source basis by more than one researcher. The issue of consistency between researchers is constantly on our minds. Consistency issues are dealt with during the editorial phase of the project, where editors will use tools to assert, for example, that a person A in one source is the same person as person B in another. We expect to have more to say about these issues during our presentation at the conference. An article in a traditional prosopography provides a well organised bundle of information to its user in the form of a narrative. What happens when the prosopography contains large collections of factoids instead? As figure I suggests, the factoid model used in these projects provides a way that the machine can generate “micro-narratives”—to use the term presented in (Ramsay 2002). These narratives will be richly linked to other data in the system, and different micro-narratives will be generated when one enters from different starting points (say, from a location, rather than from a person), or by traversing links that connect materials in each factoid to the broader database contents. Clearly, a web access mechanism for these databases will need to be significantly more complex than a simple search form which results in a list of selected items. We believe that the best interface to this will provide the blending of a searching and browsing paradigm that is now characteristic of large websites on the WWW. All three projects are now reaching the stages where some detailed exploration of these presentation and selection issues can begin. We will describe some of these issues in more detail during the conference presentation. In moving from article-based to factoid-based prosopography, all three of our projects are participating in the development of a radically new approach to the field. The role of the scholar has clearly altered in our projects. In addition to being ultimately responsible for the projects’ “content”, all project members must work together to ensure that new methodologies are developed which ensure both quality and consistency of information across the large number of individual factoids. The role of the scholarly end-user may also have to change, and here too the project has to work to ensure that the experience of the end-user searching through these factoids is positive and useful. In the end, we are finding new ways that technology can serve these essential scholarly goals.  ",
       "article_title":"Texts into Databases: The Evolving Field of New-Style Prosopography",
       "authors":[
          {
             "given":"John",
             "family":"Bradley",
             "affiliation":[
                {
                   "original_name":" King's College London ",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          },
          {
             "given":"Harold",
             "family":"Short",
             "affiliation":[
                {
                   "original_name":" King's College London ",
                   "normalized_name":"King's College London",
                   "country":"United Kingdom",
                   "identifiers":{
                      "ror":"https://ror.org/0220mzb33",
                      "GRID":"grid.13097.3c"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Using highly interdisciplinary methods we have built a collaborative infrastructure for translation and annotation of ancient texts. This generalizable infrastructure is now fully deployed in the Suda On Line ). The Suda is a 10th century Byzantine Greek lexicon of some 30,000 lemmata. After four years of continuous development we have implemented a complex yet effective and practical system. Our goal is not only to provide the SOL as a useful tool for researchers, but also to explore and facilitate the modes of scholarship now made possible by open source technology and the internet: this effort is cooperative rather than solitary, communal rather than proprietary, worldwide rather than localized, evolving rather than static. Our international team of managing editors, editors, and translators has now worked up approximately one third of the material in the Suda, quite a satisfactory rate of progress. ACH/ALLC in Glasgow had an initial presentation concerning this project; we feel that substantial further development and our positive results warrant an update at this time. In order to encourage the participation of translators and editors, and in order to make the SOL database a useful scholarly resource as quickly as possible, we make our materials available to users as soon as it is submitted. We acknowledge that this philosophy raises concerns. One of the major issues with electronic publication of scholarship is the potential it has for circumventing time-tested procedures for quality control. While we do not want simply to add to the sea of uncontrolled material on the Web, at the same time we insist on our right to experiment, and we have no desire to replicate the print-publication paradigm in electronic format. Many of the advantages that electronic publication offers, including immediacy, accessibility and adaptability, are seriously handicapped by traditional editorial processes, where chronic bottlenecks frequently develop in the effort to keep the publishing house’s imprimatur off of anything with any detectable shortcomings. In order to exploit these advantages of the web while at the same time maintaining a reasonable level of quality control, submissions to the SOL database undergo the following process of editorial evaluation and monitoring: 1. Initial submissions immediately become accessible to users searching or browsing at the SOL site, but their “draft” status is clearly marked. 2. Once a submission has been carefully vetted by one of the SOL editors for errors and significant omissions, its status as part of the SOL database may rise from draft into one of two categories: low or high. At every stage of this process, the editors who participate in vetting and improving the entry will be prominently identified to the user, along with any descriptive comments they may provide concerning their editorial work. 3. Most importantly, even an entry that has achieved high status will not be considered perfect and immutable. At the discretion of the editors, improvements, changes and additions of links and bibliography can continue indefinitely.  While this way of doing things puts more of the burden of quality control on the end user, our system of marking editorial status gives researchers significant assistance in coming to an informed decision about the reliability of the material in SOL. In fact, our system offers definite advantages over the canonical paradigm of peer review from the consumer’s point of view. In print scholarship (and electronic scholarship that merely follows the traditional model) the number, identity, and qualifications of reviewers remain hidden, and one must usually base one’s estimate of the reliability of the scholarship solely on the identity of the author and the general reputation of the venue. In the standard paradigm, moreover, the end product is more or less fixed, whereas our database is being improved continuously. This presentation will describe our project from various perspectives, including the following principal points of discussion. (1) An overview of the Suda itself, including a few examples that illustrate its diverse composition and unique value for several fields of humanistic scholarship, despite its flaws and peculiarities. (2) The multiyear interdisciplinary collaboration among computer scientists, historians, and philologists that has produced our results so far. (3) The academic ideology that guides our production of a freely-available and open-ended e-text, including significant ways in which our editorial practices diverge from more traditional ones. (4) The most important features of the online site available on a hierarchical basis to the participants and the general public. (5) The specific applications and programming technologies that enable those features. (6) Our most recent effort: generation of a unified, complete, and self-documenting XML snapshot of our data. This latest ability addresses our responsibility to ensure the long-term archival security and viability of our results, and it also allows us to experiment with powerful new technologies centered around XSLT programming and the Cocoon environment for the transformation and publication of electronic documents. The presentation will include a demonstration of these experiments and conclude with the prospects for future developments.  ",
       "article_title":"The Suda On Line: Applying Computer Technology to Ancient and Byzantine Studies",
       "authors":[
          {
             "given":"Ross",
             "family":"Scaife",
             "affiliation":[
                {
                   "original_name":" University of Kentucky",
                   "normalized_name":"University of Kentucky",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02k3smh20",
                      "GRID":"grid.266539.d"
                   }
                }
             ]
          },
          {
             "given":"Raphael",
             "family":"Finkel",
             "affiliation":[
                {
                   "original_name":" University of Kentucky",
                   "normalized_name":"University of Kentucky",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/02k3smh20",
                      "GRID":"grid.266539.d"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Some 25 years ago, Hélène Cixous provocatively anticipated a distinctly female practice of writing. She declared that écriture féminine would be marked by characteristics which challenge the logic of writing within the “phallocentric” tradition, by its focus on the female body, glorying in a femininity too long repressed, and breaking up received truth through laughter. Her argument, based on her view of the poetic, implies a form of “false consciousness” in that not all women would, or even could, produce texts from this alternative practice. Further, she declared that écriture féminine cannot be defined or identified outside of itself:  It is impossible to define a feminine practice of writing, and this is an impossibility that will remain, for this practice can never be theorized, enclosed, encoded, coded —which doesn’t mean that it doesn’t exist. But it will always surpass the discourse that regulates the phallocentric system: it does and will take place in areas other than those subordinated to philosophical-theoretical domination. It will be conceived of only by subjects who are breakers of automatisms, by peripheral figures that no authority can ever subjugate.  Hélène Cixous, \"The Laugh of the Medusa\" in Signs: Journal of Women in Culture and Society 1:4 (1976) 883.   A generation of French and America feminist critics have addressed Cixous’ declaration, pro and con, which is based on two questionable propositions: that écriture féminine cannot be defined outside of its own terms and that not all women writing may be said to participate in this unique practice. It is unfortunate that so important a declaration would be auto-marginalized by positing its own epistemological and social indefinability. While Cixous may be right, that a practice of women’s writing would be hard to identify, she carries her attack of “phallocentric systems” of knowledge—rationality and logic—to such an extreme that any attempt to demonstrate the possibility of the existence of, or isolate some of the characteristics of, an écriture féminine is invalidated. I would argue, however, that any putative feminine practice of writing should be identifiable as recessive traits in the literary production of women who predate Cixous’s declaration and that these traits may be detected using systematic methodologies. A considerable body of recent work on gender marking in language use shows important, even critical, differences in male and female use of language.See for example, Deborah Tannen, You just don't understand: Women and men in conversation (Ballantine, 1990) and Gender and discourse (Oxford, 1994). While much of this work has been restricted to less formal forms of communication—speech, e-mail, and student essays—there is some evidence that gender is an important discriminant in more formal literary texts. For example, Minna Palander-Collin finds in her study of 17th century private letters that there are marked differences between female and male writing, suggesting that the women's letters are more interactional, personal and “involved” than letters by men, which are common features of women’s communication in Present-Day English.Minna Palander-Collin, \"Male and female styles in 17th century correspondence: I THINK\", in Language Variation and Change, 11 (1999), 123-141. More generally, recent studies by Moshe Koppel, Shlomo Argamon and Anat Shimoni have detected a wide variety of simple lexical and syntactic feature differences in literary texts by men and women in the British National Corpus (BNC). Using machine learning techniques, they are able to infer the gender of an author of an unseen document with approximately 80% accuracy, with moderately better performance for works of fiction than nonfiction.Moshe Koppel, Shlomo Argamon, Jonathan Fine and Amat Shimoni, \"Automatically Categorizing Written Texts by Author Gender\", forthcoming in Literary and Linguistic Computing (2003) and Moshe Koppel, Shlomo Argamon, Jonathan Fine and Amat Shimoni, \"Differences in Writing Style Between Male and Female Authors\" (paper submitted for publication). The success of text categorization techniques to identify modern literary texts by gender of author suggests that there are gendered practices of writing and that these gendered traditions are grounded in the history of literary culture and would be an important component of Cixous’ prospective écriture féminine. In the early 1990s, I attempted to examine the question of écriture féminine using the ARTFL database, only to be confronted by the very significant gender bias of the TLF database as it was then constituted, concluding that the sample of texts by women (3.8% of the titles) was too limited to allow for useful comparisons. This limitation led directly to our ongoing effort to digitize a large collection of French literary texts by women, ARTFL’s French Women Writers ProjectARTFL's French Women Writers Project is one of many projects to digitize neglected literary and non-literary texts by women, most inspired by the Brown Women Writers Project, including the University of Chicago Library's Italian Women Writers and commercial products such as Alexander Street Press' North American Women's Letters and Diaries. See  for a partial list of current projects and products. to redress the gender bias of the corpus which was used to compile the TLF dictionary. The gender bias in the data used to compile a massive and “definitive” dictionary is itself an important example of one mechanism of how patriarchal language is propagated and authorized.\"Gender representation and histoire des mentalités: Language and Power in the Trésor de la langue française,\" in Histoire et measure VI (1991): 349-73. My initial studies of gender representation in early modern and modern French—based exclusively on male writers describing the feminine —produced some striking examples of long-term shifts in the use and meaning of common gender terms, such as femme.\"Quantitative Linguistics and Histoire des mentalités: Gender Representation in the Trésor de la langue française, in R. Köhler and B. B. Rieger (eds.), Contributions to Quantitative Linguistics (Kluwer, 1993), pp. 361-381. Age categorization of women— young/old—becomes one of the most notable patterns only towards the end of the 18th century, reflecting both the rise of the romantic novel and a new politics of desire. Equally important are long-term continuities, such as the collocation of femme with possessives, suggesting that the semantic field of the feminine begins with putting her “in her place,” being possessed by a male.See also Tuija Pulkkinen, \"The History of Gender Concepts: The Concept of Woman\", in History of Concepts Newsletter, 5 (2002), 2-5. Cixous’ complaint that “woman has always functioned ‘within’ the discourse of man” simply because women must express themselves in “the language of men and their grammar”Cixous, p. 887. is a position that is certainly implied by my initial studies and needs to be taken into account when characterizing earlier examples of feminine writing. In order to examine possible earlier practices of écriture féminine, I am assembling two corpora of about 350 literary texts each by male and female authors from the 17th to the early 20th century, balanced by time period, genre, and subject matter (or collection). The women’s texts are drawn from a variety of current holdings at ARTFL, including 110 texts in French Women Writers, 70 from the ARTFL database, 40 from BASILE (Editions Champion), and 130 from various collections produced by Editions Bibliopolis.Links to the ARTFL/PhiloLogic implementations of these collections may be found at . The comparative male corpus will be selected from the same sources. Comparisons of the two corpora will be based on several distinct types of analysis. A comparative overview of the two corpora using the combination of some of the relatively simple lexical and syntactic features used by Koppel and his team, broken down by time period; A comparison of collocations of gender terms (femme, homme, mari, mère, etc.), loosely based on results from my earlier studies, effectively looking at how men and women represent themselves and the other gender, again broken down by period and genre. Examination of the “agency” of male and female actors as represented by the tense and the functional status of selected verbs associated with gender denoting terms. And a general assessment of the pragmatics associated with literary writing by males and females, by looking at use of hedges as well as the density of pronouns and adjectives.  Finally, following Pulkkinen’s call to consider the history of the concept of “woman” as a political construct, I would like to look at contrasting representations of social groups—inclusive and exclusive use of nous and other terms denoting belonging—a technique well known in lexicologie politique.\"Enlightened Nationalism in the Early Revolution\" The nation in the Language of the Société de 1789 in Canadian Journal of History (24), 1994, p. 28ff Using a combination of quantitative and systematic qualitative approaches to the two corpora, I hope to show changing patterns of gendered writing in French literature over a several century span. The continued expansion and improvement of electronic text holdings, in terms of the quality of the data as well as coverage of wider ranges of literatures well outside of established national canons, allows us to revisit theoretical and substantive problematics that we could not address previously. Women's writing is surely a case-in-point of this laudable development, facilitating systematic examinations of propositions made by critics and theorists like Hélène Cixous. Identification of the practices and distinguishable characteristics of écriture féminine in the centuries predating her “call to the pen” may help situate the traditions of gendered discourse the past as well as their relationship to current feminine writing.  ",
       "article_title":"Écriture féminine: Searching for an Indefinable Practice?",
       "authors":[
          {
             "given":"Mark",
             "family":"Olsen",
             "affiliation":[
                {
                   "original_name":" University of Chicago",
                   "normalized_name":"University of Chicago",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/024mw5h28",
                      "GRID":"grid.170205.1"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    },
    {
       "url":null,
       "identifier":{
          "string_id":null,
          "id_scheme":null
       },
       "abstract":"  Our research group has been awarded funding by the National Cancer Institute for a rhetorical analysis of “deception” in the Tobacco Documents (TDs). These documents, which were released by tobacco industry defendants as a result of state and federal litigation and legislative hearings, cover the complete range of corporate operations in the tobacco companies, from memos to research papers to procurement invoices. The documents are stored physically in depositories in Minneapolis (the site of the original trial) and Guildford, England, and large collections of them (more than five million documents) are now available in electronic form on the Web as well. The documents represent a rich source of corporate and technical discourse which had never been subjected to systematic linguistic analysis; indeed, we are not aware that any similar corporate body of documents has ever been available for analysis. Rather than choosing specific documents for analysis, a method which would leave itself open to attack on grounds of highly selective use of data, the premise of our work is to treat the TDs as a corpus, and to apply accepted methods of corpus and forensic linguistics and rhetorical analysis. Of course, this required that we create sub-corpora for study, since we do not have the resources to include the entire set. Here we will present our experience with the planning and creation of our TD corpora: the sampling strategy, archiving, retrieval, and ultimately, making the corpora available via the Internet for further research. Our initial goal was to create a series of corpora from the TDs in order to 1) Identify TDs in which rhetorical manipulation (“deception”) may have occurred, and to estimate the extent and prevalence of manipulation; and 2) Analyze manipulation we find in order to classify it and develop means to identify similar manipulation in other industrial situations. To do so we have followed a three-part strategy for corpus creation which emphasizes rigorous sampling methods. We first drew a limited sample from the entire body of TDs so that we could determine the best classification of text types and estimate their proportions within the overall body of texts. From those text types which we considered relevant to (i.e. subject to) rhetorical manipulation, we devised quotas for creating a reference corpus of approximately 500,000 words, which we estimated to consist of 808 documents. For this reference corpus, all relevant TDs were sampled whether or not they were thought to contain any manipulation. Finally, we are presently compiling a corpus which includes all texts which we determine to contain any rhetorical manipulation, along with parallel corpora of earlier drafts of the same texts or versions of the same texts prepared for other audiences, so that detailed analysis of rhetorical manipulation can be carried out for itself and by comparison with cross-draft and cross-audience TDs. As it has turned out, the plan has been effective in the first two parts which we have now completed, but we have had to make adjustments at several points in order to take account of our preliminary findings. Once we began the process of collecting documents we immediately encountered two problems related to archiving and processing the data. The first is that there was no text available. Rather than being stored digitally as the plain ASCII text which we needed for computer-assisted corpus analysis, the tobacco documents are stored as image files, usually as TIF type. This problem was compounded by the fact that the images, although stored as large high-resolution files, are generally too poor in quality for automated text processing such as scanning and OCR. They often have pages that are tipped to one side or, in the case of dot-matrix or fax printing and handwriting, they can be practically illegible. The second problem we encountered was the structural complexity of the documents themselves. For example, just over 50 percent of the documents contained marginalia of some type, such as filing data, distribution lists, stamps of various types, editing, and handwritten comments. Most documents contained large amounts of peripheral data like names, dates, addresses, and distribution lists. Although these features are significant for archiving, they have little or no rhetorical value for our intended research. Other documents often contain or consist of forms, tables, and images that also offered little value for our analysis. To account for these problems we decided to keyboard the documents by hand as plain ASCII text and to code them with XML tags. When we investigated the existing XML tag sets, TEI in particular, we found that they are particularly well suited for archiving standard texts in standard hierarchies and for naming typesetting conventions. However, we chose to devise a set of tags specific to our project for primarily two reasons. First, we found that many of the documents collected for the corpus had a very non-standard format. In fact, we found no fixed definition for what constitutes a document in the tobacco archives. For example, a document recently coded began in the middle of a paragraph and sentence, proceeded for half of a page, changed to a summary of a court ruling, then to a policy letter, then to a table of denicotinization, and ended with a diagram of a processing facility. The second reason for devising our own tag set is that our primary interest is in archiving rhetorically significant text and events rather than the typesetting conventions used to represent them. Thus although TEI includes a full set of tags to indicate divisions and typographical conventions, use of these tags for our purposes might lead to ambiguities. For example, italics, boldface, and underlining have all been found to denote emphasis in the document set, which is rhetorically significant; however, they have also been found to denote titles, headings, names, quotations, formulas, and standard text, which may be of little value for our analysis. Thus, tagging a word in a document with a tag designed to denote typesetting, such as italics, may not be so useful when the corpus is analyzed linguistically or rhetorically simply because there is no way to know the significance of that particular event. To counter this, we have devised a set of XML tags which accommodates the structural complexity of the original documents and which reflects the purpose of our study. Data is retrieved from the XML files in a straightforward manner. We have embedded the expat XSLT engine into several Python scripts. This allows us to assemble a text corpus for study from the reference and manipulated-cases corpora according to the needs of the research. That is, with our scripts the XML files are parsed, desired tag content is selected, and the selected content is assembled and written to an ASCII text file. There are, however, two notable differences between the standard Web use of XSLT and that of our project. The first is data permanence. The output of our XSLT processing is ASCII/ANSI text which is written to file for later analysis rather than HTML sent onto the Internet. The other difference is that the XSLT output is not solely determined by the XSL stylesheet. For ease and speed in processing, some general document and tag selection is done by regular expressions in the Python script prior to calling the expat program. The end result of this initial phase of our project will be a larger general corpus of TDs for use as a reference, and a smaller corpus of “manipulated” TDs for focused analysis. Both will be archived as ASCII text with XML tags, which will allow us to generate tailored sub-corpora for specific studies using XSLT. Although these corpora are being created for our own purposes, our intent is to make them freely available to other researchers over the Internet. What we envision is an integrated Web site that provides access to the corpora in several formats: the TIF and/or PDF images of the original documents, the XML files coded with TEI compliant tags, the XML files coded with our tag set, ASCII text versions of the files, and access to a CGI version of our XSLT scripts for generating task-specific sub-corpora.  ",
       "article_title":"The Tobacco Documents Corpus: Archiving the Industry",
       "authors":[
          {
             "given":"Clayton",
             "family":"Darwin",
             "affiliation":[
                {
                   "original_name":" University of Georgia",
                   "normalized_name":"University of Georgia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00te3t702",
                      "GRID":"grid.213876.9"
                   }
                }
             ]
          },
          {
             "given":"William",
             "family":"Kretaschmar",
             "affiliation":[
                {
                   "original_name":" University of Georgia",
                   "normalized_name":"University of Georgia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00te3t702",
                      "GRID":"grid.213876.9"
                   }
                }
             ]
          },
          {
             "given":"Donald",
             "family":"Rubin",
             "affiliation":[
                {
                   "original_name":" University of Georgia",
                   "normalized_name":"University of Georgia",
                   "country":"United States",
                   "identifiers":{
                      "ror":"https://ror.org/00te3t702",
                      "GRID":"grid.213876.9"
                   }
                }
             ]
          }
       ],
       "publisher":" University of Georgia ",
       "date":"2003",
       " keywords":null,
       "journal_title":"ADHO Conference Abstracts",
       "volume":null,
       "issue":null,
       "ISSN":[
          {
             "value":null,
             "type":null
          }
       ]
    }
 ]