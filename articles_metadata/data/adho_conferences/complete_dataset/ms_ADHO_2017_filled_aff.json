[
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionAs opposed to the former idea of creative autonomy, in recent years, humanities research tends to investigate cultural contexts and circumstances, inspirational models, and the ways that knowledge, experience and expertise have been transferred over time. We address the question of \"creative transfer\" within the field of music. Due to the everlasting significance of musical works, relationships between musiciansthe entry point for such an investigation - are well documented in archives, libraries and museums. In print media, usually only a single relation between two musicians is narrated. Furthermore, it is common for the biography of only one of the two musicians to report on the relationship. Larger overviews of social networks between several musicians seldom exist. Although some digital resources exist, these are often reduced to the milieux of popular musicians like Mozart and Beethoven.Since 2005, musicologists of the project Bavarian Musicians Encyclopedia Online (Bayerisches Musiker Lexikon Online, BMLO) have systematically collected biographical data (an example is given in Figure 1) and examined relationships between musicians from print media - a tedious work that results in a unique database of great value for musicology. The BMLO contains musicians from all kinds of musical professions (e.g., composers, singers, musicologists, instrument makers, ...), most of whom had an active lifetime period living in Bavaria or a considerable influence on Bavaria. Now providing information about around 28,000 musicians, the BMLO has achieved global scope, one that is underpinned by the many musicologists worldwide who use the BMLO for their daily work. In earlier works, we developed visual interfaces on the basis of the BMLO data for profiling musicians (Jä- nicke et al, 2016), and for the distant reading of musicians' biographies ( Khulusi et al, 2016). However, the social network inherent in the BMLO has remained untouched so far. Using the BMLO, only the social network of single musicians can be observed, as is the case when using print media. In order to facilitate an extensive analysis of the entire social network concealed in the BMLO, we designed a visualization that brings together all of the relationships in the form of an interactive social network graph. In contrast to previous means of investigating the transfer of musical knowledge, we allow for the dynamic exploration of relationships among musicians over generations. Graph TopologyInformation regarding relationships to other musicians in the database is provided for 9,805 musicians of the BMLO. Only one relation exists for around 46,5% of these musicians, and just 261 musicians have ten or more relations. Adolf Wilhelm August Sandberger is the musician with most relations (97). The average number of relations for musicians is 2.6. The resultant graph structure of the social network consists of 1,420 connected components, the largest component connects 5,539 musicians, the second largest only 56 musicians - 1,385 connected components contain less than ten musicians.Due to the above mentioned topological features of the graph, the typical, straightforward visualization using a force-directed layout approach, e.g., by using tools such as Gephi (Bastian, 2009), leads to a global overview of the social network (see Figure 2). However, local structures are hardly readable, which makes an interactive exploration nearly impossible. The objective of this work was to develop a graph design that makes the social network of musicians visually accessible for the first time, and, moreover, capable of being explored in accordance with the research questions of the collaborating musicologists. We focused on the largest connected component that causes the greatest challenges for this task.  Graph & Interface Design to Analyze Teacher-Student RelationshipsThe preliminary step when generating the social network graph is filtering according to the underlying research question. At first, a filtering can be done by relationship type(s). Second, it is possible to focus exclusively on musicians with specific professions (e.g., instrumentalists). In the following discussion, we focus on the motivating example for this work: the analysis of teacher-student relationships to investigate how musical knowledge, experience and expertise have been transferred over time. The corresponding filter keeps 3,994 musicians, the largest connected component of this sub-network - the research object of the musicologists - contains 2,769 teachers and students. The Gephi output for this graph is given in Fig- ure 3. Although the structures are slightly finer due to the reduced number of nodes and edges, the highly connected part in the interior of the graph remains cluttered. Here, we list our design decisions applied in order to generate a readable graph (see Figure 4) and a navigable interface. • Temporally aligned graph: It was particularly important for the musicologists that the graph layout includes a temporal dimension, so that relations can be chronologically analyzed from left-to-right. Therefore, we applied a force-directed graph layout and used fixed x-values that represent a time-stamp, which reflects the middle of a musician's creative lifetime (see Jänicke 2016), on a horizontal time axis. As a result, the nodes only spread vertically and the chronological order remains intact.• Node grouping: Because the underlying research question investigates transfer paths of musical knowledge, we hide the nodes of musicians who never had the role of a teacher. Still, these musicians are grouped to their teachers, and can be accessed in the exploration process. This design decision reduces the number of nodes to be displayed from 2,769 to 608.• Node layout: To illustrate the significance and the influence of personalities, the sizes of nodes reflect the number of students of the corresponding teachers, which makes teachers with many students salient. Per default, node labels are hidden, but for navigation purposes, a user-defined number of node labels with the corresponding musicians names can be shown on demand. Either the most popular musicians or the teachers with most students can be highlighted.• Interactivity: Hovering over a node shows the corresponding musician and two lists of students (those who became teachers and those who did not) in a popup box. Clicking a node highlights all connections to a teacher's students who became themselves teachers. This way, transfer paths of musical knowledge can be assembled interactively.• Musical profession analysis: For the selected (via mouse click) musicians in the graph, the evolution of musical professions can be analyzed. Therefore, all musical professions of the teachers' students are listed by decreasing frequency. For each profession, a bar chart illustrates when they have been pursued. Analysis of Teacher-Student RelationshipsThis section outlines a usage scenario of the teacher-student network taking the example of Adolf Wilhelm August Sandberger who established musicology as a subject of study in Munich.First, we compare Sandberger to one of his teachers, Joseph Rheinberger, both being the teachers with the highest numbers of students (the BMLO lists 97 students for Sandberger and 87 students for Rheinberger). Of special interest was the comparative analysis of the musical professions of their students in order to assess the similarity of both teachers' studentries. Figure 5 shows the two selected teachers in the social network, and a view at the summarized musical professions of their students is given. While composition was the major musical profession of Rheinbergers students (70x), this number drops for Sandbergers students (52x). On the other hand, the number of musicologists increase (10x → 65x). Other significant changes can be seen for the professions choirmaster  → 26x). Thus, the visualization reflects a change of the musical profile of both studentries from composition to composition science- a hypothesis that could be verified with our system.Second, we examined the change of teaching since Sandberger established musicology in Munich. Therefore, we observed the musical professions of the students of Sandberger and his successors in Munich, Rudolf von Ficker, Thrasybulos Georgios Georgiades and Theodor Göllner (see Fig. 6). While the musicologist is the most frequent taught musical profession, the composer gets less and less important. The last teacher Theodor Göllner even had no student with the composer as musical profession. Thus, the change from composition to composition science that already started with Sandberger compared to Rheinberger, steadily continued with Sandberger's successors.   ConclusionThrough close collaboration with computer scientists and musicologists implementing a user-centered design approach, we developed a visualization that allows for the dynamic, interactive exploration of the social network of musicians, focusing primarily on teacher-student relationships. In contrast to out-ofthe-box tools like Gephi, we took the research questions of the collaborating musicologists into account when designing the graph and the user interface. Although detailed information about individual relation periods between musicians as well as the taught musical professions are not included in the underlying database, the provided interface facilitates a novel view on the social network of musicians, which allows to draw conclusions on the question of the transfer of musical knowledge.The value of our system for users of the BMLO is not only that social networks are visualized for the first time, but also that the graph may be filtered in accordance with the way that specific research questions can be investigated. Next to teacher-student relationships, familial or labor relationships also create valuable networks to be explored. Furthermore, it is possible to analyze sub-networks concerning musical professions, and to combine relationship types with musical professions. For example, when combining teacher-student relationships with the musical profession instrumentalist (see Fig. 7), Wolfgang Amadeus Mozart shows up at the beginning of the instrument playing knowledge transfer. ",
        "article_title": "Untangling the Social Network of Musicians",
        "authors": [
            {
                "given": "Stefan",
                "family": "Jänicke",
                "affiliation": [
                    {
                        "original_name": "Leipzig University",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            },
            {
                "given": "Josef",
                "family": "Focht",
                "affiliation": [
                    {
                        "original_name": "Leipzig University",
                        "normalized_name": "Leipzig University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03s7gtk40",
                            "GRID": "grid.9647.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " A Real-World ProblemLinguists estimate that 50-90 percent of the 6,000 to 7,000 known languages in the world will disappear in the 21 st century (Grenoble, 2011) with Harrison (2007) estimating that loss is occurring at a rate of one language every ten days. Endangered languages are often spoken primarily, or only, by Elders and as fluent Elders are lost, so is the language. Indigenous languages in Canada are not exempt from language shift; indeed, only 50 of the more than 60 Indigenous languages known to have been spoken in Canada exist today and most of these languages are classified as either endangered or already extinct (Kirkness, 1998). Only three (Norris, 1998) or four ( Kirkness, 1998) of these languages are expected to survive.The loss of an Indigenous language is associated with the loss of Indigenous knowledge and culture. Such knowledge systems incorporate social and historical dimensions including social relationships, cosmology or world views, oral history, place names, spiritual relationships, ecological knowledge, oral literatures, and philosophies (Battiste, 2008;Berkes, 1993;Hinton, 2008aHinton, , 2008bKipp, 2009). These knowledge systems are embedded within the language and the loss of the language results in the loss of the knowledge systems. Language is important to the health of the community and language revitalization has been identified as playing \"a vital role in community growth, healing, education, development, strong families and reconnection to the past\" (First Peoples' Heritage, Language and Culture Council, 2010: 7). Language loss and revitalization is a global, real world problem. ChallengesChallenges to language revitalization include lack of ideological clarification (Dauenhauer and Dauen- hauer, 1998); disagreement as to recording or sharing language (Adley-SantaMaria, 1997); differences in personal beliefs (Kroskrity, 2009); economic impacts (Adegbija, 2008;Hornberger and King, 2008;Kroskrity, 2009); the perceived status of a language and the self-esteem of speakers; and feeling shame and embarrassment about the language and culture (Dau- enhauer and Dauenhauer, 1998).The digital divide is another potential barrier to language revitalization. The digital divide separates individuals and communities who have access to technology and those individuals and communities that do not. Exacerbating the issue of the digital divide is that language programs may inadvertently become technology projects which \"often focus on providing hardware and software and pay insufficient attention to the human and social systems that must also change for technology to make a difference\" (Warschauer, 2004: 6). Discussions around the digital divide must include the technical aspects such as access to technology and problems with infrastructure as well as the social aspects including education in the technology, gender, age, language, economics, and literacy (Warschauer, 2004). TechnologyInformation and communication technology has been used with languages since the late 1800s, when audio recordings of Indigenous peoples were made on wax cylinders (Makagon and Neumann, 2008). These recordings allowed for unidirectional activity; that is, individuals could listen to the recording but could not interact with it. Today, the advances in ICT can provide an interactive, bi-directional experience in which users can interact either with the technology or other users. Language can also be captured in context with cultural activities allowing for a deeper understanding of the language. Multimedia applications are becoming increasingly easy to create and allow for the integration of video, audio, pictures, and text, as well as interaction with human beings. Access to databases and dictionaries provides teachers, administrators, and learners immediate access to language at the word, grammatical, and contextual levels. However, Tyler (2002)  notes that: \"The Internet provides people with a technology that allows them to engage in activities that they have already had ways to engage in but provides them with some added efficiencies and opportunities to tailor their interactions to better meet their needs. However, there is nothing fundamentally different about the Internet that transforms basic psychological or social life. \" (204)If we apply this statement to Indigenous language programs, technology will be most successful where the language is already being used and where the language is not being used, technology will not increase the usage as is the case of the Upriver Halq'eméylememéylem language community. An endangered language requires very different strategies than a thriving language, and these strategies should determine how digital technologies are used. For example, a community with a thriving language may use technology to encourage conversations between geographically dispersed individuals, to increase the use of language through written communication using email or chat functions, or to provide exposure to the language by posting information on social media sites or blogs. A community with an endangered language may choose to use technology for documentation and archiving so that the language is not lost forever.My research attempted to understand the effectiveness of technology within an established language program with the goal of providing additional information to help communities that are either considering a language program or have one in flight that uses technology. FindingsThe Upriver Halq'eméylememéylem language community began to use ICT in the mid-1900s to document their language. Over time, the community continued to incorporate ICT and today ICT is an integral tool in the teaching of the language. Table 1 identifies the ten technologies identified as being used with the Halq'eméylememéylem language along the top row and the learning strategies used in the first column. Understanding the how the learning strategies intersected with the technology used provided key information as to how the technology supports language learning. Participants identified that ICT is being used successfully as a supplementary tool in coordination with specific learning strategies and activities such as story-telling, games, and looking up a word or concept but that ICT is not being used to support Halq'eméylememéylem learning activities outside of those specific learning activities. Additionally, participants indicated that ICT that enables human to human interaction has significant potential to contribute to developing fluency but only if the language is already being used. Table 1 highlights that ICT is rarely used with the Halq'eméylememéylem language outside of learning situations such as classrooms. This does not seem to be related to any digital divide issues as participants confirmed that they and other community members use ICT on a regular basis for non-language related activities. There may be multiple reasons why participants do not use ICT with the Halq'eméylememéylem language; however, Burton suggested that the primary reason that Halq'eméylememéylem specific ICT is not used by community members is because people do not use the language: \"But the thing about the technology, the thing about everything, classes, education, language planning, everything that we try to do, it's like we're trying to support something that's not happening. So, if people were talking to their aunt and their grandmother or a couple that said we're going to make Halq'eméylem a part of our life and so on, then the technology and the classes could help them. But if all you have is the technology, then that's not going to solve the problem. The problem is a social problem, or a personal problem.\" (personal communication, July 11, 2013) Technology will be most successful where the language is already being used and, where the language is not being used, technology will not increase the usage as in the case of the Upriver Halq'eméylememéylem language community. Technology to revitalize languages needs to be carefully planned with corresponding plans to introduce the technology and then post-implementation activities and oversight to ensure that the language continues to grow.",
        "article_title": "I Built an App to Revitalize a Language: Now What? A Real-World Problem",
        "authors": [
            {
                "given": "Nicolle",
                "family": "Bourget",
                "affiliation": [
                    {
                        "original_name": "Royal Roads University",
                        "normalized_name": "Royal Roads University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/05w4ste42",
                            "GRID": "grid.262714.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe ways in which the British have discussed 'uncivilized' peoples which travellers have encountered throughout the history of English gives a key insight into how people in the past have identified and classified the world around them. This paper uses data from the Hansard Corpus ,-./-1../ (Alexander and Davies, MNOP-) alongside the Historical Thesaurus of English ( Kay et al, MNOP-) to analyse the evolution of how the English-speaking people have thought of those who they think uncivil in five different sensefamilies -as animals, as ill-formed people, as strange-speaking outsiders, as savages, and finally as innocents awaiting enlightenment. Only these large digital data sources can show us the patterning of who and what the British Parliament have considered to be barbarous across time. DataThis analysis became possible following the completion of the Historical Thesaurus of English (HT) in MNNU and the semantically-tagged Hansard Corpus ,-./-1..< in MNOP, both of which are currently directed by Alexander and were created by teams of scholars at the University of Glasgow.The HT is a database of all the recorded words in the history of English arranged according to their meaning; one of the world's oldest digital humanities projects, and in progress for over PN years, the HT database (stored on media from punch cards to tape to diskettes to networked storage to the Web) allows us an unparalleled resource for analysing the history of English. The Hansard Corpus ,-./-1..<, completed in MNOP, is a digital corpus of speeches in the British Parliament between those dates, consisting of O.Ybn words across Z.Ym speeches. Its contents were semantically tagged in the MNO[-OP SAMUELS project (The SAMUELS Consortium, MNOP) with disambiguated meaning codes from the HT, making it possible to search for semantic categories rather than words, as we do below. The UncivilThe category of Civilization in the HT gives us an indication of a non-typical pattern in the number of words available to describe a given concept (in English, categories normally grow throughout time) in the words referring to uncivilized and a lack of civilization, as Figure O shows.Figure (: The size of each subcategory of Civilization in the HT While the size of the uncivilized adjective category rises in the latter MN th century, there is a substantial fall at the same time in the size of the lack of civilization noun category, which we argue is connected to the shift in who has been considered to be uncivil (see below). In addition, of the [M words in the uncivilized category in the HT (see Figure M), the vast majority follow a particular path of lexicalization which we describe below, with new terms reflecting the shifting conceptualization of the uncivil throughout the times at which they were coined. Thus far this sort of analysis has been slow-paced and difficult to undertake. However, with the tagging in the Hansard Corpus ,-./-1..< we can investigate this sort of semantic and conceptual change in a much more rapid fashion by honing in on uses of these meanings in context across time. ParliamentThere are five families of meaning into which the words above can be categorised, as outlined above. In a past article (Alexander and Struan, MNO`), we assembled some evidence for this from the history of English in a non-systematic fashion. For this short paper, we instead account for all the evidence from the Hansard Corpus - over M,NNN uses of the semantic category - in order to trace across recorded Parliamentary history the shifts in the cultural, political and social attitudes towards the 'uncivilized'. This shows a substantial change in the picture which differs from the simpler five-family view of the sense evolution of uncivil we described in that earlier article.Our first change to discuss is the shift, shown below, from the uncivil primarily being foreigners in the OaNNs to being domestic persons in the OUNNs onwards. This is reflected in the changing discourse surrounding barbaric and uncivil things, where a majority of MN th century uses refer to barbaric practices and actions rather than persons: Through four other graphs, we further report on the distribution of uncivil references across the globe and between the two Houses of Parliament. We also show the changes in the five evolutionary sense-families we outline above, which is key to the foreign/domestic shift we describe. Some quotes from the corpus can briefly illustrate these changes, which here are aimed at a general body of persons, or a country:Mr Charles Adderley, House of Commons MO February OaYP: '.. .to discharge what Lord Grey described as the singular office of dispensing rude laws among uncivilized tribes.' Earl of Carnarvon, House of Lords OM May OaZ[, on India: 'But a central government is not enough. In barbarous times and in uncivilized countries, roads are the first condition of improvement; and here it will be our first duty to open and secure the maintenance of roads and trade-paths.' Mr Richard Cherry (Attorney-General for Ireland), House of Commons MN March OUNa: 'I never said that the people of Ireland were West African savages.' Lord Hylton, House of Lords Oa April OUUP: 'We can now see that in dealing with Russia we are dealing with a semi-barbarous state and a society that only knew a measure of democracy for a few years before the First World War.' Mr Andrew Robathan, House of Commons O November MNNO, on the pending invasion of Iraq: 'We should not allow a barbaric, mediaeval [sic] regime to succeed or last. We certainly do not want to go back to civil war.'As a result, we can show empirically the shift over two centuries in the ways which Members of Parliament described uncivil or barbaric entities, from foreign people or places to domestic practices. We conclude by arguing that this is the result of increased oppositionality being shown in the digital Parliamentary record, and so in this short paper we combine 'big picture' graphs of large-scale data analysis with more focused examples from the corpus record. ",
        "article_title": "Digital Hansard: Politics and the Uncivil",
        "authors": [
            {
                "given": "Marc",
                "family": "Alexander",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Andrew",
                "family": "Struan",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Linguistic DNA project (LDNA) is an AHRCfunded collaborative project (AHRC grant AH/M00614X/1) between the universities of Sheffield, Glasgow, and Sussex which is designing automatic processes to investigate the emergence and development of concepts in pre-1800 CE print. Employing Early English Books Online, manually-transcribed through the Text Creation Partnership (EEBO-TCP) as its primary dataset, supplemented by Eighteenth Century Collections Online (ECCO-TCP) and other high-quality 18th-century text collections, the project is developing and refining a processing pipeline which assembles groupings of words bound together by their contextual use in printed discourse. The project is charting development of these discourse-embedded word groups across time, investigating how they are shaped by historical and literary contexts, the boundaries and overlap between the groupings, and the interaction of 'encyclopedic' groupings with more traditional 'thesaurus'-style semantic fields.This paper discusses results from a branch of the project which is investigating incidences of rapid change in the size of semantic categories as represented in The Historical Thesaurus of English ( Kay et al., 2016). Development of concepts through size of Thesaurus categories has been investigated previously (cf. Alexander and Struan, 2013;Jürgen-Diller, 2014), although the extra dimension provided by the outputs of the LDNA processor allows a dramatic leap forward in such research by enabling identification of instances in which change in discourse-embedded word groupings acts as catalyst for corresponding rapid change in the semantic fields of English.The present case study investigates words relating to the concept of 'virginity', utilising processed time-slice subsets of EEBO-TCP as snapshots of the discourse context for these words in Early Modern English print. By building sample word-groupings (the term 'cluster' is here avoided to avoid confusion with cluster or network analysis word clusters) for each of the subsets, it establishes the discourse context of 'virginity' words at different points in the timespan covered by EEBO-TCP. Comparison of these groupings suggests change in focus of language users, through which a largely religious context of use opens out to a secular and then a poetic literary context, suggesting that society's consciousness of this concept and the scale on which it was discussed enlarged dramatically in the period covered by the sub-corpora. MethodologyIn order to select a Historical Thesaurus category for analysis, an average pattern of change over time was established. The Thesaurus arranges the words of the English language into a semantic hierarchy that is seven category levels deep with the potential for up to four further sub-category levels within any given category. Owing to the incredibly fine-grained nature of the sense categorisation in the Thesaurus, it was necessary to 'cut' the hierarchy at human scale using a thematic category set, developed during the AHRC-and ESRC-funded SAMUELS project (Grant AH/L010062/1), which is intended to allow Thesaurus users to find information at a level that is salient to human beings - i.e. neither too general nor too detailed. The number of lexemes within each category level was counted, and lexemes were filtered to include only those active within the approximate time range of the EEBO-TCP collection, i.e. 1475-1700 CE. This data was aggregated so that the change in the mean contents of a category could be viewed across time, and decade-to-decade percentage changes calculated. Individual categories were then compared to this average category change, and a deviation of more than 5% from the average change considered to be significant. Out of the categories which were marked as statistically unusual from this process, category 'AI09g Virginity' was selected as promising because the items in its lexis had a relatively low number of homographs that could skew the results towards irrelevant information.Testing of the LDNA processor outputs is being conducted on select subsets extracted to provide snapshots across the EEBO time-period. The subsets used for this paper cover the periods 1520-39, 1550-59, 1610-11, and 1649. They are designed to contain a similar number of tokens; the progressively contracting timespans reflect the concomitant growth of printed material throughout the 15 th to 17 th centuries. Each token in the text is regularised, lemmatised, and tagged with a NUPOS part of speech tag via the MorphAdorner pipeline developed by Martin Müller and Philip Burns (Burns, 2013). Data is then gathered by the LDNA processor for the token's cooccurrences within 100-and 200-word bi-directional windows which are intended to simulate paragraph-like sections of the proximate discourse (cf. Fitzmaurice et al. forthcoming). Pointwise Mutual Information (PMI) is used to provide a statistic for likelihood of word co-occurrences; a minimum PMI value of 0.5 was arrived at experimentally for identifying node-collocate pairs to be considered interesting in initial stages of investigation.Seven items - 'maid, ' 'maiden,' 'maidenhead,' 'un- defiled,' 'vestal,' 'virgin,' and 'virginity' - in the 'Virginity' category were found to be present consistently across the subsets (although an eighth -'virginal' -was present in the 1520-39 and 1610-11 subsets). The co-occurrences were then processed to identify those which occurred with multiple items in this list. Words which co-occurred with four or more items were investigated further. ResultsComparison of the co-occurrence results across the five text subsets shows a consistent shift in the patterns of word association with 'Virginity' category items. The words 'woman' and 'widow' remain strongly associated with the terms across all the subsets, demonstrating societal preoccupation with female rather than male virginity. The most evident change in the grouping is movement from a predominately religious discourse context into the secular world. In the 1520-39 subset, the Virgin Mary is intimately related to discussion of virginity. In the shared collocates listing, mother collocates with all seven of the 'Virginity' lexemes, 'mary' with six, 'angel,' 'bless,' 'hymn,' 'nativity,' and 'nazareth' with five each. Of these, only 'mother' maintains a strong association with 'Virginity' words throughout the EEBO period, appearing with four items in the 1610-11 text set and five in 1649.The secularisation of the term is suggested by the prevalence in later subsets of words relating to marriage, reflecting what appears to be a growing focus on wedlock being preceded by virginity. 'Marry' gradually increases its association with the node items, collocating with four, then five, then seven from 1520 to 1649. 'Marriage' and 'wife' both enter the shared collocate group in 1550, and remain there through to 1649, whilst 'matrimony' is present in 1550, drops out in 1610, and returns in 1649.The extensive list of shared collocates in the 1649 sub-corpus strongly reflects the greater prevalence of literary fiction and poetry in printers' output and reinforces that virginity is a topic for which the discourse context is expanding; where it was easy to intuitively group 'marry,' 'marriage,' and 'wife' together, the 1649 collocates do not form easily identifiable groupings. DiscussionThe consistency of the core items found in the subsets is interesting in its disparity with the Thesaurus data, where the increase in the number of terms present in the 'Virginity' category suggests that there should be an expanding number of items found throughout these subsets. The most likely explanation for this is loss of low frequency information through a combination of cut-off values intended to reduce noise for later clustering experiments, and difficulty in normalising/lemmatising low frequency items. A clear outcome of the analysis is the confirmation that the category of 'Virginity' contains core vocabulary which remains almost unchanged in over a century (i.e. 1520-1649), primarily a consistent group of seven items which co-occur with 'Virginity' category words.This study demonstrates that understanding of semantic development can be enriched by such cross-analysis of discursive-concept word groups with Thesaurus semantic fields and the word groups which travel through time with multiple items of Thesaurus categories.",
        "article_title": "The Seven Words of the Virgin: Identifying change in the discourse context of the concept of virginity in Early Modern English",
        "authors": [
            {
                "given": "Susan",
                "family": "Fitzmaurice",
                "affiliation": [
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sussex",
                        "normalized_name": "University of Sussex",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00ayhx656",
                            "GRID": "grid.12082.39"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Justyna",
                "family": "Robinson",
                "affiliation": [
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sussex",
                        "normalized_name": "University of Sussex",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00ayhx656",
                            "GRID": "grid.12082.39"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Iona",
                "family": "Hine",
                "affiliation": [
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sussex",
                        "normalized_name": "University of Sussex",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00ayhx656",
                            "GRID": "grid.12082.39"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Fraser",
                "family": "Dallachy",
                "affiliation": [
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sussex",
                        "normalized_name": "University of Sussex",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00ayhx656",
                            "GRID": "grid.12082.39"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Kathryn",
                "family": "Rogers",
                "affiliation": [
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sussex",
                        "normalized_name": "University of Sussex",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00ayhx656",
                            "GRID": "grid.12082.39"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Marc",
                "family": "Alexander",
                "affiliation": [
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sussex",
                        "normalized_name": "University of Sussex",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00ayhx656",
                            "GRID": "grid.12082.39"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Michael",
                "family": "Pidd",
                "affiliation": [
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sussex",
                        "normalized_name": "University of Sussex",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00ayhx656",
                            "GRID": "grid.12082.39"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Seth",
                "family": "Mehl",
                "affiliation": [
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sussex",
                        "normalized_name": "University of Sussex",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00ayhx656",
                            "GRID": "grid.12082.39"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Matthew",
                "family": "Groves",
                "affiliation": [
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sussex",
                        "normalized_name": "University of Sussex",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00ayhx656",
                            "GRID": "grid.12082.39"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Brian",
                "family": "Aitken",
                "affiliation": [
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sussex",
                        "normalized_name": "University of Sussex",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00ayhx656",
                            "GRID": "grid.12082.39"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Sheffield",
                        "normalized_name": "University of Sheffield",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/05krs5044",
                            "GRID": "grid.11835.3e"
                        }
                    },
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " s Early Modern Manuscripts Online and Shakespeare's World, The Recipes Project or Bess of Hardwick's Letters. Transcriptions in DEx are undertaken by a small community of scholars:this paper explains how our community currently works and the future collaborations we hope to undertake, as well as the possible avenues for extending the project after its forthcoming full launch.In 2010, Paul Conway argued that \"We are at the end of 'boutique' digital scanning projects for which the principal goal is … extraordinary attention to the unique properties of each artifact\" (76). This paper contends that with early modern manuscripts, \"boutique\" projects are one of the best ways forward. Compared to massive manuscript digitization projects like British Literary Manuscripts Online, DEx is a \"boutique\" project actually make texts searchable with transcription, which is always the result of paying attention to each manuscript as a \"unique artifact.\" This paper discusses the challenges that come with curating a boutique project and the ultimate benefits of having a small site that emerges from a specific set of research questions.Although small digitization or transcription projects can open up a vast field of research, they need to be findable and peer-reviewed in order to do so. I examine the obstacles to having DEx published by a traditional publisher, while questioning how to define publication for digital projects and the costs associated with creating and maintaining an open access site. Furthermore, I discuss how digital publishing must address the peer review needs of emerging scholars and provide an imprimatur and guarantee of quality for users. The final section of this paper discuss the role of Iter: Gateway to the Middle Ages and Renaissance and ReKN: Renaissance Knowledge Network in publication and peer review for DEx: A Database of Dramatic Extracts. This is an appropriate short paper for DH 2017 because it discusses a project that is in beta and active development, it engages the larger questions of how and why boutique digital projects can flourish and provide value to humanities scholarship, and it engages the theme of \"Access/Accès\" by focusing on collaboration, public-facing scholarship, and digital humanities publication. The paper focuses on a single case study: DEx: A Database of Dramatic Extracts and its community, which addresses a much-needed gap in scholarship by transcribing manuscripts that tell us what Shakespeare's audience and readers actually read.  ",
        "article_title": "Making Manuscripts Searchable: DEx, a Database of Dramatic Extracts, Digital Publication, and Boutique Projects",
        "authors": [
            {
                "given": "Laura",
                "family": "Estill",
                "affiliation": [
                    {
                        "original_name": "Texas A&M University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "SUNY Stony Brook, United States of America Our project involves developing a new kind of digital resource to capture the history of research at scientific facilities in the era of the \"New Big Science.\" The extent, scope, and diversity of research at such facilities makes keeping track of it difficult to compile using traditional historical methods and linear narratives; there are too many overlapping and bifurcating threads. In this talk, we will discuss existing methods of data collection and curation for a specific case project , the National Synchrotron Light Source Digital Archive. We are especially interested in the functional potential, in the context of this kind of tool development , of the humanistic concepts of narrative, metaphor , and performance.",
        "article_title": "Designing Tools for Macro- Scale Data Analysis in the History of Science",
        "authors": [
            {
                "given": "Elyse",
                "family": "Graham",
                "affiliation": [
                    {
                        "original_name": "SUNY Stony Brook",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Robert ",
                "family": "Crease",
                "affiliation": [
                    {
                        "original_name": "SUNY Stony Brook",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionAs the Digital Humanities gains access to a wide array of digitized corpora and matures to a discipline that creatively defines new methods for computationally close and distant readings, a growing gap has emerged between those who apply sophisticated programming, e.g., Stylo In R ( Eder et al., 2016) and those who are new to the game and need an introduction to the field. Typical of the community spirit in DH, significant efforts are underway to bridge this gap, including web-based tools for entry-level exploration including Voyant Tools (Sinclair and Rockwell, 2016) and Lexos (Kleinman et al., 2016) and domain-specific introductions to programming, including Jockers' text (2014) and the Programming Historian ( Crymble et al., 2016). This paper attempts to narrow the gap by encouraging both sides to document their experimental methods more fully to embrace previous calls for the replication of experimental methods (Rudman, 2012 et al.) and thereby teach effective practices by \"leaving a trail\" of experimental methods that enable others to execute and extend. A Good Mystery: Towards ReproducibilityA GitHub repository or \"repo\" offers a workflow that explores whether an 1831 story published under the attribution of only 'P' might have been written by Edgar Allan Poe. If so, it would be Poe's first published work. In addition to sharing a set of analytical methods applied in this experiment, the broader methodological-pedagogical goals are two-fold: (i) the dissemination of data and code should be championed as a cornerstone of DH research, thereby facilitating the replication of results and (ii) to share a workflow so that others may apply similar analyses to their texts of interest.The workflow is stored as a set of numbered folders containing the texts and scripts (code) needed to complete each step. The workflow includes: collecting texts, the preprocessing, tokenization, and culling decisions made, unsupervised cluster analyses (k-means, hierarchical-agglomerative, bootstrap consensus tree), and supervised classification methods using Stylo in R's Delta, SVM, and NSC models. Each step represents scaffolding for a \"teachable moment\" with materials provided so faculty can more easily use them with students. Scrubbing, Tokenization, Cutting, and CullingLexos, a web-based, open-source workflow of tools (Kleinman, et al., 2016) was used to upload texts and \"scrub\" them by applying the following options: (i) convert words to lowercase, (ii) all punctuation was removed, (iii) however, a single word-internal hyphen and word-internal apostrophes were kept, and (iv) all digits were removed. Each individual word is considered as its own token. Larger stories were segmented (\"cut\") into pieces. We experimented with various culling options, e.g., keeping only the most frequent words that appear in each text at least once. Cluster AnalysisAs a set of initial probes, we compared the contested story \"A Dream\" to (i) other stories attributed to Poe and (ii) mixed in with stories by other contemporaries. In the repo, we share four variations using cluster analysis:1. K-means clustering on only Poe's stories (using Lexos) 2. Hierarchical agglomerative clustering on only Poe's stories (uses a Python sklearn module and a script to convert the cluster to ETE and Newick formats) 3. K-means clustering when all stories by each author are concatenated together (Lexos) 4. Bootstrap Consensus Tree (using Stylo in R).The result from the Bootstrap Consensus Tree is shown in Figure 1. Of interest is that each author's stories cluster consistently together (with the exception that Bird's initial section of \"Sheppard Lee\" and his \"Calavar\" are found in different clades, at six and eight o'clock). \"A Dream\" clusters with the smaller Poe texts. As you'll see, we couldn't resist tossing in the four stories sometimes attributed to Edgar's brother Henry (\"Monte Video\", \"A Fragment\", \"The Pirate\", and \"Recollections\"). These four stories are found within the cluster of Poe's known works (c.f. Collins, 2013).A series of cluster analyses often serves well as a preliminary exploration, especially for scholars who are new to this game. Some of the file sizes are very small (e.g., one-half of the Poe stories in this corpus have fewer than 2000 words) and when strict culling is enforced (top-N words that appear at least once in each segment), the available set of words is reduced to only 38 when dealing with \"A Dream\" and the other eighteen Poe stories. That noted, these exploratory investigations shed some light on why some scholars consider that Poe's \"first published tale may have been 'A Dream'\" (Silverman, 1991, p87). showing \"A Dream\" consistently clustering with other Poe stories. The BCT aggregates results over multiple cluster analyses and shows those texts that satisfy a consensus number of the individual trials. Using 12 different authors and at least two texts by each author for a total of 46 stories, Stylo formed clusters of the texts for the following frequency bands when using the most-frequent words: 100 to 1000 MFW. ClassificationThree classification models differentiated authorial writing style as implemented in Stylo in R. We scripted in R alongside Stylo to test \"A Dream\" over N-trials (N=10, 100) using a random selection of files for training sets in each trial. At least one text from each author is also included in the test set for each trial. A followup Python script parses the collected results to build confusion matrices for each author to provide metrics on how well the models predict each author's works. The most-frequently occurring, top-40 words (MFW, 1-grams) that appear in all the texts at least once were used. Table 1: Attributions of the contested story \"A Dream\" over ten (10) trials with \"A Dream\" and another randomly selected Poe story in the test set in every trial. Confusion matrix values for results of testing Poe texts over all trials provide overall measures of model effectiveness. In the three cases where \"A Dream\" was attributed to a different author, Poe was ranked second. SummaryWe offer a start to an exploration to collect evidence as to whether Poe may have written the 1831 story \"A Dream\" (c.f., Schö berlein (2016) who used the most frequent character 3-grams and attributed the story to Poe using Delta, but not so when using NSC nor SVM models). Evidence and methods aside, a GitHub repo provides a framework to share experimental workflows in a spirit similar to Jupyter notebooks, as well as one that facilitates both reproducible results and opportunities for subsequent contributions. NotesForming an appropriate corpus is hard: thanks to Sam Coale, Ryan Cordell, Cary Gouldin, David Hoover, Shirrel Rhoades, and Ted Underwood. Four undergraduates: Weiqi Feng, Alec Horwitz, Jingxian Liu, and Khaled Sharafaddin worked with us on this problem. Thanks to Maciej Eder for his help with Stylo in R.",
        "article_title": "Toward Reproducibility in DH Experiments: A Case Study in Search of Edgar Allan Poe's First Published Work",
        "authors": [
            {
                "given": "Mark",
                "family": "Leblanc",
                "affiliation": [
                    {
                        "original_name": "Wheaton College",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Design considerationsEarly POS taggers used morphological and other rules to assign POS tags to input; later experience showed that purely statistical methods like hidden Markov models (HMMs) could achieve better accuracy with less effort; for tutorial descriptions of HMMs see Rabiner 1989 andCharniak 1993. For batch-mode POS tagging, accuracy and speed are obvious desiderata. Many modifications, refinements, and alternatives to HMMs have been proposed; these can improve accuracy by several percentage points. Larger training sets make a much larger difference. Schmid 1994 reports a comparison in which the least and most accurate taggers differ by two to four percentage points, while accuracy rates for small and large training sets (< 10,000 and > 1,000,000 words) differ by twelve to sixteen points.For Hapax, intended to support human annotators working on under-resourced languages, raw speed is unimportant. For any tagger, the human annotator will need to correct many proposed taggings; the key to improving annotation speed is to make corrections faster.Selecting the correct tag from a menu requires several interactions: the 80 tags in the Brown Corpus POS tag set do not fit into a single menu; many tag sets are larger. Accepting a proposed tagging for a word requires a single user-interface interaction (e.g. clicking \"OK\").So speed improves with accuracy: the fastest corrections are those not needed. But high accuracy requires large training sets, which under-resourced languages lack by definition. Some algorithms cope well with limited data. In the Brown Corpus, 92% of all tokens are tagged with the most frequent POS tag for their word type. A trivial 1-gram tagger, which just assigns the most frequent POS tag for each word form, will thus do almost as well on known words as more sophisticated algorithms. In reality, not all words are known, but a 1-gram tagger trained on as little as 2000 words from the Brown Corpus will tag 60 to 70% of input tokens correctly. Larger training sets (8000, 32000, 128000, 500000 words) again do better (68-78%, 73-85%, 77-90%, 82-92%).Also, we can make tagging errors less costly to fix. If the tagger provides one tag for each segment, every wrong guess costs a manual tag selection. If the tagger proposes several POS tags, then some errors will be as cheap as a correct tagging: one mouse-click. So the goal of Hapax's design is to minimize the need to select tags from menus, by proposing not one but several POS tags for each word.If a 1-gram tagger for the Brown Corpus proposes not one but three POS tags, the correct tag will be among those proposed 71-80%, 79-86%, 84-93%, 87-97%, or 90-98% of the time (for 2000-, 8000-, 32000-, 128000, 500000-word training sets). If five tags are proposed, the correct tag will be proposed 79-88%, 87-92%, 91-95%, 92-97%, or 94-98% of the time.If a single user interaction can accept a proposed tagging for the entire sentence, we will save one interaction for each word of the sentence. Hapax uses a standard bigram HMM to calculate the N most likely taggings for the entire sentence. The higher N is set, the greater the chances that only a single mouseclick will be required, but more time will be needed for reading and considering the proposals; it is likely that there is a point of diminishing returns. XQuery implementationHapax is implemented as a library of XQuery functions. One set of functions reads the training material and produces XML word- or POS-frequency lists from them. These list word types or POS tags by frequency, subdivided by POS tags or word types (or, for bigrams, POS of following segment). Additional functions calculate probability distributions for use with unknown words, using the technique of Charniak et al. 1993.The 1-gram tagger consults the word/POS frequency list and returns the N most likely POS tags for the given word form. The bigram tagger consults the bigram and POS/word lists and uses the standard Viterbi algorithm to calculate the most likely path through the trellis of possible taggings for a sentence. A simple modification of the algorithm allows Hapax to calculate not one path but the best N paths, with time linear in the number of tags in the trellis.Testing routines generate random test and training sets from a corpus stored as an XQuery database; in a project setting, the training sets are not created on the fly but prepared in advance and stored in a database.The primary interface for consumers of the Hapax library is the function hapax:tag(), which accepts as arguments:• An XML element representing a sentence• An indication of what frequency data to use • Optionally, a set of access functions The function calls the 1-gram and bigram taggers and returns an XML document describing possible POS taggings for the input. In the common case, the input sentence is a tei:s element, containing tei:w or tei:m elements to be tagged. Input elements may have type attributes; such a partial tagging of the sentence will affect the probabilities for the POS tags for other elements. The optional set of access functions allows Hapax to be used with non-TEI markup; the user-supplied functions are used to identify words in a sentence, detect POS tagging in the input, and add POS tags to the output.The entire Hapax library is a few thousand lines of XQuery; the rich sets of data structures (including XML as a native type), higher-order functions, and grouping constructs in XQuery and XSLT make the implementation of POS-tagging algorithms remarkably straightforward. XForms interfaceIn the ATMO project, Hapax supports a browserbased user interface specified with XForms. The form displays a document, providing an Annotate button for each sentence. When the button fires, the form sends the sentence to the Hapax back end and uses the response to build a form for accepting or changing the annotation. The most likely taggings for the sentence are shown, each with an Accept button. A \"Tag wordby-word\" button is also shown; in word-by-word annotation, each segment in the sentence is displayed with several proposed tags: first those in the full-sentence taggings, then other common tags for the word type, and a worst-case \"Tag manually\" button which exposes the POS menus. The user can tag one or more words and activate a \"Re-annotate\" button, which resubmits the sentence to the back end. This allows the user to explore the effect of one POS assignment on POS probabilities for nearby words.Within the ATMO project, data must also be segmented and spelling-regularized; those topics and their interaction with POS tagging are not discussed here. Further workHapax v1 uses standard 1-and 2-gram HMMs for POS tagging (Charniak et al. 1993). Future versions should implement Schmid's binary-decision-tree method (1994,1995), which helps with sparse data. More challenging will be adapting the directed-graph model of Xuehelaiti et al. (2013) to probabilistic POS  ",
        "article_title": "Hapax: Probabilistic part-of-speech tagging in XQuery and XForms Many programs perform part-of-speech (POS) tagging on texts [Leech et al",
        "authors": [
            {
                "given": "C",
                "family": "Sperberg-Mcqueen",
                "affiliation": [
                    {
                        "original_name": "Technologies LLC",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Black",
                "family": "Mesa",
                "affiliation": [
                    {
                        "original_name": "Technologies LLC",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionEmergent programs like those associated with the Praxis Network have redefined the possibilities for digital humanities training by offering models for project-based pedagogy. These efforts provide innovative institutional frameworks for building up and sharing digital skills, but they primarily focus on graduate education. The long-term commitments that they require can make them difficult to adapt for the professional development of other librarians, staff, and faculty collaborators. While members of these groups might share deep interests in undertaking such programs themselves, their institutional commitments often prevent them from committing the time to such professional development, particularly if the outcomes are not immediately legible for their own structures of reporting.My talk argues that we can make such praxis programs viable for broader communities by expanding the range of their potential outcomes. This talk explores the potential for collaborative writing projects to develop individual skillsets and, by extension, the capacity of digital humanities programs. While the example here focuses on a coursebook written for an undergraduate audience, I believe the model and set of pedagogical issues can be extrapolated to other circumstances. By considering writing projects as potential opportunities for project-based development, I argue that we can produce professionally legible outcomes that both serve institutional priorities and prove useful beyond local contexts. Case StudyThe particular case study for this talk will be an open coursebook written for a course on digital text analysis ( Walsh and Horowitz, 2016). In the fall of 2015, Professor Sarah Horowitz, a colleague in the history department at Washington and Lee University, approached the University Library with an interest in digital text analysis and a desire to incorporate these methods in her upcoming class. As the Mellon Digital Humanities Fellow working in the University Library, I was asked to support Professor Horowitz's requests because of my own background working with and teaching text analysis. Professor Horowitz and I conceived of writing the coursebook as a means by which the Library could meet her needs while also building the capacity of the University's digital humanities resources. Our model in this regard was as an initiative undertaken by the Digital Fellows at the CUNY Graduate Center, where their Graduate Fellows produce documentation and shared digital resources for the wider community. We aimed to expand upon their example, however, by making collaborative writing a centerpiece of our pedagogical experiment. Through her involvement in the the creation of the course materials, Professor Horowitz engaged with a variety of technologies: Markdown, Git, and GitHub. The process also required synthesis of both text analysis techniques and disciplinary material relevant to a course in nineteenth-century history. As a result of our initial collaboration in writing the materials and teaching the course, Professor Horowitz is prepared to offer the course herself in the future without the support of the library. In addition, we now possess course materials that could, after careful structuring and selection of platforms, be reusable in other courses at our own institution and beyond.This type of writing collaboration can fit the professional needs of people in a variety of spaces in the university. Course preparation, for example, often takes place behind the scenes and away from the eyes of students and other scholars. With a little effort, the hidden labor of teaching can be transformed into openly available resources capable of being remixed into other contexts. As Shawn Graham (2016) has illustrated through his own resources for a class on Crafting Digital History, course materials can be effectively leveraged to serve a wider good in ways that still parse in a professional context. In our case, the collaboration produced public-facing web writing in the form of an open educational resource. The history department regarded the project as a success for its potential to bring new courses, skills, and students into the major as a result of Professor Horowitz's training. The University Library valued the collaboration for its production of open access materials, development of faculty skills, and exploration of workflows and platforms for faculty collaboration. We documented and managed the writing process in a GitHub repository. This versioned workflow was key to our conception of the project, as we hoped to structure the project in such a way that others could copy down and spin up their own versions of the course materials for their own needs. We were careful to compartmentalize the lessons according to their focus on theory, application, or course exercises, and we provided documentation to walk readers through the technical process of adapting the book to reflect their own disciplinary content. Implications for DH PraxisMy talk argues that writing projects like this one provide spaces for shared learning experiences that position student and teacher as equals. By writing in public and asking students and faculty collaborators to discuss, produce, and revise open educational resources, we can break down distinctions between writer and audience, teacher and student, programmer and non-programmer. In this spirit, work by Robin DeRosa (2016) with the Open Anthology of Earlier American Literature and Cathy Davidson with HASTAC has shown that students can make productive contributions to digital humanities research at the same time that they learn themselves. These contributions offer a more intimate form of pedagogy - a more caring and inviting form of building that can draw newcomers into the field by way of non-hierarchical peer mentoring. It is no secret that academia contains \"severe power imbalances\" that adversely affect teaching and the lives of instructors, students, and peers (McGill, 2016). I see collaborative writing as helping to create shared spaces of exploration that work against such structures of power. They can help to generate what Bethany Nowviskie (2016) has recently advocated as a turn away from the Kantian ideal of an isolated, reasoning self and towards, instead, a \"feminist ethics of care\" to \"illuminate the relationships of small components, one to another, within great systems.\" By writing together, teams engage in what Nowviskie (2011) calls the \"perpetual peer review\" of collaborative work. Through conversations about ethical collaboration and shared credit early in the process, we can privilege the voice of the learner as a valued contributor to a wider community of practitioners even before they might know the technical details of the tools or skills under discussion.Collaborative writing projects can thus serve as training in digital humanities praxis: they can help introduce the skills, tools, and theories associated with the field, and projects like ours do so in public. Productive failure in this space has long been a hallmark of work in the digital humanities, so much so that \"Failure\" was listed as a keyword in the new anthology Digital Pedagogy in the Humanities ( Croxall and Wernick, 2016). Writing in public carries many of the same rewards - and risks. While a certain comfort with frustration can help one learn digital methods (Ramsay, 2016) not everyone is comfortable with what Stephen Ramsay (2014) describes as a \"hermeneutics of screwing around.\" Many of those new to digital work, in particular, rightfully fear putting their work online before it is published. I argue that the clearest way in which we can invite people into the rewards of public digital work is by sharing the burdens and risks of such work. In her recent work on generous thinking, Kathleen Fitzpatrick (2016) advocates \"rooting the humanities in generosity, and in particular in the practices of thinking with rather than reflexively against both the people and the materials with which we work\". By framing digital humanities praxis first and foremost as an activity whose successes and failures are shared, we can lower the stakes for newcomers. Centering this approach to digital humanities pedagogy in the practice of writing productively displaces the very digital tools and methodologies that it is meant to teach. Even if the ultimate goal is to develop a firm grounding in a particular digital topic, focusing on the writing invites students and collaborators into a space where anyone can contribute. By privileging soft rather than technical skills as the means of engagement and ultimate outcome, we can shape a more inviting and generous introduction to digital humanities praxis. ",
        "article_title": "Collaborative Writing to Build Digital Humanities Praxis",
        "authors": [
            {
                "given": "Brandon",
                "family": "Walsh",
                "affiliation": [
                    {
                        "original_name": "University of Virginia",
                        "normalized_name": "University of Virginia",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0153tk833",
                            "GRID": "grid.27755.32"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Neutralizing Author Signal in DeltaOur proposal is to neutralize the author signal directly on the Delta matrix. We use a testing corpus of texts from three Spanish authors and three subgenres. Detailed information about the corpora, files, parameters and scripts is in our GitHub repository. We applied Cosine Delta (5000 MFW) with Stylo (Eder, Rybicki and Kestemont 2016) and visualized the resulting distance matrix with Python:  We see a tendency of lower Delta values for documents of the same author (below 1.0) in comparison to documents of different authors (above 1.0). But what about the closest texts written by a different author? For the historical novel in column E, they are in the rows 14 and 15 and are historical novels, as well. This pattern is found for the majority of the texts. How could we cluster the texts preferring the closest text from other authors? And if we are able to neutralize the author signal, will we see noise or subgenre clusters?Our proposal is to penalize the distances between the texts of the same author (cf. Lu and Leen 2007 for penalization in image clustering), making them closer to the average distance of texts of different authors, then cluster the neutralized distance matrix and measure the cluster homogeneity by author and subgenre.We define the set of all documents by an author a as Aa, the collection containing all documents by all authors as C and total number of documents in the collection is defined as c:Note that each document is in exactly one author-document set Ai.First, we calculate the average distance of texts of all pairwise different authors (in fig. 2, all the distances in black). We call this value the mean of different authors or M(C) and for this collection its value is 1.16.Second, we calculate the mean of the texts of each author a M (Aa) (in fig. 2, the distances in grey).  The values in grey are now in general above 1.0: the texts of the same author have been separated, showing relations between texts independently of authorship. Now the adventure and historical novels of Baroja in columns C and D have their closest text in works of different authors but belonging to the same subgenre. In comparison with Figure 1, this dendrogram allows us to see new text relations beyond authorship but within subgenre, showing clusters with different authors but the same subgenre: for example, the cluster of historical novels by Baroja and Valle or the two very close subclusters of erotic novels by Miró and Valle. Tests and EvaluationFor the evaluation, the homogeneity of the clusters (Rosenberg and Hirschberg, 2007) was measured. This measure yields values between 0 and 1. As ground truth, the metadata about author and subgenre have been used. The results for the dummy corpus: The homogeneity of the clusters of Cosine Delta (see fig. 1) are perfect for authors and much lower for subgenre, because the author clusters contain subgenre subclusters. The homogeneity of the clusters of Neutralized Delta (see fig. 5) is lower for authorship (as expected), but not for subgenre. In this case the neutralization of the author signal only deteriorates the homogeneity for authorship but improves the homogeneity for subgenre. We have analysed different subgenres present in the whole corpus for test the method. We created subcorpora of historical, bildungsroman, erotic and adventure novels: 5 Figure 7: Homogeneities for Spanish prose subcorporaAs expected, the neutralization consistently deteriorates the homogeneity for author (between -0.26 and -0.1) while the homogeneity for subgenre is not deteriorated (between -0.08 and 0.06). The homogeneity for subgenre of adventure compared to erotic and bildungsroman get the best results (over 0.9) and they even improved on results with Cosine. Adventure novels are also best recognized in classification tasks (Het- tinger et al. 2016). Subgenres which are very difficult to differentiate like historical and adventure (Pedraza Rodríguez Cáceres 1983: 672 and1987: 459) get one of the worst results.The results are similar when testing other corpora, such as a corpus of French drama ( Schöch et al. 2015) and a corpus of Spanish American novels: Figure 8: Homogeneity values for French drama and Spanish American novels Conclusion and future workOur main goal was to present a method to neutralize the Delta distances of the same author using the difference between the mean of the author and the mean of different authors. Tested on eight subcorpora, this procedure, as we expected, deteriorates the homogeneity of authorship clusters but maintains the subgenre homogeneity, improving it for some cases. That discovers relations between texts (see fig. 5) that were hidden by authorship. This procedure brings a new way of working with Delta beyond authorship attribution.Both Cosine and Neutralized Delta show very different results for the comparison of different subgenres, something which points to the different internal structure of the subgenres. The comparison of very different subgenres (like adventure against erotic or bildungsroman) gets higher subgenre cluster homogeneity. Neutralized Delta could be used for comparing different corpora of specific subgenres and test the significance of the results to better characterize these subgenres. In an ideal scenario, we would like to test on a perfect balanced corpus where a set of authors are represented in all subgenres of the same period.For future work, we will analyse how different parameters like versions of Delta or number of MFW affect the results. We also plan to transfer the approach to an earlier step in the Delta procedure and penalize the word z-score vectors.We look forward to the feedback of the international DH community about this new use of the very effective \"expression of difference, pure difference\" which is Delta.",
        "article_title": "Neutralising the Authorial Signal in Delta by Penalization: Stylometric Clustering of Genre in Span- ish Novels",
        "authors": [
            {
                "given": "José",
                "family": "Tello",
                "affiliation": [
                    {
                        "original_name": "University of Würzburg",
                        "normalized_name": "University of Würzburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00fbnyb24",
                            "GRID": "grid.8379.5"
                        }
                    }
                ]
            },
            {
                "given": "Daniel",
                "family": "Schlör",
                "affiliation": [
                    {
                        "original_name": "University of Würzburg",
                        "normalized_name": "University of Würzburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00fbnyb24",
                            "GRID": "grid.8379.5"
                        }
                    }
                ]
            },
            {
                "given": "Ulrike",
                "family": "Henny",
                "affiliation": [
                    {
                        "original_name": "University of Würzburg",
                        "normalized_name": "University of Würzburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00fbnyb24",
                            "GRID": "grid.8379.5"
                        }
                    }
                ]
            },
            {
                "given": "Christof",
                "family": "Schöch",
                "affiliation": [
                    {
                        "original_name": "University of Würzburg",
                        "normalized_name": "University of Würzburg",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00fbnyb24",
                            "GRID": "grid.8379.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " MotivationHow does mass media affect the way we think about controversial topics such as the \"Arab Spring\"? What persuasive role do metaphors play especially in opinion pieces?During the events of the years 2010-2011 in the Middle East & North Africa region a new discourse was established in the German media; immediately these events were assessed as a \"wave\" of democratization and liberation, and have been metaphorically labeled \"Arab Spring\". Metaphors were frequently used to categorize and understand these events (Möller, 2014; Núñez, 2014).Given the premise that mass media organizes (Couldry, 2010) and shapes social reality (Luhmann, 1996), we analyze how the Arab Spring is categorized and assessed using metaphorical constructions in newspaper opinion pieces. We show ways in which particularly the use of metaphors reveals how the media tried to achieve acceptance for the events based on our cultural models (Quinn and Holland, 1987), which are grounded on our western knowledge.According to the Conceptual Metaphor Theory ( Lakoff and Johnson, 1980) metaphors are ubiquitous and exhibit a binary source-target domain structure. The knowledge that we choose to function as a source domain illustrates which conventionalized, overt or tacit knowledge we require to understand new or abstract domains (target domains) in terms of our cultural imprints. Metaphors are instantiated on the text surface and give us clues toward our knowledge basis. Thus, the required knowledge can be described in terms of ubiquitous metaphorical patterns that function as semantic \"anchors\" in texts, and in terms of conceptual knowledge clusters that function as an intertextual semantic knowledge structure.As such, we constructed a pipeline that automatically detects metaphors appearing within certain grammatical constructions, before clustering them by presumed source and target domains. The results give us insights into how the Arab Spring is metaphorically structured by semantic clusters in opinion pieces. Corpus and annotationOur corpus consists of 300 opinion pieces (Ramge and Schuster, 2001) from five German newspapers, Frankfurter Rundschau, Die Zeit, Der Spiegel, taz, and Die Welt, which have been written between December 2010 and November 2011 and cover the Arab Spring.In nine of these opinion pieces, two of this abstract's authors annotated following grammatical constructions whether they constitute metaphors: adjective-noun (AN) pairs (e.g. \"Tunisian spark\"), and genitive constructions (GEN) (e.g. \"torch of freedom\"). Due to their interrelated components they provide a good insight into the structural systematicity of metaphorical mappings (source domain → target domain). The difficulty of the task is reflected in a low inter-annotator agreement (0.45 Krippendorff's α).Common sources of annotation disagreement included heavily conventionalized metaphors such as \"social network\", personifications like \"self-consciousness of a generation\", and metaphors that need a larger context to function. As gold standard for further training and evaluation we only use the agreed upon annotations (\"annotated\", Table 1). Technical realizationTo examine our questions quantitatively, we contrast two approaches to automatically detect metaphors, namely random forests and multilayer-perceptron. The extracted metaphors are subsequently clustered ( Figure 1). To extract AN and GEN constructions we first perform automatic preprocessing, including part-of-speech tagging and dependency parsing.The random forests approach of Tsvetkov (2014) firmly roots in conceptual metaphor theory, mainly employing features extracted from manually crafted resources such as an abstractness wordlist and supersenses, to classify adjective-noun and subject-verb-object constructions. For use on other languages than English, a bilingual dictionary is required. We manually expand an existing dictionary to cover our corpus, and extend their system to classify GEN metaphors.The described feature-rich approach will be compared - with regards to what (kind of) metaphors can be found - to the shallow neural network approach by Do Dinh and Gurevych (2016), which does not presuppose any specific metaphor theory. It thus does not make use of external features, but rather learns exclusively from given annotations and their context. Preliminary experiments show that more training data is needed for this bottom-up approach.To gain further insight into usage of metaphor in our corpus, we cluster the automatically found metaphors - resp. their components - into coarse-grained semantic fields. While there are works using a theorysupported top-down approach, e.g. using source domain lists ( Gordon et al., 2015), we employ a rather unsupervised approach, without preselecting the number of clusters or manually fixing cluster centers (similar to Shutova et al. (2010), who use spectral clustering for metaphor detection). To that end, we employ Affinity Propagation ( Frey and Dueck, 2007), which we supply with cosine similarities between pretrained word embeddings ( Reimers et al., 2014) of the metaphor components. Experiments and discussionWe use cross-validation for the intrinsic evaluation of the metaphor detection part. For GEN metaphors, the tested system achieves 0.63 precision, 0.25 recall, and 0.35 F1-score, with similar performance for AN metaphors. While these results seem low, the actual output of the system when trained on all annotated instances looks promising, and the precision is improved by filtering based on named entities.The clustering creates an impression of which knowledge (source domain) is required for abstract concepts (target domains), and how abstract concepts are \"perspectivized\" in the corpus, while also giving an overview of occurring intertextual metaphors. Although the cluster assignment and the metaphor detection leave room for improvement (e.g. Figure 2: \"face of her son\"), the clusters already reveal the systematicity and constraints of metaphorical mappings. Thus, they point to strategies of newspapers that come along with the choice of the (conceptual) source domain.In Figure 2, bodily parts such as face, head, hand are used as source domains and mapped to political systems or processes (e.g. regime, democracy, revolution). This mapping draws on a long tradition in political and philosophical history (Musolff, 2004): head and face play a central role in our culture - comparing political processes with faces or heads conceptualize them as human beings. In this cluster the construction face of indicates that the events are important, thus construed as worthy to support.Those prototypical examples for ontological metaphors also support the premise of embodied cognition (Johnson, 1987;Rohrer, 2010). The positive properties and the movement character of natural elements such as wind and storm are mapped to the abstract (political) nouns freedom, revolution, or change. They receive a deontic (Hermanns, 1994) character, whereas dictatorship is conceptualized in terms of island which stands for inertia and stability (Figure 3). These examples already show how the chosen metaphors shape dualistic tendencies by categorizing the events on one hand as a dynamic movement (wind, storm) that has to be supported by western democracies, or on the other hand pleading for stability (island), thus implicitly supporting dictatorship.The analyzed clusters and metaphorical conceptualizations indicate a network of source domains that function as key concepts which structure the discourse of the Arab Spring, an assumption we will focus on in future work. Conclusion and future workOur study indicates that metaphorical constructions are important in media because of their ubiquitous use in opinion pieces. The generic extracted source domains already suggest that a specific network of knowledge is used in media to highlight certain political aspects of the Arab Spring. Furthermore, they illustrate how contents are emotionalized and ideologized during these events by metaphors and their framing effects. Usage of natural elements or body parts reduces complexity and conceptualizes the events as an organic development, in short: the Arab states become western democratic states. Thus contributing to the extension of western ideology, metaphors impart implicit cultural values. Combining our cognitive and discourse analytical questions we can summarize that the used \"bottom-up\" clustering is very helpful to get an explorative impression of the \"intertextual consistencies\" (Verschuren, 2012: 179) of chosen metaphors. They are good textual \"anchors\" and starting points to investigate the widespread metaphorical use, and thus knowledge domains, in corpora.With regard to the state of the art, corpus-based methodologies within the Digital Humanities community will benefit from our research by gaining the possibility to automatically compare thematic corpora by using the relationship of their metaphors to the common main cluster as a metric, therefore obtaining a new way to analyze the conceptual network being used. Our approach can help to facilitate corpus studies, e.g. by analyzing other discourse segments which deal with the implicit construction of identity and alterity within opinion pieces by using metaphors.In our presentation we will highlight the results and give a structured impression of the mappings and the implications of the used metaphors in our corpus and present in short our methodological basis.In future studies we will compare the conceptualization strategies of the Arab Spring and \"Refugee Crisis\" in German media, since we assume that the same metaphors and the same (metaphorical) interpretation patterns occur. Further, we plan to investigate another theory of metaphor which is based on Black (1954,1977) and Gehring (2010). The latter model is strongly interweaved with current discussions about \"Begriff\" (Müller-Meiningen and Schmieder, 2016;Gehring, 2005Gehring, , 2010 and discusses its ideological implication(s). Furthermore, the emphasis is placed on the function of metaphors as an epistemological tool by investigating, amongst others, the evolution of ideas and cultural values, e.g. in the historical text collection \"Natur&Staat\" . ",
        "article_title": "A \"Wind of Change\": Shaping Public Opinion of the \"Arab Spring\" Using Metaphors Erik-Lân Do Dinh",
        "authors": [
            {
                "given": "Alexandra",
                "family": "Núñez",
                "affiliation": [
                    {
                        "original_name": "Institut für Sprach- und Literaturwissenschaft - Technische Universität Darmstadt (Technical University of Darmstadt)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Malte ",
                "family": "Gerloff",
                "affiliation": [
                    {
                        "original_name": "Institut für Philosophie - Technische Universität Darmstadt (Technical University of Darmstadt)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Erik-Lân ",
                "family": "Do Dinh ",
                "affiliation": [
                    {
                        "original_name": "UKP Lab - Technische Universität Darmstadt (Technical University of Darmstadt)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Andrea",
                "family": "Rapp",
                "affiliation": [
                    {
                        "original_name": "Institut für Sprach- und Literaturwissenschaft - Technische Universität Darmstadt (Technical University of Darmstadt)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Petra",
                "family": "Gehring",
                "affiliation": [
                    {
                        "original_name": "Institut für Philosophie - Technische Universität Darmstadt (Technical University of Darmstadt)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Iryna ",
                "family": "Gurevych",
                "affiliation": [
                    {
                        "original_name": "UKP Lab - Technische Universität Darmstadt (Technical University of Darmstadt)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Oral histories and audio-files of interviewswith surviving punched-card operators, Busa's secretaries, and others The overall objective is to model this important early research center and its activities through a series of purpose-driven and interlinked emulations, 3D spaces, oral histories, and digitized documents and artifacts. We employ metadata to map archival materials and emulations onto the models in order to understand the material history of what is usually taken to be the first humanities computing center. In the process, we complicate the key terms themselves, including first and computing.)Most of what has been known to date about Busa's early literary data processing was derived from a handful of his own publications-first by Winter (1999); later, Jones (2016) drew on the Busa Archive to contextualize and extend his narrative account. Rockwell and Sinclair (2014) and Terras and Nyhan (2016), have continued to clarify the story in different ways. Actually modeling the machinery and workflow allows us to address specific questions about this important moment in the birth of linguistic data processing and humanities computing, such as:• What were the precise roles played by human operators between the automated stages, sorting card decks, lemmatizing word lists, programming machines via plugboards, etc.? (How were these roles stratified and gendered?)• What source texts were used for input and how were they prepared and marked up so that the operators could use them as the basis for what they punched on the cards?• At what stage did IBM agree to print customized punched cards with what amounted to data fields unique to Busa's projects? What was the nature of the data ontology behind these customizations?• What is the evidence that the work of Busa's center contributed to larger technology developments at IBM, such as Peter Luhn's development of the influential KWIC (keyword in context) protocol for information retrieval? Additional questions will surely arise during the ongoing process of modeling and cross-checking archival materials and oral histories.Although Busa's humanities computing center is our focus, we believe this methodological approach would be useful in other instances, as a way to conceive of digitization as a process of modeling artifacts and documents in relation to technology and infrastructure. We draw on theoretical approaches and methods associated with media archaeology (Parikka, 2012;Emerson, 2014;Rock- well and Sinclair, 2014;Sinclair, 2016), creative historical prototyping (U Victoria Maker Lab; Sayers et al, 2016), the archaeology of science (Haigh, 2016;Schiffer, 2001), and on the methods and expertise of digital archaeology in the field of cultural heritage, including its attention to issues of access and preservation (Koller, 2009;London Charter, 2009).The presentation at DH 2017 will include slides containing selections from the 800 historical photographs of Busa's center, as well as other images, audio files, and demonstrations, including a prototype 3D virtual model of the center. The paper will explain the project's practical aims and theoretical significance: for example, we address current debates in digital humanities about the influence of text-based analysis on today's definitions and practices; or debates about possible alternative genealogies for DH (Klein, 2012). It will also spotlight the role of gendered labor in early humanities computing, and the entanglements of early humanities technology research with corporate and government funding. Our broader methodological purpose is to take up in practice what Jeffrey Schnapp has called the \"defining design challenge of our epoch\"-\"to weave together information and space in a meaningful fashion\" (Schnapp, 2015), and the methods will be of interest and use to others who are approaching multimodal archives and interpolating the information therein.  ",
        "article_title": "Reverse Engineering the First Humanities Computing Center",
        "authors": [
            {
                "given": "Steven",
                "family": "Jones",
                "affiliation": [
                    {
                        "original_name": "University of South Florida",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Julianne",
                "family": "Nyhan",
                "affiliation": [
                    {
                        "original_name": "University College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Geoffrey ",
                "family": "Rockwell",
                "affiliation": [
                    {
                        "original_name": "University of Alberta",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Stéfan",
                "family": "Sinclair",
                "affiliation": [
                    {
                        "original_name": "McGill University",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Melissa ",
                "family": "Terras",
                "affiliation": [
                    {
                        "original_name": "University College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe digital humanities (DH) are perhaps unique amongst humanities endeavours. They force us to confront the conceptual and ethical implications that attend the union of the humanities with engineering and organizational thinking. They demand attention to tools, methods, ethics, and pedagogy, but also organizational bureaucracy, human resource management, economics, and systems maintenance. Rather than merely prompting a bland mechanization of the humanities, as critics suggest, this offers a fascinating epistemological challenge. It challenges us to rethink how human meaning and knowledge are constructed, and how they will be remade as the twenty-first century progresses. This requires a step-change in our epistemic, ethical, and collective assumptions as much as our methods.These issues are distilled in laboratory settings. Sociologist of science Karin Knorr-Cetina (1999) notes that labs function as blended communities, uniting researchers with technicians and administrators. Access to equipment, chemicals, data, and funding, impact the production of knowledge as much as pure research questions. Issues of finance and organizational power compete with creativity and the need for diversity. Scientists have been gaining insight into this since the nineteenth century, fostering traditions (and injustices) that are well understood . (Adas, 1999). Digital humanities labs challenge us to create parallel traditions, appropriate to the humanities and GLAM communities. King's Digital Lab, launched in November 2016, embraces this challenge, positioning itself as an experiment in infrastructural as well as conceptual terms.Postphenomenological perspectives current in the philosophy of technology and Science and Technology Studies (STS) help explain our approach. Writers like Donald Ihde (2009b) and Peter Kroes (Verbeek, 2010) embrace the entanglement of humans with technological tools, systems, and processes, and meditate on the material reality that informs our experience of the world. They reject the Heideggerean critique of technology, based on the modernist division of subject and object, in favour of acceptance that entanglement with culture, technology, and ideology is not only unavoidable but provides a window into the nature of human experience (Ihde 2009a). Rather than mechanizing the soul of the humanities, digital humanities laboratories force us to confront our entanglement with technology, along with its enabling infrastructures and ideologies. King's Digital Lab King's Digital Lab (KDL) builds on a 30-year legacy in digital humanities at King's CollegeLondon. The lab represents one half of a new digital humanities model, in conjunction with the Department of Digital Humanities (DDH). KDL provides software development and infrastructure to departments in the faculties of Arts & Humanities and Social Science and Public Policy, focusing on software engineering and implementing the systems and processes needed to produce high quality digital scholarly outputs. The department focuses on delivering quality teaching to its postgraduate students and growing cohort of undergraduates, and producing research outputs in line with its status as an academic department. In combination KDL and DDH include close to 40 staff, host 160 projects, served 130 million webpage views in 2014, and teach over 200 students.KDL's business, operational, and human resource plans define its research values alongside its business and technological model. It has been established with 12 permanent full-time staff: director; project manager; analysts, software engineer, developers, designers; and systems manager. Contract and temporary staff are used as infrequently as possible, ideally to offer student experience in a software development environment. The HR model is explicitly designed to foster sustainable #alt-ac Research Software Engineering (RSE) career paths. All KDL team members, permanent or contract, are encouraged to use 10% of their time on personal projects (either on their own or in collaboration with colleagues), leading to work with Raspberry PIs, virtual reality, and an interest in maker culture.The KDL model is based on deeply felt humanistic values, but reflects a level of organization required to manage entanglement with technological systems. The lab manages over 90 projects, including up to 20 that are active in some form, and ~5 million digital objects. The team manage over 180 virtual machines, on an infrastructure that uses 400GB of RAM and 27TB of data. New infrastructure platforms are being trialed that include access to cloud and high performance computing options, in a nod towards a future working with big data, visualization, and simulation. The goal is to facilitate a transition from twentieth to twenty-first century modes of computationally intensive humanities and social science research, but to do so in consciously humanistic terms.In a rejection of a simplistically 'mechanized' future, development tools are proactively managed and the lab has a 'design first' philosophy (Verbeek, 2006). This is partly a way to manage the considerable complexities that come with advanced DH research and the delivery and management of multiple projects, but it also recognizes that digital tools and methods are, at their best, beautiful. Aesthetic and quality values can extend from front-end design to technological platforms, code, tools, and methods. Data, similarly, can and should be beautiful, not only in adherence to appropriate technical standards but in its conformance to scholarly best practice and deep domain knowledge. Infrastructure and systems, likewise, are always compromised by their material design and ideology (Russell, 2014), but decisions to choose open source components and emphasize access and sustainability enhances control and agency (Friedman et al, 2015).To reduce complexity and improve sustainability, the lab uses the Python programming language, and Django web framework in preference to other options. The full technology stack is consciously oriented towards open source components, and a balance between functionality and sustainability. Figure 1This level of organization helps us manage technology, but also promotes critical awareness: the current technological state of the lab is far from perfect, but it is under control and guided by known critical values. The concept of the 'laboratory' is important in this context. Rather than mechanization, it implies experimentation and risk, but also a certain intellectual seriousness. Scientists learned what a laboratory means to them over a century ago; the humanities and social sciences are only just starting to explore the implications. They are profound, not only in terms of the epistemological implications of putting tools between the researcher and the object of study (with inevitable technical constraints), but in terms of the ideological implications of using industry approaches to software development and financial management. Consciousness of this ensures the lab is sustainable, and can continue to support scholarship as well as the careers of our team members.The technological inheritance of the lab is considerable. It includes over 90 live web-based projects, built using heterogeneous tools and programming languages by the (historic) Centre for Computing in the Humanities, (historic) Centre for eResearch in the Humanities, and the Department of Digital Humanities. Funding agencies paid for them to be built, but not to sustain them. Some Primary Investigators (PIs) have retired, or are no longer in contact with King's. Support for these projects is currently borne by the lab, generously supported by the Faculty of Arts & Humanities, but is being managed by an evolving archiving and sustainability plan that will assess each of the projects, determine their intellectual merit, and work with their owners to find the best way to maintain or archive them. The archiving and sustainability model used for this task will be published, as well as being included in the lab's Software Development Lifecycle (SDLC), to ensure sustainability will be considered on day one of every new project.The organizational chart of King's Digital Lab is flat rather than hierarchical, reflecting an aspiration to be role-based and collaborative: a shared intellectual and scholarly space that exists to experiment with new approaches as well as deliver projects on time and budget. The scale is such that the lab design has needed to be outsourced to multiple authors: director and project manager working with line management to define the business plan and financial model, analysts and developers developing the software life-cycle, UI developer leading the design vision, systems manager ensuring the infrastructure and networking model is appropriate. Together, it amounts to something complex and technologically dependent, but redeemed through a philosophy of shared ownership and conscious experimentation. ",
        "article_title": "Mechanizing the Humani- ties? King's Digital Lab as Critical Experiment",
        "authors": [
            {
                "given": "Paul",
                "family": "Caton",
                "affiliation": [
                    {
                        "original_name": "King's College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Ginestra",
                "family": "Ferraro",
                "affiliation": [
                    {
                        "original_name": "King's College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Luis",
                "family": "Figueira",
                "affiliation": [
                    {
                        "original_name": "King's College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Elliott",
                "family": "Hall",
                "affiliation": [
                    {
                        "original_name": "King's College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Neil",
                "family": "Jakeman",
                "affiliation": [
                    {
                        "original_name": "King's College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Pam",
                "family": "Mellen",
                "affiliation": [
                    {
                        "original_name": "King's College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Anna-Maria",
                "family": "Sichani",
                "affiliation": [
                    {
                        "original_name": "Huygens Institute for the History of the Netherlands (Huygens ING) - Royal Netherlands Academy of Arts and Sciences (KNAW)",
                        "normalized_name": null,
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "James ",
                "family": "Dakin Smithies",
                "affiliation": [
                    {
                        "original_name": "King's College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Miguel",
                "family": "Vieira",
                "affiliation": [
                    {
                        "original_name": "King's College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Tim",
                "family": "Watts",
                "affiliation": [
                    {
                        "original_name": "King's College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Carina",
                "family": "Westling",
                "affiliation": [
                    {
                        "original_name": "King's College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " MethodologyAs a first step in defining the \"transmedia fan\", the current project undertakes a comparative discourse analysis of online conversations of Game of Thrones fans. One of the most dramatic plot developments in the source material (Martin, 2000) was adapted to the screen in the penultimate episode of the third season, \"The Rains of Castamere\" (Benioff & Weiss, 2013). Readers of the book series had long anticipated and dreaded the events of the \"Red Wedding\", while fans of the show unfamiliar with Martin's narrative were largely taken unawares by the pivotal episode.Since the television series' inception, writers at The AV Club have written two critical reviews for each episode: one for viewers familiar with the books (i.e., \"Experts\") and one for viewers unfamiliar with the books and averse to \"spoilers\" (i.e., \"Newbies\"). What results are two completely separate reviews of \"The Rains of Castamere\" which in turn document the fans' reactions to the episode in the form of user comment threads: one comment thread where fans were expected to be shocked by the outcome of the episode and one comment thread where fans had hotly anticipated it.As a pilot project, the current work takes the content of both comment threads-a corpus of approximately 5,600 comments-and analyzes each thread separately using a qualitative coding method aligned with constructivist grounded theory (Charmaz, 2006). Through this analysis, a categorization of themes emerges illustrating tactics for negotiating intertexts and paratexts unique to each group of fans. These themes fall under two broad categories: sentimental negotiation (i.e., emotional responses) and tactical negotiation (i.e., cognitive, or reasoned responses). A comparison of categories and sub-categories between both groups provides preliminary findings to support an emergent model, or models, of the \"transmedia fan\". ConclusionThe present research represents a first step in exploring the impact of transmedia systems, as exemplified by Game of Thrones, through the study of fans. The question posed by this research is, fundamentally, an examination of how the problem of \"access\" is framed in postdigital society from the perspective of the consumer. Future research should explore the negotiation tactics observed in transmedia fans using the principles of De Certeau's (1984) everyday life practice, in order to extend its application to the broader context of modern-day consumers. The current study will contribute to the development of further qualitative and quantitative research that will more clearly define the information behaviors of the transmedia fan. This project is of relevance to researchers in media studies, fan studies, information studies and digital humanities ",
        "article_title": "Re(a)d Wedding: A Comparative Discourse Analysis of Fan Responses to Game of Thrones",
        "authors": [
            {
                "given": "Eric",
                "family": "Forcier",
                "affiliation": [
                    {
                        "original_name": "School of Information Studies - Charles Sturt University",
                        "normalized_name": null,
                        "country": "Australia",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis paper reports on efforts to integrate cultural critique into a DH project that analyzes a corpus of book-length comics, or graphic narratives. We argue that the analysis of issues such as gender, race, and class should be central to digital scholarship that aims to become accessible to the public and appear relevant to the humanities at large. Therefore, cultural criticism needs to be integrated into digital projects from the very beginning. Our research takes up calls to \"design for difference\" and to develop visualizations that \"enact [the] humanistic properties\" of complexity and contradiction (McPherson and Drucker, 242). Part I looks at the construction of a corpus of graphic novels, memoirs, and non-fiction. Basing our corpus on academic databases, international comics awards, literary histories, and online booksellers provides insight into institutional gender and racial biases, as well as the opportunity to address them. Part II takes up Drucker's criticism of network analysis as reductive and static. We present networks of a pilot corpus that pay attention to social inequality and replace reductive edges with distinct forms of communication. Part II exemplifies our intention to make DH scholarship relevant to a wider public. The broad appeal of comics presents an ideal point of entry for people who might not otherwise be interested in digital research. We apply the popular Bechdel Test (proposed by Alison Bechdel in a comic strip but used mainly to study film and TV), to highlight the male bias of graphic narratives. Corpus Analysis of Institutional Gender and Racial BiasThe traditional canon of literary studies has long been criticized for its exclusion of female, nonWestern, and minority authors. As a much younger field, comics studies lacks the extensive canons and bibliographies produced by literary historians. This does not mean that similar biases are absent, however. As part of a larger project, we have built a monitor corpus of book-length comics by drawing on sources that include academic databases (JSTOR, MLA), international comics awards, literary and cultural histories of comics, news media coverage, and Amazon.com ( Dunst et al.). Of 220 titles included in the corpus by fall 2016, 84 per cent were written by male authors and 73 per cent were identified as white. Biases are unevenly distributed: The absence of reliable bibliographies means that the size, gender and racial make-up of the population remains uncertain. Yet given the considerable differences between sources, institutional biases appear likely. To address these existing biases, two steps were undertaken. A survey sent to comics scholars (five female, five male) asked them to suggest between five and ten graphic narratives written by women that should be included in the corpus. Of a total of 53 suggestions by nine respondents, nine volumes were listed by more than one scholar and 12 had already been included in the corpus, while 14 fell outside of the sampling frame. 16 new works were added, bringing the ratio of female author to slightly less than 22 per cent. The second step includes a comparison of the monitor corpus and collections held at the Library of Congress and the Billy Ireland Cartoon Library at Ohio State University. By checking authors in these collections against a list of names that were assigned genders by the US Social Security Administration, we compare their gender make-ups and will potentially add to our corpus. Gender and Interaction Types in SemiAutomatic NetworksNetwork analysis has steadily grown as an area of research since Franco Moretti's visualization of Shakespeare's Hamlet (Moretti). Scholars have focused on automatic extraction and statistical analysis of data from novels, plays, and intellectual networks (Elson, Dames & McKeown; van de Camp & van den Bosch). Recent efforts include computing main characters and operationalizing dynamic networks (Jannidis et al; Karsdorp & van den Bosch; Xanthos et al). While these networks answer some of Drucker's criticism, the approaches remain reductive. Limiting interactions to undifferentiated edges appears particularly unsatisfying for visual media, in which communication takes visibly different forms: characters may look at and touch each other, or appear together in a panel. Despite recent advances, computer vision has trouble recognizing non-perspectival drawings and applying OCR to handwritten comics fonts remains fraught with difficulty (Dunst et al; Rigaud 2013Rigaud & 2015. As the automatic extraction of network data is some way off for comics, we focus on networks that are semantically enriched via manual annotation to engage with the central questions posed by cultural studies. The following network (Figure 3) of Karasik and Mazzuchelli's City of Glass combines different types of interaction with gender assignations: Figure 3: Interactions and gender assignations in City of GlassThese networks and the SSA name database allow us to study the relation between authors' gender and its fictional representation. Male characters are consistently more central to works by male authors. Notably, female characters show higher betweenness centrality in narratives written by women, as Figure 4 shows.  Semi-Automated Bechdel-Test for GNMLAnnotated Graphic NarrativesEfforts to automate the Bechdel Test have been limited to plays and film scripts and led to poor results (Lawrence; Agarwal et al). Three conditions need to be met to pass the test: 1. At least two female characters appear in the story. 2. There is at least one conversation between women. 3. The conversation is not about a man. Our XML-annotation language GNML, an extension of John Walsh's CBML, allows for automatically checking if graphic narratives fail criteria 1 and 2, and aids evaluation of whether the remaining narratives pass criteria 3. We decided not to rely on error-prone full automation but to use semi-automatic processes that aid human annotators/analyzers in retrieving quality results. GNML annotations contain information on all character occurrences, the gender of a character, and their interactions. As discussed by Agarwal, even sophisticated machine learning approaches lead to unreliable results in deciding whether a conversation centers on a man. Therefore, for criteria 3, we simplify decision-making by providing the annotator with a ranked list of dialogues, based on the number of male names or male personal pronouns per conversation. Significantly, even if a conversation's focus on a male character could be identified automatically, the test would still be errorprone. Conversations may span several panels or pages and automatic separation of these conversations remains difficult. Conclusions and Future ResearchWe present corpus metadata and semanticallyenriched networks of a widely popular but understudied medium that is only beginning to attract attention by DH researchers. These methods are used to analyze gender and racial biases and suggest ways in which DH can appeal to scholars in cultural studies and the wider public. Future work includes expanding pilot studies to cover our entire corpus by integrating advances in OCR and computer vision and thus working towards fully-automatic extraction and analysis. In the meantime, our networks may function as conceptual models exploring how humanistic forms of complexity can be introduced into network analysis. Analyzing and addressing racial biases against minority authors presents hurdles of a different sort. A repeat of our survey for minority authors appears unproblematic but assigning racial identity to names or authors could be viewed as unethical and, given the international nature of our corpus, would have to consider the complex relationship of racial, national, and regional identities. ",
        "article_title": "Corpora and Complex Networks as Cultural Critique: Investigating Race and Gender Bias in Graphic Narratives",
        "authors": [
            {
                "given": "Alexander",
                "family": "Dunst",
                "affiliation": [
                    {
                        "original_name": "University of Paderborn",
                        "normalized_name": "University of Paderborn",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/058kzsd48",
                            "GRID": "grid.5659.f"
                        }
                    }
                ]
            },
            {
                "given": "Rita",
                "family": "Hartel",
                "affiliation": [
                    {
                        "original_name": "University of Paderborn",
                        "normalized_name": "University of Paderborn",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/058kzsd48",
                            "GRID": "grid.5659.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Russian flu 1889-1893 epidemic reached Europe from the East in November and December of 1889 and spread over the whole globe in the space of a few months. It was one of the first epidemics of influenza that occurred during the period of the rapid development of bacteriology. In addition, it was the first ever epidemic that was publicly and intensively narrated in the developing daily press, especially those published in German located in Germany and Austria (Mirosławska et al., 2013). However, as stated in (Val- tat et al., 2011), very limited information about the epidemiology of this influenza has been found, which was based on materials published in English. While a large amount of news about the flu was published in German, it is hard to find a study on the epidemic based on German documents. These motivate our goal in this work, which is to build a framework from German materials to support research community getting more insights into the disease. Our framework consists of different components including data collection and cleaning, corpus creation, and associated tools for analysis. The framework is pictorially shown in Figure  1.  Related workThere is limited information about the epidemiology of the Russian flu epidemic 1889-1893. In (Mirosławska et al., 2013, the authors conducted an analysis to examine the impact of the epidemic in 14 cities in Europe. Their results showed that the epidemic spread quickly from Saint Petersburg, Russia to other parts of Europe with a speed of around 400 km/week and reached the American continent only 70 days after the original peak in Saint Petersburg. In addition, some detailed information about case fatality ratio and the median basic reproduction was given also. However, their work was based on reports of only two local daily newspapers in Poznań, which implies some uncertainty due to the lack of data coverage. Valleron et al., 2010 presented a case study on the transmissibility and geographic spread of the Russian flu. A similar approach was followed by Valtat et al., 2011 to examine the age distribution of the affected people and the mortality rate of this flu event. In a recent study, Ewing et al., 2016 collected contemporary reports and explored a digital humanities approach to interpret information dissemination regarding this particular epidemic. The limitations common to all of these studies are the heterogeneity and lack of coverage of data used.  Data preparation Data collectionData used in this work was collected from the Austrian Newspapers Online (ANNO) repository. ANNO contains almost all issues from many newspapers in Austria and Germany during the time the Russian flu epidemic took place. The data are accessible in both scanned PDF and OCR formats. These are appropriate for our goal in terms of extracting Russian flu related stories from noisy OCR text and checking against the scanned PDF content for validity. To establish the data collection, the keywords listed in Table 1 (along with some misspelt variations of keywords, which were included due to OCR misrecognition) were used to search the ANNO repository The search query was constrained by the time interval from 1889 to 1893. After preprocessing the search results we obtained 4,806 issues, which become the candidates to extract stories about the Russian flu. Noise reductionDue to the low quality of the scanned images of newspaper issues, a lot of noise is present in the corresponding OCR texts. The word error rate (WER) computed on sample texts is around 18.9%. Our goals here were to remove noise and correct misrecognized words as much as possible but at the same time manage keep the language as it was so that the derived corpus pertains its historical perspective. It is noted that modern German is rather different in writing and usage of many words due to the language's evolution. To cope with these issues, we adopted a snapshot of the Google-2-gram dataset for German from 1885 to 1895. The dataset was used to train our bigram-based model for word segmentation and spell checking. After running the model, the word error rate was reduced to 5.5%. Text block classificationA difficult challenge for the task of extracting complete stories is that recognized OCR text blocks are very often not aligned in the same order as they were in the original image of an issue. Our approach was to automatically pre-classify OCR text blocks to identify the ones that are more likely part of some flu-related stories. Then we developed a tool to effectively help annotators extract complete Russian flu stories. For this, we adopted the KL-divergence based technique developed in (Schneider, 2004) to build a classifier. The model was trained with 245 OCR text paragraphs and obtained recall of 81.5% and precision of 68.6%. Basically, the output of the classifier can be used to help annotators start working on an issue by looking at suggested text blocks first, from which they can then select paragraphs that are part of the same story. Extraction toolAfter completing the high-recall automatic pre-extraction, we implemented a Web-based tool for annotators to help build our corpus collaboratively. The main GUI of our tool is shown in Figure 2. After having annotators work through the whole collection, we obtained a corpus of 639 news articles about Russian flu from 42 newspapers, identified with 85.7% agreement between annotators.  Geo and temporal information extractionGiven that location and time are helpful features for exploring the development of the epidemic, we extracted and normalized geographic names and temporal expressions occurring in the corpus. For geographic names, the Geodict tool created by Pete Warden (2011) was adapted to work with country and city names in German. HeidelTime (Strötgen and Gertz, 2013) was used for temporal information extraction and normalization. Indexing and search engineWe created a search engine on the corpus to support research community in searching for information. The searching GUI is shown in Figure 3.  Exploration tools and sample resultsWe provided associated tools along with the corpus. The corpus timeline provides statistics on the number of stories in the corpus across time and news outlet. In addition, it provides interactive visualization. As an example shown in Figure 4, during the peak time in late December 1889 and January 1890, extensive news about the influenza was published. Newspapers were trying to narrate the outbreak as fast as possible. Words that appear significantly in the stories include influenza, epidemic, krankheit (disease) and erkrankt (sick). A short time after this peak period, fewer reports were published about the outbreak of the flu and communities started discussing the treatment more. Names of doctors appearing in the news (e.g., Leyden, Proust) together with words describing symptoms such as fieber (fever), kopfschmerzen (headache), appetitlosigkeit (anorexia). Furthermore, by exploring word collocations one can find even more interesting information. Figure 5 shows a frequent pattern of word collocation describing the influenza. The words heute (today) and gestern (yesterday) indicate that news about the flu is updated every day; and the word jänner (January) implies that the flu outbreaks happened during winter.  Figure 6 presents the co-occurrences of three words infuenza, erkrankt, and london over time. It shows a similar trending pattern of the words infuenza and erkrankt being used to note about the flu. In addition, one can observe that the peak time of the flu in London was from late December 1889 to early January 1890 as indicated in (Honigsbaum, 2010;Goff, 2011). This suggests that the temporal distribution of terms can give us more insights into the evolution of the epidemic. The framework also supports studying the evolution of the flu over time and geographic regions. We employed the method introduced in ( Abdelhaq et al., 2013) for localized event detection. Figure 7 shows three snapshots describing the development of the epidemic over cities in Europe during the peak time from late November 1889 to January 1890. SummaryWe have introduced a framework for research communities to explore the historical Russian flu 1889-for collaborative annotators to help build our corpus. We further presented some interesting insights that we achieved from analyzing articles in the corpus. By making the corpus and associated tools available, we provide useful contributions to the community in support of conducting studies on influenza epidemics and evaluating temporal IR models. ",
        "article_title": "A Framework for Historical Russian Flu Epidemic Exploration from German Newspapers",
        "authors": [
            {
                "given": "Tran",
                "family": "Canh",
                "affiliation": [
                    {
                        "original_name": "L3S Research Center",
                        "normalized_name": "L3S Research Center",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/039t4wk02",
                            "GRID": "grid.507815.e"
                        }
                    },
                    {
                        "original_name": "Heidelberg University",
                        "normalized_name": "Heidelberg University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/038t36y30",
                            "GRID": "grid.7700.0"
                        }
                    }
                ]
            },
            {
                "given": "Wolfgang",
                "family": "Nejdl",
                "affiliation": [
                    {
                        "original_name": "L3S Research Center",
                        "normalized_name": "L3S Research Center",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/039t4wk02",
                            "GRID": "grid.507815.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " StylometryThe texts of Bernard of Clairvaux are edited in the Sancti Bernardi Opera (SBO), Jean Leclercq's edition published in 8 volumes. Nicholas' letters and sermons, on the other hand, still lack a modern edition, and can only be found in Migne's Patrologia Latina (see table 4). Although experiments and debates as to which textual features best capture stylistic difference are still ongoing, many state-ofthe-art studies employ function words, which still prove to be the most robust discriminators for writing styles (Burrows, 2002). Function words are usually short and insignificant words that pass unnoticed, such as pronouns, auxiliary verbs, articles, conjunctions and particles, whose main advantage is their frequent occurrence, less conscious use by authors and content-or genreindependent character. Their benefit and success for stylometry in Latin prose have been convincingly demonstrated before, although the methodology still raises acute questions which keep stylometrists on the lookout for alternatives.Because medieval Latin is a synthetic language with a high degree of inflection, the texts required some preprocessing (Mantello and Rigg, 1996). For instance, enclitica such as -que and -ve had to be separated from the token in order to be recognized as a feature. Secondly, texts are more easily mined for information when the lexemes are lemmatized (which means that the instance of the word is referred to its headword) and when its words (tokens) are classified according to grammatical categories (parts of speech). For this purpose we applied the Pandora lemmatizer-tagger on the texts, a piece of software developed by Kestemont and De Gussem that is equipped to achieve specifically this (Kestemont and De Gussem, forthcoming). The third column, the part-of-speech-tag (PoS), allowed to immediately restrict the culling of most frequent words to those word categories that make up the collection of function words: conjunctions (CON), prepositions (AP), pronouns (PRO) and adverbs (ADV). This likewise filtered out some noise caused by ambiguities or homonyms like secundum. Once procedures of this sort were carried out in full, we arrived at a list of the 150 most frequent function words (MFFW) of the corpus (Figure 2) Each of the corpora was segmented into samples. This yields the advantage of \"effectively [assessing] the internal stylistic coherence of works,\" (Eder et al., 2016) which answers directly to the primary goal of the present study. The sermons were segmented into 1500 word-samples (Figures 3-4 present aexcerpts from tables describing the texts contained in each sample).  It should be noted that 1500 word-samples run the risk of increased imprecision, a consideration which should nuance any interpretation of the results ( Kestemont et al., 2013). Once the corpus was divided, each of the text samples was vectorized to document vectors. The raw counts were TF-IDFnormalized (Term frequency inverse document frequency), a procedure which divides the function word frequencies by the amount of text samples that respective function word appears in (Manning, 2008;Kestemont et al., 2016). As a consequence, less common function words received a higher weight which prevents them from sinking away (and losing statistical significance) in between very common function words. Once the data was preprocessed and regulated, two statistical techniques were applied to visualize its dynamics.The first is a k Nearest Neighbors network in GEPHI (hereafter abbreviated to k-NN) (Jockers, 2013;Eder, 2015;Jacomy et al., 2014), the second is principal components analysis (hereafter PCA) ( Binongo et al., 1999). Their respective results will prove to be similar in a general sense, yet crucially different in the details. We argue that such an additional statistical validation provides for a more accurate, nuanced interpretation and a better intuition of the data. In the first visualization, the k-NN networks, we first calculated the 5 closest text samples to each text sample by applying k-NN on the frequency vectors. Accordingly for each text the 5 most similar or closest texts were calculated, weighted in rank of smallest pairwise distance (Minkowski metric, a Eucledian metric) and consequently mapped in space through forcedirected graph drawing (algorithm Force Atlas 2). The weights were directly calculated from the distances. The intuition is then that the distances should be normalized to a (1,0) range (as a higher distance responds to a lower weight). Secondly, PCA is a technique that allows to reduce a multivariate or multidimensional dataset of many features, such as our function word frequencies, to merely 2 or 3 principal components which disregard inconsequential information or noise in the dataset and reveal its important dynamics. The assumption is that the main principal components, our axes in the plot, point in the direction of the most significant change in our data, so that clustering and outliers become clearly visible. Each word in our feature vector is assigned a weighting or loading, which reflects whether or not a word correlates highly with a PC and therefore gains importance as a discriminator in writing style. In a plot, the loadings or function words which overlap with the clustered texts of a particular author are the preferred function words of that author (see Figure 7 under \"Results\").  Firstly, when examining the visualizations, it is striking how -indeed -the diversity of Bernard's De diversis is captured. Especially PCA demonstrates the discernible stylistic incoherence as the samples burst open all over the plot (especially along the vertical axis of the second principal component), at times suggesting the interference of other writers than Nicholas or Bernard in their composition. Other samples gravitate in between Nicholas and Bernard, and in some cases Nicholas' influence on the style is undeniable. De diversis 6, 7, 21, 62, 83, 100 and 104, which Nicholas included in the letter to count Henry the Liberal (they are split up in two red samples labeled with le_ of Leclercq), do not betray an obvious affinity to Nicholas' style (although le_1 is not far off). Neither are they unambiguously Bernard's. Both samples diverge strongly and seem too hybrid in nature to be restrained. The case rather ostensifies how difficult it is to defend concepts such as \"single authorship\" or \"text theft\" in a medieval context: the le_ samples are clearly not of a \"singular\" style (nor of Nicholas's style, nor of Bernard's), but defy classification. In fact, if we compare both k-NN and PCA, Nicholas' influence in sample le_1 seems considerably larger than Bernard's. ",
        "article_title": "A Stylometric Study of Nicholas of Montiéramey's Authorship in Bernard of Clairvaux's Sermones de Diversis",
        "authors": [
            {
                "given": "Jeroen",
                "family": "De Gussem",
                "affiliation": [
                    {
                        "original_name": "Universiteit Gent",
                        "normalized_name": "Ghent University",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/00cv9y106",
                            "GRID": "grid.5342.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionHistorical sources provide us only a fragmentary testimony of historical processes. The survival of specific sources and the information they contain is very often determined by chance. Because of these circumstances, the reconstruction of many historical processes remains problematic or, in some cases, almost impossible. In addition to these problems, some long-term historical processes are very difficult to recognize from the perspective of \"event history\" due to their gradual nature. The interdisciplinary project GEHIR (Generative Historiography of Religion) at Masaryk University strives to, within the framework of the historically oriented study of religions, adopt innovative methods used in the study of the dynamics of complex systems, i.e. methods including mathematical and geographical modelling, agent-based simulations or network analysis. Within the study of historical processes these formalized methods are conceptualized as an innovative third way through which the limitations of the traditional inductive analysis of historical sources and deductive application of social-scientific and cognitive theories to social and historical phenomena can be overcome. In this paper we would like to introduce the results from a case study within the GEHIR which focused on uncovering the possible factors influencing the early spread of the cults of Isis and Sarapis in the ancient Mediterranean. MethodsIn this project, traditional methods of historical research such as critical analysis of the archeological and literary sources or categorization of the historical evidence in the form of relational database were supplemented by the following computational methods:• Network analysis is currently used in various areas of historical research, mainly to understand and analyse complex structures, e. g. trading cities in state, social groups. There are a number of historical research papers that have used network analysis; the work of Rivers et al (2012) should be mentioned.• Environmental/geographical modelling is used to reconstruct the potential of locality within an examined phenomenon that is even partially spatially-dependent. It offers a unique opportunity for researchers to understand how spatial conditions are related to other events. The work of Turchin et al (2013) and others could be used as an illustration of such methods.• Mathematical statistical analysis aims to find hidden logical patterns and existing correlations within multiple phenomena that may subsequently be interpreted in the context of the research questions. Various methods and approaches can be used, including clustering, multidimensional analyses, and exploratory data analysis. Case study: Egyptian cults in the Aegean Sea regionEarly in the Ptolemaic era, in the period between ca. 305 -167 BCE, the Egyptian cults, particularly those of Isis and Sarapis, spread successfully to ports in the ancient Mediterranean. The reasons behind this process are, however, only partially understood. The original and still respected hypotheses in the academic discussion emphasize either the maritime trade (Fraser, 1960) or Ptolemaic political propaganda (Cumont, 1911:78-80) as key factors in the spread of these cults. Both of these claims are supported by historical evidence. Ptolemaic Egypt was one of the main exporters of grain, Isis was a patron goddess of sailors and many cities in the ancient Mediterranean had close diplomatic relations with the Ptolemies. To specify our area of interest, this research is mainly focused on the early spread of the Egyptian cults in the area of the Aegean Sea, particularly the Aegean islands. There are several reasons for this selection:• the main trading routes between Alexandria and continental Greece passed through that particular area with the Aegean islands as potential places of Egyptian interest.• the first Ptolemies were politically invested in the Aegean using the islands as strategic locations for military bases and administered the Island league.• islands are more isolated worlds and thus more suitable for modelling. In order to clarify these hypotheses, we sequentially applied the described computational methods to the issues of maritime transport network reconstruction, approximation of the urgency of importing goods, evaluation of positional advantages of particular islands, and statistical correlation of different modelled factors.The first task was to reconstruct the maritime transport network, as this was the backbone of ancient Mediterranean trade. For this purpose, Pascal Arnaud's collection of ancient maritime routes were scanned and geo-referenced in GIS software and all the routes within the area of interest were redrawn. This network was then validated and modified by the location of ancient ports, the AWMC map and geometries of islands. Afterwards, we were able to use network analysis to calculate centrality values in order to approximate attractiveness of harbours.As strategic positioning is not the only determinant of trade intensity, we also decided to estimate the ratio of agricultural potential and the number of inhabitants on each island. This can then be used as an approximation of the urgency of goods import. Food production was measured based on the geographical environmental model we created. Various approaches to population estimates were put forward and discussed. We decided to use historical censuses from the 19th century, as they were taken before the demographic transition in the area and the population values should more or less correlate.In order to validate the hypothesis emphasizing the Ptolemaic political actions as a key factor of the spread of the cults, we also needed to define the political factors. The Ptolemaic garrisons dispersed in the Aegean Sea are of great importance in our research because many of them were maintained for a long period of time and could thus increase the chance of spreading the cults via Ptolemaic soldiers. The second potential factor influencing the spread of the Egyptian cults is the existence of political leagues. In the context of the Aegean Sea, Ptolemies led the Island league from ca. 287 to 250s BCE. Again, this political factor is relevant because it is geographically delimited and lasted for a longer period of time.The final and perhaps the most important dataset includes the locations of the Egyptian temples and artefacts from the above specified spatio-temporal frame (see Figure 1), collected from the RICIS catalogue (Bricault, 2005). Geographical analysis was used to calculate distances from each island to the closest army base and temple in order to evaluate spatial dispositions. The final part of the research is the statistical analysis of data obtained in previous steps. At that point we had a table containing the list of islands with their attributes based on the data (e.g. if an island occupied a strategic position on the network, or if it had a Ptolemaic garrison) and the goal was to find a solid and interpretable mathematical model that would be able to find and explain relations between these values, mainly their dependency on the distance to the closest temple on the network. Results and ConclusionsThe mathematical model we selected using the statistical analysis of covariance uncovered a few patterns within our datasets. The results suggest that there is a strong correlation between the placement of the Ptolemaic garrisons and the dissemination of Egyptian temples and artefacts in the Aegean sea. The interpretation would, in this case, be that the Egyptian military forces potentially played a significant role in the spread of the cults. However, the analysis also showed that in areas far from these military bases the islands with higher trade attractiveness intensified the presence of the Egyptian cults.The main task of this paper is to show how we can \"regrow\" the Egyptian cults in space and time using innovative computational methods from multiple disciplines. This allows us to validate the selected hypotheses from the academic discussion constructed by using traditional historiographical methods. We see great potential for using these approaches in the other case studies of GEHIR, as well as in historiography in general. ",
        "article_title": "Regrowing Egyptian cults: The potential of using modern computational methods in the study of ancient religions",
        "authors": [
            {
                "given": "Adam",
                "family": "Mertel",
                "affiliation": [
                    {
                        "original_name": "Masaryk University",
                        "normalized_name": "Masaryk University",
                        "country": "Czechia",
                        "identifiers": {
                            "ror": "https://ror.org/02j46qs45",
                            "GRID": "grid.10267.32"
                        }
                    }
                ]
            },
            {
                "given": "Tomáš",
                "family": "Glomb",
                "affiliation": [
                    {
                        "original_name": "Masaryk University",
                        "normalized_name": "Masaryk University",
                        "country": "Czechia",
                        "identifiers": {
                            "ror": "https://ror.org/02j46qs45",
                            "GRID": "grid.10267.32"
                        }
                    }
                ]
            },
            {
                "given": "Zdeněk",
                "family": "Stachoň",
                "affiliation": [
                    {
                        "original_name": "Masaryk University",
                        "normalized_name": "Masaryk University",
                        "country": "Czechia",
                        "identifiers": {
                            "ror": "https://ror.org/02j46qs45",
                            "GRID": "grid.10267.32"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Digital Humanities Librarianship is a 3-credit elec-tive open to University of Washington Masters of Library and Information Science students and a core elective in the Graduate Certificate in Textual and Digital Studies program. The course focuses on creating full contributors in the DH realm: a plethora (in academic terms, at least) of job postings for DH or Digital Scholarship librarians shows rapid growth in this area and a need for well-prepared individuals. No matter the physical location of DH centers-from libraries to academic departments to research centers-DH librarians are ideally suited for navigating the intellectual and geographic spaces of ideas, resources, and tools. In addition to its unique hybrid format (synchro-nous residential and online), the course is also notable for providing a balanced approach to the theory vs. application tension typical of MLIS programs. Most DH courses in LIS departments focus on technologies, without concomitant attention to a foundational understanding of the varied research methods and resource usage patterns of humanities scholars. The DH Librarianship course aims to provide students with an understanding of how humanities scholars work, both traditionally and digitally, as well as familiarity with resources and tools used in digital humanities scholarship. The course also covers political and practical issues: what roles do librarians play in DH research, and what roles are situationally appropriate-tech guru, data cleaner, resource purchaser, equal collaborator? During the quarter, the course tackles questions of sustainability , accessibility, ethics, and equity in representation. Guest speakers include DH librarians (both with and sans MLIS), new DH faculty in various disciplines , and seasoned humanities researchers. Assignments include disciplinary exploration, which allows students to explore resources and DH projects in philosophy, religion, fine and performing arts, languages and literatures. Student groups examine DH tools that range from timelines and mapping to text mining, information visualization, data cleaning, and network analysis, and create presentations from a shared corpus. In the final \"DH consultation\" assignment , students locate a project-in-process, abandoned , or \"complete\"-and propose options for library based support, based on a disciplinary needs assessment. They also provide suggestions for strengthening the content, usefulness, or reach of the project, as well as a tools/usability assessment, which may include creating a prototype of an improved project. The pedagogy reflects the multi-faceted disciplinary grounding and technological approach of the course content. My experience as an academic humanities librarian is bolstered by the research of many, including Melissa Terras, who discusses the need to identify core values and \"hidden histories\" of disciplines (2006), and Marcia Bates, who defines distinctions between disciplines such as the humanities and meta-disciplines like LIS (1999). In LIS we analyze the processes and domains of disciplines-in this case those involved in DH-and how those are represented, accessed, and given meaning across the corpora of recorded information. Mike Caulfield's writing (2016) on multiple digital literacies also resonates with the ped-agogies used, demonstrating the need for domain-grounded literacy to help students ascertain next steps and appropriate tools in their work with humanities scholars and projects. The hybrid format of the class contributes to the collaborative pedagogy: local online and residential students attend the technology-enabled classroom in person, and others attend via the online Zoom classroom. Students can participate via audio or text, share screens, display presentations, or work in groups. Cameras in the classroom broadcast what is happening locally, and those attending online enable their webcams to be more fully present. In addition, several students each week attend via Kubi robots, which are iPads on movable stands that online users rotate to focus on different classroom activities. Guest speakers may attend in person, or they may present and engage with students via online options. These diverse modes of learning increase the students' comfort with multiple technologies, which they are then more likely to use in their own research and teaching. Learning outcomes include familiarity with the structure of knowledge in the humanities disciplines as well as the wide array of resources that provide ref",
        "article_title": "Digital Humanities Pedagogy in the University of Washington In- formation School MLIS Pro- gram",
        "authors": [
            {
                "given": "Helene",
                "family": "Williams",
                "affiliation": [
                    {
                        "original_name": "University of Washington",
                        "normalized_name": "University of Washington",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00cvxb145",
                            "GRID": "grid.34477.33"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Lexos is a browser-based suite of tools that helps lower barriers of entry to computational text analysis for humanities scholars and students. Situated within a clean and simple interface, Lexos consolidates the common pre-processing operations needed for subsequent analysis, either with Lexos or with external tools. It is especially useful for scholars who wish to engage in research involving computational text analysis and/or wish to teach their students how to do so but lack the time for a manual preparation of texts, the skill sets needed to prepare their texts analysis, or the intellectual contexts for situating computational methods within their work. Lexos is also targeted at researchers studying early texts and texts in non-Western languages, which may involve specialized processing rules. It is thus designed to facilitate advanced research in these fields even for users more familiar with computational techniques.",
        "article_title": "Lexos: An Integrated Lexomics Workflow",
        "authors": [
            {
                "given": "Scott",
                "family": "Kleinman",
                "affiliation": [
                    {
                        "original_name": "California State University",
                        "normalized_name": "California State University, Chico",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/027bzz146",
                            "GRID": "grid.253555.1"
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Leblanc",
                "affiliation": [
                    {
                        "original_name": "Wheaton College",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionFrom the Lexicon to the eLexiconThe first fascicle of the Lexicon mediae et infimae Latinitatis Polonorum (Dictionary of Polish Medieval Latin, henceforth LMILP) was published in 1953. The project aims at providing an exhaustive account of the Latin vocabulary used on Polish territory during the Middle Ages. Addressed to a scholarly public, the dictionary does not make many concessions to a less advanced user. Information is often conveyed only indirectly, by means of typographic devices or is left to be inferred by the reader. The project of retro-digitization of the LMILP started in the mid-2011 and was completed by the mid-2014. The web application, although completed, is still subject to modifications and refinements. Dictionary AnnotationThe XML encoding of the dictionary was by no means an ultimate goal of the project. Instead, the idea was to make the rich content of the LMILP fully searchable through a user-friendly interface. This objective, however, has deeply influenced the XML schema design. The TEI (TEI Consortium 2016) was chosen as an annotation standard because at the time the work started it had been already employed in major electronic lexicography projects (Lewis-Short by the Perseus Project; DuCange by the ENC). The popularity that the standard had gained among scholars contributed to emergence of lively community which produced documentation and use cases which supplemented the \"Dictionaries\" chapter of the TEI Guidelines. Also, the very fact that the TEI Guidelines offered a set of ready-to-use tags for the description of lexicographic content was not without significance. Finally, the TEI XML was supported by major software providers, an important factor for the project in which adaptation of existing rather than writing new software was planned. WorkflowThe paper dictionary was scanned and the output of the OCR program (Abbyy FineReader 11) was exported to ODT files; from each a content.xml file was extracted and then applied a series of XSL transformations. The main goal was to simplify styles that were automatically generated by the OCR software. Resulting XML files underwent second phase of XSL processing in which constitutive parts of the dictionary, such as entry, headword, sense definition etc., were encoded. The output XML files were again retranslated into ODT format: entries were encoded as paragraph styles, other tags were represented as character styles. In the next phase of the project the lexicographers started to proofread OCR text and correct errors of automatic annotation. This task was performed with the help of LibreOffice Writer exclusively without annotators being actually conscious of the underlying XML structure. From the practical point of view, annotation consisted in verifying whether automatic XSL processing produced correct styles; if this was not the case correct style had to be applied, as in standard text processing task. This approach allowed for reducing the learning curve to a minimum so that the team members could focus on the lexicographic content. However, it also has a serious drawback: annotation in the text editor cannot produce more complex hierarchies, since paragraph and character styles allow for representing at best two levels deep nesting. TEI for the eLexiconA guiding principle of the subsequent TEI annotation was to combine editorial and lexical view of the dictionary content by (1) preserving its original text and (2) storing normalized data in attributes and empty XML elements. Typographic properties of the text, on the other hand, were not generally encoded, they are easily reconstructible though.Automatic and manual annotation consisted in three major procedures: a. translation: custom ODT styles (corresponding to elements of dictionary structure) were \"translated\" into respective TEI elements or attributes; b. grouping: deeply nested XML structure was produced from flat annotation; c. enrichment: implicit information was made explicit. TranslationThe paper justifies some of the annotation choices. Special attention is given to the peculiarities of encoding a scholarly lexicographic work.1. <entryFree> element was chosen as a container for dictionary entries. 2. Essential features of the dictionary macroand microstructure are encoded as: <form>, <orth>; <gramGrp>, <gen>, <iType>, <pos>; <etym>, <lang>, <men-tioned>; <cit>, <bibl>, <biblScope>, <date>, <quote>; <sense>, <usg>, <def>, <gloss>; <xr>, <ref>; <lbl>; <re>, <cer-tainty>, <oVar>, <note>. 3. Content and form peculiarities of the LMILP are reflected in respective attributes. So, for example, functional variation of the entries is represented in the @type attribute of the <entryFree> and can take one of the following values: main, xref, hom. 4. The TEI schema was only lightly customized: unused elements were deleted; a few content restrictions were overcome. Grouping: adding depthThe flat entry structure had to undergo heavy XSL processing, so deep nesting typical of scholarly dictionaries could eventually emerge. Relative ease of the XML-unaware manual annotation resulted in timeconsuming post-processing. The xsl:for-each-group XSL function was employed in order to structure:1. citation groups:2. etymological groups: 3. PoS and grammar groups: 4. sense groups: Enrichment: expanding the dictionary contentConsiderable effort was put into enriching the original content of the dictionary, namely: (1) resolving references, (2) normalizing strings, (3) adding redundant and/or inferred information. Resolving referencesA typical reference to an exact location in the dictionary text was encoded as follows:References to a specific entry or sense relied on the @xml:id attribute:The encoding of most frequent type of references (pointing to a source of a language use example) is illustrated in the section II B 4 above. String normalizationBy string normalization, we mean a set of various procedures applied in order to generate a lexical view of the dictionary content. Standardized strings are usually stored in @norm attributes of such elements as language or usage labels, prepositional and inflectional patterns etc. Their primary goal is to enable unified search that would be agnostic of the exact formulation of the paper dictionary. For example, when looking up philosophy-related terms one should be able to retrieve them no matter whether they have been marked with a phil. label or with more verbose formula in textibus philosophicis \"in philosophical texts\", as both are annotated as @norm=\"phil\". The second major goal of the normalization was to render chronological information consistent and machine-readable. Its proper annotation should reflect the fact that many medieval texts cannot be dated but only approximately. Apart from some obvious cases (@when attribute stores a year date, for example <time when=\"1450\">a. 1450</time>) the LMILP employs:1. century dates (<time notBefore=\"1401\" notAfter=\"1500\">saec. XV</time> ) 2. imprecise dates in year (<time notAfter=\"1120\">ante 1120</time>) or century format (<time notBefore=\"1401\" notAfter=\"1450\">saec. XV in.</time>). Making information explicitFinally, substantial effort has been devoted to making explicit what is not expressed directly in the paper dictionary, but left to be inferred by an expert user. In the LMILP, this is the case, for example, of a part of speech label which is provided for adverbs or conjunctions, but is normally omitted from verb or noun entries. Empty elements have therefore been created and their attributes filled with the inferred content. So, in a typical case, an element <pos norm=\"subst\"/> would be appended to a <gramGrp> group whenever the paper dictionary informs about a word's part-ofspeech only indirectly, by means of a gender label (f. for Lat. femininum) or inflectional ending typical of nouns (-ae): The Dictionary Web ApplicationThe last part of the paper briefly presents the overall architecture of the dictionary web application, its user interface having been already described elsewhere (Nowak 2014). Written entirely in XQuery, the application is served directly from the eXist-db instance with HTML and JavaScript code being equally stored in a database or generated on the fly. The presentation focuses on those features available in the eXist-db which are of critical importance for dictionary application design:Fig. 1 :1Fig. 1: \"XML-unaware\" annotation in the LibreOffice window ",
        "article_title": "eLexicon. Dictionary of Polish Medieval Latin: from TEI encoding to an eXist-db application",
        "authors": [
            {
                "given": "Krzysztof",
                "family": "Nowak",
                "affiliation": [
                    {
                        "original_name": "Polish Academy of Sciences Krakow",
                        "normalized_name": null,
                        "country": "Poland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionVisualization techniques developed in the sciences normally focus on the (re)presentation of empirical data. But how can we graphically express interpretations? This paper presents the intellectual framework underpinning the 3DH project (Three-dimensional Visualizations for the Digital Humanities), a collaborative project conducted at the University of Hamburg from 2016 to 2019. The project foregrounds data interpretation and develops a visualization paradigm from the epistemological perspective of the humanities. The \"third dimension\" required in DH visualization techniques is therefore not merely that of an additional quantitative z-axis. Rather, it is an axis that can 'unflatten' (Sousanis 2015) the objectivist notion of visualized data. In our presentation, we will do three things:• Digital and visual turn: Review existing visualization paradigms that emphasize the representational approach. We start with the epistemological issues raised by the digital and visual turn.• Visual modelling: Outline and discuss an interpretative modelling alternative through two case studies of existing tools, CATMA and Voyant, and Temporal Modelling, a platform for creating data through graphical means.• \"Hermeneuticizing\" visualization: Discuss the design of a full visual framework. We will present possible conventions and prototypes that use them. These inform our case studies and the envisaged infrastructure. Case studies in our presentation will be drawn from CATMA (a collaborative mark-up & text analysis environment), Voyant (a text analysis platform), and humanities research projects using base images (historical maps) and original models (for non-standard chronologies). The digital and the visual turn: a hermeneutic ceterum censeoFor centuries, academic discourse in humanities disciplines has relied predominantly on text. In DH, however, visualizations increasingly claim the status of arguments and proofs that play a decisive role in the development and presentation of ideas, findings, and conclusions.The visual and the digital turn have thus gone hand in hand - but the way in which this synergy manifests itself remains constrained in a symptomatic way. We can print a chart or render it on screen just as we can print or display a text in various media, but we normally cannot subject the chart to in-depth critique in the way we can question and respond to the text. Inadvertently, once generated and communicated as 'output', visualizations seem to take on a quasi-dogmatic quality - they are hard to deconstruct, let alone reconfigure; they state their case but seem removed from critical reflection.Most current DH visualizations are thus epistemological one-way avenues toward knowledge, from data via rendering algorithm to visual display. Charts, graphs, interactive maps, timelines, and similar representations are by and large imports from the natural and social sciences (Friendly 2008). Many of them emanate from domains of empirical research that conceptualize knowledge production as a function of empirical observation and objective measurement followed by analysis, inference, and conclusion. These approaches to visualization, however, hide two critical aspects, namely (a) the underlying human modeling of the represented phenomena as data, which is already an interpretive and meaning-creating act that often oscillates repeatedly between observation and interpretation (Kitchin 2014), and (b) the meaning-lessness of certain visual effects that are owed to contingent technological constraints (screen size, rendering, scaling, choice of color, etc.). DH is in a unique position to investigate the domains of human experience and of its expression in symbolic practices and artefacts from two complementary methodological vantage points: the numeric, which models them as statistical phenomena, and the hermeneutic, which explores them as phenomena of meaning and thus by definition as a function of interpretation (Rockwell & Sinclair 2016). Where meaning comes into focus, our theories, object models, and practices must therefore be conceptually aligned and 'hermeneuticized' -just as numeric approaches come with the pre-requisite of quantification. Against this backdrop, we propose to reintroduce the dimension of interpretation into visualization: Methodological principles of hermeneutic approaches, such as multi-perspectivity, subjectivity, and context-boundedness present a challenge which representational visualization cannot and which interpretational visualization must meet.Two questions arise: What are the defining principles of a genuinely humanistic and hermeneutically oriented approach to visualization? And how can we graphically express and support interpretation in DH visualizations - both as an activity and as a product of humanistic enquiry? Visual modeling of interpretation vs. visualization of dataIn the 3DH project, we address the former question by conceptual analysis and critique of existing approaches to visualization in DH, and then by systematically specifying and developing a visualization environment that can support higher level data interpretation rather than base-level data representation. In the presentation, we will share our survey of existing tools and their affordances but focus on two tools that we have developed, CATMA and Voyant. For example, in CATMA (Figure 1) such an activity - in this instance the interpretive act of text annotation - is executed and represented by (a) highlighting a string on screen, (b) assigning it a tag, and (c) storing the annotation in a stand-off markup file. However, the annotation is at the same time (d) visually expressed as colored underlining. Moreover, via its visual representation on screen - the colored underlining - the markup data can also be (e) inspected, analyzed, manipulated directly, and even (f) enriched with meta-annotation. This is but one example of interpretative modeling.  Figure 2) are seen as a way to deal with scale, they process large amounts of data into summary abstractions called topics that can be displayed as lists or in other ways (Montague et. al 2015). In our second case study, we will therefore show how we are adapting scale tools to create a prototypical bidirectional 3DH visual modeling environment for big data. We believe visual modeling can support not only interpretative close reading of primary data but also the reading of large collections like the collections of the Hathi Trust. 'Hermeneuticizing' base-level visualization through activators: the 3DH framework of interpretive parameters and dimensionsA key goal of the 3DH project is to develop a set of generic graphic features that can be used to create interpretative attributes and/or inflections of visual representations of data, alter underlying data structures, and activate three-dimensional space in the service of interpretative activity. These features which aim to 'hermeneuticize' visualizations are termed activators. In the presentation we will show the framework of the activator set that was developed during a series of charettes (design workshops) in 2016. The visual activators in our feature set are not simply graphical marks or animations on a screen display: They perform data structuring functions and as such provide a conceptual framework for 'hermeneuticizing' existing base-level data visualization techniques (see Fig.3). The individual features of this framework indicate and facilitate interpretative moves made by the user, such as a qualification of visualized data structures in terms of salience, irrelevance, uncertainty, degree of completeness, and other attributes or inflections. For example, uncertainty can be expressed by overlaying a standard graph with visual effects such as blur or shading, whereas the introduction of additional interpretative dimensions, such as point of view systems, parallax, relative scales, and other conventions from the visual arts, will support higher levels of interpretative critique and reflection, such as explicitly marking the historicity and context-dependency of underlying data. ConclusionAs Pinker (1990) argues, the ease with which a particular graph can be understood is a function of the processing effort that goes into the exercise: The more we can rely on 'hard-wired' encoding connections between the visual and the conceptual and the more we are guided by established graph and comprehension schemata (such as Gestalt phenomena), the less 'intelligent' effort we have to put into reading a graph. Yet in a humanities perspective such conventionalized 'ease of comprehension' is a double-edged sword: It may optimize the process of (re)cognition -but it also progressively obscures the constructedness of a visualization, turning it into an apparently self-evident object of perception. The 3DH project counters this anti-hermeneutic tendency toward reification by moving from a conceptualization of the principles of visualization as interpretative modeling to the development of a visual language framework, and finally the instantiation of the principles and language in two case studies. In terms of implementation, this approach is supported by drawing on Bertin's Semiology of Graphics and the high-level object-oriented Grammar of Graphics approach outlined by Wilkinson (2005), and features from game engines, three-dimensional modelling, and other pictorial conventions ( Panofsky (1991) and Burgin (1991)).To conclude, we will discuss next steps toward developing a 3DH environment that can act as a generic, project independent infrastructure for introducing user parameterized enunciative functionality into graphical displays. This infrastructure will make it possible to inscribe into visualizations the critical features of authorship, speaking/spoken subject, and an epistemological perspective grounded in situated and constructed approaches to knowledge. These interpretative principles are well mapped in, e.g., critical theory, narratology, visual studies, and cultural studies, but they have not been integrated into a graphical environment for hermeneutic practice yet: the methodological lacuna which the 3DH project tries to address.Figure 1 :1Figure 1: Visualization of interpretive text annotation in CATMA Our premise is that interpretation happens through the deliberate activity of an individual engaging with an image, text, display, or other artifact to create an argument about its meaning and a way it should be read. For example, in CATMA (Figure 1) such an activity - in this instance the interpretive act of text annotation - is executed and represented by (a) highlighting a string on screen, (b) assigning it a tag, and (c) storing the annotation in a stand-off markup file. However, the annotation is at the same time (d) visually expressed as colored underlining. Moreover, via its visual representation on screen - the colored underlining - the markup data can also be (e) inspected, analyzed, manipulated directly, and even (f) enriched with meta-annotation. This is but one example of interpretative modeling. Figure 2 :2Figure 2: Galaxy Viewer Current representational 'one-way' techniques like topic modeling (see Figure 2) are seen as a way to deal with scale, they process large amounts of data into summary abstractions called topics that can be displayed as lists or in other ways (Montague et. al 2015). In our second case study, we will therefore show how we are adapting scale tools to create a prototypical bidirectional 3DH visual modeling environment for big Figure 3 :3Figure 3: Framework of Concept Modeling workspace: Shows features, activators, and dimensions from various pictorial conventions. ",
        "article_title": "Modeling Interpretation in 3DH: New dimensions of visualization",
        "authors": [
            {
                "given": "Jan",
                "family": "Meister",
                "affiliation": [
                    {
                        "original_name": "Universität Hamburg (University of Hamburg)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Johanna",
                "family": "Drucker",
                "affiliation": [
                    {
                        "original_name": "University of California, Los Angeles (UCLA)",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Geoffrey",
                "family": "Rockwell",
                "affiliation": [
                    {
                        "original_name": "University of Alberta",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Entity-based Topic Modeling PipelineThe whole source code is available for public download on Github. Given a working Python, Java, and Scala runtime as well as a running MySQL installation our pipeline is ready directly out-of-the-box. The specific configuration according to the user's needs can be made via a simple text file. Three-Step Evaluation Platform Document LabelsIn order to assess the quality of the detected entities as labels we developed a specific browser-based evaluation platform, which permits manual annotations. This platform presents a document on the right of the screen and a set of possible labels on the left (See Figure 2). Annotators are asked to pick labels that precisely describe the content of each document. In case the annotator does not select any label, this is also recorded by our evaluation system. Entities and Topic WordsIn order to establish if the selected entities were the right labels for the topics produced, we developed two additional evaluation steps. Inspired by the topic intrusion task (Chang et al, 2009), we designed a platform that permits to evaluate the relations between labels and topics using two evaluation modes: For one evaluation mode (that we called Label Mode - Figure  3), the annotator is asked to choose, when possible, the correct list of topic-words given a label. For the other, he/she was asked to pick the right label given a list of topic words (aee Figure 4). In both cases, the annotator is shown three options: one of them is the correct match, while the other two (be they words or labels) come from other topics related to the same document.",
        "article_title": "SLaTE: A System for Labeling Topics with Entities",
        "authors": [
            {
                "given": "Anne",
                "family": "Lauscher",
                "affiliation": [
                    {
                        "original_name": "University of Mannheim",
                        "normalized_name": "University of Mannheim",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/031bsb921",
                            "GRID": "grid.5601.2"
                        }
                    }
                ]
            },
            {
                "given": "Federico",
                "family": "Nanni",
                "affiliation": [
                    {
                        "original_name": "University of Mannheim",
                        "normalized_name": "University of Mannheim",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/031bsb921",
                            "GRID": "grid.5601.2"
                        }
                    }
                ]
            },
            {
                "given": "Simone",
                "family": "Ponzetto",
                "affiliation": [
                    {
                        "original_name": "University of Mannheim",
                        "normalized_name": "University of Mannheim",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/031bsb921",
                            "GRID": "grid.5601.2"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Entity LinkingIn the last decade, advances in natural language processing (NLP) gave rise to word-sense disambiguation and entity linking techniques ( Cornolti et al., 2013), which automatically disambiguate entities and concepts in context and link them to knowledge bases such as Wikipedia, DBpedia (Auer et al., 2007) or Babelnet ( Navigli and Ponzetto, 2012). Among them, TagMe (Ferragina and Scaiella, 2010) has been often adopted in NLP, thanks to its decent performance on different datasets and to its easy-to-use API. Current Limitations for DH researchTagMe also highlights a few common limitations of current standard entity linking systems that reduce their applicability within most scenarios found in the heterogeneous spectrum of Digital Humanities research.• Black Box and reproducibility. As Hasibi et al. (2016) recently remarked, the TagMe RESTful API remains a black box, as it is impossible to check whether it corresponds to the system described in the original paper. Not knowing the reliability of the system limits its use for distant reading analyses, i.e. quantitative studies that go beyond text exploration.• Language Versions. Currently, TagMe is only available in English, German and Italian but does not support other widespread languages such as Chinese, Arabic, Spanish, and French, which are essential for enhancing its use in the DH community.• Infrequent Updates. TagMe has been initially created on the English 2009 version of Wikipedia and it has been updated only twice (summer 2012, summer 2016). Imagine a setting where a scholar intends to analyze a collection of mainstream news on the Middle East: before the most recent update the system was not able to detect mentions of \"Al-Nursa Front\", the former Syrian branch of al-Qaeda.• Wikipedia as Knowledge Base. TagMe, as well as other entity-linking solutions, relies on the assumption that the entries and structure of Wikipedia provide us with a comprehensive and accurate knowledge base. While this is mostly true for standard NLP and IR approaches, when it comes to humanities research this assumption shows all its limitations. As a matter of fact, linking to Wikipedia is not ideal for example when dealing with historical documents, simply because entities and concepts relevant in the corpus may be missing from such a general-purpose knowledge resource (as remarked in Lauscher et al., 2016). Specific contributionWhile we are currently working on the implementation and optimization of a domain-adaptable entity linking pipeline, at the conference we intend to present a solution for generating, in an automatic fashion, domain-specific knowledge bases from an user-created Wiki. As the creation of a complete Wiki is too time-consuming, these domain-specific wikis are used in combination with general world knowledge available on Wikipedia. In particular, we will describe how our system can make use of the following input:The XML Dump of any language version of Wikipedia and rapidly create the indexes that compose the knowledge base. This permits to have a knowledge base for each language version of Wikipedia and to update it on the spot whenever needed.Any MediaWiki website dump, such as Wikia (although it is important to consider the copyright license when downloading and using this data), to be merged into the same index. In the table we report a few examples from different Wikia sites. It is important This solution gives the scholar the possibility of creating (or improving an already existent) domain specific Wikia (a practice common in DH education, see Farabaugh, 2007 andGiglio & Venecek, 2009) on the topic she/he intends to study and identifying mentions of domain-specific and general-purpose concepts in large text collections. ",
        "article_title": "Enhancing Domain-Specific Entity Linking in DH",
        "authors": [
            {
                "given": "Federico",
                "family": "Nanni",
                "affiliation": [
                    {
                        "original_name": "University of Mannheim",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Yang",
                "family": "Zhao",
                "affiliation": [
                    {
                        "original_name": "University of Mannheim",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Simone",
                "family": "Ponzetto",
                "affiliation": [
                    {
                        "original_name": "University of Mannheim",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Laura",
                "family": "Dietz",
                "affiliation": [
                    {
                        "original_name": "University of New Hampshire",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Research contextIn the last fifteen years several studies have proposed models for evaluating the impact of digital resources ( Warwick et al., 2006;Meyer et al., 2009;Tanner, 2012). Citation analysis is commonly used in impact evaluation, and is a well-established bibliometric technique for impact analysis (MacRoberts and MacRoberts, 1989). As part of the wider field of bibliometrics, citation analysis has been used to judge the impact of academic publications and digital resources (Meyer et al., 2009). Citations are proxy measures of how frequently a document or resource is used, founded on the assumption that there is a strong positive correlation between the number of citations that a resource or article attracts, and the quality of that resource (Smith, 1980). The reality, though, is that citation practices are not always followed or understood by academics. Smith (1980) noted several reasons a scholar may choose not to cite a document: inability to obtain the document; inability to read a foreign language; lack of relevance to their work; or lack of awareness of existing work. There are further reasons for digital resources; not least a lack of awareness of how to cite them (Sukovic, 2009), and disciplinary unwillingness to acknowledge their usage (Meyer, 2009). As a result, the traditional method of mining only citations provide an unreliable picture of citation levels of digital resources.Furthermore, there is a need to account for the varied local and national context within which researchers operate. In Canada, for instance, there is a feeling among scholars that the \"limited and fragmented\" (Kheraj, 2014) newspaper digitisation programme lags behind nations such as the USA, Australia and United Kingdom. The prominence of The Toronto Star and The Globe and Mail mirrors the early years of newspaper digitisation in the United Kingdom, where the ubiquity of the Times Digital Archive encouraged some to overstate its representativeness (Bingham, 2010). Contemporary digitised newspaper resources in the UK by contrast, tend to aggregate dozens of newspaper titles into a single resource. This paper therefore explores the likelihood that this aggregation process may have caused different citation patterns among UK researchers, who are the largest group to access these resources (Gooding, 2014), providing a comparison with the differing Canadian context presented by Milligan. MethodologyTo achieve this, I will focus on newspaper titles from two British Library resources: British Library Nineteenth Century Newspapers (BNCN) and The British Newspaper Archive (BNA). I intend to identify mentions of newspapers by title within the full text of UK history theses. The dataset comes from EThOS, a national service which makes UK doctoral theses available online for searching and reading. EThOS contains approximately 440,000 records relating to theses awarded by over 120 institutions. It provides a comprehensive, systematically collected dataset for comparison of citation trends over time. Around 160,000 records provide access to searchable full text, and I have worked with British Library Labs to identify a subset of history theses published from 1999 to 2015; in total, over 6,000 theses were identified using Dewey Decimal Classifiers, covering eight years before and after the launch of BNCN in 2007. These theses will be searched using Natural Language Processing techniques to identify the incidence of specific newspaper titles that are included in BNCN and BNA, allowing me to identify how many theses use a given source, and how frequently each source was used.This project also acts as a test case for text mining of British Library collections. In 2014, the UK government introduced an exception to copyright law to ensure that researchers undertaking text and data-mining for non-commercial purposes would no longer infringe copyright (Intellectual Property Office, 2014), without requiring that publishers took steps to guarantee the availability of suitable datasets for text mining. In reality, this means that there are many potentially valuable data sources that could be legally studied, but no infrastructure to do so. Starting with the British Library's PhD thesis holdings, I intend to work with British Library Labs to explore the possibility of opening up further datasets for text mining. ConclusionThis paper will explore the ways in which historical newspaper digitisation have impacted upon historiography among early career researchers in the United Kingdom, by tracking citations of digitised newspaper titles over time in the full text of over 6,000 PhD theses. This is the first study to apply text mining of digital resource citations to the UK context. It also provides an important case study of text and data mining in legal deposit library collections, in the light of current limitations to the UK copyright exception. In doing so, this project will not only illuminate the ways in which digital resources can affect local research practices, but will demonstrate the utility of text mining in addressing the methodological limitations of citation analysis for digital resources. We must adopt new computational methods to ensure that the traces of this use are not wiped away by citation practices which continue to de-emphasise the role of digital resources in contemporary research.  ",
        "article_title": "\"A Trace of this Journey\": Citations of Digitised Newspapers in UK History PhD Theses",
        "authors": [
            {
                "given": "Paul",
                "family": "Gooding",
                "affiliation": [
                    {
                        "original_name": "University of East Anglia",
                        "normalized_name": "University of East Anglia",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/026k5mg93",
                            "GRID": "grid.8273.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Introduction and related worksIn recent years, the application of network analysis methods to literary texts has evolved into an independent research field of digital literary studies. Methods for the automated extraction of network data (named entity recognition, co-reference resolution) and their evaluation are of particular importance (El- son et al. 2010;Park et al. 2013;Agrarwal et al. 2013;Fischer et al. 2015; Waumans et al. 2015;Jannidis et al. 2016). Based on the data obtained, several types of analyses were developed: an empirical, quantitative and hierarchical description of literary characters ( Jannidis et al. 2016), corpus-based analyses exploring options for historical periodisation of literature ( Trilcke et al. 2015) and types of aesthetic modelling of social formations in and by literary texts (Stiller et al. 2003;Stiller & Hudson 2005;Trilcke et al. 2016).What has been neglected so far (although already suggested by Moretti 2011) is the application of network analysis as a tool for quantitative plot analysis. In fact, current approaches in the field of literary network analysis are not suited to gaining insights into the plot development of literary texts ( Prado et al. 2016). The sequential dimension of literary texts, as a consequence of their temporality, usually remains in the dark: what is extracted, visualised and analysed are static networks. Plot, however, is essentially a concept supposed to theoretically encompass the temporality of narrative (as well as dramatic) texts: \"the repeated attempts to redefine parameters of plot reflect both the centrality and the complexity of the temporal dimension of narrative\" (Dannenberg 2005: pp. 435). Plot can be understood as a concept for the description of the \"progressive structuration\" (Kukkonen 2013: §4) of literary texts. Research objective: plot analysisAttempts to further develop literary network analysis towards a quantitative plot analysis must consider the temporal dimension in the modelling of their research objects. The structure of a literary text is to be modelled as a changing sequence of network states. It is only through looking into these network dynamics that we can discuss network-analytic approaches for a quantitative plot analysis.Following Prado et al. 2016, we are currently extending our research on literary networks (Trilcke 2013;Fischer et al. 2015ff.; Fischer et al. 2015;Trilcke et al. 2016) to the analysis of progressive structuration. Our goal is to examine whether (and with what kind of limitations) we can flesh out an operationalisation for the plot analysis of literary texts. By doing so, we are, of course, not trying to replace the semantically rich and versatile concept of 'plot' with a quantitative and thus reductionist concept. Rather, we will show that certain aspects of what is commonly discussed within the framework of plot analysis can be retraced by means of a computer-based analysis of network dynamics.The visualisation of dynamic graphs, as is common in other domains ( Pohl et al. 2008;Frederico et al. 2011), has recently been transferred to literary networks ( Xanthos et al. 2016). While it may come in useful when close-reading a text and for didactic purposes, it is unsatisfactory when it comes to an actual corpus-based analysis. There are no canonical methods to help us compare network visualisations generated automatically by force-directed graph drawing algorithms. The reception of dynamic visualisations just does not offer practical analytical means. From a dedicated distant-reading kind of perspective, the calculation of dynamic network metrics and their statistical processing is much more promising as it offers options to describe general characteristics of networks from a larger corpus as well as to compare specific formal types of networks within the corpus. Measuring dynamic literary networksA number of basic global measures for the analysis of dynamic networks (i.e., size, density, homogeneity in the distribution of ties, rate of changes in nodes, rate of changes in ties) has been discussed by Carley (2003: pp. 135-36). In addition, Prado et al. 2016 recently suggested the application of actor-oriented measures, especially centrality indices, for the reconstruction of plot development. We are currently applying these measures to our own corpus, consisting of 465 German language plays. We also developed a set of other measures with recourse to traditional theoretical concepts for the description of specific phenomena of plot development, especially with regard to types of dramatic expositions (Pfister 1977: pp. 124-36), the \"classical\" act structure of tragedy and the composition principle of main and secondary plots (Pfister 1977: pp. 286-89). Calculation of these measures was implemented in our own network analysis tool dramavis (Kittel/Fischer 2017). Event-based measuresIn general, two types of measures can be distinguished for describing dramatic texts as dynamic networks: event-based and progression-based measures. Event-based measures are used to identify or characterise a particular point in time within a drama. In this context we developed an all-in index, a value that identifies the point in time at which all characters have occurred at least once in a drama (see Figure 1).  selected playsThe final-scene-size value characterises a specific point in time, in this case the last scene of a drama. It determines the percentage of all characters of a drama which appear on stage in the last scene. This value shows characteristic differences between dramatic subgenres, especially when comparing tragedies and comedies (see Figure 2).  Progression-based measuresWhile event-based measures allow assumptions about a particular state/point in time of the dynamic network, progression-based measures allow a general characterisation of the transformation of a dramatic network. In this regard, we introduced a measure we call the drama-change rate. The basis of our calculation is a modified Levenshtein distance, which only takes into account insertions (\"add character\") and deletions (\"delete character\"). In each case, we compare characters present in two consecutive scenes, eventually resulting in what we call the segment-change rate (Figure 3). The sum of all segment-change rates of a drama divided by the number of all segment-change rates is what we call the drama-change rate. The development of this rate can be represented in a chart, which due to its shape we tentatively called the beat chart ( Figure  4). Having calculated the drama-change-rate value for the entire corpus, we can start to compare a larger set of dramas with each other ( Figure 5). It becomes evident that our corpus does not exhibit a clear trend along the timeline. Instead, we witness the emergence of characteristic types of dramas, which differ characteristically from the other texts in our corpus. On the one hand, we can identify dramas exhibiting a high drama-change rate, i.e., highly dynamic dramas with a constant alternation of characters on stage (Fig- ure 6). On the other hand, low-dynamic dramas can be identified, with only small changes taking place between scenes (Figure 7). A further option for the comparative analysis of our oscillatory beat charts is the analysis of the standard deviation of all segment-change rates of a drama. We can again distinguish two particularly striking types: On the one hand, there are dramas with a high standard deviation, indicating that extensive changes of characters alternate with small changes; we can call them high-dynamic dramas (Figure 8). On the other hand, there are dramas showing a low standard deviation, so the change on stage takes place in the same rhythm, we call these texts low-dynamic dramas (Fig- ure 9). The preceding cases each describe characteristic deviations regarding the drama-change rate of groups of texts. In addition, a 'normal type' of drama can be identified, which corresponds to the mean value for the corpus both in terms of arithmetic mean and standard deviation (Figure 10). It also appears that strong upward shifts, accompanied by a complete exchange of characters on stage, often coincide with act boundaries (vertical orange lines), which is in accordance with historical conventions of dramatic composition. Summary and further researchOur research on dynamic networks provides basic components for a quantitative analysis of the progressive structuration of dramatic texts. Future research will have to develop and evaluate additional measures and it will be decisive to hold interpretations of these measures against the backdrop of historical drama poetics.",
        "article_title": "Network Dynamics, Plot Analysis: Approaching the Progressive Structuration of Literary Texts Introduction and related works",
        "authors": [
            {
                "given": "Frank",
                "family": "Fischer",
                "affiliation": [
                    {
                        "original_name": "National Research University Higher School of Economics",
                        "normalized_name": "National Research University Higher School of Economics",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/055f7t516",
                            "GRID": "grid.410682.9"
                        }
                    }
                ]
            },
            {
                "given": "Mathias",
                "family": "Göbel",
                "affiliation": [
                    {
                        "original_name": "Göttingen State and University Library",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Dario",
                "family": "Kampkaspar",
                "affiliation": [
                    {
                        "original_name": "University of Graz",
                        "normalized_name": "University of Graz",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/01faaaf77",
                            "GRID": "grid.5110.5"
                        }
                    }
                ]
            },
            {
                "given": "Herzog",
                "family": "Library",
                "affiliation": [
                    {
                        "original_name": "University of Potsdam",
                        "normalized_name": "University of Potsdam",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03bnmw459",
                            "GRID": "grid.11348.3f"
                        }
                    }
                ]
            },
            {
                "given": "Germany",
                "family": "Kittel",
                "affiliation": [
                    {
                        "original_name": "University of Potsdam",
                        "normalized_name": "University of Potsdam",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03bnmw459",
                            "GRID": "grid.11348.3f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn paleography, scholars study the history of handwriting, a crucial aspect of book history and manuscript studies. Paleography has traditionally been dominated by expert-based approaches, driven by the opinions of a small group of highly trained individuals. These have acquired a set of expert skills through year-long training, e.g. the ability to date a handwriting or attribute it to specific individuals. This knowledge remains very hard to render explicit, in order to share it with others. Therefore, paleographers are increasingly interested in digital modelling techniques to enhance the creation and dissemination of paleographic knowledge (Stutzmann, 2015). An important task in paleography is the classification of script types, especially now that digital libraries (BVMM, Gallica, e-Codices, Manuscripta Mediaevalia, etc.) are amassing reproductions of medieval manuscripts, often with scarce metadata. Being able to recognize the script type of such historic artefacts is crucial to date, localize or (semi-)automatically transcribe them. This paper focuses on script identification for medieval Latin manuscripts (ca. 500 AD to 1600 AD) and demonstrates the feasibility of a fairly accurate, meaningful automated classification.Medieval script classification was the focus of the recent CLaMM (Classification of Latin Medieval Manuscripts) competition. For this shared task, the organizers released a training data set of 2,000 photographic (greyscale, 300 dpi) reproductions of pages extracted from Latin manuscripts, which were classified into a 12 script type classes, including uncial, caroline, textualis and humanistic script, but also more difficult to delineate classes such as the cursiva or (semi)hybrida. The participating teams had to submit a standalone application which was able to classify unseen images and estimate the distance between them. The organizers would then apply the submissions to 1,000 resp. 2,000 test images ( Stutzmann, 2016) and evaluate their performance using various evaluation schemes. Here, we discuss the DeepScript submission to the CLaMM competition. The competition's results have been officially been released on 26 Oct. 2016. DeepScript was ranked first on task 2, i.e. the 'crisp' classification of mixed script images ( Cloppet et al., 2016). As the ground truth and results were released too recently, we limit this abstract to a general discussion of the approach; the final version and presentation of this paper will be supplemented with additional information and test results.The DeepScript submission builds upon recent advances in Computer Vision, where the use of so-called 'deep' neural networks has recently led to dramatic breakthroughs in the state of the art of image classification ( LeCun et al., 2015). The kind of neural networks applied in Computer Vision are typically convolutional: they slide small 'filters' (feature detectors) across images to make the network robust to small translations of objects. The networks make use of many 'layers' of such feature detectors, where the output of one feature detector always feeds into the next one. The use of such a stack of layers is beneficial, because this 'deep architecture' allows algorithms to model features of an increasing complexity ( Bengio et al., 2013): in the first layers of the network, very raw and primitive shapes are detected ('edges'); it is only at the higher layers in the networks that these primitive features are combined into more complex, abstract visual patterns (e.g. entire faces). These neural networks lie at the basis of e.g. modern face verification algorithms on social media websites such as Facebook.Neural networks are composed of millions of parameters which have be optimized. For this, the available training data is split out in a set of training images and a smaller set of development images (respectively ca. 1,800 and 200 images): the former is used to optimize the parameters of the network during training, the latter is used to monitor the performance of the network. The use of development data is necessary to avoid 'overfitting': it is possible for a network to start 'memorizing' the training images, so that it produces perfect predictions for the training data, but is not able any more to generalize properly to new, unseen images. By using a development set, we can stop optimizing the network, if its predictions for the development data do not increase in quality anymore. Only at this stage, the algorithm is evaluated on the actual test images.Modern neural networks are typically trained on hundred thousands of training images. In the field of Cultural Heritage data, a common challenge is that most data sets are much smaller, and CLaMM is no exception, so that the danger of overfitting is much larger. We therefore proceeded as follows: the generous resolution for each training image was downsized by one half. Next, we would select random square crops or patches from the image (150x150 pixels) and train the algorithms on batches of these crops. This approach is blunt, yet innovative, since we make no effort to extract more specific regions of interest from the images, such as individual lines, words or characters. To avoid overfitting, we also applied augmentation: each training crop would be 'distorted' through randomly varying the zoom level, rotation and translation. Introducing such noise in the input is a common strategy to combat overfitting. Below goes an example of such a set of augmented patches for a single manuscript page (Fig. 1). After each epoch, we evaluated the performance of the current state of the network through inspecting the classification accuracy on the development images: we randomly selected 30 crops from each image (without augmentation), and calculated the average probability for each output class. The full image was assigned to the class with the highest average probability. The best validation accuracy which we achieved was 91.17%, using a network architecture of 14 layers, inspired by the famous Oxford VGG net (Simonyan et al., 2015). The manual classification of CLaMM images was based on morphological differences and allographs, as defined in standard works on Latin scripts such as (Bischoff, 1986;Derolez, 2003). The confusion matrices in Fig. 2-4 for the actual and the predicted classes in the development and test data illustrate that the confusions generally make sense from a paleographic point of view (the normal textualis letter is for instance often mistaken for the Southern or semi-textualis variant).   : Classifications for the test data as a confusion matrix(task 1). Horizontal lines: ground-truth; Vertical columns: predictions. Order: 1-Uncial; 2-Half-uncial; 3-Caroline; 4-Humanistic; 5-Humanistic; Cursive; 6-Praegothica; 7-Southern Textualis; 8-Semitextualis 9-Textualis; 10-Hybrida 11-Semihybrida 12-Cursiva. There exist interesting methods to visualize which patterns the trained network is sensitive to. Using the principle of gradient ascent, we start from a random noise image and feed it to one of the filters on the last convolutional layer: during 3,000 iterations we change the image so that it maximizes the activation of this particular filter. In Fig. 3, we show the 25 artificially generated images which yielded the strongest results; clearly, the network picks up relevant patterns. The third image from the left in the first line, for instance, clearly captures the presence of loops in the ascenders of individual characters (e.g. in the 'b' or 'h', which is crucial to differentiate between e.g. a textualis and a cursive letter). These visualizations directly tackle the issue of the computational 'black box' in the Digital Humanities, and espsecially Digital Palaeography ( Hassner et al., 2013;Stutzmann et al., 2014). In our paper, we will offer further interpretations and visualizations of our model and confront these with the results from other participants in the CLaMM competition to offer new perspectives on the graphic definition of script classes in traditional paleography.",
        "article_title": "Script Identification in Medieval Latin Manuscripts Using Convolutional Neural Networks",
        "authors": [
            {
                "given": "Mike",
                "family": "Kestemont",
                "affiliation": [
                    {
                        "original_name": "Centre National de la Recherche Scientifique",
                        "normalized_name": "Centre national de la recherche scientifique",
                        "country": "Morocco",
                        "identifiers": {
                            "ror": "https://ror.org/00675rp98",
                            "GRID": "grid.423788.2"
                        }
                    },
                    {
                        "original_name": "University of Antwerp",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            },
            {
                "given": "Dominique",
                "family": "Stutzmann",
                "affiliation": [
                    {
                        "original_name": "Centre National de la Recherche Scientifique",
                        "normalized_name": "Centre national de la recherche scientifique",
                        "country": "Morocco",
                        "identifiers": {
                            "ror": "https://ror.org/00675rp98",
                            "GRID": "grid.423788.2"
                        }
                    },
                    {
                        "original_name": "University of Antwerp",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Wilhelmus has been the official national anthem of the Kingdom of the Netherlands since 1932. The song carries a wider relevance that extends well beyond the Low Countries. According to the authoritative Guinness Book of Records, the Wilhelmus is the national anthem with the oldest music in the world: we are able to date the tune and text to the years 1568-1572 during the Dutch Revolt, a key episode in the history of the Early Low Countries. Moreover, in the song's fifteen couplets, an anonymous poet has immortalized a dramatic internal monologue of William the Silent, Prince of Orange (1533 -1584), a wellknown figure who has played a decisive role in the political history of Europe ( Van Stipriaan, 2007).In the earliest sources, the Wilhelmus has invariably survived anonymously, in print collections of rebel songs (the so-called geuzenliederen or 'beggar songs') that date back to the Spanish Occupation in the Low Countries (De Bruin, 1998). Only some of these songs are attributed to known authors; the majority, including the Wilhelmus, are not. Apart from the supposed date of composition (1568)(1569)(1570)(1571)(1572), there are few historical facts that could help the attribution. Although the Wilhelmus does not explicitly choose sides in contemporary religious conflicts, circumstantial evidence strongly suggests that the text was written by an author of Flemish or Dutch descent, who was living in a German refugee community at the time, perhaps in the vicinity of Heidelberg, because of a number of striking intertextual connections to other songs that were composed in that area.Ever since its creation in the late sixteenth century, the attribution of the song has not ceased to puzzle scholars as well as other inhabitants of the Low Countries. Only decades after the song's composition, there seems to have been considerable confusion already: in various sources, we find widely divergent attributions of the song to a number of famous authors, such as Marnix of Saint Aldegonde (the mayor of Antwerp, during the city's famous Fall in 1585) or the religious author and philosopher Dirck Coornhert. Many other candidate authors would be suggested in the next centuries, the credibility of which could vary strongly. In the public opinion, Marnix has long remained the most popular candidate, although scholars have never reached any definitive agreement on the issue. As late as 1996, for instance, an entire doctoral thesis was devoted to the authorship of the Wilhelmus. In this thesis, Maljaars predominantly argued that Marnix could not have been the author, relying on traditional evidence: the results of the close reading of the Wilhelmus, and comparison between the Wilhelmus and other texts by the presumed author(s).In 2016 an interdisciplinary team of scholars has tackled this age-old issue from a new perspective: stylometry. For most of the candidate authors which have been suggested for the Wilhelmus, we have available relatively sizable oeuvres of lyrical poems or even highly similar songs. The comparison of the Wilhelmus to those reference oeuvres, using state of the art stylometric methodologies, should allow us to estimate the relative distance from the anthem to each candidate author (authorship attribution) and verify their authorship (authorship verification). Many issues, however, make this comparison far from trivial: the texts are short (the Wilhelmus only counts 500 words), we only know younger, potentially corrupted versions of the texts and rarely have autographs, the spelling of the material is highly unstable etc. We have tried to tackle the latter issue through part-of-speech tagging and lemmatizing the texts ( Kestemont et al., 2016b): instead of performing measurements on the original surface forms, we would restrict our analyses to the most frequent tag-lemma pairs (MFTLPs), which normalize the spelling of tokens.In this paper, we will report several authorship experiments, using both the attribution and the verification setup ( Kestemont et al., 2016a), in which we have compared the Wilhelmus to a representative set of contemporary authors, among which the main candidate authors as well as some background authors that merely served as 'distractors' or 'imposters'. We include a small selection of these below. Surprisingly, these experiments without exception pointed towards an obscure, vilified author who has never even been mentioned as a candidate author: Petrus Dathenus (ca. 1531-1588). The first series of plots are rather naive Principal Components analyses (300 MFTLPs) which each confront textual samples by two candidate authors and the Wilhelmus (in white). In these binary comparisons, the Wilhelmus is attributed to Dathenus without exception. The same goes for the verification experiment (which runs entirely parallel to the experiments run on the Caesarian corpus in Kestemont, et al. 2016a): when compared to both target and imposter authors, the Wilhelmus is significantly closer to Dathenus's texts than to any other candidate author from this period for which we have texts available.  Dathenus is primarily known as the author of a complete Dutch adaptation of the Psalms, which became extremely influential in the second half of the sixteenth (and which is in fact still sung today in some reformed communities). His contemporaries considered him a great and dangerous orator because of his convincing way with words. Nowadays, Datheen has the reputation of being a very poor poet. To what does he owe this bad reputation? Our present-day image of the man goes back to the late eighteenth century when the pressure grew to have his Psalm adaptation replaced by a more modern one in churches. In order to increase the pressure on Dathenus's Psalms, people started mocking the poet through the dissemination of caricatures in which the man would even be depicted with donkey ears (see Fig. 3). It is striking how strongly our present-day view of Dathenus is still determined by the highly anachronistic eighteenth image of this author, instead of that of the respected and influential individual he was known to be in his own time. In this paper, we will re-assess the sparse, historical evidence that is available for Petrus Dathenus and show that he is, in fact, an unusually strong authorial candidate for the Wilhelmus. Here, we limit our discussion to a single new fact that recently emerged. The Wilhelmus is a so-called contrafact: the song has been composed by writing a new set of lyrics for an already existing melody, a very common practice in early modern song culture. The original melody which was used for the Wilhelmus was a French song: O la folle entreprise du Prince de Condé. Musicologist have been able to pinpoint when this song was created: it must have been composed (as a Protestant song) during the Siege of Chartres in 1568. The tune must have been introduced in the Low Countries via the Wilhelmus and was not known beforehand. Therefore, it has always puzzled scholars how the Wilhelmus author might have been exposed to this French tune. Intriguingly, it turns out that Dathenus must have been present at the Siege of Chartres as a field preacher on the protestant side. Thus, although he has never made it to the official candidate list, Dathenus is in fact the only candidate, who not only has the right stylistic profile, but of whom we also argue that he was directly exposed to the base tune of the Wilhelmus.In our paper, we will not go as far as to claim that the neglect of Petrus Dathenus as a potential candidate author for the national anthem of the Netherlands has been an ideological 'cover up operation'. We will discuss, however, the anachronistic biases and prejudices which so far have prevented the identification of Petrus Dathenus as a potential candidate author. From the point of Digital Humanities, it is important to stress that we base this research on a bold computational attribution to an author who, at first sight, seems a highly unlikely candidate; a human expert would never even have dared to think of this attribution. Nevertheless, exactly because machines do not carry the same set of preconceptions as humans, the application of stylometry is able to induce serendipity in humanities research and open up new perspectives.Fig. 1 :1Fig. 1: Naive 2-dimensional PCA plots in which textual samples by two authors (Datheen vs Marnix; Datheen vs Heere) are confronted, including the Wilhelmus. Fig. 2 :2Fig. 2: Cluster map for the verification results obtained for the Wilhelmus and a number of highly relevant candidate authors (Kestemont et al. 2016a). Fig. 3 :3Fig. 3: A late eighteenth century caricatural depiction of Petrus Dathenus with donkey ears, to symbolize his alleged poetical ignorance. ",
        "article_title": "Did a Poet with Donkey Ears Write the Oldest Anthem in the World? Ideological Implications of the Computational Attribution of the Dutch National Anthem to Petrus Dathenus",
        "authors": [
            {
                "given": "Mike",
                "family": "Kestemont",
                "affiliation": [
                    {
                        "original_name": "University of Antwerp",
                        "normalized_name": null,
                        "country": "Belgium",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Els ",
                "family": "Stronks",
                "affiliation": [
                    {
                        "original_name": "Utrecht University",
                        "normalized_name": null,
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Martine",
                "family": "De Bruin",
                "affiliation": [
                    {
                        "original_name": "Meertens Instituut - Royal Netherlands Academy of Arts and Sciences (KNAW)",
                        "normalized_name": null,
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Tim ",
                "family": "de Winkel",
                "affiliation": [
                    {
                        "original_name": "Utrecht University",
                        "normalized_name": null,
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Corpora and ComparisonsFor ease of references, we assigned an acronym for each of the 14 corpora, and show their names in Chinese (Collection) and periods of publication (Time) in Table 1. The corpora consist of representative literature that has been published since 1046BC. In particular, we have at least one collection for each of the major dynasties that existed before 1644AD. The majority of our collections are of poetic works (we consider SJ, CV, HF, PT, CTP, CSP, CSL, YSX, and LCSJ collections of poetic works) which fact lends itself to the study of the effects of genres on the character distributions. A collection of poetic works for the Qing Dynasty (which lasted from 1644 to 1912AD) is unavailable because an editorial committee is still working on its production (Zhu, 1994).The corpora contain more than 42 million characters, excluding the punctuation marks that were added into the corpora by the data providers. When counting the characters, we also exclude characters that cannot be shown on ordinary computers. The frequencies of such rare and obsolete characters are not large, so ignoring them will not affect the statistical properties reported in this study.Only the ASBC was segmented and the segmentation was verified by human experts. Hence, we can inspect its character and word distributions. The other corpora were written in classical Chinese and we do not have a reliable way for segmentation, so we will only analyze the character distributions.We created charts that are based on the typical form of Zipf's law:( ) ) ( log ) ( log w r k N w f α − = ⎟ ⎠ ⎞ ⎜ ⎝ ⎛where w, f(w), and r(w) denote a word, its frequency, and rank in a corpus, respectively. The rank of the most frequent word in a corpus is 1. N is the size of the corpus, and k and 񮽙 are constants. Observations and Discussions: Influences of Genres and EpochsThe generalizability of the Zipf's law is the main reason that it has attracted the attention of many researchers. It can be applied to various natural distributions including those of part-of-speech of words ( Wang et al, 2012), city sizes ( Anderson and Ge, 2005), and corporal revenues (Chen et al, 2008). Figure 1 shows the Zipfian curves when we consider the character distributions of all of the 14 corpora. We intentionally plot the curves in one chart, although this makes the individual curves undistinguishable. Although the curves are not linear, which is common as reported in the literature, the curves show a consistent trend, suggesting a common regularity that is shared by Chinese texts that were produced over the period of 3000 years. Instead of treating the 14 corpora as a single corpus to fit the resulting distribution for Zipf's law, we examined the curves to investigate possible factors that influenced the positions of the curves. In Figure 2, we show the curves of lyrics (\"詞\" /ci2/) and poems (\"詩\" /shi1/. The left halves of the curves overlap almost perfectly, which strongly indicates that the poetic works share very close statistical characteristics. Table 2 lists ten most frequent characters found in each of the corpora and for which the curves are plotted in Figure 2. The lists are very similar, and, out of the 60 characters in Table 2, there are only 16 distinct characters (some of which are homonyms, for which only one pronunciation is provided). In fact, we can compare the most frequent characters of any two corpora, e.g., the CTP and the CSP, to further investigate their similarity (Chen et al, 2012), and we found that the most frequent 1700 characters in the CTP and the CSP are the same characters.Not all of the corpora of poetic works have similar curves. We added the curves for the SJ, CV, and HF in Figure 3, and it is evident that these new curves do not overlap with those in Figure 2 very well. The poetic works in the SJ, CV, and HF were produced very much earlier than those listed in Figure 2. While the time of the production of the corpora affects the Zipfian curves, the curves for corpora that were produced in the same dynasty may not be the same. The CTW, MZM, and CTP are three different types of works that were all produced in the Tang Dynasty. We compared the most frequent characters shared by the CTP and CTW, and found that the sets of their most frequent 2000 characters differ only in three characters. Despite such an extreme overlap, their curves in Figure 4 suggest that genre affects the character distributions. Given the above observations, one may have expected that the curves for the novels that were published in the 16 th and 18 th centuries, i.e., the JTTW and DRC, will deviate from the curves for the earlier poems, as the curves in Figure 5 show.  Character vs. Word distributionsWe considered the character distribution when we analyzed the contents of the ASBC in Figure 1, where we found that the character distributions of the vernacular and classical Chinese texts show a reasonable common trend. The ASBC contains documents that were written in vernacular Chinese, so we must also analyze its word distribution, and Figure  6 shows the curves for both the character and word distributions.A chart like that of Figure 6 can mislead one to infer that Chinese texts do not conform to Zipf's law. It is well accepted that the number of Chinese characters is limited, although there is no consensus about the exact number of characters. In contrast, there is virtually no limit on the number of legal Chinese n-grams. As a result, the sharp downturn of the character distribution and the intersection of the two curves in Figure 6 are expected, and this can be observed in languages other than Chinese in some special settings (Montemurro, 2001). We should examine the Zipfian curves on the same basis, e.g., character or word, while considering cultural factors that may influence actual language usage.  Concluding RemarksWe have judged the similarity between the Ziphian curves based on the visual closeness, though we can quantify the degree of similarity when desired (Hu and Kuo, 2005). Researchers have noticed the deviations of Zipfian curves at the high- and low-frequency ends ( Hu andKuo, 2005, Rousseau andZhang, 1992) and tried to find density functions that fit the data. The statistics at the high-frequency ends of the curves are evidently more reliable. We focused on the deviations at the high-frequency ends of the curves, and discussed how the deviations in these regions may relate to the genres and epochs of the corpora, employing the lists of most frequent characters of the corpora as extra supports.",
        "article_title": "Character Distributions of Classical Chinese Literary Texts: Zipf's Law, Genres, and Epochs Introduction and Main Findings",
        "authors": [
            {
                "given": "Chao-Lin",
                "family": "Liu",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            },
            {
                "given": "Shuhua",
                "family": "Zhang",
                "affiliation": [
                    {
                        "original_name": "Harvard University",
                        "normalized_name": "Harvard University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03vek6s52",
                            "GRID": "grid.38142.3c"
                        }
                    }
                ]
            },
            {
                "given": "Yuanli",
                "family": "Geng",
                "affiliation": [
                    {
                        "original_name": "Harvard University",
                        "normalized_name": "Harvard University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03vek6s52",
                            "GRID": "grid.38142.3c"
                        }
                    }
                ]
            },
            {
                "given": "Huei-Ling",
                "family": "Lai",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            },
            {
                "given": "Hongsu",
                "family": "Wang",
                "affiliation": [
                    {
                        "original_name": "Harvard University",
                        "normalized_name": "Harvard University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03vek6s52",
                            "GRID": "grid.38142.3c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " With many ongoing debates (Gold, 2012) and \"unwritten\" histories ( Nyhan and Flinn, 2016), the research practice of the Digital Humanities (DH) has been around for 70 years. Many works have been trying to draw general conclusions of the disciplinary structure (McCarty, 2003;Gold, 2012;Terras et al., 2013;Schreibman et al., 2016;Nyhan and Flinn, 2016), and have pointed to the potential usefulness to analyse the discipline from statistical aspects. The usefulness focuses on describing the intellectual structure, scholarly interactions and disciplinary development of DH. Some studies have dedicated their attention to these matters (Grandjean, 2016;Nyhan and Duke-Williams, 2014;Quan-Haase et al., 2015;Wang and Inaba, 2009), or have focussed on one of these topics ( Sugimoto et al., 2013), but few of them have engaged either with the bibliometric network method, or with the latest large-scale scholarly datasets to study the DH community as a whole.Therefore, to fill this gap, based on a provisional dataset that has been compiled from core DH journals, this study performs an exclusive all-author co-citation analysis (ACA) with the 200 most cited scholars by fractional citation count to map and demonstrate the intellectual structure and to identify the most influential scholar groups and topics within DH.To the best of our knowledge, this study is the first to apply bibliometric methods to visualise DH knowledge structure and the scholar clusters. This research output will make a valuable contribution to the current discussions and debates about DH knowledge structure and wider scholarly networks. MethodologyWith ACA as the main methodology, the research contains four steps, and each with a different methodology: building a DH citation index according to the publications of these journals; selecting authors as the core objects for citation analysis; assigning scholars to different distance-based clusters by calculating the author co-citation matrix to similarity matrix ( Waltman and van Eck, 2013); finally, visualising the DH citation network which aims to show the scholar clusters, and the knowledge structure and diffusion of DH.The three DH core journals that our dataset has been constructed from are: \"Computers and the Humanities\" (CHum), \"Digital Humanities Quarterly\" (DHQ), \"Literary and Linguistic Computing\" (LLC) (now \"Digital Scholarship in the Humanities\") (DSH). The bibliographies as well as the metadata of all their publications (including the reviews and editorials etc.) published until June 2016 have been collected. It should be noted that none of these journals spanned the whole period selected (1966- 2016): CHum, the first DH journal started in 1996, and ceased publication in 2004; LLC/DSH began in 1986; DHQ began in 2007. Figure 1 shows the total publications each year from 1966 until June 2016 for these journals. Author co-citation analysis (ACA) can reveal the intellectual structure of a field from its academic publications by calculating the frequencies with which two authors are cited together. That is to say, if an article cites at least one article of author A, and at least one of author B that is different from the one of A, the co-citation count increases by 1. The more co-citations two authors receive, the more likely their publications and researches are related (Bellardo, 1980). Therefore, the clusters of related authors indicate the networks of research topics, or influential focuses within a discipline.The initial findings with the top cited 200 authors displayed on the maps (see the provisional maps in Figure 2 and Figure 3) have provisionally revealed five sub-fields within DH. Both of the maps (Figure 2 and Figure 3) are distance-based. Each node on the map represents an author, and the distance between two authors is their relations (the closer the distance, the stronger the connection). Authors are distributed quite unevenly, and this makes it easy to identify clusters of related nodes. The size of the node represents the citation count this author received, and the higher the citation count is, the bigger the node. On the density map, the density value depends on the size, number and distance of the nodes around it, so the higher the density value, the colour is more red than blue.Both maps have revealed the general structure of the scholarly communication between DH scholars via publications. Horizontally across the centre of the map, there is a loosely connected circle of five DH scholar clusters: centre (focused on \"Leech, G\"), top (focused on \"Miller, G\"), bottom (focused on \"Nerbonne, J\"), left (focused on \"Holmes, D.I\"), and right (focused on \"McCarty, W\"). The clusters distribution on the density map reveals that there is a clear separation between top, centre, right clusters to left and bottom clusters. Especially the right cluster (focused on \"McCarty, W\") and the left cluster (focused on \"Holmes, D.I\") turn out to be denser than other clusters. This shows that these two clusters are more significant and have more citation influence. According to the provisional analysis, these five clusters appear to be associated with five different DH research topics: English study at the centre; general historical literacy and information science on the right; language modelling and natural language processing at the top; statistics and text analysis on the left; computational linguistics particularly on Dutch and German speaking at the bottom. These five clusters, however, are also grouped into two different bigger groups. The English study, language modelling, and general historical literacy seem to be in one group which is more related, while the statistics and Dutch-German linguistics are also very closely related to each other. Limitations and Future studyThis research is part of the first author's ongoing PhD study, funded by UCL ORS scholarship and based at the UCL Centre for Digital Humanities. The doctoral reseach maps DH intellectual, social and environmental structures using the Invisible College model (Zuccala, 2006).There are some limitations that need to be noted, such as the citation lag time. In order to build up a citation record for co-citation, it takes around five to eight years ( Hopcroft et al., 2004). This could explain that certain recognisable authors might not appear on the maps yet. Also, because the co-citation method studies the knowledge base as its subject, the map emphases more on authors published some time ago, which might not include the \"new comers\".In the future work, the ACA study will be extended to include more citation data. The ACA study will be divided into discreet periods to construct maps of different DH development stages. Given that different journals have different topical foci, the research will also analyse individual journal to discover its attribute.",
        "article_title": "The Intellectual Structure of Digital Humanities: An Author Co-Citation Analysis",
        "authors": [
            {
                "given": "Jin",
                "family": "Gao",
                "affiliation": [
                    {
                        "original_name": "Centre for Digital Humanities - University College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Oliver",
                "family": "Duke-Williams",
                "affiliation": [
                    {
                        "original_name": "Centre for Digital Humanities - University College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Simon",
                "family": "Mahony",
                "affiliation": [
                    {
                        "original_name": "Centre for Digital Humanities - University College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Melanie",
                "family": "Bold",
                "affiliation": [
                    {
                        "original_name": "University College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Julianne",
                "family": "Nyhan",
                "affiliation": [
                    {
                        "original_name": "Centre for Digital Humanities - University College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis study focuses on Arashi ha Mujyō Monogatari (\"The Tale of Transient Popular Kabuki Actor Arashi's Life\"; 1688), a novel from the early modern Japanese literature, written by Saikaku Ihara . It is a first work of a Kabuki actor's life in Japan (Kabuki is a traditional stage arts performed exclusively by male actors with the accompaniment of live music and songs). Then we will examine the \"authorship problem\" in Saikaku's works using the tools of quantitative analysis.Saikaku was a national author whose novels were published in 17th century. Saikaku's works are known for their significance for developing Japanese contemporary novels. One recent hypothesis has stated that he wrote twenty-four novels, however, it remained unclear which works were really written by Saikaku except Kōshoku ichidai otoko (\"The Life of an Amorous Man\"; 1682), Shōen Ōkagami (\"The Great Mirror of Female Beauty\"; 1684), Kōshoku ichidai onna (\"The Life of an Amorous Woman\"; 1686), Kōshoku gonin onna (\"Love Stories about Five Women\"; 1686). Although the study of his works has continued, these fundamental doubts about his authorship remain.Meanwhile, the potential of quantitative analysis of textual data and the related field of the digital humanities have also dramatically advanced. However, quantitative analysis of Japanese classical works has been behind. It has been a problem due to complications regarding development of morphological analysis software and also delayed digitalization of Japanese classical works. Previous StudiesFound by Noma in 1941Noma found and introduced Arashi ha Mujyō Monogatari in 1941. He mentioned that the novel was actually written by Saikaku, for the following reasons (Noma, 1941 and1964). (1) The handwriting of the novel belongs to Saikaku; and (2) He found a similar writing error in Arashi ha Mujyō Monogatari and Saikaku's work.Arguments for Saikaku's authorship The handwriting is not crucial in deciding if they are Saikaku's novels. According to Emoto et al. (1996), among his twenty-four novels, the handwriting of nineteen works does not belong to Saikaku. Moreover, Saikaku made a fair copy of other writer's draft such as Kindai Yasa Inja (\"The story of a hermit\"; 1686) by Kyōsen Sairoken (? -?) and Shin Yoshiwara Tsurezure (\"The book of commentary on the licensed quarters of a certain area\"; 1689) by Sutewaka Isogai (? -?).Mori (1955) has argued that Saikaku's novels are an apocryphal work mainly written by Dansui Hōjō (1663-1711) except Kōshoku ichidai otoko.As he gained a national audience, Saikaku was pressured to write on demand and in great volume. At first he wrote only one or two novels a year, however in the two years from 1687 to 1688 he published twelve books, with a total of sixty-two volumes. Saikaku's style and approach also changed at this point (Shirane, 2004).It is possible that Saikaku had some assistance (Nakamura,1969). Arashi ha Mujyō Monogatari was published in this period. Moreover, Arashi ha Mujyō Monogatari does not have a preface, epilogue, signature, namely it is not specified that it was written by Saikaku. Despite the authorship problem of Arashi ha Mujyō Monogatari remains unanswered, little work has been done about it. For that reason, this study reexamines the authorship of Arashi ha Mujyō Monogatari using a quantitative approach. DatabasesDatabase of Saikaku's Works First, we digitized all the text of 120 works of Saikaku (24 novels, 80 poem books, etc.) based on the first edition of each works (see Figure 1). Second, Since Japanese sentences are not separated by spaces, we built the rule with early modern Japanese researchers, who were editors of Shinpen Saikaku Zenshu (\"The new complete works of Saikaku\"). Finally, based on this rule, we added spaces between the words in all of the sentences. In addition, the grammatical categories' information was added. According to our database, there are 710,355 words contained in his 120 works.  Analysis and ResultsIn our previous studies, we have analyzed Saikaku and Dansui's novels, and have clarified the following two points by extracting their writing style using principal component analysis (PCA) and cluster analysis (hierarchical clustering): (1) A comparison of the Saikaku and Dansui's novels showed ten prominent features: the grammatical categories, words, nouns, particles, verbs, adjectives, adverbs, adnominal adjectives, grammatical categories bigrams and particle bigrams (Uesaka, 2015(Uesaka, , 2016; and (2) Using these features, we analyzed Saikaku's four posthumous novels (many researchers have raised questions about the authorship, because these novels were edited and published by Dansui after Saikaku's death). We found these four posthumous works indicated same features of Saikaku's novel, therefore we concluded that these four posthumous novels belonged to Saikaku (Uesaka・ Murakami,2015ab, Uesaka, 2016.In this study, we compared Arashi ha Mujyō Monogatari to Saikaku and Dansui, as authenticated novels of them (see Table 1) by ten prominent features using PCA and cluster analysis to see the differences in each novels. The analysis revealed differences of writing style between Arashi ha Mujyō Monogatari, Saikaku and Dansui. We conducted PCA with correlation matrix and these novels fall into three groups: Saikaku, Dansui and Arashi ha Mujyō Monogatari (see Figure 2). Furthermore, we conducted a cluster analysis. There also appears to be a considerable difference among Arashi ha Mujyō Monogatari, Saikaku and Dansui's novels. When calculating distances between each novels, we normalized the frequency of each words, and used the Kullback-Leibler divergence and the algorithm from the Ward method. Furthermore, we obtained similar result of the other nine features: the grammatical categories, words, nouns, particles, verbs, adjectives, adverbs, adnominal adjectives and particle bigrams.   Nishimura(?-1969). We have been building the database of these author's 13 novels, and we will do comparisons in the future study.",
        "article_title": "Verifying the Authorship of Saikaku Ihara's Arashi ha Mujyō Monogatari in Early Modern Japanese Literature: A Quantitative Approach",
        "authors": [
            {
                "given": "Ayaka",
                "family": "Uesaka",
                "affiliation": [
                    {
                        "original_name": "Organization for Research Initiatives and Development - Doshisha University",
                        "normalized_name": null,
                        "country": "Japan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " DARIAH (Digital Research Infrastructure for theArts and Humanities) is, as the name implies, an infrastructure dedicated to research in digital arts and humanities. Developed under the auspices of the European Commission, it aims to organize communities in those fields, to develop interdisciplinary projects, promoting in particular the digital dimension of humanities and arts research by disseminating good practices, and providing tools and services. In legal terms, it is what is called an ERIC, that is to say a European Research Infrastructure Consortium, which is composed of national members that have come together to promote common objectives and serve common communities. Several ERICs have been created since 2009 in Europe, mostly based on a disciplinary approach, membership by several European countries, and the pooling of services.DARIAH had already a long existence as an unofficial structure since 2005, but it become fully established as an ERIC in 2014. It brings together 17 countries in Europe which makes it the biggest ERIC in terms of members, but it is also distinct because it serves a very wide community consisting of the whole of Arts and Humanities research. This breadth poses real questions regarding the notion of access. The role of all ERICs is to share tools. services, human resources, projects, software, etc. to enrich research. Doing so for a community as broad as DARIAH's creates particular challenges.Through this presentation, we wish to give an account of the specificities of this infrastructure by presenting the issues related to the notion of access that contribute to the structuring of DARIAH. Many of our pre-existing understandings of how access impacts upon humanities and arts research date from the days of the library collection, and it is common to limit the notion of access to research data. But within the framework of a research infrastructure consortium, the notion of access is made more complex. We will evidence this different paradigm here with five examples, which we will develop in turn by explaining both the constraints and the solutions that are envisaged to solve the problems encountered.According to the Cambridge Dictionary, \"access\" has two kinds of meanings: the first one concerns \"the method or possibility of getting near a place or a person\" and the second one \"the right or opportunity to use or look at something\". Both meanings are interesting in terms of creating an expanded understanding of access in field of Digital Humanities. Even if we mostly think about the second one (open access, open data, and so on), the first one shows the necessity of being near our community and highlights the fact that thanks to digital tools we are nearer and nearer despite the distance and we are able to work together being in different places.DARIAH positions itself to support an emerging research culture in ways that invoke both of these meanings, as the examples below illustrate. Managing interdisciplinarityWhen one addresses a community as broad as that encompassed by the expression \"Arts and Humanities\", itself vague regarding the disciplines that it covers, one implicitly raises the question of what access by these Communities to this hyper-infrastructure represented by DARIAH will mean. This implies first and foremost the need to define, more or less precisely, what is meant by the expression \"Arts and Humanities\", and in particular the expression \"Humanities\", which varies across European languages and across places and times. For example, are the social sciences included or not? And on what grounds can you bring together researchers from communities as diverse as literature, history, philosophy, cinema studies and perhaps even geography and linguistics? To be useful, DARIAH needs to develop a common language with common services and tools which can be used by people in those different fields. In this case, access concerns the way DARIAH communicates with its communities and the projects it launches, to encourage interdisciplinarity without seeming intrusive. Indeed, this infrastructure has as one of its objectives to contribute to organizing a gigantic network of specialists in those fields. To do that, it is necessary to think about how to create such a network, to animate it and to make it last. Several initiatives are being developed, both thematic and national, which we will present as responses to this challenge. Managing tensions between national and international perspectivesAccess also has a political dimension. When the same tool is developed in parallel in two different countries, how can we know how to assign credit for the development? Which should be valued by DARIAH? There is thus a problem of selection, and therefore a possibility of bias, in the choice to favor promoting the access to one tool rather than another. To resolve this tension, the DARIAH community is coordinated around National Representatives who are engaged in complementary, rather than competitive, work. This work is based in particular on the dissemination of information about the activities of the national teams and the projects in which they are involved.On the other hand, some teams may wish to retain the rights or control of their tools and may not wish to make them accessible to other communities without compensation. We will see how this constraint contributes in turn to the structuring of the DARIAH ERIC.The question of access thus raises political and diplomatic problems that may interfere with more neutral criteria of quality of the tool, its durability, its usefulness. Speaking to whom?The role of DARIAH is also very broad insofar as this European consortium has an ancillary mission for the development of new communities. But this mandate is very vague. Do we mean fields of research? Specific countries? Or people inside? In this section, we would like to focus on people. Toward this end, DARIAH has specific functions in terms of teaching digital practices, not only within the current network but also beyond it, with the goal of opening and expanding it to encompass researchers (including under and postgraduate students) who may or may not have any competences in digital tools but who are interested in them. In this way DARIAH acts as a facilitator to help people to use digital tools and services.One question that we must ask, however, is whether we can or should open our community to, for example, private companies, which would help in the development of tools and / or which would benefit, once again, from inclusion. The wider the access, the less control there is. And what would DARIAH mean and how would it act if the infrastructure became open to all without distinction of disciplines, places, people? Conversely, what would be its meaning, if by privileging shared access to knowledge and tools, it decided to close the door to some? Imperatives toward democratizing the benefits DARIAH can bring come at such junctures into conflict with the possibility that too much access could dilute the infrastructure's effectiveness, distort its scale or divert its mission. Again, this is an aspect related to the question of access to which the infrastructure must respond and on which we shall give a few quick lines of reflection. Managing toolsThis point is particularly well recognised within the DH community, since it questions the interoperability of tools. For DARIAH, questions regarding managing access to tools arises in terms of languages, content, formats and, of course, sustainability. Within DARIAH, the issue of interoperability is paramount, to leverage our large scale, but also to enable disciplinary practices usage models; given that the research questions posed at the origin of these uses will vary so considerably.Specific attention is therefore paid to this aspect and in particular to data hosting. One of the first tasks that DARIAH has set itself is to work on long-term data hosting. To do this, it has, for example, relied on national hosts able to also integrate data from multiple countries. These include the CNR in Italy (via the PAR-THENOS project) and Huma-Num in France.This perspective on access reflects as well the importance of the trust that must be established between the partner countries, in particular with regard to intellectual property, as suggested in the next and final point. Building collaborative toolsDARIAH is an infrastructure that brings together 17 countries and a range of diverse disciplinary communities. In this sense, it involves collaborative work that relies mainly on the use of digital tools. But one problem remains: the too easy access to collaborative tools developed by companies that do not share the same conception of intellectual property and data security. Tools such as Google doc and Google drive, etc. are unavoidable in the context of collaboration between researchers, but the access in this case is so easy that their existence prevents the development of alternative tools that correspond more closely to the specificities of scientific exchanges. It is now important to develop virtual working environments conducive to scientific exchanges and the needs of researchers, particularly in the communities concerned.To enhance its ability to navigate the many requirements of access, DARIAH has recently launched a farreaching 3-year program of actions, which will be presented as well as an example of how a holistic approach to access can be manifested in an institutional strategy. By explaining how the document has been formulated and how community support for it has been developed, the presentation will give a worked example of how access can be negotiated across countries and disciplines.The notion of access lies at the heart of the issues dealt with by infrastructures such as DARIAH, as they seek to structure and facilitate coordination and exchange of tools, and the development of research. Questions of access, which are too often reduced to the management of the data, imply each time a positioning; And that even when the stated objective is to be open to all, it is nonetheless subject to a form of choice.",
        "article_title": "How the notion of access guides the organization of a European research infra- structure: the example of DARIAH",
        "authors": [
            {
                "given": "Suzanne",
                "family": "Dumouchel",
                "affiliation": [
                    {
                        "original_name": "DARIAH",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionWe start from the assertion that coding and codeas the source code of computer programs that is readable to humans and which drives the performative nature of software ( Ford 2015, Hiller 2015)-can be inherent parts of scholarship or scholarship by and of themselves. That is: we assert that code can be scholarly, that coding can be scholarship, and that there is little difference between the authorship of code or text (Van Zundert 2016). The dichotomy that has been often sought between on the one hand a 'pure' intellectual realm associated with scholarly writing and academic print publication, and on the other hand the 'material labour' associated with for instance instrument making or programing, is artificial.We argue the validity of this assertion along Burgess and Hamming (2011) and Clement (2016). These scholars refer to earlier work in which Bruno Latour (1993) casts the defining characteristic of modernity as a process of 'purification' which aims to contrast the human culture of modernity to nature. Burgess and Hamming observe a congruent process in academia: \"Within the academy we see these processes of purification and mediation at work, producing and maintaining the distinction between intellectual labor and material labor, both of which are essential to multimedia production\" (Burgess & Hamming 2011: ¶11). This process serves to distinguish between scholarly and non-scholarly activities: \"The distinction between intellectual and material labor is pervasive throughout scholarly criticism and evaluation of media forms. […] In addition, any discussion of scholarly activities in multimedia format are usually elided in favor of literary texts, which can be safely analyzed using traditional tools of critical analysis.\" However, this distinction is based upon a technological fallacy already pointed out-as Burgess and Hamming noteby Richard Grusin in 1984. Grusin argued that Hypertext has not changed the nature of text essentially, as writing has always already been hypertextual through the use of indices, notes, annotations, and intertextual references. To assume that the technology of Hypertext has unvealed or revolutionary activated the associative nature of text, amounts to the fallacy of ascribing the associative agency of cognition to the technology, which however is of course a 'mere' expression of that agency.Analogous to Burgess and Hamming, we argue that relegating the evaluation of scholarship to the reviewing of print publications is an equal fallacious ascribing of agency to the technology of written text. Such a narrow understanding of scholarship presupposes that something is scholarship because it is in writing, that writing makes it scholarship.It is possible to evade all such possible technological fallacies by understanding scholarship as argument. We argue therefore that scholarship in essence is argument, and that technologies enable to shape and express that argument. This is not to say that technologies are mere inert and neutral epistemological tools, obviously different technologies shape and affect argument in different ways. Different technologies can therefore enrich scholarly argument. Scholarship is thus not bound to the use of text as an epistemological technology, but essentially is in the shaping of an argument. Text and writing may still be the most celebrated semiotic technologies to express an argument, but computer code understood as 'just another' literacy (cf. Knuth 1984, Kittler 1993, Vee 2013 can equally be the carrier of scholarly argument. However, the acceptance of code as another form of scholarly argument presents problems to the current scholarly process of evaluation because of a lack of well developed methods for reviewing and critiquing scholarly code. Digital humanities as a site of production of non conventional research outputsdigital editions, web based publications, new analytical method, and computational tools for instance-has spurred the debate on evaluative practices in the humanities considerably, exactly because practitioners of digital scholarship acknowledge that much of the relevant scholarship is not expressed in the form of traditional scholarly output. Yet the focus of review generally remains on \"the fiction of 'final outputs' in digital scholarship\" (Nowviskie 2011), on old form peer review (Antonijevic 2016), and on approximating equivalencies of digital content and traditional print publication (Presner 2012). Discussions around the evaluation of digital scholarship have thus \"tended to focus primarily on establishing digital work as equivalent to print publications to make it count instead of considering how digital scholarship might transform knowledge practices\" (Purdy & Walker 2010:178, Anderson & McPherson, 2011). As a reaction digital scholars have stressed how peer review of digital scholarship should foremost consider how digital scholarship is different from conventional scholarship. They argue that review should be focused on the process of developing, building, and knowledge creation (Nowviskie 2011), on the contrast and overlap between the representationality of conventional scholarship and the strong performative aspects of digital scholarship (Burgess & Hamming 2011), and on the medium specificity of digital scholarship (Rockwell 2011).The debate on peer review in digital scholarship however, has been geared much to high-level evaluation, concentrating for instance on the issue how digital scholarship could be reviewed in the context of tenure track evaluations. Very little has been proposed as to concrete techniques and methods for more practical level applied peer review of program code. Existing practical guidance pertains to digital objects such as digital editions (Sahle & Vogler 2014) or to code as cultural artefact (Marino 2006), but no substantial work has been put forward on how to peer review scholarly code. We are left with the rather general statement that \"traditional humanities standards need to be part of the mix, [but] the domain is too different for them to be applied without considerable adaptation\" (Smithies 2012), and the often echoed contention that digital artefacts should be evaluated as such and not as to how they might have been documented in conventional articles. The latter argument probably most succinctly put by Geoffrey Rockwell (2011): \"While such narratives are useful to evaluators […] they should never be a substitute for review of the work in the form it was produced in.\"Yet, the problem is growing more urgent. Increasingly, code is created and used as a mechanism of analysis in textual scholarship and literary studies-cf. for instance Enderle 2016, Jockers 2013, Piper 2015, Rybicki et al. 2014, and Underwood 2014-which leads to the need to evaluate the technical, methodological and epistemological qualities of such code, as for instance the 'Syuzhet case' showed (Swafford 2016). The algorithms, code, and software that underpins the analyses in these examples of scholarship are not standardized 'off the shelf' software productions. These code bases are nothing like a software package or product such as AntConc that can be viewed as a generic and packaged distributable tool; a tool that might be subject to a scholarly type of tool criticism explaining and opening it for reuse by other scholars. Instead these codebases are bespoke code: they are one-off highly specific and complex analytical engines, tailored to solving one highly specific research question based on one specific set of data. Reuse, scalability, and ease-of-use are, justifiably (Baldrigde 2015), not specific aims of these code objects at all. Such might be the case with generic software, but these programs have been algorithmic instruments tailor made to serve the research case at hand. As such-and following what was argued above-we must regard these code bases as an inherent part of the scholarly argument they contribute to. And as such they deserve and require specific and rigorous peer review, like any argument in humanities research. How such peer review should be conducted is, however, a large unknown.As a contribution to the challenges of code peer review we present an experimental technique we call defactoring. Drawing on Braithwaite (2013), we have re-configured the program code that underpins a recent article by Ted Underwood and Jordan Sellers (Underwood & Sellers 2016) into a computational narrative-echoing Knuth's literate programming (1984)-to be critically analyzed and annotated. This method is intimately intertwined with the Jupyter Notebook platform, which allows for the composition of scholarly and scientific inscriptions that are simultaneously human and machine readable. The Notebook is both a document format and a platform for mixing code and prose into executable objects. We have extracted Underwood and Seller's code and defactored it into a Jupyter Notebook, available at https://github.com/interedition/paceofchange. This means we have recombined code from disparate files, linearized the execution path, demodularized function calls, and annotated code blocks with our own expository comments. As an annotated Notebook we can now engage Underwood and Seller's code directly as a scholarly inscription and more deeply interrogate the role of data, algorithms, and code in the production of knowledge.In the case of scholarship that uses computation, large parts of the intellectual importance are embodied in the code rather than living exclusively in the print publication. As our case study also shows, usually the method description in the print publication presents the intellectual contribution of the code development in a very reduced, and rather imprecise high-level fashion. The code itself is a more precise inscription of the analysis the researchers conducted. The methodological approach we present is a way to engage the code, and thus allows a peer reviewer to understand and interpret the intellectual narrative of the code. This results in a fuller grasp and understanding of the methods applied, and thus to a more comprehensive review of the intellectual effort associated with the publication.After demonstrating the work in the notebook, we will conclude our paper with a critical reflection of the reviewing work that was undertaken with it. We identify the applicability, feasibility, benefits, and drawbacks of this specific approach. We also outline some possible future directions of research that could further contribute to exploring review methods for code scholarship.   ",
        "article_title": "Defactoring 'Pace of Change': Exploring Code Review Methods for Textual Scholarship and Literary Studies",
        "authors": [
            {
                "given": "Joris",
                "family": "Van Zundert",
                "affiliation": [
                    {
                        "original_name": "Academy of Arts and Sciences",
                        "normalized_name": "Academy of Arts",
                        "country": "Egypt",
                        "identifiers": {
                            "ror": "https://ror.org/02ddjw503",
                            "GRID": "grid.442752.2"
                        }
                    },
                    {
                        "original_name": "Huygens Institute for the History",
                        "normalized_name": null,
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Matt",
                "family": "Burton",
                "affiliation": [
                    {
                        "original_name": "University of Pittsburgh",
                        "normalized_name": "University of Pittsburgh",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01an3r305",
                            "GRID": "grid.21925.3d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " User Activity Analysis and Digital Humanities Research ProcessesIn fact, there is one field of research which addresses a comparable situation and this field is user activity analysis. In user activity analysis, humancomputer-interaction is recorded in order to evaluate the user with respect to a specific research interest. The approach is used in areas like e-commerce and online social networks research in order to create services like recommendation systems (Plumbaum, Stelter, and Korth 2009) or to analyze social behavior ( Dang et al. 2016). There are few examples of user activity analysis in academic digital environments. Suire et al. (2016) use this approach in the cultural heritage domain while Vozniuk et al. (2016) applies it to model learning processes in e-learning environments.Having said that, no ready-made solution exists which can be easily used in the present context. Instead different approaches to user-activity analysis have to be evaluated in order to decide which ones can be adopted. Nevertheless, under the circumstances of evaluating digital research practices these decisions remain contingent. Digital research takes place in very different digital environments and under different conditions. Thus, in every situation in which digital research practice should be evaluated a different selection from the existing set of options might be the best. An overview of these options will be published in a DARIAH-DE report in the future.The advantage of the Wissensspeicher use-case is the fact that it is a web platform-most user activity analysis takes place on websites and in web environments. There are two major tasks which need to be distinguished. The first task is user activity tracking and concerns how the data is created. The second task is the actual analysis. It demands to evaluate in which sense the created data constitute meaningful events and how to make sense out of these events. Use-Case: WissensspeicherThe Wissensspeicher implements user activity tracking by combining three different strategies: httprequest logging, browser-event parsing and user annotations. Http-requests are logged by virtue of the Django request object and the logger library in the Python Django app that creates the website. Thereby request information can be pre-processed when it is detected. When a page is loaded in the browser a JavaScript client registers event listeners for page elements and certain user actions. Each event that is triggered causes the client to parse relevant information in the DOM of the HTML including microdata which has been created in the Django app in advance. Additionally, the user is able to directly give feedback in some situations. The created data is stored in a MongoDB database.User activity analysis is also realized by virtue of three steps. In a certain way these steps resemble the three angles of workflow, provenance and lineage. First, events are evaluated in a so called task model. This task model describes ideal sequences of actions and user goals as conceived by the project employees. Second, users are evaluated by applying the thinking aloud technique from the field of usability testing. Finally, existing data will be evaluated to identify common event sequences by computing its clusters. A systematization of the results from these evaluations will enables researchers to associate certain meanings with events in such a way that the data can be analyzed to permit insights into research practices within the use case. A Dialogue of ApproachesThis presentation will summarize activities to evaluate research practices and methods in the digital humanities. It will outline a unique and complementary approach and indicate how this approach can be used in conjunction with existing digital humanities research practices. Finally, the implementation and results will be described up to the point that such results are present after two-thirds of the project time has elapsed. ",
        "article_title": "Evaluating Research Practices in the Digital Humanities by Means of User Activity Analysis",
        "authors": [
            {
                "given": "Niels-Oliver",
                "family": "Walkowski",
                "affiliation": [
                    {
                        "original_name": "Berlin-Brandenburgische Akademie der Wissenschaften (BBWA) (Berlin-Brandenburg Academy of Sciences and Humanities)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Industrial Memories project aims for new distant (i.e., text analytic) and close readings (i.e., witnessing) of the 2009 Ryan Report, the report of the Irish Government's investigation into abuse at Irish Industrial Schools. The project has digitised the Report and used techniques such as word embedding and automated text classification using machine learning to re-present the Report's key findings in novel ways that better convey its contents. The Ryan Report exposes the horrific details of systematic abuse of children in Irish industrial schools between 1920 and 1990. It contains 2,600 pages with over 500,000 words detailing evidence from the 9-year-long investigation. However, the Report's narrative form and its sheer length effectively make many of it findings quite opaque. The Industrial Memories project uses text analytics to examine the language of the Report, to identify recurring patterns and extract key findings. The project represents the Report via an exploratory web-based interface that supports further analysis of the text. The methodology outlined is scalable and suggests new approaches to such voluminous state documents. MethodA web-based exploratory interface was designed to enable searching and analysis of the contents of the Report represented within a relational database. The relational structure detailed the categories of knowledge contained in the Report along with key information extracted from the text (Figure 1). The Ryan Report is composed of paragraphs containing an average of 87 words. These paragraphs were represented as database instances and annotations detailing semantic content were linked through the relational structure. Named entities were automatically extracted using NLTK (Looper and Bird, 2002). The Ryan Report describes key elements of an enduring system of abuse that operated in Irish industrial schools. Its paragraphs tend to focus on particular topics, allowing them to be classified and annotated. For instance, some cover the extent and nature of abuse, others present witness testimony, report on institutional oversight or on how clergy were moved from one school to another in response to allegations. By classifying paragraphs in terms of these high-level knowledge categories it becomes easier to put a shape on many of the report's findings and to analyse it to provide new readings.Some of these paragraph-categories were identified using automated text classification. Others were extracted using a rule-based search (e.g., excerpts on institutional oversight). In building classification models, a variety feature sets were examined using a random forest classifier along with manually selected test data. A bag-of-words approach to feature selection yielded results that were over-fitted due to the small samples of training data. However, feature selection based on context-specific semantic lexicons generated from a sample of seed-words using a word embedding algorithm was found to yield accurate results. Lexicons were generated using the word2vec algorithm developed by Mikolov (2013) following an approach identifying synonyms outlined by Chanen (2016). Movements of Staff and Clergy (Transfer Paragraphs)An important paragraph-category covers those dealing with the Catholic Church's response to allegations of abuse. The typical response to discovered abuse was to transfer clergy from one institution to another, only for the abuse to re-occur (e.g., \"…Br Adrien was removed from Artane and transferred to another institution...\" (CICA Vol. 1, Chapter 7, Paragraph 829)). Such transfers are described in many different ways in language that often obscures what was happening (e.g., transfers out of the Order, effectively sackings, are described as \"dispensations to be released from vows\"). We carried out a \"byhand\" analysis to find transfer-paragraphs using verb-searches and then expanded this set using machine-learning classifiers.Initial (Table 1). A classification model then classified unseen text from the Report. Witness testimonies in the Ryan Report are indicated through reporting verbs and structural speech markers (e.g., punctuation). Using these features, Schlör et. al. (2016) gained accuracy of 84.1 percent in automatically classifying direct speech. Reporting verbs in the Ryan Report are often specific to its context such as apology, allegation or concession. To extract these from the text, highest-ranking similar words across multiple word embedding models were identified based on seed terms generated from WordNet, 'said', 'told' and 'explained'. The resulting context-specific synonyms combined with WordNet synonyms formed a lexicon of reporting verbs tailored to the language of the Report (Table 2). A classification model was developed using these features along with punctuation information using 500 training examples. readings of the Report suggested a set of verbs frequently used to describe the transfer of staff and clergy, including 'transfer', 'dismiss', and 'sack'. The highest-ranking similar words reoccurring over five word2vec models were then identified. Features based on this lexicon, along with names of schools and clergy were extracted from 250 training examples Descriptions of Abusive Events (Abuse Paragraphs)To evaluate the scale of abuse throughout the industrial school system, excerpts from the Report detailing abusive events were extracted. The language describing abuse incorporates a broad range of linguistic features. A set of seed-words from which to base a semantic lexicon for feature extraction, was not immediately apparent on reading the Report. A support vector machine algorithm was therefore used to extract the most discriminative features based on a sample set of 200 paragraphs.Analysis of the support vectors showed that terms distinguishing excerpts describing abuse formed five categories: abusive actions, body parts, emotions engendered in the victims, implements and names of staff and clergy. Sample words associated with each category were then used as seedwords to generate word embedding models to extract similar terms from the Report. Features based on these five lexicons, combined with names of clergy and staff were then used to generate a predictive model of abusive events. Findings and ConclusionsThis research demonstrates how word embedding can be used to compile context-specific semantic lexicons to extract features for text classification. These features allowed paragraphs for each knowledge category to be automatically classified based on manually selected training data. The performance of classifiers was evaluated using 10-fold cross-validation on the training data and showed high levels of accuracy in categorisations (Table 4). The classification models were applied to unseen data and performance evaluated by manually inspecting classifications of 600 randomly selected excerpts from the Report as shown in Table 5. Though overall accuracy levels remained high, precision of the classifications did fall somewhat, especially in relation to identifying speech and transfers. Error analysis showed that incorrectly classified excerpts (false positives and negatives) were commonly those where the meaning of the language was subtle or vague. Paragraphs incorrectly classified as quoted speech for instance, were in fact quotations from letters and diary entries. Unidentified speech excerpts all consisted of short quoted phrases. Transfers of clergy were reliably detected. However, there was a high rate of false positives due to the fact that the transfer of children throughout the school system is described using similar language (e.g. \"The witness remembered … when he was leaving Artane at nine years of age...\" (CICA Vol. 1, Ch. 7, Paragraph 466)). Classifying excerpts describing abuse yielded few false positives but it also returned the highest levels of false negatives. In these instances, references to abuse was subtle or addressed emotional abuse. As such, it was necessary to manually filter results. This paper has demonstrated that machine learning can be used to classify text based on a limited number of examples, when used in conjunction with word embedding to generate context-specific semantic lexicons. Re-presenting the Ryan Report in the form of a relational database with a web-based exploratory interface has facilitated comprehensive analysis of the Report, and has exposed new insights about the dynamics of the system of child abuse in Irish industrial schools. In reformulating how the Ryan Report can be presented, this research presents a scalable approach to digital analysis of state reports.",
        "article_title": "Mining the Cultural Memory of Irish Industrial Schools Using Word Embedding and Text Classification",
        "authors": [
            {
                "given": "Susan",
                "family": "Leavy",
                "affiliation": [
                    {
                        "original_name": "University College Dublin",
                        "normalized_name": null,
                        "country": "Ireland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Emilie ",
                "family": "Pine",
                "affiliation": [
                    {
                        "original_name": "University College Dublin",
                        "normalized_name": null,
                        "country": "Ireland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Keane",
                "affiliation": [
                    {
                        "original_name": "University College Dublin",
                        "normalized_name": null,
                        "country": "Ireland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Distributed Character: Quantitative Models of the English Stage, 1500-1920Mark Algee-HewittThe use of network graphs to represent social networks of characters within novels or plays has played an important role in quantitative textual analysis (see for example Agwar et al 2012, Bingenheimer et al 2011, Elson et al 2010). In this paper, I move beyond the network as a visual object, and instead, draw upon the quantitative metrics of the graph to explore the large-scale changes to the structure of English drama across four hundred years. What can the overall structure of the play can tell us both about the aesthetics of literary production of a given period, and what we can learn about the play by disaggregating the morphology from both the stagecraft and the language that, until now, have made up the two poles of dramatic criticism?The power of networks lie in their precise, mathematic, description of a set of relationships that can be quantified, measured and aggregated in ways that are unavailable to the reader of a text. Yet, most work has been focused on the use of single networks to describe single plays. For example, in his work on character networks in Hamlet, Franco Moretti turns quickly from the quantitative network analysis to the qualitative approach to the plot: \"I soon realized that the machinegathering of the data, essential to large-scale quantification, was not yet a realistic possibility […] So, from its very first section, the essay drifted from quantification to the qualitative analysis of plot\" (Moretti 2011). In this paper, I introduce an automated, rule-based, parsing of the 3439 English plays in the Chadwyck Healy drama corpus in order to perform the kind of large-scale quantitative analysis that Moretti gestures towards, but is unable to realize. The algorithm uses the existing XML markup in the corpus in order to extract speeches and assign them to characters as speakers and recipients, resolving co-references to character abbreviations (this is similar to the automated method employed by Trilcke et al. 2016, although the summary statistics that I extract are quite different).Drawing on this tagged corpus, I create a social network for each drama and extract a series of summary features based on both the eigenvector and betweenness centralities of each play. The first summary statistic that I calculate is the Gini Coefficient of the eigenvector centrality. Originally designed to measure income inequality within an economic system, the Gini Coefficient is a single number between 0 and 1 that indicates how evenly a set of resources (wealth, income or, in this case, centrality) is distributed across a population (here, of characters. In the Gini coefficient measurement in the corpus at large (Figure 1), there is a clear historical pattern being played out. Over time, between 1550 and 1900, the Gini coefficients of the plays exhibits a clear downward trend, from plays with a small core and a large, non-central periphery in the early century, to plays with a relatively large core and a small periphery in the eighteenth and nineteenth centuries and a large discontinuity between 1650 and 1700. What this metric seems to indicate, then, is the disappearance of the periphery of the English drama over time. Rather than suggesting significant structural changes to the core of the play, the largest influence in the Gini coefficient is the presence of a large periphery, whose members rarely speak (and more importantly, rarely interact with the center of power) and who therefore bring down the Gini coefficient for the entire play. Over time, then, this periphery disappears as casts get smaller and actions take place among an increasingly more tightly knit set of characters. Servants, retainers, guards, acquaintances and messengers, so important during the early modern period, disappear with increasing regularity in the later periods, echoing the reduced function such figures had in society itself, as dramas move, following Habermasian logic, from the throne room to the drawing room, becoming personal and intimate, rather than mythic, political and impersonal. The second metric is the percentage of characters in the top quartile of the eigenvector centrality distribution. This measures the size of the core of the play and tells an equally striking and parallel story ( Figure   2). Although the relative regularity of the measurement makes it less immediately apparent, there is a constant historical increase in the percentage of characters in the top quartile of eigenvector centrality scores. While the falling Gini Coefficients speak to the disappearance of the periphery, this metric reveals what happens to the remaining core. Rather than follow the same pattern of the early modern period (with few highly central characters), the disappearance of the periphery means that more centrality is allotted between the core characters. This speaks not just to the increasing size of the core, but, more importantly, to the tendency of having plays that feature multiple sub-networks, each with their own protagonist. In a play with a single network, it is easy for one character to dominate it, but in a play whose action is divided between competing communities, each community can have its own central figure. If we can tell the protagonist of an early modern drama by his or her high eigenvector centrality compared to the rest of the cast, then by the seventeenth century the single protagonist has been dispersed between multiple characters who all evidence a high eigenvector centrality, distributing the function of the protagonist (and/or the antagonist) among a growing number of central characters. As opposed to the eigenvector centrality's relationship to the protagonist, betweenness centrality speaks to the mediatedness of the drama. That is, if a high betweenness centrality indicates a character that mediates other character's interactions (such that they have to pass through her), then the scaled maximum betweenness centrality of a dramatic network overall, which measures the relatively importance of bridging characters, indicates the extent to which this mediating function is important to the drama as a whole. At the level of the corpus, the normalized maximum betweenness centrality, the relative importance of the bridging character, decreases across the century, very quickly from 1590 to 1640, and then more slowly across the remaining two and a half centuries: the average maximum betweenness centrality in a play drops by over 750 across just the sixteenth century. Again, the largest discontinuity lies between 1650 and 1700: there is a clearly a lasting effect on the structure of dramatic networks from the puritan shuttering of the theaters during the interregnum. The English drama that returns during the restoration is evidently not the same as the English stage before Cromwell. Understanding Reader-Identified Social Interactions in Literature Koustuv Sinha, Andrew Piper, Derek RuthsSocial network analysis begins with the primacy of character as its object of study. In this, it fits within an aready well-established area of inquiry within literary theory, one whose formal study extends back until at least the early twentieth century if not earlier (Propp 1968). Where social network analysis differs from this tradition is through the emphasis on dynamic interactions as a key to understanding the narrative function of character. Whether exploring the afterlife of fan fiction, theories of mind, affective identification, or the typologies of character, what all of the pre-computational work on character has in common is an emphasis on understanding character in the singular. Social network analysis argues instead that the meaning of any character is a function of his or her relationships with respect to all of the other characters introduced over the course of a story (Woloch 2009). Character networks offer a way to study not simply the types or themes or affective connections between readers and imaginary people. Rather, they afford us the ability to understand the social imaginings of writers, periods, and genres.Several initial attempts to introduce social network analysis into the study of literature have already been made. Character networks have been studied within three major European epics to understand their relation to contemporary models of social networks (MacCaron/Kenna 2012); an abridged version of a single well-known literary work (Alice in Wonderland) to test differences between interactions and observations on character centrality (Agarwal 2012); nineteenth-century novels to understand the correlation between dialogue and setting (Elson 2010); as a form of narrative generation (Sack 2013); and the genre of classical drama to better understand the notion of tragic conflict (Moretti 2013;Karsdorp et al 2015).Each of these works has added to our understanding of the relationship between character and literary form in important ways. And yet at the core of each of these studies lies a fundamental assumption about the self-evident nature of an \"interaction.\" Initial attempts to use machine learning to derive interactions on prose texts have shown very poor performance (Agarwal 2012 reports a maximum F1 score of 0.61). What this indicates at least in part is that interactions are highly complex verbal constructions which we cannot easily assume pre-exist our attempts at extracting them.To counter this problem, we have designed a study to explore reader agreement across a variety of text passages (1,000) drawn from popular contemporary fiction and non-fiction. Rather than begin with a stable set of interaction types, however, our goal is to infer possible classes of interactions and then understand which of these classes generate more ambiguity among readers. We perform this in three phases. In the first phase, we ask coders to identify minimally defined interactions using a standardized web interface (where an interaction consists of two entities and an action linking them). Our goal here is not to pre-define types of interactions as in other studies (Agarwal), but to better understand how readers intuitively understand social interactions between characters. As we have shown in another study, readers indicate very high agreement in identifying character aliases (i.e. determining what is an entity (Vala et al.)). In the second phase, we use unsupervised clustering techniques to identify different interaction \"types\" based on syntactic and lexical features of the labeled interactions. Third, we then measure reader agreement across these different types. While we want to know overall how well readers agree on defining interactions, we also want to understand if different types of interactions across different types of writing (fiction/non-fiction) illustrate signigicantly higher levels of disagreement. This is a first step in understanding the unique ways literary texts generate social complexity, not simply through the quantity of interactions but also importantly through their qualities. Emma: A Feature Space for Studying Character Andrew Piper and Hardik ValaThis paper will argue that computation has an important role to play in understanding the nature of characters and the process of what we might generally term characterization - the writerly act of generating animate entities through language. With an estimated 86 characters per novel in the nineteenth century and a conservative estimate of 20,000 novels published during this period in the English language, there are over 1.7 million unique characters that appear in that one century and one language alone. Even if we condition on main characters, we are still looking at several thousand distinct entities. At the same time, there are not only a great number of characters in literature, but there is also a tremendous amount of information surrounding even one primary character. Like other highly frequent textual features such as conjunctions or punctuation, characters are abundant across the pages of individual novels. Personal pronouns alone account for roughly 12% of all tokens, and if one adds in proper names the number of character occurrences is closer to 16% - or one in every six words! Like the abundance of characters, such semiotic abundance surrounding characters poses problems for inherited critical methods. How can we be sure that our claims about \"character\" are capturing the broad and potentially diverse ways that characters are depicted in novels, this larger mass of fictional beings and what it means to be fictional?In order to address this question, we have developed a computational tool designed for the study of character. Its aim is to identify 28 different features that relate to qualities that characters may possess. These range across categories like distinctiveness (how distinctive is the main character from other characters within the novel); positionality (how often is the character the agent or object of a sentence or a possessor of some object); centrality (how important is the protagonist relative to other characters in the novel); and modality (what kinds of behaviors and descriptions inform this character's identity, such as cogitation, perception, motion, embodiment and even clothing or dress).Rather than start with known \"types\" of character, this tool allows us to implement a more multi-dimensional understanding of character and use that representation to think about the relationships between novels. Prior work on stylistic analysis has not differentiated between various aspects of texts when comparing them to each other. The novel is taken as a unified whole. Our character feature tool allows readers to begin to explore these different sub-domains of a novel, which in our case refers to the language used to construct character. In our presentation, we will discuss the mechanics that underlie the tool, which implements a modified version of BookNLP (Bamman 2014) and the Stanford dependency parser in order to identify words related to character. We will also discuss a case study in which we explore the identity of \"introversion\" in novels from the nineteenth century to the present. As we will show, the character feature tool allows us to construct not only familiar narratives about the history of the novel - wherein the representation of interiority is strongly gendered around female protagonists -but also novel and nuanced insights about that tradition when we follow these features across a broader swath of time. As we will show, interiority no longer remains the distinctive quality of feminine heroines but is transposed onto a very different generic and gender scene -the male hero of science fiction.",
        "article_title": "Studying Literary Characters and Character Networks",
        "authors": [
            {
                "given": "Andrew",
                "family": "Piper",
                "affiliation": [
                    {
                        "original_name": "McGill University",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Mark",
                "family": "Algee-Hewitt",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Koustuv",
                "family": "Sinha",
                "affiliation": [
                    {
                        "original_name": "McGill University",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Derek",
                "family": "Ruths",
                "affiliation": [
                    {
                        "original_name": "McGill University",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Hardik ",
                "family": "Vala",
                "affiliation": [
                    {
                        "original_name": "McGill University",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Expected resultsFirstly, this research will provide the first acoustic model for the study of acoustical properties of cultic theatres in the ancient Greek world. The study will assess and recover the acoustics of the Selinunte theatre.Secondly, the research will develop specific tools suitable for processing the resulting 3D models. It is also hoped that the results will provide some foundations from which to create experimental interpretative 3D reconstructions integrating acoustic models. The results will establish a new framework, which future researchers can use to advance their knowledge of the application of 3D technology for the documentation of instruments.Finally, this project will offer an innovative research method in the study of ancient Greek music. This research aims to create a field of comparative studies of archaeomusicological research.In conclusion, this research will develop a new theoretical basis, which will contribute to the establishment of a methodology at the crossroads of archaeomusicology, architecture, and acoustics and digital technologies. In addition, this study is part of a programme intended to valorise ancient cultural and musical heritage in the Mediterranean with cross-disciplinary approaches to human culture and technology, in order to unveil new meanings and create new research fields within the digital humanities and heritage science.   ",
        "article_title": "Towards a New Approach in the Study of Ancient Greek Music: The Virtual Reconstruction of an Aulos \"Early Type\" from Sicily",
        "authors": [
            {
                "given": "Angela",
                "family": "Bellia",
                "affiliation": [
                    {
                        "original_name": "University of Bologna",
                        "normalized_name": "University of Bologna",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01111rn36",
                            "GRID": "grid.6292.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " MethodologyThe data of the RF museums' statistical reports for 2015 were received from the RF Ministry of Culture in summer 2016. Museums return these mandatory reports in January and provide statistical data for the preceding year. The data were related to 2,635 museums from every region of the Russian Federation.The data were received as an Excel spreadsheet from which the parts were removed that were not related to the digitization of museum objects and the data on galleries that were for temporary display and did not store any objects. This left us with 2,367 museums. The data in the spreadsheet were sorted on the total number of objects for every museum, the number of unique objects, the number of database records with digital images, and the number of images posted online.Russian museum collections tend to consist of two parts: the main collection of unique objects and a smaller 'research collection' including duplicates and supporting documentation. While the total number of objects in Russian museum collections slightly exceed 80 million objects, the number of unique objects is 20 million fewer and equals 60 million objects. The results of statistical surveys obtained for the study reported the number of digitized objects as related to the total number of objects in a museum including their 'research collections'. This did not create a methodological problem when comparing the results with those from the Enumerate project which is a study of the outage of digitisation across Europe, funded by the European Union which happened between 2011(Europeana, 2016 where the survey asked to provide the percentage of digital images for museums' analogue collections.The percentage of digitized objects and the percentage of objects with images posted online was calculated for each geographical region of the Russian Federation and mapped to show the differences. The total percentage for the country was also assessed. ResultsThe percentage of digital images as related to the total number of museum objects across Russia was 13,5%. The percentage related to the number of objects in the main collection (roughly corresponding to the number of unique objects) was 18%.The percentage of images posted online as related to the total number of objects was 1,5%, this figure was somewhat larger if compared with the number of objects in the main collection (2%).An interesting and unexpected result was the difference between the scale of digitization in two major cities, Moscow and Saint Petersburg ( Table 1). The percentage of objects with digital images was much higher than the average across Russia in Saint Petersburg and somewhat lower than the average in Moscow. The scale of digitization across major geographical regions varied between the minimum of 6% in the Far East and the maximum of 25% in the regions adjacent to Saint Petersburg ( Figure 1, Table 1).Interestingly, the percentage of images posted online was slightly lower than the average across Russia for museums in Moscow and twice lower than the average across Russia in Saint Petersburg (Table 2).    DiscussionOur findings demonstrate that digital collections in Russian museums do exist across the country but we cannot say that their online display is representative enough to cover the culture, considering the variety in geography and ethnography.We can roughly confirm our previous results on the percentage of digitized images (Kizhner, Terras, Rumyantsev, 2016) to be around 18% as our present data show the level of digitization to be in the range of 13,5 - 18%. However, our previous results might have a sampling bias as the museums answering the questions of the survey could be interested in digitization per se and work towards obtaining more financial and administrative support to keep it going. This might result in a higher percentage. Even so, the results of the present study can only demonstrate a range of digitization scale as the percentage of images related to the total number of objects may include a significant number of duplicates and supporting materials (e.g. library books). This is not an obstacle to comparing our data with those from the Enumerate project 'which aimed to survey the extent of digitization across Europe' (Euro- peana, 2016) where the survey questions were about the percentage of the analogue collection digitally reproduced, but given the range of 13,5 - 18% we can say that the results for 2015 are much lower than the results of the Enumerate project for 2015 when the percentage of digitized collections in European museums was 31%. However, the results for Saint Petersburg collections are higher than the European average (Table 1). The percentage of images available online across Russia as related to the analogue collection is 1,5 - 2% which is lower than the percentage reported by the Enumerate project (24% of digital collections and 7,5% of European analogue collections). However, the Enumerate results included digital collections and digitally born objects available online, which complicates the comparison (Europeana, 2016).Recent criticism of digitization without proper contribution to building knowledge in the humanities (Hitchcock, 2013, Gregory et al., 2016) requires developing these studies further towards exploring how Russian museum web sites arrange images for searching and browsing and developing projects discussing the issues of open access and digital canon. This paper opens up the space for studying Russian digital collections on a national scale. It also reports on the results of looking at the scale of digitization for major geographical regions within Russia, and it will discuss the results of calculating simple correlations of digitization percentage with population density, the level of education, funding, and the number of museum goers in the regions. By doing so we can challenge the concept of the digital canon, and ask difficult questions regarding which types of culture are being digitized and made available worldwide.",
        "article_title": "Accessing Russian Culture Online: The scope of digitisation in museums across Russia",
        "authors": [
            {
                "given": "Melissa",
                "family": "Terras",
                "affiliation": [
                    {
                        "original_name": "University College London",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Inna",
                "family": "Kizhner",
                "affiliation": [
                    {
                        "original_name": "Siberian Federal University",
                        "normalized_name": null,
                        "country": "Russia",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Maxim ",
                "family": "Rumyantsev",
                "affiliation": [
                    {
                        "original_name": "Siberian Federal University",
                        "normalized_name": null,
                        "country": "Russia",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Kristina ",
                "family": "Sycheva",
                "affiliation": [
                    {
                        "original_name": "Siberian Federal University",
                        "normalized_name": null,
                        "country": "Russia",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "This paper will discuss how our newly prototyped POEMAGE visualization tool (see McCurdy et al, 2015), created to identify and visualize complex sonic relationships within individual poems, has provided poetry scholars with new ways to identify and conceptualize metaphor, which has previously been considered computationally intractable because of its semantic and syntactic complexities. It focuses not on the tool's technical details but on the ongoing re-theorization of poetry it has engendered, Close readers are trained to connect every element in a poem to every other in an ambiguous, shifting complex of meaning, which the reader, bringing her own complexities to the process, activates. This poetic dynamic makes computational analysis and visuali-zation of any aspect of poetry a challenge. The goal of our research team has been to take a single poetic element-sound-and treat it computationally and visually at a level of complexity that will make POE-MAGE useful to poets and scholars performing sophisticated close readings of poetry, even as it makes poetry more accessible to students and casual readers. Though sound interacts with the other features operating within a poem, unlike most other features it can be looked at in its own terms and is subject to computer analysis through quantification. As we began, poet Julie Gonnering Lein and I sought to preserve poetry's qualitative, aesthetic experience ; computer scientists Miriah Meyer and Nina McCurdy sought to address open questions in their field. Both goals required moving beyond what the machine could already do. Off-the-shelf software can see exact rhyme quickly, as can a good reader-who will swiftly move on to look for sonic relationships that don't replicate themselves but enact disruptive changes that are hard to identify computationally. To capture the progression of sonic clusters as they repeat in different and evolving combinations not only within but across syllables presents a computational problem that required our technical team to develop RhymeDesign, which allows users to query a broad range of sonic patterns within a poem and to design custom templates to query patterns we haven't imagined. Built on top of RhymeDesign, the POEMAGE interface visualizes and allows users to explore interactions of the queried patterns. In performing this work, we have looked for (and not yet found) computational breakthroughs that might bring metaphor within reach, a process that has required us to consider closely how metaphor works. The difficulty with metaphor inheres even in simple instances. Getting the machine to understand why \"Hope is a bird\" (or, more problematically, \"'Hope' is the thing with feathers\") is a metaphor but \"Juliet is a Capulet\" and \"Karen is a Carpenter\" may be either similar or different statements of fact is not straightforward. Poets as different as Dickinson and Donne play complex metaphors out across entire poems in elaborate and shifting figural structures. To develop a tool that can reliably identify metaphoric relationships as POEMAGE identifies sonic relationships-in real time across the entire poetic field-would require the solution of multiple open problems in computer science. However, recent readings of poems by Dickinson and others, undertaken using POEMAGE, suggest that it is possible to use the tool to access some metaphors not directly but indirectly, leveraging the fact that both rhyme and metaphor operate by substituting one word for another that is different-but-similar, and that inevitably sites of sonic difference-in-similarity point to semantic difference-in-similarity as well. In close reading, we have noted that places the machine marks as being sonically \"interesting\" are also sites of metaphorical action, and that this action often in-heres in, rather than simply existing alongside, the sonic relationships being indicated by the tool. This inherence can emerge through various kinds of sonic relationships, including but not limited to homonyms like \"knot\" and \"naught,\" which POEMAGE shows in Bradstreet's \"Prologue,\" and eye-rhymes like \"blood\" and \"mood,\" which it picks up across Pelizzon's \"Blood Memory,\" about menstruation. In presenting these words as related, even conjoined, the machine opens a space for us to tease out figural connections between a loop in a rope and nothingness, or men-strual blood and emotional pain. A more complex example of metaphor developing through sound occurs in Dickinson's #313:",
        "article_title": "Getting at Metaphor",
        "authors": [
            {
                "given": "Katharine",
                "family": "Coles",
                "affiliation": [
                    {
                        "original_name": "University of Utah",
                        "normalized_name": "University of Utah",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03r0ha626",
                            "GRID": "grid.223827.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionVoces (from Lat. vox 'voice', 'word') is an analysis and visualisation dashboard for corpus-based research in lexical semantics. Currently developed as a Shiny application communicating with R session running in the background, Voces provides users with possibly exhaustive account of how selected Latin word is distributed across the corpus and what can be told about its meaning. The application is built around a corpus which currently consists of ca. 200M words from texts dating from the Classical era (1 BCE) to the Middle Ages (14th CE). Although Voces was originally conceived as a tool of historical semantics research, the application -due to its modular design -may be modified and the code basis can be re-used in new research contexts. Information computed on a basis of a CWB-indexed corpus is presented to a user through a single-page interface composed of separate widgets arranged in a clear grid layout. Each widget is responsible for displaying in textual or graphical form a clear-cut property of word's distribution or meaning. A heavy use of data visualisation techniques renders Voces a convenient tool for exploratory analysis of textual corpora, but the grid layout is also reflection of modular architecture of the application. Each widget is implemented as a separate function which can be extended and adopted by researchers with even limited R programming skills. Use scenariosA typical use scenario is triggered when the user specifies a lemma to be looked up. If the search fails, a list of lemmas to choose from is provided. In case of success, neatly separated sections of the dashboard are populated with widgets, each of which corresponds to one sense or distributional property of the word under scrutiny. Word's frequency is summarised as a number of occurrences in the corpus (both raw and p.m.w. counts) and displayed as a highlighted point on a frequency spectrum plot (Baayen 2001). A barplot is provided for investigating change of frequency in subsequent corpus sections. Study of language variation is enabled through widgets presenting word's frequency as a function of such variables as author, work, genre, and -most importantly -time. Users are, therefore, provided with a list of authors who use the word most frequently or a word cloud summarising terms to be found in the titles of works with a particularly frequent use of the word under scrutiny. Genre variation is presented in form of a pie chart, while diachronic dimension - through a bar plot of frequency counts in partitions of the corpus. Diatopic variation study is still to be implemented.A word's meaning potential can be investigated by means of a set of widgets presenting its contextual properties. The most frequent co-occurrences are enumerated on a simple count list which may be further analysed according to period and genre criteria. A Distributional Semantics Model ( Baroni and Lenci 2010) is built from the corpus in order to enable simple meaning computation. Evert's (2014) wordspace package and a set of Alain Guerreau's scripts is employed in order to cluster co-occurrences. Similar terms of a looked up word are also computed and then presented in both textual and graphical form.Users are supported in data and visualisation interpretation through hints which accompany every widget. Their role is to explain not only what the data can mean, but also how the figures were computed, how one can interpret the geometrical properties of a plot, and so on. This, along with the availability of data sets, code snippets, and reports generated on the fly, is what makes Voces a tool of reproductive research. ArchitectureVoces was built as a Shiny application ( Chang et al. 2016). Its development was greatly facilitated by the availability of a decent documentation and community support (both particularly useful when dealing with framework's complex reactivity model). It turned out soon, however, that it may not be the best choice for web application which has to combine heterogeneous data and non-R code as well. Hence, other solutions are being tested at the moment, those in particular which would provide, for example, more flexible integration of external APIs. The most promising seems to be OpenCPU (Ooms 2014), an application which exposes R session through a RESTful API. This approach allows any application written in some of the less or more popular web development frameworks to easily communicate with an R server instance.As for the architecture, Voces depends on a CQP server instance running in the background which requires corpora to be indexed with the CWB. Communication of the R server with the CWB is assured through the rcqp package (Desgraupes and Loiseau 2012) which offers a set of useful functions providing access to both positional (token-level) and structural (document-level) attributes. Unfortunately, development of this very helpful tool seems to be less active recently and thus Voces will soon accept also tabular data as input. Previous researchNowadays, corpus linguists may chose from a vast array of free, open source and stable corpus query systems (CQS) which not only allow for efficient indexing of large corpora, but also provide a user-friendly concordance interface and offer out-of-the-box a set of such essential functionalities as collocation lists, simple corpus statistics etc. Both web (CQPweb, NoSketchEngine etc.) and desktop applications (TXM etc.) are also usually equipped with a less or more intuitive corpus management interface. Voces, a dashboard for vocabulary research, is not yet another CQS and has no intention to supersede well-established tools which cannot be easily combated in terms of either robustness or speed. Quite the contrary, the application communicates with the CWB engine and adapts some of the design choices and features of the popular CQS, while hopefully does not inherit their drawbacks.Unlike the case of the well-known CQS, more emphasis has been put on quick access to multifaceted information rather than on close analysis of occurrences. Voces does not attempt, then, to implement some of the features which are traditionally considered an important part of the corpus analytical toolbox, such as concordance sampling, sorting etc. Undoubtedly, the strength of popular CQS lies in their wide applicability: by default, they do not preclude any research scenario. Although agnostic of linguistic theory, Voces was originally built for more specific purposes and focuses on semantic properties of the word and its distribution.What is believed to be one of the main advantages of the present application is that - thanks to its modular architecture - it can be easily extended or adopted by a researcher with even moderate programming skills. In that Voces attempts to fill the gap that exists between, on the one hand, fully-blown CQS, which are normally quite conservative when it comes to adding new features, and, on the other hand, single-purpose research workflows built ad hoc by researchers. What also distinguishes Voces from other CQS is its emphasis on helping users to interpret data. A system of visual and textual hints keeps a researcher informed about where does the data come from, how have they been computed etc.The grid layout is well-known from analytical environment and is especially popular in finances or engineering (Few 2013); in humanities it was adopted, among others, in the Voyant Tools project. It offers a quick insight into otherwise dispersed data and a coherent account of word's properties. Further researchVoces is currently in an early stage of development. The work focuses on adding new functionalities and plotting types which may sometimes affect application's efficiency. Future work will focus on: (1) optimising user experience; (2) implementing tools for (a)Fig. 1 :1Fig. 1: Voces. User Interface: Word Form Distribution (tempus 'time') Fig. 2 :2Fig. 2: Voces. User Interface: Frequency Spectrum Plot (Voces. User Interface (tempus 'time') ",
        "article_title": "Voces. An R-based Dashboard for Lexical Semantics",
        "authors": [
            {
                "given": "Krzysztof",
                "family": "Nowak",
                "affiliation": [
                    {
                        "original_name": "Institute of Polish Language - Polish Academy of Sciences",
                        "normalized_name": null,
                        "country": "Poland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe main purpose of this study is to grasp the pitch transition patterns from pieces of traditional Japanese folk songs among southern three regions of Honshu (the Main Island of Japan), and by making comparisons with Koizumi's tetrachord theory in order to make the regional classification by the tendency in pitch information.A tetrachord is a unit consisting of two stable outlining tones called kaku-on (nuclear tones), and one unstable intermediate tone located between the nuclear tone. Influenced by the methods of Western comparative musicology, the Japanese musicologist Fumio Koizumi conceived of a scale based not on the octave unit but rather on the interval of a perfect fourth, and has developed his tetrachord theory to account traditional Japanese music (Koizumi, 1958). Depending on the position of the intermediate tone, four different tetrachords can be formed (see Table 1 and Figure 1). In the previous study, we have sampled and digitized the five largest song genres within the music corpora of the Nihon Min'yo Taikan (Anthology of Japanese Folk Songs, 1944Songs, -1993) from 45 Japanese prefectures, and have clarified the following three points by extracting and comparing their respective musical patterns (Kawase and Tokosumi 2011): (1) the most important characteristics in the melody of Japanese folk songs is the transition pattern, which is based on an interval of perfect fourth pitch that constructs Koizumi's four basic tetrachords; (2) regionally adjacent areas tend to have similar musical characteristics; and (3) the differences in the musical characteristics almost match the East-West division in the geolinguistics or in the folkloristics from a broader perspective. However, to conduct more detailed analysis in order to empirically clarify the structures by which music has spread and changed in traditional settlements, it is necessary to expand the data and make comparisons based on the old Japanese provinces (ancient administrative units that were used under the ritsuryo system before the modern prefecture system was established).  Overview of dataWe extracted the musical notes for works included in the \"Nihon Min'yo Taikan\", and digitized the entire scores from each province in the Kyushu district (geographically located in the southern part of Japan), Chugoku district (the westernmost region of Japan's largest island of Honshu), and Shikoku district (literally meaning four provinces, located south of Honshu and east of Kyushu district). In total, there were 474,191 tones in the sample of 2,383 songs for the 25 provinces (see Figure 2). 񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙񮽙 \" # \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\" * *  Figure 2. Geographical divisions of the three districts under the old provinces ProcedureThe procedures are as follows: (1) we digitized all the songs from each district and generated sequences that contain interval information from the song melodies; (2) extracted four transition probabilities of tetrachords for every province separately, and create a 24-dimensional data with 25 samples (provinces); and (3) applied principal components analysis (PCA) to identify patterns in the data, and to highlight their similarities and differences.In order to digitize the Japanese folk song pieces, we generate a sequence of notes by converting the music score into MusicXML file format. We devised a method of digitizing each note in terms of its relative pitch by subtracting the next pitch height for a given MusicXML. It is possible to generate a sequence T that carries information about the pitch to the next note: T = (t1, t2, … , ti, …, tn). An example of the corresponding pitch intervals for ti can be written as shown in Table  2. We treat sequence T as a categorical time series, and execute N-gram to capture transitions and their trends.Using a bigram model representing pitch transitions, all four types of tetrachords from Table 1 can be expressed as follows in ascending order: min'yo (+3, +2), miyako bushi (+1, +4), ritsu (+2, +3), and ryukyu (+4, +1). Depending on the positions of the three initial pitches in a tetrachord, six transition patterns can be considered in perceiving a tetrachord in two steps (bigram). Therefore, the amount of tetrachords within two steps can be obtained by counting the pairs of 24 transition patterns in sequence T.  Results and discussionsThe relative frequency of the first transition (unigram), is maintained between \"-5\" and \"+5\"; the interval of a perfect fourth pitch (see Figure 3). As the graph forms a symmetric shape with respect to \"0\", the pitch transitions occur almost equally in both descending and ascending order.The implementation of the PCA is summarized as follows. As shown in Table 3, the component loadings for the first three principal components of each province explain more than 85.88% of the variability. In the first column, the values of all 24 variables represent a positive quantity, and have almost the same weight. This result indicates that the profile of the first PCA axis is the persuasiveness of the tetrachord. Thus, as the value increases, the inclination of a pitch transition enhances its persuasiveness, and the value decreases, it loses its persuasiveness. In the second column, all variables for the min'yo and the ritsu represent a negative quantity, while 11 variables for the miyako-bushi and the ryukyu represent a positive quantity. According to ethnomusicological research, the min'yo and ritsu tetrachords appear frequently in Japanese folk songs. In contrast, the miyako-bushi and ryukyu tetrachords, steadily shifted from the ritsu and min-yo tetrachords respectively, and then increased in popularity as an emotional crutch (Koizumi, 1977). This result indicates that the profile of the second PCA axis is the relative pitch intervals between the nuclear tone and its intermediate tone, or in other words, the differences in patterns of transition from the nuclear tone. Thus, as the value increases, the adjacent intermediate tone forming the tetrachord tends to form a minor second interval (sort of a minor key progression), and as the value decreases, it tends to form a major second interval (sort of a major key progression).The corresponding scores for each sample are plotted in a two-dimensional space to complete the PCA (Figure 4). We see that there is a strong contrast between min'yo, ritsu, miyako-bushi, and ryukyu. It is possible to clarify the structural commonalities and differences between areas. Figure 5 is the result of applying the hierarchical cluster analysis (Euclid distance, Ward method) to the corresponding scores. In addition, if we look for a height where there are three vertical lines and trace the lines down to the individuals, the partition corresponding to three clusters. If we plot this result on a map, we see that provinces are clearly classified according to geographical factors and cultural backgrounds ( Figure  6).  Individuals factor map (PCA)  ConclusionIn this study, we digitized the melodies of endangered traditional Japanese folk songs from three regions, and quantitatively classified them according to the old province by executing principal components analysis and hierarchical cluster analysis in terms of pitch transitions based on a unit of Koizumi's tetrachord theory. As a result, compared to our previous studies on the small amount of data (e.g. Kawase 2016aKawase , 2016bKawase , 2016c, regions were successively classified according to both geographical factors and cultural backgrounds in detail, and classified the melodies into two basic groups according to the behavior of the intermediate tone. We firmly assured that the melodic structures of tetrachords in each province are shared by land and sea routes based on actual music data analysis.",
        "article_title": "Regional Classification of Traditional Japanese Folk Songs from Southwest Regions",
        "authors": [
            {
                "given": "Akihiro",
                "family": "Kawase",
                "affiliation": [
                    {
                        "original_name": "Doshisha University",
                        "normalized_name": "Doshisha University",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/01fxdkm29",
                            "GRID": "grid.255178.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn March of 2016, the failure of Microsoft's prototype chatbot, Tay, was not just a technological failure. It was a disciplinary failure. It was a failure of an industry leader to adopt a critical perspective when building systems in a complex cultural and social environment. Tay, which stands for \"thinking about you,\" was the name given to an artificial intelligence chatbot for Twitter that was quickly corrupted by users and began spewing racist, sexist, and homophobic slurs. Pundits quickly leapt to conclusions about the political beliefs of internet users, but these same pundits failed to understand that this hacking of Tay was in fact a critique of chatbots in the real world. Users of Twitter were exposing a fundamental error made by the Microsoft development team. Because the system learned direct-ly from user input without editorial control or content awareness, Tay was quickly trained to repeat slurs by users eager to embarrass Microsoft. This moment in technological development makes for an interesting anecdote, but it also represents the moment that chatbots entered the public consciousness and became nothing less than the future direction of a unified interface for the whole of the web. Of course, chatbots captured imaginations in the 90s as well. Systems like Cleverbot, Jabberwacky, and Splotchy were fascinating to play with, but they had no real application. Today, text based AI has been identified as the the successor to keyword search. No longer will we plug in keywords into Google, comb through lists of text, and depend on search engine optimization (SEO) to deliver the best content. Search will be around for a long time, but in the near future much more content will be delivered through text based messenger services and voice controlled systems. We've seen the early stages of this change in products like Amazon's Alexa, Apple's Siri, Google Now, and Microsoft's Cortana. There are now bots embedded within common platforms like Slack, Skype, and Facebook Messager. We are now approaching a world that Apple envisioned in 1987 with a mockup system called the \"Knowledge Navigator\" that sought to give users an interactive and intelligent tool to access, synthesize, present, and share information seamlessly.Humanities in the Loop We are likely decades away from a true \"knowledge navigator,\" but the second generation of these chatbots are now in development. The company that developed Siri for Apple is now in the final stages of development on a system called Viv (Matney). Viv is the first viable company to produce a unified interface for text and speech based AI assistants. Facebook is testing project M within its messenger app to allow users to issue commands, access services, and make purchases through text input (Hempel). The remarka-ble thing about M is that Facebook has built a system with \"humans in the loop.\" This means that when a service is accessed, perhaps by purchasing movie tick-ets, a human will fine tune the AI generated results for each transaction. There is currently an understanding within the machine learning community that human assisted training of these systems produces more accurate results but will also train more robust systems going forward (Biewald, Bridgwater). The current need for human in the loop systems means that we are at a crucial moment for humanists to lend their experience and critical abilities to the development and training of AI systems. In the field of machine learning, training a system to answer humanities based problems will show how these systems succeed or fail, but they will also demonstrate the value of the humanities in a digital world. If the purpose of the humanities is to better understand what it is to be human, training AI to answer philosophical, historical, or cultural questions will help us understand our experiences as we become more accustomed to intelligent systems in our lives. Grappling with AI, whether it is in a mundane consumer exchange or in matters of grave ethical importance, is rapidly becoming a practical problem in our lives.With humanists in the loop, we will better understand the social and cultural contexts in which these systems appear and avoid the regrettable failure of systems like Tay in the future. We are currently on the cusp of a revolution in the applicability of natural language understanding, artificial intelligence, and conversation based interfaces design. These technologies will have ranging consequences socially, culturally, and economically in the coming decade, but these technologies are also deeply connected to the social and cultural contexts in which they appear. My goal is to train machines to be humanists. It is the literary crit-ic's ability to close read complex philosophical, histori-cal, and artistic meaning that these systems lack. It is the ability of the historian to contextualize political and technological change within the breadth of human progress. It is the dramatists ability to understand performance and dialogue that will animate our conversations with computers. The digital humanities are well situated to make the most of NLP techniques and find culturally significant training sets.  Method: Conversational Data RetrievalBiographical and archival material has been used to train a system to allow a conversation with the famed American author William Faulkner. I will pre-sent a system trained with nearly all the interviews that Faulkner has given. Author interviews are an ex-cellent training set because the questions asked by the interview anticipate user interests and model a con-versational style of response. The interviews collected during Faulkner's visit to the University of Virginia were instrumental in building this tool. The applica-tions for such a system are numerous. A conversation with Faulkner might benefit a creative writing student in the midst of writer's block. A chatbot offers a more inviting interface for a general public. Most important-ly, Faulknerbot will represent a novel form of content discovery for student researchers. Once a user has developed a chat history worth exploring, Faulknerbot's responses link to original archival materials for research purposes.Current systems have come a long way from the toy-like chatbots that populated the web in the late 90s. After a pre-processing stage using word2vec, which vectorizes the bag of words, this model uses Tensor Flow to generate two complementary neural networks that encodes and decodes inputs and responses. This model has only recently been made accessible to non-computer science researchers recently by Google open sourcing Tensor Flow. is not based on the retrieval based model using a rule based expressions, with a heuristic to determine intent and draw from a predefined response. This is not a simplistic tree model based on nested \"if/then\" statements. Instead this uses a generative model. This generative model uses sequence to sequence learning with neural networks (https://arxiv.org/abs/1409.3215). This model links words statistically to determine \"flows\" of meaning through a word vector. Geoff Hinton calls this a \"thought vector.\" In other words, this is an end-to-end model that remains open. Rather than a retrieval method, which limits the scope of the conversation, this system dynamically learns and allows for a retention of what has been said. The generative model allows for this context based discussion without resorting to an enormous conversation log. In Tensor Flow, this operates on a Long Short Term Memory (LSTM) network. As I've said, the sequence to sequence model is based on two neural nets. One is an encoder, which encodes input data from the user. The decoder model determines the reply by generating the output, which need not echo the size of the vector. This thought vector generalizes input and links to a target response. This is not a \"feed forward\" neural net. It is a recurrent neural net that continually retrains on the training data, which is often the marker of a true \"deep learning\" system. This model makes no assumption about purpose or predetermined output. It simply reinforces relationships between thought vectors over time. There is a deeply emotional resonance that is carried through conversation. The blurring of lines between social media, search, and messaging will result in a seamless and unified interface for digital technology. Driven by the mobile space's demand for streamlined UI design, we will become more reliant on assistive technologies that can anticipate, learn, and adapt to user input. ConclusionIt is important for the humanities to anticipate this new cultural space. When the Google autocomplete system was introduced to search, there were many cultural commentators decrying the loss of independ-ent thought and the potential for entrenching damag-ing stereotypes (Postcolonial). The loss of critical awareness and even just the ability to spell. Technolo-gy that offends our sense of what it is to be essentially human is usually the next important media type. Chat-ting with machines tends to cross such lines. There are practical uses for remedial education and composition studies. A functioning Teaching Assistant Bot capable of answering questions about deadlines, assignments, and course policy would be welcome by most educators. Indeed, an AI TA has been developed recently, but it is unclear if this system can be trained on any course material or was custom built for this class (Maderer). Generalizing these systems is a difficult task, to be sure. The newly open sourced Tensor Flow machine learning library can answer questions derived from a training set of just over a million words. When we consider the limits of machine learning in intelligent assistants, scholarly communication through chat interfaces is certainly the next logical step. However these systems require humans in the loop. They requirethoughtful and critical reflection. They require an attention to depth and nuanced meaning. They require a humanist in the loop. ",
        "article_title": "Chatbot Based Content Discovery: Faulknerbot in the Archive",
        "authors": [
            {
                "given": "Aaron",
                "family": "Mauro",
                "affiliation": [
                    {
                        "original_name": null,
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionHumanities scholarship is becoming increasingly collaborative, participatory, and public facing. As humanists take up digital tools to conduct and share research, larger teams are needed to complete ever more complex computational tasks. When blending these heterogeneous teams--which may include faculty, librarians, staff, undergraduates, graduate students, postdocs, and community contributors--humanists have an ethical responsibility to offer a fair and transparent accounting of research activities. Tracing the evolution of research contributions is necessary for a range of issues facing digital scholarship such as authorship allocation, promotion and tenure, and reports to funders. The allocation of credit and authorship is an increasingly thorny issue for teams with a range of possible roles and a variety of research outputs and media types. There is, however, a large amount of data being generated by these teams that is capable of describing and measuring the contributions made on a variety of platforms and by multiple team member and community partners.Despite our inheritance of social and collaborative tools, many of these systems elide the nuance and process of humanities based research. Knowledge creation is not merely a function of how much code is produced. New knowledge is often the result of a key insight made by a team of students, staff, and faculty. These insights are generated in a complex and overlapping system of mentorship, service, teaching, learning, and authorship that are deeply dependent, social, and human. With a system that is possible of visualizing the history of a digital project over the course of years, which may see the ranks of team members change over time, primary investigators and project funders will be better able to address often thorny and ethically charged issues relating to student assessment, mentorship, authorship, promotion, and tenure. Credit, promotion, funding, and credentialing are more complex topics than ever, yet many individuals and institutions rely on simple, outdated structures to assess the value of insights made by networked teams. Social Knowledge CreationThe Penn State Digital Humanities Lab (Penn State Behrend) in partnership with the Teaching and Learning with Technology group (Penn State University Park) has developed a prototype of an ongoing project entitled the Social Knowledge Timeline (sktimeline.net). By linking together popular collaboration tools, the SKTimeline stores, analyzes, and communicates user data in three distinct areas of social knowledge creation:• Collaboration Platforms: Many scholars are turning to collaboration platforms like Slack, Yammer, and Basecamp to organize teams and foster communication within teams. These systems use an interface similar to a social media feed to pool project member input into a single narrative and eliminate the need for email. These systems help share documents and support conversations that may lead to drafting manuscripts on Google Drive and other services. These platforms offer a rich, conversational natural language data set that describes how team members mentor and support each other over time.• Version Control Systems: Github and Bitbucket are two of the most common version control platforms. These tools help facilitate large programming and encoding projects by allowing multiple coders to work simultaneously. When a team member \"commits\" code, a commit message describes the nature of the contribution as well as the date and time. This message will offer a highly granular view of coding projects as they unfold. Similarly, by including a feed from Google Drive's own version control system, document authorship may be traced with similar precision.• Social Media: Platforms like Twitter, LinkedIn, and Facebook have proven to be fast paced and engaging areas for social and cultural exchange. Twitter has long been a particularly important site for digital humanists. The SKTimeline draws together multiple hashtags and user handles to frame preserve and contextualize this often ephemeral site of both popular and scholarly debate. Hashtags associated with digital projects, conferences, publications, and even course work can be analyzed and set in real time with other platforms.Credit allocation in large teams is dependent on our ability to describe, quantify, and visualize our activities. By analyzing the rich natural language conversations generated by teams, the SKTimeline solves these ethical and institutional problems. The appearance of \"Collaborators' Bill of Rights\" for digital humanities projects in 2011 is symptom of a need for greater clarity in heterogeneous collaborative teams (Clement et al 2011). The Modern Language Association's \"Guidelines for Evaluating Work in Digital Humanities and Digital Media\" are similarly responding to appropriate credit allocation for researchers. There is a need for a more formalized and automated system of data collection and analysis for collaborative researchers across the university. Machine Learning Contributor TaxonomiesThe Taxonomy of Digital Research Activities in the Humanities (TaDiRAH) is used to quantify and describe user contributions. Machine learning systems like Google's Cloud Platform is used to conduct language analysis, and translation, image recognition, sentiment analysis, and keyword extraction. Custom machine learning systems has also been layered on to these services using the Tensor Flow library to learn the project specific phrasing for contributions. Additional text analysis will be conducted using standard tools like the Natural Language Toolkit (NLTK) to link to TaDiRAH's defined contributions. This project will reshape authorship and credit allocation in the humanities and beyond, but it will also be a perfect test bed for an emerging set of artificial intelligence tools that are now finding common application throughout society. In this way, the SKTimeline is representative of a broader cultural trend toward AI systems in aiding research.  ConclusionUndergraduate course projects, ongoing faculty research with graduate researchers, digital humanities labs, and library based digital research projects are just some of the contexts this round of user testing will examine. The data collected on participating teams against interview and form based user surveys. This kind of socially oriented knowledge creation emerges from a community of practice that moves fluidly between curricular experiences and co-curricular research experiences often hosted in DH labs, libraries, and centers. The SKTimeline seeks to solve a critical problem within scholarly communication in a digital context. The SKTimeline offers a means to capture complex narratives that constitute the organic and nuanced unfolding of humanities research.",
        "article_title": "Credit Allocation with the Social Knowledge Timeline",
        "authors": [
            {
                "given": "Aaron",
                "family": "Mauro",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab - Pennsylvania State University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionImagine you're exploring the historic center of a city with its impressive town houses, churches and monuments. What if you could just use your mobile to find out about the historic buildings around you, with detailed visual information about how they were built and the story behind them, mak-ing history come alive before your eyes?Photographs and plans are an essential source for historical research (Burke, 2003, Paul, 2006, Wohlfeil, 1986, PérezPérez-GómezGómez and Pelletier, 1997 and key objects in eHumanities (Kwastek, 2014). Numerous digital image archives, containing vast numbers of photographs, have been set up in the context of digitization projects . These extensive repositories of image media are still difficult to search. It is not easy to identify sources relevant for re-search, analyze and contextualize them, or compare them with the historical original. The eHumanities research group HistStadt4D, funded by the German Fed-eral Ministry of Education and Research (BMBF) until July 2020, is investigating and developing methods and technologies for this. The junior research group consists of 14 people - including 4 post-doctoral and 4 PhD researchers. Since a focal interest is to comprehensively investigate how to enhance accessibility of large scale image repositories, researchers and research approaches originate from the humanities, geo- and information technologies as well as from educational and information studies. In contrast to adjacent projects dealing primarily with large scale linked text data as the Venice Time Machine project (2017), sources addressed by the junior group are primarily historical photographs and plans. Historical media and their con-textual information are being transferred into a 4D - 3D spatial and temporal scaled - model to support research and education on urban history. Content will be made accessible in two ways; via a 4D browser and a location-dependent augmented-reality representation. The prototype database consists of about 200,000 digitized historical photographs and plans of Dresden from the Deutsche Fotothek (German photographic collection). Key Aspects Usage scenarios and research valuesDigital image repositories meet a wide range of needs, from research in humanities and information technologies, through museum contexts and library studies to tourist applications (Mü nster, 2011). Architectural historians have developed various methods of analyzing both preserved and never-built or destroyed structures in chronology and context (Brassat and Kohle, 2003). Style analysis, iconographic approaches, and art sociological methods all address structural historical questions. The technological possibilities of digital image repositories allow architecture historians to draw on a much larger stock of material, and to process and evaluate this from new perspectives. In addition, innovative software tools can be used to locate sources temporally and spatially, or to support dating, stylistic criticism, authorizations or archaeological investigations (Verstegen, 2007). Depending on the user group, a number of contradictory requirements must be met. Historical researchers, for example, need to be able to compare and contextualize sources (Mü nster et al., 2015, Brandt, 2012, Wohlfeil, 1986, and to trace the relationship between source and representation (Favro, 2006, Niccolucci andHermon, 2006). This includes identifying formal patterns, singularities, and discontinuities in architecture and cityscape. This raises a host of questions: How do buildings and cities change over time? In which contexts, such as political or formal developments, does a historical cityscape evolve? What similarities can be found between objects in terms of construction standards and requirements, building codes, regional, temporal or personal tastes and styles?The research group will address these and many more questions in a specific project on the interdependence between urban development and urban photography. Creating targeted tools for working with image repositoriesAn adjacent task will be to perform a systematic survey of the needs of users of image repositories, whose findings will be used to conceptualize technological support options. As historic images, objects and information are increasingly being digitized on a massive scale, more content becomes available for investigation; more cross-analyses are possible; more knowledge is collected, structured and shared (Schuller, 2009). The new scale of research and information retrieval creates many new challenges. Many scholars note that online searching for images and information is \"counter-productive\" due to the amount of irrelevant data retrieved or their limited technical abilities ( Beaudoin and Brady, 2011). Access and efficient data retrieval is inhibited for a variety of reasons. The degree of search expertise is as important as the functionalities and usability of the platform ( Kemman et al., 2014). A lot of the existing tools of research programs and applications stem from computer science and do not necessarily meet the needs of humanities scholars (Dudek et al., 2015). Users need efficient search and filter functions, an intuitive software interface and navigation system ( Barreau et al., 2014). Appropriate documentation through metadata plays an important role in ensuring sustainability (Bentkowska-Kafel et al., 2012, Maina andSuleman, 2015). In contrast, users expect an intuitive and feasible introduction to the topic and data (Maina and Suleman, 2015) with options to find out more as required. The simplest way to link and contextualize visual information is to use highlighted keywords as hyperlinks in texts and captions. Data interaction and processing tools are also essential for research (Webb andO'Carroll, 2013, Hecht et al., 2015). Photogrammetricmethods of visual knowledge generation A possible technological basis for creating access to large scale image repositories is the spatial and temporal aggregation of data, in this case historical photographs in a 4D model. The potential of photographic images ranges from pure documentation in archaeology and monument preservation, through image interpretation, for example for damage documentation, to the produc-tion of complex 3D models for archaeological investigations (Bü hrer et al., 2001). Geometrical reconstruction from historical photographs is based on photogrammetry. Information from multiple 2D images is used to acquire 2D and 3D object geome-tries and have frequently been applied on historical and measurement images (cf. Wiedemann et al., 2000, Brä uer-Burchardt and Voss, 2001, Henze et al., 2009, Siedler et al., 2011). Since some decades, traditional analytical photogrammetry has increasingly been complemented by digital image processing and analysis. The elaborate process of manual image analysis can be largely automated, resulting in large image collections from which geometric in-formation can be generated automatically (Pomaska, 2011). To date, automated photogrammetric methods are generally used primarily to evaluate current, mostly digital images. So far, this has rarely been done for historical images, as it involves specific challenges. Scanned analogue records usually have unknown camera metrics, missing or minimal object information and low radiometric and geometric resolution. Our aim is to develop application-oriented tools for photogrammetric analysis of his-torical photographs, to integrate them into the process of historical image analysis (Fig 1), and to create a spatial relationship to today's situation.  Augmented realityThe prototype 4D model, and the 4D historical photographs, drawings, plans, and information within it, will be made accessible via a location or context related information access as augmented reality (Mü nster and Niebling, 2016). This technology has gained importance in the last few years and undergone extensive testing ( Livingston et al., 2008, Zö llner et al., 2010, Walczak et al., 2011, Chang et al., 2015, Chung et al., 2015. Augmented reality describes the enrichment of the real world through virtual data, which can include 3D models, texts, pictures, films or audio data. Enriching the reality or replacing parts of reality can help to bridge the cognitive gap between our daily life experience of a city-scape and its depiction in historical photographs ( Niebling et al., 2008). In the historical context, the viewer is able to interactively capture visual and textual information about objects in their historical spatial reference system ( Ridel et al., 2014). Our investigation will focus on the accessibility of historical data: How can interaction with virtual buildings be designed? Which metaphors can be used? How can augmented reality support educational and research settings? As an alternative path, the 4D model will also be accessible via a 4D browser interface for spatially and temporally located searches in media repositories. An basing application prototype of a research platform for 3D reconstruction projects is in development was developed during a master thesis (Bruschke, 2015), employing approaches, such as semantic data linking and visualization of temporally and spatially arranged information ( Gouveia et al., 2015). Since the prototype has focused on individual building complexes, the 4D browser application has to visualize an entire city model, which also changes constantly over time. Moreover, a visual interface is proposed to make additional information accessible, such as the current and original location of the depicted object. Further features intended to support scholarly users of the prospected platform are image rectification tools and overlays combining several pictures from different periods which can shed light on changes in a building. Statistical analyses of photographed objects over time may provide information on a building's significance. Last but not least, the application should be intuitive to operate for a heterogeneous user group (Warwick, 2012). 4D browser SummaryAs a result of huge and concerted digitization efforts, extensive digital repositories of historical photographs have been created in the past few decades. This volume of data presents a major challenge to support search, access and information enrich-ment for users. In August 2016, the HistStadt4D research group started exam-ining scientific methodological requirements and intuitive user interfaces for dealing with massive media repositories from a multi-disciplinary perspective.",
        "article_title": "Novel Approaches to Research and Discovery Urban History",
        "authors": [
            {
                "given": "Sander",
                "family": "Münster",
                "affiliation": [
                    {
                        "original_name": "Technische Universität Dresden (TU Dresden)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Cindy ",
                "family": "Kröber",
                "affiliation": [
                    {
                        "original_name": "Universität Würzburg (Julius Maximilian University of Wurzburg)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Kristina",
                "family": "Friedrichs",
                "affiliation": [
                    {
                        "original_name": "Technische Universität Dresden (TU Dresden)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Jonas",
                "family": "Bruschke",
                "affiliation": [
                    {
                        "original_name": "Universität Würzburg (Julius Maximilian University of Wurzburg)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Frank",
                "family": "Henze",
                "affiliation": [
                    {
                        "original_name": "Technische Universität Dresden (TU Dresden)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Florian",
                "family": "Niebling",
                "affiliation": [
                    {
                        "original_name": "Universität Würzburg (Julius Maximilian University of Wurzburg)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn the mid 1990s, the Natural Language Processing Group at the University of Leipzig began work on the Wortschatz project which aims to provide corpora in hundreds of languages and in different size-normalisations, be that 100K, 300K or 1M sentences. As the resources grew in size, so did the number of requests for the data. In the early stages of the project a specific dump was created, parts of which even came with a small user-interface. The database dump was shared with interested researchers and partners in the business sector.After some time, however, the personnel costs of this kind of collaboration became unsustainable. For this reason, a new plan was put into motion in 2004, consisting of the development of a SOAP-based APIthe Leipzig Linguistic Services (LLS) -that enabled any interested person to access the data of the Wortschatz databases in any provided language ( Quasthoff et al. 2006, Eckart et al. 2012). Overall 20 services were provided, delivering specific information such as baseform, category classifications, and thesaurus data. The aim of the LLS was to establish a Service Oriented Architecture (SOA) for linguistic resources based on small and atomic micro-services that could be combined by users for particular needs. Users were then not only able to browse through the Wortschatz website, but also to integrate those services with their own existing digital ecosystems.In 2005 these services were made publicly available and by September 2006 all requests were systematically logged. In July 2014 the number of logged requests reached nearly one billion. While at the beginning the use was limited to academia, over time the services were increasingly used by the private and business sectors as well. Figure 2. Four workflow modes with separation of concern: editing (yellow); managing, compiling and deploying (red); hosting and operating (blue); using the LLS infrastructure (green). The Leipzig Linguistic ServicesThe intention of the overall LLS architecture was to be as simple and generic as possible. A generic architecture can be reused in different scenarios but tends to have too many parameters and options, while a simple architecture claims usability and guarantees a faster learning curve. In the following, we briefly describe the architecture of the LLS.In order to create the server-side Java code for a specific webservice, a data-set needed to be added to the webservice management (yellow zone in figure  1). The necessary edits contain, besides others, information on the name and type of the webservice (see also table 1) or parameters. Apache Ant was used as the central tool for generating the back-end services and deploying them in a Tomcat server (see red zone in figure 1). The blue zone illustrates the operations of the Wortschatz databases. Using the generic description of the webservice in the WSDLfiles a number of wrappers of generated source code were created and made publicly available by LLS users such as for C# as part of .NET, Perl, Python, Delphi, PHP, Ruby and JavaScript (see green zone in figure 1).Independently from the underlying programming languages, over the past ten years we have observed different uses in research, business and in the private sector. In research, the LLS were used in the areas of text profiles and author classification (Borchardt 2005). The services were also used as data resources for sentiment analysis or for query expansion. Users from the business field were mainly interested in using Baseform or Synonym services for improving internal search indexes. The LLS data was also used for information retrieval tasks in portals for weighting words in a word cloud or to display enriching information. Private users accessed the LLS to complete crossword puzzles. A dedicated service was installed upon request just for this purpose (see also table 1), since it was possible to query a pattern of an incomplete word with a given word length limitation. From 2008 the SOA-based cyberinfrastructure of LLS was re-used in Digital Humanities projects such as eAQUA and eTRACES (Büchler et al. 2008). Table 1 provides an overview of the 20 services offered with a breakdown of the requests and the responses. Over half of the requests (64.6%) were made to the Baseform service. Similarly, services with high-quality and often manually-curated data, such as the Thesaurus and Synonyms services, were requested more often than the quantitativelycomputed Similarity service, which provided similarly used words by assuming the distributional hypothesis (Harris 1954), and thus compared the cooccurrence vectors of two words. Even if the coverage for this service, 66.02%, is significantly higher than, for example, the Category (35.92%) or the Synonyms (4.47%) services, users appeared to prefer precision over recall for their end-user applications. Low coverage is also caused by requests to German language databases, especially by compound nouns that cannot all be included in a Baseform or Category service. Many multi-word units (MWU) were also requested. Out of all the requests, 84,760,875 (8.78%) were MWUs. With regard to the distribution of the webservice usage, only the two most frequently requested services, Baseform and Category, were queried more often than the total count of the MWU requests. This speaks to the impact of MWUs. ResultsThe less frequently used webservices in table 1 were primarily limited to internal uses, to newly installed services or, as was the case for the Crossword Puzzling service, to manual usage instead of automatic bulk requests. The following questions are discussed in the paper:1. Geographical distribution and spread of requests 2. Requested languages distribution 3. Requests by cleanliness in terms of broken encodings or sending HTML code 4. Temporal distribution including lessons learnt from incompatibility issues of used software and their new versions causing a decrease in service usage 5. Identified service chains of the atomic LLS micro-services that users built on the client-side 6. Experiences for load balancing of linguistic services 7. Interoperability issues of programming languages and interpreting the WSDL-files differently 8. Comparisons of SOAP-and REST-based webservices Conclusion\"If you build it, they will come\" is an infrastructure mantra that we can answer given the atomic micro-services of the LLS (more critical view by van Zundert 2012). However, with regard to easyto-integrate and atomic micro-services we found that users were generally very pragmatic as they requested everything that they had found in texts or on webpages, such as RGB colour-sets, URLs and other meta-information. Based on the log-files, we conclude that it is easier to request a token and look for a match in the LLS database of millions of words rather than to invest only little time in conventional pre-processing and pre-selection on the client-side. Similarly, users repeatedly requested function words, sometimes only a few minutes apart. This user behaviour entailed a significant server load and user control over the requests. This type of recurring request on unchanged data could only be considered as spam.We found that providing an infrastructure like the LLS over the course of a decade challenges the compatibility of used software components.Moreover, from a Natural Language Processing (NLP) standpoint, the results contribute to existing conversations about the difficulty of building balanced and representative corpora. In fact, user interests detected in the LLS log-files can help to enrich corpora by adding further topics. The contribution also touches upon discussions about qualitative and manually-curated data versus automatically-computed and quantitativelyavailable results of language technology algorithms. Notwithstanding the improvement of NLP algorithms, our results show that users prefer qualitative data and that they often request these services even if the domain and concept coverage is relatively low. The conclusion we draw from the user behaviour observed in almost one billion requests is that research fields, including the Digital Humanities, should share their data -no matter how small-through large infrastructure initiatives like DARIAH and CLARIN in order to increase the textual coverage of linguistic resources.Table 1 .1Overview of requests made to LLS between 2006-2014, in descending order. The Responses columns only list responses whose value was not empty.For space  constraints, the values in the Input Fields column are  abbreviated: Word (W.), Limit (L.), Pa   ",
        "article_title": "A Ten-Year Summary of a SOA-based Micro-services Infrastructure for Linguistic Services",
        "authors": [
            {
                "given": "Marco",
                "family": "Büchler",
                "affiliation": [
                    {
                        "original_name": "Georg-August-Universität Göttingen (University of Gottingen)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Thomas",
                "family": " Eckart",
                "affiliation": [
                    {
                        "original_name": "Universität Leipzig",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Greta",
                "family": "Franzini",
                "affiliation": [
                    {
                        "original_name": "Georg-August-Universität Göttingen (University of Gottingen)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Emily",
                "family": "Franzini",
                "affiliation": [
                    {
                        "original_name": "Georg-August-Universität Göttingen (University of Gottingen)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionAlthough recent research acknowledges the potential of visualization methods in DH, the predominant terminology used to describe visualizations (prototypes, tools) narrowly focuses on their use as a means to an end and, more importantly, as an instrument in the service of humanities research. While acknowledging the broad range of possible approaches to visualization, we introduce the metaphor of the sandcastle to highlight visualization as a research process in its own right. We argue that building visualization sandcastles provides a holistic approach to interdisciplinary knowledge generation that embraces visualization as (1) a dynamic interdisciplinary process where speculation and re-interpretation advance knowledge in all disciplines involved, (2) a mediator of ideas and theories within and across disciplines and (3) an aesthetic provocation to elicit critical insights, interpretation, speculation and discussions within and beyond scholarly audiences. We illustrate our argument based on our own research of an exceptional literary collection. Visualization tools vs. sandcastlesPivotal Scene A steering committee meeting for a large-scale DH project. The goal of this interdisciplinary project is a combined computational and literary analysis of a literary collection, which will include the development of visualizations to enable the open-ended exploration of this collection by literary scholars. As the discussion starts to focus on the intended project outcomes, questions around the visualizations and their practical and research contributions arise. What role do visualizations play as part of DH projects? What makes them a valid contribution? One committee member brings it to the point: \"Are we building tools or just sandcastles?\"This question contrasts sandcastles-tailored, unique, often stunning yet also transient and unstable interactive visualizations-with more pragmatic, functional and transferable visualization tools. This framing is a provocation: these approaches are not necessarily diametrically opposed or mutually exclusive, but, rather, exist along a rich continuum. Even within one research project, the process can shift from a more transient 'sandcastle' to a more targeted instrumental approach. And yet, the preference toward the latter is evident in recent DH discussions ( Gibbs et al., 2012) and in a push by funding bodies toward research with concrete, high-impact outcomes. Notably, however, visualization 'tools or prototypes' (terms typically used interchangeably) are not usually seen as research contributions in their own right ( Schreibman et al., 2010) but, at best, as facilitators of research or as a way to communicate underlying research contributions. An overly pragmatic approach to visualization, and DH tool-building more generally, however, not only risks overlooking the value of the design process but also relegating computer science and design to service-based roles. What happens when we consider, as Bruno Latour has argued, that \"far from fulfilling any purpose\", a new technology actually \"explor [es] heterogeneous universes that nothing, up to that point, could have foreseen and behind which trail new functions\" (Latour, 2002: 250)? What happens when we attend to the design process-and its many detours-as a research process in and of itself?As a relatively young research field, information visualization (InfoVis) has seen calls to carefully and critically (re-)evaluate sometimes dated assumptions (see Kosara, 2016). Similarly, despite the increasing application of visualization in diverse DH contexts (Jä- nicke et al., 2015), it remains a relatively new approach and a call for generalizable visualization tools-drawing on science-based use cases-may reproduce unexamined assumptions and overlook important nuances of humanistic data and inquiry that is typically of a qualitative and interpretative nature (Drucker, 2011). As Latour argues, the ways in which we represent our arguments changes the way in which we argue (Latour, 1986). Introducing visualization into literary studies, introduces new modes of knowledge production. As such, we need to engage in open-minded and open-ended explorations of visualization as a research (rather than engineering) process, paying close attention to the ways this process changes our perspectives on data and research questions. At a time when leading practitioner-theorists suggest design as central to DH (Burdick et al., 2012), we must develop a more nuanced, critical language to discuss and further engage with the wide range of design approaches, especially from fields such as InfoVis and human computer interaction (HCI) that already combine design practice and research.The call for a broader perspective on technology design within DH is not new, but it is increasingly urgent as the pragmatic value of visualization tools risks overshadowing the profoundly fertile design process as an intellectual and cognitive practice or a \"method of thinking-through-practice\" (Burdick et al., 2012). Previous work has discussed \"tools\" in DH as \"experiments\" or \"embodiments of ideas\" (Sinclair et al., 2011), advocated for prototypes as arguments in their own right (Galey & Ruecker, 2010), and highlighted visualization as a starting point to humanities research rather than a means to an end ( Forlini et al., 2015b;Hinrichs et al., 2016). Furthermore, critical perspectives from within the DH (Drucker, 2011) and InfoVis communities ( Dörk et al., 2013;Hullman & Diakopoulos, 2011) call for further examinations of the rhetoric of visualizations. Expanding on these discussions, we reclaim the 'sandcastle' as a lens through which to critically examine current DH discussions of technology design and to promote an open-ended, speculative and process-oriented approach to visualization design based on a robust model of interdisciplinary collaboration that advances knowledge in all research fields involved. Our argument is grounded in critical theory, design research, HCI and InfoVis, as well as in our own experience of combining research in literary studies and visualization to explore an untapped collection-the Gibson Anthologies of Speculative Fiction ( Forlini et al., 2015b;. Building sandcastles at the intersection of literary studies and InfoVisOur project-the Stuff of Science Fiction-explores a vast untapped collection of 10,000+ science fiction stories single-handedly compiled into 888 handcrafted anthologies by the avid science fiction fan, artist and collector Bob Gibson (see Fig.1). This unusual collection raises a number of questions regarding the evolution of science fiction in the context of popular periodicals and the role of fan practices in sustaining and promoting this popular genre. Working with a subcollection of 1,500+ stories, we developed interactive visualizations that came together as the Speculative W@nderverse (see Fig. 2) to help us explore and analyze these stories through their metadata.The W@nderverse can be considered a tool, or at least a prototype, and we have discussed it as such in our own humanities (Forlini et al., 2015a;Forlini et al., 2015b) and InfoVis publications ( Hinrichs et al., 2016). In many ways it is a means to certain valuable ends: (1) it makes the Gibson anthologies explorable from different (visual) perspectives by multiple scholarly and public audiences, (2) it has generated insights about the collection, and (3) it showcases InfoVis design considerations specific to visualizing untapped literary collections (Hinrichs et al., 2016).  However, if we reflect on our process with our initial research questions on one end and the visualization as a reflection of our research outcomes on the other, it becomes clear that the W@nderverse is not just a tool, at least not in the narrowly instrumental sense. It only appears to be a means to certain ends in retrospect when we overlook our many detours in order to narrate (for the sake of dissemination) a direct line from our questions to our contributions. However, our grant proposal and the copious notes through which we documented our research process (see Neu- staedter & Sengers, 2012) remind us of our initial intentions and reveal the transformative nature of our collaborative \"prototyping\" process, which profoundly altered our research questions and intentions as well as our perspectives on the collection and our respective disciplines-literary studies and InfoVis. The W@nderverse is therefore both the mediator and manifestation of our exploratory and interdisciplinary research process. Our many design detours (necessitated by ongoing archival discoveries and visualization experiments, see Fig. 3), show what is now largely invisible yet fundamental to the W@nderverse: our research thinking through visualization, an approach that has its parallel in HCI with \"research through design\" (Zimmerman, 2007). In order to investigate humanities questions from truly novel perspectives and to engage in profoundly interdisciplinary collaborations that combine humanities and visualization research (not engineering!) approaches, we advocate for research thinking through the creation of visualization sandcastles as:• Aesthetic and in-flux manifestations of visualization as a speculative, provocative process that generates insights about: 1-the underlying collection; 2-(visualization) design considerations; 3-needs of the intended audience(s); 4-and new research questions which, in turn, drive the development of new (and different) sandcastles and grounded insights valuable to all involved disciplines,• Dynamic mediators that by provoking and guiding discussions can bridge boundaries between disciplines (e.g., literary studies & InfoVis) and between academic and fan endeavors, and• Aesthetic provocations that can promote critical discussions of best practices for studying and making accessible cultural collections among scholarly and public audiences. ",
        "article_title": "In Defence of Sandcastles: Research Thinking through Visualization in DH",
        "authors": [
            {
                "given": "Uta",
                "family": "Hinrichs",
                "affiliation": [
                    {
                        "original_name": "University of St Andrews",
                        "normalized_name": "University of St Andrews",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02wn5qz54",
                            "GRID": "grid.11914.3c"
                        }
                    }
                ]
            },
            {
                "given": "Stefania ",
                "family": "Forlini",
                "affiliation": [
                    {
                        "original_name": "University of Calgary",
                        "normalized_name": "University of Calgary",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03yjb2x39",
                            "GRID": "grid.22072.35"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Modelling Computer assisted conceptual analysis in text (CACAT) Jean-Guy Meunier Conceptual analysis paradigmsIn many fields of scientific research, be they social sciences, natural sciences or even professional practices, abstract or highly theoretical concepts are explored to discover their content and deepen the knowledge they embed. However, there is no consensus on the nature of a concept or on the methodology to analyze them. For example, how would one proceed in analyzing the concept of Evolution in Darwin's writings? Three radically different paradigms parameterize the methodology: philosophical, linguistic and cognitive.In the philosophical paradigm, concepts are identified to the meaning of predicative words. For some, their analysis aims at finding the conditions (necessary, sufficient, fuzzy, etc.) under which these words refer to objects, events or actions in a possible or actual world. For others, analysis consists mainly of identifying the sense or intention of these words as related to the epistemic or metaphysical conditions for their understanding. Finally, for some others, an analysis should consider the use and context (linguistic, social or other) of these words. Hence, in this philosophical paradigm, conceptual analysis becomes a sort of logico-pragmatic analysis of the meaning of words. In our Darwin example, this paradigm would therefore  ask what are the meaning conditions of the word evo- lution when Darwin uses it. In the linguistic paradigm, concepts are also related to the meaning of words. For the Saussurian structuralists a concept is the core meaning embedded in the structure of the signified (le signifié)signifié) of words. For the neo-structuralists, the generativists and the cognitivists, a concept is also equated to the semantic content of predicative linguistic expression, and meaning is understood as a complex set of semantic properties (features, relations, frames, nets, etc.,) underlying isolated words or their position in sentences and discourse. Here, conceptual analysis becomes identified with classical semantic analysis of words. In Darwin's works, the analysis would explore the semantics properties of the English word evolution: for instance, it would study its lexical content, its synonyms, its topics, is semantic nets, etc.In the cognitive paradigm, concepts are the results of cognitive or mental operations. For psychology, they are seen as a sort of cognitive categorization. For the analytical and hermeneutic traditions of philosophy of mind, they are mental states or world representations. Conceptual analysis consists then in exploring how semiotic or linguistic forms embed categories, intentions, conceptual spaces, beliefs, mental states, Weltanschauung, etc. Hence conceptual analysis bears resemblance to an exploration of cognitive operations or states: representing, categorizing, reasoning, argumenting, entailing, etc. In our analysis of Darwin, this cognitive paradigm would focus the analysis on the mental operations underlying the meaning of evolution. How is this category of mental representation acquired, built reasoned on, argued, etc.?Choosing a paradigmatic methodology for analyzing concept is difficult, then, because not one of them is canonical. Conceptual analysis becomes an even more acute problem when computations are introduced in the methodology. The level of complexity of the task is so high that is not obvious how a computer assisted conceptual analysis of text (CACAT) project can be realized. Should it be computer-tool-driven or model-driven? Tool driven approachesThe first type of approach is tool-driven. Once a methodology inspired by one of the paradigms presented above is chosen, its practitioners use some computer programs already built and inserts them in appropriate moments of the analysis procedure. Many computer tools for this task actually exist.A first set of tools focuses on the lexical expressions of a concept. The most classical ones are concordancers, collocation and lexical analysers, taggers, etc. These tools explore the lexical properties and contexts of one or a few canonical predicates, expressing the specific concept to be analyzed. The limits of these types of tools lie in their underlying design hypothesis: a conceptual content is to be explored through specific canonical expressions. Such a hypothesis restricts the exploration of the conceptual content to one or a specific number of predicates. This is problematic, for as we know, concepts can be expressed in language in a myriad of ways. For example, it would be very problematic to restrict Darwin's concept of evolution to the analysis of the word evolution alone. Secondly, they may produce results that are larger than the original text. This is the case of the concordance of the concept of Esse in the Thomas Indexicus. Finally, sometimes, the opposite happens. These tools may deliver only a fraction of the overall textual segments or word collocations whose content is pertinent. For instance, in Darwin only uses a few dozen times the lexical form evolution. Hence concordance, collocation, etc. on such a small sample are not very fruitful for a conceptual analysis.A second set of tools highly influenced by classical AI approaches focuses on natural language processing (NLP). These tools are sensitive to various meaning aspects of words, such as their semantic definition, their encyclopedic, pragmatic discursive content, etc. They promise to deliver finer results for a conceptual analysis. But these tools also have limits. Their underlying hypothesis is that these semantic, pragmatic and encyclopedic information added in the grammar and the lexicon will enhance the exploration of conceptual content. Unfortunately, the added information has often been collected from common and ordinary semantic knowledge of shared language usages. Such tools will then often tend to identify already known properties belonging to this common information about the lexical conceptual word under inquiry. And most of the time, it will ignore the properties that precisely are the one that are specific to the concepts analyzed mainly when they are original, and belong to a reflexive, creative literary or reflexive discursive process, etc. These semantic properties would not be part of the common doxastic conceptual content. For instance, a philosophy scholar using such types of tools would not be very satisfied in discovering that Darwin's concept of evolution is a name meaning an action of the type change and applied to the object: natural species.Recently, a last set of tools that are more mathematically grounded, such as neural net and Bayesian classification, vector semantics, machine learning, deep learning, etc., have become appealing and are used in language processing, They can process large data and learn semantic information by themselves. But like the other set of tools they have their limits. First, they are nor readily usable. They are in fact very complex algorithms, and are not easily mastered by humanities scholars. Secondly, their lack of traceability becomes a major obstacle when applied to large and theoretical textual data where results become difficult to evaluate. Thirdly, they seem more successful for information retrieval applications than for digging into deep conceptual content. For the moment, we are not sure that how they can effectively assist conceptual analysis.From these remarks, it does not seem to us that conceptual analysis can only be a serendipity tooldriven approach. The results produced by these tools have not yet convinced the scholarly community that practices expert conceptual analysis. Model driven approachesThe second type of approach is model-driven. Recent philosophers of science such as Morgan and Morrison (1999), Giere, (1999), Leonelli (2007) for instance, see science as a building models process where models are heuristic means for describing, explaining and understanding reality. And Mc Carthy (1999), has seen this modelling approach as a means of better understanding digital humanities interpretative projects. For our part, we explore this hypothesis and see CACAT as type of scientific inquiry where various models are used as intermediaries for understanding the analysis of highly theoretical and abstract concepts. In this perspective, we distinguish four types of models: conceptual, formal, computational and experimental.A conceptual model defines parameters for identifying, explaining and understanding the properties and structure of linguistic items expressing conceptual content. A formal model translates certain aspects of a conceptual model in some controlled formal language that describes or identifies properties and relations of these conceptual expressions. A computational model translates some formal expressions of the formal model into algorithms and programs. Finally, an experimental model designs implementation of these formal models in a concrete computer where the analysis can be simulated and ultimately evaluated in correspondence to the other models.In a concrete procedure, all these models interact and can be modified and adjusted. This allows the inquiry to be controllable and repeatable. It has been our own experience that, if a computer assisted conceptual analysis project is to be successful it must construct at least these four models. A CACAT project cannot bypass these models and their interactions.Designing these models, their interactions and their experimentation to see CACAT as a scientific endeavour and not just computer gadget exploration. But each model is not built easily. And nothing comes smoothly. They are part of the research process. And much work must be done to clarify the conceptual, formal, computational and experimental models pertinent for a successful and pertinent conceptual analysis. Digital epistemology for concept analysis Mathieu ValetteIn the humanities, theory is most of the time outlined with texts: papers, books, conference presentations, lectures etc. we claim that the scientist is first a reader and a text producer. This textuality is so ordinary that it is almost invisible, and, as such, not considered as an object of science. Moreover, theories are read as synchronic systems, or even achronic systems, depending on their specific purposes (describing one fact, explaining one phenomenon...). Scientists appropriate models and concepts like tools; they have to know their function and how to manipulate them, but they do not care about knowing practical details of their enunciation. In fact, they ignore them, more or less. They find such details embarrassing, because they make concept borders fuzzy: lexicons, glossaries, and also handbooks, as they extract the concepts from their context, and standardise the definitions, creating an illusion of stability and tangibility. But concept textuality necessarily has an incidence, not only on interpretation, but also on theorisation. If the scientist is a text producer, then theorisation is the construction of meaning. Theorisation is forced by enunciation, and scientific works, beyond their materiality, can be considered as text.The textual aspect of scientific works had been noticed by those in Europe looking at epistemological culture. In this respect, French philosopher Michel Foucault's works, in the 1960s, must be acknowledged (see e.g. Foucault 1969). Foucault put in place a philological analysis of discourse centred on the combination and evolution on specific discursive structures. His purpose is, firstly, to recognize \"discursive formations\", i.e. stabilized relations, regularities between objects, types of speech act, concepts and topics; and, secondly, to recognise breakpoints in idea system his-tory. Foucault followed the example of some of his famous predecessors, such as Gaston Bachelard, Georges Canguilhem and Martial Gueroult. Bachelard's notion of Epistemological break, or Canguilhem's notion of concept shifts shows, for instance, that the history of a concept is not that of its increasing rationality and refinement, but that of the different fields in which they have been designed and validated. What we will call digital epistemology is a linguistic approach to this style of French epistemology.Our topic is the study of scientific texts using, on the one hand, corpus linguistics tools which have been developed over the 40 last years and, on the other hand, a linguistic methodology (see Rastier 2009, Valette 2003. Thus, our purpose is to develop tools and methodology Foucault did not have, among other reasons because some textual phenomena-as, for example, lexicon evolution, which depends on the reader's subjectivity-are invisible to a classical philological analysis. Concept emergence, concepts' individual and inter-related evolutions, the appropriation of a specific thematic, palinode, etc. constitute further examples. We do not adopt the logician's position, considering that conceptualization is a linguistic phenomenon with its own construction rules linked to a particular function of language. Neither do we ignore the psychological, social and interactional reasons of the development of concepts. Firstly, we consider that textualityi.e. the constraints of the textual layout, formulations, be they constraints of syntactic, semantic, lexical or related discursive traditions (including genres and speech)-plays a major role in concept formation. Secondly, we do not consider texts only as resources to mine and extract terminological and conceptual material, but as archives, or, in other word, as the objective tracks of the process of creating concepts.In essence, we focus here on concept emergence considered as the result of a slow and gradual stabilisation of contextual semantic feature. Drawing on recent critical readings of Saussure's semiology (see Rastier 2015), we propose to consider a concept as a stabilized semantic form; that is, as a combination of semantic features (or semes) mainly inherited from various contexts in which it has occurred. Eventually, we link concept design with text production rather than identification of items in a general ontology (Valette 2010). Topic models for conceptual analysis Louis ChartrandThe last two decades have seen the rise of topic models in natural language processing (NLP). From the early successes of Latent Semantic Analysis, which decomposes datasets into \"conceptual\" dimensions, the introduction of probabilistic and generative models have enabled the discovery of underlying structures that condition the lexicon of a text. Those structures, in turn, are used to construct meaningful representations of corpuses and documents, and have proven fruitful in improving performances in many NLP tasks.Those tools have interesting potential for the Digital Humanities, as they discover entities which are, on one hand, robust features of textual data, and, on the other hand, easily representable and interpretable by humans. For instance, topics may help in tasks such as categorizing documents or selecting a relevant subcorpus for analysis. However, once topics are represented using the words to which they are likely to be associated, they can also be used to make sense of what a set of textual segments are about, or to visualize the evolution of discourse in a corpus through time. As such, topics have interesting potential when it comes to representing textual data and improving our analyses of it.In this presentation, some prominent topics models-LSA, LDA and DTM-will be presented and contrasted, and their potential uses for Digital Humanities will be discussed. Latent Semantic Analysis (LSA)Introduced by Deerwester et al. (1990), LSA used tools from linear algebra, in particular singular vector decomposition, to transform the representation of text segments in the form of word counts to a representation in the form of participation to \"concepts\", or semantic dimensions.As words give us a good idea of what a text is about, it is common practice in text mining to represent text segments by counting its words. A document containing \"apple\", \"orange\" and \"pear\" a high number of times each is likely to talk about fruits. And if multiple documents share the same words, they are likely to share common topics. However, this approach does not fare well with synonyms, which it does not recognize.The LSA uses co-occurrence in word uses to synthesize the word-count representations into more compact semantic dimensions. As synonyms tend to have the same cooccurents, they also tend to participate to the same semantic dimensions. As a result, the new representation is closer to a semantic representation than was the word-count representation, hence the name \"Latent Semantic Analysis\". Latent Dirichlet Allocation (LDA)While LSA still is part of every NLP reputable toolset, it falls short in at least two key aspects. Firstly, its semantic dimensions are hard to read for a human: from a list of its most prominent words, it is usually hard to give a satisfying interpretation of what a semantic dimension is about ( Chang et al. 2009). Secondly, it has no clear hypothesis as to how text is structured. On one hand, this makes it harder to explain semantic dimensions in terms of linguistics, psychology or discourse analysis. On the other, it means that LSA gets only part of the picture, and better algorithms with additional assumptions might produce better semantic dimensions.Latent Dirichlet Allocation ( Blei et al. 2003) is a probabilistic models which attempts to address this latter issue, and ends up addressing the former as well. It supposes that in a corpus, there is a certain number of topics, which, when activated, make it more or less likely for specific words to be present. Thus, when someone writes a document, LDA assumes that she selects a certain restricted number of topics, which in term condition which words will be found in the document. Using this assumption and an arbitrary number of topics, the algorithm infers the most likely list of topics, and their most likely assignments to documents.As such, it produces once again a representation of documents or text segments from word counts, but in terms of topics rather than more abstract conceptual dimensions. The words most likely to be present when a given topic is activated are often visibly related, either semantically or because they participate in a transparent narrative. As such, they are easily read by a human interpreter, and can be used to give a sense of what documents, sub-corpuses or textual segments are about. Dynamic Topic Models (DTM)Another boon of LDA is that its probabilistic model can be modified account for particularities of the corpus, or to model features that we want to study in particular. For example, if we have a corpus that spans across decades, we might expect topics to evolve with time, as society and culture change.To model this, the algorithm devised by Blei & Laf- ferty (2006) uses a corpus split in time slices (say, per year) and topics are split accordingly, such that topic 1 at time 1 is different from topic 1 at time 2. Then, a Markov assumption is enacted on the time series: topic 1 at time 1 conditions topic 1 at time 2, which conditions topic 1 at time 3, etc. This gives topics the freedom to evolve, while enforcing a certain degree of conservatism.Using this, one can not only track topics more efficiently, but also see the evolution of topics across time. What is a topic?What is it, however, that we are talking about when we speak of LSA's conceptual dimensions or LDA's topics? Can it be equated with the notion of TOPIC that we encounter in discourse analysis, for instance?While there are a variety of definitions for words such as \"topic\" and \"theme\", most agree that a topic is what a text is about (Rimmon-Kenan, 1995). On this score, LDA's topic does seem to agree with the common notion of TOPIC: a word list representing a LDA topic is read as a representation of what the textual data is about (Blei, 2013). Furthermore, the information the probabilistic model captures is the one that is redundant across a number of text segments. As such, it highlights words and concepts which keep coming back as they put in relation with various entities in sentences. In other words, textual discourse and narratives are being sewn around them.On the other hand, humans tend to make slightly different representations of topics compared to machines (Chang 2010), more readily constructing topics around concepts and thus providing sparser (more compact) representations. As Chang suggests, this might be because humans build these representations using general domain knowledge, whereas topic models try to infer this knowledge from word distributions. This seems to tell us that we should understand LDA topics as indicators, or reconstructed traces, of the topics that underlie a text, but not as true representation of topics themselves. Using topic models in conceptual analysisAs Chang's 2010 experiment suggest, topics entertain special relation with concepts, as a topic tends to be associated with a restricted number of concepts which are expressed very often in the text.As such, topic models' potential in representing textual data can be exploited to discover associations that are likely to be useful to conceptual analysis and other philosophical analyses. For instance, it can help the analyst identify the most important parts of a corpus, and those that can be discarded. They can also be leveraged to build representations of the contexts in which the concept of interest appears, thus giving a sense of the topics with which it is associated. Using DTM, one can also get a sense of the evolution of a concept within a diachronic corpus. Beyond discovery of new informations concerning a concept's expression in a corpus, topic models can be useful to test some association, as the structures they uncover are relatively robust.That said, as the DTM model shows, topic models can be used in a large variety of use cases, as their model can be expanded to take into account a corpus' metadata and thus open new and innovative avenues for conceptual analysis and the Digital Humanities in general. Unsupervised natural language processing for conceptual analysis of events Jackie Chi Kit CheungIn unsupervised machine learning, an algorithm is trained to discover regularities in data without access to human-provided labels. Such techniques can be useful in conceptual analysis of text, in cases where we do not have or want to impose a schema on the text corpus under analysis. The basic intuition behind unsupervised natural language processing techniques is that objects that appear in similar contexts in the data should be assigned similar representations, such that they can be grouped into clusters.Unsupervised models differ according to several characteristics, from the type of information that is made available to the learner, to how similarity is defined between the different objects that are modeled, to the expected form of the output cluster that is learned. For example, the Latent Dirichlet Allocation (LDA) topic model ( Blei et al., 2003) is a probabilistic model which is given access to multiple documents for training. The crucial assumptions behind the LDA model are that each document can be described as a mixture of multiple topics, and each word in a document is generated by one of the topics in that mixture. As a result of training an LDA model, multiple topics are learned, which correspond to clusters of words that tend to co-occur in the same documents.More recently, there have been a number of unsupervised models that have been used to discover the structure of a sequence of entities and events that appear according to some narrative in the natural language processing literature (Chambers and Jurafsky, 2008;Cheung and Penn, 2013). This is accomplished by explicitly modelling the sequential dependencies of events as they appear in a document. I will provide an overview of the assumptions of the event structure being learned by such models. For example, some methods produce discrete sequences of prototypical event and participant roles. In the work of Chambers and Jurafsky, (2008), narrative chains are learned corresponding to prototypical roles in a narrative. A chain such as _ accused X; X claimed _; X argued _; _ dismissed X might correspond to a defendant in a trial. Other work frame the problem as a task for probabilistic learning. Cheung and Penn (2013) define a probabilistic sequence model, in which the structure of an event and its participants are explicitly represented in the model as latent random variables. The nature of a learned cluster, then, would be how it influences the conditional probabilities of generating other cluster labels, as well as the word emission distributions from that latent topic (as in an LDA model).I will discuss how such models can be used to discover templates of prototypical events, including how events and event participants are typically expressed in language. Such approaches can easily be applied to multiple domains, including texts in the legal or medical genres, because they make minimal assumptions about the structure of events, and do not require training data. I also discuss other applications of these models to information ordering, and automatic summarization, which may be of interest to researchers in conceptual analysis for the digital humanities. A computer-assisted analysis of SYMPTOM in psychiatry Marie-Noëlle BayleThe Diagnostic and Statistical Manual of Mental Disorders (DSM-5) is a general classification and diagnostic tool used in the rich and diverse universe of mental health. Being widely distributed and available online, it allows everyone to have a direct access on how to make a psychiatric diagnosis. To facilitate its reading, laypeople and professionals alike may consult definitions for important notions in the glossary section. However, these few lines will often fail to capture the complexity of a term. For instance, at the core of the clinical assessment of a disorder lie its signs and symptoms. Therefore, a proper understanding of what a symptom means, and how this concept relates to the disorder, is essential to the diagnostic approach.In the DSM-5 glossary, symptom is defined as \"A subjective manifestation of a pathological condition.\"In the practice, it is often treated as a necessary and/or sufficient condition for to seek diagnosis, or as a constraint on possible diagnoses. As such, non-doctors and patients often think of a symptom as a singular event, and conceive of it purely in terms of content and a means to a diagnosis.However, as experience of mental disorders is often messier, we may wonder if such a notion of SYMPTOM overlooks important features of the way this notion is actually reflected in the DSM-5. Our hypothesis is that both the glossary definition for this concept and the common understanding of this notion fail to account for such essential aspects. To provide evidence for this claim, we performed a computer-assisted conceptual analysis of text (CACAT) for the concept SYMPTOM in the DSM-5. We find evidence that SYMPTOM is strongly associated to a dimension of temporality, and that it is expressed in relation not only with the disorder, but also remission. As such, it is proposed an improved definition would not only better reflect the content of the DSM-5, but could also contribute in a better understanding of assessment and practice of diagnostic. MethodThe dataset consists in a small corpus composed with the most relevant chapters of DSM-5. Computational Text mining method and manual qualitative approach were used in a processing chain for the conceptual analysis of SYMPTOM. Firstly, extraction of the textual data from noisy sources and cleaning of the text were performed. Secondly, all sentences with the term \"symptom\" in it, plus one sentence before and after, were extracted, yielding a set of textual segments, on which stemming and lemmatisation were performed. Thirdly, the pretreated data was used to create a document-term matrix with the TF-IDF weighting scheme. Fourthly, from the matrix, textual segment clusters were produced with the k-means algorithm. Fifthly, the most salient words in each cluster and in the whole subcorpus were first represented using word clouds. Finally, relations of similarity between the most relevant words in each cluster and in the subcorpus were represented in a 3d space. All steps were performed using common R modules (tm, RWeka, qdap, cluster, knn, ggplot2, rgl). Each cluster was interpreted as a specific field in which a hypothetical conceptual property of SYMPTOM is expressed. Categorization was done by annotating manually the most typical textual segments in every cluster according to cosine distance to the centroid. Annotations consisted in the main conceptual property of SYMPTOM expressed in a segment.Syntheses of these annotations were done for each cluster. ExperimentationFrom the subcorpus, 2036 sentences containing the term \"symptom\" were extracted which contained 5761 different word types. The words most associated with symptom in this subset of the corpus are disord (disorder), criteria, presen (presence), sever (severity), medic (medical, medication). Using k-means, 30 clusters were extracted but 17 are deemed noisy, as they contain 10 textual segments or less. Most of the remaining clusters are located in the section II of the DSM (diagnostic criteria and codes), several having most of their segments in a specific disorder chapter. The converse, however, does not hold. DiscussionAnalysing those clusters reveals that SYMPTOM has a transdiagnostic property, and is not only defined by its specific content. For example, let us examine cluster #8, which contains 98 segments, 90% of which fall in the three chapters about psychotic and mood disorders. Symptom in this cluster is linked with the words depress, hypomania, mania, but also with episode, period, full, meet. Furthermore, annotation of joined documents shows that temporality is a conceptual property of SYMPTOM. It modifies the pathological dimension of its content. SYMPTOM is not in direct causal relation with DISORDER; it is a dynamic sign whose presence needs to be situated in an episode, regardless of whether its content is depressive or manic. Therefore, the mere presence of a symptom is not sufficient for a diagnosis. Conversely, SYMPTOM is also linked with the concept of (partial) remission. SYMPTOM and DIS-ORDER have a complex relationship on a continuum between negative (remission) and positive (disease) poles.In conclusion, a mixed method, combining computational and manual processing and using quantitative and qualitative approaches, was applied in our conceptual analysis of the concept SYMPTOM. SYMPTOM appears to be a more complex and dynamic concept than patients and other non-doctors usually understand it to be. As a result, a better understanding of this complexity would likely profit assessment, diagnosis and treatment of mental disorders. ",
        "article_title": "Computer-Assisted Conceptual Analysis of Textual Data as Applied to Philosophical Corpuses",
        "authors": [
            {
                "given": "Jean",
                "family": "Meunier",
                "affiliation": [
                    {
                        "original_name": "Université du Québec à Montréal (Quebec a Montral - UQAM)",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Louis",
                "family": "Chartrand",
                "affiliation": [
                    {
                        "original_name": "Université du Québec à Montréal (Quebec a Montral - UQAM)",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Mathieu",
                "family": "Valette",
                "affiliation": [
                    {
                        "original_name": "Institut National des Langues et Civilisations Orientales",
                        "normalized_name": null,
                        "country": "France",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": " Jackie C.K. ",
                "family": "Cheung",
                "affiliation": [
                    {
                        "original_name": "McGill University",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Marie-Noëlle ",
                "family": "Bayle",
                "affiliation": [
                    {
                        "original_name": "Université du Québec à Montréal (Quebec a Montral - UQAM)",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Level 1Level 1 diagnostics provide project-level, as opposed to document-level, consistency checking to establish the internal coherence of the project, primarily through ensuring referential integrity. We borrow the phrase \"referential integrity\" from the MLA's \"Guiding Questions for Vetters of Scholarly Editions\" (2011), which advises peer-reviewers of digital editions that link to multiple databases to see if \"referential integrity [is] enforced within the database(s).\" This includes checking for non-existent pointers, duplicate @xml:ids across the project, and erroneously encoded references (e.g. tagging a place name as a bibliography reference). Ensuring referential integrity is particularly complex for projects that use \"abbreviated pointers\" to facilitate internal linking (see TEI Consortium (2016)), since it may not be obvious to the encoder which resource is being referenced by a pointer. Thus, the first level of diagnostics checks both whether or not an object pointed to actually exists and whether or not the markup correctly represents the relationship between the element and the target resource. For instance, to check all instances of the relationship shown in Fig. 2, a number of different tests are actually done: Figure 2: a simple referential integrity check. 1. Every <name type=\"org\"> points at an @xml:id which exists in the project. 2. The element pointed at by <name type=\"org\"> is an <org> element in the ORGS1.xml document. 3. Every <name> element which points at an <org> element in ORGS1.xml has @type=\"org\". For small-scale projects, this kind of referential integrity check could be accomplished with Schematron, since a Schematron rule using XPath 2.0 can read external documents, but for a project of any significant size, this is impractical. For example, Schematron checks to confirm the rules above may add around six seconds to document validation in the Oxygen XML Editor, causing frustration for editors, while simply checking that a linked location exists would require the processing of over a thousand files in this project, since each location is a distinct file. Level 2 While Level 1 diagnostics generally focus on coherence and consistency, Level 2 is more concerned with completeness. Level 2 diagnostics provide progress analysis, generate to-do lists, and identify situations that may indicate error, but require human judgement. These include cases in which:• Two bibliography or personography entries appear sufficiently similar that they may be duplicates.• Several <name> elements point to the same authority record, but the text of one of them is significantly different from the others, so it may point at the wrong target.• A document in the project is not linked from anywhere else, and therefore cannot be \"reached\".Such issues cannot be automatically rectifiedthey are not necessarily errors-but they must be examined. Figure 3 shows an example of the first check, which uses a similarity metric to identify potential duplicate bibliography entries. to identify duplicate bibliography entries. At Level 2, we also generate to-do lists for specific sub-projects, providing a set of tasks for the project team to focus on in order to reach a milestone or publish a particular document. The definition of \"done\" for a specific document may transcend the document itself. For instance, before we deem a particular edition of a text publishable, we may require that all authority records (people, places, publications) linked from that document are themselves complete, so the to-do list for a given document may require work in a variety of other documents in the project Level 3Armed with a comprehensive set of Level 1 and Level 2 diagnostics, and assuming our data is managed using a version-control repository such as Subversion or Git, we can now generate diachronic views of the project's progress. A script can check out a sequence of incarnations of the project, weekly over a period of months, for instance, and run the entire current diagnostic suite against it; we can then combine these snapshots to get a clear sense of how our work is proceeding. This also means that every time we develop a new diagnostic procedure, we can apply it to the entire history of the project to see the trajectory of project work with respect to the datapoint in question. Two examples, this time from the Nxaʔamxcín Dictionary project (an indigenous dictionary project described in detail in Czaykowska-Higgins, Holmes, and Kell (2014)) appear in Figs 4 and 5 below. Figure  4 shows the number of completed dictionary entries in orange, rising steadily over a period of 18 months, and the number of occurrences of a known problem: duplicate instances of the same gloss. These duplicates rise along with the number of entries until October 2016, when this issue was added to our diagnostics process, and the encoders were able to address it.  Fig. 5 shows cases of broken cross-references, which also tend to increase along with the number of completed entries, but we can see from the graph that the issue was aggressively addressed in two separate campaigns in fall 2015 and summer 2016. New instances continue to appear, however. The number of broken cross-references, tracked against completed entries. Fig. 6, from a different project, shows how this approach can be used to forecast completion dates for tasks in a project based on the progress rate so far.  ConclusionAs Matthew Kirschenbaum (2009) tells us, there \"is no more satisfying sequence of characters\" than \"Done.\" The overall purpose of a digital edition project is to finish and publish the edition, and this requires not only that the document-level encoding be valid, but also that the entire dataset be coherent, consistent, and complete. Programmed diagnostics enable projects to enforce coherence and consistency, manage the workflow effectively, and measure their progress towards completeness. ",
        "article_title": "Beyond Validation: using programmed diagnostics to learn about, monitor, and successfully complete your DH project",
        "authors": [
            {
                "given": "Martin",
                "family": "Holmes",
                "affiliation": [
                    {
                        "original_name": "University of Victoria",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Joey",
                "family": "Takeda",
                "affiliation": [
                    {
                        "original_name": "University of British Columbia",
                        "normalized_name": "University of British Columbia",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03rmrcq20",
                            "GRID": "grid.17091.3e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe automated detection of historical text reuse is still in its early stages. To reinforce its research, it is necessary to investigate the way a text is reused in order to understand the broader context. Here is where the necessity of lexical resources supporting this task comes in, especially when a text is nonliterally reused, and words are substituted with semantic equivalents, such as synonyms or other semantically similar words. Our long-term goal is to formally model reuse transformations. The analysis of the amount and type of substitutions of words with lexically related words enables insights into how text is reused. Applying these insights into future development of detection methods helps to improve them. We have previously investigated two datasets of Bible reuse, trying to understand how reuse is modified (when operations are performed on word pairs) and how linguistic resources support this task. To achieve this, we need to study more and different cases of reuse. In this short paper, we propose and report on work that extends the number of reuse excerpts we investigated in previous work (Moritz et al., 2016), and take another linguistic resource, BabelNet into account. We aim at investigating the current state of lexical resources' support for a Latin reuse dataset. We compare the support we can get from an additional lexical resource to previous results. Specifically, we investigate BabelNet (BN) (Navigli and Ponzetto, 2012), a multiple resource network pulling from different sources, and we compare the reuse detection support (how many words are covered) between BN and Ancient Greek WordNet (AGWN) ( Bizzoni et al., 2014), which also contains Latin WordNet (Minozzi, 2009). Both are recently developed resources and the most common for the Latin language. BabelNet is produced from a range of different, contemporary sources, such as Wikipedia and Wikidata. We are interested in the extent to which BabelNet is able to cover words and relations from an ancient reuse dataset. We are especially curious about what words are still supported by current resources. Our ultimate goal is to simulate a transformation process that also supports nonliteral reuse. This can help to model the changes that were applied to an ancient text during its reuse history. BackgroundThe field of automatically detecting historical text reuse is still in its early stages. To date, Büchler (2013) combines state-of-the-art NLP techniques to address reuse detection scenarios for historical texts, ranging from near copies to text excerpts with a minimal overlap, using a method, which selects ngrams from an upfront pre-segmented corpus. While the approach can discover historical and modern text reuse language-independently, it requires a minimal text similarity. Recognizing modified reuse is difficult in general. Alzahrani et al. (2012) study plagiarism detection techniques, such as n-gram-, syntax-, and semantics-based approaches. However, as soon as reused text is modified (e.g., word substitution), most systems fail. Finally, lexical resources support the identification of relationships between words, but they are not free from issues (Jing, 1998) that can appear when they are used to adapt a general lexicon to a specific domain ( Miller et al., 1990). DataOur dataset contains excerpts from twelve works-mainly sermons and treaties (Literature)-and two work collections-sermons and lettersfrom the Latin writer Bernard of Clairvaux (c.f., Moritz et al., 2016). All those texts come from the Sources Chrétiennes collection (c.f., Mellerin, 2014) (changes in format and orthography may be inserted by the editor). The Biblindex project (Mellerin, 2014) extracted over thousand Bible reuse exerpts from these works, each of which points to a Bible verse. We use the Latin Bible from Biblindex, called Biblia sacra juxta vulgatam versionem (Weber R., 1969) to link the excerpts to the respective Bible verses. We come up with 1,128 unique reuse-tobible-verse pairs. Table 1 shows one example.  MethodologyWe use AGWN, which is automatically constructed from Greek-English digitized lexicons, which again were provided by the Perseus Project (Crane, 1985) and also aligns to Minozzis Latin WordNet (Minozzi, 2009). BabelNet ( Navigli and Ponzetto, 2012) is a multilingual semantic network that integrates lexicographic and encyclopedic knowledge from WordNet (Fellbaum, 1998), Wikipedia, and others. We further use lemma lists from the Biblindex project, as well as the Latin lemma list from the Classical Language Toolkit (CLTK), which is available in the online GitHub repository of the CLTK ( Johnson et al., 2014Johnson et al., 2016, to increase the hit rate when querying both resources.To model the transformation in-between two text excerpts, we define replacement operations (OPs) (see Table 2) that represent the transformation of a reuse to the Bible verse it refers to, as well as an algorithm that identifies those operations between word pairs of a reuse and a Bible verse in a prioritized order. Our algorithm first finds all possible operations for a reuse word, and then applies the most literal operation using the counterpart Bible verse word, which fulfills this operation. This means that if no perfectly or lemmatized matching word is found, relationships of semantic closeness (such as synonyms) for a given word are retrieved. We call the group of semantic operations non-literal operations (c.f., Table 3). We apply our algorithm (which identifies the operations) on Bernard's reuse, first using the relationships queried from AGWN and second, using BabelNet. Afterwards, we show which operations are identified, and calculate a support value for both processes. Results Table 3 shows the identified operations. Using AGWN, we encounter a high ratio of synonyms (repl_syn), a lot of co-hyponyms and a significant number of hyperonyms and hyponyms. With BabelNet these figures are about a tenth as high. Table 3 shows that the values for NOP, lower and lem (matching words, and words with same lemma) slightly differ in-between both word nets. This is caused by a design decision of our algorithm, which pragmatically permits to reassign a word when it already was used in an association with an earlier word.  Fig. 1 shows that AGWN outperforms BabelNet in identifying semantic relations, which represent nonliteral text reuse, because these ratios are much lower for BabelNet than for AGWN. We further encounter three significant descents: between 0% and 10%, 30% and 40%, and 50% and 60%. Looking into samples deeply, we find three patterns: i) the more semantic related words are replaced in a reuse, the more likely it is an allusion or analogy, and the less paraphrased or verbatim it is; ii) short allusions are better covered by the Latin synsets than paraphrases with a high ratio of semantic related words; iii) paraphrases with a high literal ratio are covered best. We summarize that both word nets cover paraphrased reuse to a certain extent of replaced words, and AGWN better identifies allusions. : Ratio of non-literal (semantic) operations, aggregated in 10%-steps in relation to the whole reuse length. The reuse number is displayed logarithmically due to clarity reasons. Lastly, we calculate a support value, which determines the ratio of non-literal operations (c.f., Table 3) compared to them including unsuccessful resource look-ups (no_rel_found) in both, AGWN and BN. For AGWN this value is about 28%, for BabelNet about 6%. Both values are to be understood as lower bounds, because often there is no reasonable relationship in-between two words.Even if BN coverage is poor, its results tell us, which words of a dataset of medieval, Biblical Latin and Latin of the church fathers are prevailed in a current resource. Some examples are words such as gloria (glory) (contained in 17 synsets), corona (crown) (contained in 10 synsets), or nemo (nobody) (contained in 4 synsets). ConclusionWe identified the ratio of non-literal reuse in a Latin dataset and showed the support of two lexical resources. Our results show that language resources for Latin reuse are limited and that only a small part of the required coverage is supported. This result raises awareness for the lack of resources for ancient data, despite the growth of language resources for modern languages. Our future work includes refining our operation set, analyzing more languages, increasing the size of our datasets, and investigating probability measures for those data in lexical hierarchies. Since lexical resources will never completely cover the vocabulary at hand, we further consider the application of a form of word embedding.",
        "article_title": "An Automated Approach to Model the Transformation Process of the Reuse in Bernard de Clairvaux: How Do Lexical Resources Help?",
        "authors": [
            {
                "given": "Maria",
                "family": "Moritz",
                "affiliation": [
                    {
                        "original_name": "Georg-August-Universität Göttingen (University of Gottingen)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Marco",
                "family": "Büchler",
                "affiliation": [
                    {
                        "original_name": "Georg-August-Universität Göttingen (University of Gottingen)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn the mid-1850s, American educator and editor Elizabeth Peabody (1804-1894) set off from Boston to ride the rails. She traveled as far north as Rochester, NY; as far west as Louisville, KY; and as far south as Richmond, VA, in order to promote the textbook she had recently published, A Chronological History of the United States (1856). Along with her suitcase, Peabody traveled with a large fabric roll, which, when unrolled, displayed a grid-like array of colored squares that represented the major events in U.S. history. In the nineteenth-century version of a product demo, Peabody would arrange the \"painted centuries,\" as she called them, on the floor, and invite potential textbook adopters to sit around the charts and contemplate the colors and patterns that they perceived (9).Although not described in terms of visualization--the term did not enter common parlance until the early twentieth century--Peabody's ideas about the uses of her charts anticipate many of the benefits associated with visualization today: the ability to \"offload\" mental processing \"from cognitive to perceptual systems,\" to \"enhance\" pattern recognition through \"abstraction and aggregation,\" and, crucially, to interact with and potentially \"manipulate\" the visualization itself (Card et al. 1999, 16). For Peabody did not only imagine that her readers would interpret the \"data\" presented on her charts; she also intended for them to create charts of their own. To this end, Peabody also sold workbooks of blank charts, so that students could read each chapter of her textbook, and then convert the list of events that followed into color and position, according to her visual scheme.   Project OverviewDrawing from recent digital humanities work relating to historical fabrication (e.g. Elliott et al. 2012, Sayers 2015, as well as from our own previous explorations of historical visualization techniques (e.g. Foster et al. 2016), we set out to recreate and enhance Peabody's pioneering visual design. In particular, we focused on Peabody's ideas about interaction and interpretation, since her ideas about the tripartite relation between data, text, and image--and the role of the reader in translating between each-- speak directly to current debates in the digital humanities about the importance of acknowledging data as \"capta\" (Drucker 2011), and of recognizing the role of individual interpretation in both the design and reception of visualizations (Posner 2016). In our project, we focused first on reimagining Peabody's original interaction for the web, employing current information visualization research to suggest techniques for emphasizing the interrelation between the data and their visual display. We then began a project to recreate the floor-sized version of Peabody's chart using physical computing materials, so as to further explore the embodied aspects of Peabody's visualization scheme. In the following sections, we describe the design choices involved in each recreation--the digital and the physical-- with particular attention to how we sought to amplify Peabody's ideas about interaction, interpretation, and embodiment through our reimagined interfaces. The Shape of History: Reimagining Interaction and Interpretation for the WebThe website located at shapeofhistory.net represents the culmination of a year-long iterative design process. From Peabody's original textbook, we distilled four conceptual modes of interaction: an \"explore\" mode, designed to explain to novice users how to interpret her charts, and how to translate between text and image; a \"lesson\" mode, designed to allow users to create their own charts, drawing upon Peabody's original data; a \"compare\" mode, designed to call attention to how choices in visual display affect the charts' ultimate interpretation; and a \"play\" mode, intended to facilitate the most open-ended form of interaction and expression. To implement the site, we employed a combination of HTML5, CSS, and JavaScript, including Bootstrap.js for site structure, jQuery for navigation and site-level interaction, and D3.js and two.js (along with custom JavaScript) for the visualization components.At each juncture, we considered how to enhance Peabody's original designs and interactions. For instance, when recreating the grid that would serve as the primary typographical form, we remained faithful to the original design and color palette, while adding additional minor grid lines (in light gray) so that users would know where to click (White 2011). In order to emphasize the relation between text and image, an important feature of both the \"explore\" and \"compare\" modes, we added a simple interaction, known as \"brushing,\" so that hovering over a single event in either the text or the image would simultaneously highlight both elements, as well as the corresponding location on the chart's key (Stasko 2007). For the \"lesson\" mode, we augmented the features developed for the other two modes with a more guided experience, akin to the lesson that Peabody described in her textbook, through proceduralized interaction (Bogost 2007). In the lesson, users must read each event, one at a time, translate it into color, and then place the colored square in the appropriate location on the grid. Through enhanced user cues, such as converting the cursor to a pointer as it hovers over the grid, and highlighting empty squares as the user hovers over them, users are guided through a digital version of the interactive lesson that Peabody envisioned in print. Reimagining Peabody's historical visualization scheme for the web helps to underscore how she understood interpretation as a fundamental part of the process of perceiving visualizations. Her visual design bears very little relation to the immediately intuitive images that we associate with visualization today. And yet, for Peabody, the abstraction of the chart was part of its purpose; she intended her charts to be individually interpreted by each person who encountered them. More than that, she envisioned her charts as lessons in themselves--lessons that often took time and effort in order to complete. In this way, the interactions she envisioned, while made quicker and more intuitive through their digital recreation, lose some of their original intent, in that Peabody did not identify efficiency as a feature of her designs. Instead, she viewed the interpretive process--sometimes difficult and often slow--as the best source of historical knowledge. The \"lesson\" of The Shape of History, as distinct from Peabody's original scheme, is a reminder of how little interpretation is intended-- even if it is still required--when encountering visualizations of data today. The Floor Chart: Reimagining Embodiment through Physical ComputingWhile the digital version of the project emphasizes Peabody's interest in facilitating interaction and interpretation, it does not convey the embodied aspects of the original interaction; looking at a screen is a far different experience than walking around a rug-sized chart on the floor. To reimagine this embodied mode of interaction, we designed a one-meter by one-meter floor chart, consisting of a matrix of thirty by thirty individually addressable light-emitting diodes (LEDs). Each LED corresponds to one subsection of Peabody's original chart, so that the 900 possible events can be represented. (We cannot account for multiple simultaneous events, however). The LEDs can be pre-programmed via custom software, which makes use of Adafruit's NeoPixel library. We are also in the process of developing a flexible touch interface, using conductive copper tape and neoprene, so that the LEDs can be controlled through a soft button-like interaction. Both the LEDs and the touch interface are controlled by an Arduino Mega 2560 microcontroller.  We view this project as one of speculative design (Dunne and Raby 2013). Since Peabody's original floor charts were not preserved, we must speculate about everything from the size of the chart, to the colors employed, to the events depicted. While we have textual accounts, in Peabody's correspondence, of how nineteenth-century viewers would interact with the floor charts, the original charts were obviously not programmable. What the reimagined floor chart teach us, then, is about how we might incorporate embodied elements into current visualization design practices, as much as about how viewers interacted with large-scale visualizations in the past. It also reminds us about the labor involved in fabricating the original charts. (Peabody complained about the magnitude of the task in her correspondence). The work of data visualization, while not always expressed in physical form, is always the work of many hands. Conclusions and Next StepsIn their foundational essay on historical fabrication, Devon Elliott et al. observe that \"working with actual, physical stuff offers the historian new opportunities to explore the interactions of people and things\" (2012). In this project, we have sought to extend these opportunities for exploration to include the interactions of people with data, as well as with their visual display. Our project underscores the foundational role of interpretation in designing and perceiving visualizations; and shows how interaction is crucial to the interpretive process. It also points to future modes of visualization, not yet imagined, that might better emphasize embodied ways of knowing. In terms of next steps, for the website, we plan to think through what a more scholarly version of the site, with room for more explanatory text, might look like. For the physicalization, we are continuing to implement the touch interface. From there, we will focus on the aesthetic aspects of the rug, exploring options for light-diffusing fabrics to frame the LEDs, and light-blocking materials to create the grid-lines. ",
        "article_title": "The Shape of History: Reimagining Nineteenth- Century Data Visualization",
        "authors": [
            {
                "given": "Caroline",
                "family": "Foster",
                "affiliation": [
                    {
                        "original_name": "Georgia Institute of Technology United States of America",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Adam",
                "family": "Hayward",
                "affiliation": [
                    {
                        "original_name": "Georgia Institute of Technology United States of America",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Svyatoslav",
                "family": "Kucheryavykh",
                "affiliation": [
                    {
                        "original_name": "Georgia Institute of Technology United States of America",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Angela",
                "family": "Vujic",
                "affiliation": [
                    {
                        "original_name": "Georgia Institute of Technology United States of America",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Maninder ",
                "family": "Japra",
                "affiliation": [
                    {
                        "original_name": "Georgia Institute of Technology United States of America",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Shivani",
                "family": "Negi",
                "affiliation": [
                    {
                        "original_name": "Georgia Institute of Technology United States of America",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Lauren",
                "family": "Klein",
                "affiliation": [
                    {
                        "original_name": "Georgia Institute of Technology United States of America",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionBernoulli-Euler Online (BEOL) is an interdisciplinary research project funded by the Swiss National Science Foundation focusing on the mathematics influenced by the Bernoulli dynasty and Leonhard Euler. It is being carried out by the Bernoulli Euler Centre and the Digital Humanities Lab at the University of Basel. Its main goal is the integration of different edition projects relating to the Bernoullis and Leonhard Euler into one target platform, offering appropriate functionality for researchers interested in the history of science.The methodological efforts will also be applicable to other editions since they are developed in a generic way. BEOL is based on Knora, a generic infrastructure for humanities data. Goal of the BEOL-platform and its technical basisBEOL aims at integrating three edition projects, that are currently all technically different and thus incompatible with one another:• Basler Edition der Bernoulli-Briefwechsel (BEBB): BEBB is an online edition that is based on the MediaWiki software and hosted by the University Library of Basel. It is connected to the library's metadata catalogue for manuscripts (Basler Inventar der Bernoulli-Briefwechsel) _ The letters are encoded in Wiki markup and are converted to HTML to represent them on the web. The mathematical formulae are encoded in LaTeX.• Leonhardi Euleri Opera Omnia (LEOO): LEOO is a printed edition of the works of Leonhard Euler that was begun in the early 20 th century. In the context of BEOL, the volume containing Euler's correspondence with Christian Goldbach (Euler 2015) will be integrated as a proof of concept. This volume has been prepared using LaTeX (as well as the volume with Euler's correspondence with Daniel Bernoulli that has been published recently). We expect to be able to integrate all the other recent volumes set in LaTeX in a similar manner. For the older volumes, the printed books would have to be scanned (including OCR) and marked up.• Jacob (I) Bernoulli's scientific notebook Meditationes: The manuscript is held in the university library of Basel (shelfmark L Ia 3, 367 pages) and has already been digitized. The manuscript consisting of 287 entries is being transcribed at the Bernoulli Euler Centre using XML (The XML format is specified closely to the TEI specifications P5, so it can be transformed quite easily to TEI/XML) for the text and LaTeX for the mathematical notation that is embedded in the XML.The three edition projects do not only overlap thematically, but also in terms of the persons involved (authors, mentioned persons) and bibliographical items (literature referred to in the texts, references in-between the editions' texts). Letters exchanged between members of the Bernoulli dynasty, Leonhard Euler and contemporary mathematicians and scientists are an important part of these edition projects and thus it is desirable to identify and match the persons in all editions in order to display their relations. The technical basis for BEOL is Knora, an infrastructure for humanities data (Rosenthaler and others 2015) consisting of an RDF-triplestore, an OWL base ontology, and a RESTful API that allows for querying and adding to the data. The base ontology (see prefix 'Knora' in Figure 1) defines common value types (such as a calendar independent format to represent dates using the Julian Day Number) used among humanities projects and can be further extended in project specific ontologies. BEOL will provide such an ontology (see prefix 'BEOL' in Figure 1), defining its own resource classes and properties needed to represent the edition projects' texts and entities. Wherever possible, existing ontologies will be reused by making subclasses and subproperties. BEOL is part of the NIE-INE project, which aims to create a general-purpose infrastructure for digital editions, using Knora as its technical foundation. A focus of this project will be abstracting out concepts shared by different projects and formalising them as ontologies.  Figure 1 represents all relations between persons (We refer to the Integrated Authority File (GND), and in order to represent locations, we will also refer to GeoNames), letters, and manuscripts (we also link to the catalogue of the Basel university library that keeps many of the original copies of the letters and manuscripts of BEOL), as well as their properties as directed graphs. For reasons of clarity, we use a simplified model here. The coloured rectangles indicate that these have been imported from different edition projects which - considered in isolation - do not allow for this kind of overview. Moreover, indices and bibliographies have to be unified on the BEOL platform (e.g., Christian Goldbach occurs both in BEBB and LEOO). The BEOL platform will be connected to Early Modern Letters Online, so it will be interoperable with other edition projects. Importing editions to the same target environmentIn order to represent all three editions in the same target environment, they have to be homogenised first. We decided to do so using an XML-based approach. This has the additional advantage that we can make both the texts of BEBB and LEOO available as TEI/XML to the outside world quite easily by applying XSL transformations. We can also use the same routine to import the editions into BEOL. Knora converts XML-encoded texts to RDF in order to store them in the triplestore. From RDF, an XML document can be recreated that is equivalent to the one originally imported. A mapping defines the relations between XML elements and attributes and the entities defined in the ontology.• BEBB Wiki markup can be transformed to XML using a MediaWiki parser. Wiki tags and structures are mapped to XML tags, and references to other letters, bibliographical items, and images (facsimiles of the letters) can be handled. Once the letters are available on the BEOL platform, the old URLs will have to be forwarded.• The Goldbach-volume of LEOO is set in LaTeX and can be converted to XML using LaTeXML. Additional mappings to the available standard functionality and customisations can be provided using Perl scripts. LaTeXML provides a MathML conversion for mathematical formulae.• The Meditationes are transcribed in an XMLbased format (see LaTeXML). Derived texts of these files can be generated using XSLtransformations. In this way, several layers (diplomatic, normalized) of the text can be produced. Our approach addresses segments defined on the facsimile (see Figure 2) and turns them into a reading text step by step.  One of the main challenges in the BEOL project is the faithful representation of mathematical notation and its relation to the surrounding text (see Figure 2) using web technologies. At the moment, we are using MathJax (which accepts both LaTeX and MathML as input formats) to render the mathematical formulae in the web browser. We also consider MathML as an option, although not all web browsers fully support MathML.We are aiming at developing a browser based user interface that will be based on the Angular 2 framework (although in the meantime, we are already using Knora's current interface, SALSAH) that not only makes it possible to present the texts on the web and to offer search functionality, but also to add to the data (sufficient permissions provided). The users may create their own annotations on the BEOL platform. Basically, the user interface interacts with the Knora API in order to create new resources, manipulate properties etc. Since BEOL is based on Knora, all of its generic functionality can be used for this purpose. ConclusionBEOL integrates three different edition projects into one platform and allows researchers to query previously separated contents and add to them. The specific problems posed by the combination of text and mathematical notation can be addressed in a generic manner. All the functionality to be developed will be part of Knora and can be reused by other projects dealing with scientific texts from mathematics and physics.",
        "article_title": "Integrating historical scientific texts into the Bernoulli-Euler Online platform",
        "authors": [
            {
                "given": "Tobias",
                "family": "Schweizer",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab - University of Basel",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Sepideh",
                "family": "Alassi",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab - University of Basel",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Martin",
                "family": "Mattmüller",
                "affiliation": [
                    {
                        "original_name": "Bernoulli Euler Centre University of Basel",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Lukas",
                "family": "Rosenthaler",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab - University of Basel",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Helmut",
                "family": "Harbrecht",
                "affiliation": [
                    {
                        "original_name": "Bernoulli Euler Centre University of Basel",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Digital Humanities Lab (DHLab) is a research group within the faculty of Humanities of the University of Basel. The research profile of the DHLab integrates computer science, digital imaging, computational photography and the accessibility of digital objects in humanities research. The project \"Digital Materiality\", an interdisciplinary project in collaboration with the Seminar of Art History, examines how new digital methods and techniques can be used to describe the reflection of light on surfaces of artworks. Of main interest are mosaics and early prints; both categories have a strong interaction with light and standard photographic approaches are not able to capture the dynamic component of the light-surface interdependence that is specific for this kind of objects.For art historians who study and work with mosaics or any other object of complex surface composition, it is difficult to capture and visualize the important surface features in such a way that research can be done with the reproduction. The major problem is the static nature of photographic reproductions, which does not allow interaction. Static, two-dimensional photographs cannot visualize appropriately the sparkling effect caused by the surface properties of the countless light reflecting tesserae of a mosaic, for example. Similar considerations apply for early prints, books and parchments. The visual impression that these objects convey is hardly delivered by a photograph. Metallic inclusions give the artwork a dynamic appearance caused by the change of reflectance behavior of glossy compared to matte material. Furthermore, the scholar may want to integrate information coming from other types of scientific photographs, such as infrared or ultraviolet illuminated or induced fluorescence images, to increase the visual impression of renderings of artwork. In particular, the combination of such scientific photographs with other imaging techniques like Reflectance Transformation Imaging is advantageous because multiple visual impressions can be combined in a way that would not be possible in reality. Reflection Transformation ImagingRTI (Reflectance Transformation Imaging) ( Mudge et al, 2007;Malzbender et al, 2001) is a promising approach to go beyond the limitations of conventional photographic methods. However, RTI, as it is used today, has some drawbacks that are critical in the context of the reproduction of mosaics, metallic inclusions or most other artwork.RTI makes it possible to interactively display the light reflection as a function of the light incident angle and the structure of the surface captured. RTI needs a series of digital images from a fixed camera position, in which the light source is illuminating the object in each capture from a different position. In order to produce RTI renderings of objects of different size, several lighting systems have been developed that consist of a number of light sources that are mounted on the inside of a hemispherical dome. Those light sources can be switched on and synchronized sequentially with a digital camera. The fixed design ensures a fixed and equal distance between the numerous light sources and the object. Thus, with such an automated setup the image capture process is drastically accelerated and, in addition, ensures a high reproducibility. In the Digital Humanities Lab such a dome has been developed as well; In this specific case it is an alloy design, which is equipped with 50 LEDs and an electronic control system, which enables the automatic sequential triggering of the light sources and the synchronization with the camera. The image sequence, recorded in this way, serves in a second step as data basis for a subsequent pixel-based modeling. For this purpose, a mathematical model -normally a function of second order -is fitted for each pixel position, which represents the set of all image points from the different directions of illumination, i.e., it is parameterized so that the square mean error of all data points relative to the function curve becomes minimal.The reflection model so far used in the original RTI method described by Hewlett Packard ( Mudge et al, 2006) corresponds to the mentioned simple secondorder function. Thus it has only a relatively low mathematical complexity and therefore a limited precision to represent the actual surface reflection. This specific function, however, describes matt surfaces, a Lambertian radiator, very precisely. However, this method is not suitable for glossy materials, since this simple model does not correspond to the physical laws of gloss reflection. A further disadvantage is that the method, as it is usually used today, needs a specially designed viewer application.In the Digital Humanities Lab, the method has been enhanced in order to be able to present more complex surfaces that are made from different materials. In our approach the complexity of the mathematical model is increased so that we can handle diffuse and gloss surface components at the same RTI image. In other words, this improvement makes it possible to model and display materials with different gloss levels using the same mathematical model. A suite for humanities scholars 'needsFor humanities research another important aspect is the compatibility to web-based Virtual Research Environments (VRE). In principle, a functional graphical \"front-end\" with a connection to a database can be called a VRE. The aim of such a digital infrastructure for research is to allow scholars to work with methods and tools in the digital domain as they would do it in a conventional \"analogue\" process. This should be done in such a form that the scientists can intuitively recognize and use the well-known concepts and working methods that are offered by the VRE. This requires some basic functions:• In any case, a browser-based client-server solution is preferable to a stand-alone application. In such a way collaborative work can be more easily achieved.• In many humanities disciplines intensive work is being done with image material and objects, in which specific areas (region of interest, RoI) are often to be emphasized. Therefore corresponding graphic elements (lines, polygons) are necessary, with which such areas can be marked. These graphical elements can, for example, be polygonal line sections or rectangles, which allow the marking of object parts.• The method of evaluating image material, which is common in the human sciences, is of a more qualitative nature. This kind of work requires extensive and powerful tools to capture descriptive and contextual metadata (annotations, transcriptions, comments). These meta-objects have to be linked with the actual primary object and need also to be stored in this way.The visualization in a VRE must be multi-media. Besides text, image, sound and video, it is also of advantage to be able to display objects in a virtual 3D space.To be able to integrate an RTI solutions into a webenvironment to support real-time collaborative work needs specific web-technologies ( Palma et al, 2010;MacDonald and Robson, 2010). The presented RTI viewer is fully web-compatible and it can be integrated in most browsers to support high-quality client-side visualization; the key technology that allows this integration is WebGL. This OpenGL-based programming interface, which has been optimized for \"embedded systems\", is nowadays integrated into any modern Web browsers. WebGL is a license-free standard developed to work seamless in conjunction with the programming language JavaScript. For the application in a browser this means that 3D functionalities are provided without the need to load any additional plug-ins. The performance and range of functions of WebGL are impressive. WebGL is also supported on most mobile devices, such as smartphones and tablets, which further increases the range of applications. The improved performance of WebGL, in contrast to many other graphics libraries, also allows for fluid interactive work. Long rendering times are left out and the visual latency is minimal. WebGL offers a variety of functionalities, ranging from simple grid models to complex animated, textured and illuminated surfaces. The fact that the use of graphic elements for marking RoIs is easily possible with this large range of functions is, of course, self-evident.A Reflectance Transformation Imaging recording that is represented in a browser using WebGL. Around the left eye of the sacred head is a branding to be recognized, as well as a corresponding commentary, left in the picture window. The parameters of the viewing situation are shown on the right side, which are also stored in the data model. (Source:DHLab, University of Basel)The integration of all those technologies and new developments allows us to present an improved solution to reproduce and render surfaces of different materials (matt and glossy) in a fully web-based environment implemented in JavaScript and WebGL, running on standard computers and mobile devices. In addition to RTI image processing, photographs in the UV and IR domain can be captured, displayed and superimposed with the same system to allow the user to compare the same region under different light condition. For flexibility, performance and data permanence aspects our RTI image server will follow the International Image Interoperability Framework (IIIF). IIIF defines a standardized URL syntax to serve digital images online in the field of cultural heritage and research. The region of interest, resolution, rotation and the file format of a requested image can be indicated on the URL. SIPI, the Simple Image Presentation Interface, developed by DHLab, provides an IIIF compliant image server which is ideally suited for our scope. Due to the fact, that the front-end is compatible with any standard web-browser, it can be integrated in a virtual research environment using Knora. Knora is a software framework, developed by DHLab, for storing, sharing, and working with primary sources and data in the humanities. Knora builds the fundament for the Swiss National Data Center for Research Data in Humanities that is operated by the DHLab. The source code is publicly available on Github at the address in reference.This integrated environment allows researchers to interactively control the viewing and light conditions of e g. a digital mosaic rendering. Regions of interest can be chosen, annotated, saved, and shared with other scholars. The full set of contextual and technical metadata is stored and time stamped to be fully reloadable and citable back to any point of its history of changes. ConclusionsThe combination of the Enhanced-RTI and the Scientific Image Viewer (SIV) enables us to convey the impressions of these highly dynamic light-surface interactions and the information provided by IR and UV imaging e.g. to researchers who cannot visit the actual artwork in situ. The visual impression can be enriched by meta information that can shared with other scholars. The presented RTI based solution is also helpful to document the current condition of objects more accurately, e.g. before and after a restoration. Sustainability and simplicity of RTI image data is drastically improved by the use of a IIIF server. The presented infrastructure allows the strict separation of image data and meta information. As a result, any RTI image rendering is fully reproducible and therefore perfectly suited for digital archiving, following the requirements of performance, permanence and reliability. ",
        "article_title": "Enhanced Reflectance Transformation Imaging for Arts and Humanities",
        "authors": [
            {
                "given": "Peter",
                "family": "Fornaro",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab - University of Basel",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Andrea",
                "family": "Bianco",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab - University of Basel",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Aeneas",
                "family": "Kaiser",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab - University of Basel",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Lukas",
                "family": "Rosenthaler",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab - University of Basel",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            },
            {
                "given": "Lothar",
                "family": "Schmitt",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab - University of Basel",
                        "normalized_name": "University of Basel",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s6k3f65",
                            "GRID": "grid.6612.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " RelevanceOne of the most important fields of action in research which is developing along with the digitalization of information is research data management (RDM). The universities face the challenge of offering their researchers adequate structures and services. The managing board of the German universities organized in the German Rector's Conference (HRK) has identified this as a key task (HRK 2014 and2015). Moreover, in the recently published position paper called \"Performance by diversity\" the German Council for Scientific Information Infrastructures (RfII) makes a series of recommendations concerning how research data should be managed in the future (RfII 2016). The RfII was tasked by Germany's Joint Science Conference (GWK) with formulating broad-based recommendations for the science system in Germany as a whole. In addition, according to estimates by the German Research Foundation (DFG), up to 90% of the digital generated research data and results are still getting lost (Winkler-Nees 2011, p. 5) or \"disappear in the drawer\" (Kramer 2014) shortly after completion of research projects and are therefore not available for further use and reuse. MethodAs a structured way to gain information, we decided to follow the six stages process recommended for survey research (Mü ller et al. 2014). In addition, the survey is based on the relevant articles published in the handbook \"Methods of Library and Information Science\" by Umlauf, Fü hles-Ubach & Seadle ( Umlauf et al. 2013). The six steps are briefly explained below.The Internet survey, the online questionnaire as well as the detailed report are published on the DCHWebsite (http://dch.phil-fak.uni-koeln.de/umfrage-2016.html; see also Kronenwett 2017). Research goals and constructsThe goal of the survey is to contribute to the conceptual development of the Data Center for the Humanities (DCH), which was founded as a central infrastructure service institution by the faculty dealing with humanities research data in 2013. In practice, the enquiry aims to characterize the present situation and to obtain information on the demands in the sectors RDM and consultation services offered by the DCH in cooperation with the University and City Library of Cologne (USB), one of the local partners of the DCH. Another goal was the comparability with other surveys conducted in the field. Population and samplingBecause RDM should be handled in a way specific to each discipline ( Sahle et al. 2013), the survey targeted only researchers at the Faculty of Arts and Humanities of the University of Cologne -one of the largest humanities faculties in Europe. The survey's population is limited to the academic staff of the Faculty of Arts and Humanities of the University of Cologne. In particular, the survey focused on researchers who are responsible for data-driven research projects. Questionnaire design and biasesFirstly, with regards to content, conceptual and methodical design of the survey, Internet surveys and online questionnaires on research data at national and international scientific institutions and research institutes were analyzed so far available (the website \"forschungsdaten.org\" offers an overview regarding national and international surveys on research data). Secondly, the questionnaire design was tailored and adapted to suit the unique circumstances which can be found at the Faculty of Arts andHumanities (DCH 2016, CCeH 2016). Finally, the results of a series of expert interviews carried out by the DCH with researchers at the Faculty were taken into account (Blumtritt 2016, p. 16). The questionnaire addresses five issues, namely (1) research data, (2) use of data archives, (3) support for research data, (4) discipline and position, (5) interest. Review and survey pretestingAfter a review of the questionnaire with stakeholders, such as the dean's office, the library and the data protection officer, a test link was sent to 20 potential subjects. This included representatives of all subject groups of the Faculty of Arts and Humanities (Faculty of Arts and Humanities 2016) as well as external experts (mostly sociologists and colleagues with RDM-background). After several feedback loops, the questionnaire was further modified and optimized. Implementation and launchThe questionnaire was compiled using Kronenwett & Adolphs online survey tool (Kronenwett & Adolphs 2017). It was put online from 2016-05-30 to 2016-06-12 (2 weeks). Depending on individual answers the questionnaire contained up to 24 questions. Data analysis and reportingThe questionnaire was completely answered by 136 participants (out of 191 persons who started the survey) which is 71.20% completion rate. The following selection of data analysis and reporting takes into account only these participants (n=136). ResultsOur objective in the compilation of the questionnaire was to answer the following questions: 1) What research data are available? 2) What is the need for research data? 3) What support do the members of the Faculty of Arts and Humanities want from the DCH?Regarding the first question, sustainability and data volume were important to us. As far as sustainability is concerned, the majority of respondents are storing their research data on their local computers: 70% work computer, 70% private computer, multiple responses were possible (see fig. 1). Only 14% are storing their data in a data archive, a number that is also reflected in other questions like how many participants can imagine their data being stored in a data archive. Regarding sustainability standards the given answers are fatal results since structured access and retrievability of research data are only ensured in professional data archives. Cloud solutions are also quite popular (35% use by commercial vendors and 14% of scientific vendors) because they ensure overall data access and data share. But regarding aspects like data plausibility, traceability or even long-term preservation they are totally unsuitable.This result could be explained by the fact that the participants do not reflect their approach to sustainability and traceability. The vast majority of the respondent's self-assessment regarding their own skills in RDM is rated to be average or even less (71%) (see figure 2). Sustainability is seen as a problem. 66% of the participants state that data could be lost when there is no one to be responsible for the website. 60% fear problems with data conversion. There is some sensibility towards the issues of finding the data (45%) and documentation of the data (41%). Figure 3 shows all answers concerning problems the participants see with preserving research data. Interesting is that both privacy and data theft are the concerns voiced the least frequent (11% each), despite the fact that we encounter these concerns frequently in our consulting practice. But this could be an anomaly due to us asking from the user perspective rather than the data giving perspective which is more typical to our consulting. In an effort to improve our services towards the faculty, we also asked which services should be provided by the DCH. Here legal and technical issues featured prominently (74% and 73% respectively; also cf. Fig. 4). Requests for storage (72%) and consultation (66%) on general issues are also in high demand. Conclusion and outlookAs a result of the survey, we propose the following recommendations for action for the University of Cologne on the one hand and for the DCH on the other hand. Together with the local library (USB), the DCH now offers legal counseling on the subject of research data with a specialized lawyer. We are currently planning a project for improving the sustainability of living software systems, since the survey showed that this is an eminent problem in our faculty. Based on the projected storage space from the survey, we have negotiated with the computing center to provide that space centrally for all the members of our faculty.In our talk, we will give more details on the study and its results and will also compare it to other surveys conducted internationally. We feel that surveys of this nature are an important tool to shape strategic decisions made in institutions concerned with research data.",
        "article_title": "A Survey on Research Data at the Faculty of Arts and Humanities of the University of Cologne",
        "authors": [
            {
                "given": "Simone",
                "family": "Kronenwett",
                "affiliation": [
                    {
                        "original_name": "Cologne Center for eHumanities",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Brigitte",
                "family": "Mathiak",
                "affiliation": [
                    {
                        "original_name": "Cologne Center for eHumanities",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " WritersIn contrast to their counterparts from the older generation who were devoted to a single identity such as being a novelist, columnist, or essayist, young creative writers who have won literature awards and published top-ranking books grew up within a social environment of PTT, blogs, and Facebook. They are accustomed to managing their primary literate role while at the same time enjoying multi-tasking by being active across new media platforms and acting as popular commentators for social issues related to their academic training such as education, sociology, gender study, and philosophy. They obtain a vast quantity of high quality information from their social media stratosphere and they share opinions and ideas through social media services with their followers as a form of self-presentation, or \"performance\" in Goffman's term. IllustratorsInternet and social media has brought changes to people's reading habits in many ways; for example, \"Picture Reading\" and illustrated creative blogs have become very popular in Taiwan over the last decade. Therefore, illustrators who devote effort to character creation and social media fan page management are increasing in number. They obtain inspiration from daily life and can transform their observations into creative content. These illustrators integrate their interests and capacities to share their artwork on the Internet, and they attempt to develop a business model that involves a multivariate form of production. Social innovatorsSocial innovators attempt to employ strategies and actions to solve community or city problems through teamwork with the goal of making the world a better place. They use technology as a means of connecting resources in the process. Their goal is to penetrate social problems and issues, identify the needs of communities they target, develop innovative solutions, and take action. One of their primary strategies for achieving this goal is to collaborate with local people to create stories for the public, who thus can understand the value and meaning behind these stories. Their storytelling is multimodal in nature to accommodate the needs of the general public. MakersThe \"maker movement\" has emerged in Taiwan over the last several years, and under the social environment of learning by doing, makers create a small amount of delicate products and are very open regarding sharing their ideas and creative processes. They suggest that traditional education in Taiwan neglects the significance of DIY and are enthusiastic about the culture of makers, who emphasize learning, doing, and sharing. They are very persistent about their own interests, and empower themselves with knowledge and skills through self-directed learning, pursuing an ideal living style and working arrangement. Insight from word clouds and the co-occurrence of key wordsThe word clouds of writers illuminate a central focus is on words, articles and literature. The literacy practice of these writers is to express feelings and provide arguments about issues. By contrast, illustrators are interest-driven; they enjoy sharing life stories and ideas with their friends, and are followed by social media fans who adore the characters they have created. The central focus of social innovators is on society, community, and local issues and needs; and their resources and contributions include teamwork, space, activities, cultural artifacts, and communication between people and the government. Finally, makers are interest-driven and they enjoy sharing life experiences through their creations and products like illustrators. Much like social innovators, makers access information and create content in both the cyber world and in the physical world.  ",
        "article_title": "The new literacy practice of young Taiwanese writers, illustrators, social innovators, and makers",
        "authors": [
            {
                "given": "Su-Yen",
                "family": "Chen",
                "affiliation": [
                    {
                        "original_name": "National Tsing-hua University",
                        "normalized_name": null,
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Jason",
                "family": "Chang",
                "affiliation": [
                    {
                        "original_name": "National Tsing-hua University",
                        "normalized_name": null,
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Yu-Hsuan",
                "family": "Wu",
                "affiliation": [
                    {
                        "original_name": "National Tsing-hua University",
                        "normalized_name": null,
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionCadaster plans are cornerstones for reconstructing dense representations of the history of the city (di Lenardo and Kaplan, 2015). They provide information about the city urban shape, enabling to reconstruct footprints of most important urban components (buildings, streets, canals, bridges) as well as information about the urban population and city functions (census information, property, rent prices, etc.) (Noizet et al, 2013). Cadaster plans are usually the results of coordinated campaigns with standardized methods of measurement and representation. This means that large sets of documents follow the same representation conventions. This regularity opens the possibly of efficient automated process for analyzing them and possibly transforming the information they contain in geo-referenced databases that can be used as part of historical geographical information systems ( Gregory et al, 2001).However, as some of these handwritten documents are more than 200 years old, the establishment of a processing pipeline for interpreting them remains extremely challenging. This may explain why, to our knowledge, no such system exists in the literature. This article reports our effort in this domain, presenting the first implementation of a fully automated process capable of segmenting and interpreting Napoleonic Cadaster Maps of the Veneto Region dating from the beginning of the 19th century. Our system extracts the geometry of each of the drawn parcels, and classifies, reads and interprets the handwritten labels. We believe the general principle of technologies used in the process could be adapted to other cadastral funds, although this has not been tested in the present study. MethodologyLiterature on map processing includes works on many different types of maps, from roads to topographic maps, including hydrographic and cadastral maps. Most studies focus on particular problems and features and thus develop techniques that are highly map specific ( Chiang et al, 2014).Our work addresses the particular case of the Napoleonic cadaster of Venice dated 1808, but aims at developing a method highly adaptable to other cadasters with little extra effort.We propose a system that segments the cadastral map, identifies and extract segmented objects such as parcels and identifiers and recognizes the extracted hand-written digits. A demo code with examples of the results can be found at https://github.com/dhlab-epfl/cadasters. The method is summarized in Fig. 1.  PreprocessingUsually, the processed images are ancient documents that have been digitized. To deal with the natural ageing of paper and eventual spots on the map without losing details, we use a non-local means de-noising method ( Buades et al, 2005) to smooth the image. SegmentationWe address the task of extracting the desired information from the document as a segmentation problem, which is a recurrent problem in image processing. A graph-based segmentation approach is adopted, which models the image as a weighted undirected graph. This allows to process the pixels or regions in the spatial domain of the image but also to use higher level information such as connections, similarities and dependencies between the elements.Because a group of pixels sharing some similarities are more perceptually meaningful than a simple pixel, we use SLIC method ( Achanta et al, 2012) to create superpixels. Superpixels are clusters of pixels that share similarities and spatial proximity and have the advantage of reducing the complexity of image processing tasks.A graph is a mathematical structure composed of vertices and edges, representing a system of connections or interrelations among a set of objects. It is widely used to model relations, to study information systems or to organize data. In our case, the graph representing the image is initialized with superpixels as vertices. Its edges connect neighboring vertices (superpixels) and each edge has a weight which is a measure of the dissimilarity between neighboring elements. The distance (or dissimilarity) metric is based on color and edge/ridge features.The oversegmentation of the image resulting from superpixel generation is then reduced by grouping superpixels into homogeneous regions and merging the corresponding graph vertices. Our approach uses global homogeneity, meaning that the method minimizes intragroup dissimilarity and maximizes intergroup dissimilarity. The 'dispersion' of edge weights (i.e standard deviation within a region) allows to spot high-weighted edges within a group and thus disconnect dissimilar vertices (i.e remove their edge) to end up with independent homogeneous regions. Region ClassificationThe merged regions are classified into 3 classes: text, contour/delimitations and background (smooth textures such as parcels or streets) using a SVM classifier. The training data is composed of manually annotated samples of maps coming from the Napoleonic cadaster of Venice. Parcel ExtractionThe classification results allow the determination possible parcels candidates. A flood fill algorithm is applied, using a ridge detector to indicate boundaries. The chosen ridge detector was originally developed as a vessel enhancement filter ( Frangi et al, 1998) and looks for multiscale second order local structures of the image that can be considered tubular. The obtained measure indicates how similar the structure is to a tube, and so it is able to detect ridges. Starting from one point in the regions labeled as background (seed point), the flood fill algorithm floods each zone, i.e parcels, streets, etc. and stops at the boundaries (output of the ridge detector).Each parcel of the image is extracted as a polygonal shape and the polygon's corner points are stored in GeoJSON format. If the image file is georeferenced and contains geographical information (a GTIFF file for instance), polygons are exported according to the spatial reference system provided. This allows a fast and easy integration of the shapes into a geographic information system (GIS) and geographic information on the parcels can easily be collected. Digit ExtractionThe parcel identifier is usually contained within the parcel. This observation and the extracted polygons' information can be used to correct misclassified text regions and improve identifier extraction. Elements labeled as text regions are localized, delimited by bounding boxes and grouped so that neighboring characters are extracted together. Again, information from polygons is used to determine whether neighboring digits belong to the same identifier or not (i.e whether neighboring digits are located in the same parcel/polygon). Boxes that do not correspond to identifiers or digits are removed according to specific criteria. Finally, the boxes containing the parcels' identifiers are extracted.Since the digit recognition step requires horizontally oriented digits to output accurate prediction, the identifiers' boxes are rotated. A principal analysis component is applied to the binary image of the extracted numbers to determine the angle of the rotation. Digit RecognitionThe horizontally oriented numbers are separated into digits that are processed individually. A good digit segmentation is primordial since connected or overlapping digits lead to incorrect recognition. A Convolutional Neural Network (CNN) with two convolutional layers, two fully connected layers and a final softmax layer for multiclass classification is used to predict the identifiers. The CNN is trained on a mixed dataset composed of MNIST dataset ( LeCun et al, 1998) and digit samples from Sommarioni register and has a performance of 99.1%. When predicting the numbers, the network outputs the inferred number with a confidence level indicating the reliability of the result. ResultsThe proposed approach shows promising results in parcel extraction and identifier recognition. We performed the first 'proof-of-concept' evaluations on manually labeled data taken from different cadaster samples. The total number of annotated objects are shown in Table 1.Most parcels and identifiers were correctly extracted (Table 2 & 3), which assured us of the feasibility of their automatic extraction. The precision can still be increased for example by using feedback from digit recognition results, i.e, the prediction and its confidence level permit the discarding regions where no reliable identifier has been recognized.  Table 1: Count of ground-truth objects Table 2: Results of parcel extraction with different Intersection over Union (IoU) thresholds Concerning the digit recognition, only 10% of the identifiers had their digits correctly recognized. Since the models used have shown good performance on nicely detached digits, this is not the fault of the recognition algorithm itself but rather of the digit segmentation procedure. The current segmentation is the main hindrance to an efficient digit recognition, thus, further work should focus on a better number processing algorithm. Another alternative is to avoid the segmentation problem and use a recurrent neural network such as LTSM to process the number as a sequence. PerspectivesOur work shows promising results for easing and accelerating cadaster processing, especially given our method's efficient parcel segmentation and digit identification. Moreover, the export of a parcel's geometry into GeoJSON format opens up further perspectives to efficiently geo-reference ancient maps. The system can be extended and integrated into a user interface to take better advantage from the results, for example by allowing the user to correct or add information about parcels and identifiers.  Table 4: Results of parcels' number recognition The proposed method creates a bridge between previously seperate two data types: the raster object and the vector object. Currently, web-mapping tools consider vector objects as separate layers on the raster maps, and each object needs to be manually redesigned. The automatic vectorization process enables us to perform the visualization and annotation processes directly on the cartographic source without the prerequisite of complex skills. It should greatly facilitate large scale exploitation of such kinds of documents.",
        "article_title": "Machine Vision algorithms on cadaster plans",
        "authors": [
            {
                "given": "Sofia",
                "family": "Oliveira",
                "affiliation": [
                    {
                        "original_name": "École Polytechnique Fédérale de Lausanne",
                        "normalized_name": "École Polytechnique Fédérale de Lausanne",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s376052",
                            "GRID": "grid.5333.6"
                        }
                    }
                ]
            },
            {
                "given": "Isabella",
                "family": "di Lenardo",
                "affiliation": [
                    {
                        "original_name": "École Polytechnique Fédérale de Lausanne",
                        "normalized_name": "École Polytechnique Fédérale de Lausanne",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s376052",
                            "GRID": "grid.5333.6"
                        }
                    }
                ]
            },
            {
                "given": "Frederic",
                "family": "Kaplan",
                "affiliation": [
                    {
                        "original_name": "École Polytechnique Fédérale de Lausanne",
                        "normalized_name": "École Polytechnique Fédérale de Lausanne",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/02s376052",
                            "GRID": "grid.5333.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionMemoryGraph is a medium to record memories as the augmentation of a photograph, which is a medium to record photons. MemoryGraph is a new photographic technique that creates a layer of \"memories\" (or more precisely, time-series photographs) which are taken by the same composition at different times. Photographs of the same composition can be taken by traditional cameras, but this requires substantial effort, because matching a printed photograph with the current landscape requires mental rotation (Shepard et al., 1971), a psychologically demanding task. MemoryGraph simplifies this task through direct overlay of a photograph and the current landscape on a camera viewfinder with adjustable transparency. We demonstrate that this simple idea opens up new possibilities for the critical interpretation of photographs in the context of digital critique. Related MethodsMemoryGraph focuses on the spatio-temporal relationship between photographs and the real world. This field of interest has given rise to many methods similar to MemoryGraph. For this reason, we begin with a comparison between MemoryGraph and these related methods in order to characterize MemoryGraph's unique properties.First, it is necessary to compare MemoryGraph with time-lapse animation, which is also a time-series image of the same composition. The fundamental difference between timelapse animation and MemoryGraph is in time scale and device dependence. Time lapse animation deals with high frequency observations of seconds to minutes using the same location-fized camera, while MemoryGraph deals with low frequency observations over a period of days to years. These observations may potentially use different cameras that are not fixed at the location. In short, MemoryGraph is a tool to realize fixed point observation at any place for any time interval.Secondly, we contrast MemoryGraph with augmented reality (AR), which focuses on the alignment of the real space and the virtual space so that a photograph can be seen as an overlay on the real space through a camera viewfinder. On the contrary, MemoryGraph focuses on the overlay of a photograph on a camera viewfinder as a graphic reference to illustrate the composition of a further action (namely, taking a photograph). This key contrast suggests fundamentally different roles between augmented reality and MemoryGraph. Augmented reality is a tool for exhibition, in the sense that alignment is controlled by a tool, while the user is allowed to be a passive visitor who experiences an environment prepared by someone else. Quite the opposite, MemoryGraph is a tool for participation: the \"visitor\" controls the alignment, and the user should be an active explorer who searches for the best match between photograph and real-life landscape. In short, MemoryGraph is a tool for actions, such as participatory annotation.Our final comparison discusses photo-sharing services dedicated to old photographs. For example, Historypin (Shift, 2010) is designed to share photographs of the past using a map interface, while an advanced system might use a street-view interface to place photographs in 3D. Both tools aim to link photographs to the real world, however, their methods of achieving this goal do not rely on ventures into the \"real world\" to \"place\" the photographs. Photo sharing services depend on \"arm-chair\" annotators, but MemoryGraph depends on \"field\" annotators who visit a real place to take another photograph. In short, MemoryGraph is a tool to motivate people to move in the real world. Proposed MethodMemoryGraph is designed as a mobile app for two reasons. First, the idea of \"graphic reference\" overlay on a camera viewfinder cannot be implemented on traditional cameras that do not expose API (application programming interface). By re-purposing the viewfinder of a smart phone, MemoryGraph can extend the grid-based reference of a traditional camera viewfinder to a graphic reference such as an old photograph. Secondly, in order employ MemoryGraph as a field work tool, the use of a mobile phone is the best choice for mobility and also for real-time information sharing with the server.The task of the user is to find the best match between a graphic reference and the real world using an opaque viewfinder with adjustable transparency. Direct comparison between two scenes not only reduces the burden of mental rotation for the user, but also makes the search enjoyable: the movement of the user gives real-time visual feedback that suggests how \"good\" the move is. This gamification effect motivates users to search for better matching, and promotes photographic crowd-sourcing for participatory annotation.The outcome of this task is two types of data. One is a photograph that records the current landscape, and another is the metadata of the photograph such as latitude, longitude, and direction observed by sensors in a mobile phone. Metadata may be enhanced later to add a title, and description (among other things), and users can upload metadata and photographs to the server for sharing. The uploaded data can then be used for scholarly research, because the GPS coordinates and the temporally different views of the landscape are valuable resources for understanding the landscape changes. ResultsThe MemoryGraph (CODH, 2016) app is available for free on Google Play, but an iOS version has not yet been released. The predecessor app, MemoryHunt (Kitamoto, 2015), has been used for both the study of cultural landscapes and the monitoring of disaster recovery (DSR, 2014).To study cultural landscapes, we held several workshops with both scholars and laypersons who tried the app in the field. An example is shown in Figure 1, where the reference image was a photograph of Imperial Palace Moat in Tokyo. The photograph was easy to interpret, but the actual place was difficult to find. Figure 2 shows how participants walked along the moat to find the best match. At each place, participants took the photograph that they believed to be the best match, but the best solution of Figure 1 was found after many trials of all the participants.  For the second purpose, we held workshops at Kobe in Japan, which was severely damaged by the earthquake in 1995, and Aceh in Indonesia, which was severely damaged by the tsunami in 2004, to understand how the city recovered from the disaster. The app was enjoyed by local people in two countries, and some children were involved more actively than adults due to the gamification effect.MemoryGraph can be generalized in several ways. First, it can be generalized from landscape to object: for example, time-series photographs of the same person at different places. Second, it can be generalized to cross-media reference: for example, taking the same composition at a \"sacred place\" of pop-culture work inspired by the real landscape. Discussion on Digital CritiqueDigital critique, or what has been referred to in other words as digital criticism (Kitamoto, 2016) or data criticism (Kitamoto, 2014), was proposed by the author as a framework for digital scholarship in the humanities. It has been applied to the criticism of non-textural sources such as maps and photographs with the intent of using them as evidence for historical studies. Although the digital humanities are often considered quantitative or metric humanities, digital critique focuses on the digital creation and management of humanities knowledge. Several types of digital tools have been designed for the critical interpretation of textual and non-textural sources, including success stories such as the identification of Silk Road ruins, or the characterization of mistakes in the renovation of old Beijing maps ( Kitamoto et al., 2014) ( Kitamoto et al., 2016). We claim that MemoryGraph is another digital critique tool that asks the following research questions in order to use a photographic source as historical evidence.The first research question asks what the variance and invariance is in the target landscape over time. Invariance is often found as artificial features such as roads and monuments, or natural features such as a mountain ridge, but in other cases, the identification of invariance requires training of the user in terms of interpretive performance on the historical landscape.Secondly, we ask how historical evidences can be best integrated across different sources, such as old maps, old photographs and historical databases. Invariance found through the app is a hint to links different sources over time, and this contributes to expanding our understanding by integrating knowledge from different sources.Our final research question asks why the photograph was taken in this setting. This is because the best match is theoretically achieved by the same physical posture between the original photographer and the user (Kitamoto, 2015). This means that as a user tries to find the best match using the app, a user can re-experience the original photographer who took the photograph in the same posture. The physical overlay of the user and the photographer may lead to research questions investigating the emotion or the way of thinking of the original photographer at the time. This has potential to give birth to new interpretation of photographs from the physical or emotional perspective.",
        "article_title": "MemoryGraph: Digital Critique of Old Photographs Using a Mobile App that Enhances the Interpretation of Landscape",
        "authors": [
            {
                "given": "Asanobou",
                "family": "Kitamoto",
                "affiliation": [
                    {
                        "original_name": "National Institute of Informatics",
                        "normalized_name": "National Institute of Informatics",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/04ksd4g47",
                            "GRID": "grid.250343.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The project Mapping the Enlightenment: Intellectual Networks and the Making of Knowledge in the European Periphery is funded by the Research Centre for Humanities (RCH) for the academic year 2016-2017. The project uses interactive mapping tools for visualising, exploring, and analysing the intellectual and geographical networks developed by Greek-speaking scholars of the Ottoman Empire during the 17 th and 18 th centuries. Based on the convergence of the latest achievements in digital mapping and historiographical discussions about the representation of the Enlightenment, the project develops user-friendly interactive and dynamic web maps of the itineraries of traveling scholars, and visually represent the building of networks between scientific centers and peripheries. Additionally, this dynamic system provides multi-layered maps that enables users to query and visualize data and flows through a modern and robust environment. This interactive interface offers a simple and effective way of showing how the intellectual networks developed within the European periphery in the 17 th and the 18 th centuries contributed to the shaping of knowledge during the Enlightenment. The project is a collaboration between the ",
        "article_title": "Mapping the Enlightenment: Intellectual Networks and the Making of Knowledge in the European Periphery",
        "authors": [
            {
                "given": "Vassilis",
                "family": "Routsis",
                "affiliation": [
                    {
                        "original_name": "Centre for Digital Humanities - University College London",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Eirini",
                "family": "Goudarouli",
                "affiliation": [
                    {
                        "original_name": "The National Archives",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Manolis ",
                "family": "Patiniotis",
                "affiliation": [
                    {
                        "original_name": "National and Kapodistrian University of Athens",
                        "normalized_name": null,
                        "country": "Greece",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In blog posts, Miriam Posner and BethanyNowviskie have both addressed the structures that impede women from connecting to digital humanities. The increase of women in higher level positions within universities have led to changes in the infrastructure, with child care and nursing nests cropping up on campuses across the country. Similarly, people of color have been engaging in critical university studies long before the 1990s when the field is said to have emerged. By demanding space as students and faculty, in addition to advocating for rights as the laborers that built and maintain these institutions, people of color have organized through concerted effort to bring about changes in institutional culture and structure.The question of diversity and inclusion that informs this intervention is intimately linked to institutional austerity and the precaritization of intellectual labor. Post-2008, the economic pillaging of the university is undeniable. It is also undeniable that the digital humanities emerged as a contemporary force for disciplinary transformation during this precise economic shift. Matthew K. Gold states this matter of factly in his introduction to the 2012 Debates in the Digital Humanities anthology, \"The Digital Humanities Moment\": \"At a time when many academic institutions are facing austerity budgets, department closings, and staffing shortages, the digital humanities experienced a banner year that saw cluster hires at multiple universities, the establishment of new digital humanities centers and initiatives across the globe, and multimillion-dollar grants distributed by federal agencies and charitable foundations.\"The university's loss coextensive with DH's boon requires further interrogation. This paper builds on Bailey and Gold's work by directly linking concerns in DH for diversity and inclusion to economic disparities in the university via critical university studies. I do so not to condemn DH for its rise, but to continue to unearth its radical potential. While the literature in critical university studies is broad, I connect DH to two critical university approaches in particular: decolonial feminism and Autonomist Marxism. Both approaches augment current debates in DH as they forefront questions of inclusion, diversity, and economic variance, but also provide more pointedly political approaches to pedagogy and tool-use.In her 2003 book, Feminism Without Borders, Chandra Talapade Mohanty claims that \"the moment we tie university-based research to economic developmentand describe this research as fundamentally drive by market forces-it becomes possible to locate the university as an important player in capitalist rule\" (173). This claim is couched in a decolonial method committed to developing \"the urgent political necessity of forming strategic coalitions across class, race, and national boundaries,\" but it is also motivated by a commitment to feminist struggle (9). The university is a site of decolonial feminist struggle in particular because it is a \"contradictory place where knowledges are colonized but also contested […] It is one of the few remaining spaces in a rapidly privatized world that offers some semblance of a public arena for dialogue, engagement, and visioning of democracy and justice\" (170). What follows is therefore a simple claim, but one that is difficult to reconcile in a contemporary context, especially as it might apply to DH: \"Feminist literacy necessitates learning to see (and theorize) differently-to identify and challenge the politics of knowledge that naturalizes global capitalism and business-as-usual in North American higher education\" (171).It does not take a careful reader to detect a radical undercurrent to Mohanty's interest in feminist literacy, nor should it be a surprise that those disproportionately affected by institutional inequity might rely on a radical political logic with which to situate their intellectual labor. Perhaps the strongest emergent DH interest in which Mohanty's work carries the most methodological weight, however, is found in Roopika Risam's essay, \"Navigating the Global Digital Humanities: Insights from Black Feminism.\" There, Risam argues thatAs the field of digital humanities has grown in size and scope, the question of how to navigate a scholarly community that is diverse in geography, language, and participant demographics has become pressing. An increasing number of initiatives have sought to address these concerns, both in scholarship-as in work on postcolonial digital humanities or #trans-formDH-and through new organizational structures like the ALliance of Digital Humanities Organizations (ADHO) Multi-Lingualism and Multi-Culturalism Committee and Global Outlook::Digital Humanities (GO::DH), a special interest group of ADHO.We see similar issues at work in #transformDH and feministDH more broadly. However, Alan Liu's recent claim to a critical infrastructure studies augments these concerns. Liu summarizes his interest in critical infrastructure studies as a \"call for digital humanities research and development informed by, and able to influence, the way scholarship, teaching, administration, support services, labor practices, and even development and investment strategies in higher education intersect with society.\" The rhetorical shift from \"critical university\" to \"critical infrastructure\" is interesting here. Where Liu goes so far to say that most, if not the whole of our lives, are organized through institutional mechanisms formative of a \"social-cum-technological milieu,\" \"the word 'infrastructure' give [s] us the same kind of general purchase on social complexity that Stuart Hall, Raymond Williams, and others sought when they reached for their all-purpose word, 'culture.'\" Paired with Risam's work above, Liu draws us to closer to a critique that would mirror Mohanty's.At the same time, Mohanty's decolonial approach dialogues with Autonomist Marxist approaches to the same problem. Writing of their work with CAFA (Committee for Academic Freedom in Africa), George Caffentzis and Silvia Federici comment on institutional formations like those that Mohanty invokes, but also those that are already operative in DH: global initiatives organized around a common goal. Where Caffentzis and Federici depart from the question of DH infrastructure is certainly a question of technological focus, but also it is also a political one. \"As was the factory,\" Caffentzis and Federici write, \"so now is the university\" (125). The import of this claim comments on our institutional alliances, as well as our collective understanding of what educational institutions are for. Thinkers of critical university studies define the university this way because it maximizes the forms of solidarity that are available to us in the face of sovereign institutional control.For both DH and the Autonomist approach, solidarity is most prominently featured in tool-use and production. Following Caffentzis and Federici, Gigi Roggero mobilizes critical university studies toward a reinvention of the tool. In his article, \"Notes on Framing and Reinventing Co-research,\" he argues that \"tools of inquiry have to be reinvented at the level of the general intellect's networks, going beyond the division between the virtual and the real,\" in order to maximize living labor's break with capital, opening up a space for co-research to form a \"material base for revolution\" (520-521). DH's reinvention of the library, the archive, and the application of technology to humanistic inquiry more generally have never been more apt. At the same time, a strong dialogue with Liu and Risam's work, stemming from Bailey's claim to DH as critical university studies, is brought to the fore in Roggero's work.This paper concludes by theorizing what forms of alliance/solidarity might be drawn between DH's transformative work at the level of infrastructure with critical university studies' political work at the level of the institution. I argue that the university is not a freestanding institution; it is embedded within processes of real subsumption that span the whole of contemporary life. Concerns for diversity and inclusion are contoured by this fact, and the transformative power of tool-use extant in DH praxis resist it.  ",
        "article_title": "Digital Humanities as Critical University Studies",
        "authors": [
            {
                "given": "Matt",
                "family": "Applegate",
                "affiliation": [
                    {
                        "original_name": "Molloy College",
                        "normalized_name": "Molloy College",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0277z0p27",
                            "GRID": "grid.419950.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn this talk, we would like to outline a proposal for a shared task (ST) in and for the digital humanities. In general, shared tasks are highly productive frameworks for bringing together different researchers/research groups and, if done in a sensible way, foster interdisciplinary collaboration. They have a tradition in natural language processing (NLP) where organizers define research tasks and settings. In order to cope for the specialties of DH research, we propose a ST that works in two phases, with two distinct target audiences and possible participants.Generally, this setup allows both \"sides\" of the DH community to bring in what they do best: Humanities scholars focus on conceptual issues, their description and definition. Computer science researchers focus on technical issues and work towards automatisation (cf. Kuhn & Reiter, 2015). The ideal scenario-that both \"sides\" of DH contribute to the work in both areas- is challenging to achieve in practice. The sharedtask scenario takes this into account and encourages Humanities scholars without access to programming \"resources\" to contribute to the conceptual phase (Phase 1), while software engineers without interest in literature per se can contribute to the automatisation phase (Phase 2). We believe that this setup can actually lower the entry bar for DH research. Decoupling, however, does not imply strict, un-crossable boundaries: There needs to be interaction between the two phases, which is supported by our mixed organisation team. In particular, this setup allows mixed teams to participate in both phases (and it will be interesting to see how they fare).In Phase 1 of a shared task, participants with a strong understanding of a specific literary phenomenon (literary studies scholars) work on the creation of annotation guidelines. This allows them to bring in their expertise without worrying about the feasibility of automatisation endeavours or struggling with technical issues. We will compare the different annotation guidelines both qualitatively: by having an indepth discussion during a workshop, and quantitatively: by measuring inter-annotator agreement. This will result in a community guided selection of annotation guidelines for a set of phenomena. The involvement of the research community in this process guarantees that heterogeneous points of view are taken into account.The guidelines will then enter Phase 2 to actually make annotations on a semi-large scale. These annotations then enter a \"classical\" shared task as it is established in the NLP community: Various teams competitively contribute systems whose performances will be evaluated in a quantitative manner.Given the complexity of many phenomena in literature, we expect the automatisation of such annotations to be an interesting challenge from an engineering perspective. On the other hand, it is an excellent opportunity to initiate the development of tools tailored to the detection of specific phenomena that are relevant for computational literary studies. This talk has two purposes:• To discuss these ideas and collect feedback and propositions. This is also an explicit invitation to contribute in the setup of this initiative. We are also welcoming a discussion about the phenomena that should be included.• To advertise the idea of a shared task and to invite possible participants. The success of STs relies on a certain number of participants. Given that this has never been organized in the DH community before, we want to spread this idea throughout the community to gather estimates of potential participants. The Importance of AnnotationsIn computational literary studies, many phenomena cannot directly be detected from the text surface. To find and categorize such phenomena as, for example, the \"narrated time\" in a novel, it is first necessary to have an in-depth understanding of the text, knowledge about its author or literary conventions, or knowledge of the text's historical context. Therefore, instances of such phenomena need to be annotated either by human experts or software that is tailored to this task.Unfortunately, many theories describing interesting phenomena are very difficult to apply to real texts. It has been shown numerous times (e.g., Reiter, 2015, Musi et al., 2016) that annotating theories or concepts directly can lead to very poor inter-annotator agreement (IAA): Different annotators have different interpretions of not only the text, but also descriptions of the theoretical concepts. Although subjective annotations have their merit, studying annotations on large scale depends on their consistency, i.e., a high IAA. In addition, many theories are underspecified and provide examples for illustrations only. Creators of annotation guidelines often have to interpret what is meant by a certain statement and extend definitions to cover examples found in real texts.Annotation guidelines serve as a mediator between the annotators and a theory (that may use specialised vocabulary). Additionally, such guidelines often contain re-appearing instance patterns and their modes of annotation and/or exceptions, as well as many examples from real texts (see below).We see the creation of annotation guidelines as one of the cornerstones of large scale text analysis in computational literary studies. Additionally, the creation of annotiation guidelines supports systematic disciplinary discussions about concepts and thus may lead to additional findings relevant for the theoretical discourse (e.g., Meister, 1995; Gius and Jacke, forthcoming). Experts from the field literary studies are well-suited to work with annotation guidelines, as annotation of literary phenomena in literary texts can be seen as a special form of close reading. Phase One: Annotation GuidelinesThe Shared TaskIn theory, any phenomenon can be addressed in this fashion, as long as it can be defined inter-subjectively, is reasonably frequent, and is of interest in computational literary studies. As a starting point, we propose to address the issue of narrative levels (Pier, 2014). Narrative levels are a core concept in narrative theory (Genette, 1980;Bal, 1997) which in turn has shown to be a promising foundation for automatisation in literary theory (Bö gel et al. 2015). The first reason for choosing to examine narrative levels is their ubiquity: every narrative text necessarily consists at least one, most texts contain many narrative levels; each element of a text can be assigned to a specific level. The second reason is our intuition that a definition of \"level\" in guidelines is as achievable as the automated detection of levels by computers.Concretely, participating teams are asked to create guidelines for the detection and annotation of a) narrative levels and b) the relation of the narrator(s) to the narrated world (i.e., is the narrator part of the narrated world or not?). Participants are not bound to adhere to a specific narratological theory. The result of this phase, however, will be a fixation on a set of guidelines (that instantiate a theory).We will select a number of literary narrative texts and provide copyright-free digitized corpora. All \"official\" texts (development and test sets) will be English literary texts. Naturally, the second step will be to extend this framework to other languages and/or phenomena. EvaluationIn NLP shared tasks, the predictions of the systems are compared against a fixed test set- the \"gold standard\". Since there is no gold standard in Phase 1, we will evaluate the guidelines using an unseen data set. Each participating team annotates this set using their own guidelines before the guidelines are submitted. Submitted guidelines will be anonymized and re-distributed among the participants. Each participant is asked to annotate the evaluation data set using two other annotation guidelines. In addition, we will be collecting annotations from students.The evaluation data set will thus be annotated according to each participant's guidelines four times (1x self, 1x student,  2x other participants).This setup allows direct calculation of inter-annotator agreement. However, IAA should be only one aspect in evaluating the guidelines, but not the only one. Therefore, we will submit a workshop at DH 2018 to discuss the submissions and select final ones. This setup also allows the merging of different annotation guidelines as well as adaptation according to the discussion during the workshop. Soon after the workshop, Phase 2 will commence. Phase Two: Automatic PredictionIn Phase 2 of this endeavour, the selected guidelines of Phase 1 will be annotated on a large scale by student assistants. Since we do not yet know how long, complex and involved the guidelines will be, there should be close communication between the organizers and the team responsible for the selected guidelines.As soon as the texts have been annotated, they will be made accessible to participants. For the deadline in Phase 2, participants will be asked to process a new data set - one that has not been released before - and return the predicted annotations to the organizers, who will then make evaluations using standard measures (e.g., accuracy). Finally, there will be a workshop in which each participating team presents its system. This workshop is projected to be coordinated with the LaTeCH workshop series, which has taken place at ACL conferences in the past years (one of the authors of this paper has been a workshop organiser in past years).There will be no fixation on computational approaches, statistical models, programming languages or environments to tackle this problem. The main benefit of shared tasks towards automatisation is that different approaches can be compared directly. Restricting this possibility space would directly harm the goal. Technical Details and TimelineThis proposal is innovative in a number of ways: Shared tasks are a new kind of framework in the DH/H community, as such, focalisation have not been investigated in this way before. Last but not least, a shared task with the goal of creating annotation guidelines has not been organized before (to our knowledge). We believe it is more important to do this right than to do this fast, hence we are looking at a rather lengthy timeline.Attracting enough participants is the main challenge from the organiser's perspective. The main incentives we envision for contributors are excellent publication opportunities: All submitted and generated materials will be published online (open access) under the umbrella of the shared task. Each individual work will be citable. This includes the submitted annotation guidelines, the produced consensus guidelines, and explanation and commentary documents.In addition to the publication incentive, we believe that our approach is an important contribution towards systematic text analysis in the DH realm. We count on the playfulness and curiosity (in the best sense!) of the DH community to take part in this experiment. ",
        "article_title": "A Shared Task for a Shared Goal: Systematic Annotation of Literary Texts",
        "authors": [
            {
                "given": "Nils",
                "family": "Reiter",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Evelyn",
                "family": "Gius",
                "affiliation": [
                    {
                        "original_name": "Universität Hamburg (University of Hamburg)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Jannik",
                "family": "Strötgen",
                "affiliation": [
                    {
                        "original_name": "Max Planck Institute for Informatics",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Marcus",
                "family": "Willand",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Introduction to the CorpusThe corpus consists of digitized facsimiles of two Swiss newspapers, \"Journal de Genè ve\" (JDG) from years 1826 to 1997 and \"Gazette de Lausanne\" (GDL) from years 1804 to 1997. Scanned daily issues of each journal were transcribed using an optical character recognition (OCR) system (Rochat et al, 2016). The entire scanned data weighs more than 20TB, which makes most usual analysis techniques out of reach for regular desktop computers. This corpus has been the focus of several studies analyzing textual data (such as linguistic changes (Buntix et al, 2016) and named entity recognition ( Ehrmann et al, 2016) ). An example of different layouts of GDL's first page is given in Figure 1 which shows the evolution of various features, such as title size and position, fonts and number of columns. Bitmap Factorial AnalysisIn order to analyze layout evolution, we propose to build a static layout representation for every year in the corpus. Thus, when studying each newspaper's first page, we define the pixel t of the static representation í µí±ƒ \",$ of month m of year y así µí±ƒ \",$ % = 1 í µí± \"$ í µí±ƒ \",$,) % * +,)-. Where í µí± \"$ is the number of issues in month m of year y and í µí±ƒ \",$,) is the first page of day d of month m of year y. The pixel t of the static representation í µí±ƒ \" of year y is then defined as  . Different layouts of GDL in years 1825, 1850 and 1875 (top, left to right), 1925, 1950 and 1975 (bottom,   These representations give a vision of the mean layout over the course of a given year. Each yearly representation can be projected in a two-dimensional space by performing a principal component analysis (PCA) which maximizes the covariance on every pixel. This method is analogous to the eigenfaces method used for face recognition (Turk andPentland, 1991a, 1991b) We compute the eigenvectors, that we named eigenpages, as well as the eigenvalues of the covariance matrix of the pixels. The yearly representations are then projected in the two-dimensional space of the two eigenvectors which have the highest eigenvalues. The resulting projections of yearly mean images of JDG and GDL from years 1900 to 1998 are portrayed in Figure 3. In these figures, each point is a yearly image and consecutive years are linked in order to highlight the change over time. The further apart the points are, the bigger the layout's changes occurring between two years. Visual inspection reveals several clusters of years with a similar layout. Furthermore, homogeneous sequences of years may be clustered automatically based on the (unprojected) distance between them (e.g. by computing the distance between year y and y+1 and \"cutting\" the sequence of years at positions where their distance exceeds an arbitrary threshold.  DiscussionThe PCA technique allows us to quantify layout changes by covariance analysis of the pixels of yearly representations. The proportion of covariance information shown by the PCA is 73% for JDG and 76% for GDL. Visual interpretation reveals different chronological clusters which are displayed in Tables 1 and 2 along with their mean positions in the twodimensional space of eigenpages as well as mean images representing these periods (computed in the same way as yearly images, cf. Figure 2). These mean images reveal the major layout transitions in each journal which may be summarized as follows: Journal de Genè ve (JDG):• 1900-1915: 6 columns, title above columns 2 to 5, little space between columns.• 1916-1931: 4 columns, title above columns 1 to 4, more space between columns.• 1932-1964: 4 columns, change of the layout around the title and the first title position. • 1965-1968: 4 columns, change of the layout around the title, boxes with black borders begin to appear.• 1969-1991: 4 columns, total change of the title, title above columns 2 to 4, logo appears, more space between columns and boxes, article titles are bigger.• 1992-1995: 5 columns, fusion of JDG and GDL, big change of layout, boxes inside boxes begin to appear, more stable structure.• 1996-1998: 6 columns, big change in title font, previous column layout replaced by a more classic one, article titles are placed at the top of the first page. Gazette de Lausanne (GDL):• 1900-1945: 6 columns, title above columns 2 to 5, little space between columns.• 1946-1966: 7 columns, title above columns 2 to 6, more space between columns yielding particularly small column sizes. • 1967-1970: 5 columns, title above columns 2 to 5, first column begins before the title which is on the right, advertisements placed below the page. • 1971-1973: 6 columns, more classic layout with article titles at the top. • 1974-1991: 4 columns, lots of space between columns and articles, bigger article titles.• 1992-1995: 5 columns, fusion of JDG and GDL, big change of layout, boxes inside boxes begin to appear, more stable structure.• 1996-1998: 6 columns, big change in title font, column layout replaced by a more classic one, the article titles are placed at the top of the first page. The automatic clustering method described in previous chapter has been applied on unprojected distances and produce similar clustering results (depending on the threshold parameter). Qualitative analysis confirms that the resulting clusters are all separated by important layout transition phases.   This analysis is also useful to compare several newspaper publishing strategies. We projected the two newspapers in the same two-dimensional space representation (presented in Figure 3) using the same method with yearly representations of both journals in order to compare their chronological trajectories. The covariance information shown by the PCA is 67%. Visual inspection reveals three main clusters for each journal. Each of these clusters turns out to correspond to groups of clusters that has been detected in the previous projections. We observe that the layout of both journals has evolved in a similar way but with different timescales. GDL is more dispersed than JDG and has explored different strategies during the period 1900-1966. However, GDL has adopted a style more similar to JDG style between 1967 and 1973 just before it entered a major layout transition in 1974 (5 years later than JDG).  ConclusionThese first results demonstrate a promising method of detecting layout evolution automatically. The method is applicable to a large variety of longitudinal image corpora without any prerequisites, since it only requires images in bitmap format. It make it possible to compare several corpora and determine periods of layout transitions in a common two-dimensional space for visual interpretation. In addition, unprojected distances can be used to determine layout changes in an entirely automatic fashion, by analyzing the representation space through clustering algorithms. Future work on this method should include the integration of an alignment method in the bitmap preprocessing step, because alignment errors may impact the pixel covariance analysis and eigenpages creation.Were í µí± \" is the number of month representations í µí±ƒ \",$ in year y. A diagram of the process is shown in figure 2. Figure 11Figure 1. Different layouts of GDL in years 1825, 1850 and 1875 (top, left to right), 1925, 1950 and 1975 (bottom, left to right). ",
        "article_title": "Layout analysis on newspaper archives",
        "authors": [
            {
                "given": "Vincent",
                "family": "Buntinx",
                "affiliation": [
                    {
                        "original_name": "École Polytechnique Fédérale de Lausanne (EPFL)",
                        "normalized_name": null,
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Frédéric ",
                "family": "Kaplan",
                "affiliation": [
                    {
                        "original_name": "École Polytechnique Fédérale de Lausanne (EPFL)",
                        "normalized_name": null,
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Aris ",
                "family": "Xanthos",
                "affiliation": [
                    {
                        "original_name": "Université de Lausanne",
                        "normalized_name": null,
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn the scholarly domain, annotation is a fundamental activity (Unsworth, 2000). Current webbased annotation facilities enable a specific way of annotation (via note-taking, highlighting or commenting) which are useful when scholars are exploring or gathering an initial set of resources, but more sophisticated support is needed for detailed analysis, close reading, and data enrichment. At this point, it is important to take into account the structural relations between documents and their parts. For example, when annotating a letter, annotation tools should be aware that a targeted text fragment is the name of the sender, or that the annotation of a film targets the intellectual work instead of the specific version or copy on which the annotation is made.In addition, many standalone tools use annotation models with idiosyncratic solutions to enable the relations between different media objects and their parts, which limits the possibilities to exchange those annotations . In general, there is a lack of necessary details for durable access to and interpretation of annotations. For this, detailed information is needed about the annotated object, the annotator and the annotation itself ( Melgar et al. 2016, Walkowski & Barker, 2010. In this paper we focus on the requirements for the annotated object, in a web-based environment, and propose a method for making necessary details of objects openly available for any annotation tool. Requirements of scholarly annotationIn line with (W3C 2017b) we refer to the object that is annotated as the annotation target, the content of the annotation as the annotation body and who or what creates the annotation as the annotation creator. All three are complex entities with aspects that have consequences for interpreting an annotation (Melgar et al., 2016).Annotation Creator: With respect to the creator it is important to know the intention/motivation for making the annotation (Walkowski & Barker, 2010) and when sharing and reusing annotations, their level of expertise, both in terms of the scholarly domain and in the nature of the annotation task (e.g. the amount of experience/expertise of the annotator in classifying objects according to a controlled vocabulary).Annotation target: of the target it is important to know which part of the object is targeted. This is not merely about addressing media fragments. Media (e.g., html, mp3, jpg) are carriers of abstract information objects (scenes in movies, chapters in books, objects in pictures) with different conceptual levels (e.g. work, expression or manifestation , see Figure 1) and it is essential to be able to address those abstract objects and the relationships between them.Annotation body: Of the content of the annotation it is important to know its nature (a natural language comment, structural or subject metadata, a link to another resource), in what form it is made (e.g. closed representation or natural language representation), at what level of control (from mostly uncontrolled to strictly controlled and structured) and for what scholarly purpose, e.g. gathering or exploring sources or thematic or stylistic analysis ( Melgar et al., 2017). State of the ArtThere are various models for capturing digital annotations to make them accessible and interpretable. The Web Annotation Data Model (W3C 2017aModel (W3C , 2017b) is a generic model that covers aspects of the annotation body, target and creator. This model focuses on annotations in the context of online social interaction (e.g., commenting, sharing), not necessarily on scholarly annotations done during analysis or data enrichment .An extended model specifically for scholarly research was proposed by Hunter et al. (2011), which includes context aspects for both the annotation body and target. The Annotating All Knowledge Coalition is also directed at scholarly annotation and lists several issues, including:1. The lack of support for discovery, sharing and reuse of annotations. 2. Underutilization of collections. 3. The closed and non-standardized nature of current annotation tools. Current annotation support is either part of a suite of functionalities in monolithic applications with their own models for annotation (e.g. TextGrid , Textual Communities , eLaborate , CATMA for text,Elan and Anvil for multimedia materials, and QDA software packages for mixed media qualitative data analysis), or they lack specificity in describing the annotation target, e.g. Hypothes.is (Perkel, 2015) and Pundit ( Grassi et al., 2012) and site-specific annotation tools, e.g. in The Diary of Samuel Pepys). Building on earlier work ( Melgar et al., 2016), in this paper we argue the need for application support for more specificity of the annotation target (see Figure 1). We identify two additional issues with the current state-of-the-art: 4. The W3C annotation protocol lacks support for a potential annotation target identifying and describing itself to the annotation tool. 5. The model also lacks a schema, which would allow scholars or website maintainers to define constraints for a specific class of annotations that is applicable in the context of a specific group of scholarly objects. Use case: annotation in scholarly digital editionsThese issues are illustrated by a scenario of a digital scholarly edition where scholars have a need for annotation support (Boot, 2009, Robinson, 2004, Siemens et al., 2012. Consider an edition that wants to incorporate an external annotation tool into its pages ( Figure 2): an edition server shows an edition to a client in a browser. The annotation client runs within that same browser window, but doesn't know about the edition's structure and it talks with its own server. To communicate intelligently with the user, the annotation client needs information about the structure of the edition, which has to be provided by the edition.The annotation tool should know about the edition's structure for a number of reasons:• The edition often contains multiple representations of the same text fragment. There might be a diplomatic and a critical transcription, one or more translations, audio versions, and who knows what other versions, and annotations made in one of these should be available in others; • Other sites may have other editions of this particular text. It should be possible to exchange annotations between them; • The edition has an internal structure, e.g. a book divided in chapters, or the fragments appearing in modern authors' drafts, or the elaborate structure with multiple apparatuses of some editions of medieval texts. An annotation that refers to a specific component of an edition should be able to address that component and know what sort of component it is.• The edition should be able to propose suitable annotation types for its components. For personal names, it might suggest an annotation type that links the person to an authority file. For transcriptions, there might be special annotation types for proposed corrections to the transcription. Edition collaboratories could use the annotation functionality to solicit multiple sorts of specialised information from its collaborators.This proposal requires that: (i) the edition describes itself and its structure to the annotation tool, and provides suitable labels for the annotatable objects; (ii) the edition can suggest annotation types for the annotatable objects; (iii) the effort to integrate annotation functionality in existing editions is minimal; (iv) the annotation tool is generic, but able to handle the created annotations with awareness of the structure that they apply to (it can e.g. return aggregated annotations); (v) the annotation targets are durable and not formulated in terms of HTML structure; and (vi) URI's should be treated as opaque (i.e., we shouldn't try to guess the relations between the annotated components based on their URIs); and lastly (vii) URIs should be canonical. Proposed SolutionWe propose a solution similar to Schema.org (an initiative for adding structural semantics to information on the web) whereby descriptive information about annotatable resources is made accessible to the client by embedding it in the HTML presentation layer through RDFa attributes (Figure 3), using an extensible resource descriptive ontology. Figure 4 shows a basic ontology for text objects (left half of Figure 4) with an edition-specific extension for the example edition (right half of Figure 4). This ontology shares concepts with the FRBRoo ontology ( Bekiari et al., 2015) but starts from specific annotation-related concepts. In future work we will investigate extending the ontology with FRBRoo concepts.Although this approach is focused on annotation of resources on the web, the same principle could be applied in offline annotation, if the offline resources are described in a similar way and annotation clients are developed to make use of this. Also, descriptive information for textual sources can be embedded as markup, but for audiovisual documents, this has to be done via a separate representation, for instance using SMIL (Bulterman et al., 2008).  Methodological impactIn our proposal annotatable resources describe their own semantic structure, thereby facilitating finegrained annotations. With the RDFa attributes, annotation clients can identify the annotation target in terms of the resource structure (issue 4), which makes annotations less dependent on specific views on the underlying object. Furthermore, this allows development of lightweight open source annotation clients that web services can easily embed to bring annotation to collections of scholarly interest (issue 3).This makes it easier for scholars to use and reuse annotations to support the argument made in a scholarly article (issue 1). It allows distinguishing different groups of annotations, so researchers can choose to display certain groups of annotations, thereby avoiding being drowned by irrelevant annotations (issue 5). It facilitates employing annotation functionality to ask for targeted comments on resource parts (what do you think of this translation? What clarification of this material are you missing?). Scholars can also more meaningfully combine and compare them across collections and media types, e.g. analyse the correspondence between book and film versions of an intellectual work (issue 2).If the annotations are consistently stored using open protocols, it becomes possible to reference them in scholarly publications. Collateral benefit of floating this form of 'deep web' semantics to the surface is that other external services such as search engines can also use the exposed semantic information to reason about available resources. ",
        "article_title": "Facilitating Fine-grained Open Annotations of Scholarly Sources",
        "authors": [
            {
                "given": "Peter",
                "family": "Boot",
                "affiliation": [
                    {
                        "original_name": "Huygens ING",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            },
            {
                "given": "Ronald",
                "family": "Dekker",
                "affiliation": [
                    {
                        "original_name": "Huygens ING",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            },
            {
                "given": "Marijn",
                "family": "Koolen",
                "affiliation": [
                    {
                        "original_name": "Huygens ING",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            },
            {
                "given": "Liliana",
                "family": "Melgar",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn Switzerland, the panorama of scientific research is deemed to be deeply affected by language barriers and strong local academic identities. Is this impression confirmed by data on research projects? What are the factors that best explain the structure of scientific collaborations over the last forty years? Do linguistic regions (Switzerland is divided into three principals) or local academic logics really have an impact onto the mapping of research collaborations and to what extend are they embedded in disciplinary, historical and generational logics?We focus on the very large database of the Swiss National Science Foundation (SNSF), the principal research funding agency in Switzerland, which lists all the 62,000 projects funded between 1975 and 2015. While scientometric studies generally focus on measuring work - and financial - performance, we aim to raise awareness on pursuing a socio-history analyse of Swiss academic circles by crossing the SNSF data with a prosopographic database of all Swiss university professors in the twentieth century provided by the Swiss Elite Observatory (OBELIS). Beyond the interest for the history of science and universities, we explore the noteworthy technical challenge of a network analysis of nearly 88,000 researchers and more than a million of collaborations.By combining those two databases, we measure the temporality and spatiality of academic collaborations, i.e. to define a way to deal with the volume of information in order to provide not only a global vision but also to enable a fine processing of personal trajectories. SourcesThe SNSF database has been placed under an Open Data licence in spring 2016. Called \"P3\" for \"Projects, People, Publications\", it contains detailed information on all the projects funded since 1975 (around 500 per year in the beginning, almost 3,000 per year today, see Fig.1), as well as the whole list of people involved in the projects. The database can sometimes be incomplete about the discipline and institutional affiliation of individuals, since it depends directly on the project submission interface where some fields may be left empty. Thus, this gap is partly offset by the junction with the Swiss professors database that provides systematic data on Swiss professors. Thus, the projects are classified according to a standard tree of scientific disciplines.  MethodologyWe are interested in the 2006-2015 period, ten years during which 25,000 projects involving 45,000 people produce a graph of more than 350,000 edges. On the one hand, this short periodization allows us to confront our assumptions to our data before analysing the full corpus. On the other hand, it helps to test the effectiveness of our tools and the interoperability of the two databases to prepare a complete and longitudinal modelling.We therefore extracted a 2-mode network of people and projects from the database and then projected it into a 1-mode network of people only (the nature of the link being to be affiliated as collaborators to the same research project). If usually a relatively simple task, the transformation of a 2-mode graph into a 1-mode graph is here greatly complicated by the mass of information to process: when the graph matrix contains billions of positions, most softwares are reaching their limits. We will then divide the dataset into smaller units (here, transforming the network year after year helps make it bearable to a standard processor). Analysis and VisualisationThe topography of the network obtained for 2006-2015 ( Fig.2) is quite remarkable. The center of the network is not, as it is often, the densest region, which would have meant that a single discipline or field of study was likely to play a role of interface between others. Instead, we observe an almost circular distribution of individuals, recalling other \"science maps\" based on the organization of institutions of bibliometric analysis ( Rafols et al. 2010). Data visualization, and in particular the representation of complex networks, is not an end in itself but a tool for questioning the structure of the dataset (Grandjean 2015). But while a further research will focus on more detailed indicators to qualify individual positions (in particular, centrality measures, as detailed by Koschützki et al. 2005 or Newman 2010), this first overview still shows that some groups of disciplines form very obvious clusters. This is the case of physics (right), medical sciences (bottom left) or earth sciences (top right). Others are sparsely connected or dispersed within other communities, as is particularly the case for disciplines like economics/business studies or chemistry, which seem to be more engaged in interdisciplinary collaborations or projects that include a limited number of employees (large experimental science projects partly explain the density of these groups). We also assume the structure of the network to differ among disciplinary specificities and temporality (Bourdieu, 2004;Gingras, 2012;Heilbron & Gingras, 2015). Are most connected disciplines also the most prestigious ones?  PerspectivesWith the information contained in the list of projects, we see that it is also possible to assign individuals a disciplinary category extracted from the projects involving them. As it happens that a researcher is participating to projects labelled in different disciplines, this approach will lead to a reflexion on the measurement of interdisciplinarity within a comparative study between a selection of « open » and « closed » disciplines.We will also see that it is possible to develop a multi-level analysis to compare the graph clustering to the many Swiss institutional and disciplinary « geographies », in order to historicize their development.",
        "article_title": "Complex Network Visualisation for the History of Interdisciplinarity: Mapping Research Funding in Switzerland",
        "authors": [
            {
                "given": "Martin",
                "family": "Grandjean",
                "affiliation": [
                    {
                        "original_name": "University of Lausanne",
                        "normalized_name": "University of Lausanne",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/019whta54",
                            "GRID": "grid.9851.5"
                        }
                    }
                ]
            },
            {
                "given": "Pierre",
                "family": "Benz",
                "affiliation": [
                    {
                        "original_name": "University of Lausanne",
                        "normalized_name": "University of Lausanne",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/019whta54",
                            "GRID": "grid.9851.5"
                        }
                    }
                ]
            },
            {
                "given": "Thierry",
                "family": "Rossier",
                "affiliation": [
                    {
                        "original_name": "University of Lausanne",
                        "normalized_name": "University of Lausanne",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/019whta54",
                            "GRID": "grid.9851.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn the field of global history, especially when it comes to « histoire croisée », the use of metaphors describing the vertical organization of a society, its structuration into layers or into overlapping systems, is common. The temptation to use a second metaphor, calling everything a \"network\", is also very important in this domain, whose objects of study are often transnational organizations with multiple branches, intertwined within umbrella organizations, sharing board members and including several levels of secretariats and subcommittees (Grandjean 2017). However, the use of these images is not limited to historical studies, since we use the same vocabulary in other disciplines to describe social situations or textual structures.When we go beyond the metaphor to develop a formal analysis, we often produce multigraphs who, because they simultaneously express horizontal and vertical relationships, are generally unsuitable for the analysis (and visualisation, except in very simple cases). If the \"exploratory\" dimension of social network analysis -and especially the fact that its display is relatively subjective -is often a subject of criticism, we propose here to play with the visual representation to show precisely how an original modelling can improve the reading of complex graphs, and helping to restore a \"morphological\" (Moretti 1999, 68) information where disorder seems to prevail.Based on two examples from archives mapping and theatre character networks, this paper proposes a reflection on the different ways to take account of verticality in graphs. In particular, we are developing a way to impose a macro-structure to a network, allowing a two-dimensions view that reflects the hierarchical affiliations of its components. We will see that this method, by constructing a stable visual representation in time and space, helps to compare different types of relationships and/or different time states of the graph. Network levelsWhat is evident in an affiliation network is not always explicit in other situations, but a multimode graph is always the expression of a form of multilevel network (Lazega and Snijders 2016). For instance, there is an implicit hierarchy among the committees level and the level of individuals within them. It is thus easy to imagine such networks as superimposed layers, linked by the vertical affiliation links. And this analysis is obviously interesting because these vertical links are not the only ones to influence the model structure: committees in the upper stratum may themselves be organized into their own horizontal structure, as well as individuals, in the lower stratum, can weave relationship regardless of the structure of the committees to which they belong. This kind of macro/micro-structure comparison is not new in sociology: through sociometric approaches of urban social structures, for example, some address the organization of metropolitan communities together with that of interpersonal relationships (Laumann 1973). This raises the issue of representing these networks within a two-dimensional plane, e.g. by changes in the colour and shape of the markers ( Wang et al. 2016), or by an artificial transfer of the upperlevel in a region of the graph that enables them to be read ( Zappa and Lomi 2015). When the low complexity of networks allows, some may also use three-dimensional representations, clearly indicating the superimposed planes (Brailly and Lazega 2012). The model: projecting structure on relations Fig. 1 visually explains a relatively simple multilevel graph, with four levels of actors (documents D, exchanged by individuals C belonging to subinstitutions B themselves grouped by top-institutions A) and five different types of relationships, including three vertical. This example depicts an institution, but it can be exported in a wide variety of domains : it may well be a medieval family network (C) in villages (B) under the authority of lordships (A), and sharing agricultural properties (D). Or theatre characters having friendship relations (C), organized in groups (B) and appearing together in scenes (D). In these examples, we see very concretely how the 2-mode graph express vertical relationships.Secondly, we proceed to a flattening of the hierarchical structure of the two upper levels as sets containing the elements to be studied (here, individuals), as in Fig. 2 (I). Now it is no longer the horizontal relationships between individuals (C) that affect the display of the graph but these sets, fixed once and for all.Creating a stable spatialization is the condition for a comparative analysis: we can therefore display side by side the graph of document exchanges (J, product by projecting G as a 1-mode graph of individuals) and the graph of interpersonal relations (I), without a reorganization of nodes that would make the hierarchy unreadable. Figure 2. To reduce the complexity of a multigraph and allow a comparative analysis of different types of relationships, we gather the micro-level relations (I and J) in the macro-level structure. Here, we therefore compare interinstitutional relations (H), the personal relations of the individuals (K) and the documents exchanged between them (L), all summarized in sub-institutional level. Then we move to the upper level by summarizing the individual relationships as relations between the groups they belong to. We can now compare the institutional relation between these groups (H) with personal relationships (K) and the exchange of documents (L). In our example, we see that the patterns are very different, even though a majority of relationships logically occur within the subinstitutions (see self-edges in K and L). ApplicationsWhen Sampson, in the central square of Verona, calls his colleague Gregory, also a servant in the Capulet family, he creates the first edge of the character network of Shakespeare's tragedy « Romeo and Juliet » (for more discussion of the study of character networks, see Trilke et al. 2015 andXanthos et al. 2016). The readability of the interaction graph of this introductory scene, the confrontation of the two hostile houses, is greatly enhanced by a highlight of these vertical relationships (Fig. 3), the affiliations of all the protagonists to family identities that will structure the plot. Figure 3. The character network of Shakespeare's tragedy \"Romeo and Juliet\" (Grandjean 2015). Two characters are connected if they appear simultaneously in a scene. On the left, the network is spatialized with a classical force-directed algorithm, and on the right by imposing a \"family geography\") (family in the inner circle, servants in the outer circle). In the context of more complex networks, where it is less about creating a new visual and pedagogical artifact to facilitate narratological studies than to find a way to automate pattern detection, we will also discuss the case of network analysis of large archive corpora (Grandjean 2014). In this case (Fig. 4), we will show in particular that it is possible to detect individuals that bypass institutional hierarchy, when horizontal relationships do not align with vertical affiliations.  Nations (archives 1919Nations (archives -1927. On the left, the network spatialized with a force-directed algorithm, and on the right spatialized by imposing a vertical hierarchy, flattening the affiliation of each individual in a kind of \"institutional geography (Grandjean 2016). ",
        "article_title": "Multimode and Multilevel: Vertical Dimension in Historical and Literary Networks",
        "authors": [
            {
                "given": "Martin",
                "family": "Grandjean",
                "affiliation": [
                    {
                        "original_name": "University of Lausanne",
                        "normalized_name": "University of Lausanne",
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": "https://ror.org/019whta54",
                            "GRID": "grid.9851.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionStorytelling is a central form of human artistic expression. An ingredient of the appeal of stories is their emotional content. Readers of literature form explicit mental representations of fictional characters' emotional states (Gernsbacher, Goldsmith, and Robertson, 1992;Vega, 1996). Even more, a gripping \" […] literary work produces a complex emotional experience in the reader. This experience is inseparable from the depictive content of the narrative\" (Hogan, 2011). This raises the question of the relationship between the narrative and emotional levels in literature. We explore how computational emotion analysis can contribute to the characterization of story genres, which are difficult to define and for which various criteria have been proposed, including stylistic ones ( Biber and Conrad, 2009) and narratological ones (Chatman, 1978).Our hypothesis is that genres can be linked to the development of predominant emotions over the course of the text. To test this, we present a computational model of multi-label emotion analysis of literary genres and apply it to a set of English literary works from the Project Gutenberg for five genres, namely adventure, romance, mystery, science fiction, and humorous fiction. We identify prototypical shapes for each genre and show that this analysis produces results, which can find a place in the computational analysis of literary genres and extend existing stylometric approaches.Cuddon (2012) defines adventure as \"a form of fiction [...] in which the hero conventionally undergoes a series of testing and episodic adventures\" and mystery as a narrative involving the \"detection of crime, with the motives, actions, arraignment, judgement and punishment of a criminal\". Baldick (2015) defined Romance as narratives with \"improbable adventures of idealized characters\". Today, however, the term covers many forms of fiction, including love stories. We use the term romance as a literary genre in this broader sense. Regarding science fiction stories, it is generally agreed that they are \" [...] about an amazing variety of things, topics, ideas. They include trips to other worlds, quests, the exploration of space...\" (Cuddon, 2012). Humorous fiction is comical literature \"written chiefly to amuse its audience\" (Cuddon, 2012). MethodsWe calculate emotion scores for eight basic emotions, namely joy, sadness, trust, disgust, fear, anger, surprise, and anticipation (Plutchik, 2001). We use the NRC Emotion Lexicon (Mohammad and Turney, 2013). Since the data in Project Gutenberg is diachronic, this method of emotion recognition might not be appropriate for older texts and, in general, may suffer from low recall. However, it can be considered a high-precision approach suitable for our purpose.To obtain an emotion analysis for a story, we start by computing emotion scores for spans of text (Klinger, Suliya, and Reiter, 2016). Formally, we compute the score es(e, S) for an emotion e and a span of tokens S=<tn,…,tm> as where De is a dictionary containing words associated with emotion e and 1t∈D is 1 if t ∈ D and 0 otherwise. We do this for each of our eight emotions, obtaining an eight dimensional \"emotion vector\" for each span. We analyzed the stability of our results across different settings and found that different dictionaries affect the actual values but not the relation between different time steps. These scores are not probabilities, but could be transformed if needed.To observe development over time, we could use sliding windows; however, continuous time series are notoriously difficult to interpret. Therefore, we adopt a simpler scheme inspired by the five-act theory of dramatic acts (Freytag, 1863), according to which dramas are divided into five acts: exposition, rising action, climax, falling action, and denouement. We consequently divide each text into five successive, equal-sized spans (since different texts have different length, the size of acts varies between texts) that we assume to correspond roughly to dramatic acts in Freytag's theory, with exposition at position 1 and denouement at position 5, and compute an eightelement emotion vector for each Act. Experimental EvaluationWe now demonstrate how this emotion aggregation into five acts can contribute to the understanding of different literary genres. DataWe collect 2113 books from Project Gutenberg that belong to five genres found in the Brown corpus (Francis and Kucera, 1979), namely adventure (585 books), romance (383 books), mystery (380 books), science fiction (562 books), and humorous fiction (203 books). The corpus is available from the authors upon request.The selection is based on the Library of Congress Subject Headings in the metadata. We select all books that contain the word \"Fiction\" in combination with one of the following labels: \"Adventure stories\", \"Love stories\", \"Romantic fiction\", \"Detective and mystery stories\", \"Science fiction\" or \"Humor\". Furthermore, we reject books with the following labels: \"Short stories\", \"Complete works\", \"Volume\", \"Chapter\", \"Collection\", \"Part\". This constraint is aimed at excluding files which contain partial or multiple works. Qualitative analysisEach plot in Figure 1 depicts the act-by-act development for one emotion with their emotion score es(e,S). Since we interpret shapes rather than values, we omit the legend. The average over all books is shown in a dark-colored line. The area around that line corresponds to confidence intervals at a 95% confidence level.For sadness, anger, fear, and disgust, all five genres show a close correspondence, namely a consistent increase of the emotion from Act 1 through Act 5 -corresponding to gripping narratives. Mystery and science fiction lack the drop in anger and tend to end with higher levels of this emotion. Joy is inverse to these emotions, showing a decreasing tendency from Act 1 to Act 5 for all genres with exception of humorous fiction that shows a plateau between Acts 1 and 4.In adventures, all emotions increase in the first half of the books, but drop sharply between Act 4 and Act 5. This is consistent with Whetter (2008), according to whom adventures are marked by a late drop in emotional tension when the hero's misfortunes come to an end. The only exception is trust that shows increase towards the end for all genres, which is especially noticeable in adventures. A potential reason is that prototypical adventure novels are 'upbeat' in that they cultivate virtues such as courage and loyalty (Baldick, 2015, p. 5) and depict heroes that do not lose trust even amid unexpected dangers.The results for anticipation and surprise show less coherent tendencies which we find difficult to interpret. These two emotions appear less constitutive to the narrative structure of genres, at least those that we currently consider: anticipation and surprise can occur under (almost) any circumstances. Mystery fiction has a slightly different pattern, where anticipation exhibits steady increase from Acts 1 to 4 and its peak coincides with the peak for surprise at Act 4. Quantitative analysisWe analyze the genre-specific time course of emotions quantitatively by computing associations between genres and the Act in which an emotion \"peaks\" in a story. We define a random variable vie for an emotion peak as vie=1 iff the highest value of emotion e is in Act i. The association between each genre and emotion peaks vie follows point-wise mutual information (Church and Hanks, 1990), where probabilities are computed as relative frequencies over the dataset. Figure 2 gives insight into the genre-specific emotion patterns. For instance, disgust is characteristic of Act 4 for all genres. The only exception is science fiction that does not list disgust or surprise among the top 10 features. Trust is important at the beginning and in the end of adventures and science fiction, but is missing in mystery. Similarly, romance fiction is not characterized by anticipation among top-ranked features, corresponding to its \"anticipation\" curve that decreases monotone from beginning to end. Interestingly, humor is the only genre that does not contain joy among the top 10 features.  Related workSentiment and emotion in fiction have been previously addressed computationally by Mohammad (2012), Nalisnick andBaird (2013), Heuser, Moretti, andSteiner (2016), among others. Samothrakis and Fasli (2015) is the only work we are aware of which discusses emotions in context of genres.The study most related to ours is Reagan et al. (2016). They propose a pipeline that tracks emotions in text. Their main claim is that stories typically follow one out of six \"emotional arcs\" regarding happiness. Conclusion, discussion and future workWe investigated the relationship between emotional development in literature and genre and observe differences among emotions. The genre of adventure stands out, especially concerning the end of the story arc. Our results can provide a novel starting point for characterizing similarities and differences within and between literary genres.Our observations require further investigation regarding the underlying factors. For instance, it might be argued that the pattern for mystery stories is dominated by the subgenre of crime fiction. Future work will combine our distant reading approach with close and scalable reading. Furthermore, to improve emotion recognition, we plan to use distributional methods for expanding the existing lexical resources and adapting them to texts from different historical periods (cf. Buechel, Hellrich, and Hahn, 2016).",
        "article_title": "Prototypical Emotion Developments in Literary Genres",
        "authors": [
            {
                "given": "Evgeny",
                "family": "Kim",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Sebastian",
                "family": "Padó",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Roman",
                "family": "Klinger",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis paper reports on work in progress aimed at facilitating the creation, sharing, linking, and analysis of data about the movement of people, ideas, cultural practices, and commodities between places, over the course of history. Products of the Linked Places project include: conceptual and logical models for historical routes; a temporal extension of the popular GeoJSON data format, called GeoJSON-T; several varied exemplar data sets converted to GeoJSON-T format; prototype web software for browsing and visualizing that data; and Python scripts to convert data between CSV, GeoJSON-T, and RDF compatible with the Pelagios Gazetteer Interconnection Format. Substantial interim work products are shared in the Linked Places and GeoJSON-T GitHub repositories and have been reported in some detail in two blog posts (1,2) . MotivationA growing number of historical gazetteers are being developed in the course of digital humanities research projects (Berman, Mostern & Southall, 2016). Their spatial temporal coverage is typically limited to a particular area and period due to factors of scholarly quality, cost, and relevance to a given project. Coverage extents do vary considerably, from a single city for a few generations to a region for several centuries. With few exceptions, these gazetteers are unpublished as such; instead they are spatial tables contained within, and integral to, the larger project data store.Because historical gazetteers are difficult and timeconsuming to produce, it is vital they be published, when possible, in a way that permits linking them-an activity that the Pelagios project has made great strides in facilitating. An emergent network of specialized gazetteers holds terrific promise, not only for re-use, but ultimately as a distributed, increasingly comprehensive geographical (i.e. spatial-temporal) index to linked data from numerous domains, including history, archaeology, literary studies, philology, and several of the social sciences. The focus of such an index, and encyclopedic applications it enables, will be on individual places, typically at the scales of cities and points of interest.Such systems are highly desirable, but given a large volume of data about individual places we can also begin harvesting, creating, and sharing data about the connections between them. We should be able to ask of historical gazetteers: What journeys and historical routes has a given place been a waypoint on? And, what flows of people, ideas, and commodities has it been a source or sink for?But the Linked Places and GeoJSON-T projects have been undertaken with an even larger, \"moonshot\" vision in mind: a system allowing scholars and the general public to visualize and analyze the emergence, growth and spread of human settlements, their changing attributes, and the dynamic connections between them, including the diffusion of technologies and cultural practices.To realize these ideas, we need a) lots of data, and b) methods and means for merging or linking them. In some respects, we are starting from scratch; data about historical movement is sparse and stored in disparate forms. Much of it will be newly generated, for example by parsing texts, transforming tabular records, or digitally tracing lines on historical maps. Merging and linking operations will require that the form of data from different sources (or abbreviated catalogues thereof) be either standardized (in the case of merging), or similar enough that automated alignment is feasible.The majority of works on geographic networks concerns physical media like roads and rail, whereas movement data is eventive. Geographers have modeled migration flows and disease diffusion for several decades, providing theoretical bases for their analysis that are outside our present scope. An overview of that work is found in (Lowe & Moryadas 1975). An excellent and more recent work on mobility and geographic movement is Tim Cresswell's \"On the Move\" (2006). We are not aware of any efforts to model data for historical routes computationally, however the core abstraction we build upon is the traditional graph/network model of nodes and edges credited to 18th century work of Euler (Biggs, et al 1986). A Modeling PatternData modeling is as much an art as a science (Simsion & Witt 2004), but some core best practices are well-known. A typical first step is establishing what entities are to be represented, what their essential attributes are, and what relationships obtain between them (cf. Chen, 1976). This step is often best accomplished collaboratively, in an iterative process undertaken by domain experts. Our results were immediately published to blog posts and relevant listservs, and the resulting input was useful in refining the model.When the modeling context is an individual research project, it hardly matters what names are given those entities and relationships-only that the data store's internal logic be sound and well understood by project members. But if, as in this case, the system will accommodate data from many sources or be accessed by others, we need to find broad agreement on a conceptual model and a vocabulary for its constituents between as many prospective participants as possible-that is, to describe the ontology of the research domain. Although much ontology engineering of this sort has involved comprehensive high-level ontologies such as the CIDOC-CRM , the development and implementation of small ontology design patterns (ODP) has been gaining favor since the introduction of that paradigm by Aldo Gangemi (2005). Such patterns, by any name, are \"reusable successful solutions to a recurrent modeling problem\" (definition provided by the Association for Ontology Design & Patterns (ODPA) ) which can be used alone or assembled in modular fashion for larger requirements. Examples include patterns for \"Place,\" \"Event,\" \"Participation,\" and \"Region.\"And so the first step taken in the Linked Places project has been to develop an ontology design pattern for the historical movement of something between two or more places over some physical channel, either for some time during or throughout a timespan. The pattern, visualized in Figure 1, comprises the following conceptual understandings:A route describes an attestation of one or more occurrences of the movement of something (e.g. people, commodities, information) between two or more places, either for some time during or throughout a time_period. Routes are composed of one or more segment, each of which is composed of two places and a path (corresponding to nodes and edges in network parlance), the locations and temporal attributes for which may be unknown or unspecified. Movement between places occurred upon ways (the term used by OpenStreetMap) -physical channels such as roads, rivers, canals, railways, footpaths, and sea lanes-and may have been directional.The three types of routes considered here are journeys, flows, and historical_routes:A journey is the record of a specific instance of travel by one or more individuals. Examples include: the 7th century pilgrimage of the Buddhist monk Xuanzang across China and India; the first voyage of Captain James Cook, between 1768 and 1771.A flow is the record of the movement of something (commodities, people, ideas) between two places, aggregated as a magnitude over a period of time. Examples include: the transport of captive Africans between West Africa and Bahia in the 17th century; letters between certain correspondents in Paris and Prague in the 18th century; a source network of late Neolithic obsidian artifacts and known source locations on the Anatolian Plateau.A historical_route asserts a single or composite named course of travel between places, taken repeatedly by unspecified individuals over time, usually for purposes of commerce. Examples include the Silk Road and the Amber Routes. Some correspond with named roads, for instance the Via Salaria in Italy is both a way and a historical_route. Additional axioms indicated by the relations and cardinality expressions (e.g. 0…*) in Figure 1 include:• All routes are sourced, normally to textual or cartographic documents • The way for a segment (its physical path described by a geometry) may be known and represented, unknown, or ignored (Segments with unspecified ways will typically be visualized as a line or arc) • Each segment has one or more temporal attribute (\"when\"), which can be a time_period, (possibly named) or a sequence (e.g. after segment n) • Routes and their component segments can have any number of attributes (properties), dependent upon data sources and project requirements The ontology pattern we introduce here is specialized, as compared to high level ontologies like CIDOC-CRM. We have not yet mapped our distinctive entities (route, journey, flow, historical_route, segment, when) to existing ontologies. The term place is commonly found, but usually is synonymous with location; the sense we are adopting is that of the Pleiades gazetteer, but is not in a published ontology that we're aware of. In any case, we feel it is best to first lay down a logically coherent set of terms and at a later date attempt to align them with other ontologies. FormatsThe route ODP has informed our development and implementation of recommended standard data formats. It turns out all three types of routes can be effectively described in GeoJSON-T, an extended version of GeoJSON, the widely-used format for representing geographic FeatureCollections. A FeatureCollection of routes will include both Place and Route features. Route segments are articulated as an array of one or more geometries in a route's GeometryCollection. GeoJSON-T allows optional \"when\" objects, both for each feature at the same level as its geometry object and for segment geometries (Figure 2). Features and segments have certain required properties as shown, and can have unlimited project-specific properties.  DataTo date, seven exemplar datasets have been converted from a typical CSV format to GeoJSON-T, using a newly developed Python program. Three are for journeys: two by individuals (a 7th century pilgrimage and a modern circumnavigation), the third by 840 Venetian ship convoys in the 13-15th centuries. Another dataset aggregates those ship journeys as flows having magnitudes of journeys and ships. The last three are historical_routes: the Roman era itinerary of the Vicarello Beakers, the route system between courier stations in Ming Dynasty China, and a large set of \"Old World\" trade and pilgrimage routes . SoftwareThe widespread adoption of GeoJSON has demonstrated that for a data format to be useful, there must be software with visualization and analysis capabilities that supports it. Accordingly, an essential element of the Linked Places project is development of proof of concept web software to render GeoJSON-T data, for both routes and places alone, to a map and timeline together. The development of that software is ongoing, and publicly available. ",
        "article_title": "Linked Places: A Modeling Pattern and Software for Representing Historical Movement",
        "authors": [
            {
                "given": "Karl",
                "family": "Grossner",
                "affiliation": [
                    {
                        "original_name": "World Heritage Web",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Merrick",
                "family": "Berman",
                "affiliation": [
                    {
                        "original_name": "Harvard University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Rainer",
                "family": "Simon",
                "affiliation": [
                    {
                        "original_name": "AIT Austrian Institute of Technology GmbH",
                        "normalized_name": null,
                        "country": "Austria",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis paper examines the opportunities, approaches and issues of automatically classifying historical newspaper articles from the Netherlands for 'genre' as an expression of the historically and culturally determined conception of journalism. Genre is defined as \"language use in a conventionalized communicative setting in order to give expression to a specific set of communicative goals of a disciplinary or social institution, which give rise to stable structural forms\" (Hand- ford 2010). As Barnhurst and Nerone (2001) argue: \"The form includes the way the medium imagines itself to be and to act. In its physical arrangement, structure, and format, a newspaper reiterates an ideal for itself.\" Examining the generic form of newspaper articles form a historical perspective therefore sheds an interesting light on the way newspaper journalism has developed.The digital era, which is typified as the 'age of abundance' of historical newspaper material, poses new challenges to historical research. Historical approaches to selecting and analyzing newspapers, rooted in the assumption of a scarcity of available material, had to be replaced with social scientific methods, such as quantitative content analysis (Nicholson 2013;Broersma 2009). Yet, manual quantitative content analyses are still highly time consuming and therefore expensive. Moreover, even then the size of the material that can be covered represents only a small part of the amount of material available (Har- bers 2014). Automated forms of (content) analysis have the potential to alleviate or even solve this issue. As such, automatic forms of content analysis would be highly suitable for longitudinal and also comparative historical research into the development of newspapers, which particularly grapples with the overabundance of available research material (Broersma 2009).However, although these approaches have a great appeal to researchers (Allen, Waldstein & Zhu 2008;Grimmer & Stewart 2013), research in this vein is mostly done in information science and linguistics. It seldom has a press historical perspective (Broersma 2009). Moreover, the emphasis has mostly been on topical modeling (Lee & Myaeng 2002), whereas attention for automatic classification of style and genre is scarce. Rather than determining what subjects and themes are being discussed, this project aims to examine genre as a modes of expression of newspapers, shedding light on the discursive context (Handford 2010). This is a particularly difficult task as genres are dynamic and can change or fade away over time while new ones can emerge. Moreover, genres are ideal-typical discursive constructs, which means the textual manifestations do not always match the characteristics of these constructs perfectly, nor can they always be clearly delineated from other genres.The research question therefore is:To what extent and how can historical newspaper articles be automatically classified for genre?To examine this question, we have designed a research project that builds on an existing set of metadata describing several textual characteristics, such as genre, of a large sample of historical newspaper articles. This dataset was the result of a large-scale research project into the historical development of European newspapers with the title 'Reporting at the boundaries of the public sphere. Form, Style and Strategy of European Journalism, 1880-2005'. The set of metadata is derived from a manual content analysis that coded for genre based on a detailed rule-based coding manual. The metadata set relates to a corpus of approximately 33.000 Dutch newspaper articles from three types of Dutch newspapers, divided over the sample years 1885, 1905, 1925, 1965, 1985and 2005(Harbers, 2014. This set of metadata thus provides us with a number of labeled example articles that can be used to train and formally evaluate a classifier that is able to automatically predict the genre of additional samples of historical newspaper articles. In order to so the metadata first has to be connected to the full text of the corresponding digitized articles in the Dutch newspaper repository of the National Library (KB).The second step is to use this enriched dataset to train a classifier that can classify historical newspaper articles automatically.This paper first shows a way to connect the metadata to the digitized historical newspaper articles (Phase 1). Ultimately, it offers an outline of a concrete machine learning approach, applying linear and nonlinear classifiers, to predict the genre of a newspaper article. As a part of this, the paper discusses the different tools we have tried out and the problems we have encountered in the process (Phase 2). Specifically, the paper reflects on the way the rule-based approach to determining genre in the manual content analysis relates to the training of an automatic classifier based on machine learning techniques.In order to link the articles described in the existing metadata set to the corresponding KB data, we first created a number of rules to find the most promising candidate links for each item in the original data set, based on the position of the article on the page, its size, and the presence of images and quotes. Since these rule alone turned out not to be sufficiently accurate, a simple classifier was trained to select the best link from the candidate set, if any, based on features such as the difference in size and number of images present between the article as described in the original data set and the article as found in the KB repository, as well as author mentions and subject matter, amongst other features. By only accepting links predicted by this classifier with a relatively high confidence value approximately 50% of all articles could be automatically linked, with an error rate of 0.5%. Some genres were underrepresented in the automatically linked set and were manually expanded upon.The data set resulting from phase 1 was then used to create a training and test set for the actual genre classifier. This, again, involved taking several steps: first, the OCR for the articles in the set was retrieved and cleaned of quoted text by means of regular expressions. Next, the remaining text was pre-processed with the Natural Language Processing software package Frog, performing tasks such as segmentation, tokenization, and part-of-speech tagging. From the resulting annotated text, the relevant features for each article were calculated, such as number of words, number of sentences, number of direct quotes removed, and, most importantly, number of adjectives and various types of pronouns found in the text. These features, together with the existing genre labels were finally used to train a Support Vector Machine to choose one of eight possible genres for each article, ranging from news report and interview to news analysis and opinion article.The initial results we obtained with this classifier were quite promising. Our first attempt resulted in an accuracy score of 58%, with the default accuracy by predicting the majority class or genre being 46%. The confusion matrix made from the predictions provides important information about which genres are most difficult to predict and what mistakes are most commonly made. This information can help us to fine-tune the existing features. Moreover, we have not yet implemented all the textual features that typify the different genres, such as named entities occurring in the text, sentiment, or self-classification to name only a few. Finally, we will compare the results of different algorithms, including deep learning approaches. Based on this, we expect to be able to improve these initial results in the coming months and present our final results in more detail at the conference.  ",
        "article_title": "Distinguishing Newspaper Genres. Exploring Automated Classification of Journalism's Modes of Expression",
        "authors": [
            {
                "given": "Frank",
                "family": "Harbers",
                "affiliation": [
                    {
                        "original_name": "University of Groningen",
                        "normalized_name": "University of Groningen",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/012p63287",
                            "GRID": "grid.4830.f"
                        }
                    }
                ]
            },
            {
                "given": "Juliette",
                "family": "Lonij",
                "affiliation": [
                    {
                        "original_name": "National Library of the Netherlands",
                        "normalized_name": "National Library of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/02w4jbg70",
                            "GRID": "grid.425631.7"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe study of literature has traditionally focused on the literary work, and sometimes its author, rather than on the response that works evoked in their readers. The arrival of the computer in the study of literature has not really changed that -perhaps unsurprisingly, as reader response has never been systematically recorded. The fact that readers have begun to document their reading and reading response on websites is therefore very fortunate ( Gruzd andRehberg Sedo, 2012, Maryl, 2008: 390- 406). Booksellers' sites such as Amazon and review sites such as Goodreads, as well as weblogs, forums and general-purpose social media sites provide access to first-hand reading reports. Though most research on these sites focusses on (behavior of) users (Nakamura, 2013: 238-243, Thomas andRound, 2016: 239-253), we are beginning to see them being used in literary research (Finn, 2011) . This paper presents the Online Dutch Book Response (ODBR) database, that was designed to facilitate research into book response. At present, the database holds reviews and other response items from an online bookseller as well as from four Dutchlanguage mass review sites, including, where available, the information about books, reviewers and sites necessary to put the reviews into context.To show one type of research that the database supports, the paper displays a clustering of reviews by genre, based on frequently used words. I discuss the clustering and what it suggests for further research. Content: mass review sites, reviews from booksellers' sites, weblogsWhile the ODBR database was designed to hold any kind of (online) book discussion, at the moment it holds 280,000 response items from four mass review sites: three from the Netherlands (hebban.nl, dizzie.nl and watleesjij.nu) and one from Flanders (lezerstippenlezers.be). It also holds 313,000 items from the main online bookseller in the Netherlands, bol.com, and 36,000 brief expert reviews. We are currently working on downloading about 400 book blogs.Mass review sites are sites where the main focus is on book reviews uploaded by the sites' users. The best known example is Goodreads (goodreads.com) (Thelwall and Kousha, 2016: 972-983). Hebben.nl, for example, is currently the largest mass review site in the Dutch-language area. It is in many respects like Goodreads: users post reviews, they can follow other users and they can create book lists. Other users can respond to the reviews or vote them up or down. There are moderated reading clubs on specific books. Apart from the user-contributed content, Hebban also holds a fair amount of editorial material, among other things expert reviews, blog posts, interviews and giveaways. Lezerstippenlezers.be ('readers tip off readers') is somewhat simpler in that it lacks the social functions of the other sites. Noteworthy about dizzie.nl is among other things that the site was downloaded for inclusion into the ODBR database only days before it was closed down. The site administrators explicitly stated in their final announcements that no archive would be kept. The ODBR database may be the most complete record of the site's existence. Since then, watleesjij.nu has also been closed down. Database design and contentThe purpose of the database is to facilitate research into online book response. It should be able to store the response texts as well as information about the response's context. Response items are not just reviews. They include book lists, expert reviews, blog posts and review responses and other response types. Ratings and tags are also stored in the database. Figure  2 shows the main entities in the database. The database now contains the following numbers of records: 146,800 books, 58,000 user accounts, 40,000 friendships/followers, and 628,800 book responses. The types of the responses are given in Table 1. The 'response' items (blogresponse, etc.) are responses to a response: a blogresponse is a response to a blogpost. This large collection of book response items and context information creates many different possibilities for research. First of all, the response texts facilitate the investigation of response to individual books, authors and genres. The texts show the norms that readers apply as well as the way reading affects them. The availability of expert reviews alongside general readers' responses creates the possibility to study differences between (semi-)professional and lay reading. Other types of content enable different lines of research. Information about friends and followers can help investigate the influence of the social environment in reading choices and book appreciation. As many writers use the book review platforms to get in touch with their (potential) readers, the collected data can also provide insight into their marketing strategies. Information about the book lists that readers create (for instance 'to read', 'read in 2014') shows which books are perceived as similar, prompting the question whether they will also be rated similarly. The integration of the discussions and the context information from multiple sites in a single environment that facilitates integrated querying is something that, as far as I know, has not been done before.Unfortunately, because of copyright and privacy concerns, the database is not accessible over the web. Researchers who are interested in accessing its content are asked to contact the author. Clustering by genreOf the research possibilities that the ODBR database offers, here I discuss just one example, an investigation in the word use in reviews by genre. This is an interesting subject, as word usage can be considered as an indication of how people respond to books. I will show a clustering of the genres by word use, which should give a first indication of which genres are perceived by readers as similar.Dutch publishers use a shared system for classifying their books. This so-called NUR code (an abbreviation meaning Dutch-language Uniform Categorization) covers aspects of format as well as genre. On some of the downloaded sites, books were assigned NUR codes. As the load process of the database tries to merge the book information from multiple sites, NUR codes are available for 75% of the reviews. In the computations, reviews were merged by NUR code. Relative frequencies were computed for all words, these frequencies were then transformed into z-scores. The Euclidean distances between the frequency vectors for the 200 most frequent words were computed and formed the basis for the clustering dendrogram in Figure 3. The figure only shows NUR codes for which there are more than 500 reviews available. For other choices for most frequent words and distance measure, the clustering is largely similar. Looking for an explanation, we can drill down to look at the individual words underlying the dendrogram and see for example how readers of (literary) thrillers talk about plot and plot lines, as one would expect. They also speak about 'main characters'(plural), while people who discuss literature use the singular 'main character'. That suggests an interesting difference between literature on the one hand and (literary) thrillers on the other.To praise a book, readers of literature use 'beautiful' and 'nice', readers of (literary) thrillers use 'good' or 'great'. For other observations I am still looking for an explanation. Why, for example, do people who discuss literature use more personal pronouns? That question, like many others, requires further investigation. ConclusionUntil now, most humanities-oriented researchers that have worked on online book discussion sites and communities have taken a qualitative approach (Fister, 2005: 303-309, Foasberg, 2012. The ODBR database is meant to facilitate quantitative research into online book discussion and, through the lens of online book discussion, into literature, both with respect to its effects on readers and as a social phenomenon. The rich data model and the large quantity of available data should provide support for both language and network oriented research approaches.",
        "article_title": "A Database of Online Book Response and the Nature of the Literary Thriller",
        "authors": [
            {
                "given": "Peter",
                "family": "Boot",
                "affiliation": [
                    {
                        "original_name": "Huygens ING",
                        "normalized_name": "Huygens Institute for the History of the Netherlands",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04x6kq749",
                            "GRID": "grid.450092.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe significance of place names exceeds the usually admitted frame of deictic and indexical functions, as they enfold more than a mere reference in space. In the western tradition, a current of reflexion which seems to date back to the 1960s has provided the theoretical foundations of the \"spatial turn\", whose epitome is the concept of space as emergent rather than existing a priori, and composed of relations rather than structures (Warf, 2009). The emergence of current named \"GeoHumanities\" (Dear et al., 2011) or \"Spatial Humanities\" (Bodenhammer et al., 2010), has prompted for a transfer of research objects between disciplines as well as an enforcement of the spatial turn in practice through specific methods of analysis. The common denominator consists in opening up new spaces and experimenting in a transdisciplinary perspective (Domínguez, 2011) in a field which has been evolving at an exponential pace since the last decade (Caquard and Cartwright, 2014).In this paper, I introduce a visualization of collocations of toponyms in the satirical literary magazine Die Fackel (\"The Torch\"), originally published and almost entirely written by the satirist and language critic Karl Kraus in Vienna from 1899 to 1936. This work carries heterogeneity at its core and contains a considerable variety of toponyms (Biber, 2001) which are highly significant because of the multinational nature of the Austro-Hungarian empire and the later formation of a territorially diminished state.In order to provide an additional, synthetic access to a digital edition of the work which is already available online (AAC-Fackel corpus), I set out on a distant reading experiment leading to maps meant to uncover patterns and specificities which are not easily retraceable during close reading. I focus on the concept of visualization, that is on the processes and not on the products (Crampton, 2001), and present them together with a critical apparatus, by giving a theoretical perspective on what is being shown and seen. In fact, digital methods in humanities ought to be criticized (Wulfman, 2014) and the cartographic enterprise bears both a thrill and a risk: \"adding more to the world through abstraction\", and \"adding to the riskiness of cartographic politics by proliferating yet more renders of the world\" (Gerlach, 2014). Extraction of toponymsThe particular task of finding place names in texts is commonly named place names extraction, toponym resolution, or geocoding. A first stage involves the identification of potential geographic references, while a second stage resides in a disambiguation process ( Leetaru, 2012). Toponym resolution often relies on named-entity recognition and artificial intelligence ( Leidner and Lieberman, 2011). However, knowledge-based methods using fine-grained datafor example from Wikipedia - have already been used with encouraging results ( Hu et al., 2014).The present endeavor grounds on a specially curated gazetteer: during the 20th century there have been significant political changes in Central Europe that have severely affected toponyms, so that geographical databases lack coverage and detail. Consequently, the database developed at the Austrian Academy of Sciences (Academy Corpora) in cooperation with the Berlin-Brandenburg Academy of Sciences (Language Center) focuses on Europe and follows from a combination of approaches: gazetteers are curated in a semi-supervised way to account for historical differences, and current geographical information is used as a fallback. Wikidata API and the Geonames database are used to build the database semi-automatically.The tokenized files of works to be analyzed are filtered and matched with the database by finite-state automatons (Barbaresi and Biber, 2016): toponyms (single or multi-word expressions) are extracted using a sliding window. A cascade of filters is used: current and historical states; regions, important subparts of states, and regional landscapes; populated places; and geographical features. Disambiguation being a critical component ( Leetaru, 2012), an algorithm similar to Pouliquen et al. (2006), who demonstrated that an acceptable precision can be reached that way, guesses the most probable entry based on distance to Vienna (Sinnott, 1984), contextual information (closestcountry, last names resolved), and importance (place type, population count). The results are projected on a map of Europe using TileMill. From collocations to lines of thoughtIn a further analysis, I visualize co-occurrences of extracted toponyms, which can be considered to be a subset of GeoCollocations (Bubenhofer, 2014), in order to draw sequences, airborne lines following their order of appearance. The word \"network\" is to be used with circumspection as Latour (1999) suggests. Although it is ubiquitous in the terminology of the spatial turn, the now predominant interpretation in the sense of the World Wide Web suggests an immediacy which is contrary to the acceptions it had before, so that the concept of \"meshwork\" is more appropriate (Ingold, 2007). I thus interpret Figure 1 as a general meshwork which makes it possible to visualize paths depicting chains of thought (Gedankengänge) as well as their intensity (welltrodden or seldom). If they may reveal spatial patterns that would otherwise remain hidden in texts ( Bodenhammer et al., 2010), these linkages are also \"mappings and tracing imposed on the data\" (Wulfman, 2014) which are not meant to be interpreted without further filtering.  A rhizome as entry to Die FackelThat is why I refine the map by selecting a subset of the co-occurrences -the maximal distance between two extracted place names is fixed to twenty tokensand by color-coding qualitative features - the typology of place names and the axis of time. The most frequent place names are printed out. Surfaces (regions for instance) cannot be represented as such because of historical evolutions and because of the difficulties of linking surfaces without tampering with map readability. Coastlines are depicted in gray to give a sense of orientation, no boundaries are drawn, as they are of a changing nature and may superimpose an artificial reading of the map (Smith 2005). The notion of rhizome has been used in corpus linguistics by Scharloth et al. (2013) to qualify discourses captured by collocation graphs, it has originally been coined by Deleuze andGuattari (1987 [1980]). This concept is particularly adequate for Kraus, as the Austrian satirist has always been concerned by the multiple aspects of discourse and reality. In addition, his work in Die Fackel evades distant reading processes because of the number of citations used and its ever present and extensive usage of parody. It would be vain to design an authoritative cartography of Die Fackel: following from the principles of heterogeneity and \"asignifying rupture\" (ibid.), the lines are frequently interrupted. Phenomena in the low-frequency range are filtered out by the human eye, but clusters and interpretation cues may emerge which provide a different access to the work. In this regard, Figure 2 depicts a rhizome connecting heterogeneous information, just as we are all \"traversed by lines, geodesics, tropics, and zones marching to different beats and differing in nature\" (ibid.). ConclusionI have presented a distant reading experiment which consists of connecting toponyms extracted and projected on maps. The latter are meant to be released as an additional feature to the existing digital edition. The Software and gazetteer will be made available under open-source licenses, for development files (please refer to the Github repository). The first example displays unfiltered lines of thought, whereas the second one grounds on a refined analysis and lets the practical image of a rhizome emerge. While Die Fackel criticizes mechanical, instrumental language (Hirt, 2002), the \"well-informed\" linguistic instruments can help materializing dots or sequences, but not without \"human\" intervention. The filtering steps on the projection echo the hermeneutic circle of the philological tradition; they also make computed information visible and apprehensible which could remain \"blind\" otherwise ( Barbaresi, 2012). This is not an authoritative cartography of Die Fackel but rather an indirect depiction of the viewpoint of Kraus and his contemporaries. Drawing on Kraus' vitriolic recording of political life, toponyms in Die Fackel tell a story about the ongoing reconfiguration of Europe. As the map conveys the uncanny sensation that the continent is a field on which points east and west are projected, the lines of force entangle European countries and capitals. Their spatial patterns document an inclination for major cultural centers, whereas the chronological dimension captures a major shift towards the end of publication: the force field intensifies as its range narrows, showing both the interplay of major European powers of the time and the emergence of transatlantic (westwards) and transeuropean (eastwards) relationships. This evolution can be read as an intensification of tensions and a prefiguration of other schemes, this time of military nature. ",
        "article_title": "Toponyms as Entry Points into a Digital Edition: Mapping Die Fackel (1899-1936)",
        "authors": [
            {
                "given": "Adrien",
                "family": "Barbaresi",
                "affiliation": [
                    {
                        "original_name": "Austrian Academy of Sciences",
                        "normalized_name": "Austrian Academy of Sciences",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/03anc3s24",
                            "GRID": "grid.4299.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Textbook ResearchThe field of textbook research is one of the more recent and more diverse areas of academic investigation. The condensed and canonical character of the information selected for inclusion in textbookshere understood as conventional textbooks -gives them central significance in academic, political and educational respects. Textbooks, as carriers of the knowledge and information that one generation wishes to pass on to the next, frequently find themselves at the center of political controversy. As such, their importance as an object of investigation in historical and cultural research has gained significant traction in recent decades. However, textbook research has not yet found dedicated representation as a main subject at universities.As a non-university institution, the Georg Eckert Institute (GEI) for International Textbook Research conducts and facilitates applied and multidisciplinary research into textbooks and educational media primarily informed by history and cultural studies. For this purpose, the GEI provides digital and social research infrastructure services such as its renowned research library and various dedicated digital information services, such as Edumeres, a virtual network with modules for specific aspects of textbook research. As such, the GEI realizes a unique position in the international field of textbook research.The study of textbooks has not only been facilitated by growing institutional support and infrastructure but also by the proliferation of new digital methods and resources in humanities research. In the digital humanities, the investigation of research questions is supported by a range of increasingly sophisticated digital methods such as automatic image and text analysis, linguistic text annotation, or data visualization. Digital tools and services combined with the increasing amount of resources available through digital libraries (such as the German Digital Library, the Deutsches Textarchiv, and Europeana) and research infrastructures (such as CLARIN or DARIAH) provide digital support for textbook analysis. Digital Information ServicesAt the GEI, the shift towards more digitally oriented research has resulted in a range of digital information services specifically tailored to textbook research. EurViews, for example, is a multilingual digital platform containing primary sources from twentieth and twenty-first century history textbooks from around the world that manifest particular concepts of Europe and Europeanness (for example, see Gehler and Vietta, 2010;Best et al, 2012, andChakrabarty, 2000). The service also offers essays, commentaries, and educational histories written by designated experts in the field. EurViews is a useful tool for historians searching for relevant, reliable and hard-tofind research materials on topics related to textbooks. The materials may provide inspiration for research projects or be the starting point for more extensive searches for sources. EurViews also demonstrates that the printed monograph is no longer the dominant form of publication but that digital representation is gaining in importance.Digital representations are increasingly becoming objects of investigation themselves. GEI-Digital , for example, a digital library focusing on out-of-copyright works published between the inception of textbooks in the seventeenth century and the demise of imperial Germany in 1918, holds potentially relevant textual resources for EurViews. Other important information services provided by the GEI offer factual and bibliographic data relevant for textbook research: edu.data, for example, provides information about textbook systems in individual countries, and the library catalogue contains bibliographic metadata about textbooks from around the globe.While these services all contain resources and information that are immediately relevant to textbook research, their content is frequently stored in isolated data silos that lack appropriate interfaces or standardized data models and which prevent convenient use or exchange of data within the GEI or with external services. For example, even though GEIDigital makes metadata about its resources available as METS/MODS encoded data via an OAI-PMH interface, EurViews lacks the interfaces that would enable it to utilize these full-text resources. Similarly, data on textbook systems from edu.data or bibliographic metadata from the library catalogue cannot be reused by existing or new research projects. External digital humanities infrastructures such as CLARIN-D are also unable to easily access texts stored in GEI-Digital or EurViews since those platforms contain plain text documents only which are not semantically annotated. WorldViewsThe WorldViews project, which is financed by the Federal Ministry of Education and Research (BMBF), addresses these challenges in order to further elevate access to textbook resources. Its aim being to enhance discoverability, reusability, and sustainability of textbook resources in cultural and historical research and in the digital humanities. The formative use case has been the digital platform for textbook sources, EurViews, a service that has proven to be in high demand- Since 2015 EurViews has had an average of 400 visitors per month, a comparatively high number for this kind of digital academic service-and which constitutes one of the cornerstones of the GEI's research infrastructure.Two significant milestones have been achieved on the path to enhanced access. The first is the selection of the Text Encoding Initiative (TEI) standard that will be used to encode the semantics of conventional textbook resources and thus facilitate access to fulltext sources. In addition, the Component MetaData Infrastructure (CMDI) framework has been chosen for the overall integration of metadata at the GEI; a framework also compatible with the CLARIN-D infrastructure.An extensive search for publicly available and dedicated encoding schemas for textual characteristics of conventional textbooks yielded no results. Therefore, a profile specifying the most relevant metadata and textual features in textbooks was developed from scratch after consulting historians at the GEI. The only viable option for creating an encoding schema based on the profile turned out to be the TEI encoding standard. The TEI schema created for textbooks focuses on basic elements for the selective and formal description of those structural and semantic features that are immediately relevant for WorldViews. For example, headings of sections or the semantics of particular paragraphs should provide the necessary semantics to enable retrieval scenarios to contextualize search results and to formulate more precise queries targeting particular segments of the text. This groundbreaking schema is designed to provide the nucleus for more comprehensive descriptions of whole textbooks, such as those found in GEI-Digital.The Component MetaData Infrastructure (CMDI) provides a framework that allows blueprints of distinct metadata components to be defined and reused. CMDI allows standard metadata components to act as profiles for virtually any kind of metadata. CMDI profiles have been created to describe textbooks resources in WorldViews (we have switched to the new version of CMDI, released in mid-2016) and we are currently investigating their application in more fact-based resources such as edu.data. By using CMDI as a general framework for metadata descriptions, full-text resources can immediately be indexed by CLARIN's Virtual Language Observatory and can be analyzed using its various tools and services such as Weblicht. The CMDI description of GEI resources allows for internally standardized search and retrieval operations in federated search scenarios.The second milestone for better access to textbook resources was the implementation of a logic tier. Central components of this tier are Solr-indices for federated searches, tools for handling digitization workflows, controlled metadata annotation and fulltext annotation of textbook sources and, significantly, a Fedora repository that will dynamically provide access to standardized representations of textbook resources and other data from the various digital infrastructure services at the GEI for internal as well as external consumers. After extensive evaluation of similar repository software such as DSpace, Fedora was selected due to the greater customizability and flexibility offered by its strong modularity and existing applications with CMDI metadata. Furthermore, use of Fedora is a prerequisite for becoming a CLARIN center, one of the long-term objectives of the GEI. Through the logic tier digital platforms such as EurViews are able to send queries for bibliographic metadata on newly added textbooks directly to the library catalogue or they can directly reuse textbook resources from other systems such as GEI Digital or obtain contextual data from databases such as edu.data.  ",
        "article_title": "WorldViews: Access to International Textbooks for Digital Humanities Researchers",
        "authors": [
            {
                "given": "Steffen",
                "family": "Hennicke",
                "affiliation": [
                    {
                        "original_name": "Georg Eckert Institut for International Textbook Research (GEI)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Lena-Luise ",
                "family": "Stahn",
                "affiliation": [
                    {
                        "original_name": "Georg Eckert Institut for International Textbook Research (GEI)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Ernesto William ",
                "family": "De Luca",
                "affiliation": [
                    {
                        "original_name": "Georg Eckert Institut for International Textbook Research (GEI)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Kerstin ",
                "family": "Schwedes",
                "affiliation": [
                    {
                        "original_name": "Georg Eckert Institut for International Textbook Research (GEI)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Andreas ",
                "family": "Witt",
                "affiliation": [
                    {
                        "original_name": "Institut für Deutsche Sprache (IDS)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionA lot of Digital Humanities (DH) research involves applying Natural Language Processing (NLP) tasks, such as, sentiment analysis, named entity recognition, or topic modeling. A large amount of NLP software is already available. On the one hand, there are frameworks that bundle software for different tasks and languages (e.g., NLTK [Bird et al, 2009], or xtas1), and on the other hand there are tools that target specific tasks (e.g., gensim, Rehurek and Sojka, 2010). As long as researchers do not need to combine tools from different packages, it is usually relatively easy to write scripts that perform the task. However, for innovative research, combining tools often is required, especially when working with non-English text. This abstract presents work in progress on NLP Pipeline (nlppln), an open source tool that improves access to NLP software by facilitating combining NLP functionality from different software packages2.nlppln is based on Common Workflow Language (CWL), a standard for describing data analysis workflows and tools (Amstutz et al, 2016). The main advantage of using a standard is that any existing NLP tool can be integrated into a workflow, as long as it can be run as a command line tool. This flexibility is missing from existing frameworks for creating NLP pipelines, such as DKPro (Eckart de Castilho, and Gurevych, 2015) using the UIMA framework (Ferrucci, and Lally, 2004). In addition to improved reuse of existing software, CWL increases research reproducibility, as it provides a standardized, formal record of all steps taken in a processing pipeline. Finally, CWL workflows are modular. This means that individual processing steps can easily be swapped in and out.To demonstrate how NLP tools can be combined using nlppln, we show what need to be done to create a pipeline that removes named entities from a directory of text files. This is a common NLP task, that can be used as part of a data anonymization procedure. The SoftwareAn NLP pipeline or workflow is a sequence of natural language processing steps. A 'step' represents a specific NLP task, that is executed by a single tool. Tools require input and produce output. The basic units in CWL are command line tools (i.e., tools that can be run from the command line). In order to be able to run a command line tool, CWL needs a specification. The nlppln software helps creating those specifications. In addition, nlppln provides functionality to convert existing NLP tools written in Python to command line tools. Finally, the software helps users to combine (existing and new) processing steps to pipelines.In the next section, we explain how nlppln facilitates creating NLP steps, and in \"Constructing Pipelines\" we demonstrate the creation of an NLP pipeline for data anonymization. Generating Stepsnlppln allows users to generate CWL specifications for existing NLP tools. To simplify the generation of CWL specifications, we use a convention for NLP tasks. The convention assumes that there can be two types of input parameters: a list of files for which the command should be executed, and/or a file containing metadata about the texts in the corpus. Output parameters consist of a directory where output files are stored (usually there is one output file for every input file) and/or a file in which metadata is stored. So far, almost all steps that are currently available in nlppln follow this convention. Be that as it may, we would like to emphasize that it is possible to deviate from this convention; for example, when existing NLP functionality requires different parameters (e.g., the name of a directory containing the input files instead of a list of input files). This does however mean that the user has to adapt the CWL specification by hand.In addition to CWL specifications, nlppln allows users to generate boilerplate Python command line tools. A boilerplate command line tool contains generic functionality, such as opening input files and saving output files, but lacks implementation of the specific NLP task. The generated Python command can be used to turn existing NLP functionality into command line tools, and to create Python command line tools for new NLP tasks.Python commands and associated CWL steps are generated using a command line tool that requires the user to answer a sequence of yes/no questions. Listing 1 shows what that looks like for a (hypothetical) command 'command', that takes as input a metadata file and multiple input files, and produces as output multiple text files and metadata.Listing 1: Generating a CWL specification and associated boilerplate Python command using nlppln. Constructing PipelinesTo combine text processing steps into a CWL pipeline, nlppln provides an interface that allows users to write a simple Python script. We demonstrate this functionality by creating a pipeline that replaces named entities in a collection of text documents. Named entities are objects in text referred to by proper names, such as persons, organizations, and locations. In the example pipeline, named entities will be replaced with their named entity type (i.e., PER (person), ORG (organization), LOC (location), or UNSP (unspecified)). The pipeline can be used as part of a data anonymization procedure.The pipeline consists of the following steps:1. Extract named entities from text documents using frog (van den Bosch et al, 2007), an existing parser/tagger for Dutch 2. Convert frog output to SAF, a generic representation for text data3 3. Aggregate data about named entities that occur in the text files 4. Replace named entities with their named entity type in the SAF documents 5. Convert SAF documents to text All steps required for this pipeline are available through nlppln. Listing 2 shows the script that creates a CWL workflow for this pipeline. After importing nlppln (line 1), a new WorkflowGenerator object is created (line 3), and the available NLP steps are listed (line 4). Next, the script specifies the workflow inputs (line 6). In this case, there is a single input, which is a directory containing text files. This directory is the input of the first step, which is frog_dir (line 8). The output argument txts contains the internal CWL name of the input parameter (line 6). By assigning its value to the input argument dir_in of frog_dir (line 8), the output is connected to the input. Steps 1 to 5 from the pipeline description correspond to lines 8 to 12 in listing 2. After the remaining steps steps of the workflow are added (lines 9-12), the workflow outputs are specified (line 14). Finally, the workflow is saved to a CWL file (line 16).Listing 2: Python script for constructing the pipeline to replace named entities in text files. ConclusionTo help DH researchers to (re)use and combine existing NLP software, we presented nlppln, an open source Python package for creating flexible and reusable NLP pipelines in CWL. nlppln comes with ready-to-use NLP steps, facilitates creating new steps, and helps combining steps into standardized workflows that are portable across different software and hardware environments. Compared to existing frameworks for creating NLP pipelines, CWL and nlppln add flexibility and improved research reproducibility.nlppln is a work in progress. An important challenge that needs to be addressed is the fact that there is no standard data format for representing text and/or information extracted from text. This means that we will have to add NLP steps that convert different data formats (cf. Eckart de Castilho, 2016)). For future work, we plan to implement additional NLP steps and pipelines, including functionality that targets more languages. We would also like to add visualizations of pipelines and allow users to run pipelines directly from nlppln. ",
        "article_title": "Flexible NLP Pipelines for Digital Humanities Research",
        "authors": [
            {
                "given": "Janneke",
                "family": "Van Der Zwaan",
                "affiliation": [
                    {
                        "original_name": "Netherlands eScience Center",
                        "normalized_name": null,
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Wouter",
                "family": "Smink",
                "affiliation": [
                    {
                        "original_name": "University of Twente",
                        "normalized_name": null,
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Anneke",
                "family": "Sools",
                "affiliation": [
                    {
                        "original_name": "University of Twente",
                        "normalized_name": null,
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Bernard",
                "family": "Veldkamp",
                "affiliation": [
                    {
                        "original_name": "University of Twente",
                        "normalized_name": null,
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Sytske",
                "family": "Wiegersma",
                "affiliation": [
                    {
                        "original_name": "University of Twente",
                        "normalized_name": null,
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Gerben ",
                "family": "Westerhof",
                "affiliation": [
                    {
                        "original_name": "University of Twente",
                        "normalized_name": null,
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The pilot siteKodungallur is a quiet town in Kerala, situated near the mouth of where one of the two arms of the perennial Periyar River empties itself into the Arabian Sea.Known during the colonial period as Cranganore, Kodungallur was earlier famous as capital to the Perumals of Kerala during the formative 9th-12th centuries CE of Kerala history and culture (Narayanan, 1972).Until recently, Kodungallur was considered to be the ancient port of Muziris, described in Greco-Roman texts, such as the 'Periplus of the Erythrean Sea' (Casson, 1989), and referred to as Muchiri, in Tamil Sangam poetry. While recent research points towards Pattanam, a place about 9 km upstream from Kodungallur, as a probable contender for the exact location of the ancient port ( Cherian et al., 2014), even today, the reputation of Kodungallur as the entry point of multiple religions - Christianity, Judaism and Islam, into the Indian sub-continent from across the seas, is proudly cherished by the people of the locality.The Cheraman Perumal Mosque in Kodungallur, probably the earliest in India, its architecture providing testimony to the region's syncretic past (Picture: Own)Among the various temples that dot the town, the one that attracts the most devotees today, almost eponymous with the town, is the Kodungallur Amma temple -dedicated to the Mother Goddess -the Mother of Kodungallur. The Kodungallur Bhagavathy Temple or Sree KurumbaKaavu (Picture: Own)The temple, or 'kaavu' (sacred grove), has, in public perception, a kind of notoriety associated with its annual 'Bharani' festival -gained from the festival's rather peculiar rituals. During the Bharani period, devotees in large numbers throng to the temple from distant areas, clad in vivid red, clanging sickles in their hands, singing profane songs. The culminating 'Kaavu theendal' ritual of the festival is widely commented on and studied. The Kaavu Theendal (Source: Wikipedia)It was this site that the noted local historian, late Dr.N.M. Nampoothiri, former Dean of the Centre for Heritage Studies, Kerala, suggested to this researcher as a pilot. Over the years of 2014/2015, as I started gathering the history and lore related to Kodungallur and the temple from various sources, it was the Bharani rituals that stood out and piqued one's curiosity the most -with its peculiarity, and yet, its unquestionable popularity that drew faithful crowds.'Who' were the people who came for this festival? What drew them? As I went through the articles written about the festival, I saw that theorizations ranged from it having been a practice instituted to drive out Buddhist monks, to the shrine being the memorial of the Silappathikaram protagonist Kannaki. But none of those served to explain the relevance of the festival today -why it continued to thrive and attract people. What also struck one was the contrast between the general perceptions about the festival, and the attitude of those intimately associated with the locality and the festival.In Limories, I have presented the Kodungallur Bharani, as I saw and experienced it, during 2014 and 2015. My emphasis has not so much been on the past, as on the present, as I tried to capture what had fascinated me -the continuities and connections the locale and the festival had nurtured and preserved, across time and place.The presentation -as a photo-narrative A photo-narrative was decided on as the best approach to convey the richness of the festival, chosen after abandoning other text-based approaches. The photo-narrative had the advantage of being more accessible -both to the people I had sourced the content from, and to wider audiences.Most Bharani participants speak Malayalam or Tamil. The crowd is mixed -literate and illiterate. While youngsters are familiar with English, the older people are not.As I tried out various approaches to present my content, I realized that the photographs were the most evocative of all, and that weaving the narrative around those would help bridge literacy and language barriers, reaching out to young and old, local and global. ExcerptsSignboard on the platform of one of the numerous trees that surround the temple, indicating the locality of the group to assemble at this site, (Picture: Own)One of the groups assembled in 2014 (Picture: Own) ValidationOnce the presentation had reached a logical point, in 2016, I returned to the various persons I had obtained information from, for their validation and feedback. I also had independent knowledgeable individuals review the content, to ensure I had captured key terms in the narrative correctly. The snapshots of the Bepur group viewing the site, are below. One of the youngsters of this group had asked me, when I was gathering my content - \"What will be the result of your research? Will we get to see it, or will it sit in some library that we cannot access?\" Although such an in-person validation was possible for the pilot site, as the system scales to multiple locations and contributors, as is the eventual goal, quantifiable and less resource-intensive methods of validation, using computational tools and systems as appropriate, will need to be implemented. Future work Multi-Lingual CaptionsThe photo-narrative approach needs to be explored further, to include multi-lingual capability, especially in the narrative captions that will further increase accessibility. SemanticsWorking out the conceptual categories and a suitable meta-data taxonomy to describe the content, will be crucial to make Limories a 'living' site. The diagram below depicts the overall process flow currently envisaged for the system. Scaling outIn parallel, strategies to scale out to other sites also need to be worked out. A second site has been chosen. The prospect of using local volunteers is also being explored. Social media as a source?As Limories scales out, it 'might', in due course, use appropriate and relevant social media content. However, a primary consideration, if so, would be appropriateness of content. A key intent of Limories is to reach out to all audiences, including children, for the younger generation to know the localities around them, and the associated memories they hold, better. Content would need to be accordingly appropriate.Note: Chart to be read bottom-up Working out the conceptual categories and meta-data, is the next major stage in the Limories journey, as the chart illustrates. This, and scaling out to other sites, with controlled crowd-sourcing, are crucial next steps for Limories, seminal to realizing its vision, to help answer: What is it that makes a place unique and special, to all the people in whose memories the place resides?   ",
        "article_title": "LIMORIES: Expanding Access to Local Histories and Memories with Computational Aids in the Indian Context",
        "authors": [
            {
                "given": "Lakshmi",
                "family": "Valsalakumari",
                "affiliation": [
                    {
                        "original_name": "Centre for Exact Humanities - International Institute of Information Technology Hyderabad (IIIT-Hyderabad)",
                        "normalized_name": null,
                        "country": "India",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionHistorical hand-written manuscripts are an important source of information for our cultural heritage, and automatic processing of these can help exploring their content faster and easier.A special type of hand-written manuscripts that are relatively common in archives and libraries are encrypted, secret documents, so called ciphers. Ciphers may contain and hide important information for the history of science, religion or diplomacy and therefore shall be decrypted and made accessible. The automatic decryption of historical hand-written ciphers is the main focus of the project DECODE: Automatic decoding of historical manuscripts. In order to reveal the content of these secret messages, we collect and digitize ciphertexts and keys from Early Modern times, build a database, and develop software tools for (semi-)automatic decryption by crossdisciplinary research involving computer science, language technology, linguistics and philology.Ciphers use a secret method of writing, often by transposition or substitution of characters, special symbols, already existing alphabets, digits, or a mixture of these. The encoded sequences are usually meticulously written and often segmented character by character to avoid any kind of ambiguity for the receiver to be able to decode the content, but continuous writing of some sequences where the symbols are connected also exists. In addition, the cipher sequences might be embedded in cleartext, i.e. texts in a known natural language, as illustrated in the picture below (Archivio Segreto Vaticano, 2016).The first step for deciphering and making accessible the secret writing is their digitization and transcription. Transcription can be performed either by hand where a person types in the encrypted text symbol by symbol, or by (semi-)automatic means with a possible post editing by manual validation. Manual transcription is time-consuming and expensive, and prone to errors. Automatic methods applied to ease the transcription process are preferable. However, image processing techniques developed so far for historical text manuscripts, such as the ones from the project TransScriptorium, are not fully adequate for dealing with encrypted documents for several reasons. First, the transcribing system cannot benefit from any lexicon or language model because the key is, a priori, unknown. Consequently, the use of an optical model alone is prone to errors, especially when there are ambiguities in the shape of digits/characters. Second, many ciphers contain a mixture of plaintext and encrypted text (ciphertext), which requires specifically adapted handwriting recognition methods. Third, the arcane nature of the symbols used calling for semiotic analysis, which requires the study of techniques closer to hand-drawn symbol recognition rather than handwriting recognition ones.In this paper, we study the feasibility of the current image processing techniques in order to digitize ciphers by recognizing and transferring the symbols into a computer-readable format. For this purpose, we present a semi-automatic transcription method based on Deep Neural Networks, followed by a manual validation. We compare the results with a complete manual transcription, and analyze the human time effort of the two scenarios. Image Processing MethodologyThe handwriting recognition system has the following steps: First, each document has been binarized, deskewed, and the text lines have been segmented using projection profiles. The images of the text lines are the input of the Multi-Dimensional Long Short-Term Memory Blocks Neural Networks (MDLSTMs) ( Graves and Schmidhuber, 2008;Voigtlaender and Doetsch, 2016). Contrary to previous techniques applied for recognition (e.g. HMMs), MD-LSTMs obtain good results without the need of computing feature vectors from the image.For each text line, the output of the network is a sequence of digits. The system also detects when a digit has a dot above or below. Whenever the confidence of the system when transcribing a certain digit is low, the symbol \"?\" is used. This denotes that an expert user must check it.In this work, the networks were trained using 15 cipher pages from six different ciphers (with six different handwritings) in order to learn the handwriting style variability. For validation set, we used 5 cipher pages from the same ciphers as in the training set but different pages. It must be noted that the amount of training pages is not enough for the transcription of cleartext, so all text words appearing in the document are denoted as \"x\". The transcription must be performed by an expert.Finally, and with the aim of improving the visualization of the results and facilitating the posterior validation and correction task, we used force alignment between the result of the neural network and the input image. Manual vs. Automatic Transcription and CorrectionFor the tests, we chose 14 new unseen cipher pages, of which 12 pages were taken from four ciphers in the training set, and two pages came from two new, previously unseen ciphers, which means that these handwriting styles have not been learned during training. In this way, we can also analyze the generalization and scalability degree of the method for new handwriting styles.To compare the speed of automatic versus manual transcription in a fair way, each manuscript was manually transcribed by one person, whereas the output from the automatic transcription was corrected and validated by a different person.In the manual transcription, the transcriber opened the image of the cipher, and transcribed it symbol by symbol in a text file. Contrary, for validation, the transcriber opened the output from the automatic transcription as a picture where the cipher page was segmented line by line and the suggested transcription was reproduced below. As it can be observed in the figures below, the symbol \"?\" appears when the system is not confident on the transcribed digit. Also, if the system detects a dot above the digit, then the transcription also contains the dot.The results obtained by manual transcription and validated transcription from automatic output were compared and shown in Table 1. In average, the automatic system transcribes the digits with an average accuracy of 88 %. However, one of the ciphers (Francia_18_3_233r), written by a writer whose handwriting was not represented in the training set, was more difficult to the system to automatically transcribe and accuracy decreased to 61%. Table 1. Summary of results per line and cipher given the time in minutes for manual transcription, as well as the validation and correction of automatic transcription; the accuracy of automatic transcription, and the average rate of manual transcription and validation per line. Rows in red color denote those where the manual transcription is faster.The results show that in most cases manual transcription is 15% slower on average compared to the automatic transcription with post-editing if the accuracy of the image processing is above 90%. When accuracy is lower, validation time usually increases because the more transcription errors we find, the more effort it takes to localize both the wrong symbol(s) in the picture and in the transcription file. For each error, the user usually starts to read the line from the beginning.It is also noteworthy that we do not count the time it takes to prepare and train the automatic transcription models, including the preprocessing of the images (cut the margins, clean the bleed through, etc.) and the time for training the models. We also noted that the validators would have benefited from a user-friendly transcription tool where transcription suggested by the model was aligned with the original symbol in the picture. However, the automatic transcription clearly helped to differentiate between two different symbols written similarly, thereby helping the user to identify the symbol set represented in the cipher. ConclusionWe have shown that image processing can be used as base for transcription followed by a post-processing step with user validation and correction. Even though image processing techniques need to be trained today on individual handwritings to reach high(er) accuracy, they might be of great help to identify the symbol set represented in the manuscript and to make clear distinctions between symbols, hence can be used as a support tool for the transcriber.In this work, we focused on ciphers without any esoteric or other symbol sets, which might be more difficult for an automatic recognition system. Also, we have identified only cipher sequences; cleartext was only detected without any further transcription.In the future, we would like to test combining image processing and automatic decryption in one step to skip the time-consuming transcription step and create synergy effects as both image processing and automatic decryption tools rely on language models that could be used simultaneously. Another alternative is image processing for validation of the manual transcription, which might be an interesting alternative to investigate in the future.  ",
        "article_title": "Transcription of Encoded Manuscripts with Image Processing Techniques",
        "authors": [
            {
                "given": "Alicia",
                "family": "Fornés",
                "affiliation": [
                    {
                        "original_name": "Universitat Autònoma de Barcelona - UAB Barcelona",
                        "normalized_name": null,
                        "country": "Spain",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Beáta",
                "family": "Megyesi",
                "affiliation": [
                    {
                        "original_name": "Uppsala University",
                        "normalized_name": null,
                        "country": "Sweden",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Joan",
                "family": "Mas",
                "affiliation": [
                    {
                        "original_name": "Universitat Autònoma de Barcelona - UAB Barcelona",
                        "normalized_name": null,
                        "country": "Spain",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Brief summary This paper provides a brief overview of library best practices for digital curation, with particular attention to the areas that highlight disciplinary tensions between library science and the humanities. The authors introduce the University of Victoria's grant service \"menu\" for digital preservation and hosting services, and outline some of the most promising models for balancing creativity with sustainability in DH project design. We will suggest roles for libraries, researchers, administrators, and funders in helping to create technical and social conditions that nurture sustainable research projects in the digital humanities and beyond. Abstract Knowledge building is an iterative process that refines and extends previous research. Through citation, we acknowledge our debt to scholars and theorists whose work enables our own. The",
        "article_title": "Negotiating Sustainability: The Grant Services \"Menu\" at UVic Libraries",
        "authors": [
            {
                "given": "Lisa",
                "family": "Goddard",
                "affiliation": [
                    {
                        "original_name": "University of Victoria",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Christine",
                "family": "Walde",
                "affiliation": [
                    {
                        "original_name": "University of Victoria",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionExisting studies of anglophone digital humanities (DH) curricula have examined course syllabi (Terras 2006; Spiro 2011) and the development of programs at specific institutions (Rockwell 1999;Siemens 2001;Sinclair 2001;Unsworth 2001;Unsworth and Butler 2001;Drucker, Unsworth, and Laue 2002;Sinclair & Gouglas 2002;McCarty 2012;Smith 2014). This study adds to the literature on teaching and learning by presenting a survey of formal degree and certificate programs in anglophone DH.While these programs represent only part of the entire DH curricula, they are important in several respects: First, they reflect intentional groupings of courses, concepts, skills, methods, and techniques, purporting to represent the field in its broadest strokes. Second, these programs include explicit learning outcomes, and their requirements form one picture of what all DHers are expected to know upon graduation. Third, formal programs organize teaching, research, and professional development in the field; they are channels through which material and symbolic capital flow and thereby shape the field itself. Finally, these programs, their requirements, and coursework are one way-perhaps the primary wayin which prospective students encounter the field and make choices about whether to enroll in a DH program and, if so, which one.In addition to helping define the field, a study of DH programs also has the capacity to comment on pedagogical discussions in the literature. Hockey, for example, has long wondered whether programming should be taught in the field (1986) and asks, \"How far can the need for analytical and critical thinking in the humanities be reconciled with the practical orientation of much work in humanities computing?\" (2001). Also skeptical of mere technological skills, Mahony and Pierazzo (2002) argue for teaching methodologies or \"ways of thinking\" in DH, and Clement examines multiliteracies in DH (e.g., critical thinking, commitment, community, and play), which help to push the field beyond \"training\" to a more humanistic pursuit (2012,372). Others have called on DH to engage more fully in critical reflection, especially in relation to technology and the role of the humanities in higher education (Brier 2012, Liu 2012, Walzer 2012).These and other concerns point to longstanding questions about the proper balance of skills and reflection in DH. While a study of DH programs cannot address the value of critical reflection, it can report on its presence (or absence). These findings, together with our more general observations about DH activities, give pause to consider what is represented in, emphasized by, and omitted from the field at its most explicit levels of educational training. MethodologyWe compiled a list of 37 DH programs active in 2015, drawn from public listings (UCLA Center for Digital Humanities 2015; Clement 2015), background literature, and web searches (e.g., \"digital humanities masters\"). In addition to degrees and certificates, we included minors and concentrations in which humanities content was the central focus, and omitted digital arts and media programs in which this was not the case. Because our sources and searches are all English-language, it limits what we can say about global DH.We recorded the URL and basic information (e.g., title, level, location) about each program and looked up descriptions of any required courses in the institution's course catalog. To analyze topics addressed in these programs, we applied the Taxonomy of Digital Research Activities in the Humanities (TaDiRAH 2014), which attempts to capture the \"scholarly primitives\" of the field ( Perkins et al. 2014). TaDiRAH contains forty activities terms organized into eight parent terms ('Capture', 'Creation', 'Enrichment', 'Analysis', 'Interpretation', 'Storage', 'Dissemination', and 'Meta-Activities'). TaDiRAH was chosen for its basis in the literature on \"scholarly primitives\" (Unsworth 2000), as well as three earlier sources (an arts-humanities.net taxonomy, DIRT categories and tags, and a Zotero bibliography) and community feedback and revision.We applied terms to program/course descriptions independently and then tested intercoder agreement, which was extremely low. We attribute this to the many terms in TaDiRAH, complexity of program/course description language, questions of scope (i.e., using a broader or narrower term), and general vagueness. We did find discussing our codings helpful and, in doing so, were able to agree. Accordingly, each of us read and coded every program/course description and discussed them until we reached consensus. Often, this involved pointing to specific language in the descriptions and referencing TaDiRAH definitions or notes from previous meetings when interpretations were discussed. Findings and discussionThe number of DH programs has risen sharply over time, beginning in 1991 and growing steadily by several programs each year since 2008 (see Figure 1).  GeographyMost of the programs studied here were located in the US (22 programs, 60%), followed by Canada (6 programs, 16%), the UK (5 programs, 14%), Ireland (3 programs, 8%), and Australia (1 program, 3%). We note that these programs are all located in Anglophone countries and that TaDiRAH, too, originates from this context, which necessarily limits what we can say about DH programs from a global perspective. StructureLess than half of these DH programs grant degrees: some at the level of bachelor's (8%), most at the level of master's (22%), and some at the doctoral level (8%) (Figure 2). The majority of these programs are certificates, minors, or specializations-certificates being more common at the graduate level and nearly one-third of all programs studied here. We also examined special requirements of these programs, finding that half require some form of independent research (see Figure 3), and half require a final deliverable, referred to variously as a capstone, dissertation, portfolio, or thesis (see Figure 4). About one-quarter of these programs require fieldwork, often an internship (see Figure 5).  Location & disciplinarityMost of these programs are housed in colleges/schools of arts and humanities, but about one-third are outside of traditional schools/departments, in centers, initiatives, and, in one case, jointly with the library (see Figure 6). Most DH concentrations and specializations are located within English departments, commensurate with Kirschenbaum's claim that DH's \"professional apparatus…is probably more rooted in English than any other departmental home\" (2010,55). in this study Elective courses for these programs span myriad departments and disciplines, from humanities departments (art history, classics, history, philosophy, religion, and various languages) to along with computer science, education, information and library science, design, media, and technology. DH activitiesWe analyzed our TaDiRAH codings in two ways: overall term frequency (see Figure 7) and weighted frequency across programs (see Figure 8). To compute weighted frequencies, each of the eight parent terms were given a weight of 1, which was divided equally among the subterms in each area. These subterm weights were summed to show how much of an area is represented, regardless of its size.Analysis and meta-activities (e.g., 'Community building', 'Project management', 'Teaching/Learning') make up the largest share of activities, along with creation (e.g., 'Designing', 'Programming', 'Writing'). It is worth noting that 'Writing' is one of the most frequent terms (11 programs), but this activity certainly occurs elsewhere and is probably undercounted because it was not explicitly mentioned in program descriptions. The same may be true for other activities.   . Digital humanities programs in this study and their required courses (by area)Enrichment and storage terms (e.g., 'Archiving', 'Organizing', 'Preservation') were generally sparse (only 1.9% of all codings), but we suspect these activities do occur in DH programs and courses-in fact, they are assumed in broader activities such as thematic research collections, content management systems, and even dissemination. Generally, there seems to be less emphasis on content ('Capture', 'Enrichment', and 'Storage' terms) and more focus on platforms and tools ('Analysis' and 'Meta-Activities' terms) in the programs studied here-or at least, those may be more marketable to prospective students and institutions, two important audiences of program webpages. Theory and critical reflectionTo analyze theory and critical reflection, we focused our analysis on two terms: 'Theorizing' and 'Meta: GiveOverview', which we used to code theoretical or historical introductions to DH itself. We found that all programs studied here included some mention of theory or historical/systematic overview (see Figure 9). Our codings, of course, do not reveal anything further about the character of this reflection, whether it is the type of critical reflection called for in the literature, or how it interfaces with skills and techniques in these programs.  Further directionsWe plan to publish our data and visualizations publicly for researchers, students, and those developing curriculum: http://bit.ly/DHprograms. We believe it provides a baseline of field growth, areas, structure, and learning experiences, which can be used to measure changes in future, in addition to providing a data-driven perspective on the field today.In that respect, we hope this study gives the community pause to consider how DH is described, represented, and taught. If there are common expectations not reflected here, perhaps we could be more explicit about those, at least in building our taxonomies and describing our formal programs and required courses. Conversely, if there are activities that seem overrepresented here, we might consider why those activities are prized in the field (and which are not) and whether this is the picture of DH we wish to present publicly. ",
        "article_title": "A Survey of Digital Humanities Programs",
        "authors": [
            {
                "given": "Chris",
                "family": "Sula",
                "affiliation": [
                    {
                        "original_name": "The Amistad Research Center",
                        "normalized_name": "Amistad Research Center",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05k3n5g92",
                            "GRID": "grid.446496.d"
                        }
                    },
                    {
                        "original_name": "University of Pittsburgh",
                        "normalized_name": "University of Pittsburgh",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01an3r305",
                            "GRID": "grid.21925.3d"
                        }
                    }
                ]
            },
            {
                "given": "S",
                "family": "Hackney",
                "affiliation": [
                    {
                        "original_name": "The Amistad Research Center",
                        "normalized_name": "Amistad Research Center",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05k3n5g92",
                            "GRID": "grid.446496.d"
                        }
                    },
                    {
                        "original_name": "University of Pittsburgh",
                        "normalized_name": "University of Pittsburgh",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01an3r305",
                            "GRID": "grid.21925.3d"
                        }
                    }
                ]
            },
            {
                "given": "Phillip",
                "family": "Cunningham",
                "affiliation": [
                    {
                        "original_name": "The Amistad Research Center",
                        "normalized_name": "Amistad Research Center",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05k3n5g92",
                            "GRID": "grid.446496.d"
                        }
                    },
                    {
                        "original_name": "University of Pittsburgh",
                        "normalized_name": "University of Pittsburgh",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01an3r305",
                            "GRID": "grid.21925.3d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " ContextThe Oxford English Dictionary (OED) is widely considered to be the greatest philological and lexicographical achievement in English. As the first fascicles of the OED were being prepared, editor James Murray professed his dedication to \"The perfection of the Dictionary in its data\" (1880: 129, orig. emph.). The \"data\" of the work is its 2.43 million quotations, a significant portion of them from poetic and other literary texts, which both shape and illustrate the various sense definitions of roughly 600,000 English words and word forms. Conversely, since its publication, poets have relied on the OED to guide their deployments and arrangements of English words in poems. This reciprocal intertextuality has led to two striking facts which have yet to be fully explored: 1) that the OED's definitions of English words depend to a significant degree on poetic language, which is striking because by any standard account, poetic usage tends away from the denotative or definitional and towards the connotative and metaphorical; and 2) that much English poetry of the last hundred years contains a philological, etymological, and lexicographical dimension, informed by the OED.Although the Second Edition of the OED ( Murray et al., 1989) was among the first large reference works to be prepared for public and academic communities in digitized, marked-up form, and despite the current and ongoing revision of the dictionary (Simpson et. al., 2000-), no version has ever been marked-up with additional metadata. Information we might expect to find in a modern text dataset, such as author gender, genre of quotation, and type of publication, is not included in the OED. Attempts to incorporate such information into studies of linguistic, literary and cultural questions have until now have therefore been limited to \"case-study\" or \"sampling\" methodologies.\"The Life of Words: Poetry and the OED\" addresses this by working directly with legacy versions of the electronic OED, enhancing these with appropriate metadata about the quotation evidence. With the enhanced dataset, alongside modern large text corpora, we then generate quantitative and qualitative assessments in two broad fields of inquiry: 1) What has been the influence of poetry on the English language's most comprehensive lexicographical work? and 2) What influence has the OED had on Englishlanguage poetry? To take a modern turn on Murray, our concern now is the perfection of the dictionary in its data, metadata, and comparative data. Paper OutlineThe preamble to this paper will briefly introduce the project, discussing its background, methods employed, and current stage of development, and offering some observations regarding the use of dictionaries in general, the OED in particular, and specifically the \"opened-up\", marked-up and directly accessible enhanced OED, as \"evidence\" for interpretation in a number of scholarly domains, a methodological topic which has received recent attention in dictionary studies, literary studies, and linguistics (e.g. Coleman 2013a, 2013b; Coleman and Ogilvie Hoffman 2004). The bulk of the presentation will be devoted to an exploration of the enhanced OED, demonstrating some top-level findings relevant to current scholarship in three fields. In the history of lexicography, recently there has been much discussion on the interpretation of OED quotation evidence as a complex indicator of both the prestige of certain kinds of writing over time, and the particular judgements, biases, and practices of the nineteenthcentury philologists who compiled the First Edition   (Ogilvie 2013;Brewer 2012Brewer , 2010Brewer , 2009Brewer , 2007Considine 2009;Mugglestone 2005Mugglestone , 2000Willinsky 1994). In the first main part of the paper, I give an overall quantitative assessment of the generic make-up of OED quotations, comparing the First and Second Editions, and discuss the implications of this for literary and for cultural history. Next, at the intersection of lexicology and literary studies, I offer a re-assessment of claims surrounding the linguistic inventiveness of canonical authors such as Shakespeare and Milton (Goodland 2011(Goodland , 2010Brewer 2013, Crystal 2000Gray 1989;Schafer 1980) based on benchmarks for the period and genre of their various works. Finally, in the realm of literary criticism, I demonstrate a number of ways that information embedded in the OED can be harnessed to detect literary tropes such as allusion and etymological wordplay, either in poems that have been directly influenced by the OED, and those for which the evidence is less conclusive.   ",
        "article_title": "Opening up the Oxford English Dictionary: what an enhanced legacy dataset can tell us about language, lexicography, and literature",
        "authors": [
            {
                "given": "David-Antoine",
                "family": "Williams",
                "affiliation": [
                    {
                        "original_name": "s University",
                        "normalized_name": "Sri Venkateswara University",
                        "country": "India",
                        "identifiers": {
                            "ror": "https://ror.org/05weahn72",
                            "GRID": "grid.412313.6"
                        }
                    },
                    {
                        "original_name": "University of Waterloo",
                        "normalized_name": "University of Waterloo",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01aff2v68",
                            "GRID": "grid.46078.3d"
                        }
                    }
                ]
            },
            {
                "given": "St",
                "family": "Jerome'",
                "affiliation": [
                    {
                        "original_name": "s University",
                        "normalized_name": "Sri Venkateswara University",
                        "country": "India",
                        "identifiers": {
                            "ror": "https://ror.org/05weahn72",
                            "GRID": "grid.412313.6"
                        }
                    },
                    {
                        "original_name": "University of Waterloo",
                        "normalized_name": "University of Waterloo",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01aff2v68",
                            "GRID": "grid.46078.3d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " AbstractThe modeling of mass quantities of cultural objects has led the humanities in new and sometimes discomforting directions. Digital humanists have come to realize the stakes of such practices: emerging paths of scholarship that supplement but may also fundamentally alter the research methodologies and outputs of the humanities. Matthew Wilkens writes that the necessary \"decay\" of critical techniques that pay close attention to those objects is \"a negative in itself only if we mistakenly equate literary and cultural analysis with their current working method\" (Wilkens, 2012). One question that remains is if the quantitative working method of such large analyses is compatible with that \"current working method\" - in this case, the individual interpretation and critique of texts. In the years since Franco Moretti's \"distant reading\" paradigm became a commonplace, scholars have tested this useful if problematic dichotomy of \"close\" and \"distant\" reading. In 2013, for instance, Andrew Piper writes of the \"topology\" resultant from the dispersive techniques of programmatic text analysis. Setting the lexical components of texts in relation to one another, Piper envisions a tactic of \"focalization\" that can allow \"readers\" of such deconstructed texts to understand the relationships between those components (or characteristics) at multiple scales (Piper, 2013: 375). And yet there is always a \"between\" those multiple scales: \"for every unity there can always be something between it and that which it succeeds\" (381). Subsequently, there also exist important relationships between these scales of information that inform one another. When macroanalysis brings us to a point where we must return our close attention to our objects of study, we need be reminded of the model that brought us to that perspective in the first place. In focalizing, we are best served to maintain multiple foci. It is a natural tendency to want to confirm macroanalytic results by reading texts and by paying attention to the details of our mathematical models. But how might we do so while keeping the complex relationships of those models in mind? Figure 1. \"Topic Words in Context\": an in-browser tool for exploring the scales of data in a topic modelWhen digital humanists use topic models to explore large corpora of texts, they do so at an inherent disadvantage. Typically presented with flat files listing topics and topic weights, they are left to interpret these lists and figures separate from the texts that have just been modeled. Several significant tools have been developed to help scholars visually navigate the textual relationships in topic models. However, in the past few years I have been working on a practical, critical methodology for understanding topic models and the relations between their outputs and that \"current working method\" of the humanities: human-guided close and contextual reading. For this talk, I will take attendees on a visual exploration of a topic model of Emily Dickinson's poetry using a highly interactive and playful data visualization I developed for my Master's thesis, called \"Topic Words in Context\" -or \"TWiC\", for short. TWiC is a multi-paneled environment for web browsers that allows users to explore and juxtapose multiple scales of data in topic models of digital corpora. It uses shapes, colors, and cross-panel highlighting to get users of topic models from \"big\" data to \"small\" and back. Importantly, it also provides an alternate \"publication\" view that resituates modeled texts back into their original publication contexts (e.g. texts split for modeling purposes or texts within a collection). Recalling Piper's topological concepts, TWiC brings our focus simultaneously to these many textual and statistical relationships at play within a topic model. From corpus-wide topic distributions to texts to the topics themselves, each scale of the model when set against each other can reveal hierarchical qualities that enrich and move beyond the semantic/linguistic relationships frequently associated with the word lists of topic models. (Documentation and color screenshots of TWiC are available at in the README.md file at github.com/jarmoza/twic.) Of the many analytical techniques TWiC makes possible, I will demonstrate how we can produce expressive, critical comparisons between our close readings of texts and the smallest of quantitative scales in a topic model: individual texts and individual topics. We will look at different weighting schemes for topic and topic word distributions, how to quantitatively characterize and visualize them, and then how to compare them to traditional literary criticism. As it turns out, the expressiveness of a topic model functions differently depending on the context in which we depict its data. To show this I will turn our attention to the literarybibliographic focus of my Master's thesis, Emily Dickinson's \"fascicle\" books of poetry. Emily Dickinson died in 1886, leaving behind in a small, wooden box an unpublished trove of poetry numbering near 2,000 individual works. Many of those poems were bound, hand-sewn into tiny books that have come to be known as her \"fascicles.\" While her poems were being prepared to be published, a family feud arose that split the collection of her manuscripts, and also resulted in the fascicle ordering of her poems being lost for years. A long-studied, now-canonical poet, Dickinson is considered a proto-modernist, someone whose style influenced many of the American poets of the early twentieth century. However, given the size of the task, a comprehensive assessment of every piece of her writing has rarely been attempted. It would not be until the midtwentieth century that the painstaking effort to rediscover those original orderings was made by bibliographers, notably R.W. Franklin. With that work completed, Dickinson scholars like Eleanor Elson Heginbotham, Sharon Cameron, and others have provided assessments of a select few of these orderings using all of the critical tools at their disposal honed by years of reading Dickinson: interpretations that pay attention to style, context, biography, history, textual studies, and more. Even so it just may not be humanly possible to provide a comprehensive perspective of her writing via such individual attentiveness. Dickinson's poems and their bibliographic history therefore present a fortuitous and somewhat unique set of circumstances for digital humanists. Her oeuvre as a poet is large enough in size to be mathematically modeled. There is a known ordering to much of her works. And her words are \"truth told slant\" enough in their polysemy to problematize a topic model's expectations of the relationships between them - even at the level of the individual line, let alone across several works. While digital humanists still want to closely consider our objects of study away from computation, we also want to consider them from these new perspectives that digital modeling methods provide. We tend to bounce between considerations of modelinduced order and the contextualizing work done by human-imposed orderings. This talk will provide a case study of how those differing worlds of order relate, how they sometimes either complement or contrast with one another. By combining the outputs of a topic model with the context of Dickinson's fascicle orderings, for instance, one can make quick comparisons between the qualitative discursive relationships of her poems and the quantitative relationships established by a topic model of them. I will introduce TWiC and its publication view to attendees as a means of exploring topic models, showing how it can be used to facilitate close readings of texts that focus on model data as well as on prior humanities criticism of those texts. This exploration will take us from the patterned, colorful shapes of TWiC to several types of analytic visuals that unweave the probabilistic threads of a topic model. By the conclusion, we will be able to proportionally compare the interpretations of a Dickinson critic and a topic model of Dickinson's poems as they are situated in the fascicle books. ",
        "article_title": "How to Close Read a Topic Model: TWiC Reads Emily Dickinson's Fascicles",
        "authors": [
            {
                "given": "Jonathan",
                "family": "Armoza",
                "affiliation": [
                    {
                        "original_name": "New York University",
                        "normalized_name": "New York University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0190ak572",
                            "GRID": "grid.137628.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " ReparativeLivingstone Online, now in its twelfth year (2004 present), is a digital museum and library that draws on recent scholarship and international collaboration to restore one of the British Empire's most iconic figures to his global contexts. Our digital collection of highresolution manuscript images and critically-edited transcriptions - 11,000 images and 700 transcriptions by 2017 - is among the largest on the internet related to any single historical British visitor to Africa. Our site publishes important research on Livingstone's legacy and explores the many ways his ideas have circulated over time. Uniquely, we also takes its visitors far behind the scenes of our work - documenting step-bystep the international collaboration among archives, scholars, scientists, librarians, computer programmers, and other specialists that has made our project possible.Our use of spectral imaging to uncover the material history of Livingstone's manuscripts gives us important insights into the conditions under which Livingstone and other imperial explorers wrote - from unacknowledged contributors to the many environments through which the manuscripts circulated. In foregrounding these dimensions, we are also creating a new approach to using spectral imaging in cultural heritage projects because spectral imaging has primarily been used to unearth layers of text, rather than to examine the broader circumstances of imperial record-making and the preservation of imperial records over time. This new use of spectral imaging also constitutes an ethics of access, which is framed by critical essays that explore Livingstone's uncredited information sources. Livingstone Online here puts forward an idea of access as uncovering the hidden hands and voices of the past.For instance, in our study of Livingstone's 1871 Field Diary, we have collaborated with spectral imaging scientists to develop pseudocolor (false color) images to differentiate passages that Livingstone originally wrote from those he added latter and those added by other hands. Likewise, the development of animated spectral images has enabled a chronological reconstruction of events in the life of the diary. By contrast, recourse to images made by principal component analysis (PCA) has uncovered stains on pages otherwise invisible to the naked eye and has introduced us to dimensions of manuscript history otherwise not even suspected to exist.In addition, we frame these spectral images with paratextual tools that value equally the different kinds of information that Livingstone records. For example, few or no other record remains of many of the villages or the African and Arab individuals Livingstone mentions. As a result, our integrated glossary offers unique, otherwise unavailable geographical information that circulated during Livingstone's time in Central Africa and enumerates the names of people that might otherwise be lost to history. The glossary and other critical materials also provide insight into the complex social dynamics that operated in areas where Livingstone traveled. Overall, Livingstone Online offers a version of access in which the freely available manuscript pages are only a starting point; spectral imaging technology combined with critical building helps construct a reparative ethos of access.  Future-OrientedAlongside such reparative work, we also strive to design a project that looks toward the future. Livingstone Online makes our project documents available to an almost unprecedented degree in order to make our publicly-funded research fully accountable, to illuminate our work practices, and to support future digital projects. Our extensive downloadable primary materials (including 12,000 images, 3000 metadata records, and hundreds of transcriptions) are supplemented by freely available project materials, images, and working documentsthe things often hidden behind the public face of digital projects. We have curated access to over 600 project documents, including planning documents, spectral image processing information, and essay notes, in order to illuminate the long-term history of our project work. Likewise, access to our grant narratives and working documents de-mystifies funding processes and international, interdisciplinary collaboration in order to support the work of other scholars, especially junior or independent scholars and those new to DH.In addition, our site is technically accessible in a range of ways. We have built the site with sustainable, community-supported, open-source technologies such as Drupal for our front end and Fedora for our back end, which means that others can access our underlying code (which is fully available from Github) to reuse and modify it for their own projects. To promote use of our site more broadly, we've also worked to make the site inviting to scholars and general users alike, using intuitive, visually-driven site design. Through our site design, open-source code, and transparent documentation, we hope to foster user-led interpretation over passive reception of authorized knowledge. Figures 2,3. Livingstone Online's six section pages, two of which are pictured here, each rely on a diverse range of historical illustrations and contemporary images to complicate the notion of a definitive Livingstone.As part of this effort, our site is also fully mobile accessible, including for complex functions such as the review and study of archival manuscripts and transcriptions. This opens up Livingstone's documents and our critical materials to the parts of the world where he worked and travelled; for many people on the African continent, for instance, mobile technology is the main access point to the internet. Likewise, just under a quarter of our collaborating archives are in Africa, and we are actively working on developing additional relationships with African-based archives, in the interests of not only bringing new Livingstone materials into our site, but also to encourage collaboration with African-based scholars and general audiences.In these ways, we hope to initiate a conversation about the biases and assumptions inherent in the ways that technological advances shape our preservation of the past. We also hope to develop a nuanced practice of access that is embedded in our site design, spectral imaging processing, and transparent documentation, as well as made explicit in our critical materials. Thinking of access beyond openness means creating more historically-minded digital collections that also look to future knowledge creation by an array of populations, not all of them academic or based in the west. ",
        "article_title": "Livingstone Online: Access Beyond Openness",
        "authors": [
            {
                "given": "Megan",
                "family": "Ward",
                "affiliation": [
                    {
                        "original_name": "Oregon State University",
                        "normalized_name": "Oregon State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00ysfqy60",
                            "GRID": "grid.4391.f"
                        }
                    }
                ]
            },
            {
                "given": "Adrian",
                "family": "Wisnicki",
                "affiliation": [
                    {
                        "original_name": "University of Nebraska - Lincoln",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn 1624, Henry Wotton highlighted the importance of \"the Properties of a well-chosen Prospect\", a concept of viewing landscapes, which Wotton dubbed \"the Royaltie of Sight\" (Wotton, 1624, p.14). Sixteenth- and seventeenth-century English designed landscapes were artificially organised to optimise the 'prospect' whilst reflecting the landowners' individual perspectives. However, little analysis has been attempted into determining the extent and characteristics of 'prospect' and 'perspective' at specific sites. The destruction and modernisation of estate landscapes has hindered their analysis and reshaped perceptions of their appearance and development. In this paper, I will therefore demonstrate how 3D-GIS has the capabilities to change this. For my ongoing PhD research, I am introducing 3D-GIS as a new computational tool for producing 3D representations of estate landscapes but also to subsequently provide the geographical and historical context to analyse the visual experience within them. This methodology will contribute to not only the study of designed landscapes but of historic landscapes generally. Literature ReviewIn the study of designed landscapes, country houses, gardens, parks, and the wider estate landscape have been individually and collectively explored and analysed. This is evident from the literature produced in disciplines such as literary history, art history, architectural history, garden history, archaeology, geography, and landscape history. Whilst each approach has its own merits, a noticeable lack of collaboration and acknowledgement of alternative methods has created a \"disciplinary vacuum\" (Spooner, 2010, p.7). In my research, landscape history is the leading discipline, but it also embraces the contributions of the other disciplines, thus creating an altogether multidisciplinary approach.Our current understanding is predominantly based on research into well-documented and surviving estates. Consequently, there is a bias against sites which no longer survive or are lacking in evidence. Additionally, they are analysed separately from the wider landscape, which is frequently overlooked despite its significant bearing on the development and utilisation of designed landscapes. Although its contributions have been mentioned, the wider landscape has not been examined in detailed concordance with designed landscapes (Dix, 2011, p.152).As a result, little research exists into how individual estates were experienced within their landscape context. Whilst the history of the senses, experiences and attitudes towards the landscape have been researched (Rippon, 2012;Hamilakis, 2013;Whyte, 2015), the examination and application of these concepts to historic landscapes, including designed landscapes, is minimal. Therefore, a greater understanding of contemporaries' perceptions and experiences of designed landscapes can be reached by ascertaining how such notions were applied to individual estates.In this paper, these issues will be addressed and a solution provided. This project utilises computer science as a conduit to address the many disciplines already featuring in the study of designed landscapes. Geographical Information Systems (GIS) is becoming increasingly popular in the spatial humanities, particularly historical and landscape studies (Gregory and Geddes, 2014;Knowles (ed.), 2002), but few have used GIS to analyse individual designed landscapes (Dalton, 2012) and 'prospects' within them (Spooner, 2009). Furthermore, 2D-GIS is typically used, which does not provide a suitable perspective for comprehending the intended experience within these landscapes. On the other hand, Computer Aided Design (CAD) can digitally restore lost landscapes (Virtual Past, n.d.), including designed landscapes (Urmston and Historic England, n.d.), thus providing equal opportunity for their examination alongside surviving specimens. However, the essential difference between GIS and CAD is the handling of spatial data, and CAD is unable to conduct further spatial analysis (Abdul-Rahman and Pilouk, 2007, pp.4-5). Therefore, by incorporating CAD into a GIS environment, 3D-GIS can provide a more immersive perspective for the analyst to explore and analyse these landscapes more appropriately and effectively. 3D-GIS is, therefore, a promising approach for the study of historic landscapes. Regarding designed landscapes, the reconstructive capabilities of 3D-GIS have been demonstrated (de Boer et al., 2011). However, 3D-GIS has only more recently been trialled for the analysis of individual estates. This project builds upon my previous work, looking specifically at the 'prospect' with the use of 'viewshed' analysis (Stewart, 2015). By literally adding a new dimension, 3D-GIS has great potential to improve our understanding of designed landscapes. Case StudyIn this paper, I will present a case study which demonstrates the potential of 3D-GIS. My work adopts a regional approach, focusing on Norfolk, Suffolk and Essex. I will present work from one of these geographical areas. The combination of the aforementioned disciplines will provide the foundations upon which the recreations within CAD and GIS can be supported. This will involve the utilisation of polygons and imported CAD models, based on data extracted from geo-referenced contemporary maps, architectural plans, earthwork plans, fieldwork and archaeological surveys, alongside a range of qualitative historical documents, including estate accounts, inventories and contracts.Once digitally recreated in 3D-GIS, the estate along with its landscape context can then be subjected to further spatial analysis. For the purposes of this study, the visibility of a 'prospect' can be ascertained using 'viewshed' analysis. To calculate this, topographical data produced by LIDAR is required, with amendments to replicate the historic landscape recreated in 3D-GIS. Viewshed analyses are then conducted from predetermined vantage points. Landowners ensured that 'prospects' could be enjoyed from certain places within their estates, such as the piano nobile of the country house, the rooftop, and from structures and landscape features within the grounds of the estate. Coupled with animations, whereby the act of movement throughout the landscape can be generated, the 'prospects' that contemporaries experienced can be recreated from these positions within the landscape.Finally, the results can be interpreted in light of the evidence analysed using 'reception theory'; the creativity and reactivity of individuals in response to certain works (McGregor and White, 1990, p.1). This will focus on each individual landowner's possessions, such as published texts or artwork, but also personal correspondence, including letters and diaries. This research will utilise reception theory, which has not yet fully implemented into the study of designed landscapes (Hunt, 2013, p.7), in order to ascertain contemporary 'perspectives' towards these landscapes through the 'prospects' experienced within them. I will present the findings from this case study. This will demonstrate not only the synthesis of the approaches and resources addressed, but will provide the best opportunity to analyse the visual experience and thus improve our understanding of sixteenth- and seventeenth-century designed landscapes. ConclusionThe potential of 3D-GIS to rekindle the analysis of sixteenth-and seventeenth-century designed landscapes will be evident. The results from this case study, as part of a larger study, will help to explore and examine what these landowners' visual perceptions were. Subsequently, our understanding will improve regarding how individual landscape designs have been influenced by their landowners' thoughts and ideas, and the extent of their individuality or conformity to contemporary fashions in landscape design and appreciation. My project will progress research into designed landscapes whilst demonstrating the methodological benefits of 3D-GIS. As a virtual environment, analytical tool, and database, this approach can subsequently contribute to studies in landscape conservation, heritage management and outreach activities, with scope to benefit research within other areas of the spatial and digital humanities.   ",
        "article_title": "'The Royaltie of Sight': A 3D-GIS recreation of 'prospects' and 'perspectives' within an English designed landscape, c.1550-1660",
        "authors": [
            {
                "given": "Elizabeth",
                "family": "Eleanor",
                "affiliation": [
                    {
                        "original_name": "Univeristy of East Anglia",
                        "normalized_name": "University of East Anglia",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/026k5mg93",
                            "GRID": "grid.8273.e"
                        }
                    }
                ]
            },
            {
                "given": "Rose",
                "family": "Stewart",
                "affiliation": [
                    {
                        "original_name": "Univeristy of East Anglia",
                        "normalized_name": "University of East Anglia",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/026k5mg93",
                            "GRID": "grid.8273.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis paper discusses the importance of physical spaces, as a source of context, with relation to electronic resources. The presented arguments are based on a geo-located mobile application (due for launch in November 2016) developed for the Letters of 1916 project that allows users to see letters from the collection near them. The application is available for the Android platform and allows users to read letters from the Letters of 1916 based on the device's geolocation providing a different way to engage with the resource. While the abstract considers a wider perspective, the presented paper will focus more on the implementation of the application and the means of access it provides. ContextThe Letters of 1916 is Ireland's first public engagement, digital humanities project. It collects, digitises, transcribes, encodes, and makes available through its electronic platform epistolary documents written about Ireland between 1 November 1915 and 31 October 1916. The goal of Letters of 1916 is to reassess the narrative of this period through the collection of letters (including postcards and telegrams) with a tangible link to Ireland. The year 1916 was one of transition for Ireland: between its involvement in the Great War and the rise of militant nationalism, the country was divided by sentiment, separated by ideals. The rhetoric of the letter presents personal perspectives and individual memory traces; the collected letters provide an insight into those fragmentary stories that, inspected together, constitute a collective consciousness.While the project uses its Explore database to provide a traditional interface (including search and browse functions as well as the ability to view the letters sorted by location, topic, institution, and author's gender, etc.) to engage with the collection, the mobile application is limited to discovery through geo-location. The motivation behind such an application is to provide a different way to engage with the resource through a dynamic-content interface. FrameworkAs a project born out of humanities research, it is important to ground the mobile application within a framework that provides clarity about its function and design. The application addresses questions of access and investigates the following three key areas:Re-inserting space Archival objects are decontextualised artefacts that are removed from their place of origin and collected within resources outside their purpose and provenance. The importance of providing geographical positions to objects within collections, both as means of providing access and as a method of context-attribution, has been discussed broadly (Harekama 2012;Cusworth et. al. 2015). Mapping objects to digital maps is a common practice in collection building and may be seen in projects like Mapping the Republic of Letters or Photogrammar, for instance. The objects in the Letters of 1916 collection are written between individuals within a historic context and a spatial context. Letters are spatiallyaware objects - written from a place and to a place; their understanding is deeply grounded within the geographical spaces they occupy. It is, then, obvious that their experience should be enhanced by providing them with geo-locations. The mobile application presents these letters within the spaces they are created or written to; the experience of reading a letter standing on the same street as it was composed is different to reading it on a screen detached from its physical space. SerendipityCollections are only as good as the access they provide. Most digital collections provide access through text-searches, which, at some level, always assume that the user knows what she is looking for. Even with faceted searches, the sense of discovery is limited by a prior knowledge of the collection. A geolocated retrieval of content is not mediated by textual input but through the user's own existence in physical space. That tangible sense of discovery, or rather, that serendipitous moment of coming across a piece of history is mediated, not by some prior knowledge of the collection, but as a consequence of the user's location in the world. Aura, magic, and historyStanley Cavell (1979), while discussing photographs, tries to find the relationship between the subject and the image of the subject. His exploration attempts to address that connection between the physical object and its representation; he explores ideas of aura (Benjamin 1969), magic (Barthes 1977), and history (Tagg 1988) to think about the experience of seeing the physical object and the impression of it; while he identifies that there is a differences, he struggles to describe quite what that is. Building on this line of inquiry, I would like to argue that reading a letter within the space it is conceived or received provides a mode of perception that cannot be found if it is read outside of it. Whether we choose to call that aura, magic, or history, the importance of connecting the object to its physical space lends a multi-sensory engagement that is lacking in other forms of digital interfaces. ImplementationThis mobile application is designed with input from two rounds of internal user testing and feedback in 2016. A focus-group is planned for summer 2017 to further test and enhance the user experience from both software and conceptual perspectives.The Letters of 1916 has multiple moving parts and the application is designed and conceived within that existing framework. The project invites the public to transcribe and encode images of the collected letters. These machine-readable documents are then put through a rigorous editing phase which create valid TEI (Text Encoding Initiative) files. The sender's and receiver's addresses (that are present in the letters) are given unique identifiers and, consequently, geocoordinates. Specific fields(title of the letter, sender's location, and link to the letter) found within the edited TEI documents are extracted and converted into a JSON object. This is the data structure and format that the application uses to dynamically pull information. The mobile application is built using the Adobe PhoneGap framework (for the Android platform). The application reads the user's device geolocation; it uses the haversine formula to calculate the distance between the user and the letters. The application calculates the distance between two points on the earth's surface and measures the distance between them in metres. This distance orders the letters from the least to the greatest (see Fig. 1). It is then possible to display the letters closest to the device's geolocation. Clicking each object opens an in-app browser that makes the text of the letter available in a mobile-friendly interface (see Fig. 2).  ConclusionsThe presented mobile application takes electronic resources outside the paradigm of the desktop computer and provides a meaningful way of engaging with the resource. The presented paper will consider how this application is built and the means through which it makes resources accessible. It will also consider the drawbacks and limitations of such a system opening up to discussion the use of mobile technology in the experience of cultural resources.",
        "article_title": "Serendipitous Moments: Building a Geo-located Mobile Application for the Letters of 1916",
        "authors": [
            {
                "given": "Vinayak",
                "family": "Das Gupta",
                "affiliation": [
                    {
                        "original_name": "Maynooth University",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The project not only makes a muchneglected part of Canadian history available for mainstream scholarly use, it also provides a foundation for modeling identity and representing time and space in TEI. There is much to model spatially and temporally. In the 1960s, the Toronto's gay scene was located on Yonge Street and by the 1970s the bars had migrated north along Yonge from Queen Street towards Wellesley Street. Two of the most famous bars on this strip were the Parkside at 530 Yonge Street and the St. Charles Tavern at 488 Yonge Street, both owned byNorman Bolter. The neighbourhood surrounding the former sites of the Parkside and the St. Charles is currently a site of massive development. Gone are the seedy bars and massage parlours. Large glass condos are rising from bulldozed lots and from the tops of heritage buildings.The building that housed the St. Charles Tavern still stands above Yonge Street at Wellesley. It is slated to become a condo in 2017, the iconic clock tower that served for decades as a meeting spot for the gay community will be preserved at the base of a proposed 153 metre glass spire. The incorporation of this piece of gay history into a shining skyscraper only furthers serves the ideology that scapegoats the gay community as the root cause of gentrification, making the million dollar condos imagined by a developer seem like an inevitability, rather than just one potential future amongst many. What is lost in this new instance of facadism is the history of the St. Charles Tavern, a violent history neither the city nor developers seem keen to acknowledge or commemorate.For example, tracing the events outside the St. Charles Tavern on Halloween from 1968 to 1981 reveals Toronto's violently homophobic history, and the inadequate policy and police response to that violence. In the 1960s it was illegal for men to wear clothing of the opposite sex in Toronto, a rule that was relaxed at Halloween. Every Halloween St. Charles Tavern hosted a drag show, and every year a homophobic mob congregated outside the bar to throw eggs and rotten vegetables as the queens arrived. In 1968, police found gasoline bombs behind the tavern, and yet in the following years police still permitted crowds to hurl insults shout slogans such as \"kill the queers\", and throw debris at the patrons. It took a decade for the police to start arresting people in the mob; in the meantime a volunteer gay defense patrol, Operation Jack O'Lantern, escorted patrons through the neighbourhood and in through the back door of the tavern. Between 1975 and1978, 14 patrons of Toronto's gay bar scene were murdered. Eight of those murders went unsolved amid accusations of police homophobia, which were only exacerbated by homophobic articles published in the Toronto Police Association Journal. Publishing this history online, not only in text, but in map form, will make it accessible to an audience that will include not just scholars, but the citizens of Toronto. By developing accompanying pedagogical tools to guide users through the map, we hope to engage local residents, as well as the thousands of students at nearby universities and schools, in working to reclaim that lost history, and undoing some of the effects of the gentrification of the mind.  ",
        "article_title": "Beyond the Historic Facade: Skyscrapers, Scapegoats, and the Digital Reclamation of Toronto's Queer Streetscapes",
        "authors": [
            {
                "given": "Constance",
                "family": "Crompton",
                "affiliation": [
                    {
                        "original_name": "University of British Columbia",
                        "normalized_name": "University of British Columbia",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03rmrcq20",
                            "GRID": "grid.17091.3e"
                        }
                    }
                ]
            },
            {
                "given": "Michelle",
                "family": "Schwartz",
                "affiliation": [
                    {
                        "original_name": "Ryerson University",
                        "normalized_name": "Ryerson University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/05g13zd79",
                            "GRID": "grid.68312.3e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Roles and Capabilities --successful digitalhumanities work is most often conducted in teams, with roles and capabilities from three complementary categories: technical experts, champions of engagement, and content innovators. Within these broad categories, there is great deal of overlap and interdependence; and it is through the overlap of roles and interdependence of capabilities that DH can flourish. Depending on the organizational model in place, projects may draw upon skilled collaborators at various places within an institution (e.g. museums and archives, central IT units) or even at other institutions through consortial agreements. 4. Communication and Outreach --in the early stage of institutional support for digital humanities, individual practitioners are not aware of one another, and there are no established channels for disseminating news and announcements about events, workshops, grants, and other opportunities relevant to digital humanities. As institutional support becomes established, digital humanities mailing lists and event calendars are created, funding is accessible for one-off events and activities, and beginner-oriented training becomes available. High capacity for communications and outreach involves coordinated, regular communication, dedicated funding for activities, and multiple levels of training covering a range of skills. 5.Acceptance --i.e. the acceptance of digital humanities work as a component of promotion and tenure, of course assessment, and of performance reviews for librarians and IT staff. The white paper notes that academic acceptance of digital humanities work is significantly influenced by developments within particular disciplines, for example, the development of guidelines such as those produced by the Modern Language Association (2012), American Historical Association (2015), and College Art Association (2016) for evaluating digital humanities scholarship. Nonetheless, an institution can increase its capacity at the local level by having department chairs, deans and provosts publicly take a position supporting the assessment of digital scholarship as part of tenure dossiers and advocate for the consistent application of disciplinary guidelines, where available. Acceptance of digital humanities also applies to the evaluation of courses with digital humanities components, as well as to performance evaluations for librarians and staff who support digital humanities work. In the early stage, librarians and IT staff provide digital humanities support \"on the margins\" of their jobs, whereas at a higher-capacity stage, digital humanities support is an officially recognized aspect of IT staff and librarians' job descriptions, and is factored into performance evaluations accordingly. To complement the capacity building framework, the white paper includes a section on getting started with developing institutional capacity. This section has pointers for how to do an environmental scan and needs assessment, a discussion of interdisciplinarity, recommendations for the kinds of partnerships that support institutional capacity-building, and a number of commonly-used organizational models. ",
        "article_title": "A Capacity Building Framework for Institutional Digital Humanities Support",
        "authors": [
            {
                "given": "Quinn",
                "family": "Dombrowski",
                "affiliation": [
                    {
                        "original_name": "Coalition for Networked Information United States of America",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    },
                    {
                        "original_name": "UC Berkeley",
                        "normalized_name": "University of California, Berkeley",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01an7q238",
                            "GRID": "grid.47840.3f"
                        }
                    }
                ]
            },
            {
                "given": "Joan",
                "family": "Lippincott",
                "affiliation": [
                    {
                        "original_name": "University of Virginia",
                        "normalized_name": "University of Virginia",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0153tk833",
                            "GRID": "grid.27755.32"
                        }
                    },
                    {
                        "original_name": "University of Victoria",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    },
                    {
                        "original_name": "University of Maryland",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Qiwei Li liqiwei2000@gmail.com Rice University, United States of AmericaWe aim to demonstrate a new methodology for detecting shifting nuances of ideas in Chinese intellectual history. Our subject is Chen Duxiu (1879- 1942), founder of the Chinese Communist Party (CCP) and one of the most important historical figures of twentieth century China. We combine the latest textmining tools with statistical analysis and natural language law, based on word frequency calculations. Because the Chinese language does not have spaces between words, and because each word is comprised of either a single or multiple characters or morphemes, segmentation of Chinese text poses a set of different challenges than for English corpus. By using a Chinese tokenization plug-in for R called JiebaR, we have developed a more precise method to create a Chinese natural language curve that conforms closely to Zipf's law, a commonly used model for distribution of words in a corpus. Zipf's law states that given a body of natural language text, the frequency of any word is inversely proportional to its rank. ( Ha et al., 2003) For Chinese language, Zipf's law applies for words made up of multiple morphemes. (Xiao, 2008) Our assumption is that a word is significant when its position on our curve deviates from the fitted curve based on Zipf's law. Departing from an existing method developed by Prof. Jin's team, we created two groups of keywords, and called them \"anchor\" and \"companion\" words. ( Jin et al., 2014) \"Anchor\" words have large residuals (high deviation from the standard curve), and \"companion\" words have a high correlation with \"anchor\" words. We used the formula for Pearson's correlation coefficient to find the companion words. The coefficient has a value of between +1 and -1. The companion words provide the context with which to interpret the anchor words. Unlike Prof. Jin's team, we included keywords made up of more than two characters, such as the three character word for Nationalist party, Guomindang (國 民黨), the four character word for citizen's assembly, guomin huiyi(國民會議), and the five character word for Marxism, makesizhuyi (馬克思主義).The goal of our research is to analyze changing meaning of the concept of patriotism in Chen's writing from the beginning of his publishing career, ca. 1897, to the end of his life, 1942. Why is Chen such a fascinating figure to study? In addition to creating a political party that changed the course of Chinese history, Chen was equally, if not more, influential in bringing about the first cultural revolution of twentieth century China. Living at a time of great political unrest, Chen wrote passionately on the need to reform the people by revolutionizing Chinese culture, thoughts, and politics. After founding the CCP in 1920-21, Chen was expelled from the party in 1929 due to ideological differences. He was subsequently jailed by Chiang Kai-shek's Nationalist Party, and died a political pariah a few years later. As his political and personal fortune waxed and waned, Chen's conception of patriotism shifted. For this paper, we chose six important anchor words: citizen (國民), youth (青年), democracy (民主), revolution (革命), people (民族) and being patriotic (愛國). Our method is replicable for other corpus, with the understanding that the conclusion is derived with the additional layer of human deliberation.  ",
        "article_title": "A New and Improved Method to Text-Mining in Chinese: Closer Language Segmentation in Detecting the Shifting Meaning of Patriotism",
        "authors": [
            {
                "given": "Annie",
                "family": "Chao",
                "affiliation": [
                    {
                        "original_name": "Rice University",
                        "normalized_name": "Rice University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/008zs3103",
                            "GRID": "grid.21940.3e"
                        }
                    }
                ]
            },
            {
                "given": "Qiwei",
                "family": "Li",
                "affiliation": [
                    {
                        "original_name": "Rice University",
                        "normalized_name": "Rice University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/008zs3103",
                            "GRID": "grid.21940.3e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Introductionthat are passed using the HTTP request (e.g. embedded in the URL or as URL-parameters). Recently, the International Image Interoperability Framework (IIIF) emerged as a standard syntax to pass parameters such as image size in pixels, cropping region, rotation angle and file format using a well-defined URL syntax. The IIIF-standard allows to share image resources between web-applications such as Virtual Research Environments (VRE) in a standardized way. There are quite a few IIIF-compliant image servers around, some using a native implementation, some wrapping image transformation programs using a script language such as python.By a mandate of the State Secretariat for Education, Research and Innovation (SERI), the Swiss Academy of Humanities and Social Sciences (SAHSS) has created a new institution for the preservation and long-term curation of research data in the humanities (Data and Service Center for the Humanities, DaSCH). It ensures permanent access to research data in order to make it available for further research. A pilot program started in 2013 and has been successfully finished. The DaSCH will be permanently installed on January 1st 2017, to guarantee seamless services. The Digital Humanities Lab (DHLab) of the University of Basel has been mandated with the operation of the new institution. For this purposed, DHLab has developed a flexible platform based on semantic web technologies (RDF, RDFS, OWL). Besides text sources, about 500,000 high-resolution images have been ingested to the system during the pilot phase. We decided to use IIIF for presenting the images in order to maximize the interoperability with external systems. Furthermore, we need to preserve only one image file, since IIIF allows using the archiving master also for dissemination and presentation. However, none of the existing IIIF-compliant servers satisfied our needs. Design RequirementsTherefore, we decided to design and implement our own IIIF server guided by the following requirements:• Interoperability with internal databases (e.g. RDF-triplestores, RDBMS etc.) containing annotations, metadata etc. as well as access permissions.• Preservation of all embedded metadata (e.g. EXIF, XMP, TIFF etc.) during all format conversions• ICC color profile conversions where necessary• User authentication compatible with the proposed draft of the IIIF for authentication • High-performance transformation of images including rotation, format conversions (e.g. 16 bit to 8 bit depth) etc.• Support of Secure Socket Layer (SSL/https) • A configurable image cache in order to reduce the computational load on the server • Support of cross origin resource sharing (CORS) • Import and transformation of images. The server must be able to import images and convert them to the desired master file format (in our case JPEG2000).• Features beyond the scope of the IIIF-standard such as adding watermarks, size restrictions etc.• Integrated simple webserver functionality • Modular extensibility, e.g. integrating support for RTI imaging (both initial transformation and serving a web-based RTI-viewer - Fornaro et al, 2016) ImplementationIn order to fulfill these requirements, we decided to use C++11 to develop a native, modular, high performance IIIF-compliant image server. A decisive feature was to make the server scriptable with a script language without compromising on performance. We decided to use Lua, an extremely fast, performing, and extensible script language with a small footprint that has only small overhead. It is widely in use for computer games such as World of Warcraft or Minecraft. In order to support different image file formats, we use open source libraries such as libtiff, libjpeg etc. and a modular, extensible architecture. In order to support the JPEG2000 image format, we rely on the kakadu-library. Unfortunately kakadu is not open source, but it is one of the most performant JPEG2000-libraries available. In addition, when acquiring a license, the full source code is provided. Depending on the licensing model, the free distribution of binary packages is included.In order to reduce the computational load of the server, an efficient caching service has been implemented. The canonical IIIF-URL is used as key for caching since it is unique for each parameter set of the IIIF request.The Lua-interpreter has been extended with SIPIspecific functionality. Using configurable routes, a fully IIIF-compliant image server has been implemented with the following features:• Full support of SSL (https) using the OpenSSL library.• Preservation of metadata. We use the open source exiv2 library and own parsing/generating routines to bridge the differences between the different image formats.• ICC profiles are interpreted and converted using the open source \"little cms\" library.• Before serving an image, a configurable preflight lua-script may be executed. Within this script different tasks can be performed, e.g. querying a database for access rights, adding watermarks, ICC profile conversions etc.• Native support of JSON Web Tokens. JWT's may be analyzed using simple Lua functions (e.g. in the pre-flight script) for authentication and access control.• Querying other databases using RESTful services. These RESTful query functions are also exposed to Lua.• Image upload using HTTP multipart/formdata headers. The upload process and file conversions can be controlled with simple Lua scripts.• Cache control with a simple web-based administration interface.• Native support for sqlite3 databases from the embedded Lua.SIPI is open source and can be found on Github. In order to use the JPEG2000 format, a licensed copy of the kakadu library has to be provided by the user. We will provide an extensive manual and binary downloads (including JPEG2000 support) for all major Linux distributions, OS X and a docker image on http://sipi.io ConclusionSIPI is a fully IIIF-compliant native image server which integrates extremely well into existing platforms. The flexibility provided by the embedded scripting language, as well as the features going beyond the IIIF specification, allow the integration of IIIIF-based interoperability into existing imaging platforms and image repositories. The support of the secure socket layer (https) access control is a necessity in the environment of digital humanities.SIPI can easily be customized and extended for special purposes. Elaborated imaging methods (e.g. support for multi-spectral images, image processing functions etc.) could be implemented using the existing SIPI server as base. However, such enhancements will require extensions to the IIIF-syntax. Further development at the DHLab will include the preprocessing of RTI-images as well support for other media types such as PDF and moving image. We are also working on the implementation of the Amazon S3 client-API in order for SIPI to directly serve images that are stored in a S3 compatible cloud storage.    ",
        "article_title": "Integrating Image Resources Into Virtual Research Environments For The Humanities -a Simple Image Presentation Interface (SIPI) based on IIIF",
        "authors": [
            {
                "given": "Lukas",
                "family": "Rosenthaler",
                "affiliation": [
                    {
                        "original_name": "University of Basel",
                        "normalized_name": null,
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Andrea",
                "family": "Bianco",
                "affiliation": [
                    {
                        "original_name": "University of Basel",
                        "normalized_name": null,
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Peter R. ",
                "family": "Fornaro",
                "affiliation": [
                    {
                        "original_name": "Digital Humanities Lab - University of Basel",
                        "normalized_name": null,
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " ObjectivesThe aim of the current project is to gain unique insights into popular music by assembling a new database that will combine score-based and audiobased parameters. As such, we are releasing melodic and lyric transcriptions, as well select signalprocessing data to supplement a subset of 100 songs from the already publicly available McGill Billboard Database ( Burgoyne et al., 2011). SampleThe McGill Billboard Database is a systematically sampled, professionally curated collection of harmonic transcriptions for more than 700 distinct songs that made the Billboard Hot 100 weekly charts between 1958 and 1991. Along with time-aligned chord transcriptions, each file features metadata regarding the title of the song, the performing artist, the chart date, the highest rank the song ever achieved on the Billboard Hot 100 chart, and the number of weeks the song spent on the charts. Our corpus focuses on a 100-song collection taken from the original McGill Billboard Database. More than 70 unique artists are represented, ranging from 1958 to 1991. Transcription ProcessAll songs were divided and assigned randomly among a group transcribers, including the authors. Each transcriber was in charge of finding the appropriate recording matching the original chord transcription from the McGill Billboard Database and transcribing melodic and lyrical information using their preferred notational software. In order to alleviate discrepancies among the different transcribers, a set of guidelines in the form of a \"Transcription Style Guide\" was established and distributed prior to the transcribing process. This file stipulated minimum requirements for every transcription, as well as general instructions on how to notate potentially more challenging pitch nuances, such as slides, scoops, and ornaments.We used the timestamps already available in the original McGill Billboard Database to automatically retrieve acoustical information. The data was encoded separately for the left and right channels, thus maintaining information related to stereo panning. Encoding FormatWe opted to encode the new transcriptions in the Humdrum format. Humdrum (Huron, 1995) is both a syntax to encode music information in ASCII representation and a set of tools dedicated to the manipulation of such files, alleviating the problem of having to write a dedicated parser. Figure 1 represents a typical Humdrum file in our corpus.  Impact and Future WorkWe hope that this new collection of musically-rich data will yield new and unexpected research on popular music, and allow possibilities that were, up until now, virtually impossible. We believe that supplementing traditional score-based data (e.g. harmony and melody) with lyrics and loudness descriptors is a necessary step into developing a holistic theory of form in popular music.Our plans for the future are manifold. We hope to increase the size of our corpus, with the goal to eventually provide complete annotations for every harmonic transcription in the McGill Billboard Database. We also wish to continue supplementing this corpus over the next several years with more detailed data. More specifically, we hope to have instrumental solos, drumming patterns, back vocals, and more acoustical information, including spectral annotations.",
        "article_title": "Supplementing Melody, Lyrics, and Acoustic Information to the McGill Billboard Database",
        "authors": [
            {
                "given": "Hubert",
                "family": "Gauvin",
                "affiliation": [
                    {
                        "original_name": "Ohio State University",
                        "normalized_name": "The Ohio State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00rs6vg23",
                            "GRID": "grid.261331.4"
                        }
                    }
                ]
            },
            {
                "given": "Nathaniel",
                "family": "Condit-Schultz",
                "affiliation": [
                    {
                        "original_name": "McGill University",
                        "normalized_name": "McGill University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01pxwe438",
                            "GRID": "grid.14709.3b"
                        }
                    }
                ]
            },
            {
                "given": "Claire",
                "family": "Arthur",
                "affiliation": [
                    {
                        "original_name": "McGill University",
                        "normalized_name": "McGill University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01pxwe438",
                            "GRID": "grid.14709.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Reckoning with Redlining: Public Engagement with \"Mapping Inequality\" Robert NelsonIn keeping with the conference theme of access and its emphasis upon public-facing scholarship, this presentation will reflect upon hundreds of comments and several conversations from the lay public about \"Mapping Inequality\". A collaboration of teams at four universities, \"Mapping Inequality\" currently includes nearly all of the more than 150 \"security maps\" and nearly 10,000 \"area descriptions\" created by the Home Owners' Loan Corporation during the Great Depression. These maps assessed mortgage risk for thousands of neighborhoods in U.S. cities large and small on a scale of \"A\" to \"D\". \"A\" neighborhoods were deemed \"best,\" presenting minimal risks for banks and lenders; \"D\" neighborhoods were deemed \"hazardous\" for mortgage financing.These grades were explicitly racialist and racist. HOLC's survey instruments asked local agents to quantify the \"infiltration of\" undesirable populations of African Americans and immigrants. To cite just a few examples, a small subsection of a Tacoma neighborhood was graded \"D\" though otherwise identical to the surrounding \"B\" neighborhood because \"Three highly respected Negro families own homes and live in the middle block of this area facing Verde Street. While very much above the average of their race, it is quite generally recognized by Realtors that their presence seriously detracts from the desirability of their immediate neighborhood.\" Proximity to black neighborhoods was enough to impact HOLC's risk assessment. A subsection of a neighborhood in Richmond was graded \"C\" rather than \"B\" because \"Respectable people but homes are too near negro area D2.\" In contrast, a Camden neighborhood kept an\"A\" grade despite bordering an African American neighborhood, but only because \"High walls separates this section from the colored area to the south\" that effectively prevented their \"spread.\"These grades had real consequences. Through this HOLC program, the federal government reinforced redlining as a best practice within the real estate industry, in effect cutting off hundreds of thousands of African Americans off from equitable access to mortgage financing and thus homeownership, which arguably was the most significant mechanism of familial wealth accumulation in twentieth-century America. While of course it is by no means the only or even primary cause, this redlining program helped to contribute to generational wealth disparities between white and black Americans, where today the median wealth of white households is a shocking 13 times that of black households.We designed \"Mapping Inequality\" not only with researchers but activists and the general public in mind. More than 150 of the HOLC maps have been georectified (nearly all of them, though we still have a few to add and undoubtedly a few more will surface), and polygons for each neighborhood added. The site is location aware and asks new users if they want to view their, or alternately the nearest, city. The opacity of the raster maps can be adjusted to help viewers connect the grades to the contemporary cityscape. Nearly all of the neighborhoods polygons can be clicked to read the area description survey. In short, we designed it hoping to encourage viewers to grapple with the materials related to their own localities and to prompt them to make connection to the present.The introduction and other contextual materials on the site convey the authors' collective assessment that \"New Deal era housing policies ... helped set the course for contemporary America.\" We also include a visualization inspired by Ernest Burgess's concentric circle theory to suggest that HOLC policies definition of the interior of cities as \"slums\" functioned as a selffulfilling prophecy. Nevertheless, the site prioritizes access, exploration, and reuse of these important primary source materials. We do not provide these materials completely without commentary, but through our design choices we do facilitate and encourage relatively direct engagement with HOLC's maps and surveys.All of these materials were available in the National Archives. While materials for many cities had been digitized, to date there has been no comprehensive collection let along one offering the functionality of \"Mapping Inequality.\" We have no doubt that these materials will be useful to researchers--not just historians but economists, urban planners, artists, medical doctors, etc.--and will facilitate a far more nuance understanding of HOLC and its consequences. We also have abundant evidence that these materials are a boon for activists working on fair housing and other social justice causes as well.While we're excited about this, we intentionally developed the site as a public history project that aimed to spark conversations about wealth and racial inequality in American cities past and present. By that measure, the project so far has been a success. Two and a half weeks after being released, the map has received about 44,000 visits and been the subject of online coverage from NPR, National Geographic, Slate, CityLab, FastCo., Forbes, and Curbed, all of which have narrated the state's role in fostering redlining and wealth inequality.In the comment section of these stories, in stories in local news sources, and in social media there has been a broad-ranging conversation about wealth and racial inequalities. On one of the spectrum, some respondents have been dismissive of the project, suggesting that this happened 80 years ago and is a remnant of the past that has little relevance today; one person notably characterizing the site as nothing more than \"historical racism porn.\" Others have responded to such comments that this is important inasmuch as many of these 80-year old maps resemble the racial and class landscapes of America today and that the government's role in reinforcing redlining and racial disparities of wealth isn't widely understood.Beyond these arguments about the impact of HOLC and relevance of redlining for understanding inequality in twenty-first-century cities, some of the most interesting and revealing comments have come from people for whom the maps have prompted reflection upon their own family histories. \"I grew up in Detroit in the late 50's and 60's,\" one man wrote. \"My address was 20400 Monte Vista, the corner Monte vista and Norfolk. Two streets east, starting at the corner of Birwood and 8 mile, was a wall. The wall was 12-15 feet high, made of grey concrete blocks and ran behind the homes towards 7 mile, extending to an abandoned army base at the corner of Pembrook and Birwood. The wall was built to divide the neighborhoods, one side was all African Americans, The other all Caucasian. My mother lived on one side of the wall, it was all African American. She once (just once) told me that one could hear the White families on the other side of the wall talking, see them occasionally if a ball came over the fence and they asked for it, most times they did not. One had no contact, ever. The wall is still there, physically and emotionally.\" While so far this story is rather exceptional in its detail, it has prompted us to think about the possibility of using the site to solicit and collect stories about the consequences of redlining and segregation on particular individuals, families, and communities.Given that \"Mapping Inequality\" has been at the center of several online conversations and hundreds of comments on websites two and a half weeks after it was first released, I'm optimistic that it will continue to occasion more conversations about the role of racism, redlining, and the state in inequalities of wealth in American cities. This presentation will provide an opportunity to critically reflect upon these materials and gauge the success and failures of this particular digital humanities project and perhaps the digital humanities more generally in informing socially and politically important public conversations. This presentation will also reflect upon the pros and cons of interpretive framing in digital humanities projects aimed at the public. Mapping the Federal Writers ProjectLauren Tilton and Taylor Arnold \"Mapping the Federal Writers Project\" will explore the role and implementation of deep mapping and spatial analysis in interpreting and understanding documentary expression in 1930s America. As Bodenhamer argues, deep maps are \"visual, timebased genuinely multimedia and multilayered\" (Bodenhemer, 11). These maps allow for nuanced spatial analysis in the service of new humanities questions and arguments. We will focus on the application of these concepts in a new extension of Photogrammar (photogrammar.yale.edu), a digital and public humanities project focused on print and visual culture in 1930s America.Photogrammar (photogrammar.yale.edu) uses methods from the digital humanities and digital resources to further contextualize and open new avenues of research into the federal project and documentary record of the era. In its current version, Photogrammar maps 170,000 photos from the Great Depression and World War II that comprise the United States Farm Security Administration and Office of War Information (FSA-OWI) photographic archive. Importantly, the collection includes some of the most prominent documentary photographers of the 20th century including Dorothea Lange and Walker Evans. Users can explore the collection through interactive maps through the use of spatial analysis; search by photo captions, photographer and time through text analysis; and browse by color and the faces depicted in the photographs through the use of image analysis. This new stage -funded by the American Council of Learned Societies-involves adding a new layer to Photogrammar -the Federal Writer's Project (FWP), which funded writers to capture and describe the complexities of American life during the Great Depression (Couch, Hirsch, Mangione, Penkower, Stewart). Dozens of writers were sent to document through words the impact of the great depression on people's lives across the country. Prominent literary scholars such as Nelson Algren, known for The Man with the Golden Arm , and Ralph Ellison, known for Invisible Man. In the process, they asked people to provide their life histories pioneering the practice of oral history, a critical methodology in the field of history and in the humanities more broadly.Using deep mapping to expand our understanding of 1930s America, Photogrammar is creating links across archives in order to place the FSA-OWI in the larger federal effort to document America during the Great Depression. Merging collections from the University of North Carolina - Chapel Hill Libraries and the Library of Congress, the FWP includes over 4,000 life histories. Interviews are being plotted on a new geographical layer allowing search by space and time.For example, a user will be able to follow an interviewer as they move across a state and the country to collect oral histories in the same way they can now follow documentary photographs like Dorothea Lange and Walker Evans. Users will be able to search the new FWP layer independently or along with the geographical layer of FSA-OWI photographs and photographers. As a result, they will be able to compare the oral histories to the photographs taken in the same area allowing user to compare and contrast the documentary record created and funded by the federal government. As well, the new and cleaned transcripts created over this year are allowing for refined search functionality including faceted browsing and full text search. We are also experimenting with the role Natural Language Processing techniques such as Named-Entity Recognition to create new ways to browse the collection ( Finkel and Manning 2005). In all, users will be able to explore the FWP and FSA-OWI spatially, temporally and through faceted searching allowing the public to explore the broader documentary record of the era relationally through deep mapping.The second half of the paper will focus on methodology. We will start by discussing the potential benefits and potential difficulties of cross-institution collaboration in the cleaning and processing of data. In our experience, working with institutionally and spatially separated groups requires careful planning, but this extra up-front work improves both the general workflow and final products. We will touch on what scholarly and which technical questions guided our creation of a database schema for inputting metadata from the Federal Writer's Project. We take into consideration theoretical work regarding the creation of \"smart data\", best practices regarding TEI markup (Schö ch, TEI Initiative on Libraries), and principles for creating normalized database tables (Codd 1971). The discussion will culminate in showing how these carefully curated data sources are made interactive and public on our website. The geographic data are plotted using custom layers created in CARTO (formerly CartoDB), giving a great deal of interactivity out of the box. Specifically, we will extrapolate on how we designed the interactivity to enhance the other data collections on the Photogrammar website, to make new arguments and pose new questions about about documentary expression in 1930s America, to realize the principles of deep mapping and to engage with various publics. Mapping Silicon Valley Jason Heppler\"Mapping Silicon Valley\" explores the role of spatial history in the urban environment of post-World War II Santa Clara Valley. Silicon Valley is the product of competing landscapes. The geographer D. W. Meinig refers to landscapes as \"a naı̈ ve acceptance of the intricate intermingling of physical, biological, and cultural features which any glance around us displays\" (Meinig, 1979). Wildlife refuges, fenced military installations, city and neighborhood districts, and polluted sites all hold definitions on the land. Historian Richard White has referred to this as \"hybrid landscapes,\" where cultural ideologies clash over conflicting uses of natural resources. The hybrid landscape is neither purely wild nor purely built, but instead a construction of natural and cultural systems that shape and create place (White, 2004). People define places by embedding ideas on the landscape. In cities, urban planners lay down grids of roads, zones, and regulations that divide cities along labor, leisure, and consumption, thus imbuing certain places with particular meaning. Landscapes, as Meinig notes, are \"a great exhibit of consequences,\" and are \"symbolic, as expressions of cultural values, social behavior, and individual actions worked upon particular localities over a span of time\" (Meinig, 1979).By viewing Silicon Valley through the lens of landscapes and space, I argue for the importance of place in shaping a suburban vision of what urban historian Margaret O'Mara has called high-tech urbanism. Silicon Valley has come to represent the future of post-industrial economic development. Places as varied as Atlanta, Georgia; Philadelphia, Pennsylvania; Cleveland, Ohio; Omaha, Nebraska; Bangalore, India; Mission Hills in the Guandong Province of China; and Shenzhen, China, have looked to Silicon Valley as a model for economic and urban revitalization through high-tech economic development. Indeed, high tech is often drenched in green--from high-tech office campuses to \"smart cities\" that promise to transform work, leisure, transportation, and urban space into a more sustainable future.The work of this high-tech landscape has been decades in the making and has come with high environmental costs, despite the promise of clean and green cities. Silicon Valley epitomized the trend of conflating a lack of smokestacks as a proxy for sustainable industrial development. High-tech landscapes centered around industrial research and scientific industry promised growth without pollution, but that promise was an impossible standard.\"Mapping Silicon Valley\" is a broad discussion of three map-centric projects that have moved through different stages. The first set of maps were data driven thematic maps, produced largely during the course of my dissertation research. These maps were created largely out of a desire to understand the transforming landscape in Silicon Valley, from city growth and conflicts over urban space to the widespread presence of pollution and neighborhoods most threatened by toxic chemicals. The first section of this paper will reflect on the methodological underpinnings of these maps and their application to environmental humanities, while also discussing some of the potential shortcomings and enhancements that would make these maps more useful for historical research.The second mapping project is oriented around digital public history. Called Silicon Valley Historical (http://svhistorical.org) and built on the Curatescape platform, the project seeks to collect archival material and narrate the importance of specific places to the Valley's history. Contributions to Silicon Valley Historical are not solely driven by scholarly contributions, but also rely on contributions by students and volunteers in close association with area universities, colleges, historical associations, and historical societies. The material contained in Silicon Valley Historical is meant, in part, to step away from the business-centric stories so often associated with Silicon Valley and consider more fully the urban spaces that were affected by the growth of this high-tech region. Still in its early stages of planning, this section will discuss the challenge of working with community partners and developing a sustainable and scalable digital history project that seeks to serve both the community it studies as well as students and scholars who will find the project useful.The final mapping project, still under planning and a partnership with the Stanford Spatial History Project, is tentatively titled From Orchards to Suburbs: Changing Landscapes in Silicon Valley and will represent the most technological and research heavy aspects of the project. As I investigate the politics surrounding the creation of place, this project will allow for the spatial exploration of zoning laws, general plans, government reports, and city council meeting minutes. The current design envisages the ability to navigate through a map and, depending on the viewport, presenting a list of primary sources available for reading about particular places in Silicon Valley. These will be accompanied by some computational and statistical tools for doing text analysis to uncover more about the kinds of conversations happening about particular places in the city and how they are being thought about.Collectively, \"Mapping Silicon Valley\" will critically reflect on these projects and their evolution as research and public history projects. The paper will further delve into the opportunities for deep mapping and interactivity for exploring the changing landscapes of Silicon Valley.  ",
        "article_title": "Mapping 20th Century America",
        "authors": [
            {
                "given": "Lauren",
                "family": "Tilton",
                "affiliation": [
                    {
                        "original_name": "University of Nebraska - Omaha United States of America Robert Nelson",
                        "normalized_name": "University of Nebraska at Omaha",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04yrkc140",
                            "GRID": "grid.266815.e"
                        }
                    },
                    {
                        "original_name": "University of Richmond",
                        "normalized_name": "University of Richmond",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03y71xh61",
                            "GRID": "grid.267065.0"
                        }
                    }
                ]
            },
            {
                "given": "Taylor",
                "family": "Arnold",
                "affiliation": [
                    {
                        "original_name": "University of Nebraska - Omaha United States of America Robert Nelson",
                        "normalized_name": "University of Nebraska at Omaha",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04yrkc140",
                            "GRID": "grid.266815.e"
                        }
                    },
                    {
                        "original_name": "University of Richmond",
                        "normalized_name": "University of Richmond",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03y71xh61",
                            "GRID": "grid.267065.0"
                        }
                    }
                ]
            },
            {
                "given": "Jason",
                "family": "Heppler",
                "affiliation": [
                    {
                        "original_name": "University of Nebraska - Omaha United States of America Robert Nelson",
                        "normalized_name": "University of Nebraska at Omaha",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04yrkc140",
                            "GRID": "grid.266815.e"
                        }
                    },
                    {
                        "original_name": "University of Richmond",
                        "normalized_name": "University of Richmond",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03y71xh61",
                            "GRID": "grid.267065.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " 1) The Book Thief (2007) by Markus Zusak 2) The Amazing Adventures of Kavalier and Clay (2000) by Michael Chabon 3) Gold Boy, Emerald Girl (2010) by Yiyun LiWe are keen to answer whether a Chicago setting and, more particularly, particular measures of linguistic sentiment about Chicago people and places, have measurable influence on the popularity of books across Chicago. Specifically, we are interested in examining the question of whether \"Chicago\" books, fictional and nonfictional, checked out in greater numbers when they feature characters, events, and places situated close to the readers' own neighborhood library branch? (We use CPL library branch as a proxy for patron home address, which we cannot know from the library system's anonymized checkout data.)The results of this analysis have the potential to provide empirical answers to long-standing questions in digital humanities research: in his Atlas of the European Novel 1800-1900, for example, Franco Moretti speculated that perhaps \"fictional spaces are particularly suited to happy endings,\" but did not have hard numbers to judge one way or the other at the time (18 n.6). More recently, in The Bestseller Code, Jodie Archer and Matthew Jockers argue that \"while it does matter whether an author chooses a city or the wilderness, the specific city does not matter all that much when it comes to bestselling\" (227). Our sentiment analysis findings will contribute to open research questions: maybe in fact the city matters when readers are in that city, and when the places and people in that same city are written about in particular linguistic registers. If literary form and real geography do have detectable ties to one another, our project ought to be able to capture the effect.To compare the circulation pattern of non-Chicago and Chicago-related OBOC books, we used one year of city-wide circulation data for each book, starting with the date of the title's public announcement as the OBOC choice. This data was normalized by dividing the circulation raw numbers with the total number of visitors for that year and multiplying the result by 1000. Normalizing by the number of circulated copies was sometimes difficult because some branches did not have any copies (but could borrow them from other branches). Given that libraries oftentimes allocate books based on the size of the library and based on the number of visitors, we decided to normalize by the overall number of visitors. Distribution patterns for Chicago and non-Chicago related sets of books are represented through the histograms and QQ plots in Figure 1. As this analysis indicates, the circulation distribution across 79 Chicago library branches does not follow bell-shape distribution and is positively skewed. The Wilcoxon signed rank test was used to test to what degree the difference in the distribution for these two sets can be attributed to chance. Results indicate that the probability that the difference in the circulation distribution across 79 Chicago library branches for the two (paired) sets of books can be attributed to chance is very low (p < .01). and three non-Chicago related books. The y-axis in Figure 2 represents the difference between the checkouts per 1000 visitors for Chicago related and non-Chicago related books. Figure 2 indicates that the three non-Chicago related books circulated more than the set of Chicago related books in some library branches in the Chicago area (where the line drops into negative difference). In some branches, however, the difference is almost minimal. The plot also indicates that, in some branches, the OBOC Chicago-related choices had, in fact, more checkouts than the non-Chicago OBOC choices (where the difference is positive). Although it is difficult to establish which factors contribute to this difference in circulation and although we cannot attribute this difference between the two distributions to the mere fact that one set contains references to Chicago whereas the other does not, we plan to represent the library branches that are associated with a larger number of checkouts for Chicago non-related books and those that are associated with a larger number of related books on the Chicago map and analyze them against the sociodemographic and socioeconomic characteristics of different branches (obtained from the American Community Survey data). In the future, we plan to add more Chicago related books to the analysis and observe how this may affect this observed pattern.A further question of interest to us is, do the sentiment measures for these texts map in consistent ways for different neighborhoods? To examine these questions, we rely on Stanford CoreNLP natural language processing capabilities ( Manning et al., 2014, http://stanfordnlp.github.io/CoreNLP/). Given that the identification of places and locations is important for our analysis, we use a tool that has consistently achieved good rankings and, in general, boasts superior accuracy rates when compared to other named entity recognizers ( Rodriquez et al., 2012;Atdağ & Labatut, 2013): the Stanford Named Entity Recognizer, a part of the CoreNLP suite of tools. Before running the named entity recognizer, the text is first tokenized into sentences using the NLTK sentence tokenizer (http://www.nltk.org/). The CoreNLP program then tokenizes sentences into words, identifies lemma for each individual word, uses the Penn Treebank part of speech information (Toutanova & Manning, 2000), and also notes Persons, Locations, Time Reference, and Numbers in the sentences ( Finkel et al., 2005). We are specifically interested in locations as this category would not only identify Chicago as a location but also its streets and landmark buildings. For sentiment analysis, we are using the Stanford sentiment analysis tool (Socher et al., 2013)-also part of the Stanford CoreNLP-to annotate each sentence with the sentiment score on the following scale: Very Positive, Positive Neutral, Negative, Very Negative.Preliminary analysis on the sentiment associated with sentences that contain the word Chicago in the three Chicago-related books is indicated in Figure 3: The raw count of sentences with their sentiment ratings was normalized by the total number of sentences that contain the word Chicago. Noticeable in Figure 3 is that although these three books differ according to genre, and although they differ in terms of topical coverage and date of publication, we see a rather similar sentiment score pattern with respect to sentences that contain the word Chicago. We suspect that this overall similarity pattern will start to change as we dig deeper into the location data: we must note here that this initial analysis above does not yet take into account local references such as Pizzeria Uno, Pullman, the South Side, Monroe Street, and the like, but do not also use the word Chicago in the sentence. We plan to obtain a set of place names associated with Chicago through resources such as Open Street Maps and GeoNames and search for all the occurrences of Chicago place names. Additionally, we plan to use the indexes in the back of some of the books as trusted sources of local place names. ",
        "article_title": "Real and Imagined Geography at City-Scale: Sentiment Analysis of Chicago's \"One Book\" Program",
        "authors": [
            {
                "given": "Ana",
                "family": "Lucic",
                "affiliation": [
                    {
                        "original_name": "DePaul University",
                        "normalized_name": "DePaul University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04xtx5t16",
                            "GRID": "grid.254920.8"
                        }
                    }
                ]
            },
            {
                "given": "John",
                "family": "Shanahan",
                "affiliation": [
                    {
                        "original_name": "DePaul University",
                        "normalized_name": "DePaul University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04xtx5t16",
                            "GRID": "grid.254920.8"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis paper reports a co-words domain analysis of Buddhism literature collected by DLMBS (Digital Library and Museum of Buddhist studies) at National Taiwan University. Established in 1995, the DLMBS is one of the most comprehensive online repository of Buddhist research materials. It currently contains over 400 thousand records of books, research papers, theses and dissertations in 45 languages and digitized Buddhist scriptures. A controlled vocabulary, which is in five languages, including Chinese, English, Japanese, German, and French, was used to help users search DLMBS's bibliographic database. By using cooccurrence data of author assigned keywords in the bibliographic records, this study attempts to generate co-word networks in three different languages, Chinese, English, and Japanese, to compare regional focuses on Buddhist studies.Co-words analysis has been shown to be effective in mapping the intellectual structure of disciplines (He, 1999;Leydesdroff, 1989). While it has been widely used in the domains of sciences and technologies (e.g. Buitelaar, Bordea, & Coughlan, 2014;Ding, Chowdhury, & Foo, 2001;Bhattacharya & Basu,1998;Looze, & Lemarie, 1997;Peters & van Raan, 1993a, 1993bCourtial, 1994;Tijssen, 1992;Callon, Courtial, & Laville, 1991;Rip & Courtial, 1984), to the best our knowledge, it has so far not been applied to humanities. Part of the advantage of using co-word in sciences and technologies is the highly codified subject languages, therefore a higher degree of consistency between concepts and terms in these fields. We believe that a cross-language co-word analysis of Buddhist studies literature would be a worthwhile endeavor for a couple of reasons: Firstly, it has been pointed out that there has been a wide variety of methods, perspectives and subject matters within the international communities of Buddhist studies. The heterogeneity of its scholarships can be partly traced to their geographic roots (Cabezón, 1995). It is therefore interesting to empirically study whether and how the intellectual structures reflected in the published literatures in these language communities differ from one another. A comparison of the intellectual structures can shed light on knowledge interests shared and distinct in these three language communities. From the methodological plain, co-word analysis also provides a viable alternative to citationbased network analysis in humanities where the citation structure is known to be much sparser than in sciences and technologies. Procedures and analysisThree separate co-word networks were generated in three different languages where nodes denote the keywords and edges the strength of their cooccurrence. Unlike in most of the previous co-words analysis, where keywords were extracted from titles and abstracts, author assigned keywords were used here as it is believed that they are more representative to the content of the articles and tend to have higher degree of consistency than keywords in free-text. For monographs and other types of publications, the subject-headings assigned by human indexers were used as keywords. Edge weights were normalized by both the inclusion and the Jaccard index (Courtial,1986;Callon, Law, & Rip,1986).Thus three word similarity matrixes were generated so social network analytical methods such as cohesion, centrality and community-detection (Blondel et.al., 2008) could be performed with a view to exploring the social and cognitive structure of Buddhist studies manifested in its published literature in respective languages. Specifically, the study seeks to answer the following interrelated research questions: firstly, are there recognizable branches or specialties in Buddhist studies? If so, what might these areas of research be? Cross-languages comparisons were also made to examine the similarities and differences of the intellectual structure in different language communities. Table 1 gives the numbers of items pertinent to various publication types analyzed in three languages in DLMB. Due to the enormous size of the networks, some sorts of filtering are required to make the groupings intelligible; node degrees was used as the filter as many of the little connected nodes tend to generate noises that impairs meaningful interpretation. To determine the proper threshold of node degree, one needs to consider three criteria: the quality of the clustering, the interpretability of the individual clusters, and the preservation of information. A high threshold would filter out large amount of nodes hence the greater loss of information and low modularity values. On the other hand, a low threshold would result in difficulty in interpreting individual clusters as they tend to lump together heterogeneous topics. Thus a trade-off needs to be made. We approached this matter by performing modularity analysis at different threshold levels so the values of their modularity, the resulting number of communities, as well as the size of the networks could be recorded. The following heuristics were used to select the proper thresholds: to preserve about 10 percent of the total nodes, to limit the number of communities from 10 to 20, and to preserve a high degree of modularity, which is commonly used as the indicator of clustering quality. Figure(1-3). Node degree thresholds and resulting network attributes in three languages Table 2 reports the thresholds and their corresponding attributes of the resulting networks. After filtering out lesser connected nodes, modularity maximizing community detection method was then performed to identify the subdomains in each language network. A two-stage approach was adopted here. As some of the communities resulting from the first-round of clustering can still be very broad and heterogeneous, a second modularity analysis was performed on these relative \"super\" clusters (i.e. clusters with more than 400 nodes), the joint results produce a two-levels hierarchical structure. Three experts in Buddhist studies were then interviewed to help us interpret the clusters (See Figure 4 and 5).",
        "article_title": "A cross-language comparison of co-word networks in Digital Library and Museum of Buddhist Studies",
        "authors": [
            {
                "given": "Muh-Chyun",
                "family": "Tang",
                "affiliation": [
                    {
                        "original_name": "National Taiwan University",
                        "normalized_name": "National Taiwan University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05bqach95",
                            "GRID": "grid.19188.39"
                        }
                    },
                    {
                        "original_name": "National Taiwan University",
                        "normalized_name": "National Taiwan University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/05bqach95",
                            "GRID": "grid.19188.39"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionWeiso(衛所制), which means \"guardhouse\", is one of the military units of the barracks used by the Chinese dynasty Ming  to maintain peace throughout its empire. WeisoEvent is a web-based digital humanity research tool targeting Ming Weiso events recorded in Ming Shilu, which contains the imperial annals of the Ming emperors. WeisoEvent is composed of two parts: (1) an event type classifier that categorizes paragraphs according to their event types; (2) an analytics tool that shows (1)'s result, markups named entities, links guard mentions to Academia Sinica's Chinese Civilization in Time and Space (CCTS) spatial-temporal platform, and provides four visualization functions. Historians can use this tool to search for specific event types and gain insight into the relationship between particular guards and those event types, not only improving the efficiency but still maintaining the quality of research. Event type classifierNormally, one would develop a supervisedlearning-based text categorization system to classify paragraphs into different event types. This involves defining a set of categories and annotating example texts for each category. However, lacking the human resources needed for such a task, we use unsupervised text clustering, which groups paragraphs into clusters by event type, to generate categories and their corresponding paragraphs for training an automatic event classifier. Although the results are not as accurate as those of pure supervised text classification, this hybrid approach is an acceptable tradeoff.In clustering algorithms, each paragraph is represented as a vector. In previous studies, paragraphs have been represented using the vector space model (VSM), which represents each text as a feature vector of terms. However, this approach loses the ordering and ignores semantics. Yet another representation scheme inspired by word2vec is the \"Paragraph Vector\" proposed by (Le and Mikolov, 2014), an unsupervised framework that learns continuous distributed vectors for pieces of text. In their model, entire paragraphs are represented as vectors. The vector representation is trained to predict the words in a paragraph. More precisely, they concatenate the paragraph vector with several word vectors from a paragraph and predict the following word in the given context. Le's Paragraph Vector model has many advantages. First, it is mostly unsupervised and works well with sparsely labeled data. Second, it is suitable for text strings of various lengths, ranging from sentences to whole documents. Finally, it can overcome many weaknesses of the bagof-words and bag-of-n-grams models. Because it does not suffer from data scarcity and high dimensionality, it also preserves the ordering and semantic information.In summary, we propose a classification method which is based on clustering. First, we employ a named entity (NE) recognizer to label texts. Second, we train a paragraph vector model to represent paragraphs as vectors. Third, we cluster paragraphs with length <40characters. Finally, we use the clustering results as gold-standard categories with which to train a support-vector-machines classifier to predict other paragraphs' categories.We compare our method with the state-of-the-art paragraph clustering method using continuous vector space representation proposed by (M. Chinea- Rios et al., 2015). They use word2vec to learn word vectors and represent each sentence by summing the vectors of the words in that sentence. Like Chinea-Rios et al., we use the k-means algorithm to cluster vectors. We set the number of clusters to 68. We refer to the evaluation measures used in ( Le and Mikolov, 2014). We generate sets of three paragraphs: two with the same event type and one with a different event type. Each set is referred to as a paragraph triplet. The distance between the two vectors with the same event type should be closer than the distance between either of these two and the unrelated one. We collect 923 paragraph triplets and compute the accuracy. Our best configuration that combines word dimensions and named entity dimensions to generate paragraph vectors achieves an accuracy of 62.49%, outperforming Chinea-Rios et al.'s pure text-clustering approach (M. Chinea-Rios et al., 2015) by 24.65%. Analytics tool interfaceWeisoEvent groups paragraphs with similar subjects into clusters automatically and each cluster is named manually according to the main subject of its paragraphs. Clusters with related topics are grouped into broader categories for search convenience. For example, we group \"earthquake\", \"conflagration\" and \"hailstorm\" into the event category \"disaster\". Users can modify event category titles by clicking a button on the top-right of the webpage (see Fig. 1, [1]) Figure 1. System interface. Fig. 1 shows our research tool webpage, which consists of three main windows: (a) search parameters, (b) search result visualizations, and (c) search results snippets. Search parameters: Users can search for oneor more guards by typing the name into the search box (Fig. 1, [2]). For convenience, a user may also import a guard list by clicking the \"import\" icon ( Fig. 1, [3]). Notably, if two guards are queried, their event timelines are displayed in parellel, like Jian-zhou (建州) and Wu-che (兀者) guards shown in window b. After at least one event category is selected, the search results are shown in windows b and c. 2. Search result visualization tools: In window b, users can select among four visualization options at the top of the frame. Option 1 hides or shows an event timeline of the search results on the page. When the event timeline is enabled, event-type labels corresponding to each retrieved paragraph are displayed chronologically on the timeline. For reference, the timeline shows the CE year at the bottom of the window (Fig.  2, [4]) and the name of era, which usually corresponds with the reigning Ming emperor, at the top (Fig. 2, [5]). When a user clicks on an event icon in the timeline, the corresponding text snippet is displayed in window c, highlighted in yellow (Fig. 2). Figure 3 takes Jian-zhou guard (建州衛) as an example to depict options 2 to 4. Option 2 is the bar chart. Each bar corresponds to a Chinese era name and represents the total number of paragraphs for the three selected event types in that era. Option 3 shows each bar sub-divided by color to show the distribution of paragraphs of each event type (\"come over and pledge allegiance\"/ \"reward alien\"/ \"tribute-reward\") in each era. By clicking Option 4, a pie chart shows the distribution of the three selected event types in the entire dataset. The slice for each event type is labeled with the number of paragraphs of that event type and its percentage of the total. These data visualizations offer historians a quick statistical overview of selected event types.  Finally, we conduct a case study targeting Jurchens subordinated garrisons, including Wu-che guards (兀 者諸衛) , Jian-zhou guards (建州諸衛), Mao-lian guard (毛憐衛) by using the proposed tool to obtain statistics regarding tribute event types.We compare our event classification results with Cheng's study (N. Cheng, 2015), which used Ming Shilu as the source to investigate the tribute events during Yongle (永樂), Hongxi (洪熙), and Xuande (宣德) periods. We regard the paragraphs categorized as \"tribute-reward\", \"come over and pledge allegiance\", and \"reward alien\" event types as those potentially illustrating tribute events and manually check them. For Wu-che, Jian-zhou, and Mao-lian guards, 69, 86, and 40 paragraphs are identified as the above three types, respectively. Among these paragraphs, 66, 77, 37 are correct, which are close to the numbers of tribute events in Cheng's study for these three guards (60+, 70+, 30-). This study was done within 16 manhours. These preliminary results are consistent with Cheng's manual analysis results and show that our tool not only helps historians study Weiso events more efficiently but also keep the quality.",
        "article_title": "WeisoEvent: A Ming-Weiso Event Analytics Tool with Named Entity Markup and Spatial-Temporal Information Linking",
        "authors": [
            {
                "given": "Richard",
                "family": "Tzong",
                "affiliation": [
                    {
                        "original_name": "Center for GIS - Academia Sinica",
                        "normalized_name": null,
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Yu-Ting",
                "family": "Lai",
                "affiliation": [
                    {
                        "original_name": "National Central University",
                        "normalized_name": null,
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Yu-Chun",
                "family": "Wang",
                "affiliation": [
                    {
                        "original_name": "Chunghwa Telecom",
                        "normalized_name": null,
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Sunny Hui-Min ",
                "family": "Huang",
                "affiliation": [
                    {
                        "original_name": "Institute of History and Philology - Academia Sinica",
                        "normalized_name": null,
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "I-Chun ",
                "family": "Fan",
                "affiliation": [
                    {
                        "original_name": "Center for GIS - Academia Sinica",
                        "normalized_name": null,
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Introduction Authentic text and graded readerOne of the objectives of second language (L2) learning is to be able to read and understand a variety of texts, from novels to newspaper articles, written in the language of interest. These texts written with a native audience in mind are commonly referred to as authentic texts or \"real life texts, not written for pedagogic purposes\" (Wallace, 1992). Authentic texts, however, can present too many obstacles for L2 learners with too low a level of knowledge. The complex language structures and advanced vocabulary of these 'real' texts can have the unwanted effect of demotivating the reader ( Richard, 2001). The gap between the learner's limited L2 knowledge and the fluency of authentic texts creates an ideal space for graded readers. Graded readers are \"simplified books written at varying levels of difficulty for second language learners\" (Waring, 2012). Through graded readers original classic works can be adapted to match the learner's level of knowledge, thus providing the ideal tool to tackle 'real' themes, narratives and dialogues. From authentic text to graded readerOne such graded reader is a newly adapted version of Jane Austen's Pride and Prejudice (edition of 1813) that one of the authors of this paper wrote (Franzini, 2016) as part of a collection for learners of English as a foreign language (EFL).For authors, the process of adaptation of a text for a learning audience is complex. In order to simplify the text the author will necessarily have to make grammatical changes and lexical substitutions following vocabulary lists, shorten the text by cutting out entire paragraphs and events, and in some cases eliminate entire chapters and characters. Together with these changes, which can be defined as 'structural' because they are dictated by hard requirements of length and standardised level of difficulty, the author will also make a series of judgment calls at a sentence and word level. These changes, which are here defined as 'cognitive', include processes that are more intangible and that are a consequence of a native author's 'feeling' that the original text is too difficult for learners. These include elaborating, clarifying, providing context and motivation for unfamiliar information and non-explicit connections (Beck et al., 1991). Research ObjectiveThe objective of this study is to computationally analyse the manual process behind the simplification of a historical authentic text aimed at producing a graded reader. More specifically, it aims to classify and understand the structural and cognitive processes of adaptation that a human author, more or less consciously, is able to perform manually. Do the applied changes follow strict rules? Can they be classified as forming a pattern? And if so, can they be reproduced computationally? Related ResearchResearchers have long been addressing the issue of text simplification for a variety of purposes. A similar study to this was made by Petersen who compared authentic newspaper articles with abridged versions (Petersen and Ostendorf, 1991). Similar studies have been made, for example, to create a reading aid for people with disabilities (Canning, 2000, Allen, 2009). DataThis study considers two sets of data. The first is the entire original novel (ON) Pride and Prejudice. The second dataset the graded reader (GR) published by Liberty. The GR has been compressed from the 61 chapters of the ON to 10 chapters. When comparing word tokens, the GR has a size of 12.6% of the ON (Tab. 1). The language was simplified to match the upper intermediate level B2. To guide the choice of vocabulary, the author chose to follow the Lexitronics Syllabus ( Lexitronics, 2009).  Methodology ReadabilityAs a first step towards analysing the differences and similarities between an authentic text and a graded reader, we decided to evaluate if what is published as a graded reader can computationally be considered a simplified version of the original. The method chosen to make this investigation was to conduct two different readability tests, namely the ARI test and the Dale-Chall Index test on the data. Both tests were designed to gauge the comprehension difficulty of a text by providing a numeric value, which corresponds to a particular school level of a native speaker of the language tested.The results show that both tests yield similar scores and satisfy the hypothesis that this particular GR can be computationally proven to be, in terms of 'understandability', a simplification of the ON.  Difference AnalysisIn order to analyse the process of adaptation, a difference analysis was conducted by considering both those elements that changed from the ON to the GR, and those that, by contrast, remained the same. The analysis is structured into chapters, sentences and words, so as to proceed in order from the largest unit of text to the smallest.When adapting a text, whether it is for a graded reader, a play or a film, the rationale behind the selection of certain parts over others is normally content-based. Here the author selected the most dynamic parts of the novel, which included dialogues, moments of suspense, movements of the characters and revelations. The selection of some scenes of the plot over others is purely a 'cognitive' choice of the author because it is entirely subjective. However, by using a text reuse detection software (TRACER) on both texts it was possible to visualise where the majority of reuses occur. These concentrate in particular around the beginning and the end of the novel (dark green in Fig. 1). 'Structural' changes made at a sentence level present patterns that can be more systematically identified. For example, by comparing sentence length, it was noted that on average the ON contains longer sentences (24 words) than the GR (16.22 words) (Fig.  2). Though this might seem like an obvious result, it appears less so when one thinks that, in order to simplify a concept for a language learner, it is often necessary to use additional words to elaborate or clarify it. the texts In order to conduct a difference analysis on the smallest unit of text - the word - we looked at all the words that appear frequently in the ON, but that never appear in the GR, in order to understand what kinds of words the author found necessary to drop.  Table (3) shows that 14 out of the 34 words listed (ca. 35%) are too advanced for level B2. Some of the other words, though accessible to B2 learners, were replaced with easier synonyms. We also conducted an analysis on parts of speech and how they differ in the two data sets (Tab. 4).  Conclusions and further researchThis study is a first step into the realm of text simplification and adaptation regarding graded readers for L2 learners. By conducting a difference analysis between the two texts, it was observed that at plot level the selection of scenes has no impact on the difficulty of a text. The text reuse detection software used, however, identified which parts of the plot have been preserved and which have been eliminated for the sake of a consistent, yet shorter, story line. It was observed that the beginning and the end of the novel were the parts that were adapted most faithfully.The identification of reuse over the whole novel was also a step towards pinpointing where sentences were reused verbatim and where they were not. Where the sentences have undergone heavy changes, we can observe to what extent they were modified, how and why. At a sentence level, we noted that reducing the length of the sentences is a successful simplification strategy. A further study would have to be conducted to best understand how sentences were split or reduced, and consequently how the syntax of a sentence was affected by its shortening.At a word level, the simplification of the text appeared to be dictated by the elimination and replacement of difficult vocabulary and certain parts of speech, such as comparative and superlative adjectives. The word length does not appear to be an indicator of difficulty. While it was observed that both the readability tests were based on sentence length as a parameter, only the ARI test, however, considers word length as another parameter. A test on the wordlength distribution of the ON versus the GR shows that, in this case, the word length bears no importance in assessing the difficulty of a text. Further research would have to be conducted in order to learn if it is easier for an L2 learner to remember a word not because of its length, but because of its repeated presence in a text. The insights gained from this study will be useful in future work on automating the simplification process.",
        "article_title": "From Jane Austen's original Pride and Prejudice to a graded reader for L2 learners: a computational study of the processes of text simplification",
        "authors": [
            {
                "given": "Emily",
                "family": "Franzini",
                "affiliation": [
                    {
                        "original_name": "eTRAP Research Group - Georg-August-Universität Göttingen (University of Gottingen)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Marco",
                "family": "Büchler",
                "affiliation": [
                    {
                        "original_name": "eTRAP Research Group - Georg-August-Universität Göttingen (University of Gottingen)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionJazzCats (Jazz Collection of Aggregated Triples) is a prototype project which uses Linked Open Data (LOD) to support musicological, historical, and prosopographical analyses. It has increased access to (and the openness of) data published online through a twofold process: firstly, information hitherto unavailable to users has been shared and incorporated into the project, and secondly, data previously locked in non-Open types (e.g. PDF) has been published in a machine-readable format, increasing discoverability in the context of the wider Web. Connections between datasets that could only be identified through a human user engaging separately with each existing project have now been made explicit, and the resulting aggregated data is queryable from a single userinterface (UI).Three projects contribute to JazzCats: a social network connecting musicians through various types of relationships is provided by LinkedJazz (Pattuelli, 2016); details of solos within performances (including pitch, key, and chord changes) are available from WJazzD ( Pfleiderer et al., 2016);and Body&Soul (Bowen, 2013) is a discography of over 200 recordings. These complementary data contain instance-level overlaps for recordings and musicians.Bringing these resources together has enabled a new type of research question, possible only through using criteria from one dataset to inform and hone results from another. Limitations of existing data publication methodsThe sub-projects at the heart of JazzCats engage with the 5 Star Standard of LOD (Berners-Lee, 2006) to different extents (see Table 1). The data for Body&Soul has been published online as a PDF, making it an ideal example of 1 Star categorization. WJazzD allows users to download both the database and the software (2 Star). LinkedJazz provides two separate data-dumps of RDF (with an additional, earlier set of triples also available), containing both dereferenced URIs and those which point to humanreadable pages. We have categorized this project as 5 Star, because DBpedia resource URIs (related through owl:sameAs relationships to LinkedJazz resource URIs) are used (even if the retrieving of additional data from external sources is currently not possible via the LinkedJazz SPARQL endpoint). Publishing the information from the first two datasets with distinct HTTP URIs, connecting them to each other as well as to the RDF acquired from LinkedJazz, makes JazzCats 5 Star standard.Conversion to Linked Data (LD) does not automatically ensure that information is more reusable or discoverable by data consumers on the Web ( Bechhofer et al., 2013;Janowicz et al., 2014). Closed systems can benefit from LD, and whilst adherence to the LOD paradigm is an essential criterion for enabling reuse of any project's RDF by other data publishers, effective queries by a wider base of users can be restricted by idiosyncratic or project-unique vocabularies. To encourage good practices, Janowicz et al. (2014) have introduced a 5 Star rating, ranging from LD without vocabulary use (0 Star) to a vocabulary that is linked to by other vocabularies (5 Star). The term vocabulary is used in a broad sense to include all types such as schemata, and ontologies. We have categorized LinkedJazz as 5 Star because it links to other vocabularies and metadata about the vocabulary is available.JazzCats makes extensive use of properties and classes from other vocabularies, including the Music Ontology (MO) ( Raimond et al., 2007), the Event Ontology ( Raimond and Abdallah, 2007), FOAF (Brickley and Miller, 2014), and SKOS ( Miles et al., 2005). It is currently classified as a 5 Star since metadata about the JazzCats ontology is available in a dereferenceable and machine-readable form, but other vocabularies do not yet link to JazzCats.  (Bangert, 2016). An existing workflow (Nurmikko-Fuller et al., 2016) was then reproduced to map this tabular data into RDF using an Open-Source data integration tool (Web-Karma). This workflow relied on domain expert user-input to complete the ontological modeling and instance mapping stages within Web-Karma (University of Southern California, 2016). To support the future alignment and enrichment of this data with other musicological datasets, the underlying ontological structure extensively utilizes the properties and classes of the MO. The data structure has been documented on the website of the JazzCats project ( Bangert et al., 2016).Both the data and the software for WJazzD are available for download from the Jazzomat Research Project website. Although structured data, and in adherence with the 2 Star LOD publication criteria, information in this form is not accessible for machineinferencing, and the clustered tables can be difficult for human users to navigate. The data was converted to RDF by repeating a second workflow as described in Nurmikko-Fuller et al. (2016) using the D2R server (Cyganiak and Bizer, 2012). This automated process produced clusters of triples based on database information categories (e.g. melody, beats, sections), which are mostly expressed through xsd:strings and xsd:integers. Mappings were made where applicable to connect these elements together using MO properties and classes. An overview of the ontological structure, and a detailed subsection illustrating the different properties are documented and defined on the JazzCats website.LinkedJazz provides two separate datasets for entities and the 12 different types of interpersonal (both professional and social) relationships between them. Adding these RDF-dumps to the JazzCats triplestore enables queries combining this prosopography with performance metadata derived from the other projects which make up the entirety of the JazzCats data.Publishing these datasets as RDF using common vocabularies and ontologies known to have been utilized in other digital musicology projects increases their discoverability and the value. As data publishers, adhering to LOD standards allows us to further benefit from any additional future linkage. A conscious decision has been made at the onset of the JazzCats development process to publish RDF, ontologies, and raw data in Open and accessible formats, with appropriate licensing, to allow for the replication of our workflow, verification of our findings, and reuse of any or all of the composite parts of the project. JazzCats is made available under the Open Data Commons Open Database License ( Miller et al., 2008). The aim throughout has been to produce and publish data that adheres to the FAIR data principles of being findable, accessible, interoperable and reusable (Wilkinson et al., 2016). Evaluation of JazzCatsAlthough the data in JazzCats adheres to a 5 Star standard of LOD for accessibility and openness, the current UI presents a barrier to access for human users. At present, the project RDF (RDF Core Working Group, 2014) is contained within an instance of the Open-Source version of the Virtuoso (OpenLink Software, 2016) triplestore, and is only queryable using the default SPARQL endpoint, accessible from the JazzCats website. Example queries demonstrate how results can be generated by drawing from all three datasets simultaneously (Nurmikko-Fuller, 2016), but engagement with the data beyond the parameters set by these existing samples requires the user to have the necessary skills to construct new SPARQL queries (W3C RDF Data Access Working Group, 2008). Aware of the dichotomy of skills between music scholars and the ability to formulate such queries, the authors acknowledge that at present, the site UI presents notable barriers to access.To address this, we have provided extensive documentation of the underlying ontological structures on the project website. Each of the subprojects is illustrated with diagrams that include an interactive feature which provides the scope notes for a given class when hovered over with the cursor (see Figures 1-3). The diagrams also show the inherent connectivity of the graphs within JazzCats, and the directionality of properties (arrows running from domain to range). The combination of easy access to appropriate documentation for the data, the underlying ontological structure, and examples of functioning queries enables specialists to access JazzCats data according to their research interests. In addition, a Pubby interface ( Cyganiak and Bizer, 2011) serves as an alternative Linked Data front end that enables users to navigate through the triples without the need to use SPARQL. Influenced by ongoing work at ResearchSpace ( Oldman et al., 2016), planned future developments of JazzCats include the development of a GUI that will allow users to generate ontologically valid queries using dropdown lists generated by available properties for each class. This step will further help open JazzCats for experts and scholars along the full length of the digital humanities spectrum. ",
        "article_title": "All the Things You Are: Accessing An Enriched Musicological Prosopography Through JazzCats",
        "authors": [
            {
                "given": "Terhi",
                "family": "Nurmikko-Fuller",
                "affiliation": [
                    {
                        "original_name": "Australian National University",
                        "normalized_name": null,
                        "country": "Australia",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Daniel",
                "family": "Bangert",
                "affiliation": [
                    {
                        "original_name": "University of New South Wales",
                        "normalized_name": null,
                        "country": "Australia",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Alfie",
                "family": "Abdul-Rahman",
                "affiliation": [
                    {
                        "original_name": "Oxford University",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe standard process for negotiating legal and quasi-legal texts over more than two hundred years has been a parliamentary one that (with variations) is still recognizable as the one described in Thomas Jefferson's Manual of Parliamentary Practice (1801) (U.S. Government Publishing Office, 2016; May, 1844). Proposals are examined by a series of committees, amendments being proposed and voted on throughout this process. Since the late eighteenth-century, many of these negotiations of historical note have left records that record the proposals made and the outcome of decisions taken. Such records are difficult to read-especially when they concern any protracted or complicated process of negotiation, since it rapidly becomes impossible for a reader to keep track of the state of the documents under discussion. Fully comprehending the records if not read in chronological sequence is impossible.Building on code written for collaborative document editing, we have built a sophisticated, web-accessible platform for the study of negotiated texts. We kept the underlying data-model as simple and generic as it could be while modelling the various procedures suggested by a range of Parliamentary Procedure handbooks. We considered the needs of several distinct classes of users - those doing the work of datacapture, those reviewing that work, those wishing to comment on the detail of the text, producing secondary materials for a variety of audiences, and those wishing to navigate through the material for a variety of purposes. Initial applicationOne such process of negotiation was that which created the United States Constitution. The Records of the 1787 Convention, despite being imperfect and not (initially) intended for public release, in fact enable a detailed reconstruction of the work of the Constitutional Convention. These records have been available in various printed forms since 1819 when the official Journal was first printed; these printed records have been digitized as both images and transcribed text (Lilian Goldman Law Library, 2008;Silverbrook and Johnson, 2007;National Archives Catalog, 2016;Li- brary of Congress, 2016). While indexing and searching increase the utility of both paper and digitized versions (by allowing readers to discover when particular topics were debated), neither format allows the reader to understand the full context of a particular debate. This is of no small importance, because opinions of participants in the negotiations about particular matters shifted as surrounding questions were answered one way or another. Other digital projectsWhereas the Comparative Constitutions Project has pioneered the comparison and display of finished constitutional texts (Elkins and Ginsburg, 2005), and some other web projects have attempted to make the text of the United States constitution easier to navigate (Surden, 2015), our project focuses instead on the process of negotiation. Other projects have attempted to overcome the limitations of the Convention's records by giving users narrative outlines of the key events in a way that can guide their reading (Lloyd, 2016;Lin- der, 2016;EDSITEment, 2016). More generally, websites tracking the progress of Parliamentary debates have focused on milestone moments in the history of texts, rather than letting users track the detail of a document's evolution (Parliament, 2016;Tauberer, 2004). ChallengesThe records relating to formal negotiations are typically a set of minutes that record of proposals and the votes taken upon them. The principal aim of those recording the minutes is to facilitate the record-keeping process necessary during the work of committees, not to provide later readers with an easy way to reconstruct any particular moment. Each formal proposal to amend a document has at least two contexts that are relevant to readers -what does the document look like when the amendment is proposed? what does the document look like when the amendment is approved or rejected? Due to the nature of committee work, these contexts may differ significantly. Making sense of these records, therefore, poses a significant memory-burden on readers. Detailed and specific discussion of issues presented by these records is hampered by the need for authors to provide their own reconstruction of elements of this process, presented in a narrative form that is necessarily partial and can itself become difficult for readers to follow.This problem might have been partially addressed using creative re-purposing of either version-control systems designed for computer-science applications (such as the tools rcs (GNU, 2013), git (Software Freedom Conservancy, 2016), or mercurial (Mercurial, 2016)) or else the creation of a layered XHTML document (The University of Virginia Press, 2009). However, we rejected these solutions as being either incapable of fully capturing the nature of the source material or else as likely to result in a fragile platform that would have been too much tied to the specific documents and unsuitable for more general applications. Since future work will compare different negotiations, a more generic platform that could work with a variety of sources with minimal new coding was required. Our solutionQuill is a newly developed platform for the study and presentation of formal negotiations. It was developed initially with a view to presenting the records of the 1787 Federal Convention that created the Constitution of the United States, but was designed to be a generic platform applicable to a wide range of materials, including the creation of constitutions, treaties, and legislation. The model captures formal negotiations -that is those where there is a procedure of considering and deciding upon discrete suggestions for the wording of documents, and where minutes capturing these deliberations have been taken.An innovation was to present not merely the reconstruction itself but to integrate a publishing platform that would allow authors to present their own commentary on the material in a way that would allow analysis to be presented alongside specific events within the timeline.Links to relevant material held on other websites (for example, images of the original manuscripts) are similarly presented to users where relevant. In this way, the website integrates with existing materials, enhancing their value as well as its own and avoiding duplication of effort. We encourage such co-operation with other projects through machine-readable interfaces, a flexible permissions system and a system of resource collections that allow third-parties to manage links to their own assets and control how they appear within our platform.For the 2016 release we relied on the 1911 edition of the Convention Records published by Farrand, even though we know these to be imperfect transcriptions of the surviving manuscripts. This choice allowed us to focus on the development of the software platform. The Quill Platform is capable of presenting different versions of the same event, and future work using the original manuscripts will refine our presentation of the records. Supporting information-seeking and explorationNegotiations of this type are extremely complex and assisting users to access the information they require is a challenge. In our public interfaces, we have guided users to access material in several ways. To acclimatize users to the idea of navigating the history of an evolving text, we present a Secretary's Desk view, which allows users to navigate the state of documents as they existed at the end of each committee session (see Figure 1). This view hides much of the complexity of the negotiations, but allows new users to quickly grasp the concept of our reconstruction. We also present visualizations that allow users to explore the structure of negotiations through a variety of tree-diagrams (Her- man, et al. 2000) (see Figure 2) and sunburst-style (Stasko, et al. 2000) visualizations.The role of individuals and specific delegations, as well as voting patterns, are presented in separate visualizations. All of these views guide users who need more detail down to views that present the work of individual committee sessions moment by moment. Users looking for information on a specific topic are guided towards a search tool. In addition, users can also navigate the platform through a variety of resource and commentary collections, making it possible to provide users with a more guided experience. Conclusions and future workThe process of negotiating the constitution was complicated (we have modelled close to 4,000 discrete events), and our presentation transcends the possibilities of narrative accounts while making access to and intelligibility of the extant sources much quicker and of greater utility for a broad range of users. The model system itself is content-agnostic and could be used to model a wide range of similar processes. Future work will continue to enhance the user experience both through refinement of the visualizations and user interface, and through the creation of guided views in to the material in collaboration with others, as well as expanding the range of material.Wider public engagement and education is a key aim of this project. We are collaborating with nonprofit organizations in the United States to develop guided views suitable for classroom use and integrated in to existing curriculum materials.",
        "article_title": "Quill: Reconstructing the Secretary's Desk for the Records of the 1787 Convention",
        "authors": [
            {
                "given": "Nicholas",
                "family": "Cole",
                "affiliation": [
                    {
                        "original_name": "Oxford University",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Alfie",
                "family": "Abdul-Rahman",
                "affiliation": [
                    {
                        "original_name": "Oxford University",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Grace ",
                "family": "Mallon",
                "affiliation": [
                    {
                        "original_name": "Oxford University",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Kate",
                "family": "Howarth",
                "affiliation": [
                    {
                        "original_name": "Oxford University",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe structured repositories in cultural heritage have become the most used infrastructure for knowledge management towards different kinds of systems and platforms, ensuring a complete interoperability and reachability of data. Thanks to the semantic web paradigm, we are able to manage and enrich our data using formalisms and data standards: examples include digital libraries and digital archives, as well as SPARQL endpoints. However, the fragmentation of data produced by different kinds of mapping methodologies and different representations of the knowledge to be managed leads to some discrepancies between domains and results obtained during data retrieval.A typical example is the study of an inscription, which will be addressed by linguists with regards to language; by philologists with regards to its text; by historians as a primary source; by archaeologists as material testimony of events and by conservationists as a piece of matter to be preserved and restored. In that scenario, semantic interoperability and standardization are two fundamental elements as they guarantee the circulation of knowledge inside a shared environment.This paper aims to present the methodology followed in the Nurcara Project concerning resource integration and knowledge management. Nurcara consists of a dataset (almost 300 records) composed of textual Latin documents from the Medieval Age that provide historical context for the area of Monteleone Rocca Doria in Sardinia (Sassari, Italy) between the 11 th and 15 th centuries. Starting from an SQL database, our solution focuses on the development of a semantic framework solution able to both automatically map the relational dataset in CIDOC-CRM ontology on-thefly and aggregate the knowledge from different repositories or endpoints. This method aims to semantically enrich Nurcara's dataset with external resources from the same domain, such as SPARQLendpoint and Linked Data resources. Nurcara projectNurcara started from the following directive: to make accessible to the community the huge amount of published and unpublished medieval documents about Monteleone Rocca Doria that until now have been only accessible from the researchers.Thanks to a collaboration between the Spanish Ministry of Cultural Heritage and the University of Sassari, it has been possible to start archival research of historical documents from the National Archive of Cagliari, the Historical Archive of Alghero and the General Archive of the Crown of Aragon in Spain. During the research almost 300 medieval documents in Latin, Catalan, Castilian and Sardinian were cataloged and stored in a MySQL database. The documents dated from the 11 th to 15 th centuries and related to Monteleone Rocca Doria (SS), which made them useful for defining the socio-economic context of the area during the medieval age. Interoperability and semantic enrichmentThe interoperability and semantic enrichment of the data represents one of the most important milestone of the project. It is crucial to publish the dataset in an accessible format with Semantic Web technology such as SPARQL or Linked (Open) Data.In order to reach our goal, we started with the conceptualization of the entities from the relational database and the definition of the semantic schema. CIDOC-CRM has been chosen as the main reference model. CIDOC-CRM is an ontology created in order to offer \"definitions and a formal structure for describing the implicit and explicit concepts and relationships used in cultural heritage documentation\". The CIDOC-CRM model is a semantically rich model used to conceptualize the cultural heritage domain composed of 86 classes and 138 properties. The purpose of CIDOC-CRM is to provide a common definition for heterogeneous forms of information, and to enable their integration despite possible semantic and structural incompatibilities.In order to translate data stored in the Nurcara relational database to the CIDOC-CRM ontology (expressed in RDF-Resource Description Framework) we have used the D2RQ tool that will provide an automatic mapping between the relational database and the CIDOC mapping schema.D2RQ is a popular mapping platform for publishing relational data as a virtual RDF graph. It enables legacy relational databases to be exposed on the Web, according to the principles of Linked Data, and to be included in the Semantic Web. D2RQ exposes relational databases as SPARQL endpoints. It translates SPARQL queries posed on a virtual RDF graph to SQL queries posed to the underlying relational database. Mapping SolutionThe mapping process started with the definition of the main entities useful for the definition of the conceptual model directly from Nurcara's database:• Document (E84 Information Carrier) • Author (E39 Actor) • Issue place (E53 place) • Issue date (E50 Date) • Type (E55 Type) • Title (E35 title)We chose these entities as a starting point for the mapping activities because from a conceptual point of view, they cover the minimum knowledge representation useful for the description of the documents from Nurcara's database. Furthermore, with the aim of defining a more suitable (eventoriented) semantic model with respect to CIDOC-CRM ontology, we defined some \"abstract\" entities that are outside the structure of Nurcara's database, but are present as classes in CIDOC ontology:• Dataset (E73 Information Object) • Event (E5 Event) • Activity (E7 Activity)Here, the use of the abstract classes as an intermediate layer allowed us to guarantee a more detailed and coherent conceptualization of the proposed model with respect to CIDOC-CRM ontology. Then, thanks to definition of the \"Dataset\" as E73 Information Object, it is possible to extend the proposed model with CRMdig (developed as a compatible extension of ISO21127) application profile from CIDOC-CRM ontology, which allows us to encode metadata about the steps and methods of production of digitization products.The defined conceptual schema has been directly implemented as a mapping file (using D2RQ syntax) in D2RQ in .ttl format, in order to ensure the live mapping process between Nurcara dataset and CIDOC-CRM ontology.In order to enrich and extend the dataset, we have also considered in the mapping.ttl schema other Linked Data end-points such as DbPedia or Geonames, which are specified by prefix and reachable via a proper SPARQL query directly from D2RQ SPARQL end-point. Data accessibilityAs previously stated, after the mapping process, the data are expressed and exposed as RDF graph based on CIDOC-CRM structure. HyperlinkingThe D2R server supports hyperlink navigation by providing links on the RDF and XHTML levels. Any RDF triple whose object is a dereferenceable URI can be seen as a hyperlink. This is how resources published by the D2R Server are interlinked with other databases and external RDF documents. To aid discovery of related resources, D2R Server includes an rdfs:seeAlso triple with every resource description that points to an RDF document containing links to other resources produced by the same ClassMap (In our case, DbPedia or Geonames). If resources are identified with external URIs, then an additional rdfs:seeAlso link points to a local RDF/XML document that contains everything the database knows about the resource. By dereferencing the external URI and by following the rdf:seeAlso link, RDF browsers can retrieve both authoritative and non-authoritative information about the resource. SearchThe D2R Server allows the users to query non-RDF databases using the SPARQL query language over the SPARQL protocol. Queries are executed against a virtual RDF graph representing the complete database. Query results can be retrieved in the SPARQL query result as XML or in SPARQL/JSON serialization. ConclusionIn spite of progress in the area of RDF storage, a large quantity of data is still stored in non-semantic repositories or relational databases.Nevertheless, especially in the domain of the humanities, we are seeing growth in the use of semantic platforms for the management of digital data as a common method of knowledgement management. The CIDOC-CRM ontology is one of the most used in digital humanities. In this proposal we want to show how a specific dataset as Nurcara can be successfully integrated under the much more generic CIDOC-CRM structure, and how the knowledge can be easily enriched thanks to LOD integration. In order to reach the goal, the scalability and the full customization offered by the D2RQ server has played a crucial role.The future development of this project concentrates on the integration of the Description Logic algorithm and reasoning system with the purpose of increasing resource discovery with respect to the domain utilized by the Nurcara project.  ",
        "article_title": "Lifting knowledge from the Medieval Age: CIDOC-CRM for the Nurcara Project",
        "authors": [
            {
                "given": "Matteo",
                "family": "Lorenzini",
                "affiliation": [
                    {
                        "original_name": "Austrian Academy of Science",
                        "normalized_name": "Austrian Academy of Sciences",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/03anc3s24",
                            "GRID": "grid.4299.6"
                        }
                    },
                    {
                        "original_name": "Università degli Studi di Sassari",
                        "normalized_name": "University of Sassari",
                        "country": "Italy",
                        "identifiers": {
                            "ror": "https://ror.org/01bnjbv91",
                            "GRID": "grid.11450.31"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionData modelling is an essential process of almost any digital humanities project ( Flanders and Jannidis, 2015). Whether texts, images, or any other form of data is mapped or analysed, a model has to be conceptualised that describes the data and forms the bedrock of the application that contains or analyses the data.Since data modelling in the humanities is largely perceived as an epistemological process, rather than an ontological process, there is a tension between the way in which material and knowledge presents itself and the manner in which material and knowledge can be described on a generalised or abstracted level. As Flanders and Jannidis (2015: 236) have pointed out: \"Some of the most fertile and urgent areas of digital humanities research involve the question of how to develop data modeling approaches that accommodate both the self-reflexivity required by humanities research and the actionability and computational clarity required by the digital domain.\"In this paper we reflect on a data modelling approach that has proven to be an effective teaching practice as well as a useful research method. The iterative data modelling approach we put forward focuses on a continuous shift between three levels of data modelling: conceptual level, logical level, and interface level. We have found that this approach provides students and scholars in the humanities (\"scholars\") with the skills they need to translate their body of data or research question into an operational process that produces rich (inclusive; fuzzy and uncertain) and complex (advocate divergent classes) actionable datasets. It is important to note that even though it is useful when a scholar can develop their own data model for computer-aided analytical purposes, we should not underestimate the learning curve this new skillset requires.This paper focuses on experiences we gained from data modelling practices in the humanities aimed at developing a relational database. We draw on the results of over 20 courses and workshops for scholars we have held in the past three years on developing data models and using database applications. These insights are also informed by the continuous development of the research environment nodegoat, developed by the authors of this paper, and the scholarly collaborations resulting therefrom. ChallengesMost scholars do not perceive their material or knowledge as 'data' (Posner, 2015). Once a scholar has accepted that lists of people, statements, and ideas can also be seen as data and that we do not necessarily need to be able to count with them, it becomes clear that their material or knowledge can be modelled as well. Like the analog card catalog, a database helps to store data properly and sustainably. This allows us to filter and query the data. We can then also create networks and analyse relationships. It is important to note that vagueness, uncertainty, and incompleteness can be incorporated in a data model.To allow scholars to operationalise the data modelling process, three levels of a data model have to be studied. The conceptual level, the logical level, and the interface level describe the data at hand, each in its own way. Here, it is necessary to reflect critically on hidden assumptions in the choice of entity types and classifications (Erickson, 2013). An iterative data modelling approach is largely research driven, although existing standards could be used as well. By asking scholars to operationalise their own data model, rather than using or implementing a preexisting model, they get acquainted with the complexities and granularities that operationalising a data model entails.The interface challenge - how to operate a database application? - is very important. We see the translation of the data model into an actual database as a vital step to get a good understanding of the data modelling process. We prefer to do this with a database application that has a graphic user interface to be able to iterate quickly and to easily compare data models. Teaching PracticeThe participants in our data modelling workshops ranged from undergraduates to established scholars. These workshops were either in the format of intensive one day workshops or stretched over multiple events in the course of months. During a workshop, we first addressed the aforementioned challenges to show that the challenges participants face are not new and that we can critically reflect on them. Secondly, we did collective exercises to give participants an understanding on how data models and database applications work.When we then asked them to conceptualise a data model based on their own research question, most participants did not know where to start. The reason for this seemed to be twofold. First, they were unable to process new information regarding data modelling and the database application into an operational process. Since most of the participants were trained to conduct research with a syntagmatic dimension in mind, a linear text, it was hard to execute a research process that leads to a paradigmatic dimension, a database (Manovich, 1999). Secondly, since they were invested in the complex and unique aspects of their research project, they were unable to operationalise a coherent model while keeping relevant variables and complexities in mind (Beretta, 2016).To overcome this, we introduced an iterative approach that took them back and forth from their research question to a partial conceptual data model, to a partial logical model, and to a partial functional database application. This process helped them to first understand how to translate one class of information to a single, non-relational, data table. Once they could process basic typed values (strings/numbers), they started to work with texts, images, dates, and locations. These steps informed participants on the transformation of a conceptual idea into a table with fields in a database application. They first focused on the basics, the finite, while leaving growing complexity, the infinite, to next iterations: creating additional data tables and constructing relationships between them. After these practical questions had been tackled, attention was shifted towards uncertain data, fuzzy data, and the question on using existing standards for a data model.In literature on data modelling processes, a distinction is made between the conceptual/logical level and the level of the application. A data model should be portable and not dependant on one application ( Flanders and Jannidis, 2015). However, this does not mean that the conceptual/logical level may not be informed by the application while teaching data modelling practices. The feedback loop between these different levels has proven to be an essential step in helping scholars understand how their own research project can be translated into a data model and a functional database application. Research MethodThe iterative data modelling approach is also of value as a research method. The aforementioned distinction between the conceptual/logical level and the interface level works well when the data for a data model is complete and unambiguous and the process in which the data model plays a role is completely mapped out. Obviously, these variables rarely hold true for research projects in the humanities.Oftentimes the data model does not correspond with data at hand. First, a data model may ask for data that is not there for the majority of data objects. Secondly, data may be too vague to fit typed fields defined in a data model. Thirdly, a data model may lead to a research process that is too time consuming due to its level of detail. In all these cases, revisions of the data model are needed in order to continue the research process.Instead of smoothing out irregularities in the data by simplifying the data model, the model should be adjusted to reflect the existing complexities, vagueness, and uncertainties. As Rawson and Muñ oz (2016) have stated, scholars should \"see the messiness of data not as a block to scalability but as a vital feature of the world which our data represents and from which it emerges.\" We encourage scholars to include these data driven practices into their data model and have developed various strategies and features, such as 'reversed classification', to allow them to do this in nodegoat (van Bree and Kessels, 2014;van Bree and Kessels, 2017).With the iterative methodology applied in nodegoat, we have facilitated research projects in the range of: disambiguation of Babylonian letters, questions of provenance and intertextuality in medieval manuscripts, creation of a multi-sourced 19th century context of conference attendance on social issues, mapping structures of violence in 1965 Indonesia, and documentation of an actor-network towards an encyclopedia of romantic nationalism. An iterative data modelling approach allows scholars to enrich their data model during the research process. While a scholar may first want to use their own model, this can later be transformed or mapped to existing standards, like CIDOC-CRM, or semantic web standards. The data itself may be enriched by adding external identifiers such as VIAF identifiers or identifiers to other linked open data resources. This last point is important when data is published as an actionable dataset online (Berners-Lee, 2006). ConclusionIn this paper we have set out to describe an iterative data modelling approach that helps scholars become confident in modelling their data and that functions as a research method for database development in the humanities. We have argued how a continuous shift between three levels of data modelling helps to conceive actionable datasets and establishes a framework for dealing with the complexities associated with humanities research.  ",
        "article_title": "Iterative Data Modelling: from Teaching Practice to Research Method",
        "authors": [
            {
                "given": "Pim",
                "family": "Van Bree",
                "affiliation": [
                    {
                        "original_name": "LAB1100",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Geert ",
                "family": "Kessels",
                "affiliation": [
                    {
                        "original_name": "LAB1100",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Academy of Sciences and Literature in Mainz provides online access to the Regesta Imperii - a very extensive historical dataset based on documentary sources of German-Roman kings. About 125,000 regestae of emperors and popes are searchable and viewable in this online portal. The current user interface offers direct access to single documents through different form-based search facilities, as well as through a catalogue that directly reflects the structure of the regestae volumes as they have been created in this long-term project. In order to further improve access to this large data volume, we suggest an additional approach based on coordinated views. The usage of coordinated view approaches is very common in many domains ( Stasko et al, 2008, Vuillemot et al, 2009, Koch et al, 2011). However, there is no publicly system available, which would provide a suitable access to this historical dataset. The motivation for this new approach is twofold. On the one hand, we improve the support for search and exploration tasks in this historical data set that are based on imprecise information needs or on a less deep understanding of the available information. In practice, such imprecise queries can quickly lead to an overwhelmingly large number of search results. Allowing users to create and refine queries in a visual way, while offering immediate feedback on the number of entries requested, can help to cope with underspecified queries and help refining them iteratively ( Jänicke et al, 2012). On the other hand, we offer a powerful means for visually analyzing the available information and understanding complex relationships by providing different linked perspectives on subsets of the collections. These perspectives include views on historic persons and entities as well as temporal and spatial information contained in the regestae. A usage scenario shows successful application of the approach. Visual approachWe implemented a web-based visualization that is easily available to humanities scholars, since it does not require users to install software. The web-based visualization uses the library Data-Driven Documents (D3) ( Bostock et al, 2011) and runs with a web browser supporting HTML5, SVG, CSS, and JavaScript. Data processingIn the Regesta Imperii database all documentary sources of German-Roman kings are available in full text (xml-files). This data set consists of regestae volumes and register information. Regestae volumes are short summaries of a text and contain important metadata of a document, such as the title of the document, an ID for the unique assignment, date of issue, and place of issue as name and coordinates.Since places, persons, and institutions may be known by different names, entries can be overlooked within the full-text search. Therefore, place and personal registers provide a central resource for work with the regestae and contain a list of places, persons, institutions, and additional information, such as the numbers of the regestae in which the entity is mentioned, a reference to another entity entry (if available), and a unique id.Since the regestae have been manually digitized, the xml-files can contain syntax or other transmission errors,such as different date formats, geo-coordinates or tags. Therefore, the data must be parsed and well prepared in order to use them for a visual analysis. Visual approachAfter the regestae and register volumes have been successfully parsed and loaded into our system, users can start their exploration in the main view as depicted in Figure 1. The main view consists of the five coordinated views: (A) timeline view, (B) map view, (C) register view, (D) overview filter view, and (E) results view. We initially depict all available data and enable users to create and refine queries in a visual way iteratively, to reduce the overall set. For example, users can select certain time periods and places in which the regestae were published or persons and places that are mentioned in the regestae.The timeline view consists of two stacked timelines and enables users to select a time period by clicking and dragging, as depicted in Figure 6, comparable to the Simile approach (Huynh, 2008). The first timeline allows a coarse filtering and represents the wholetime range of the regestae. Once a timespan is selected, the second timeline is updated and permits a finer selection of the upper selected time range.To get an overview where the regestae were published, users can discover the map view as depicted in Figure 2. The map view uses the JavaScript library Leaflet (Agafonkin, 2014) which supports interactive features such as zooming and panning. The red circles in the map represent places where the regestae were published and the circles size is scaled proportionally to the places occurrences, similar to the approach (DARIAH-DE, 2015). This helps to get a quick overview of important places. When hovering over a circle, a tooltip shows the corresponding place name. By selecting one or more circles (highlighted in yellow), the places are added to the search query. The register view represents the entities from the register volumes in an alphabetically sorted hierarchical structure as depicted in Figure 3. Users can explore the different entries by clicking on the different hierarchies. Nodes, which contain further entities are displayed in darker color. Furthermore, users can select one or more entities to adapt the search query. From the overview filter view, users can get a summary about all the selected places and entities which determine the search results. In addition, users can deselect places and entities from the list to adapt the search query.Based on the combined search query, the result view lists all regestae entries, which are included in the search results as depicted in Figure 5. For each entry, the list displays the following metadata: title of the regesta, issuer, place and date of issue. By clicking on the icon in the column entities, users get the information of all entities occurring in the regesta in a separate list view. Furthermore, users can select the icon in the last column uri to jump directly to the corresponding regesta entry on the web page of the Regesta Imperii. This enables users to analyze the regesta in detail.  Usage scenarioIn the following section, we present a usage scenario that occurred during one of the joint workshops of users from the Academy of Sciences and Literature in Mainz and the developers of the approach. Therefore, our usage scenario represents instead the insights and lessons learned through several sessions with two experts from the Regesta Imperii.In a first step, the expert explores and analyze the map view. That way, she gets a quick overview of the places where regestae were published. During the exploration, the place Nürnberg (Nuremberg) has aroused her interest, since she has already examined these entries a long time ago. To find out more about the regestae entries, she starts a search query by selecting Nürnberg (Figure 6A) in the map view and discovers the search results in result view. However, the result set is too large for a further deeper analysis. Therefore, she refines the search query by selecting the time period from 1440 to 1450 in the timeline view ( Figure 6B), because she is especially interested in the early regestae entries. As the next step, she searches for entities that are mentioned in the regestae with the aid of the results view. She finds out that there are many connections to French entities. To further analyze that, she selects the hierarchy \"Frankreich, Königreich\" (France, Kingdom) in the register view, as depicted in Figure 6C. By adjusting the search query, she received a specified subset of the collection for a further analysis. This way, she finds that primarily French kings are mentioned in the regestae entries.During the analysis, she learns from the map view that Neustadt near Bremen ( Figure 6D) has many regestae entries which she did not expect. To inspect this in detail, she selects Neustadt and explores the result list. By analyzing the different entries in the list and web page of the Regesta Imperii, she finds out that the geo-coordinates were manually digitized incorrectly. Consequently, the expert corrects the entries in the database by assigning these entries to the actual place Neustadt near Vienna.While these sessions, we received a lot of feedback that showed that our approach improves access to the large data volumes of the Regesta Imperii and facilitates search and exploration tasks, as well as assisting in understanding complex relationships and gaining new insights.  Conclusion and future workThe presented interactive web-based approach has been evaluated through expert feedback that recommends it as an effective method for exploration analysis.We are planning to extend the different linked views to support users with additional information. Concerning this issue, we have implemented the relative distribution of the regestae volumes in the timeline view, as depicted in Figure 6, and we are currently working on the co-highlighting between the views. We will also ensure that experts from the Regesta Imperii are able to correct errors that arise during the digitization process interactively.",
        "article_title": "Interactive Visual Exploration of the Regesta Imperii",
        "authors": [
            {
                "given": "Markus",
                "family": "John",
                "affiliation": [
                    {
                        "original_name": "University of Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Christian",
                "family": "Richter",
                "affiliation": [
                    {
                        "original_name": "University of Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Steffen",
                "family": "Koch",
                "affiliation": [
                    {
                        "original_name": "University of Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Andreas",
                "family": "Kuczera",
                "affiliation": [
                    {
                        "original_name": "University of Mainz",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Thomas",
                "family": "Ertl",
                "affiliation": [
                    {
                        "original_name": "University of Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe digital humanities offers the promise and potential for global, multilingual, and multicultural collaborations as enabled by digital technologies that deliver public-facing scholarship that enriches and expands research communities. Realizing that potential requires approaches that embrace complexity, spanning the technological, social, procedural, and community concerns. In 2016-2017, the Digital Library of the Caribbean (dLOC) is undertaking a research project to study and advance support for the Digital Humanities specifically with Caribbean Studies, focusing on leveraging technologies to support public-facing scholarship, open access to research and results, enabling digital humanities research publications with technologies that meet access needs, and collaboration among scholars and communities. This presentation provides background on dLOC as a digital library, dLOC's evolution into a community connecting place and platform for digital humanities and Caribbean Studies, and the opportunities and needs for the digital humanities for enlarging scholarly and worldwide community access to Caribbean Studies. About dLOCThe Digital Library of the Caribbean (dLOC) is a unique, open access, collaborative, international, multi-lingual digital library for resources from and about the Caribbean and circum-Caribbean, providing access and ensuring preservation for Caribbean materials (digitized and born-digital) and publicfacing scholarship.dLOC focuses on how a community of practice can best create a digital library in terms of contents, functionality, and robust governance for inclusivity and diversity. In dLOC's governance and operational model, partner institutions agree to shared goals and processes following a governance structure for: inclusive and distributed collection development where partners select materials, permissions-based infrastructure (partners retain all rights to materials), functional hubs, decentralized/local digitalization and digital curation, collaborative activities to develop the community of practice and increase capacity through collaboration. For all of this work-spanning tools for digitalization, online digital library functionality for patrons and users, tools for advanced querying and mining, and tools for automating reporting and user management which are necessary for operations at scale and reports to different governmental and academic entities-dLOC's technologies have been defined by and created in collaboration with partner institutions and scholars. Thus, over the past 12 years, dLOC has developed through collaboration into as a socio-technical (people, policies, communities, technologies) platform supporting collaboration among partner institutions, developing and enhancing communities of practice, and building intellectual infrastructure. dLOC, Caribbean Studies, and Digital HumanitiesdLOC originally focused on building content, and has grown into one of the largest open access collections of Caribbean materials with over 2.5 million pages of content, over 40 partner institutions (universities, colleges, libraries, archives, museums, government agencies, NGOs, publishers, and scholarly societies, as well as many contributing scholars and private collectors), and over 3 million views each month. With this growth, the dLOC Executive Committee, Scholarly Advisory Board, and full community recognized the need to emphasize new stages of development, specifically focused on further engaging scholars in digital humanities practices that build upon dLOC's commitment to access, preservation, the production of public-facing scholarship, and engaging across institutions and communities to further the community of practice as a constellation of communities of practice where all involved are leveraging the affordances of technology to further public humanities and interdisciplinary aspects of modern scholarship.In 2016, the dLOC Executive Committee charged the dLOC Digital Scholarship Director with undertaking research into next steps for dLOC's sociotechnical development (people, policies, technologies, communities) in relation to new opportunities with the digital humanities. dLOC currently supports the digital humanities in many forms, including curated materials and collections, digital humanities exhibits, pedagogical resources, teaching guides, and supporting faculty in developing online research and teaching materials.dLOC's work in the digital humanities grapples directly with questions of access in the digital age. For example, one of dLOC's digital humanities projects is Haiti: An Island Luminous. Haiti: An Island Luminous began when then-PhD student Adam Silvia recognized the importance of materials about and from Haiti in dLOC, both for the significance of each item and for the sheer scale of materials available. Many of the materials in dLOC were not known to exist in the world, before being located by partner institutions and then digitized. Once the materials became available, the necessary scholarly and information ecosystems to link and cite the items was not in place. Indeed, scholars would be unlikely to search for items that were believed to have been lost to history. To create the necessary community connections, Silvia contacted members of the dLOC team and began collaborating to develop what was planned as an online exhibit to showcase materials. As the project began, the scope continued to grow. In its final release, Haiti: An Island Luminous is a curated edited online collection with contributions from over 100 of the top Haitian Studies scholars. Haiti: An Island Luminous is in English, French, and Kreyò l, and has been taught in schools in Florida and Haiti. Developed first online, the site is being developed for offline use on standalone kiosk/tablet installations, to meet access needs despite limited connectivity. Discussions are underway on the possibility of a print version to meet needs for limited online and electrical access, with the print version to include all pages from the curated edited collection online, samples of items, and with an accompanying USB drive.Another example of dLOC's digital humanities work is the course \"Panama Silver, Asian Gold: Migration, Money, and the Making of the Modern Caribbean.\" The course focused on the lesser studied Asian and Indian indenture in the Caribbean and was a Distributed Online Collaborative Course (DOCC), taught in multiple semesters and multiple iterations, most recently with the teaching team comprised of faculty, librarians, and archivists at the University of the West Indies, Cave Hill Barbados, University of Miami, University of Florida, and Amherst College. This collaboratively developed and taught course engaged the teaching team and students across all of the campuses in the use of materials in dLOC, other digital resources, and not-yet-digital resources held at each of the institutions to create new works of digital scholarship with students from different campuses working collaboratively. The course is underway for revision to focus on migration and mobility, to further expand the potential collaborators and fields engaged in this interdisciplinary collaborative course.Both the DOCC series and Haiti: An Island Luminous present real examples of imagined possibilities for expanding Caribbean Studies in the digital age for creating public-facing scholarship, enhancing access to scholarly works, opportunities from the digital humanities in pedagogy and academic curricula, and the potentials for access as enabled by collaboration among scholars and communities. Expanding Access through dLOC for Caribbean Studies and Digital HumanitiesCaribbean Studies is an interdisciplinary field with broad connections across languages and cultures. dLOC takes its definition of the Caribbean from the Association of Caribbean University, Research, and Institutional Libraries, which defines the Caribbean as \"the area of the Caribbean archipelago, the mainland countries including the Guianas, and the states of the United States which border on the Caribbean Sea or Gulf of Mexico.\" Even this is immediately expanded with diasporic connections. dLOC's success as a digital library is made possible by the recognition that the Caribbean exceeds the boundaries for any specific geographic area, language, or field of study, and by the generous framing for Caribbean Studies with the respect and support for interdisciplinary and diasporic connections. dLOC was founded to meet the needs for preservation and access as a first step in supporting expanding the field of Caribbean Studies. As both the series of DOCCs and Haiti: An Island Luminous demonstrate, simply creating online access to materials is only part of the equation. As Adam J. Banks explains in Race, Rhetoric, and Technology: Searching for Higher Ground, access includes material access, which is met with materials being online, at least for those with online access and the functional access to be able to find and use materials. The digital humanities offers ways of conceiving and creating the means for experiential access, where materials are relevant for people's lives, and transformative access, where there is \"a genuine inclusion in technologies and the networks of power that help determine what they become, but never merely for the sake of inclusion\" (45). This presentation will review methodologies and results from the 2016-2017 research on the digital humanities and Caribbean Studies. The research includes site visits to the US Virgin Islands, Jamaica, Barbados, Curaçao, Trinidad, and Leiden (for the Dutch Caribbean Archives) to discuss current activities, projects, terminology, and framing to enable connections across communities. The site visits include engagement with scholars from multiple fields, museum professionals, librarians, archivists, governmental representatives, educators, and others. All materials from the research visits are shared with communities engaged and connected with Caribbean Studies through conferences and various means. This grounded approach allows for identification, recognition, and connection of digital humanities work being done, even when not previously labeled as such. The presentation will include findings on how dLOC is enabling responses and activities to meet the needs for material through transformational access through technologies, procedures, communities, and technologies.  ",
        "article_title": "Generous and Generative Communities for the Digital Humanities with the Digital Library of the Caribbean and Caribbean Studies",
        "authors": [
            {
                "given": "Laurie",
                "family": "Taylor",
                "affiliation": [
                    {
                        "original_name": "University of Florida",
                        "normalized_name": "University of Florida",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02y3ad647",
                            "GRID": "grid.15276.37"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Martian is a best-selling science fiction novel by Andy Weir that became a hit film in 2015. The novel exists in two versions, or variants: Weir self-published The Martian on his personal website in 2011 (hereafter, \"Martian1\") and began selling it on Amazon.com in 2012. Crown Publishing subsequently bought the rights, edited the book, and re-released it (hereafter, \"Martian2\").The research presented here investigates what exactly changed when The Martian got edited. At first glance, the two versions appear essentially the same, with no major changes to plot, character, or structure. A closer look using a combination of quantitative and qualitative methods, however, reveals a number of noteworthy changes, as well as notable changes that result from thousands of seemingly minor copyedits. AimsThe aim of our research is to identify what changed between the two variants of The Martian using a combination of close reading and digital methods, analyze why those changes are important, and propose a methodology for comparing self-published and later-edited novels, an increasingly common phenomenon. We hypothesize that the editing process of a leading publishing house results in a novel that is more \"mainstream\", i.e. socialised, domesticated, and appealing to a general audience. In order to test this hypothesis, we explore a range of aspects, including style, content, and character. Our research also aims to bring a critical perspective to the strengths and weaknesses of a variety of qualitative and technical methods in identifying the edits and assessing their importance. Related WorkIn addition to work in digital genetic criticism (e.g. van Hulle 2008), a small number of studies use digital methods to explore variants of contemporary fiction. Yufang Ho (2011) compared the 1966 and revised 1977 versions of John Fowles's novel The Magus, while Martin Paul Eve (2016) looked at differences in the US and UK versions of David Mitchell's Cloud Atlas. As both Ho and Eve use different methods from one another and from us, it appears that no standard method has emerged so far for this type of research. DataThe data used for this research is primarily two plain text files of the variants of The Martian. Martian1 was obtained in PDF format from Andy Weir's website. Martian2 was obtained by scanning a print copy, performing OCR with manual corrections. We consider this our best option given the legal issues regarding text protected by copyright. Methods and Results Basic collationWe used the Wdiff frontend to the \"diff\" algorithm (Hunt & McIlroy 1975) to produce a collated version of Martian1 and Martian2 and assess the number and extent of the edits. We then used bespoke Python scripts to classify the edits identified by Wdiff.We found a total of 5146 edits were made to the novel. While 92% of the 101,000 words in Martian1 remain unchanged in Martian2, the remaining 8% of the words undergo some type of edit, whether they are deleted or modified ( Figure 1). The sheer number of edits calls for automatic means to classify them and detect any patterns.  Automatic Classification of EditsEdits were automatically classified into two broad categories: script-detectable copyedits, and all other edits. Script-detectable copyedits includes changes in capitalization, whitespace, hyphenation, spelling of numbers, abbreviations, or combinations thereof ( Figure 2). All other edits were classified as insertion, deletion, expansion or condensation and as \"minor\" or \"major\", depending on the Levenshtein distance (Figure 3). Of the 5146 edits, 2863 (or 55%) were script-detectable copyedits, while 2283 (or 45%) comprised the rest. The code used as well as the collation data obtained are available on GitHub.   Cumulative Effect of the Script-Identifiable CopyeditsTaken together, the 2863 script-identifiable copyedits have substantial effects upon the text. Weir's many misspellings and misuse of hyphens and capitalization are corrected. Numbers in Martian1 are overwhelmingly written numerically, and 765 of these become words in Martian2, e.g. \"8\" becomes \"eight\". We found 231 instances of edits involving abbreviations, e.g. \"L\" becomes \"liters\".The copyedits work together in different ways when they appear in protagonist Mark Watney's narration or in sections written in the third person (Figure 4). When Watney narrates, the hundreds of misspellings, numerals, and scientific abbreviations in Martian1 support the fiction that he is a scientist working in extreme conditions. Martian2 increases readability but eliminates the stylistic realism of Watney's text. When Weir uses, for instance, numerals in the dialogue of other characters, the effect can be jarring. Martian2 corrects this for the better.  Detecting transpositions with CollateXWdiff does not detect transpositions, or text that has been moved to a different location in the novel. Using CollateX (Dekker & Middell 2011) as described in Schö ch (2016) revealed a total of 126 transpositions. Twenty-eight (or 22%) involve punctuation and should be considered artefacts of the method; 43 (or 34%) represent transpositions of a single word, showing stylistic preferences on the word-order level; 55 (or 44%) concern multi-word expressions which change the overall construction of a sentence or paragraph more substantially. Figure 5 shows a relatively minor transposition appearing in combination with a contraction of a sentence. We conclude that, quantitatively and qualitatively, transpositions were not a major part of the edit to The Martian. However, future work could apply the same method to other, comparable variants of novels to gain better reference points. Close Reading of Other EditsWhen we grouped the other edits, placed them into a spreadsheet, and manually inspected them, a number of thematic and stylistic shifts between Martian1 and Martian2 became apparent.Profanity is a key stylistic feature of The Martian that is substantially cut and softened by the edit. Words like \"fuck\" and \"shit\" are substantially reduced (by about 33% and 15%, respectively), while numerous other words and phrases are softened with \"lesser\" profanity or simple non-profanity (e.g. \"the shit hits the fan\" becomes \"all hell breaks loose\"). Figure 6 shows a selection of these edits. Similarly, crude and sophomoric humor is cut in key instances. The plot of The Martian revolves around solving one problem after another to rescue an astronaut, Mark Watney, stranded on Mars, while relatively little text is devoted to Watney's emotions or inner world. In Martian2, however, Watney expresses significantly more emotion: he misses his family and friends more and expresses despair, loneliness, and introspection more often. The Martian Additionally, Martian1 contains an epilogue that is completely cut in the edit. It portrays Watney, back on Earth, being openly and profanely rude to a young fan. In Martian2, meanwhile, text is added to have Watney express gracious appreciation for all the parties involved in his rescue and a widespread faith in human nature. The edit therefore alters the tone of the ending substantially.We believe that all of these changes, analyzed together with close reading, serve to align Watney's character with our overall hypothesized goal of the edit: to make Watney more \"relatable,\" \"nice,\" and \"human,\" and thus to appeal to a wider audience. Edits Over the Course of the NovelPatterns in the edits related to textual progression are revealed by measuring the absolute Levenshtein distance of the script-identifiable copyedits and other edits line by line (Levenshtein distance is a metric for measuring the difference between two sequences, see Navarro 2001).  Sum of absolute Levenshtein distance per line  over textual progression (script-identifiable copyedits in red, other edits in blue). Figure 7 shows the sum of the absolute Levenshtein distances for each line of the novel (with SavitzkyGolay smoothing applied). The graph shows the substantial modifications to the ending of the novel, but also a large number of locations with smaller but nonetheless above-average modifications. Conclusion and Further ResearchWe have identified and analyzed a number of key features that emerged from the editing of The Martian, notably on the level of style and character, which combine to make the novel more appealing to a wider audience.Ongoing research into The Martian concerns the relative frequency and function of parts of speech, quantifying the amount of syntactic change, and the legal issues affecting the obtaining and processing of the texts. We hope to present these additional findings in the near future.As for our typology of edits, an established methodology for classifying edits in the companion fields of textual analysis and scholarly editing is the distinction between the \"accidentals\" and \"substantives\" used by the Greg-Bowers tradition and included in the MLA Committee on Scholarly Editions' Guidelines for Editors of Scholarly Editions (Modern Language Association, 2011). Scholars are not unanimous, however, in supporting this. G. Thomas Tanselle, for instance, found these terms \"misleading and often untenable in their implication of a firm distinction in all cases\" (Greetham 1992, pp.335-336). Further, there appears to be no widely-applicable typology of edits in digital scholarly editing and collation, with different materials calling for different typologies (see TEI-L 2016).Our typology of edits departs from previously proposed ones by focusing entirely on types which can be identified automatically, based on surface features. While limited in scope and excluding any semantic criteria, our typology may serve as a first approach to the edits of any text and allow quantitative comparison of some key phenomena. We believe that our method could be applied to other variants of fiction - by itself or incorporated alongside another taxonomy, including accidentals/substantives -particularly to novels which begin as self-published works but are later edited and re-released, an increasingly important phenomenon in contemporary fiction.",
        "article_title": "What Changed When Andy Weir's The Martian Got Edited?",
        "authors": [
            {
                "given": "Erik",
                "family": "Ketzan",
                "affiliation": [
                    {
                        "original_name": "University of London",
                        "normalized_name": "University of London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/04cw6st05",
                            "GRID": "grid.4464.2"
                        }
                    }
                ]
            },
            {
                "given": "Christof",
                "family": "Schöch",
                "affiliation": [
                    {
                        "original_name": "University of Würzburg",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionSpam, or unsolicited commercial communication, has evolved from telemarketing schemes to a highly sophisticated and profitable black-market business. Although many users are aware that email spam is prominent, they are less aware of blog spam (Thom- ason, 2007). Blog spam, also known as forum spam, is spam that is posted to a public or outward facing website. Blog spam can be to accomplish many tasks that email spam is used for, such as posting links to a malicious executable.Blog spam can also serve some unique purposes. First, blog spam can influence purchasing decisions by featuring illegitimate advertisements or reviews. Second, blog spam can include content with target keywords designed to change the way a search engine identifies pages (Geerthik, 2013). Lastly, blog spam can contain link spam, which spams a URL on a victim page to increase the inserted URLs search engine ranking. Overall, blog spam weakens search engines' model of the Internet popularity distribution. Much academic and industrial effort has been spent to detect, filter, and deter spam (Dinh, 2013), (Spirin and Han, 2012).Less effort has been placed in understanding the underlying distribution mechanisms of spambots and botnets. One foundational study in characterizing blog spam ( Niu et al., 2007) provided a quantitative analysis of blog spam in 2007. This study showed that blogs in 2007 included incredible amounts of spam but does not try to identify linked behavior that would imply botnet behavior. A later study on blog spam (Stringhini, 2015) explores using IPs and usernames to detect botnets but does not characterize the behavior of these botnets. In 2011, a research team (StoneGross et al., 2011) infiltrated a botnet, which allowed for observations of the logistics around botnet spam campaigns. Overall, our understanding of blog spam generated by botnets is still limited. Related WorkVarious projects have attempted to identify the mechanics, characteristics, and behavior of botnets that control spam. In one important study (Shin et al., 2011), researchers fully evaluated how one of the most popular spam automation programs, XRumer, operates. Another study explored the behavior of botnets across multiple spam campaigns ( Thonnard and Dacier, 2011). Others ( Pitsillidis et al., 2012) examined the impact that spam datasets had on characterization results. ( Lumezanu et al., 2012) explored the similarities between email spam and blog spam on Twitter. They show that over 50% of spam links from emails also appeared on Twitter. The underground ecosystem build around the botnet community has been explored (Stone-Gross et al., 2011). In a surprising result, over 95% of pharmaceuticals advertised in spam were handled by a small group of banks ( Levchenko et al., 2011). Our work is similar in that we are trying to characterize the botnet ecosystem, focusing on the distribution and classification of certain spam producing botnets. Experimental DesignIn order to classify linguistic similarity and differences in botnets, we implement 3 honeypots to gather samples of blog spam. We configure our honeypots identically using the Drupal content management systems (CMS) as shown in Figure 1. Our honeypots are identical except for the content of their first post and their domain name. Ggjx.org is fashion themed, npcagent.com is sports themed, and gjams.com is pharmaceutical themed. We combine the data collected from Drupal with the Apache server logs (Apache, 2016) to allow for content analysis of data collected over 42 days. To allow botnets time to discover the honeypots, we activate the honeypots at least 6-weeks before data collection.We generate three tables of content for each honeypot (Bevans and Khosmood, 2016). In the user table, we record the information the spambot enters while registering and user login statistics that we summarize in Table 1. This includes the user id, username, password, date of registration, registration IP, and number of logins. In the content table, we record the content of spam posts and comments which we summarize in Table 2. This includes the blog node id, the author's unique id, the date posted, the number of hits, type of post, title of the post, text of the post, links in the post, language of the post, and a taxonomy of the post from IBM's Alchemy API.   Lastly, in the access table, we include data and meta-data from the Apache logs. This includes the user id, the access IP, the URL, the HTTP request type, the node ID, and an action keyword describing the type of access.Our honeypots received a total of 1.1 million requests for ggjx, 481 thousand requests for gjams, and 591 thousand requests for npcagent. Entity ReductionIt is widely accepted that spambot networks, or botnets, are responsible for most spam. Therefore, we algorithmically reduce spam instances into unique entities representing botnets. For each entity, we define 4 attributes: entity id, associated IPs, usernames, and associated user ids. To construct entities we scan through the users and assign each one to an entity as follows.1. For a user, if an entity exists which contains its username or IP, the user is added to the entity. 2. If more than one entity matches the above criteria, all matching entities are merged. 3. If no entity matches the above criteria, a new entity is created.We summarize the entity characteristics in Table 3. The maximum number of users in one entity is almost 38 thousand for ggjx with over 100 unique IP addresses. These results confirm what is expected - the vast majority of bots interacting with our honeypots are part of large botnets. This also allows us to perform content analysis exploring what linguistic qualities differentiate botnets.  Content AnalysisTo better understand botnets, we use natural language processing (Collobert and Weston, 2008) for analyzing the linguistic content of entities. For our analysis, we consider various feature sets as proxies for linguistic characteristics as summarized in Table 4. We use a Maximum Entropy classifier (Mega M, 2016) to test which features differentiate botnets. In order to test a feature, we train the classifier with 70% of the posts, randomly selected, from the N largest entities and test it with the remaining 30% of the posts. Our final results are the average of three runs.The first feature set we test is Bag Of Words (BoW) which models the lexical content of posts. Put simply, each word in a document is put into a 'bag' and the syntactic structure is discarded. For implementation details, see our technical report (Bevans, 2016). In Figure  2, we show our analysis of the BoW feature set.When considering the top 5 contributing entities, the classification accuracy is less than 95% which implies that the lexical content of botnets varies greatly. The second feature we consider is the taxonomy provided by IBM Watson's AlchemyAPI. Alchemy's output is a list of taxonomy labels and associated confidences. For the purpose of our analysis, we discard any low or non-confident labels. In Figure 3, we show our analysis of the Alchemy Taxonomy feature set which highlights the accuracy of Alchemy's taxonomy. We note that the Alchemy Taxonomy feature set is dramatically smaller in size than the BoW feature set while still providing high performance. This indicates a full lexical analysis is not necessary but a taxonomic approach is sufficient. Our third feature is based on the links in the posts. To create the feature, we parse each post for any HTTP links and strip the link to its core domain name.The classifier with the link feature set had varied results, as shown in Table 5, where it was reliable in differentiating ggjx entities but less reliable for the other two honeypots. These results correlate with link scarcity from Table 2.  We test the normalized vocabulary size of a post as a feature. We derive this from the number of unique words divided by the total number of words in the post. As shown in Table 5, the vocabulary size does not differentiate botnets.We also form a feature set based on the part-ofspeech (PoS) makeup of a post using the Stanford PoS Tagger. The Stanford PoS tagger returns a pair for each word in the text, the original word and corresponding PoS. We create a BoW from this response that creates an abstract representation of the document's syntax. As shown in Table 5, the PoS does not differentiate botnets.  ConclusionsIn this paper, we examine interesting characteristics of spam-generating botnets and release a novel corpus to the community. We find that hundreds of thousands of fake users are created by a small set of botnets and much fewer numbers of them actually post spam. The spam that is posted is highly correlated by subject language to the point where botnets labeled by their network behavior are to a large degree re-discoverable using content classification (Figure 3).While link and vocabulary analysis can be good differentiators of these botnets, it is the content labeling (provided by Alchemy) that is the best indicator. Our experiment only spans 42 days, thus it's possible the subject specialization is a feature of the campaign rather than the botnet itself.",
        "article_title": "Understanding Botnet- driven Blog Spam: Motivations and Methods",
        "authors": [
            {
                "given": "Brandon",
                "family": "Bevans",
                "affiliation": [
                    {
                        "original_name": "California Polytechnic State University United States of America",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Bruce",
                "family": "Debruhl",
                "affiliation": [
                    {
                        "original_name": "California Polytechnic State University United States of America",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Foaad",
                "family": "Khosmood",
                "affiliation": [
                    {
                        "original_name": "California Polytechnic State University United States of America",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The Literariness of Topic ModelingThis short paper reports on the progress of my attempt to construct a reading of topic modeling using state-of-the-art literary criticism. I argue that dominant digital humanities understandings of topic models assume some of the characteristics of literature most essential to twentieth-century criticism - counter-factuality, a mediated form that is ultimately separable from aesthetic characteristics, and an efficient, self-enclosed, total form. More specifically, I show that topic models also tend to be read by digital humanists according to the assumptions, protocols, and caveats we accord to the interpretation of realist fiction. The result is that while often revealing and productive, many digital humanists' uses of topic modeling are indebted to assumptions about the literariness and fictionality of topic models that we have yet to fully understand. Drawing on work by Stephen Ramsay, Johanna Drucker, Alan Liu, and others that theorizes continuities between the values of literary criticism and computational processes, I suggest that we temporarily set aside the idea that topic modeling reveals the \"contents\" of a set of novels (or of any other corpus). Instead, drawing on Roland Barthes' late work on The Preparation of the Novel, we might rethink topics as preparatory notes written by no one, as an imaginary archive whose contents furnish a productively alienating, too-perfect map of the novel's preparation. In Preparation, Barthes moved away from his earlier work's emphasis on totalizing interpretations of literature's meaning to think about models of the text that allow for a more partial and slow view of the process of meaning creation (See Buurma and Hefferman, 2014) Topic modeling has the potential for helping us towards a Barthesian reimagination of the novel's reader as the novel's writer, of the search for the fantasy origins of a novel as a method that pulls us away from formal totality and a form-content divide. While this reorientation comes out of literary studies, I also suggest that it might have applications for more instrumental uses of topic modeling outside the realm of the humanities, in which assumptions about topics as equivalent to a document set's \"contents\" also tend to draw on our conventions for reading realist genres. Fictionality and the Topic ModelThe past few years have seen the rapid popularization of topic modeling among humanist scholars in general, and among scholars of literature in particular (see Blei, 2014;Erlin, 2014;Goldstone and Under- wood, 2014;Laudun and Goodwin, 2013;Meeks, 2013;Jockers and Mimno, 2013;Rhody, 2013 and2016;and Tangherlini and Leonard, 2013). The literature on topic modeling abounds in stern and salutary warnings about the limits and dangers of topic modeling for humanistic study. One can read about the dangers of introducing algorithmic black boxes into literary research, the concern that literary scholars are unprepared to fully (or even partially) interpret the topic models and their related data, and the worry that they fail to understand even the interpretive choices made during corpus preparation. Part of the worry derives from a larger assumption that topic modeling \"reveals\" the \"contents\" of novels. We assume that literary critics dipping their toes into topic modeling will shed their traditional interpretive caution in the face of the algorithm's authority, and will misunderstand the un-semantic nature of topics or accept meaningless correlations as meaningful. I want to suggest that all such warnings are relevant only given a very limited understanding of what a topic model is, its imagined relation to the corpus from which it derives, and the goals of the model's interpreter. These warnings do usefully help us think about some of the seemingly inconsequential interpretive choices we make when we choose chunk and clean documents, apply stoplists, select a number of topics to train, and, most importantly, assign semantic labels to unsemantically generated topics. And yet these warnings assume either that topic models aspire to be mimetic maps of the corpuses they model or that technologically unsophisticated interpreters of topic models imagine that this is the case. Schmidt (2013) warns, for example, that \"simplifying topic models for humanists who will not (and should not) study the underlying algorithms creates an enormous potential for groundless - or even misleading - \"insights.\"\" He worries that a pair of assumptions about topic models - that they are \"coherent\" and \"stable\" - \"let humanists assume that the cooccurrence patterns described by topics are meaningful; topics are useful because they describe things that resemble \"concepts,\" \"discourses,\" or \"fields.\"\" He is worried, that is, that the appearance of semantic meaning we find in \"good\" topics will seduce humanists into thinking that they have discovered the \"contents\" of novels - whereas what topic modeling really offers us is exactly a non-semantic machine indexing of a set of texts about which our approaches tend to be based on ground assumptions about semantic meaning. This is not surprising; the assumption that topic models are a realist genre is pervasive in literature on topic modeling, literary and otherwise (for example, Airoldi et al, 2015, describe good topics with the example of \"trout fish fly fishing water angler stream rod flies salmon…\" explaining that the topic \"is specific. There is a clear focus on words related to the sport of trout fishing. It is coherent. All of the words are likely to appear near one another in a document. Some words-water, fly-are ambiguous and may occur in other contexts, but they are appropriate for this context. It is concrete). Yet if we relieve ourselves of this constraint and instead substitute a more plausible frame - the topic model's fictionality - we will be able to enjoy a wider range of relations between model and corpus.In place of assuming that topic models belong to the realm of realism, then, we might pay more attention to the generative uncertainty of topic modeling and to its literal fictionality. Topics are probabilistically-created formations, and the algorithm that generates topic models is based on the enabling--but crucially, counterfactual--\"assumption that documents have multiple topics.\" (Boyd-Graber et al., 2015). By looking at the documents we offer it, the algorithm generates topics that, in given proportions, compose each document. (Or, rather, it generates the probability that a certain percentage of words in every given document were generated by a given particular topic.) Topics, of course, don't actually exist prior to the documents that generate them; they don't actually exist independently in the same way the documents at all. They are, in a certain sense, fictions. Topics are things that might have existed - but didn't! - given the existence of the document set in question. While we can and sometimes do relegate this fact to the realm of methodology, the fictionality of topics is crucial to remember for any literary-critical uses of topic modeling, for it reminds us that these models offer us a view of our document set radically at odds with any other more literal sources of a novel we might use - such as an author's notes towards a novel, or a catalog of the virtual or actual library of books a novelist brings to the writing table, or even the looser sense of social \"discourses\" that exist prior to novels and which we might imagine in part \"composing\" a novel. As Boyd-Graber et alia note, \"Topic models are based on a generative model that clearly does not match the way humans write. However topic models are often able to learn meaningful and sensible models.\" (2014: 15). Using a few targeted examples drawn from topic models of corpuses of nineteenth-century novels of varying sizes and comparing them to some examples of nineteenth-century novelists' notebooks, I suggest that reimagining topic models as fictional notes might be not just a theoretical exercise but a practical way of conceptualizing the relation between topic model and corpus. ",
        "article_title": "The Preparation Of The Topic Model",
        "authors": [
            {
                "given": "Rachel",
                "family": "Buurma",
                "affiliation": [
                    {
                        "original_name": "Swarthmore College",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionHow do people look at and experience art? Which elements of specific artworks do they focus on? Do museum labels have an impact on how people look at artworks? The viewing experience of art is a complex one, involving issues of perception, attention, memory, decision-making, affect, and emotion. Thus, the time it takes and the ways of visually exploring an artwork can inform about its relevance, interestingness, and even its aesthetic appeal. This paper describes a collaborative pilot project focusing on a unique collection of 17th Century Zurbarán paintings. The Jacob cycle at Auckland Castle is the only UK example of a continental collection preserved in situ in purpose-built surroundings. While studies of the psychology of art have focused on individual works and distinctions between representative/nonrepresentative topics, no work has been completed on the aesthetic appreciation of collections or of devotional themes. In this paper, we report upon the novel insights eye-tracking techniques have provided into the unconscious processes of viewing the unique collection of Zurbarán artworks. The purpose of this pilot study was to assess the effects of different written interpretation on the visual exploration of artworks. We will discuss the potential implications of these techniques and our understanding of visual behaviours on museum and gallery practice. The project brings together established research strengths in Spanish art history, experimental psychology, digital humanities, and museum studies to explore, using eye-tracking techniques, aesthetic reactions to digital representations of the individual Zurbarán artworks as well as the significance of the collection as a whole. OverviewOur experience of art develops from the interaction of several cognitive and affective processes; the beginning of which is a visual scan of the artwork. When regarding an artwork, a viewer gathers information through a series fixations, interspersed by rapid movements of the eye called saccades. The direction of saccades is determined by an interaction between the goals of the observer and the physical properties of the different elements of the scene (e.g. colour, texture, brightness etc). Importantly, studying eye movements offers an insight that does not depend on the participants' beliefs, memories or subjective impressions of the artwork. Previous eye tracking research has highlighted the potential to transform the ways we understand visual processing in the arts (see for example Brieber 2014;Binderman et al., 2005) and at the same time offers a direct way of studying several important factors of a museum visit (Filippini Fantoni et al., 2013;Heidenreich & Turano 2011;Milekic 2010).Zurbarán's cycle of Jacob and his Sons has been on display in the Long Room at Auckland Castle for over 250 years. It is the only cycle to be preserved in purpose-built surroundings in the UK, and one of very few of its kind in the world. It has a long history in scholarship (Baron & Beresford 2014), but many key aspects of its production and significance have not yet been fully understood. In this study we used eyetracking in the first stage of exploring audience experience of the extensive Spanish art collections of County Durham, of which the 13 Zurbarán artworks (there are actually only 12 Zurbarán artworks, the 13th Benjamin, is a copy by Arthur Pond) are a key part of, to investigate the ways in which audiences look at Spanish art, how aesthetic experience is evaluated and whether audiences can be encouraged to approach art in different ways. This pilot project primarily investigated how participants visually explore artworks and provides new insights into the potential eye-tracking has to transform the ways we understand visual processing in arts and culture and at the same time offer a direct way of studying several important factors of a museum visit, namely to assess the effects of label characteristics on visitor visual behaviour. MethodThe aim of this study was to determine whether the accompanying written context influences how digital artworks are visually experienced. Whether contextual information impacts on where participants first look (first fixation), if gallery labels influence the time participants choose to view artworks and, especially, whether it influences their aesthetic appreciation of the works. We expected viewing time for artworks and corresponding labels to be predicted by participants' subjective experiences, artwork related features, and contextual factors. Accordingly, we measured viewing time, fixation, and saccades for each artwork and corresponding label using a fixed eye tracking technology (Tobii TX300) in a laboratory setting.Forty Six students from the University of Durham participated in this study. All participants had normal or corrected vision, no formal training in arts or art history and received course credit for taking part. All participants gave informed consent. The study was approved by Durham University's Department of Psychology Ethics committee. A third of the participants were randomly assigned to the Museum Context group (nMC =16), who inspected digital images of the paintings in conjunction with the contextualizing labels currently in use at Auckland Castle, which rely heavily on relating the content of individual compositions to the words of Jacob in Genesis 49; a third to the Aesthetic Context group (nAC = 15), who received labels foregrounding issues of aesthetic and interpretive interest; and the final third to the No Context group (nNC = 15), who received only basic attribution data: title of composition, name and date of artist, date of composition, and nature of medium (i.e. \"oil on canvas\").All stimuli were taken from Auckland Castle's collection of Jacob and his Twelve Sons by Francisco de Zurbarán. Each participant viewed high-resolution digital reproductions of the original artworks presented on the Tobii TX300 screen based eyetracker. The stimuli were presented in the same sequence for all participants. ResultsPrevious authors have shown that when a human being is portrayed in a painting, gazing behaviour is mostly focused on the human figure, independently of contextual elements also depicted in the image. In particular, attention is given to the face area, and it plays a fundamental role in aesthetic judgment ( Ro et al., 2007;Massaro et al., 2012;Villani et al., 2015). Given these considerations, three key regions of interest (ROI) were identified; the head, the clothes and the contextualising element. Saccades and fixations were identified offline in Tobii Studio using the default algorithm (onset/offset criterion of 70 degrees/second and a minimum dwell time of 80ms).The key variables of interest for each ROI were (1) Frequency of First Fixation, (2) Time to First Fixation and (3) Total Fixation Duration.When comparing the first fixation data across the three participant groups (Museum Context group (MC), Aesthetic Context group (AC) and No Context group (NC)), it possible to see an interesting trend (Fig  1) that suggests that contextual labelling appears to change the proportion of participants fixating on the face. The study revealed that the AC labels succeeded in disbursing the gaze more effectively than those that are current MC labels. In all thirteen paintings, evidence shows that participant visual behaviour changed in response to the written interpretation. This suggests that an aesthetic context labelling approach is more successful in stimulating and/or training the gaze than one rooted in theological extrapolation. The pilot study also found that contextual labelling has a significant effect on influencing levels of aesthetic appreciation, and on the ways in which the gaze can be trained and/or manipulated to engage with areas of interest that would otherwise be overlooked. Reorienting the content of individual labels away from scripture and towards questions of aesthetics and interpretation produced a statistically significant reduction in aesthetic appreciation, which, given that the face is a key driver of aesthetic judgements, is consistent with the finding that the aesthetic context also reduced the participants dwell time on the face. This paper will also discuss a how participants identify and rank the artworks in terms of authenticity and value. By ranking compositions, we will crossreference attitudes with the prices (all different) paid by Bishop Trevor at auction in 1756, considering how aesthetic tastes have changed. Summary and ConclusionsTo date, studies of museum and gallery visitor behaviour primarily investigate how people behaviourally and cognitively respond to the design and layout of exhibits. However, they largely ignore the behavioural responses at the 'exhibit-face ' (vom Lehn and Heath 2006) or the 'fat moment' (Garfinkel 1967) of visitors' action. Eye-tracking techniques have provided novel insights into the unconscious viewing processes of the 'fat moment' of the unique collection of Zurbarán artworks. The study highlighted statistically significant variations in levels of aesthetic appreciation. More importantly, the experiments indicated that by changing the written interpretation gaze can be redirected towards areas of conceptual significance, challenging the face bias which traditionally plays a fundamental role in aesthetic judgment. ",
        "article_title": "Aesthetic Appreciation and Spanish Art: Insights from Eye-Tracking",
        "authors": [
            {
                "given": "Claire",
                "family": "Bailey-Ross",
                "affiliation": [
                    {
                        "original_name": "University of Portsmouth",
                        "normalized_name": "University of Portsmouth",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/03ykbk197",
                            "GRID": "grid.4701.2"
                        }
                    }
                ]
            },
            {
                "given": "Andrew",
                "family": "Beresford",
                "affiliation": [
                    {
                        "original_name": "Durham University",
                        "normalized_name": "Durham University",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01v29qb04",
                            "GRID": "grid.8250.f"
                        }
                    }
                ]
            },
            {
                "given": "Daniel",
                "family": "Smith",
                "affiliation": [
                    {
                        "original_name": "Durham University",
                        "normalized_name": "Durham University",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01v29qb04",
                            "GRID": "grid.8250.f"
                        }
                    }
                ]
            },
            {
                "given": "Claire",
                "family": "Warwick",
                "affiliation": [
                    {
                        "original_name": "Durham University",
                        "normalized_name": "Durham University",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/01v29qb04",
                            "GRID": "grid.8250.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis paper explores the development of the Personae tool (code available on Github), an interactive resource for exploring patterns of speeches by and mentions of characters in dramatic texts. Initially developed to examine works by Shakespeare, the tool has broad application to dramatic texts.Visualising the frequency, extent, and position of dialogue relating to a particular character presents users with a simple and immediate measure of that character's prominence within the play. The Personae tool enables users to select and visualise individual characters' involvement, producing a novel means of exploring large-scale structural, narrative, or character-focused patterns within the text.The tool is intended to facilitate character-based analysis and reveal structural patterns at the scale of the play. The tool was conceived with exploratory potential in mind, and is designed to allow users to customise the visualisation according to their particular interests or to follow a more speculative and disinterested reading of the play's character-based features.This deliberate aim emerged from the heuristic development process described below, and a desire to produce an extensible exploratory tool for dramatic texts. From an initial focus on using digital tools to visualise the tangling and disentangling of character names and identities in The Comedy of Errors, our interest broadened into exploring the potential for using character data to visualise larger structural and narrative patterns.We were also motivated by the use of network analysis and visualisation for scholarship on Shakespearean and other literary texts, including work by Yose et al (2016), Grandjean (2015), Moretti (2011), andStiller, et al. (2003). These analyses are similarly character-based and have yielded many interesting insights. But in the reduction of the textual data to nodes and edges (characters and their interactions), network analysis has obscured the temporal. The work of Xanthos, et al. (2016) maintains this temporal dimension, while exploring the dynamics of the character networks as they evolve. In contrast, by visualising the characters at the level of the play as a whole, we aim to preserve characters' locations within the space of the text, thereby enabling analysis of the dramatic time and structural duration of the play. Tool DevelopmentTool development took part in two phases. First, the data was extracted and transformed into a suitable format. The user interface was then designed using an iterative process that enabled the exploration of various approaches to data presentation and interaction. Data PreparationThe tool uses data contained in XML files provided in the New Variorum Shakespeare editions of The Comedy of Errors and The Winter's Tale.Data was extracted using a custom-developed Python script which iterates through each play's XML file extracting character and name data, along with line number, scene and act identifiers. The data as output as JSON, which reduces the complexity of using it with the JavaScript-based user interface. User Interface DesignThe tool's web-based user interface (Figure 1) was developed using the open source Javascript library, D3 (Bostock et al, 2011). Personae developed from a fixed and static visualisation of The Comedy of Errors to a more interactive and exploratory tool. In the heuristic spirit of the tool itself, we describe here its various iterations, the stages of its development, and the motivations for various changes to its design and functionality throughout the process. Personae's focus on character and temporal visualisation is present in the first iteration of the tool (Figure 2). Speeches and mentions are plotted along a timeline, with a tabular view switching between the five acts of the play. All speeches and mentions are colour-coded, resulting in some interesting patterns and densities at certain parts of the text, but lacking the facility for isolating chosen characters. In addition, the tabular view of the five acts lacked the desired holistic view of the entire play.  Expanding the Second IterationThe second iteration of the tool adopted the circular layout of the tool to plot character involvement across the entire play, as shown in Figure  3. At this point, the tool was still static, and its focus on the two pairs of twin characters in The Comedy of Errors represented a desire to deploy visualisation for a particular exploratory purpose. The play operates on the basis of identity and confusion, as Antipholus and Dromio of Syracuse are mistaken for their Ephesian counterparts, and vice versa. Our aim was to plot the speeches of these four characters to see if the visualisation revealed any insights into how the identity question was introduced and managed at a structural level. An additional avenue of exploration in the second iteration of the tool was the geographical mapping of locations mentioned in the play. The Comedy of Errors is known for including the only mention of America in Shakespeare's plays, among several other placenames in its text. In some respects, this visualisation gives a false impression of The Comedy of Errors as a worldly play. While eighteen locations are mentioned in the text, several of these are ironically located by Dromio of Syracuse on Nell the kitchen-maid's body, because \"she is sphericall, like a globe: I could find out / Countries in her\" (Act 3, Scene 2). The final interfaceAs useful as this view of the play proved, we felt at this point that a more dynamic and interactive interface was required to allow users to test hypotheses like our own, or to undertake more exploratory and experimental visualisations of the data, as illustrated in Figure 4. The circular layout was retained, as it provided a useful method of presenting the play as a whole, while maintaining the temporal dimension of the character interactions. The character-selection menu and the scene-divisions in the outer ring were thus added in the final stage of development. Also added were visualisations of higher level metrics to illustrate the number of times a character speaks, and the number of lines they speak. ConclusionA major part of the tool's value is its extensibility. It may be used to create character visualisations for any play which is XML-encoded according to quite minimal specifications, and offer the opportunity to undertake comparative analysis of structural, narrative, and character-based patterns in different plays. Indeed, while the development of the tool focused on The Comedy of Errors, a similar visualisation of The Winter's Tale ( Figure 5) was generated from New Variorum Shakespeare XML files with no revision to our code. The trajectory of Personae's development from fixity to interactivity represents a conclusion that we drew in the course of this project: that a visualisation tool developed for a particular purpose need not be confined to its use for that objective alone. The modular and open-source principles of software development have contributed to a rich and fruitful habit of sharing within the field of Digital Humanities, and we hope that others will build upon the tool that we have developed here.Indeed, we have plans for further developments and improvements to Personae. Working towards a tool which will enable structural and thematic comparison of the thirty-six plays in the First Folio, the next phase will test for structural correlations in a thematic grouping of five additional Shakespearean plays. This development will strengthen Personae's potential for generating insights into macro-level structural analysis of dramatic texts, while testing its technical extensibility by incorporating XML files from another source, The Bodleian First Folio.",
        "article_title": "Personae: A Character- Visualisation Tool for Dramatic Texts",
        "authors": [
            {
                "given": "Justin",
                "family": "Tonra",
                "affiliation": [
                    {
                        "original_name": "National University of Ireland",
                        "normalized_name": "National University of Ireland",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/00shsf120",
                            "GRID": "grid.9344.a"
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "Kelly",
                "affiliation": [
                    {
                        "original_name": "National University of Ireland",
                        "normalized_name": "National University of Ireland",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/00shsf120",
                            "GRID": "grid.9344.a"
                        }
                    }
                ]
            },
            {
                "given": "Lindsay",
                "family": "Reid",
                "affiliation": [
                    {
                        "original_name": "National University of Ireland",
                        "normalized_name": "National University of Ireland",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/00shsf120",
                            "GRID": "grid.9344.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionWithin the context of social network analysis (SNA) for literary texts the automatic detection of family relations and similar social relations between characters in novels would be an important step for any macroscopic analysis. Manual labeling is rather inefficient since the text snippets that explicitly describe a relation are sparse within the long text documents; therefore we combine two techniques, active learning and distant supervision, which are often used to overcome data sparsity.Inspired by distant supervision which uses a high quality information resource to support information extraction from other data, we used expert summaries of literary texts, German novels mainly from the 19th century, since relevant text snippets are much more frequent in summaries. Then we applied an uncertainty-based active learning strategy labeling selected sentences from the novels and the complete summaries. The results show that training on summaries and evaluating on data derived from novels yields reasonable results with high precision and low recall similar to humans solving this task. After a brief discussion of related work in the next section, the data set and the necessary preprocessing for this work are explained in section three. Section four describes our method in detail and shows strengths and weaknesses. Related workThe challenge of training an algorithm capable of generalizing from a small set of manually labeled data has created a multitude of approaches like active learning and distant supervision. A good survey on active learning is given in ( Finn et al., 2003). Usually it starts with a seed set of manually annotated data. A classifier is then trained and new instances that appear to be very different from the current training data are proposed for manual labeling until the quality of the classifier stops improving. Successful algorithms include Multi-instance Multi-label Relation Extraction ( Surdeanu et al., 2012).Another method specifically used for relation detection in newspapers is distant supervision ( Mintz et al., 2009): Given some facts (e.g. Michelle Obama is the wife of Barack Obama), usually stored in a database, the aim is to match those facts to the text (e.g. every sentence containing Michelle and Barack Obama indicates that they are married). The training of the classifier is then performed on the pseudo gold data. Even though the idea appears to be simplistic, the results are comparable to those obtained by active learning. Jing et al. (Jing et al. 2007) successfully applied relation extraction for SNA in an end-to-end manner and reported that most problems were caused by coreference resolution. Data and preprocessingWe created three datasets from 213 expert summaries, available from Kindler Literary Lexicon Online, and 1700 novels derived from project Gutenberg and annotated relations between characters:• We split 500 novels into sentences and applied an uncertainty-based active learning strategy (explained below) to iteratively select new examples (in this case full sentences) using a MaxEnt classifier. In total, about 1100 sentences were labeled in this way. This was labeled by annotator 1. (From now on, we refer to this as the novel data set) • We split our summaries into sentences and applied the same active learning strategy to select new examples, thereby generating about 1300 labeled sentences. They were labeled by annotator 1. (From now on, this is called summaries I) • Each of the 213 summaries has been manually labeled with all character references, the co-reference chains amongst them and relation annotations for pairs of entities that are explicitly mentioned to be in a relation. They have been labeled by annotator 2. (From now on, we call this summaries II)The applied active learning strategy started by manually selecting about 20 seed training sentences which were manually labeled with information about relations between character references. The seed examples were chosen by matching a wordlist containing indicative expressions (such as \"mother\", \"father\", \"servant\" or \"loves\") to the text, to enable the classifier to learn relations from different relation types in an unbiased fashion (which usually changes during training because the underlying distribution of relations is heavily biased towards family relations).On those seed examples we trained a binary Maximum Entropy classifier which was applied to thousands of unlabeled sentences. The sentences were then ranked by uncertainty of the classifier. Uncertainty for a sentence, in our case, was defined by extracting all pairs of character references first, applying the classifier to every pair and then assigning the minimum probability to the sentence. The classifier was retrained on command of the user and the ranking of the unlabeled sentences restarted. By applying this strategy, we observed that the average certainty of a sentence rises with every iteration and decided to stop the manual labeling once there was no sentence with a classifier probability below 60% for the novels and 70% for the summaries (this does not mean we reached saturation in classification gain). For the labeling, we used a total of 57 hierarchically ordered relation labels, inspired by (Massey et al., 2015) (see figure 1). All these labels relate personentities with each other, such as \"motherOf\" or \"loves\".  : The four main relation types which are further differentiated in 57 relation types in totalThe inter-annotator-agreement (IAA) between summary data set I and summary data set II was measured in two ways:1. A true positive appears when both annotators mark the correct span of the annotation as well as the correct label and the correct arc direction where a correct arc links the two entities in the direction as it is expressed in the text (labeled interannotator agreement). 2. A true positive appears when both annotators mark the correct span and arc direction of the relation (unlabeled interannotator agreement). Table 1 gives an overview of the IAA results. Additionally, we determined 55.5% as the normalized Cohen's Kappa between our annotators. The results for the IAA are surprisingly low (amount of labeled relations in summaries I compared to the relations in summaries II). The reasons are yet unclear and have to be investigated; we assume that one of them is the high variance of possibilities to express social relations. Labeling the complete summaries may also be more difficult because the annotator needs to read the text completely and might use background knowledge to annotate relations which are only implicit in the text. Method and evaluationTo compare the transfer from summaries to novels, we trained a classifier, specifically a maximum entropy classifier based on boolean features generated from rule templates because previous work has shown that this classifier is superior in classification accuracy compared to kernel machines, pure rule based approaches or other supervised classifiers such as support vector machines ( Krug et al., 2017). Training was done on a data set using the annotations as features and the classifier was applied either to test data from the same set or to a different data set resulting in three evaluations:• A 5-fold cross evaluation within the novel data set.• Training on the snippets of the summaries (summaries I or summaries II) and evaluation on the novel data set. Table 2 shows the result of this experiment for the indata and cross data evaluation of the relation detection component. The results of a 5-fold in-data set evaluation for both of the data sets and the results for a cross-data set evaluation. Each number represents a micro-average score, i.e. we count every true-positive, false-positive and falsenegative in a document and calculate the average scores based on these quantities. We choose the micro score since the label set is rather unbalanced between classes. The efficiency of an approach is measured by calculating number of relations / number of sentences.Results that are very similar to working directly on the novel (60.2% F1) are achieved by using the model trained on the extracted sentences from the summaries to retrieve information about character relations in the novels (62.1% resp. 59.2%). Since our test data is generated by active learning and only the most difficult examples were chosen for labeling, we expect our results to be a lower bound compared to data in complete novels.If we use a model trained on the complete summaries, we experience a drop in precision. This drop was to be expected, since the amount of additional labeled relations in the novels is high according to the IAA results (this manifests in the low recall in table 1) as well as can be seen in the labeling efficiency. Altogether, the quality and efficiency of using a classifier trained on summaries are comparable to training on the novels directly based on our data. SummaryWe presented an approach to increase labeling efficiency for relation detection in German novels by transferring knowledge from summaries to novels. It could be shown that using the summaries as trainings data will achieve similar results to using the novels, but the summaries are much shorter and relevant sentences are much more frequent. The interannotator agreement for this task is also relatively low which may point to an explanation for the comparatively low results of the automatic approach.",
        "article_title": "Overcoming Data Sparsity for Relation Detection in German Novels",
        "authors": [
            {
                "given": "Markus",
                "family": "Krug",
                "affiliation": [
                    {
                        "original_name": "University of Wuerzburg",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Isabella",
                "family": "Reger",
                "affiliation": [
                    {
                        "original_name": "University of Wuerzburg",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Fotis",
                "family": "Jannidis",
                "affiliation": [
                    {
                        "original_name": "University of Wuerzburg",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Lukas",
                "family": "Weimer",
                "affiliation": [
                    {
                        "original_name": "University of Wuerzburg",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Nathalie",
                "family": "Madarász",
                "affiliation": [
                    {
                        "original_name": "University of Wuerzburg",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Frank",
                "family": "Puppe",
                "affiliation": [
                    {
                        "original_name": "University of Wuerzburg",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe question of minimal sample size is one of the most important issues in stylometry and nontraditional authorship attribution. In the last decade or so, a few studies concerning different aspects of scalability in stylometry have been published (Zhao and Zobel, 2005;Hirst and Feiguina, 2007;Stamatatos, 2008;Koppel et al., 2009;Mikros, 2009;Luyckx and Daelemans, 2011), but the question has not been answered comprehensively. In his recent study, Eder proposed a systematic approach to solve the problem in a series of experiments, claiming that a sample should have at least 5,000 running words to be attributable (Eder, 2015).The above studies (and many other as well) tacitly assume that there exists a certain amount of linguistic data that allows for reliable authorial recognition, and the real problem at stake is to determine that very value. However, one can assume that the authorial fingerprint is not distributed evenly in a collection of texts. Just the contrary, many experiments seem to suggest that the authorial voice is sometimes overshadowed by other signals, such as genre, gender, chronology, or translation. Some authors, say Chandler, should be easily attributable, while some other authors, say Virginia Woolf, will probably have their fingerprint somewhat hidden. Moreover, authorship attribution is ultimately a matter of context: telling apart Hemingway and Dickens will always be easier than distinguishing the Bronte sisters. On theoretical grounds, then, the minimal sample size can not be determined once and forever for the entire corpus, but may be different for different texts in the corpus. Method and DataTo scrutinize the above intuition, a controlled experiment has been designed, in which particular text samples were assessed independently (one by one) and compared against the corpus. A following procedure was applied: the entire corpus served as a training set, out of which one text at a time was excluded. This temporarily excluded text was further pre-processed: in many iterations, longer and longer samples of randomly chosen words were excerpted (100 independent samples in each iteration), and then tested against the training set. In each iteration, the total number of correctly \"guessed\" authorial classesa single value between 0 and 100 -was recorded, resulting in a row of accuracy scores for a given text as a function of its sample size. The same procedure was repeated for each text in the corpus. The above setup does not need to be supplemented by any crossvalidation, because the experiment itself is a variant of a leave-one-out cross-validation scenario. Moreover, each text is re-sampled several times, which can be perceived as an additional way of neutralizing potential model overfitting.The experiments were repeated a few times. (Burrows, 2002). However, Delta was used as a general classification framework combined with a few custom kernels that seem to outperform the original setup. These included Cosine Delta ( Evert et al., 2016), min-max measure ( Kestemont et al., 2016), Eder's Delta ( Eder et al., 2016), and, obviously, the original measure as introduced by Burrows and mathematically justified by Argamon (2011). Secondly, all the tests have been repeated for different vectors of input features, or most frequent words: 100, 200, 300, 500, 750 and 1,000. While the choice of the vectors' lengths was arbitrary, it was aimed to follow usual stylometric scenarios in their various flavors, ranging from a considerably short list of mostly frequent words, to a longish vectors overwhelmed by content words. Firstly, three different classification methods have been tested: Support Vector Machines (SVM), Nearest Shrunken Centroids (NSC), and a distance-based learner that is routinely used in authorship attribution tests, namely Burrows's DeltaThe aforementioned method of testing was applied into two roughly similar corpora (one at a time): a corpus of 100 English novels by 33 authors (male and female), covering the years 1840-1940, and a similar corpus of 100 Polish novels. Both corpora, referred to as the Benchmark Corpus of English and the Benchmark Corpus of Polish, have been compiled by Jan Rybicki (pers. comm.). The corpora used in the experiment, as well as the complete code needed to replicate the study, will be available in a GitHub repository. ResultsA lion's share of tested samples revealed a very consistent and clear picture. According to intuition, the performance for short samples falls far beyond any acceptance rate, sometimes showing no correct \"guesses\" at all. This is followed, however, by a very steep increase of performance which immediately turns into a plateau of statistical saturation, despite the number of analyzed features (frequent words). An example of such a behavior is The Ambassadors by Henry James (Figure 1), as well as many other novels by Blackmore, Chesterton, Foster, Lytton, Meredith, Morris, Thackeray, and Trollope. As one can see, the amount of text needed for a reliable attribution is less than 2,000 words (!), an amount radically smaller than the previous study suggests (Eder, 2015). Sometimes the picture is somewhat blurry, nevertheless the same general shape reappears, as in the case of Felix Holt by George Elliot (Figure 2). As one can see, using shorter vectors of features requires longer samples to extract the authorial profile.  Optimistic as they are, however, the results might differ significantly. E.g., in some cases, the statistical saturation does not really take place, even if very long samples are used (Figure 3: scores for Saints Progress by John Galsworthy). What is more important, however, the final results additionally depend on the number of analyzed features. In Figure 4, a representative example of this behavior has been shown, namely Bleak House by Dickens.   Last but definitely not least, there are a few texts that are never correctly attributed, no matter how long the extracted samples are ( Figure 5). The question why some novels were misclassified will be addressed in a separate study. Here, it should be emphasized that such a behavior is unpredictable. Certainly, it can be easily detected, as long as one tests novels of known authorship; it becomes an obstacle, however, when one tries to scrutinize an anonymous text. Detecting OutliersThe outcome of the above experiment shows that the minimal sample size can be lowered substantially, from ca. 5,000 running words as suggested previously (Eder, 2015), to less than 2,000 words. However, this is true only for those texts that exhibit a clear authorial signal; otherwise the risk of severe misclassification appears. To take advantage of the above results, then, one has to be sure which category an analyzed text belongs to. In a controlled experiment, the task is simple, in a real-case attribution study, however, one has no chance to fine-tune the model by testing the disputed sample against the corpus. What if an anonymous text does not reveal a clear accuracy curve, as the one in Figure 1?To overcome the sample size issue of unknown texts, an additional measure can be involved to supplement the accuracy scores. (Due to limited space in this abstract, a compact outline of the proposed solution will be presented, rather than a complete algorithm). In the case of misclassification, one would like to know if the wrong response is consistent, or if different classes were assigned chaotically. To address this question, an indicator of consistency would be useful. The Simpson index is a very simple measure of concentration when observations are classified into a certain number of types (Simpson, 1949):λ = Σpi2where pi is the proportion of observations belonging to the ith type. The index can be easily adopted to indicate imbalance between assigned classes in supervised classification. To this end, the obtained classification scores (for a given sample size) have to be divided by the total number of trials (in this case, 100). The value 1 reflects purely consistent results, lower values mean that the assigned classes were fuzzy. To make a long story short: the texts that distribute their accuracy curves as in Figure 1 will also exhibit the same shape of the diversity index (see Figure 6). However, when the accuracy scores are low and/or ambiguous, the diversity index might provide a priceless hint. It is especially important when the accuracy scores are consistent (Figure 5), and the Simpson index is not (Figure 7). Instead of being mislead (\"Stevenson did not write Catriona\", which is not true), we are warned that the classification is inconsistent. Thus, to reliably test a minimal size of a disputed text, one has to take into account two values (accuracy and diversity). The bigger the dispersion between the indices, the smaller the probability that the text is attributable - perhaps a longer sample has to be involved, or a different set of features? ConclusionThe study was aimed at re-considering the minimum sample size for reliable authorship attribution. The results of the experiments suggest that a sufficient amount of textual data may be as little as 2,000 words in many cases. However, sometimes the authorial fingerprint is so vague, that one needs to use substantially longer samples to make the attribution feasible. A question of some importance is to which category an unknown (disputed) text belongs. ",
        "article_title": "Short samples in authorship attribution: a new approach",
        "authors": [
            {
                "given": "Maciej ",
                "family": "Eder",
                "affiliation": [
                    {
                        "original_name": "Institute of Polish Language, Polish Academy of Sciences, Poland",
                        "normalized_name": "Polish Academy of Sciences",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/01dr6c206",
                            "GRID": "grid.413454.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Research QuestionsHrómundar saga Gripssonar traditionally belongs in the corpus of legendary sagas (fornaldarsö gur); it was included in the second volume of Rafn's (1829) Fornaldarsögur Norðrlanda, and in Bjö rner's (1737) Nordiska kämpa dater. The saga as it is known today, however, is a post-medieval re-working of a metrical version of the story known as rı́murrı́mur (Brown, 1946), and is probably not much older than seventeenth century. Therefore it does not necessarily fit well with the other texts included the corpus of legendary sagas, as they usually date from the fourteenth and fifteenth centuries (Driscoll, 2005:207). This makes Hrómundar saga Gripssonar an interesting case study for investigations of the text's genre affiliation in the extant manuscripts preserving the saga. Does it appear frequently in manuscripts with the older legendary sagas or with younger rı́murrı́mur-based narratives? To answer this question, I first examine the manuscript context of legendary sagas as a corpus, based on collaborative research with Rowbotham and Wills ( Kapitan et al., 2017); second, I examine the position of Hrómundar saga Gripssonar within the corpus and its relationships with other texts. State of the ArtMuch discussion in the field of Old Norse studies centers on whether the legendary sagas deserve to be considered a separate literary genre, or should instead be analyzed as chivalric literature (Quinn, 2006). One of the main reasons for these considerations seems to be the fact that the term fornaldarsö gur is not attested in the medieval texts; it was introduced in the early nineteenth century by C.C. Rafn, who published a collection of texts under the title Fornaldarsögur Norðrlanda (Rafn, 1829). Rafn's selection of texts and the definition of fornaldarsö gur as a corpus of texts dealing with events taking place in Scandinavia before the settlement of Iceland, however, was not detached from pervious scholarship of early eighteenth century (Lavender, 2015). The current discussion on the legendary sagas as a corpus (or a genre) is polarized around contradicting opinions. Some scholars suggest that the legendary sagas had to be considered a separate category in pre-modern period, because they are frequently bound together in the manuscripts (GuðmundsdóttirGuðmundsdóttir, 2001:cxlvii;Mitchell, 1991:21) while others, using the same argument, emphasize strong connections between the legendary sagas and the chivalric sagas ( Driscoll, 2005:193). Additional problems arise when classifying the generic hybrids (Rowe 1993;2004) appearing within the corpus, or distinguishing between the legendary sagas (fornaldarsö gur) and the late legendary sagas (fornaldarsö gur sı́ðarisı́ðari tı́matı́ma; Driscoll 2005). Even though scholars eagerly turn towards the manuscript context to support their claims regarding the genre classification, no comprehensive overview of the legendary sagas' codicological context has yet been presented. This gave rise to the project Stories for all times conducted at the University of Copenhagen, which created a complete catalogue of manuscripts preserving legendary sagas. The catalogue contains 818 TEI-conformant XML-based manuscript descriptions with over 8000 items, 1764 of which are classified as legendary sagas and 920 as chivalric sagas. This amount of data is much too large to be analyzed manually, therefore it is necessary to apply computer-assisted analysis in order to draw some general conclusions regarding this corpus and the relationships between these texts. MethodsThe first part of my paper, which aims to establish the position of Hrómundar saga Gripssonar within the wider context of the manuscript, draws on the network analysis of the corpus, conducted in collaboration with Rowbotham and Wills (Kapitan et al., 2017). There, the codicological context of a text was considered as a system of relationships between texts, and following Hall's (2013) approach in his network of chivalric sagas, texts were represented as nodes, manuscripts as edges, both visualized with the free visualization software Gephi. The second part is based on database queries aimed at obtaining detailed information about particular manuscripts and their contents. The main focus of the analysis was to examine the manuscripts preserving the complete texts of Hrómundar saga Gripssonar in Icelandic, therefore the manuscripts containing excerpts and translations were ignored. The distribution of texts appearing frequently alongside Hrómundar saga Gripssonar by century has been obtained using XPath queries of the online catalogue Stories for all times. Main FindingsAs a result of this research, the hypothesis can be confirmed: generally, texts belonging to one genre appear most frequently in manuscripts with other texts belonging to the same genre. However, an interesting transmission history of Hrómundar saga Gripssonar suggests a close association of this saga with the late legendary sagas, and in particular Bragða-Ölvis saga. Both Bragða-Ölvis saga and Hrómundar saga Gripssonar are post-medieval reworkings of older metric versions of the stories (rı́murrı́mur), and for both texts the manuscript AM 601 b 4to (Aj rni MagnússonMagnússon Institute, Reykjavı́kReykjavı́k) was suggested as the witness carrying the best text of the saga (Andrews, 1911;Brown, 1946;Hooper, 1934;Hooper, 1932). Even though Hrómundar saga Gripssonar appears most frequently with texts classified as late legendary sagas in pre-1800 manuscripts, after 1800 the texts classified as (traditional) fornaldarsö gur start to dominate. The late Bragða-Ölvis saga dominates the pre-1800 setting, but the distribution changes in the nineteenth century when Þorsteins saga Víkingssonar, Starkaðar saga gamla, Friðþjófs saga ins fraekna, and Hálfs saga Hálfsreka appear more frequently (as presented on figure below). Starkaðar saga gamla is a lateeighteenth century saga written by Snorri Bjö rnson (1710-1803), utilizing traditional legendary motifs of Saxo's Gesta Danorum and legendary sagas (Driscoll, 2009:209;Simek and Hermann Paísson, 1987:331), so its co-occurrence with other legendary sagas starting from the eighteenth century onwards is not surprising. The three remaining texts that started to appear more frequently alongside Hrómundar saga Gripssonar in nineteenth-century manuscripts were all published in the same volume of Rafn's Fornaldarsögur Norðrlanda, in which Hrómundar saga Gripssonar was published (volume II); likewise FriðþjófsFriðþjófs saga ins fraekna, and Hálfs saga Hálfsreka appeared in Bjö rner's edition from 1737 together with Hrómundar saga Gripssonar. This shows how printed editions influenced the saga's transmission in the manuscript form. A text, which once showed strong connections to another rı́murrı́mur-based narrative, became detached from its previous setting and gained new, print-influenced context after becoming part of printed editions.  RelevanceThe topic of this paper fits in the advertised panel \"Quantitative stylistics and philology, including big data and text mining studies,\" as it employs database quarrying and network analysis of significant amount of data.",
        "article_title": "Network analysis of the manuscript context of Old Icelandic literature",
        "authors": [
            {
                "given": "Anna",
                "family": "Katarzyna",
                "affiliation": [
                    {
                        "original_name": "University of Copenhagen",
                        "normalized_name": "University of Copenhagen",
                        "country": "Denmark",
                        "identifiers": {
                            "ror": "https://ror.org/035b05819",
                            "GRID": "grid.5254.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The Cultural Heritage Data Reuse Charter: an encompassing cooperation frameworkThe Cultural Heritage Reuse Charter is an online environment dedicated to all actors taking part in scholarly reuse of digital data generated by Cultural Heritage Institutions. It addresses five actors: Cultural Heritage Institutions, Cultural Heritage Labs, Researchers, Data Centers and Research Institutions.• Cultural Heritage Institutions (GLAM) are considered in their function as curators of collections and objects in their physical form and as potential primary initiators of corresponding digital surrogates, from basic descriptions (catalogues of collections, metadata for specific objects) to more elaborate outputs (scans, 3D models, physical analyses, etc.) (Ray, 2014).• Primary data can be hosted by CHIs or by Higher Education Institutions like universities, but they are in many cases curated by dedicated data centers. These centers play a key role in guaranteeing the stability, the visibility and the long time availability of the primary data. The engagement expected from them in the context of the Charter is of a more technical nature and should ensure a concrete implementation of the CHI-researcher relationship.• Cultural Heritage laboratories have a highlevel expertise in Cultural Heritage. They give essential insights into Cultural Heritage history, technologies, environment, and alteration.• Researchers are invited to sign in person, independently from the institution for which they are working at the time they sign the Charter. However, academic institutions (departments, universities, research institution or funding agencies) wishing to sign the Charter, or even make it a requirement for their members or the projects they fund or host, are welcome to do so as well.The Charter environment allows all five actors to declare general principles (common work ethics), and more broadly all the relevant information needed to understand how a given dataset can be reused. It allows its users to get in contact with partners they would want to work with. Institutions can declare their collections; researchers their research interests and existing publications so that these are connected together. Doing so, all of them always have the possibility to define precisely which aspects of their profile information they wish to make public and which not.By joining forces, and by sharing the information associated to Cultural Heritage collections, the Charter will help document the knowledge generation process and, consequently, increase the quality of data and metadata accessible to research.Signing the Charter implies making a statement about the technical quality of the data to be reused, or the data derived by such a reuse. The implementation of appropriate standards is considered a key node for the stabilization of data access (Romary, 2011). More broadly, the Charter offers a concrete implementation framework for the FAIR principles (make the data findable, accessible, interoperable and reusable). The online environment in practiceThe principles described below are addressed by a series of components allowing to define the conditions of reuse for each type of data. The Charter environment offers a framework that can be either picked among a set of recommendations or formulated in a text field by the concerned institutions or actors according to their needs and wishes.This framework encompasses all questions related to the reuse of Cultural Heritage Data:• Long-term and persistent access to metadata, texts, images (in the case of a manuscript for instance: archival metadata, scan of the manuscript, transcription, annotation)• Licensing of the content (linking to relevant documentation allowing for instance researchers to gather information on licensing and citation practices they often lack)• Formats and standards (also connecting to further information) • Enrichments (connection of scholarly work and CHI work) • Dissemination of both CHI information and research (visibility of the work of all stakeholders) • Retro-provision (communicating enrichments based on CHI data to the CHI they originally emanate from)• Quality control at all levels according to appropriate standards.In practice, users of the Charter register in the online environment in their primary function as Cultural Heritage Institution, Researcher, Cultural Heritage Lab, Data Center or Research Institution. Identification of entities are realized on the basis of existing standards such as ORCID for researchers.In the researcher profile, three main areas are to be defined by the registered user. First, he/she has to abide to the reuse principles defined by the Cultural Heritage Institutions regarding the collection he/she wants to work on; this is the \"use of primary data\" area. Second, he/she has to declare the dissemination principles he/she favours. In this \"dissemination of secondary data\" area, he/she can gather information on licences. The third area is that of the \"cooperation ethics\", in which the researcher declares that he/she will follow best practices in citing the other Charter partners involved in his/her endeavour. This threefold profile is the basis on which the researcher can reach out to institutions or collections he/she wishes to work with. TimeframeThe Cultural Heritage Reuse Charter is currently under development. Workshops in which information will be gathered especially on the expectations of Cultural Heritage Institutions will take place in Berlin (November 2016), Paris (November 2016), Rome (January 2017) and Dublin (February 2017). Additional input from the other actors is gathered in parallel (interviews). A soft launch of the web interface is planned for the summer of 2017, so that the interface as well as the benefits for the first signatories can be demonstrated in Montreal .This abstract is in English in order to reach the widest possible community. Presenting the paper in French or having the slides to the presentation in French would be possible as well.  ",
        "article_title": "Access to cultural heritage data: a challenge for the Digital Humanities",
        "authors": [
            {
                "given": "Anne",
                "family": "Baillot",
                "affiliation": [
                    {
                        "original_name": "Centre Marc Bloch",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Marie",
                "family": "Pures",
                "affiliation": [
                    {
                        "original_name": "Institut national de recherche en informatique et en automatique (INRIA)",
                        "normalized_name": null,
                        "country": "France",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Charles",
                "family": "Riondet",
                "affiliation": [
                    {
                        "original_name": "Institut national de recherche en informatique et en automatique (INRIA)",
                        "normalized_name": null,
                        "country": "France",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Laurent",
                "family": "Romary",
                "affiliation": [
                    {
                        "original_name": "Institut national de recherche en informatique et en automatique (INRIA)",
                        "normalized_name": null,
                        "country": "France",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " BackgroundDespite the variety and breadth of definitions of DH (e.g., Gold 2012 andTerras 2012), narratives of its history have been surprisingly homogenous. Hockey (2004) and later authors (Svensson 2009(Svensson , 2010(Svensson , 2012Kirschenbaum 2010;Dalbello 2011) all ground DH in mid-20th century humanities computing, a view that is all but orthodox in short and anecdotal histories of the field. According to this narrative, DH begins in 1946 with the Index Thomisticus and proceeds through advances in corpus linguistics to the founding of the journal Computers and the Humanities (CHum) in 1966. These early projects are hindered by storage capacity, hardware costs, and processing limits; progress is slow. Though Svensson (2009) admits that not every article during this time is about text analysis, he notes that the field had narrowed enough by 1986 for Literary and Linguistic Computing (LLC) to supplant CHum as the premier humanities computing journal (note the journal titles). Hockey similarly describes the 1970s and 1980s as a period of \"consolidation\" of text analysis methods. As storage and processing capabilities increased from the late 1970s onward, structured electronic text and multimedia archives dominated the field, followed in the 1990s by Internetenabled hypertexts, digital libraries, and collaborative editing. The overarching theme of this narrative is text, with the plot revolving around corpora of increasing size and susceptibility to machine analysis.Though this account dominates historical views of the field, it raises four separate concerns. First, it privileges certain disciplines, projects, and tools at the expense of others (e.g., quantitative history, which is absent from the narrative). Second, it fails to chart an actual historical path from early work in text analysis to \"big tent\" DH (Jockers & Worthey 2011;Pannapacker 2011a, b), encompassing everything from digital archives and databases to GIS, network analysis, new publishing formats, digital pedagogy, and so on. Third, it precludes historicizing and contextualizing current work that falls outside of text analysis, which may lead to a lack of attention to method, its historical complexities, and points of convergence with related fields such as the social sciences. Finally, these histories all suffer from a lack of evidence; the narrative is assumed and applied rather than documented.An alternative approach would attend to the various methods, platforms, and tools that animate current DH work and investigate their origins in the literature. Ball (2013), for instance, has drawn attention to longstanding interest in computers and technology within writing studies. Scheinfeldt (2014) has pointed to the historical importance of oral history in the history of DH. Significantly, Nyhan, Flinn, and Welsh's (2013) project, \"Hidden Histories,\" collects and archives oral histories from those who worked in the field during its first decades. These efforts are in broad solidarity with the empirical history presented here and contribute to the growing number of heterodox histories of DH. MethodologyThe corpus for this study consisted of 1,334 research articles published in Computing and the Humanities (1966Humanities ( -2004 and Literary & Linguistic Computing (1986-2004. This end date reflects the final issue of CHum and predates wide circulation of A Companion to Digital Humanities (2003), which was important in shaping DH in many ways. We omitted introductions, reviews, conference reports, and other articles that did not primarily present original research.We manually inspected each article for media type and applied one of six categories (e.g., text, image, sound, object, number, other). For articles that addressed more than one media type, we recorded 'multimedia'. After inspecting several hundred articles, we added 'technology' as a media type to accommodate articles primarily about technology (e.g., AI, databases, hardware), rather than its application to a particular media. Using all eight codes (see Table 1), we coded or re-coded all the articles. In addition to media type, we recorded information about each author's discipline(s) and country of institutional affiliation. In the case of faculty appointed to more than one department, we recorded the discipline as 'multiple,' reasoning that an interdisciplinary appointment is more than a simple conjunction of its constituent departments. For authors located outside of traditional academic departments, we used one of three codings, where appropriate: 'center', 'non-academic', or 'GLAM' (galleries, libraries, archives, and museums). The remaining cases were clustered into one of 21 broad disciplines spanning the humanities and other areas.Finally, we recorded whether each article had a focus on teaching and learning (e.g., courseware, language learning software).Data on media type, disciplines, and teaching and learning were visualized using the free software Tableau  Public  and  are  available at http://bit.ly/earlydh. FindingsThe number of articles published each year varies from 6-50 (see Fig 1), the latter owing mainly to a double issue of CHum published in 1994/1995, which we recorded as 1995 because of its copyright date. Given the varying number of articles per year, we report several figures below as relative percentages each year (relative to the total number of articles that year). In cases where the two journals are compared, we also report relative percentages (relative to all articles from that journal in the corpus) because there are nearly twice as many total articles from CHum as compared to LLC, given their years of coverage.  Media typeText is the most frequently studied medium (59% CHum, 72% LLC), but sound, multimedia, and reflections on technology are all present in the early literature (see Figs 2-3). These distributions vary by journal, with text being much more prominent in LLC. 'Other' is, admittedly, a rather large category at around 4% overall, but the heterogenous articles found there are not easily resolved into one or more media types, which is the focus of these codings, or even a primary theme, such as 'technology.' To some extent, many of these articles speak to the emergence of a field with its own meta-level discussions about theory and the production of knowledge. These articles are found throughout the early literature of DH and increase slightly around the end of the corpus, when \"digital humanities\" as such might be said to emerge.  DisciplinarityThe distribution of authors' disciplines present in each journal is shown in Fig. 4. Computing and computer science is most frequent, largely because of the amount of coauthors from those areas. English language and literature is the most frequent humanities disciplines, commensurate with Kirschenbaum's claim that DH's \"professional apparatus…is probably more rooted in English than any other departmental home\" (2010,55). However, authors from languages and literatures departments other than English are nearly as common, as are centers, labs, and non-academic affiliations.  . Articles by disciplineSome disciplines work with certain media types more than others (see Fig 5). For example, scholars of languages and literatures work almost exclusively with text, while art historians appear to favor multimedia.  LocationTogether, CHum and LLC represent nearly 50 different countries based on authors' institutional affiliations (see Fig 6). A small but appreciable portion of articles (5.6%, 75 articles) are international (i.e., with co-authors from institutions in different countries). However, the vast majority of authors in CHum and LLC hail from American and British institutions (respectively), though this predominance declines over the course of both journals (see Fig. 7). This data, as well as the largely Anglophone nature of these journals, presents a limited picture of early DH. A fuller analysis would include work published in other places and languages.  Teaching & LearningThere has been longstanding interest in teaching and learning in the field (as shown in Fig. 8), though less so within LLC. Peaks in each graph reflect special issues on teaching and learning published by each journal.  Discussion and Future DirectionsRather than focusing on select disciplines, projects, tools, etc., this study includes the full range of early DH work (to the extent it appears in our corpus). The breadth of this picture helps set up the \"big tent\" view found in current accounts of the field. It also gives ground for historicizing and contextualizing the myriad forms of DH work today. One can imagine exploring this data to discover early DH articles about sound, in classics, from France, etc. and then consulting those primary source articles. Our study does provide some evidence for the claim that early DH work involves text experiments. Significantly, however, it documents the actual extent of that work (59% CHum, 72% LLC), and in so doing, highlights other work in the early history of the field. Our next steps include exploring additional sources to expand our corpus. In part, this includes investigating disciplinary journals for early DH articles. We might also identify such articles or journals by mining citations in our CHum/LLC corpus or by consulting sources such as the Companion. There are existing lists of early DH books as a starting point for monographs.In addition, the full text of our corpus presents several possibilities for analysis, including a citation study that might address questions of transference between disciplines and the degree to which corpus articles cite each other (forming their own scholarly discourse) as compared to literature outside of core DH journals. ",
        "article_title": "The Early History of Digital Humanities",
        "authors": [
            {
                "given": "Chris",
                "family": "Alen Sula",
                "affiliation": [
                    {
                        "original_name": "School of Information and Library Science - Pratt Institute",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Heather ",
                "family": "Hill",
                "affiliation": [
                    {
                        "original_name": "School of Information and Library Science - Pratt Institute",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " BackgroundIn the late evening of March 18, 2014, a small group of students and young activists stormed into the main chamber of the Taiwan's Legislature in protest of the hastily reviewed and pending signature of the CrossStrait Service Trade Agreement (CSSTA) with China ( Figure 1). The occupation of the Legislature would take several weeks and grow into an island-wide movement with strong popular support. In its aftermath it would amend the course of Taiwan politics, as well as its relation to China. It was a major contemporary event in Taiwan, and continues to influence the political landscape and societal reflection in the country. The occupation of the Legislature was streamed live, and when people vacated the chamber they left behind a massive amount of supporting artifacts and documentary materials. What would one do with these artifacts and materials, presumably soon to be abandoned, vanished and forgot? What could an archivist -or anyone who ever was involved in the movement - do in preparing for the future generations to remember the present events?A few historians in Academia Sinica, Taiwan, seized this opportunity and reached a general agreement with the occupants to systematically collect what was in the Legislature main chamber before they would prepare to end the protest. Afterward Academia Sinica suddenly got hold of a large collection of artifacts created by thousands of participants in a contemporary event. In this paper we discuss a few issues involved in digitizing and archiving artifacts from contemporary events of this nature. We outline our approach to addressing them, and we present the current status of this archive. The archive is called the 318 Civil Movement Archive at Academia Sinica. More background information about it can be found at the archive website [About]. As the movement started on March 18, 2014, has since better known as the Sunflower (Student) Movement [W-en] [W-zh-tw], in this paper the term Sunflower Movement Archive and the term 318 Civil Movement Archive will be used interchangeably to refer to the archive initiated at Academia Sinica. Often we simply call it the Archive. PrincipleTo strive for general access to the Sunflower Movement Archive probably was our topmost principle when we were starting to digitize the artifacts. This principle, however, shall be applied to a conflicting context of requirements and constraints. On the one hand, making the archive publicly accessible - on the Web of course - keeps Academia Sinica accountable to the activists (and to the public as well) about what it is doing. Academia Sinica will keep its promise in preserving all the artifacts it has acquired, and the proof is in the form of a Web catalog of all the digitized artifacts. On the other hand, as the artifacts are made by individuals, and some are of a personal nature (encouraging notes to the activists, for example), the individuals' personal privacy, publicity rights, as well as copyrights can be vulnerable when digital copies of the artifacts are made available for all to exam and use.Because of these considerations, only thumbnail images of the artifacts are made available on the Web for the Archive. The thumbnails are still useful for artifact identification (more about this later), but they are of no plausible other values. In addition, sensitive information inscribed in the artifacts, such as recognizable signatures and phone numbers, has to be pixelated to prevent misuse. No doubt there are boundary cases challenging our judgments. Often we will rather be safe than be sorry, hence will not release even thumbnail images at all for some artifacts. Still, how shall we deal with a banner with hundreds of signatures, sent in by overseas students to support the occupants? Scrubbing out all the signatures from the digital image of the banner will surly defeat the purpose of such an expression of solidarity. We make it a general rule that if it is a form of public communication, it shall be made public, even if there are personal information (names, signatures, affiliations, etc.) on the artifact.If what are made available are just thumbnail images and artifact metadata, a Web archive will not be too interesting. As participation to the movement is both personal and collective, we hope people will use the online archive to identify artifacts of their own (creation), and to make available high-resolution images of their artifacts to the public for general reuse. That is, we want the Web archive to be a conduit to help transit a collection of orphaned works into a domain of collective remembrance. A feature is built into the online catalog to allow registered users to identify artifacts of their own. Once identified, the user can choose to release the high-resolution image of the artifact to the public under one of the six Creative Commons Licenses, or more openly to elevate it to the public domain by using the CC0 Public Domain Dedication. Of course the claimant can choose to declare to reserve his/her copyright to the work. In this case, the high-resolution image will not be made public. To facilitate better search into the Archive, each item in the collection is annotated with rich metadata, including a transcription of the text appearing on the artifact (the words in a note, for example). People have used this feature to find and release artifacts of their making in spite of (or because of) the artifacts have been archived (and put online) for this historical event.At the same time when the physical artifacts were being digitized, we also began to collect \"born digital\" documentary media such as photo images and audiovisual recordings. At the time of the Sunflower Movement, these media were widely dispersed on media sites (e.g. YouTube), social networks (e.g. Facebook), or Web storage services (e.g. Dropbox). After the event, these media may be removed for various reasons, buried in new materials, or hard to find. Many service providers where these media are hosted often scale down the uploaded originals into low-resolution media, transform them into less desirable formats, and/or strip out all the metadata embedded in the original media (e.g. EXIF data in photos). These tainted media are not for archival purposes. We chased down some of the most wellknown citizen media activists who were broadcasting and reporting the events. We acquired batches of original files from them. By going after the original producers, we also get to keep better records of the provenance of the digital media in the collection. Many providers chose to donate the entire collections on their hard drives to the Archive, by using the CC0 Public Domain Dedication. Use and IdentificationWe feel it necessary to have a Terms of Use (ToU) for the publicly accessible catalog to the Archive [ToU]. By this, we will be able to communicate clearly to the public the purposes of the Archive, as well as various conditions and considerations in using the catalog. As the catalog is free and open to all, even without registration, to search and browse the Archive, we do not want the ToU to sound discouraging. Still, the catalog is the outcome of a provisional project at a research institution. We cannot really warrant the continuity and accuracy of the catalog and its associated services (in particular when funding was very uncertain in the beginning). Nor should we be held liable for people's use of these services. The ToU keeps users aware, and requests their understanding, of the right of publicity, the right to privacy, and other rights of the individuals whose artifacts are collected -or whose appearances are recorded -in the Archive. We also worry about the Archive being used by the authority as a source of evidences to pursue legal cases. Therefore, specifically in the ToU, we ask all users \"not to cause civil or criminal disputes,\" and \"not to commit harassment, threat or other misconducts on any individual.\"We now demonstrate how anyone can use the Archive. Let us use as an example the hand-written note shown in Figure 2. This item is a small post-it note written by a student from the Chinese University of Hong Kong [10531]. It is part of a large panel sent in by the students from Hong Kong to support the students occupying the Legislature [12958]. The panel is shown in Figure 3, with all the notes attached to it, as it is in the catalog. Each note attached to the panel has been individually digitized and cataloged; the note in Figure 2 is but one of them on the panel. Figure 4 is a photo of the panel hanging on the wall of the main chamber of Legislature during the occupation. As the hand writing in the note has been transcribed into text and becomes part of the item's metadata, one can search for it in the catalog using a few key phrases. In the note, the student says s/he is from the Department of Social Work (社工系). Using this three-character Chinese phrase, we search and indeed find this item in the catalog, as shown in Figure 5.Would anyone actually use the catalog to search and identify his or her own artifacts? We asked this question ourselves when deciding to add functionality to the catalog to allow registered users to identify, online, artifacts of their own. We were not sure. But once the functionality is there, and after some publicity about our work on the Archive, some people do start to identify their works and mail us their Copyright Declaration and Release forms [Id]. Figure 6 shows a work of art [18247]. It was identified by its creator using the catalog to the Archive. After the identification, he also releases the art work under a CC BY-NC-SA 3.0 TW license. By identifying it, the work can now be attributed to his name (佐瑪, Zuoma). By releasing his works under a Creative Commons license, he allows us to make available high-resolution images of his works for people to download. Figure 7A shows the template of the Copyright Declaration and Release form. A customized form will be generated automatically once an artifact has been identified by its maker. The PDF form will have included all the necessary information (about both the identifier and the identified item). It need only to be printed out, signed, and mailed in -no stamp required ( Figure  7B) - by the identifier to Academia Sinica.Who is Zuoma, the maker of the art work [18247], and what does he look like? People may ask. We shall know as we happen to meet him in person! In Figure  8, he is holding his own work, now a part of the collection of the National Museum of Taiwan History (NMTH). Since November 2016, all physical artifacts in the Archive had been transferred to the museum by a mutual agreement between Academia Sinica and the NMTH. The photo was taken at a press event on 2016-11-14 at the NMTH, where a one-day conference was held on topics of preserving and archiving artifacts from contemporary events. By building information systems encouraging people to reconnect with artifacts that had been forced to be left behind, we aim to help resurrect and disseminate people's stories of the movement. In this particular case, we did get to learn why and how Zuoma made this and other art works in the Sunflower Movement. Current StatusThe catalog of the Archive has been online since March 2015, roughly one year after the events setting off the Sunflower Movement. So far we have not received any complaint about putting the catalog online. For longterm preservation, Academia Sinica has made arrangement with the National Museum of Taiwan History (NMTH) to transfer the Archive to the Museum. The information systems managing the entire collection of digital media, including highresolution images of all the artifacts, are developed and released as open source software packages. As such, Academia Sinica and the NMTH can both host the digital archive on the Web. Currently the digital archive is still hosted at Academia Sinica even though all the artifacts had been transferred. The museum by itself has been collecting artifacts from various contemporary events for many years, including those from the Sunflower Movement. What Academia Sinica had collected were from the main chamber of the Legislature. The NMTH collects many more from other sources. There is a tentative plan between Academia Sinica and the NMTH to mutually enrich their digital collections on the Sunflower Movement.An online recollection of the 318 Civil Movement, drawing from a group of individuals loosely connected to the people working on the Archive, was announced and made public on March 18, 2015, the first anniversary of the events. The recollection is a website expressed as a map of Taiwan covered with images and narratives; these are individual stories told with supporting materials drawn from the Archive or from other sources [Expo]. We imagine any person, any group of individuals, can use this catalog to the Sunflower Movement Archive to tell their stories. Each item in the catalog has a permanent link; anyone can use the links to weave stories about the various events in the movement. DiscussionRemembrance of contemporary events can be both personal and collective. When artifacts are collected from contemporary events, individual and public access considerations constrained what shall and can be done with the artifacts. We hope we have maintained a balance in setting up the Sunflower Movement Archive. We hope our experience can draw some attention to, and incite more discussion about, the issues that are involved in building archives of contemporary events.We would like to emphasize that the work on the Sunflower Movement Archive is but one among the many in the field of digital archiving and curation. We opt not to give an overview of the best practices, nor cite the many literature, in this brief paper as we fear we cannot do it properly with the current time and space constraints. A balanced and comprehensive survey of the field will be obligatory when an extended version of this paper is to be prepared. Nevertheless we note that our effort is most related to the many existing works in catching the ephemeral but personal (as compared to those in holding on to the permanent and institutional). We look forward to learning from and further extending the practices in post-disaster remembrance and diasporic recollection. ",
        "article_title": "Remembrance of Contemporary Events: On Setting Up The Sunflower Movement Archive",
        "authors": [
            {
                "given": "Tyng-Ruey",
                "family": "Chuang",
                "affiliation": [
                    {
                        "original_name": "Academia Sinica",
                        "normalized_name": null,
                        "country": "China",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe impact of Johann Wolfgang von Goethe's epistolary novel Die Leiden des jungen Werthers marks a singular moment in German literary history, as it sparked a remarkable critical and productive literary reception. After the publication in 1774, so-called Wertheriaden - literary adaptations that are formally, thematically and structurally guided by Goethe's novel -appeared in various languages. These texts form our corpus for the Werther Project, which is part of the interdisciplinary DH Project CRETA of the University of Stuttgart.Literary studies have mainly concentrated on individual texts of this literary reception and on the individual genres to which the adaptations belong. Our research approaches focus on a macro-perspective to recognize recurring structures, elements, motifs and character constellations in all of the Wertheriaden with the help of computer-assisted analysis. From this perspective, we want to determine how closely the literary adaptations are based on Goethe's original text Werther. CorpusOur Werther corpus contains about 150 German and 30 English texts from different genres and literary periods -mainly (epistolary) novels, dramas and poems. The rationale for our corpus selection is based on Werther's publication in 1774 and its translation into French (1776) and English (1779). In the last two decades of the 18th century no other work of German literature was ever been translated or adapted in English as often as Goethe's novel.To establish comparative parameters for such a heterogeneous corpus (which also allow an analysis across all literary genres), we concentrate on specific features which are present in Goethe's original work. Among these discernible and typical characteristics (Martens 1985) are the triangular relationship of the three main characters, the concept of the \"Werther character\" (emotional, artistic), the monoperspectival narration, the role of nature (with motifs such as \"Herbst\", i.e. autumn), the subject's relation to society, the so-called \"sickness unto death\" (\"Krankheit zum Tode\") with the protagonist's subsequent suicide, as well as structural, stylistic and linguistic similarities (Horré 1997). These reference points will be applied leading to a large scale comparison.One approach is to compare the different texts on the basis of network and lexical information. In this context, we are concentrating on the visualization of the typical character network of Goethe's original text and whether this triangular constellation reoccurs in the Wertheriaden. Character NetworksThe analysis of character constellations is an essential part of literary studies. It shows the dynamic structure of the interactions of the central characters in the text and characterizes them in relation and contrast to each other (Pfister 1982). In the last few years, there has been a growing interest in graphtheoretic visualization and analysis of social networks in literary texts -especially in dramatic texts (Hettinger et al., 2015, Trilcke 2013, Moretti 2011). Central character constellations occur in all literary genres and form the basis for conflict constellations, particularly for the static counter-narrative between a protagonist and an antagonist. In Goethe's Werther, this typical constellation is extended by another figure -Lotte. She mediates between the emotional protagonist Werther, who is immortally in love with her, and Albert, her fiancée and Werther's adversary with opposite characteristics. A central aspect of the Werther Project is the identification of this triad by visualizing the character constellation in the original text and the literary adaptations, as well as detection of deviations. MethodFor the determination of character relations, name lists were derived from Goethe's epistolary novel and its adaptations in close reading. This lexicon-based approach leads to better results than machine learning techniques for named-entity recognition, since complex entities (\"Graf C.\" or \"Frau von S.\") which exist in the original text have to be determined without doubt. For each character we paid attention to synonyms (e.g. Lotte, Lottgen, Lottchen) and in case of the Werther character we added the personal pronoun \"I\", due to the first-person narration. Connections are established when two named entities are located at an adjustable distance of n tokens (normalization, stop word removal or sentence borders can be set as parameters). Based on this simple heuristic approach, the typical triad can be identified in both Goethe's Werther ( fig. 1) and in Ulrich Plenzdorf's modern adaptation Die neuen Leiden des jungen W. (fig. 2) from 1972. Another constitutive element of the plot is the addressee of the messages (Wilhelm/Willi) of the Werther character (Werther/Edgar). With respect to the triangular relationship, we aim to compare the vectors of a single pair, like Werther-Lotte in Goethe's novel, with a corresponding pair, e.g. Edgar-Charlie in Plenzdorf's adaption. Die neuen Leiden des jungen W. produces a denser network based on fewer character nodes with more weighted edges. That means Edgar has relatively intense contact to other characters besides his message partner. In contrast, many more characters appear in Goethe's novel, but often only once in connection with Werther, which reduces the network's density.  Future prospectsThe character constellation is determined by an opposition of the sensitive Werther to the rational opponent character Albert, with their love interest Lotte displaying character traits of both. Furthermore, an examination of the relationship pairs with regard to their context is planned.The usefulness of such an approach is illustrated by the sentence below, which defines the relationship of Lotte and Werther in context:Sie [Lotte] stand auf ihrem Ellenbogen gestützt und ihr Blick durchdrang die Gegend, sie sah gen Himmel und auf mich, ich [Werther] sah ihr Auge tränenvoll, sie legte ihre Hand auf die meinige und sagte - Klopstock! The term \"Klopstock\" is distinctive for the \"WertherLotte\" pair. It reflects their spiritual kinship and characterizes their relationship. However, this expression is found exactly once in Goethe's epistolary novel. Overall, the \"Werther-Lotte\" pair appears 81 times in the text. Based on these instances we aim to characterize their relationship even with specific terms like \"Tanze\", \"Porträt\" or \"Klopstock\". This will complete the network visualization of the characters with a description of their individual relations, both in Goethe's novel and its adaptations. ",
        "article_title": "Digital Analysis of the Literary Reception of J.W. von Goethe's Die Leiden des jungen Werthers",
        "authors": [
            {
                "given": "Sandra",
                "family": "Murr",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Florian",
                "family": "Barth",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIncreasingly, art historians, curators, and other arts scholars in an array of institutions-from galleries, libraries, archives, and museums (GLAMs) to the academy-are turning to digital platforms to facilitate and disseminate new scholarship, promote and provide access to collections, and to engage audiences both within institutional spaces and across geographical networks. Our presentation seeks to use our personal experiences working on digital projects in and related to Digital Art History (DAH) as a mode of analyzing and assessing the current state of the field from the perspective of the paid contract worker. As graduate students with minimal formal training in the Digital Humanities (DH), but with an avid interest in developing our theoretical and methodological knowledge, we feel that our contribution to this emergent and continuously evolving conversation is of value to academic researchers planning and undertaking large-scale digital research projects, particularly those who might be hiring and training research assistants. Because we also seek to contextualize some of the issues we have encountered in the intersections (and interstices) between DAH and DH tools and methods, our study speaks to how these align (or resist aligning) within the conditions of academic institutional realities, where we, as graduate students in Art History, are negotiating the differences between traditional and digital approaches to the discipline. Project DescriptionIn this paper, we examine how these issues manifest in an on-the-ground institutional context: that of the university research assistantship. We seek to analyze our roles as contract researchers on digital projects, while extending the scope of our examination to include some of the wider-ranging issues that came to light during the course of our work. We then suggest a number of viable solutions and best practices in response to some of the problems we have each encountered, and offer observations about how these issues are systemic within the field of Digital Art History, digital collections, and archives in generalparticularly those rich in visual materials.While traditional Digital Humanities methods work in some instances (for text or data mining, the creation and dissemination of certain kinds of archival or library-based catalogues, GIS-related geographical applications, and so on), we note that DAHparticularly where it engages with a specifically material- or object-based approach-requires its own specialized methods, most of which are still emergent or have yet to be developed. Borrowing from Johanna Drucker's writings on the specific needs of art historians working in digital environments, Diane M. Zorich's studies of extant digital initiatives in cultural institutions in the United States, and bolstered by our own direct experience as graduate students and museum professionals, we perceive that much work needs to be done in the field of DAH.Perhaps even more importantly, we observe that more effort needs to be directed toward making knowledge of effective DAH methods and platforms with DAH-specific affordances accessible within institutional environments that may not have the infrastructure or expertise in place to carry out DH methodologies, and where digital projects are known to have academic capital yet are not effectually supported. It is our hypothesis that these issues, while generally unintentional, are the result of a maladapted atomization of attitudes and methods that were never intended for the disciplinary particularities of projects dealing specifically with visual, aesthetic, and/or material culture; or for the specific needs of broad public-facing institutions such as GLAMs.Furthermore despite the gradual emergence of highly visible projects concerning the digital study and dissemination of visual materials-projects such as Object:Photo at the Museum of Modern Art, and the Getty Foundation-funded Online Scholarly Catalogue Initiative-these projects are almost exclusively the purview of large, well-funded institutions. At the same time, we have seen some training opportunities for new digital art historians, such as the 2015 Building a Digital Portfolio institute at George Mason University. As productive as these short-term training institutes are, however, scholars new to DAH require more intensive opportunities to develop these skills, something that many art history departments are currently unprepared to provide. Left out of the equation too are small and mid-sized GLAMs, even those affiliated with large universities, which lack the personnel, experience, and funding to pursue rigorous digital projects. As graduate students and researchers, we have seen first-hand how the absence of a robust network to share ideas and opportunities, teach new skills, and develop new approaches can hamstring the full potential of DAH projects. Although there is a clear interest in and need for platforms and methodologies for Digital Art History, in most instances there is not yet a thriving ecosystem to develop and share them. ConclusionsAlthough we offer this as a personal, narratological account of our experiences working as research assistants on digital projects (at Brock University, Ontario; and the University of Arizona, respectively), we also analyze how the programming aspects of these projects may be handled in differing institutional contexts-whether researchers are trained in digital applications, out-of-the-box platforms are adopted, or programming and development are outsourced-and examine these practices in light of their compatibility with both the practical and more abstract aspects of our respective endeavors. We will look at how these decisions inform and shape project development trajectories, as well as how they delimit these projects' longer-term potential. Importantly, we will consider issues of accessibility as they concern research assistants, and examine our own limitations in relation to the usability of the tools and platforms at our disposal in our respective institutions. Additionally, we examine how our independent, unpaid projects allow for the extension of boundaries and exploration in the field, and consider the ethical implications of how unpaid work informs and supports our contributions to funded, institution-based projects.   ",
        "article_title": "Negotiating Meaning and Value: Institutional Research Assistantships, Digital Projects, and Art History",
        "authors": [
            {
                "given": "Julia",
                "family": "Polyck-O'neill",
                "affiliation": [
                    {
                        "original_name": "Brock University",
                        "normalized_name": "Brock University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/056am2717",
                            "GRID": "grid.411793.9"
                        }
                    }
                ]
            },
            {
                "given": "Molly",
                "family": "Kalkstein",
                "affiliation": [
                    {
                        "original_name": "University of Arizona",
                        "normalized_name": "University of Arizona",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03m2x1q45",
                            "GRID": "grid.134563.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionWhen an earthquake struck Nepal in 2015, the band One Direction sent tweets encouraging their fans to donate to relief efforts, while an Indian activist tweeted accusations of Christian missionaries trading conversions for aid. While Twitter users were quick to bring their own agendas to the Nepal earthquake, does the same hold true for earthquakes in other parts of the world? A series of earthquakes that struck Kumamoto, Japan, and then Muisne, Ecuador in 2016 attracted a substantial amount of Twitter attention as well, yet as far as we are aware, the One Direction fans and the Indian activist made no comment. These users are onlookers to all three earthquakes: in other words, they are not directly affected by these events, but they tweet about them. This paper explores onlookers' responses across three different earthquakes: the 2015 Nepal earthquake, and the nearly-simultaneous earthquakes in Kumamoto and Ecuador in 2016, which we treat as a single event. This present work expands on our previous conclusion that onlookers tend to bring their own agendas to disasters. This paper shows that users who tweeted about the Kumamoto and Ecuador earthquakes were generally more interested in the earthquake or the affected areas than their own agendas, as their interest in the earthquake could not be predicted by interests in other topics. BackgroundA substantial amount of research has explored how social media causes users to engage with political, social, and humanitarian problems; however, opinions on social media's effectiveness-whether it causes users to donate money or participate in campaignsare mixed. Some argue that displaying concern in social media is more about acquiring social capital than effecting change (Shulman; Gladwell; Morozov, The Net Delusion; Morozov, To Save Everything, Click Here), while a Pew Research Center survey finds that social media does create change (Raine, Purcell, and Smith). One analysis found that charities' use of social media does not increase donations (Malcolm), while another finds that certain tweeting strategies do (Gasso Climent) although tweets may not raise awareness about the charity's causes (Bravo and Hoffman-Goetz). All these studies concur that social media enable substantial discourse about crises. The question we explore here is how much of this conversation is predicted by a user's preexisting interests, and how this varies even among the same type of event in different areas. MethodologyWe followed a similar data collection process for both Nepal and the Kumamoto and Ecuadorean earthquakes: we sampled data from Twitter's REST API to attain a broad sample of onlookers. For Nepal, we had gathered a dataset of tweets sent during the three weeks following the Nepal earthquake by searching for any tweets that mentioned the word \"Nepal\" from April 24, 2015 to May 8, 2015. We then randomly selected 15,000 users from this set and harvested all tweets they sent between April 24, 2014 andMay 8, 2015. We attempted to capture only English-speaking users to increase the likelihood that we would capture users not directly affected by the earthquake, but we still found some users who tweeted in multiple languages. This left roughly 11,000 onlookers for Nepal. For Kumamoto and Ecuador, we gathered a dataset of tweets sent in the two weeks following the Kumamoto earthquake that mentioned \"Kumamoto\" or \"earthquake.\" We randomly selected 30,000 users and harvested every tweet they sent between March 16 and May 16, 2016. We collected more users, but fewer tweets for each user, than we did in the Nepal dataset so as to look for users who displayed a broader set of interests. This left around 25,000 onlookers in Kumamoto and Ecuador. We were able to filter out non-English tweets much more effectively in the latter dataset than the Nepal dataset.For the tweets for each event, we made a bipartite graph of users to words, and performed community detection using a method proposed by Okamoto and Qiu (2015)  [2], which allows for overlapping communities. Okamoto and Qui's method takes a single parameter, alpha, which controls the resolution of community detection: the smaller its magnitude, the larger the number of detected communities. We set alpha to 0.001 in both cases. The output of this method was a list of each node (users and words), and a percentage ranking rating its affinity with each community. We used these results to generate a list of top words in each community, which told us what users who tweeted about that community were interested in. From this process, a number of topics emerged, which we labelled manually according to our interpretations of the top words in each.Since this method also gave us a ranking for users' affinities to each community, it allowed us to examine the influence of other topics on a user's likelihood to tweet about either event. We wanted to examine how much a user's propensity to tweet about other topics predicted the probability that he or she would tweet about topics related to the earthquake. We ran multivariate linear regressions on each topic in the dataset using the Python sklearn module (Pedregosa et al.). We ran one regression for each topic, in which we treated a user's propensity to tweet about the topic under consideration as a dependent variable predicted by his or her propensity to tweet about other topics. ResultsOur analysis demonstrated a certain predictive power for some topics in each dataset. Applying this process to the Nepal tweets produced 17 topics about a variety of concerns, from entertainment to world events. Table 1 shows these topics. Two of them, topics 5 and 15, treat the earthquake directly.A correlation exists between tweeting about entertainment topics and tweeting about the earthquake. Tweeting about topic 15 predicts that a user will tweet about topic 2, which is about pop music: the top words include \"fifth,\" \"harmony,\" \"video,\" and \"Justin.\" This correlation is the strongest in the dataset; few other topics show nearly as much correlation. Consequently, we observe a degree of correlation between tweeting about entertainment topics and tweeting about the disaster in Nepal. While the more targeted topics, like the One Direction topic, do not show much correlation with other topics, the more general entertainment topic does. Table 1: Topics for Nepal, showing probability of tweeting about one topic (X-axis) given likelihood of tweeting about other topics (Y-axis) In summary, we observe correlation between tweeting about entertainment topics and tweeting about the Nepal earthquake. Those who bring other agendas such as an interest in a particular musical group to the disaster tend to tweet mostly about those topics.Does the same hold true for Kumamoto and Ecuador? Table 2 shows a few topics from the Kumamoto and Ecuador earthquakes. Our analysis demonstrates that an onlooker's propensity to tweet about some topics could be predicted by interest in others. For example, a user who tweeted about news topics, such as U.S. politics (specifically, topic 43) or Asian news (topic 4), was likely to tweet about Nigerian politics (topic 2). Likewise, a user who tweeted about Japanese Entertainment (37) was also likely to tweet about other entertainment topics. On the other hand, no such correlation was observed in the opposite direction: no topic predicted a user's tendency to tweet about topics 6 and 22, the earthquake topics. All coefficients in those regressions were under 0.01. The two topics that focus on Kumamoto are relatively closed: users who tweet most about the Kumamoto earthquake tweet about little else during this period.Our interpretation is that users who tweeted about Kumamoto or Ecuador were specifically interested in earthquakes, Japanese culture, or the affected regions. The majority of users who tweeted about the Kumamoto and Ecuador earthquake topics were interested in specialized topics relevant to the events: they were not, for example, One Direction fans. We therefore conclude that while some users tweeting about Kumamoto and Ecuador were motivated by general interests in news or entertainment, they were a much smaller group than in the Nepal dataset. ConclusionsWe find that while users often brought their own agendas to tweeting about Nepal, fewer did so when tweeting about Ecuador and Japan. Users who tweeted about Kumamoto and Ecuador tended to focus on topics related to the earthquakes, and less on issues that the earthquakes might demonstrate.Our future work will test these conclusions with other earthquakes. In particular, we will examine the 2011 Tohoku Earthquake which raised serious political issues. Additionally, in our present work, we treat the Kumamoto and Ecuador earthquakes as a single event because distinct \"Kumamoto\" and \"Ecuador\" topics did not emerge from our text mining, which itself is suggestive of how Twitter users understood them. In our future work, we will probe more deeply for differences between the two earthquakes. Table 2 :2Topics for Kumamoto and Ecuador, showing  probability of tweeting about one topic (X-axis) given  likelihood of tweeting about other topics (Y-axis) (truncated  for space)   ",
        "article_title": "Missionaries, Politicians, and Boy Bands: Onlooker Behavior on Twitter During the Nepal, Kumamoto, and Ecuador Earthquakes",
        "authors": [
            {
                "given": "David",
                "family": "Shepard",
                "affiliation": [
                    {
                        "original_name": "University of California, Los Angeles (UCLA)",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Takako",
                "family": "Hashimoto",
                "affiliation": [
                    {
                        "original_name": "Chiba University of Commerce",
                        "normalized_name": null,
                        "country": "Japan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Hiroshi",
                "family": "Okamoto",
                "affiliation": [
                    {
                        "original_name": "RIKEN Brain Science Institute",
                        "normalized_name": null,
                        "country": "Japan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Tetsuji ",
                "family": "Kuboyama",
                "affiliation": [
                    {
                        "original_name": "Gakushuin University",
                        "normalized_name": null,
                        "country": "Japan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Kilho ",
                "family": "Shin",
                "affiliation": [
                    {
                        "original_name": "Hyogo University",
                        "normalized_name": null,
                        "country": "Japan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe field of Digital Humanities is characterized by complex questions and interdisciplinary discussions. That applies not only to the cultural science-based side of the humanities but also to the digital representation of cultural artefacts and, among other things, their processing, analysis, preservation and long-term availability. In order to handle these issues, interdisciplinary collaboration is required to pool and to coordinate expert knowledge and skills.The Digital Humanities project ZenMEM (Centre of Music -Edition -Media) represents such an interdisciplinary project. Since September 2014, researchers from Paderborn University, the Hochschule für Musik Detmold and OstwestfalenLippe University of Applied Sciences combine their expertise from musicology, various fields of computer sciences (contextual informatics, software engineering, usability engineering and music informatics) and media studies (media education and media economics) to investigate processes of change and new possibilities of the transition from analogue to digital music and media editions.In this context, digitization can be discussed as distributed through material infrastructures and displayed on computer devices (cf. Huber 1998). But it can also be considered in non-physical dimensions. In this perspective cultures are containing specific structures that build a para-material quality (cf. Schrage 2006).\"Writing is a material act; textual production in any medium has always been a part and product of particular technologies of inscription and duplication.\" (Kirschenbaum et al. 2009).A third perspective can be discovered between these poles on the transition from material and immaterial (cf. Holl 2010, Manovich 2011, McGann 1991. The fourth dimension in this context refers to the fact that \"Digital Humanities work embraces iterative, in which experiments are run over time and become object to constant revision. Critical design discourse is moving away from a strict problem-solving approach that seeks to find a final answer: Each new design opens up new problems and - productively - creates new questions.\" (Burdick et al. 2012) Between these perspectives, we explore interdisciplinary approaches of digital (music) editions. First, the dimension of digital representation and the contents on the surface (cf. Manovich 2001). Here, the traditional Human Computer Interface (HCI) to digital editions is questioned and expanded to incorporate Computer Interfaces, ultimately demanding to \"granting access to machines\".The second perspective focusses on the relevance of the user for all types of media (cf. Fiske 2001). In this view, we present a qualitative research study (cf. Denzin 2000;Przyborski & Wohlrab-Sahr 2009;Keuneke 1999) dealing with the changing work processes of musicologists and music editors in the context of digital music editions. These changes have, among other things, lasting impacts on their scientific research, orientation, academic ethos and knowledge building.The third paper deals with the structural changes in scientific work in context of Digital Humanities. A growing number of interdisciplinary academic projects causes an increasing need for researchers with the key qualification \"interdisciplinarity\" as well as methods to verify the projects' efficiency and quality (cf. von Kardorff 2000).The analysis of the complex interactions among these dimensions leads to essential issues of the digitization of (music) editions: The access to perspectives of digital representation, the impacts on scientific work and access to user views and at last a dimension of access to changing interdisciplinary project work within the Digital Humanities.In case of acceptance, this multiple paper session would be presented by Bianca Meise (media education and qualitative research, Paderborn University), Peter Stadler (musicology, Paderborn University) and Franziska Schloots (media economy and management, Paderborn University). Digital Editions and the Interface. Granting Access to Machines Peter Stadler Johannes Kepper Daniel RöwenstrunkThe history of Digital Editions and its main driving force, the Text Encoding Initiative (TEI), is a success story. It not only enabled the digital presentation and preservation of scholarly editions but subsequently facilitated further research on this digital material and the development of tools and methods, pushing forward the DH sector as a whole.The TEI puts a lot of effort into its (prose) Guidelines and the formal specification of the schema(s) - it ultimately developed a meta language (ODD = One Document Does it all) for the definition and documentation of TEI schemas and customizations, leading to a well-documented, highly flexible interchange format. \"This is standardization by not saying 'Do what I do' but instead by saying 'Do what you need to do but tell me about it in a language I understand'.\" (Cummings 2013) The Music Encoding Initiative (MEI) adopts these principles, empowering it to encode a wide variety of musical styles and genres through its modular approach. Yet, a long-standing issue with the resulting XML documents is the lack of interoperability. True interoperability would allow the 'blind' reuse of files within one's own processing chain (cf. Bauman 2011). But due to the TEI's and MEI's flexibility of encoding even the (assumed) simplest operations can hardly be processed blindly, e.g. the extraction of a plain text version (aka the \"throwing away of angle brackets\") or the extraction of a single voice from a score.Generally speaking, digital editions are potentially multifunctional and enable multiple views on the text (cf. Sahle 2013;Pierazzo 2016), yet this potential can hardly be activated from the outside but has to be revealed by a standardized endpoint or interface. Most commonly, the only (public) interface of a digital edition is the HTML version accessible online with a (Javascript enabled) browser. Different views are created from one TEI source file and prepared for a human reader, e.g. a \"Semantic Edition\" or a \"Philological Version\" (see the respective tabs at e.g. http://burckhardtsource.org/letter/508). While this interface and its functionality is adequate and pleasing for a human agent, it's more or less useless for a machine agent which would have to grab the web pages and parse idiosyncratic flavours of (X)HTML(5). Only a few digital editions offer the direct download of TEI encoded files and even less offer a dedicated (and well documented) interface for machines.Within the ZenMEM and ViFE research infrastructures we are trying to remedy this shortcoming of digital editions by developing dedicated APIs for our projects. We believe that this has at least two advantages:1. intrinsic: better documentation of our own work 2. extrinsic: facilitated reuse of our work by othersThese project APIs are developed and documented using the Swagger framework (http://swagger.io) and the resulting configuration files can be found at https://github.com/Edirom/ViFE-API. Along with those project specific APIs we strive to develop a meta API with a core set of functions which are to be supported by all projects. There is an ongoing discussion on what these core functions areespecially since we are dealing with various materials, from music to text encoding, from placeographies to bibliographies, from sketches to prints. On the other hand, we are confident to come up with a set of generic functions that could then easily be adopted by other projects. With these interfaces to digital editions we will finally leave the traditional resource based approach to digital editions behind and move on to a new functional, truly dynamic approach. A changing paradigm? Implications of digitization on musicologists work on editions Bianca MeiseDigital Humanities are focusing on digital data. Therefore, issues on modelling, representation, analysis and annotations are crucial dimensions of research like processing and archival storage. But digital data and the procedures quoted before are used by editors and have impacts on their scientific work, too (cf. Edwards 2012). This contribution is focused on an access to the editors as special user and producer group in the process of digitization through a qualitative empirical study. In this study, it is neither the editor nor the data alone, rather, it is the act of editing, analyzing, representing and annotating that gives insights into the relation between media, material and subjects that allow deep insights into the transformational impacts of digitization. First, I discuss the changes of the scientific working process from analogue to digital. Secondly, the challenges and potentials of this change of paradigm will be discussed. At last, as a result of these findings, the future inquiries of digitization of music editions, changes of work processes and not least the education and varying access to knowledge work will be pointed out.Digital music editions have a lot of potential for editors, scientists and recipients (cf. Veit). Thus, the digital availability accelerates and arranges the editorial scientific process. Furthermore, the different representations of the digital offer a great transparency and confirmability. In this perspective, the digitization of music editions enriches the work of musicologists (ibid.). A lot of issues are discussed in context of digital humanities refer to the digital representation or the transformation of cultural artefacts, their further analysis and processing. These subjects are considered on data. The work, examination and handling of the digital and the influences of the knowledge evolving within is discussed less. In this contribution, the formal layer (cf. Kirschenbaum 2008) or cultural layer (cf. Manovich 2001) and moreover the performative interactions between the material and the subjects will be considered (cf. Drucker 2013). Issues like the challenges to adopt digital techniques, to prepare digital sources, to represent and analyze them are important, too. The examination of the various practices of adoption, handling and orientation offer deep insights into the possibilities of digitization and its relevance for scientific work. This study allows a rare attendant insight in the transformation of a media paradigm in music philologists' work.The editions contain different types of \"texts\" like several notation formats, facsimiles, born digital annotations, text and audio files. Before digitization there are analogue procedures of searching, collecting, arranging and reviewing. Since the digitization of music editions, the shift is not only digital. Moreover, it's both digital and analogue, material and corresponding practices, that construct the editions. It is the operation, the handling of the musicologist with cultural, material and immaterial artefacts. To develop the importance of media and the corresponding methods in digital music editions, its necessary to explore these various interactions. Like the radical contextualization of the cultural studies pointed out that object and subject, media technology and context (cf. Winter 2010) continuously affecting each other and be interwoven.Based on the qualitative research perspectives of the Grounded Theory Methodology (cf. Strauss & Corbin 1996;Denzin 2000, Przyborski & Wohlrab- Sahr 2009) in this study eight narrative guided interviews with problem-centered parts have conducted. Because of the academic void of the user perspective qualitative research offers great potentials to get essential insights and generate first hypothesis in this field (cf. Flick et al. 2000). The investigation on varying routines or the knowledge of the musicologists even if it's implicit (cf. Polyani 1985) is not simple to transfer in direct questions. In fact, the qualitative research provides many methods to manage this problem (cf. Flick 2002). The guided interview is an inquiry method that follows a structure, but allows a lot of free space for further situated questions and narrative answers (cf. Keuneke 2000). The questions are focused on the routines of the working process of music philologists, their evolving (implicit) knowledge and their biographical reference points to music editions. The qualitative material has been analyzed by a modified coding version of the grounded theory (cf. Strauss & Corbin 1996) like Przyborski and Wohlrab-Sahr (2009) pointed out.The empirical data and their interpretation documented, that the change in the working process in not only a shift from analogue to digital. Moreover, the hole scientific process is changed and the work on digital music editions have deep impacts on editorial, juridical, organizational, social and not at least educational processes. Digitization changing the scientific work organization, the editorial work and the perspective on editions and the interwoven knowledge. Interdisciplinary research and knowledge building -Development of recommendations for action for academic projects in the digital humanitiesBianca Meise, Franziska SchlootsJörg Müller-Lietzkow Dorothee M. MeisterThe heterogeneous research field of digital humanities is characterised by a high level of connectivity and interdisciplinarity. Not least because of this the digital humanities turned out to be a dynamic field with high velocity of scientific discourses (cf. Gold 2012). In this context, the development of research infrastructure for interconnected and collaborative scientific work (cf. Reichert 2014) as well as the management of interdisciplinary research projects represent particular challenges. Whereas especially the economically characterised management research (cf. Bea, Scheurer & Hesselmann 2011/ Steinmann & Schreyögg 2005) deals with project management processes in companies, network groups or other cooperation forms, project management in academic contexts is hardly found in research literature. One important distinctive feature, especially for academic projects without industrial partners, is that the goal dimension of academic projects fundamentally differs from economic projects. Its focus is not a typical measurable economic gross profit or other definable objectives. The openness for results as an important part of scientific research requires a modified form of project management. However, strategic and systematic project management is hardly implemented in the structure of universities, particularly in cultural science departments and projects. Oftentimes, a rather \"muddling through\" (Lindblom 1959) can be noticed. This is not a desirable condition because consequences might be increasing transaction costs as well as reduce chances for longterm and stable research collaborations. Furthermore, the special structures in academic contexts must be considered concerning the planning, implementation and organisation of collaborative interdisciplinary projects.In view of these conditions, the importance of professional project evaluation is evident, especially in interdisciplinary projects involving cultural scientists as it is the case in digital humanities projects. Digital humanities can be seen as an extension of the complexity dimension according to Rinza (1998): A high scientific novelty grade is associated with an increased risk for the project goals and a relatively large project group might increase the dependencies in the work processes because of the close interlocking of the individual departments.The growing concentration of interdisciplinary academic projects causes an increasing need for scientifically verified evidence of their efficiency and quality (cf. von Kardorff 2000). For this purpose, summative evaluations are used to review the effectiveness and the achievement of defined targets as well as formative evaluations which can contribute to the optimization of an ongoing project and can provide the basis of strategic decisions and changes (cf. ibid.). Within this paper, the procedure will be clarified through a concrete case study from the field of digital humanities.In the project ZenMEM (Centre of Music - EditionMedia), a formative evaluation in the ongoing project was conducted so its results could be used to develop recommendations for action for the further process.In ZenMEM, researchers from Paderborn University, the Hochschule für Musik Detmold and Ostwestfalen-Lippe University of Applied Sciences investigate processes of change and new possibilities of the transition from analogue to digital music and media editions since September 2014.The project is initially promoted for three years by the German Federal Ministry of Education and Research (BMBF) and combines experiences and expertise as well as concepts and methods from musicology, various fields of computer sciences (contextual informatics, software engineering, usability engineering and music informatics) and media studies (media education and media economics). The focus of their research is on musical and other non-textual objects in context of digital editions. In this sense, the researchers are able to link to their preliminary scientific work as well as international developments like the Edirom project or the standards of Music Encoding Initiative (MEI) and Text Encoding Initiative (TEI) . They participate in its further development and examine innovative interacting and editing functions for the creation of digital music and media editions. In addition to research work, corresponding software tools are developed, technical advice and coordination is provided to external projects and training activities in form of workshops, courses and lectures are conducted and further developed. These steps are being accompanied by qualitative and quantitative user studies which take the entire creation process of digital editions into consideration. The results flow back into research and development work within the ZenMEM center. This short project description gives an impression of the complexity of the tasks in this joint project and the amount of heterogeneous research questions the cooperation partners are dealing with.To conduct a formative evaluation for the ZenMEM project in order to develop recommendations for action for the further project progression, a qualitative half-standardized written survey was chosen as research method. For this purpose, a questionnaire with 18 questions was given to all project members. These questions were relating to previous experiences with working in interdisciplinary projects, special challenges within the project, specific work packages and personal goals. In the last section, the researchers were asked for valuation of ongoing processes and constructive suggestions for improvement. In addition, a shorter questionnaire was sent to the project leaders. Thus, the perspectives of different project status groups could be surveyed.For the analysis of the results, amongst other things, a categorization based on the concept of SWOT analysis was performed. Developed at Harvard Business School in the 1960s for the strategic planning in companies, the SWOT analysis is a versatile tool (cf. Schawel & Billing 2009) describing the strengths, weaknesses, opportunities and threats of any company or project which constitute a basic structure for developing strategic recommendations for actions (cf. Kotler, Berger & Bickhoff 2016). To derive concrete statements from these four dimensions, the categorization had to be refined. Regarding the evaluation method of coding with Grounded Theory (cf. Przyborski & Wohlrab-Sahr 2009) a differentiated categorization within the root categories was performed.In this context, it is important to note that Grounded Theory is a methodology as well as an evaluation method (cf. Strübing 2004). The paper at hand focuses on Grounded Theory as an evaluation method to collect phenomena, condense them into concepts, identify categories and uncover connections between them. Following, recommendations for action could be worked out from the combinations of single phenomena in order to optimize processes within the project. Furthermore, indications could be provided to identify which actions should be focused and which should be avoided (cf. Pepels 2005). Individual competences of the project participants were also taken into consideration. Among other things, it became clear that interdisciplinarity in such a project can be considered as both opportunity and challenge, especially regarding the different preconditions, approaches and methods. People not only hold various paradigms and epistemological interests but also act within different organizations and social systems - nearly all university departments and courses are oriented mono-disciplinary (cf. Dressel et al. 2014). Stringent guidance is needed to enable successful interdisciplinary research. In the ZenMEM project, superiority of the subject disciplines was at least a little bit softened and reflected by mutual work observations as well as agreements on certain terminology so that a basal equality of the participating research partners could be established and constantly evolved.While in this case, systematic knowledge building took place directly from within the project, it can be discussed how the mediation of interdisciplinary competences could be implemented a lot earlier. As already indicated, interdisciplinary collaborations in academic contexts are of increasing significance and the demand for qualified young researchers with the key qualification interdisciplinarity (cf. Mainzer 2013) grows. More and more universities offer degrees in digital humanities or related fields. However, corresponding curricula show very different weightings of cultural science-based or computer science-based contents (cf. Schubert 2015). A closer inspection of digital humanities projects and a comparison between project experience and curricula might be helpful to develop degree courses so that interdisciplinary competences and shared knowledge can be gained.   ",
        "article_title": "Reconstruction - Representation - Collaboration: Interdisciplinary approaches to changes in contexts of digital (music) editions",
        "authors": [
            {
                "given": "Franziska",
                "family": "Schloots",
                "affiliation": [
                    {
                        "original_name": "Paderborn University",
                        "normalized_name": "University of Paderborn",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/058kzsd48",
                            "GRID": "grid.5659.f"
                        }
                    }
                ]
            },
            {
                "given": "Bianca",
                "family": "Meise",
                "affiliation": [
                    {
                        "original_name": "Paderborn University",
                        "normalized_name": "University of Paderborn",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/058kzsd48",
                            "GRID": "grid.5659.f"
                        }
                    }
                ]
            },
            {
                "given": "Peter",
                "family": "Stadler",
                "affiliation": [
                    {
                        "original_name": "Paderborn University",
                        "normalized_name": "University of Paderborn",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/058kzsd48",
                            "GRID": "grid.5659.f"
                        }
                    }
                ]
            },
            {
                "given": "Jörg",
                "family": "Müller-Lietzkow",
                "affiliation": [
                    {
                        "original_name": "Paderborn University",
                        "normalized_name": "University of Paderborn",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/058kzsd48",
                            "GRID": "grid.5659.f"
                        }
                    }
                ]
            },
            {
                "given": "Dorothee",
                "family": "Meister",
                "affiliation": [
                    {
                        "original_name": "Paderborn University",
                        "normalized_name": "University of Paderborn",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/058kzsd48",
                            "GRID": "grid.5659.f"
                        }
                    }
                ]
            },
            {
                "given": "Johannes",
                "family": "Kepper",
                "affiliation": [
                    {
                        "original_name": "Paderborn University",
                        "normalized_name": "University of Paderborn",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/058kzsd48",
                            "GRID": "grid.5659.f"
                        }
                    }
                ]
            },
            {
                "given": "Daniel",
                "family": "Röwenstrunk",
                "affiliation": [
                    {
                        "original_name": "Paderborn University",
                        "normalized_name": "University of Paderborn",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/058kzsd48",
                            "GRID": "grid.5659.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionWho undertook the foundational work of the discipline now known as Digital Humanities (DH)? Whose work merits inclusion in the history of the genesis of DH - the leaders of scholarly projects? Their research assistants? Their administrators? Their funders? Have important contributions to early DH projects gone unacknowledged or been silenced by the field's dominant 'founding father' narratives? How can a better understanding of previously undocumented contributions to the founding of DH allow us to evaluate the centrality of processes like collaboration and interdisciplinarity to the development and establishment of DH?This paper describes our research on the 'hidden contributions' to the Index Thomisticus project of Fr Roberto Busa S.J. . Busa is often said to be the founding father of DH: \"Most fields cannot point to a single progenitor, much less a divine one, but humanities computing has Father Busa, who began working (with IBM) in the late 1940s on a concordance [the In- dex Thomisticus] of the complete works of Thomas Aquinas\" (Unsworth 2006; see also Hockey 2004 and the more nuanced analysis Busa's role in Jones (2016)). Our research has uncovered the details and nature of the contributions made by the punched card operators who transcribed the (pre-edited) texts of Thomas Aquinas and related authors into machine-actionable data using punched card technology, thus completing the essential preliminary work on the Index Thomisticus. The operators were the mostly female trainees of the keypunch school that Busa had set up in Milan in 1956Milan in (and that ran until c.1967 as well as the female keypunch operators who worked with him in his Literary Data Processing Centre (CAAL). In addition to recovering the specifics of their work we have also sought to better understand their personal experiences of working on the project and whether the skills they learned were of subsequent benefit to them. Despite the formidable amount of work that they undertook, and the crucial nature of their task to the project, the identities of these women and the nature of their contributions were largely unknown and unacknowledged until this research was undertaken. MethodologyPrevious research on the history of DH has shown that when used with care oral history can contribute to a grounded history that exposes overarching processes while acknowledging through personal narratives the agency and creativity of a plurality of individuals, and not just the great men and women of scientific advancement (Nyhan, Flinn, Welsh 2015). An oral history approach was again adopted for this project; ten of the female punched card operators who had worked with Busa for various durations between 1954 and 1970 were interviewed. • What training did you receive?• What kinds of work did you do?• Was the training you received useful to you in later life?• Please share a memory of Fr Busa?It was also agreed that Passarotti should ask other questions as he saw fit, for example, in response to an interesting point that was raised by an interviewee. All interviewees signed a waiver form in advance of the interview that gives permission for their recollections to be published. A grant from the European Association for Digital Humanities (EADH, see the call for funding proposals) was secured so that the recorded interviews could subsequently be transcribed and translated by a Research Associate (Ana Vela), who is fluent in both Italian and English. Nyhan then worked through the translations in order to edit them further for clarity and check them, as far as was possible, for factual accuracy. She subsequently carried out a close reading and qualitative analysis of the interviews in order to identify common themes and telling divergences. This was followed by a historical-interpretative analysis that compared and contrasted the issues identified in the oral history sources with relevant primary and secondary literature. Finally, we wrote the results up as narrative history. FindingsWhat emerges from the interviews is an insight into the social, cultural and organisational conditions that the female punched card operators worked under and how they were treated in what was, structurally at least, a male-dominated environment. The interviews contain a wealth of recollections about the following issues in particular:• The womens' discovery of the training school that Busa set up • Their entrance test to the school For example, regarding the usefulness of their training, it opened opportunities that would otherwise have been blocked to them. A number of them went on to work as keypunch operators on an early machine translation project in the EURATOM Center at Ispra, Milan (On Busa's connection to Ispra, see Busa, 1980, p.86).Nevertheless, the interviews collectively give the sense that the women were seen as a source of low-cost and low-skilled labour. They did not have opportunities to progress from the position of keypunch operator and their training seems to have been the minimum necessary to carry out their roles. Most were not even made aware of the wider significance or aims of the Index Thomisticus project. Despite the existence of other research projects like EURATOM, mentioned above, and that an 'employment path' in the context of research computing was beginning to open up, their potential longer-term contributions to such work were not considered or fostered. ConclusionIt is almost a cliché to say that DH's collaborative nature makes it distinct and differentiates it from traditional Humanities. However, our research on the Index Thomisticus project has prompted us to ask whether claims about the centrality of collaboration to DH are more problematic than they first appear. As we will show, collaboration was the basis on which Busa's Index Thomisticus was realised. However, in the 'incunabular phase' (see Rockwell et al. 2011) of DH some forms of collaboration were considered more worthy than others and the contributions of the many female (and occasionally male) punched card operators who did the work of the project were not acknowledged. Until our research, their identities and the nature of their contributions had essentially disappeared from both the historical record and the collective memory of the DH community. This gives rise to a number of interrelated questions that have not yet been adequately addressed by scholarship on the history of DH: when and how did collaboration take on its significance for the field? What has influenced decisions about what kinds of DH collaborations have and have not tended to be acknowledged and how has this changed over time? What is the significance of the alleged cleaving of DH from the practices of the mainstream Humanities in regard to collaboration? Accordingly, our paper will also aim to open a wider discussion about the history of collaboration and the role it played in the formation and establishment of DH.•Their training and tasks as keypunch opera- tors • The organisational hierarchy of the Index Thomisticus workforce • Their awareness of the aims of the project and of Humanities Computing and Computa- tional Linguistics more generally • Their knowledge of Latin • Usefulness of the training to them in later life • Their memories of Busa. ",
        "article_title": "Uncovering 'hidden' contributions to the history of Digital Humanities: the Index Thomisticus' female keypunch operators",
        "authors": [
            {
                "given": "Julianne",
                "family": "Nyhan",
                "affiliation": [
                    {
                        "original_name": "University College London",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            },
            {
                "given": "Melissa",
                "family": "Terras",
                "affiliation": [
                    {
                        "original_name": "University College London",
                        "normalized_name": "University College London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/02jx3x895",
                            "GRID": "grid.83440.3b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Introductionnodegoat allows scholars to build datasets based on their own data model and offers relational modes of analysis with spatial and chronological forms of contextualisation. By combining these elements within one environment, scholars are able to instantly process, analyse and visualise complex datasets relationally, diachronically and spatially; trailblazing. nodegoat follows an object-oriented approach throughout its core functionalities. Borrowing from actor-network theory this means that people, events, artefacts, and sources are treated as equal: objects, and hierarchy depends solely on the composition of the network: relations. This object-oriented approach advocates the self-identification of individual objects and maps the correlation of objects within the collective. Research Environmentnodegoat is a web-based research environment that facilitates an object-oriented form of data management with an integrated support for diachronic and spatial modes of analysis. This research environment has been developed to allow scholars to design custom relational database models. nodegoat dynamically combines functionalities of a database management system (e.g. Access/FileMaker) with visualisation possibilities (e.g. Gephi/Palladio) and extends these functionalities (e.g. in-text referencing, LOD-module) in one web-based GUI. As a result, nodegoat offers researchers an environment that seamlessly combines data management functionalities with the ability to analyse and visualise data.The explorative nature of nodegoat allows researchers to trailblaze through data; instead of working with static 'pushes' - or exports - of data, data is dynamically 'pulled' within its context each time a query is fired. The environment can be used in selfdefined collaborative configurations with varying clearance levels for different groups of users.As a result of nodegoat's object-oriented set-up, everything is an object. In the case of a research project on correspondence networks, this means that a researcher would define three types of objects in nodegoat: 'letter', 'person', 'city'. Each object relates to an other object via relations (e.g. a letter relates to persons to identify the sender/receiver and this letters has been sent from/received in a city). In an extended research process, researchers could also define themselves as objects in the dataset, their sources or other datasets. Due to the focus on relations and associations between heterogeneous types of objects, the platform is equipped to perform analyses spanning multitudes of objects. By enriching objects with chronological and geospatial attributed associations, the establishment and the evolution of networks of objects is fully contextualised. In nodegoat, these contexts and sets of networked data can be instantly visualised through time and space. This open-ended approach makes nodegoat different from tools like the Social Networks and Archival Context Project, Alan Liu's Research Oriented Social Environment, the Software Environment for the Advancement of Scholarly Research, Prosop, or tools with a main focus on coding of qualitative data as seen in various computer-assisted qualitative data analysis software. With its object-oriented approach, nodegoat facilitates the aggregation of collections, coding of texts, and analysis of networks, but models these methods towards the creation and contextualisation of single objects that move through time and space. Facts & Figuresnodegoat is conceptualised and built by the independent research firm LAB1100, based in The Hague, The Netherlands. In order to share the functionalities of nodegoat with the scholarly community, scholars and research institutes are invited to use nodegoat for their own research purposes. Over 300 scholars have a personal research domain on nodegoat.net. Over 15 institutional partnerships have been established with universities, research institutes, and museums in The Netherlands, Belgium, Luxembourg, and Germany.A nodegoat user forum and FAQ is hosted on the Historical Network Research website. In the course of 2018 an open source package of nodegoat will be ",
        "article_title": "nodegoat: Enabling Explorative Research",
        "authors": [
            {
                "given": "Pim",
                "family": "Van Bree",
                "affiliation": [
                    {
                        "original_name": "Tel Aviv University",
                        "normalized_name": "Tel Aviv University",
                        "country": "Israel",
                        "identifiers": {
                            "ror": "https://ror.org/04mhzgx49",
                            "GRID": "grid.12136.37"
                        }
                    },
                    {
                        "original_name": "Tel Aviv University",
                        "normalized_name": "Tel Aviv University",
                        "country": "Israel",
                        "identifiers": {
                            "ror": "https://ror.org/04mhzgx49",
                            "GRID": "grid.12136.37"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionMany libraries and museums around the world have been releasing their digital collections and making them accessible online. They provide new opportunities for people to acquire information, but they also pose new challenges for accessing these large quantities of humanities resources. Language barriers are one of the main issues for accessing multiple databases in different languages. In this paper, we propose a method to link Ukiyo-e prints between databases in different languages by exploiting semantic similarity of metadata values across languages, in order to achieve our ultimate research goal that aims to provide multilingual access to multiple and diverse databases. We believe our proposed method could assist users in accessing Ukiyo-e databases regardless of languages.Ukiyo-e, Japanese traditional woodblock print, is known as one of the popular arts of the Edo period (1603-1868). Many libraries, museums and galleries in Western countries have digitalized Ukiyo-e woodblock prints with the metadata values in different languages. Table 1 shows an example of the same Ukiyo-e prints that are exhibited in multiple databases with metadata values in different languages. For linking the same Ukiyo-e prints between databases in different languages, our previous methods ( Batjargal et al., 2014;Kimura et al., 2015;Kimura et al., 2016) utilize the metadata values to calculate the similarity between Ukiyo-e prints, in which the metadata values are translated into the same language by using bilingual dictionaries or online machine translation services. Resig (2013) has developed an image similarity based Ukiyo-e print search system, which is able to search the same Ukiyo-e prints from multiple databases regardless of languages. However, this method cannot be applied to other humanities resources that have no images, such as texts, audio, video and so on.In this paper, we use the metadata values to calculate the similarity between Ukiyo-e prints, which is the same as our previous methods ( Batjargal et al., 2014;Kimura et al., 2015;Kimura et al., 2016). The difference is that we calculate similarity between metadata values of Ukiyo-e prints in different languages without translating. MethodologyOur method is based on word embeddings ( Mikolov et al., 2013a), which are dense, lowdimensional and real-valued vectors for representing words. By using word embeddings, the words with a similar meaning have closer distances in a vector space, which means the semantic relationships between words can be captured. An example is shown in Fig. 1, in which two words \"storm\" and \"hurricane\" that express similar concepts are closer in a vector space (only two dimensions are shown for simplicity). Word embeddings can be learned by using the Word2Vec toolkit, which employs a simple neural network model that can be trained on a large amount of unstructured text data in a short time (billions of words in hours). Fig. 1 An example of capturing the sematic relationships between words by using word embeddingsOur proposed method is motivated by the idea of Mikolov et al., (2013b) that the same concepts have similar geometric arrangements across languages. Fig.2 illustrates the vector representations of Japanese words (\"雨\" and \"嵐\") and English words (\"rainfall\" and \"storm\") that are used to describe weather phenomena. It can be seen that the same concepts (e.g. weather phenomena) in Japanese and English have similar geometric arrangements in a vector space.What is more important is that the relationship between vector spaces that represent these two languages can possibly be captured by learning a mapping between them, e.g. a liner mapping (dotted arrows in Fig.2). If we know some word pairs in Japanese and English, e.g. \"雨\" and \"rainfall\", \"嵐\" and \"storm\", we can learn a mapping that can help us to transform other words in the Japanese vector space to the English vector space. Fig. 2 The vector representations of words that are used to describe weather phenomena (\"storm\" and \"rainfall\") and time (\"evening\" and \"night\") in Japanese (left) and English (right)Our goal is to measure the similarity between Ukiyo-e prints by using their metadata values in different languages. Motivated by the idea above, we represent textual metadata values as vectors in each language. Then, we learn a mapping between vector spaces that represent two languages in order to transform the vector representations of textual metadata values from source language space to target language space. Once we obtain the vector representations of textual metadata values in target language space, we can calculate the similarity between metadata values in different languages. Fig. 3 illustrates how our method works. Firstly, we represent the titles of Ukiyo-e prints by additive combination of the vectors of words that compose the titles (Step 1 shown in Fig. 3). And then, we learn the mapping between vector spaces that represent different languages by using some title pairs in Japanese and English (Step 2 shown in Fig. 3), which can help us to transform metadata values from one language space to the other language space.  ExperimentsWe conducted experiments to evaluate our proposed method in linking the same Ukiyo-e prints in Japanese and English.In the experiments, the titles are used to calculate similarities among Ukiyo-e prints. Based on our method, the Japanese and English titles are represented by additive combination of the vectors of words that compose the titles. We train the Japanese and English word vectors on Japanese and English Wikipedia articles using Word2Vec toolkit.In the process of learning the mapping between two language spaces, we use 600 Japanese-English parallel short sentence pairs for pre-training the mapping between Japanese and English vector spaces.In order to make this mapping more suitable to Ukiyoe titles, we further use 74 pairs of Japanese and English Ukiyo-e titles to optimize this mapping, in which each pair of titles refers to the same Ukiyo-e prints. This optimized mapping are used to transform other vectors of the titles in Japanese space to English space.We calculate the similarities between the titles of Ukiyo-e prints using cosine similarity. For each Japanese title, after we obtain the mapped vector in English space, our method outputs the most similar English title vector as its corresponding English title.We use 173 pairs of Japanese and English Ukiyo-e titles as the test data to evaluate our method. The precision at top-n are used to measure the experimental results, which means the percentage of Japanese titles whose truly corresponding English title are ranked in top n. In order to verify the effectiveness of using Ukiyo-e titles to optimize the mapping, we show the results of both conditions of using Ukiyo-e titles and without using them in the pre-training. The experimental results are shown in Table 2. These results show that the precisions are further improved by using Japanese and English Ukiyo-e titles to optimize the mapping between Japanese and English vector spaces. The experimental results also confirm the usefulness of our proposed method for linking the same Ukiyo-e prints in Japanese and English. ConclusionOur proposed method measures the similarity between metadata values without using any bilingual dictionary or online machine translation system. Moreover, our proposed method represents the metadata values using word embeddings, which can capture the semantic relationships between metadata values.In the future, we will evaluate our method for linking Ukiyo-e prints in other languages. ",
        "article_title": "Linking the Same Ukiyo-e Prints in Different Languages by Exploiting Word Semantic Relationships across Languages Yuting Song",
        "authors": [
            {
                "given": "Biligsaikhan",
                "family": "Batjargal",
                "affiliation": [
                    {
                        "original_name": "College of Information Science and Engineering - Ritsumeikan University",
                        "normalized_name": null,
                        "country": "Japan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Akira",
                "family": "Maeda",
                "affiliation": [
                    {
                        "original_name": "College of Information Science and Engineering - Ritsumeikan University",
                        "normalized_name": null,
                        "country": "Japan",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Transforming Musicology project began in 2013, with funding from the UK's Arts and Humanities Research Council. It had three primary aims: to explore how technology could enhance, and perhaps 'transform', the practice and dissemination of conventional musicological research; to explore how large, new online sources of information, such as social media, might be exploited by digital musicology; and to look, in a modest way, at how digital approaches might be adopted in a sustainable way by the musicological community and beyond.In this paper, we give an overview of the project and how it developed, discuss our strategy for approaching these goals, and reflect on how an inclusive strategy for both research and teaching can be effective, despite its organisational and financial costs. The Transforming Musicology research projectConventional musicology is represented in the project by two research strands, one drawing on music from the early modern period and with an emphasis on corpus-based research, and the other considering both the psychological perception and the historical reception of Wagner's use of leitmotifs, centred on his Ring cycle. The project also funded four mini-projects, selected from an open call, to extend the range of research that Transforming Musicology could cover. These, like our research on musicology of online communities, could then be treated as case studies for an analysis of technology needs and overlaps, points of connection, workflows and the areas where connective technologies such as Linked Data might be most effective.The early modern strand of our project builds on two corpora: Early Music Online (EMO), scanned images and metadata for 300 sixteenth-century printed music books owned by the British Library; and the Electronic Corpus of Lute Music (ECOLM), a collection of full-text encodings and metadata for lute music. These contain closely interrelated music, and Optical Music Recognition tools were used to support the extraction of musical information from the facsimile images. Additional funding supported the creation of an open Linked Data union catalogue connecting the resources to one another and to relevant external resources, and the creation of a prototype application that supports both the linking process, and the publishing of web pages based on the now-linked resources. In collaboration with the BBC (the UK's national broadcaster) this resulted in demonstration pages for a BBC radio programme, the Early Music Show, allowing navigation of broadcast programme information, enriched with images and information from EMO, ECOLM, DBPedia and other sources of Linked Data ( Weigl et al., 2016). Meanwhile, the musical content can be analysed to explore the creative process of arranging the vocal music of EMO for the lute, as seen in ECOLM (Lewis, Crawford & Mü llensiefen, 2016) The Wagner research strand has two elements. Firstly, we are exploring the early development of guidebooks and other media aimed at helping listeners recognise and follow the so-called leitmotifs in Wagner's four-opera cycle, Der Ring des Nibelungen. These diverge from the composer's own description of the use of motifs in his works, and tease out different aspects of the musical and dramatic themes of the works. In addition to the musicological question here, we also investigate whether a semantic web ontology can usefully represent the diversity of motifs, their different expressions and their relationships (Rindfleisch, 2016).The second Wagner element is psychological, with biometric readings taken from audience members during performances of all four operas of the cycle. Along with memory tests, these provide us with a huge amount of data to interpret, and challenges for how to publish that data in ways that are useful to others.Further to this real-time physiological data, we also have expert annotations of a score, recording performance aspects that might be significant in the subject responses but not recorded in the notation, along with a complex set of information for aligning the time-based responses with each other and with a musical score (Baker & Mü llensiefen, 2016).A vast quantity of music of all kinds has become available on the web during recent decades and has engendered a correspondingly large amount of commentary, whether simply in the form of 'likes', or through intense online discussion on specialist websites to sophisticated scholarly articles. We carried out a pilot investigation of user communities on a lyric-annotation website, genius.com, formerly Rap Genius. With over 3,000,000 annotations, this can be regarded as a paradigmatic and valuable musicological resource which needs to be approached with the techniques of Big Data; our work has been centred on the overlaps between the networks of annotators and the songs and artists they annotate (Fields & Rhodes, 2016).The four mini-projects have contributed: audio analysis of historical electronic music; ornamentation style in traditional Irish flute playing (Janč ovič , Kö kü er, and Baptiste, 2015); a big data approach to finding the sources of the poetry used in medieval sacred songs; and an exploration of 18-20th-century London musical life based on digitised concert information from programmes and newspapers (Dix et al., 2014).For these investigations, we have evaluated how semantic web technologies may offer solutions for bridging between disparate data and tool sets, and help document the research and its data, making it more reproducible as a result. (Nurmikko-Fuller and Page, 2016) The Transforming Musicology website's Publications section offers a more complete list of research outputs than can be covered here.We have embedded engagement with technology, musicology and other music-related communities in our work in general, but the most significant step for us in ensuring sustainability for digital musicology has been the creation of a week-long digital musicology workshop as part of the Digital Humanities at Oxford Summer School. The summer school is the largest of its kind in Europe, and is made up of a framework programme, consisting of a morning session each day, after which students attend the more specialist workshop they have selected.By summer 2017, this will have run in three consecutive years and attracted a diverse set of over fifty students. Teaching Digital Musicology at the Digital Humanities at Oxford Summer SchoolAlthough courses do exist on specialist areas within musicology (such as the Music Encoding Initiative summer school in Paderborn) this is the first dedicated to the whole discipline. In designing the curriculum, the Transforming Musicology team have reflected on how we can best make the methods that we have investigated over the course of the projectalong with others that we know are used and work well -readily accessible to the wider musicology community.Just as we have endeavoured to be inclusive in our approach to the musicological research that we have undertaken and supported within the project, we have also sought to create a space in which musicologists can be introduced to, and given the opportunity to experiment with, a wide variety of tools and approaches. It is important to us that the questions that musicologists investigate are not distorted when technology is brought to bear, and our teaching reflects this by balancing sessions about general-purpose tools with domain-specific use-case descriptions.Students are eased into the week with an introduction giving an overview of the week ahead, but also some personal reflections about moving into a digital musicology approach from a more traditional background, and each day begins with presentations of case studies to motivate the techniques introduced during that day. Each day broadly considers a single topic, covering audio processing on both a small and large scale, music encoding, Optical Music Recognition and music processing. The final day presents more case studies and closes with a round table discussion which is intended to give the students the opportunity to reflect on the week and their own research practice and consider if and how the skills that they have learned can be useful to them.The workshop is intended to stimulate ideas rather than to make programmers and audio engineers of our students, although we do still expect students to perform simple programming tasks and introduce them to key audio concepts and tools. Such lofty, longterm goals are difficult to evaluate. We take immediate feedback through a form on the last day, although the longer-term effect of attending can only be seen as the students develop their research in the coming years. Shorter-term impacts, including collaborations with tutors on papers and research proposals, and increasing contributions to conferences and workshops with an explicitly digital aspect can already be seen.Musicology has always been interdisciplinary in nature, and has been transforming itself based on contributions from computing and web technology for over fifty years. We see our project as contributing to and supporting that transformation, both through our own research and through developing the skills of others.  ",
        "article_title": "Digital musicology: through research and teaching",
        "authors": [
            {
                "given": "Tim",
                "family": "Crawford",
                "affiliation": [
                    {
                        "original_name": "University of London",
                        "normalized_name": "University of London",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/04cw6st05",
                            "GRID": "grid.4464.2"
                        }
                    }
                ]
            },
            {
                "given": "Kevin",
                "family": "Page",
                "affiliation": [
                    {
                        "original_name": "Oxford University",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "Lewis",
                "affiliation": [
                    {
                        "original_name": "Oxford University",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "De Roure",
                "affiliation": [
                    {
                        "original_name": "Oxford University",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " the Murkutu Project and the Intellectual Property Issues in Cultural Heritage Protocol), the cross-pollination of an interdisciplinary community of digital researchers with tribal communities, cultural heritage organizations, and academic scholars has been significantly underdeveloped. Thus, it is not surprising that there are only a handful of funded digital projects documenting Native American life: the American Indian Treaties Portal, a digital collection of the final texts of 366 of the 375 American Indian treaties recognized by the United States Department of State, and the digitized Journals of Lewis & Clark Expedition created in partnership with the Center for Digital Research at the University of Nebraska Lincoln. Mark of the Mississippians: A Multi-Platform Digital Media Project (Cahokia MoundsMuseum Society) and Meeting the Earthworks Builders, a flashbased video game are currently being developed by the Ohio State University.. Additionally, NEH Public Programs just funded Indians of the Midwest, an educational website focused on recent scholarship on Native peoples and the Newberry Library Collection.The barriers to digital fluency in Native American studies are varied and include such obstacles as cultural rules regarding access to sensitive materials, the advanced technical expertise that software and hardware often requires beyond basic digitization, the costs of digital infrastructure and proprietary license fees, and issues of community engagement and trust that might limit the display of digital materials about Native peoples. It might be tempting to assume that the uptake of digital humanities method and pedagogy in the academic, cultural heritage, and tribal communities is one of lack of information or funding, but in fact the digital humanities researchers included as part of the Digital Native American and Indigenous Studies project are finding that it is also the cultural barriers to access, display, and analysis across differing types of digital materials that are challenging our ability to leverage digital tools, resources, and approaches.Digital Humanities articulates three parallel interdisciplinary commitments to \"openness\": 1) a commitment to open access publishing; 2) a commitment to open access/open source software development; and 3) a commitment to open access data. While the first two trends have received deep and lasting attention via scholarly publishing and digital commons enterprises and the open source development movement promoted by github and other code repositories, the commitment to open access data has been largely undertheorized. Using case studies of Digital Humanities projects that have been developed using Native American and Indigenous content, this submission suggests that Native and Indigenous content complicates the current technical application of open source development driven by digital aggregators and application programming interface development. By highlighting ethical issues around the use, reuse, and distributed architectures encouraged by common digital humanities technologies, this submission suggests that the rhetoric and practice of the open access data movement obscures both Native agency in determining the use of community materials as well as the role of technical determinism in proliferating the violence of colonial archives on Native communities. Questions this submission engages with include: How do we deal with born-digital research data in Native American and Indigenous contexts? How do we as scholars responsibly engage in digital research in Native communities? How do organizations and institutions navigate the cultural, legal, and ethical contexts of the communities whose objects they hold? How can free and open source software solutions be leveraged to build community engagement? Finally, what might be recommended for tribal communities who desire to launch their own digital projects but may have concerns about resources, access, infrastructure, and preservation?   ",
        "article_title": "Indigenizing the Digital Humanities: Challenges, Questions, and Research Opportunities",
        "authors": [
            {
                "given": "Jennifer",
                "family": "Guiliano",
                "affiliation": [
                    {
                        "original_name": "Indiana University",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    },
                    {
                        "original_name": "Purdue University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Carolyn",
                "family": "Heitman",
                "affiliation": [
                    {
                        "original_name": "University of Nebraska–Lincoln",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " American Lynching: Uncovering a Cultural Narrative, byAndrew Bales. This site explores America's long, dark history of lynching and the role of newspapers as both catalysts for killings and platforms for reform. Historical Agricultural News, by Amy Giroux, MarcyGalbreath, and Nathan Giroux. The site is a tool for exploring information on farming organizations, technologies, and practices, as a window into social, economic, and political history.Chronicling Hoosier, by Kristi Palmer, Caitlin Pollock, and Ted Polley. This site tracks the origins of the word \"Hoosier,\" its geographic distribution, and its positive and negative connotations over time. USNewsMap.com, by Claudio Saunt and TrevorGoodyear. This site allows users to discover patterns, explore regions, and investigate how words, terms, and news spread.Digital APUSH, by Ray Palin and the A.P. U.S. History Students at Sunapee High School. This class used word frequency analysis to discover patterns in news coverage of several major issues such as secession, the KKK, and Plessy v. Ferguson. This paper illustrates the potential impact of humanities collections such as Chronicling America when their data are made freely and easily available. It describes the data available in this huge data repository, the mechanisms researchers and students can use to access it, and some of the challenges inherent in its large span. The paper addresses some of the technical specifications and program guidelines for the National Digital Newspaper Program, in which metadata standards for both access and preservation were primary concerns for the NEH and the Library of Congress in creating and maintaining the program. It also explains how the program's decade-old community has cultivated shared practices and specifications that contribute both to the longevity of this dataset and to other newspaper digitization efforts across the country.Then, the paper gives information about the winning entries in the first Chronicling America Historic American Newspapers Data Challenge in 2016, which touched on a variety of important humanities themes like religion, race, literature, violence, agriculture, law, and geography. It explains how winners used cutting-edge technology to produce maps, visualizations, tools, and data mashups. Winners reported their initial questions, the importance of this data in answering them, their methods, and their future plans for the projects they built. This paper highlights their processes and the variety of results they produced.Finally, the paper discusses broader lessons for humanities users of \"big data\" in Chronicling America. It shows the value of a contest in publicizing data in the humanities and its uses. It showcases different modes of collaboration among humanities researchers, libraries, information technology professionals, and other partners in the research endeavor. The paper also explores some of the challenges for humanities scholars working with large datasets, including representation, bias, categorization, and documentation. The intellectual work of the digital humanities projects described here involves the same problem identifying and research that humanists have always pursued, but the methods and investigation present new questions and opportunities. The paper suggests ways of maximizing the benefits of large datasets such as Chronicling America to the humanities.  ",
        "article_title": "Using Big Data to Ask Big Questions",
        "authors": [
            {
                "given": "Leah",
                "family": "Weinryb",
                "affiliation": [
                    {
                        "original_name": "National Endowment for the Humanities United States of America",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In Philip K. Dick's Do Androids Dream of ElectricSheep the U.N. secretary proclaims, \" [m]ankind needs more empathy\" (1968). The poignancy of Dick's novel is its accurate expression of the social challenge of diminishing human empathy. The author offers empathy as the defining characteristic of humanity. As is often the case, science fiction foreshadows our future: longitudinal studies show decreasing rates of empathy in college students over the last three decades. If we believe that empathy is indeed a vital quality, then humanists are uniquely qualified to address this decline: extensive research suggests that empathy can be taught, specifically by reading fiction. Furthermore, preliminary trials indicate that virtual reality (VR) effectively evokes feelings of empathy in viewers. In both cases, the medium can provide the audience with access to situations outside of their everyday experience, offering a perspective into the lives of people unfamiliar to the reader/viewer. Take, for example, the work of documentary filmmaker Chris Milk that immerses the viewer in war torn villages in order to impact immigration policy (see \"How virtual reality can create the ultimate empathy machine,\" 2016) or the content of the New York Times VR application which addresses a wide variety of social justice issues from all over the world. However, as critics such at Janet Murray rightfully argue, the impact of VR is dependent on the execution, which is still in development stages: \"[t]he technical adventurism and grubby glamour of working in emerging technologies can make it hard to figure out what is good or bad from what is just new\" (\"Not a Film and Not an Empathy Machine,\" 2016). As the digital humanities have encountered with other emerging technologies -most notably data visualization techniques - these new forms need to be critiqued as they evolve (Drucker, 2012). Inviting students and educators to collaborate with industry professionals in the process of consuming, critiquing, and creating open access VR content creates the opportunity to design thoughtful immersive experiences that may address the decline in empathy in college age students. This presentation will explicate a study-in-progress devised to measure the pedagogical impact of VR content in combination with design thinking assignments used to combat desensitization and evoke empathy across the disciplines.This research is supported with a case study of students in a series of linked courses at a small liberal arts college in Baltimore, MD. Students were exposed to VR content intended to increase their feelings of empathy for people who represent the \"Other\" in various ways, such as gender, race, ethnicity, and social class. This study was created through a crosscampus collaboration between faculty from the humanities, social sciences, and school of design alongside the theater director and librarians. Using empathy as the central question, each course integrated VR content and related readings into the curriculum. In each case, VR provided access to experiences not possible within the classroom space, for example an immersion into a refugee camp, a simulation of the human brain, and a documentary depicting gender bias across cultural contexts. The VR was scaffolded into each course in discipline-specific ways. For instance, the literature courses focused on readings that depict representations of virtual bodies in tandem with theory on posthumanism, particularly the work of Katherine Hayles and Donna Haraway. At the same time, the theater program produced The Nether by Jennifer Haley, which raises questions about the laws governing virtual spaces through depictions of pederasty and the murder of young children. Simultaneously, courses in psychology and human services integrated VR to discuss the impact of immersive content on social justice reform, and nursing courses looked at the application of VR for patient care and education. To varying degrees, this work was supplemented with readings on feminism, race theory, and disability studies in order to support discussions of \"othering\" with students. After analyzing the VR content in conjunction with the course materials, students were asked to design a VR experience intended to evoke empathy in the context of a discipline-specific audience. Additionally, members of a local VR company contributed as guest speakers and offered internships for interested students. Surveys were distributed at the beginning and end of the semester that prompted students to define, discuss, and debate empathy. At the end of each course students were interviewed to identify which methods of engagement increased their empathy toward people (in some cases characters) they felt were unlike themselves in significant ways.As a part of this submission the syllabi and assignments will be shared. Ideally, the speaker will bring a VR headset and gaming laptop so participants can experience and consider how this emerging technology can evoke empathy by providing access to geographical, cultural, political, and biological content unfamiliar to the viewer. The goal is to receive audience feedback on the first stage of this study in order to improve and refine the methods before executing the plan on a larger scale. This study is IRB approved and student consent will be obtained for any student work that is presented.  ",
        "article_title": "Teaching Empathy Through Virtual Reality",
        "authors": [
            {
                "given": "Amanda",
                "family": "Licastro",
                "affiliation": [
                    {
                        "original_name": "Stevenson University",
                        "normalized_name": "Stevenson University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/0149s5s84",
                            "GRID": "grid.264314.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionFor more than 30 years, digital 3D modelling and in particular reconstruction methods have been widely used to support research and education in the digital humanities, especially but not exclusively on historical architecture. While technological backgrounds, project opportunities, and methodological considerations for the ap-plication of digital 3D reconstruction techniques are widely discussed in literature (e.g. Arnold and Geser, 2008, European Commission, 2011, Frischer, 2008, Bentkowska-Kafel et al., 2012, Bentkowska-Kafel, 2013, Kohle, 2013, our interest is to investigate digital 3D reconstruction as a scholarly area and to derive implications for further organizational and methodical development. Against this background, this research work is dedicated to support the dissemination process by investigating the following research questions:• What scenarios and practices exist for the employment of digital methods and approaches for scientific research within the field of digital 3D reconstruction, especially in art and architectural history? • What are requirements and recommendations for the design of tools and media related to digital 3D re-construction?• How can the use of digital methods and, in particular, digital 3D reconstruction techniques in visual human-ities be learned and taught? ResearchAgainst the background of these research interests, our research activities include to investigate (1) a scholarly community, (2) usage practices occurring with-in single projects and to gain implications for an appropriate organizational development. Moreover, practical application and a development of implications for (4) the conception and creation of a user-centered design of software tools and environments for digital humanities scholars and an (5) implementation in student education are in focus. Research methods and approaches used within the described research activity mainly derive from social and information sciences as well as from science and technology studies (cf. Hackett et al., 2008, Boczkowski andLievrouw, 2008, pp. 955) and comprise e.g. bibliometric analysis (cf. Vinkler, 1996), quantitative and qualitative empirical analysis as well as structuring approaches like mind mapping (Table  1). Relevant research has been active since 2010 in 12 projects (ongoing till approx. 2020) on the local, national and EU levels, with our department's participation.  ResultsConsidering a scholarly community on digital 3D reconstruction and modeling, discourses on major conferences during the last 25 years were mainly led by institutions from European Mediterranean countries, covering primarily technological topics. In particular, statues and buildings in Mediterranean countries dating from all periods Anno Domini deliver rich content for such reconstruction (Fig. 1). Institutions with high numbers of publications, connections to co-authors or the extraordinary importance to link groups of researchers to each other can be identified on a structural level (cf. Fig. 2). Due to the high complexity and team based workflows, aspects and usage practices for communication, cooperation, and quality management are of high relevance within 3D reconstruction projects. Especially if people with different disciplinary backgrounds are involved, visual media are intensively used to foster communication and quality negotiations (cf. Fig. 3), for example by comparing source images and renderings of the created virtual reconstruction. Furthermore, several projects successfully adopted highly standardized conventions from architectural drawings for interdisciplinary exchange (Mü nster, 2013). To support an organizational development, we ran five workshops to identify ongoing research topics and challenges, involving around 100 researchers in total. Current challenges named (cf. Fig. 4) aim at a research and development of sustainable and practicable approaches to access wider scientific communities and audiences and include aspects such as widely interoperable documentation and classification strategies and schemes, an overarching cataloguing of projects, and the creation of objects as well as strategies and technologies for an exchange between different technological domains and approaches of usage. Regarding design implications for digital environments, we investigated, for example, that relatively little visual information is needed to allow observers to distinguish buildings from each other or to identify a single building and gain information about its spatial relation and shape (Mü nster et al., 2017b). Moreover, we adopted and evaluated team project-based learning approaches to support student education in digital 3D reconstruction (cf. Fig. 6). As observed in two courses so far, a development of procedures and strategies for cooperation within student project teams for creating virtual representations evolves slowly and is mostly caused by emerging problems and urgent demands. Related competencies are based highly on implicit knowledge and experience. As a consequence, a teaching of implications and best practices prior to commencing a project is less effective than coaching during the project work.    SummarySince it is our vision to establish digital 3D reconstruction as a scholarly accepted and widely used research method in humanities, it seems to be crucial to add a critically reflected methodological basis and anchor it in academic culture. To draw this currently missing \"big picture\", our department performed research on digital 3D reconstruction within various projects by combining both theoretically and practically grounded research parts. This comprises research on scholarly communities, scholarly practices and methodological recommendations as well as implications for interaction design, teaching and dissemination. ",
        "article_title": "Digital 3D Reconstructions as a Scholarly Area of Digital Humanities -Scholars, Projects, Implications and Perspectives",
        "authors": [
            {
                "given": "Sander",
                "family": "Münster",
                "affiliation": [
                    {
                        "original_name": "Technische Universitä",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe research described in these pages is made possible by openly available Classical texts and linguistic resources. It aims at performing semi-automatic analyses of Paulus Orosius' (385-420 AD) most celebrated work, the Historia adversum Paganos Libri VII, against its sources. The Histories, as this work is known in English, constitute the first history (752 BC to 417 AD) to have been written from a Christian perspective. To do so, Orosius drew from earlier and contemporary christian and pagan authors, providing a rich narrative fraught with intertextual references to poetry and prose alike.Orosius' vast network of references challenges automatic text reuse detection tasks both qualitatively and quantitatively. In fact, information retrieval algorithms face differences in reuse style -from verbatim quotations to paraphrase and allusions (Navarro, 1991)-and millions of words to sift through. To understand how Orosius reused texts, it is necessary to detect, extract, classify and evaluate all references and compare them to their sources, mindful of the balance between the precision of the results and their recall or number. Related WorkExisting research on Orosius' sources for the Histories is scattered and often focusses on his relation to one author or work only, albeit acknowledging the full spectrum of sources (e.g. Coffin, 1936;Sihler, 1887). The size of the source-texts (see Table 1) makes it extremely difficult to produce a comprehensive and detailed manual exploration of all of Orosius' references.\"It would be burdensome to list all of the Vergilian echoes [...]\" (Coffin, 1936: 237) What Coffin describes as \"burdensome\" can be accomplished with machine assistance. To the best of our knowledge, no existing study, traditional or computational, has quantified and analysed the reuse habits of Orosius.The Tesserae project, which specialises in allusion detection, is the most similar to the research presented here (Coffee, 2013), with the difference that it does not yet contain the text of Orosius nor many of its sources, and that the results are automatically computed without user input.In contrast, our approach, TRACER (Büchler et al., 2017), offers complete control over the algorithmic process, giving the user the choice between being guided by the software and to intervene by adjusting search parameters. In this way, results are produced through a critical evaluation of the detection. Research Questions and GoalOur research began with the following questions: how does Orosius adapt Classical authors? Can we categorise his text reuse styles and what is the optimal precision-recall retrieval ratio on this large historical corpus? How does detection at scale affect computational speed?This project tests the stability of historical text reuse detection on a corpus of Latin authors where Orosius is our target text. We evaluate our computed results against known reuses published in commentaries to Orosius, thus corroborating existing findings but also potentially uncovering previously unnoticed reuse. In so doing, we refine our workflow and resources in order to advance historical text reuse detection for Latin. DataAll of the public-domain works for this study were downloaded from The Latin Library. We chose this collection over other analogous resources as it provides clean and plain texts (.txt), the format required by our text reuse detection machine TRACER. Table 1 outlines the authors and works under investigation in chronological order. To give an idea of the size of the texts, the 'Tokens' column provides a total word-count for each work; the 'Types' column provides the total number of unique words; and the 'Token-Type Ratio' shows how often a type occurs in the text (e.g. a TTR of 3 indicates that for every type in a text there are three tokens on average. Generally, the higher the ratio the less linguistic variance in a text). This table reveals the language and challenges we should expect when detecting reuse. For instance, Caesar, Lucan and Tacitus share similar text lengths but Caesar has a higher TTR; this tells us that Caesar's text has less linguistic variety than Lucan and Tacitus. Conversely, if we look at Suetonius in comparison to Lucan and Tacitus, we notice a larger text but a similar TTR. This indicates a high linguistic variance in Suetonius' text, and one that can prove challenging for text reuse detection.  Reuse StylesOrosius employs a variety of reuse styles, ranging from verbatim quotations to allusions and paraphrase (Navarro, 1991). The reuses are as short as two words (ibid.) or as long as sixty-five words, and sometimes invert the word order of the original text (Elerick, 1994). MethodologyOur workflow makes use of three resources: first, a TreeTagger Latin language model for Part-ofSpeech (PoS) tagging and lemmatisation (Schmid, 2013). We chose to work with TreeTagger as, unlike other taggers, it comes with a pre-trained model for Latin (trained by Marco Passarotti). Since submitting this abstract, we also began experimenting with the LemLat morphological analyser. Secondly, we used the Latin WordNet lemma list and synonym set to support the detection of paraphrase and paradigmatic relations; and TRACER, our text reuse detection machine (see also, the list of TRACER's 700 algorithms).First, the data is acquired and prepared: the texts are downloaded, semi-automatically cleaned (by \"cleaning\" we mean the removal of footnotes, section numbering, special characters and typos in the texts) and, where possible, spelling variants are normalised. Next, the texts are lemmatised and tagged for PoS. We then run TRACER with different parameters in order to define the diversity of the reuses in the corpus. TRACER can split a detection task into six subtasks, each containing parameters that users can customise or (de)activate depending on the type of detection required (see Figure 1). The reader will notice that the Pre-processing step also contains lemmatisation. This does not mean that TRACER can lemmatise any text but it currently supports lemmatised input data from the Stanford CoreNLP English lemmatiser and the TreeTagger Latin lemmatiser. ResultsTagged and lemmatised text accounts for 93.1% of the tokens in the corpus. A 7% of words could not be lemmatised due to typos in the text (e.g. missing white-spaces), which we are manually or semiautomatically correcting; similarly, some words could not be successfully tagged, as the lemmas are not included in the TreeTagger's parameter file (e.g. named entities).To perform detection with TRACER, all texts were initially segmentised by sentence. The average sentence length measured across the whole corpus is thirty-one words per sentence. A first detection task between Orosius and all other authors was conducted at the sentence level. However, this failed due to the presence of very short text reuses. For this reason, the segmentation was changed to a moving window of ten words, thus giving TRACER smaller units to process. In the Selection step (see Figure 1), we first experimented with max-pruning (i.e. the removal of high frequency words) but eventually settled on a PoS-based selection, which considered nouns, verbs and adjectives as more relevant features than function words, thus significantly increasing the recall and the overall quality of the results.For the Scoring (see Figure 1), we used the resemblance score, which measures the ratio of overlapping features with the overall unique set of features of two alignment candidates. Figure 2 illustrates the results of this detection process: over 45% of reuses identified in Orosius overlap with the source texts by four words, and that roughly 93% of all candidates have overlaps of 3, 4 or 5 words, indicating a fragmentary reuse style rather than block-copying. This detection task took approximately 30 hours to compute. A second TRACER experiment was run between Orosius and Tacitus to explore and evaluate the results of the detection on a smaller scale. Commentaries to Orosius reveal the presence of fifteen reuses of Tacitus, ten of which refer or allude to text that no longer survives. This means that TRACER can only try to match five known reuses, which differ in style. In this experiment, we used a moving window of fifteen words and synonym replacement in order to identify paraphrase as well as verbatim quotations. TRACER identified forty reuses, of which thirty-six false positives and two new finds. Figure 3 illustrates these results. Figure 3. Top: an example of a false positive produced by TRACER in detecting reuse between Orosius and Tacitus. Bottom: two new finds yielded by TRACER, an analogy and a potential reuse. Colours match up the similarities between the aligned candidates. Limitations and Future WorkThe retrieval accuracy of TRACER partly depends on the accuracy of the trained models of TreeTagger and the Latin WordNet data. An error analysis is needed in order to verify the accuracy of our cleaned and automatically-tagged data, and to determine the effect of this incorrect tagging on text reuse detection. Depending on the outcome of this analysis, we will consider re-tagging our corpus with a more advanced tagger, such as LemLat ( Passarotti, 2007) and/or LatMor, or even training a tagger on the different types of Latin constituting our corpus.We are currently running TRACER comparisons between Orosius and each of the other source authors in our corpus to verify the presence of previously unknown reuse, corroborate known reuse and improve our detection techniques.Additionally, we plan on comparing Orosius against non-reused authors, such as Plautus or Apuleius, to examine TRACER's performance on \"negative\" texts. Ratio of the the scored overlap (in %)Scored overlap with a moving window of 10",
        "article_title": "Orosius' Histories: A Digital Intertextual Investigation into the First Christian History of Rome",
        "authors": [
            {
                "given": "Greta",
                "family": "Franzini",
                "affiliation": [
                    {
                        "original_name": "Georg-August-Universität",
                        "normalized_name": "University of Göttingen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01y9bpm73",
                            "GRID": "grid.7450.6"
                        }
                    },
                    {
                        "original_name": "Georg-August-Universität Göttingen",
                        "normalized_name": "University of Göttingen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01y9bpm73",
                            "GRID": "grid.7450.6"
                        }
                    }
                ]
            },
            {
                "given": "Marco",
                "family": "Büchler",
                "affiliation": [
                    {
                        "original_name": "Georg-August-Universität",
                        "normalized_name": "University of Göttingen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01y9bpm73",
                            "GRID": "grid.7450.6"
                        }
                    },
                    {
                        "original_name": "Georg-August-Universität Göttingen",
                        "normalized_name": "University of Göttingen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01y9bpm73",
                            "GRID": "grid.7450.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Audio Digitization StationBegun in year one (2014), this station sought to develop a technologically accessible digitization solution for audio-cassette tapes, most of which contain oral histories produced in the 1980s and 90s. We will outline the accessibility concerns at the heart of this project, including how the audio digitization protocol the project established sought to build organizational comfort with digitization practices by integrating the system with existing database infrastructures and ways of organizing volunteer labour at the archives. This part of the paper will also address how the Collaboratory identified trans audio collections within the archives, and will touch on some of the challenges in locating these materials. This material will be useful for humanities scholars in sexuality and gender studies who are concerned about creating digital preservation plans for audio-based primary source materials they might collect or create. Mirha-Soleil Ross PapersBegun in year three (2016), this project is working to process, digitize, and improve access to the personal papers of Mirha-Soleil Ross, one of the most significant figures in transgender activism and cultural production in Canada. Donated to the archives in 2008, this large collection had not yet been processed by archives volunteers and as result, was mostly inaccessible to researchers. We are working in collaboration with Ms. Ross, and several members of the trans community, to organize these materials, determine access restrictions for sensitive content, create an online finding aid, and an online digital collection using the Omeka platform, which will sample from the larger volume of material. This part of the paper asks how digitization might provide an opportunity to repair damaged relationships between LGBT organizations and trans communities. It also considers some of the unique privacy concerns scholars must consider when providing online access to materials that document trans lives.   ",
        "article_title": "University-Community Digitization Partnerships: Accessing Trans Collections in LGBT Community Archives",
        "authors": [
            {
                "given": "Elspeth",
                "family": "Brown",
                "affiliation": [
                    {
                        "original_name": "University of Toronto",
                        "normalized_name": "University of Toronto",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03dbr7087",
                            "GRID": "grid.17063.33"
                        }
                    }
                ]
            },
            {
                "given": "Cait",
                "family": "Mckinney",
                "affiliation": [
                    {
                        "original_name": "University of Toronto",
                        "normalized_name": "University of Toronto",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03dbr7087",
                            "GRID": "grid.17063.33"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Walter Charlton published his Physiologia Epicuro-Gassendo-Charltoniana, or, A fabrick of science natural, upon the hypothesis of atoms in 1654. And though Thomas Hobbes never citesLucretius directly in his Leviathan (1651), the Latin poet's ideas about the material nature of the universe are a distinct antecedent to Hobbes's mechanical philosophy.Even around the time of the revival, the reception of the DRN was vexed. Lucretius's statements about the materiality and mortality of the soul, the role of chance in the universe, and the detachment of the gods were far too radical ever to gain wide acceptance in early modern England. In the backlash against Hobbesianism in the 1660s, Lucretius was a prime target for criticism. Cottegnies (2016) argues that the backlash against Hobbes's ideas in the 1660s also marks the end of the Epicurean moment. Theologians writing against Hobbes attacked Lucretius's atheistic materialism. And though Lucretius's stature as a poet continued to grow, praise for DRN was almost always tempered. His more extreme ideas were to be dismissed or ignored. In the notes to his translation of DRN in 1682, the first complete translation published in English, Thomas Creech argued against Lucretian and Epicurean attitudes toward the soul and divinity (Creech and Dryden, 1700; these citations refer to the text published using the PhiloLogic build of eebo). John Dryden, the preeminent arbiter of literary taste of his era, quoted Lucretius often in his plays and included translations of select passages in Sylvae (1685). In his preface to that collection, Dryden praised the directness of Lucretius's poetic expression, pointing out the \"positive assertion of his Opinion\" and his \"Magisterial authority.\" But the subject of his poem is \"naturally Crabbed\" and the poet himself is \"often in the wrong\" (Dryden, 1685; text published online by Philologic) .Reuses and citations found in the Digging into Data database suggest that this basic framework for understanding Lucretius largely played out across the 18th century. Lucretius was at once an admired poet, a materialist attacked for not admitting divine involvement in the universe, and a philosopher who in fact had important things to say about living well. Even so, the alignment database allows us to see a handful of authors, mostly medical and scientific writers, whose views of Lucretius and his ideas veer from this basic narrative. Mainly toward the middle and latter parts of the century, some reuses suggest a less troubled acceptance of Lucretius's naturalism.Through this presentation, we hope to show that this alignment database, through the accumulation of so many instances of citation, can facilitate a kind of large-scope reading that allows scholars to gain a nuanced sense of longer term intellectual trends. Built on a huge quantity of uncorrected OCR, the database provides scholars the specific source evidence --and a ready means to access it -- that they might need as a starting point to pursue even deeper investigations into the thought of the 18th century.  ",
        "article_title": "Tracing Swerves of Influence: text reuse and the reception of Lucretius in 18th-century England",
        "authors": [
            {
                "given": "Charles",
                "family": "Cooney",
                "affiliation": [
                    {
                        "original_name": "University of Chicago",
                        "normalized_name": "University of Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/024mw5h28",
                            "GRID": "grid.170205.1"
                        }
                    }
                ]
            },
            {
                "given": "Clovis",
                "family": "Gladstone",
                "affiliation": [
                    {
                        "original_name": "University of Chicago",
                        "normalized_name": "University of Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/024mw5h28",
                            "GRID": "grid.170205.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionGutenTag is a cutting-edge resource that allows literary researchers of all levels of technical expertise to perform large-scale computational literary analysis. It allows users to build large, clean, highly customized worksets and then either analyse them in-system or export them as plain text or richly-encoded TEI. It has been built from the ground up by literary scholars for literary scholars: rather than relying on off-the-shelf tools poorly suited to the domain of literature, we have developed many of the components ourselves based on the specific demands of literary research. GutenTag is fully open-source, its analyses are based on entirely open corpora, and researchers can save and distribute all the parameters of their analyses, allowing for unprecedented reproducibility of research in a field plagued by siloed corpora. GutenTag is easy to use, permitting casual non-programmers to perform complex computational literary analysis via an online interface, while offering additional offline customization options to more advanced users. Although GutenTag was initially designed to facilitate our own research in polyvocality and dialogism, we show here that it can be leveraged to intervene in pressing debates unrelated to our specific research, such as the discussion surrounding Matthew Jockers's analysis of gender in Macroanalysis. Overview of GutenTagThe system has grown considerably since our initial proposal, presented to an audience of computer scientists (Brooke et al., 2015). Below, we review the main features of the software with particular emphasis on recent improvements.Interface: GutenTag is primarily accessed through an HTML GUI, accessible via the web or as a downloadable tool (both can be accessed from http://www.projectgutentag.org). In offline mode, the configuration files can be saved and loaded, and additional lexicons and other lists used for analysis can be specified by the user. A Python API is also included.Corpora: The original version supported only the 2010 image of Project Gutenberg USA, but we have expanded support to all texts from Project Gutenberg USA as well as Project Gutenberg Canada and Australia, which include many additional texts published after 1922 and still under copyright in the USA.Metadata: Document collections of interest can be defined using a variety of metadata tags. These include metadata provided by Project Gutenberg (title, author, author birth, author death, and, for some texts, Library of Congress classification and subjects). We have added genre (fiction, non-fiction, poetry, drama), determined using a sophisticated machine classifier, as well as author and text information (author gender, author nationality, publication date, publication country, single work or collection, etc.) derived from (mostly) unstructured resources including Wikipedia and the texts themselves.  : The GutenTag interface, showing the creation of a workset based on advanced metadata (Genre, Author Sex, Author Nationality, Date of Publication)Text cleaning and tokenization: Sophisticated regex-based heuristics are applied to remove metatext elements related to Project Gutenberg before, after, and sometimes within the text boundaries. Literature-specific tokenization is provided, preserving important information needed for downstream analysis.Structural Tagging: This module identifies the main structural elements of the texts. First, heuristics are used to identify the likely boundaries between front matter, body, and back matter. Identification of structure within the main text is driven primarily by the identification of headers, and fully supports recursive structures including entire embedded texts which can have their own front and back matter separate from that of the anthology. Structural tagging is sensitive to genre: in the context of fiction, we identify parts, chapters, and speech; for poetry, we identify poems, cantos, stanzas, and lines; for drama, we identify acts, scenes, speakers, speech, and stage directions.Lexical tagging: GutenTag includes lemmatization and POS tagging. There are several built-in lexicons which capture semantic and stylistic distinctions, and users can define their own lexicons, including multiword lexicons. Most recently, and most relevant to our case study below, we have added our own stateof-the-art literature-specific named entity recognition system (LitNER) which bootstraps from context-based clustering of common named entities to distinguish previously unseen people and locations from other named entities ( Brooke et al. 2016b). For fiction, we group individual person names into collections of characters, and then assign speech events to these characters in the vicinity, using efficient, rule-based logic inspired by work in He et al. (2013). We identify the indicated sex of these characters primarily using large lists of names and titles; when a name does not appear on our list, we fall back to matching common sex-indicative character n-grams automatically derived from those lists (e.g. names ending with \"a\" tend to be female).TEI output: When corpus output is required, we use XML-based TEI format as the default output format when structure (rather than simply tokens) is requested. Analysis: In addition to building corpora for exporting, GutenTag users can directly compare the distribution of relevant lexical tags or other textual metrics across multiple corpora as defined in the metadata filtering phase. The latest version includes a selection of standard textual metrics (e.g. average sentence length), part-of-speech based metrics such as lexical density, and metrics that rely on structural/lexical tagging, such as the amount of dialogue and the amount of dialogue that has been assigned to female characters. Advanced users can easily define their own textual metrics using Python; these then become available through the main interface. We also welcome requests for metrics from the DH community. Research ApplicationsGutenTag was initially developed to facilitate our own research in literary dialogism ( Hammond et al. 2016, Brooke et al. 2016a). GutenTag allows us to perform three crucial steps in our research process: first, to build customized corpora (a set of novels published from 1880-1950, for which it yields 4,088 results); second, to identify passages of character speech in each novel and assign a unique character to each passage of speech; and third, to calculate a measure of dialogism for each text using an algorithm based on our six-style approach ( Brooke et al. 2016a). Further, GutenTag allows us to save our workflow in a parameter file so that it can be reproduced by other researchers.GutenTag is designed as a general system, however - not merely as a vehicle for our specialized research. We thus present an example of how it can be employed (by a non-programmer) to investigate a prominent debate in Digital Literary Studies, Matthew L. Jockers's discussion of gender and authorship in Macroanalysis. Jockers argues that female authorship can be predicted reliably through topic modelling, based on the presence of themes that \"correspond rather closely to our expectations and our stereotypes\" such as \"Affection and Happiness,\" \"Female Fashion,\" and \"Infants\" (Jockers 2013). A reader might respond to Jockers's analysis by querying his assumptions about literary authorship; specifically, his failure to distinguish between authors and characters. Suppose that female characters were just as likely to discuss \"Female Fashion\" in novels written by men as those written by women, but that female authors tended to include more female character speech in their novels, as Muzny et al. (2016) suggest. If this were so, Jockers's findings would not confirm stereotypes about female authorship, but simply reveal the tendency of female authors to include more female voices in their texts than men.GutenTag is uniquely suited to investigating such a question. Its advanced metadata and sophisticated lexical tagging allow it to easily and rapidly analyze the question of female character speech in a large corpus of English-language novels.   Figure 3 shows that female authors in the twentieth century included approximately the same amount of dialogue as a proportion of total text length as male authors, but that in the latter half nineteenth century, they included approximately 5% more than men. Since Jockers focuses on the nineteenth century, this finding alone might impact his conclusions. As Figure 4 shows, GutenTag supports Muzny et al.'s contention that female novelists incorporate far more (approximately twice as much) female dialogue compared with male novelists. The finding that the proportion of female dialogue decreased from the late nineteenth to the mid-twentieth century, in both female and male authors, is one that bears further investigation -particularly in relation to the emergence in that period of popular genres, such as children's literature, Westerns, and romance novels.  Mean proportion of dialogue allotted to female  characters in prose fiction, female vs. male authors, by  nationality, 1850-1949 Sample sizes as follow, in number of texts. Scottish: 31 female, 80 male. Canadian: 49 female, 78 male. English: 339 female, 1308 male. American: 572 female, 1545 male. Australian: 38 female, 104 male. Irish: 21 female, 92 male.In Figure 5, we employ GutenTag's ability to filter results by author nationality. The marked discrepancy between proportion of female dialogue in male authors from England and the United States again suggests the need for an further investigation of genre; for instance, whether the American preference for male-centred genres like the Western might explain the result. Looking at GutenTag's fine-grained outputs, we observe that the texts with the lowest proportion of female dialogue are those directed at a young male audience (especially adventure fiction for boys) while those with the highest proportion consist largely of fiction for young women (L. M. Montgomery's Anne of Green Gables devotes over 90% of its dialogue to female characters). These findings might prompt our hypothetical researcher to engage in a smaller-scale study of the representation of gender in children's literature. Because all texts in GutenTag are accessible to users, it easily accommodates such movements from large-scale analysis to close reading. ConclusionGutenTag allows researchers of all levels of technical expertise to perform advanced large-scale literary analysis, as well as to independently test the hypotheses and conclusions of prominent research in the field. Our case study further shows how the integrated, end-to-end GutenTag system allows users to raise new research questions in the course of their analyses (such as the correlation between the emergence of children's fiction and the proportion of female dialogue) and then, since all its corpora are accessible, to shift scales and explore these questions through close reading. ",
        "article_title": "GutenTag: A User-Friendly, Open-Access, Open-Source System for Reproducible Large-Scale Computational Literary Analysis",
        "authors": [
            {
                "given": "Adam",
                "family": "Hammond",
                "affiliation": [
                    {
                        "original_name": "University of Toronto",
                        "normalized_name": "University of Toronto",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03dbr7087",
                            "GRID": "grid.17063.33"
                        }
                    }
                ]
            },
            {
                "given": "Julian",
                "family": "Brooke",
                "affiliation": [
                    {
                        "original_name": "University of Melbourne",
                        "normalized_name": "University of Melbourne",
                        "country": "Australia",
                        "identifiers": {
                            "ror": "https://ror.org/01ej9dk98",
                            "GRID": "grid.1008.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " MaterialTeksty Drugie is a Polish literary journal dedicated to literary scholarship. It has been published since 1990 by the Institute of Literary Research of the Polish Academy of Sciences. It focuses on literary theory, criticism and cultural studies, while also publishing articles by authors from neighbouring disciplines (philosophy, sociology, anthropology). The journal publishes monographic issues dedicated to particular topics or approaches within literary and cultural studies. All those features make it a good example for exploring the vicissitudes of Polish literary scholarship.The corpus consists of the entire collection of papers published in Teksty Drugie (excluding letters, surveys, notes, etc.) in the years 1990-2014 (2,553 texts, 11,310,638 words). The material covering the years 1990-1998 was digitised, OCR-ed, and then manually edited, in order to exclude running heads, editorial comments, and so forth. Obviously, some textual noise -e.g. a certain number of misspelled characters -could not be neutralised. The material from 1999 onwards was digitally-born, but even though a small number of textual issues might have occurred. We believe, however, that distant reading techniques are resistant to small amounts of systematic noise (Eder, 2013).Given the nature of Polish, which is highly inflected, lemmatization was necessary for a reliable processing of texts. The corpus has been lemmatised with LEM 1.0. (Literary Exploration Machine) developed by CLARIN-PL (see: Piasecki, Walkowiak, Maryl 2017). MethodTo scrutinise the formulated hypothesis, we applied one of the methods of information retrieval that recently attracts a good share of attention in Digital Humanities circles, namely topic modelling in its classical variant known as Latent Dirichlet Allocation (LDA). The method, introduced by Blei (2012), allows for finding co-occurring cohorts of words that presumably reveal (latent) semantic relations.The experiments were performed using a tailored script in the R programming language, supplemented by the package 'stylo' ) for text preprocessing, and the package 'mallet' (McCallum, 2002) for the actual LDA analysis. A bimodal network of the relations between topics were produced using the software Gephi ( Bastian et al., 2009).Topic modelling relies on the assumption that particular topics are defined by words co-appearing in a given context. Hence, the definition of \"context\" is crucial to allow for any reliable observations. A few different solutions have been suggested (e.g. Blei, 2012;Jockers, 2013). In our approach, we did not split input texts into smaller samples, which was motivated by the fact that the vast majority of the studies published in Teksty Drugie are rather short.Other parameters used in the study included: a stop word list containing 327 words (mostly function words, numerals, and very common adverbs), 100 topics extracted in 1,000 iterations, with the obvious caveat that this choice was arbitrary. ResultsA general overview of the obtained results shows a few interesting patterns. Firstly, we analysed and categorised the topics on the basis of their predominant words. The categories are as follows: literary theory (e.g. literature, fiction, text), poetics (e.g. verse, novel, short story, rhetoric) and methodological approaches (e.g. deconstruction, comparative literature, postcolonial studies, psychoanalysis); history of literature (e.g. romanticism, contemporary poets) and cross-cutting research themes (e.g. death, politics, literacy).A thorough exploration of such models requires a topographical visualisation capable of showing the connections between various topics, which often share a key word (cf. Goldstone and Underwood, 2012). The network (Fig. 1) is too large to be adequately rendered in this paper (a higher resolution image of Figure 1 is available online), yet even without the knowledge about concrete topics presented, we may see (partly thanks to ForceAtlas2 layout, which highlighted this feature) that groups of topics in our corpus are concentrically distributed. This onion-like distribution allows us to distinguish between the central topics (i.e. those who appear in many different papers) and those who appear less often or sporadically and hence are not particularly well-connected with other topics. For instance, in the geometrical centre of the network we may find topics and words pertinent to literary scholarship: literature, literary, comparative literature, national literatures, Jewish studies, fiction, together with some names of contemporary authors. Outliers are also interesting, and could be assigned to 3 groups: (1) expressions in foreign languages, (2) particular research topics or discourses which introduce quite a hermetic language, not shared in other topics, (3) noise (e.g. word bits generated through some errors in OCR). Yet it has to be noted that even the most accurate rendering of the topical distribution is still only a static snapshot insensitive to changes. In order to see the evolution of topics, we need to visualise them on a temporal axis. Due to a shortage of space we present here only a few examples, to show the application of our method. All dot plots are presented below with a trend line based on two period moving average. Fig. 2 represents the gradual shift of interest from more literature-oriented approaches, to the cultural ones. Both red (topic 19: literature, literary, writer, work) and green (topic 5: literature, research, theory) seem to be dominating until approx. 2007, when the blue line (topic 49: culture, cultural, social) overtakes the green line for the first time. Three years later it becomes the dominant approach, marking the shift in the overall content of Teksty Drugie. Topic analysis allows us to not only trace the evolution of the journal itself but also to see how the real-world events shape the topics undertaken by literary scholars. Fig. 3 shows the influence of the political transformation in Poland on the content of Teksty Drugie. We see a similar pattern in trends of all topics presented: grey (topic 60: power, society, state, fight, war, law), red (topic 36: political, communism, Polish People's Republic), blue (topic 7: Polish, Pole, national), yellow (topic 94: censorship, exile, novel, positivism, country, London, political). All of them are quite important in the early 1990s and the interest gradually fades until the end of this decade. The spikes around 2001/2002 are caused by the publication of monographic issues which make certain topic more dominant. E.g. Issue No.1-2/2000 was dedicated to socialist realism hence the spike of \"communismrelated\" issue in that year.This trend shows how political events (namely the transformation and forming of the new democracy) are dominating even the literary scholarship. It could be also the case that more politically charged issues (e.g. history of censorship in Poland) could have been published only after the fall of the communism, hence so many articles in that period. The last trend we would like to discuss is the emergence of the Holocaust studies in Teksty Drugie. As we can see in the Fig. 4, the red trend line (topic 59: Jew, Jewish, antisemitic) is visible on the fairly same level all through the 25 years, whereas the blue one (topic 18: testimony, Holocaust) is virtually nonexistent until 2001. This sudden boom can be linked to the publishing of the Polish edition of Neighbors by Jan Gross (2000) and the investigation into the role of Polish civilians in the genocide perpetrated in the city of Jedwabne during the World War II. This case opened a long process of re-investigating the troubled Polish-Jewish past, which could be traced also in the issues of Teksty Drugie. ConclusionsIn this study we tried to show how extra-textual events influence the content of literary scholarship on the example of Holocaust studies and political transformation, which entailed the prevalence of topics related to politics, power, society, state, and communism in the early 1990s. In the subsequent studies we plan to compare the results of topic modelling with bibliographical data in order to check whether the dominance of a certain topic stems from the large number of scholars who pursue it, or if it instead depends on the fact that a small group of authors published more often than others.",
        "article_title": "Topic Patterns in an Academic Literary Journal: The Case Of Teksty Drugie Modelling Literary Scholarship",
        "authors": [
            {
                "given": "Maciej",
                "family": "Maryl",
                "affiliation": [
                    {
                        "original_name": "Institute of Literary Research Polish Academy of Sciences",
                        "normalized_name": null,
                        "country": "Poland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " BackgroundTo say that digital reading platforms (web, ereaders, smartphones, etc.) are changing the way reading has been practiced in the age of print is now a truism. Often, these shifts have been the object of elegiac meditations (Birkerts, Manguel) and pedagogical concerns (NEA's \"Reading at Risk\" and \"To Read or Not to Read\"). These approaches highlight the 'erosion' that the practice of reading is undergoing. A useful corrective has been Andrew Piper's Book Was There. His call to think about reading from the basis of \"the relationship between reading and hands, [and] the long history of how touch has shaped reading and, by extension, our sense of ourselves while we read\" (3) has brought to light the bodily handling of reading objects. Our understanding of reading and text as a performative kind of practice has begun to take shape. For Rita Raley, text is \"the whole of the event, its physical, logical, and conceptual architecture; the enactment and experience; its temporal structures; and associated social and juridical protocols\" (2013: 21). Similarly, for Johanna Drucker, we need to elaborate \"a different conception of artifacts (books, documents, works of textual or graphic art), one in which reception is production and therefore all materiality is subject to performative engagement within varied, and specific, conditions of encounter\" (2014). Electronic Literature InstructionsThough a sizeable body of e-lit scholarship has focused on matters like textuality, software, meaning making systems, code, preservation, etc. reading has not been at the forefront. The experimental qualities of e-lit have produced a great diversity of interaction modalities and ways of handling the objects that supports said experimentation. Nevertheless, an understudied convention of e-lit are the instructions accompanying most works. These instructions go from some of the most radically sensuous like Serge Bouchardon's Blow, \"Blow to read the text then to spread the words. This scene requires a microphone\" (Fig 1); to more conventional ones like \"click your mouse at the right edge of the screen to move right to a new region of texts [and] tap the arrow keys to move\" from Nick Montfort and Stephanie Strickland's Sea and Spar Between (Fig 2). Strickland's Sea and Spar Between The paratextual dimension of these instructions has been on the margins of the studies focusing on particular works. Cumulatively, however, the information kept on these reading instructions can signal the shifts that e-lit has enacted on reading in digital platforms. Even more broadly, as a sample of the many experimental and still unstable standards of reading in digital platforms, e-lit reading instructions can offer evidence of practices that may be becoming more integral to the act of reading. Similarly, the mentions of the hardware (mouse, microphone, keyboard, etc.) required to read e-lit works provide detailed insights on the historical role that technological developments have enacted on reading as a performative engagement. Metadata and DescriptionThe new affordances offered by e-lit increase the scope of bibliographic practices in ways that allow for more coordinated retrieval but also create greater depth and specificity in records as datasets. Many descriptive metadata standards have been largely shaped by the predominance of print media, and while its qualities overlap with those of e-lit, emerging characteristics of the latter require extensible structures to be considered comprehensively described. E-lit specific metadata sets have been developed by initiatives like Electronic Literature as a Model of Creativity and Innovation in Practice (ELMCIP) and the Electronic Literature Directory (ELD). However, among them there is still a variety of descriptive approaches and objectives. The Consortium on Electronic Literature (CELL) has taken significant steps towards interoperability and a \"consensual model for the object of this field\" (Baldwin) while still acknowledging the particular interest of individual archives and datasets. Efforts like this may still be enhanced through greater reconciliation with existing bibliographic procedures. In e-lit the interaction between a work and a reader poses new ontological considerations for descriptive orientations suitable to examine the particularities of reading in digital platforms. Hayles' concept of intermediation proposes that the emergent processes that occur when reading e-lit forms a dynamic heterarchy in which both reader and work continuously inform and shape the trajectory by which reading unfolds (2007:100). This process reframes the ontic nature of work and reader, dissolving the duality into a single event. New qualities surface as a result, highlighting the different modalities through which intermediation occurs and inform bibliographic projects. Navigating these qualities and the language with which they are described poses new challenges for interoperability with similar collections in addition to being synthesized into the extensive legacies of bibliographic practices. The HandbookA Handbook of E-Lit Reading collects screenshots of the reading instructions of e-lit works and documents them according to storage medium (web, iPad, CD-ROM, floppy disk, etc.), actions or gestures (blow, click, scroll, type, etc.) and hardware (web cam, keyboard, mouse, microphone, etc.). The pilot target corpus includes ~200 works from over twenty countries anthologized in the Electronic Literature Collections Vol. 1, 2, and 3. The data extracted from the instruction pages will be analyzed and cross- referenced in order to observe emerging patterns like most common practices, most resilient ones, the rise and fall of some of them, as well as the introduction of new technological developments in hardware or software that have marked important shifts in the creation and reading of e-lit. This corpus, though not exhaustive, provides a starting point to design a suitable metadata set and test the categories included in it. Ultimately, the corpus will also encompass enough data to start drawing a hypothesis and identify future directions for the project.Fig 1 .1Fig 1. Screenshot from Serge Bouchardon's Blow ",
        "article_title": "A Handbook of Electronic Literature Reading",
        "authors": [
            {
                "given": "Élika",
                "family": "Ortega",
                "affiliation": [
                    {
                        "original_name": "Northeastern University",
                        "normalized_name": "Universidad del Noreste",
                        "country": "Mexico",
                        "identifiers": {
                            "ror": "https://ror.org/02ahky613",
                            "GRID": "grid.441462.1"
                        }
                    }
                ]
            },
            {
                "given": "Erik",
                "family": "Radio",
                "affiliation": [
                    {
                        "original_name": "University of Arizona",
                        "normalized_name": "University of Arizona",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03m2x1q45",
                            "GRID": "grid.134563.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Short DescriptionMy talk will focus on the problems and possibilities of conserving historic text generators, 'historic' meaning roughly . For this purpose, I will develop the scientific interests in these generators either from the perspective of literary history and from the perspective of the history of knowledge, since many of the text generators in question were connected explicitly to scientific interests.A system for conserving and editing historic text generators, that will enable researchers from both of these fields to access historic text generators in order to study their esthetics and functioning needs, in orderto take into account not only the generated texts, but also the generating texts, meaning software and prerequisites such as flowcharts. In my talk, I will focus on the materiality and the potentiality of the historic text generators in order to propose a platform solution that should enable scholars to edit (and publish) historic text generators. Full AbstractIn their manifesto Zur Lage ( Bense and Döhl (1964)) (State of Things) Max Bense and Reinhard Döhl name several tendencies of contemporary literature. The sixth and last point of their list calls on a cybernetic and material poetry (\"kybernetische und materiale Poesie\"). Historically and biographically, Bense and Döhl were placed at one of the centers of concrete poetry and concrete art (Stuttgart, Germany), and their manifesto reflects the artistic and academic developments made in these areas. But already in 1964, when Bense and Döhl published their manifesto, experiments had taken place that had taken the notion of a cybernetic poetry quite literally, using computers and software to generate literary texts. (Strachey Okt. 1954;Lutz 1959;Levin 1963;Gunzenhäuser 2004;Link 2004) The automatic generation of texts by way of combining syntax, vocabulary and a (pseudo) random generator that serves to fill the syntax positions with proper words from the vocabulary has a history dating back to the 17th century. However, it was the use of computers that prompted artists and researchers to expand their experiments and develop complex setups applying hundreds of syntactical structures and bigger vocabularies (as did, for example, Stickel [1967] ).The research on early text generators has focused on biographical data and the literary history of automatic text generation (Bülow 2007), the autoreflexivity of digital texts (Cramer 2003) and the genealogy of text generating systems (Link 2004). A literary and cultural history of text generators needs to take into account not only the texts that were produced, but also the modes and methods of production. In my talk, I will argue that in order to understand the history of automatic text generation, we need to join these different perspectives and research either the generated texts and the modes and methods of text generation. Now the first problem when dealing with text generators (and the texts generated with them historically) is the availability of the material. Only a small fraction of historically generated texts has been published, and in most cases the documentation of the conceptual and technical setup is only partly available through publications, not to mention the actual code that was used to produce the text in the first place. This is due to the fact, that literary scholars have tended to ignore texts of this kind -that is, experimental text which barely fulfills any definition of poetry - and are only beginning to recognize them as a certain type of literature in its own kind. The second reason for a newly developed recognition for early text generation experiments is the fact that automatic text generation has become ubiquitous in our time, from simple twitter bots that employ the same methods as the early text generators (meaning simple syntactical structures filled with randomly chosen words), via the modern ELIZAs of contemporary electronic assistant systems, to complex machine learning algorithms that synthesize Shakespearean plays. (Goodwin 2016) So in order to further research on early text generators, the multiple dimensions of text generators will have to be made available to researchers from all fields concerned with the history of automatic text generation. Since I am, by profession, a philologist, my approach is limited to the document side of things and does not consider hardware conservation or emulation.Documents connected to text generators comprise flowcharts, punched tape, source code data on magnetic memory, print-outs, project documentation in various paper formats. For a digital representation, these materials can be digitized either in graphical or in textual format. Graphical representations can be annotated with their respective metadata describing their material, function, authorship etc. As for the texts contained in a text generator as a historic object, we are dealing with three different kinds of texts:• texts that have been generated • texts that have been used to generate texts (mainly software)• texts that could have been or can potentially be generated (the full outcome of the underlying algorithm)Texts in groups 1 and 2 are connected in a genetic way. If we take the Stochastic Texts by Theo Lutz and Rul Gunzenhäuser (Lutz 1959) as an example, we can see that there is a published version of the generated texts and a raw version (published in Büscher, 2004). The differences between these versions are significant. From a text-genetic perspective, these differences lead to the production of the texts or, in other words, the execution of the underlying program. The execution of the program leads to the implementation of the program or the source code, the source code leads to the conceptual implementation of the algorithm (e.g. flowcharts) and thus to the abstract underlying algorithm.Thus, a genetic perspective on text generators leads not only to the raw version of the output of the computer, but to the implementation of the algorithm in source code as well. The title of my talk searches to reflect this notion of text genesis, because the link between source code and output is not a certain similarity between two texts, but a functional, algorithmic connection. I think that problematizing this kind of functional genetic connection can also be fruitful for philology dealing with texts where computers don't generate all of the text, but help in generating certain features of the text. In order to describe and show this kind of connection in different text generators, it will be necessary to develop standardized notions for annotation schemas (Currently, the annotation of source code is not part of the TEI Guidelines (TEI-Consortium and Lou Burnard 2014)Finally, studying the poetics of a text generator might also mean to study more generated text than is (historically) available. A representation of a text generator thus not only should comprise the source code itself, but also the possibility to run the algorithm in order to generate some of the potential texts. The best solution would be, of course, to run the code on the historic machines it was developed for. Not only would this repeat the original experiments, it would also hopefully reproduce the glitches that can be seen in the raw output that has been preserved and archived. (Büscher 2004) However, such a procedure would be way too costly. An easier way is to implement the reconstructed algorithm in a modern programming language and to offer researchers the possibility to run the code without being forced to undertake major retrocomputing tasks just in order to gather some more textual outcome.Since this is a work in progress, I am confident that by August 2017 I will be able to outline not only the problems, but also some solutions for implementing a platform and database to preserve and present historic text generators.   ",
        "article_title": "Genetic Editions to the Extreme? Conserving Historical Text Generators",
        "authors": [
            {
                "given": "Claus-Michael",
                "family": "Schlesinger",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Broad access to online digital humanities projects raises questions about who our audiences are and how we respond to their engagement with our work. Using Project Vox as a case study, we highlight motivations and implications for providing audiences more access to decisions that shape the evolution of a project. In doing so, we hope to encourage critical discussion about how we conceive, assess, and engage the diverse audiences of digital scholarship.",
        "article_title": "Access for Whom? Rethinking Audience and Editorial Decisions through Project Vox",
        "authors": [
            {
                "given": "William",
                "family": "Shaw",
                "affiliation": [
                    {
                        "original_name": "Duke University",
                        "normalized_name": "Duke University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00py81415",
                            "GRID": "grid.26009.3d"
                        }
                    }
                ]
            },
            {
                "given": "Liz",
                "family": "Milewicz",
                "affiliation": [
                    {
                        "original_name": "Duke University",
                        "normalized_name": "Duke University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00py81415",
                            "GRID": "grid.26009.3d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Consortium, is an online and annotated digital reconstruction in Unity 3D of a seminal battle of the Irish Easter Rising of 1916. A goal of the project was to leverage phenomenologies of time and space as afforded by virtual world technologies to provide new insights and help to answer what had heretofore been intractable questions about how the battle unfolded. The battle, as well as the 3D reconstruction, received a great deal of attention during the Centenary of the Easter Rising, including the specially commissioned radio documentary (Documentary on One, RTE Radio 1) Battle at the Bridge. Building on the interest in and success of this project , the project team was encouraged to further develop the project so that it could be used in secondary schools. Due to the exigencies of technology in Irish classrooms, including limitations on booking lab time with many computers too old to run the VR software, it was decided that a different approach would be taken. Hence the BMSB Augmented Reality mobile app was developed (figure 1), to educate students about the facts of the battle, while providing a deeper and more holistic understanding of war and its effects. At the same time, it provided the project team with the opportunity to leverage digital humanities research within a participatory engagement setting to reach out to second level students and teachers to provide them with opportunities of engaging with cutting edge technologies integrated into a student-led learning environment. This project shares a philosophic approach with other maker projects in the Digital Humanities, as Say-ers et al. (2016), quoting Neil Gershenfield, describe in A New Companion to Digital Humanities as \"the pro-grammability of the digital worlds we've invented\" applied \"to the physical world we inhabit\" in which objects \"move easily, back and forth, in the space between bits and atoms.'' Thus Augmented Reality (AR), a technology that superimposes digital content, including images, animations, and annotations, over the real world, was decided on as the key technology to translate our research into a classroom setting. Previous research on the affordances of AR for teaching and learning have highlighted that interactivity, collaboration , problem-solving, and narratives mediated through technology can aid both engagement and understanding (Dunleavy et al., 2009). Several AR applications have been developed for primary and secondary education for fact-based topics, such as geometry, astronomy, chemistry, and the human body (see for example Chromville Science; Anatomy 4D; Elements 4D. However, AR applications for humanities subjects are limited. To the authors' knowledge there is no history based AR application that is designed for use in the classroom, although there are several mobile-based student-centred AR for outdoor historic sites (see for example: Schrier, 2005 for the Battle of Lex-ington and Singh et al., 2014 for the Christiansburg Institute).",
        "article_title": "Phygital Augmentations for Enhancing History Teaching and Learning at School",
        "authors": [
            {
                "given": "Susan",
                "family": "Schreibman",
                "affiliation": [
                    {
                        "original_name": "Maynooth University",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            },
            {
                "given": "Brian",
                "family": "Hughes",
                "affiliation": [
                    {
                        "original_name": "Maynooth University",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            },
            {
                "given": "Neale",
                "family": "Rooney",
                "affiliation": [
                    {
                        "original_name": "Maynooth University",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            },
            {
                "given": "Colin",
                "family": "Brennan",
                "affiliation": [
                    {
                        "original_name": "Maynooth University",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            },
            {
                "given": "Maynooth",
                "family": "University",
                "affiliation": [
                    {
                        "original_name": "Maynooth University",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            },
            {
                "given": "Ireland",
                "family": "Fionntan",
                "affiliation": [
                    {
                        "original_name": "Maynooth University",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            },
            {
                "given": "Mac",
                "family": "Caba",
                "affiliation": [
                    {
                        "original_name": "Maynooth University",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            },
            {
                "given": "Hannah",
                "family": "Healy",
                "affiliation": [
                    {
                        "original_name": "ie Maynooth University",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe World Wide Web provides the research community with an unprecedented abundance of primary sources for diachronically tracing, examining and understanding major events and transformations in our society (such as the rise of euroscepticism or the impact of the recent economic crisis). For two decades, public and private institutions have preserved these born-digital materials for future analysis (Gomes and Costa, 2011). However, these collections are now so large that it is infeasible for researchers to study political and social phenomena by examining them in their entirety.Creating event collections. A common solution that web archives are currently adopting for sustaining the use of the collected sources in humanities research is to offer topic-specific collections. For example, on Archive-it, the Internet Archive presents a few collections on large-scale events such as the Boston Marathon Bombing, Black Lives Matter and the Charlie Hebdo terrorist attack.The collections are curated \"by the Archive-It team in conjunction with curators and subject matter experts from institutions around the world\" .Another solution for creating event collections from large datasets is a filtering approach that collects only documents that mention the name of the event; this method has been employed for example in temporal summarization tasks (see Aslam et al., 2013).Current limitations. The collections created following one of these two approaches share crucial limitations: a) they are small in number; b) the selection process is not always transparent; c) they generally offer only documents that are closely related to an event but lack information on background stories as well as contextual clues. Especially the latter is a crucial issue for historical analyses.Our vision. We are currently developing a solution for creating event collections that identifies not only the core documents related to the event itself, but most importantly sub-groups of documents which describe related aspects. We do so through an expansion process that is informed by latently relevant concepts and entities from a knowledge base, whose presence in documents is interpreted as one of many indicators of relevance.Specific contribution. At the DH conference we intend to present the final results of our study, together with its application for supporting research in political and social history. MethodLet us consider an event, for example the Syrian Civil War, as a node in a knowledge graph (e.g. DBpedia).As a first step, a domain expert will identify a series of other entities in the knowledge graph that are highly related to the event (in Nanni et al., 2016, we show that this step could be automatized adopting a simple relatedness measure). These could be people, such as Bashar Al-Assad as well as countries (e.g. Turkey, Russia, United States), concepts (e.g. Protests) and other specific events (e.g. The Refugee Crisis). These initial seeds will support us in retrieving other related entities and concepts from the knowledge graph in an automated fashion (we described our solution in Nanni et al. 2016).While retrieving related entities is important, these are meaningless without human-readable descriptions of the entity's relation to the event. As a matter of fact, the entity United States has many different aspects, and only few of them are related to the event Syrian Civil War.In order to retrieve entities in context, we use Wikipedia as an initial corpus. Next, relevant passages from the documents are identified in the collection by information retrieval. Having the entity in context will tell us with which words, concepts and other entities it frequently appears together (a complete overview of the method is presented in Nanni et al., 2017). For example, if a document mentions the United States together with James Foley and ISIS, it is likely to be related to the Syrian Civil War, even without mentioning these words explicitly. Case studiesWe are currently working on two different research tasks:1. The first study is focused on identifying political speeches on foreign events (such as elections in other countries) in the US Congressional Records (1989Records ( -2016, which are available on Congress.gov and through the Internet Archive (Congress.gov provides full-text access to daily congressional record issues dating from 1995, beginning with the 104th Congress. Proceedings for previous years are available on THOMAS). The goal is to measure the amount of attention that US politicians give to international events in correlation with other internal affairs. 2. In the second study, we intend to detect similar patterns during the early rise of anti-establishment protests. Our aim is to uncover small events, which did not turn into large-scale insurrections and therefore are not studied sufficiently. The work is conducted on a large (16 terabyte) web archive of news, blogs, forums and social media, namely the TREC Streaming Corpus (This corpus is a huge web archive collection collected between 2012 and 2014). Finally, the goal of the project is to obtain a better understanding on how and why specific protests succeed while others do not (also in correlation with analyses from the previous study). ExperimentsIdentifying related entities. In a previous work ( Nanni et al., 2016), we have first established the quality of our entity-relatedness solution (Eventipedia), by comparing it with a series of other baselines commonly used in the field. The results are reported in Figure 1. Figure 1. Evaluation on entity-event relatedness (from Nanni et al., 2016). While our entity-relatedness solution outperformed the other tested methods, it also showed a few limitations. In fact, this approach (in its fully automated fashion) tends to privilege specific entities over the most commonly mentioned entities. We address this in developing techniques that are supervised by domain experts; this ensures us to always consider the most relevant related entities.Collect entities in context. Additionally we studied the identification of human-readable descriptions of the entity's connection to the event. We compared our entity link-based approach with a common information retrieval heuristic which considers the first sentences of the entity's Wikipedia article, as a relevant passage (Wiki-Intro). The results are presented in Figure 2. In 45% of the cases, the Wiki-Intro was a sufficient explanation. However, our Eventipedia approach provides sufficient explanations in 68% of the cases. We remark that for nearly all cases, where Eventipedia does provide a snippet, this is also relevant. In contrast, the Wiki-Intro only provides a good explanation in 42% of the cases. This is because many event-relevant entities (e.g. the United States) are often more popularly known for other accomplishments and therefore the first paragraph is not a good description of entity involvement in the event.Retrieve relevant documents. We are currently assessing the quality of our information retrieval solution, which uses entities and contextual passages to retrieve documents about specific events with an approach based on Dalton et al. (2014). We report here the very first results of our study on retrieving speeches about foreign elections. This work has been conducted both on the US Congressional Records on the New York Times Corpus.We compared it with two baselines, a) retrieving all documents that mention the name of the country (e.g. \"Syria\") and b) retrieving all documents that precisely mention the name of the event (e.g. \"election in Syria\"). It is evident that the first solution is recall-oriented, while the second, already adopted by the TREC temporal summarization task, favors precision.Given an event, such as the Syrian presidential election, 2007, all three methods produce a ranking of documents. We examine the quality of the ranking considering 15 different elections. For each election, we created a gold standard of 45 documents (relevant and non relevant). Table 1 presents the results in term of mean-average precision (MAP) on the two datasets. The results of this initial study lead to a few findings. First we see that, especially on the US Congressional Records, using the name of the event permits to collect a fraction of relevant documents, but not all of them. Second, our solution, which uses related entities in context, provides good performance quality on both datasets. Third, our solution is able to identify materials that do not explicitly mention the name of the event.\"Non-relevant\" documents. We next analyze what kind of \"non-relevant\" documents the different systems retrieved among the top elements of the ranking. Non-relevant documents retrieved by the \"Event name\" solution often discuss different topics and simply mention the event out of context; these documents could be for example general summaries of the previous week.Instead, non-relevant documents retrieved by Eventipedia are often related to the political activity of a foreign country, but not specifically about the election. For example, they could mention the visit of a candidate to Washington, a few months before the vote. It is evident that choosing one method over the other will shape the event-collection in a different way. Ultimately, it is up to the humanities researcher to decide which documents are most important for the analysis.",
        "article_title": "Building Entity-Centric Event Collections For Supporting Research in Political and Social History",
        "authors": [
            {
                "given": "Federico",
                "family": "Nanni",
                "affiliation": [
                    {
                        "original_name": "University of Mannheim",
                        "normalized_name": "University of Mannheim",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/031bsb921",
                            "GRID": "grid.5601.2"
                        }
                    }
                ]
            },
            {
                "given": "Nikolay",
                "family": "Marinov",
                "affiliation": [
                    {
                        "original_name": "University of Mannheim",
                        "normalized_name": "University of Mannheim",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/031bsb921",
                            "GRID": "grid.5601.2"
                        }
                    }
                ]
            },
            {
                "given": "Simone",
                "family": "Ponzetto",
                "affiliation": [
                    {
                        "original_name": "University of Mannheim",
                        "normalized_name": "University of Mannheim",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/031bsb921",
                            "GRID": "grid.5601.2"
                        }
                    }
                ]
            },
            {
                "given": "Laura",
                "family": "Dietz",
                "affiliation": [
                    {
                        "original_name": "University of New Hampshire",
                        "normalized_name": "University of New Hampshire",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01rmh9n78",
                            "GRID": "grid.167436.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionBesides time, characters and plot, space is one of the main components in storytelling. But despite its importance as a category for the setting of narrative action and unlike the other mentioned categories, the conceptualisation of space has long been neglected in narratological research. This holds true even after the so-called spatial turn (Soja 1990) in cultural history, that lead to a renewed interest and to fruitful insights into space as a metaphorical concept. However, a systematic description of the means by which space is created in narratives is still in its beginnings (e.g. Den- nerlein 2009, Piatti et. al. 2009. This is at least partly due to the fact that space poses substantial problems for modeling. The creation of space in narratives is often dynamic and based on implicit information: Rather than constructing a given, mathematical space beforehand, stories tend to evolve their setting in relation to its characters that constitute space through their actions. Spatial information in stories therefore highly depends on the characters that act, move or perceive within it. Especially in fiction, this also means that spatial information is often fuzzy and imprecise (Piatti et. al. 2009), since narrators quite frequently are more interested in telling a story than designing a detailed, coherent setting for it. Whereas these problems are hard to handle in traditional literary studies, they present serious yet interesting challenges for a digital formalization.In our paper, we will illustrate the complex tasks that have to be tackled by a digital narratology of space based on an exemplary annotation workflow, that we will outline for the description of spatial elements in Jules Verne's Around the world in Eighty Days. Challenges for a digital narratology of spaceWe describe the following major problems that can be grouped into a chain of work-tasks:1. Basic information on the setting of a narrative can be retrieved by extracting the place names from a text (NER). However, an automatic extraction is flawed by well-known problems of disambiguation (cf. for example, Barbaresi / Biber 2016). Since space in narration is highly dependent on characters that are placed in it, these entities have to be detected as well. 2. Place names are not the only kind of spatial information that can be found in texts. Besides others, space is also constituted with the help of nouns that do not necessarily have an inherent spatial component (for example, a car in a text can be the subject of a description, but it turns into a space marker if someone enters it). 3. Spatial entities (names and nouns) can be referred to by co-reference. 4. Spatial entities in a text are not always the setting of narrative actions. Place names or nouns can also only be mentioned, dreamed of, remembered, reflected on etc. This different functionality has to be taken into account when it comes to the automatic generation of literary maps (eg. Moretti 1998, Piatti 2008. To capture this opposition, Dennerlein (2009) separates event regions from mentioned spatial objects in her conception. Event regions are defined as spatial zones, where events take place. In contrast, mentioned spatial objects contain all spaces that are not event-related. Piatti et. al. (2009) develop a similar model: Their concept of setting closely corresponds with event regions, whereas projected space and marker give a finer differentiation of the notion of mentioned spatial objects (cf. Figure 1).  Annotation WorkflowThe complexity of spatial information demands for a multi-faceted approach. Figure 2 shows a spreadsheet with the beginning of chapter 14 of Verne's Around the World in Eighty Days that has automatically, semi-automatically and manually been enriched with multiple layers of annotation.First, the text was tokenized, lemmatized and POStagged (columns 1-4).Second, Named Entity Recognition (NER) was applied to the text (columns 5, both steps were performed with Weblicht [2012]). The NER also identifies the names of the characters. The results of the NER have to be corrected manually. To improve the automation of this step the exploitation of other toponymical bases like Geonames and OpenStreetMap will be discussed.Thirdly, to generate column 6, we used theme-specific wordlists that we built on the base of existing lexicological ontologies (GermaNet for German Texts [Hamp/Feldweg 1997, Henrich/Hinrichs 2010. English Wordlists as shown were provisionally generated manually, but can be built in a similar way). These wordlists were used to automatically tag the text. In Fig. 1, 'valley' has been annotated as LSC, which means that it belongs to the word field landscape. So far, we created wordlists for landscape and architecture (in German), which cover a high amount of place nouns. The annotation can be manually supplemented and new wordlists can be created.Fourthly, in column 7, 'the beautiful valley of the Ganges' was (manually) annotated as event region according to the model of Dennerlein (2009). An automatic differentiation between event regions and mentioned spatial objects will be a challenging task. However, we consider a rule-based extraction of dependency paths to approach the problem Figure 3 shows a parse tree of the sentence from Verne's text. The pattern [Character -SUBJ -Verb of Motion -OBJ -place noun] is likely to indicate an event region. By gaining several similar patterns with high precision regarding the identification of event regions, we assume to collect features for a future implementation of machine learning methods.Fifthly, in column 8, coreferences were annotated manually. We will consider different kinds of co-references: Spatial entities can be referred to by nouns (e.g. 'Paris / 'city') or pronouns. Also, certain deictics ('here', 'there') might refer to spatial antecedents. However, a reliable automatic coreference resolution, which would be highly desirable for many kinds of narratological analysis, is out of the scope of this paper.Finally, we included a column (9) for annotations that are based on the modified tag-set of the ISOSpace-standard (Pustejovsky et. al. 2011a und b), as they are presented in the SpaceEval Annotation Guidelines (2014). Visualizations and OutlookWith the help of this semi-automatic and multi-layered method, we hope that we can make use of the strength of the different approaches and combine their advantages (it would be highly beneficial, for example, to combine Dennerlein's category with an ISOspace-annotated text to enhance their rule-based detection).The potential of their combination shall be demonstrated by two examples of visualizations that draw on named entities and wordlists. A network of spatial markersAs Piatti et. al. (2009) pointed out, the impreciseness and semantic potential of spatial information in literary texts sometimes demand visualizations other than geographical maps. Figure 4 shows a co-occurrence network of characters and place markers in Around the world in Eighty Days: Characters appear in red, place names in yellow. Place nouns are divided into the sub-categories landscape (green), architecture (grey) and transport (blue). In a straightforward approach, we established edges whenever a character and a spatial marker appear in the same sentence. The nodes have been sized according to their degree (the number of their connections), which can be related to Juri Lotman's (Lotman 1977) concept of mobile vs. immobile characters: Characters which are connected to many places like Phileas Fogg and Passepartout are more likely to be main characters than characters with a lesser degree.(The visualization was established with Gephi [ Bastian et al. (2009)]).Word-list-based Frequency Analyses Figure 5 shows the distribution of landscape and architecture terms over the whole text of Around the world in Eighty days compared to a corpus of 451 German novels taken from the TextGrid-Repository (Text- Grid Konsortium 2006-2014, licensed CC-BY-4.0), which cover a time range from 1700 to 1920.Every text was chunked into 10 'segments' (x-axis), for which we calculated the relative percentage of the vocabulary from the corresponding word field ('value', yaxis). The graph shows a noticeable peak in the use of architectural vocabulary in the last third of the text, which can serve as a starting-point for a close reading of the text. However, to take full advantages of distant reading techniques for spatial analysis, more refined methods and annotated corpora are necessary. We hope that these methods can be developed by considering the challenges outlined in our basic model in this paper. ",
        "article_title": "Towards a Digital Narratology of Space",
        "authors": [
            {
                "given": "Gabriel",
                "family": "Viehhauser-Mery",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            },
            {
                "given": "Florian",
                "family": "Barth",
                "affiliation": [
                    {
                        "original_name": "Universität Stuttgart",
                        "normalized_name": "University of Stuttgart",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04vnq7t77",
                            "GRID": "grid.5719.a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "In its ability to extract feature sets, relate texts within an abstract space, and semantically parse groups of texts, computational textual analysis has functioned primarily as a formalist intervention into literary study. When practitioners venture outside of the formal features of the texts themselves, it is author or date that serves as the point of contact between the text and its wider context. And yet, texts offer a rich history of reception: as different interpretive communities (Fish 1980) receive and reinterpret novels, poems or plays, they recontextualize the literary object to suit the particular socio-cultural goals of their period or nationality. Lacking detailed accounts of reading practices at large scales, even traditional practitioners of literary history have been unable to reconstruct the history of reception of even the most historically canonical texts. In this project, we leverage the ability of Digital Humanities to recover, at least provisionally , a large-scale history of textual reception by exploring the patterns of citation that reveal the attention paid to specific texts across their history as read-erly objects. How are certain canonical texts cited over time and what can the attention paid to different segments of texts with a rich reception history tell us about the reading or social practices of different historical periods? How do different groups of readers (particularly authors and critics) quote text differently as they make use of passages in their own writing? And how do specialists and non-specialists cite the same text differently? By exploring the locus of attention within a canonical text, both across groups of readers and across history, we provisionally reconstruct a historically and socially contingent map of a text's reception history. As Piper and Algee-Hewitt have argued in \"The Werther Effect\" (Piper and Algee-Hewitt 2014), practices of citation, the embedding of the language of a text within other works, can reveal patterns of reception even within a single author's corpus. In this project , we expand this approach multi-dimensionally, identifying passages of canonical works quoted in other novels, in critical articles by specialists and non-field specialists, and in a larger undifferentiated corpus of text. The scale of our analysis enables us to identify what parts of a text have received the most writ-erly attention overall and how that attention has been shaped over time. By moving from semantics to passages , we switch our attention from the intangible metrics of semantic similarity, to specific, quotation-level instances of citation that demonstrate specific attention to identifiable parts of our target texts. Drawing on work in sequence alignment by David Smith et al (2013) and Richard So et al. (forthcoming), we will therefore be able to explore patterns of attention that have been paid to a text by identifiable groups of readers. We argue that these patterns of citational-ity serve as a proxy for the reception of a text: while necessarily limited to readers who themselves were authors (or critics), they nevertheless represent an important category of reception available to analysis. To extract the quotations, we used Python's \"dif-flib\" module, wrapped up as a parallelized MPI program that runs on an HPC cluster. To compare any two individual texts-for example, when checking for passages from Hamlet inside of a novel from the Gale American Fiction corpus-the texts are first split into tokens and passed through a filter that removes a set of 200 stopwords. This speeds up the alignment algorithm (the high-frequency words that get pulled out make up a significant portion of the total words in any given text, producing shorter sequences) and also has the advantage of making the alignment process less sensitive to small changes in function words, which seem to get shuffled around or changed fairly frequently when a text is quoted. For example, a change from: And crook the pregnant hinges of the knee Where thrift may follow fawning to",
        "article_title": "Reconstructing Readerly Attention: Citational Practices and the Canon, 1789-2016",
        "authors": [
            {
                "given": "Mark",
                "family": "Algee-Hewitt",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "Mcclure",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            },
            {
                "given": "Hannah",
                "family": "Walser",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The map (open for update) has been designed via an open online tool Google My Maps (Department of Informatics Problems of the Humanities, 2017)Before designing the interactive map, we developed the DH centre database including such fields as title, address, abstract, and information about the unit head, website, contacts, and date of foundation. ArcGIS allows us to automatically process the data on the map according to the database fields and visualize the search results, providing an understandable picture of the digital humanities across the globe.The results of the preliminary study performed in 2015 have been published earlier ( Mozhaeva et al, 2016). In the present paper we focused on DH units rather than on papers, extended the data and updated it (as of August 2016). We have extended the number of resources, languages and DH units, finished an interactive map of DH centres and a catalogue of open access software for DH studies that are going to simplify the search for new partners, developing network projects.Those 430 units in 42 countries that we have found are spread over the world (Figure 1):• in Europe: 174 units, mostly in Great Britain (27), France (25), Germany (21) and Russia (17);• in Asia: 65 units, mostly in Japan (33) and China (16); • in North America: 9 units, including 7 in Brazil and 2 in Argentina; • in Pacific basin countries and Australia: 17 units (11 in Australia and 6 in New Zealand); • in Africa: 5 units (Republic of South Africa). The mostly widespread unit types are centres (139 units that is 32%), laboratories (67 units i.e. 16%), institutes (59 units, 14%) and university units (departments, subdivisions, faculties, schools) (57 units, 13%). There are 14 \"groups\", 13 \"initiatives\" and 13 'societies\" that give 3% of the general number of units that have been found.The dynamics of creating the DH units shows that about 20% of them have been created in the 1960-80's at the major universities as centres for the Humanities research that were positioned as DH units in 1990-2000. About 25% of the studied units have been created in the 1990's, 39% - in the 2000's, 16% - in the 2010's. A steady increase in the number of DH units is observed since the mid 2000's. In the 2010's there is active institutionalization of DH: permanent organizations, educational and research units (centre, laboratory, institution, department, chair, school) at major universities, research institutions, etc. are being established. Among the studied 430 DH units such units amount to more than 75%. At the same time, the number of temporary units and teams (projects, groups, initiatives, etc.) created to solve specific problems is being reduced.There is a growing attention to the educational activities in the field of DH (masters and postgraduate programs, short-term training and courses), which is typical for 26% of the studied DH units. The applied developments are becoming more and more important:• 22% of the studied units develop and introduce new digital tools, methods and models; • 27% of units provide a variety of digital resources, services and platforms, mobile applications, multimedia systems, 3D-models, GIS objects, etc.;• 7% of units develop online tools for learning.The analysis of the main directions of scientific and educational activities in the field of DH found that in the number of the DH units the focus is not only on the use of digital tools, but also on the study of the results of their application, the impact on the transformation of learning processes in the field of the Humanities and social sciences.We spotted researchers' consolidation in the framework of this research field, as well as the development of common principles, methods and scientific digital tools. We revealed 28 network associations (association, network, platform, consortium, alliance) that are both national and international, mostly continental and transcontinental (Mozhaeva et al, 2016).The development of DH network infrastructure goes along with the extension of information interactions among specialists from diverse DH units worldwide. The infrastructure of DH centres reflects the units of the DH information environment. The increasing trend of centres comes with the trend for extension of the information environment and of those information interactions that build it.Another trend in DH is intercultural information interactions that might become real during face-to-face meeting at the international conferences, workshops, seminars and so on, or continue virtually via the Internet technologies video presentations, webinars, forums, social networks, blogs and others. It is worth mentioning that virtual communication is very common for DH (it is possible via virtual research environments, communities, networks and associations, specialized online resources, services and platforms, Skype, blogs, forums, publications and discussions in social networks). Studying that type of interactions allows us to model the DH information environment and forecast the perspectives of this multidisciplinary field.An example of today's indicator of the intense and effective information interactions in DH environment is for instance a growth in number of blogs, e-journals and pages devoted to the DH, as well as number of subscribers to these editions. The social networks analysis (Twitter, Facebook, LinkedIn, Instagram, YouTube, VK) shows that the most popular and ever-growing platform for DH is Twitter. The number of subscribers to @DHQuarterly and @DHNow on Twitter is 7,034 and 23,6 thousand people accordingly in August 2016.The study that we performed allowed us to see the scale of the extension of digital humanities, visualize and localize 430 DH units, classify the major directions of their activity. The interactive map of DH centres extends the opportunities for research communication, facilitates setting the conditions for integrative processes in the development of Digital Humanities, as well as launching multidisciplinary and international research and educational projects. We consider the growing number of DH centres, as well as the qualitative change in their activity and expansion of the information environment, as evidence of the dynamic development of this field.",
        "article_title": "Infrastructure and information environment of Digital Humanities from the perspective of scientific analysis",
        "authors": [
            {
                "given": "Galina",
                "family": "Mozhaeva",
                "affiliation": [
                    {
                        "original_name": "National Research Tomsk State University",
                        "normalized_name": "National Research Tomsk State University",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/02he2nc27",
                            "GRID": "grid.77602.34"
                        }
                    }
                ]
            },
            {
                "given": "Polina",
                "family": "Renha",
                "affiliation": [
                    {
                        "original_name": "National Research Tomsk State University",
                        "normalized_name": "National Research Tomsk State University",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/02he2nc27",
                            "GRID": "grid.77602.34"
                        }
                    }
                ]
            },
            {
                "given": "Ulyana",
                "family": "Zakharova",
                "affiliation": [
                    {
                        "original_name": "National Research Tomsk State University",
                        "normalized_name": "National Research Tomsk State University",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/02he2nc27",
                            "GRID": "grid.77602.34"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionDigital humanities teaching programs are today widespread mostly in North American and European universities. However, they are very different in terms of content, focus and audience and as mentioned, they are distributed unevenly around the world, while often linked to diverse academic policies. Some programs are long-term focused, shaped as masters or graduate degrees (with well-known pioneer examples at King's College, Virginia or Pisa, with the so-called Informatica Umanistica). Summer and spring schools have also had great importance in this field, getting researchers into digital humanities as a complementary training and opening new horizons for traditional DH scholars. Names such as Oxford, Victoria and Leipzig deserve a special mention in this area.Networks like DiXiT, and DH infrastructures such as DARIAH and CLARIN are making continuous efforts to centralize, standardize, unify and inform on the different nature, options and advantages of DH training programs. Some examples of these efforts can be appreciated in the deliverables of DARIAH (Sahle 2013), the CUNY list of curricula, the DARIAH course registry and German open-access teaching materials, as well as many other resources available on the web, with special mention to the DARIAH Teach Erasmus plus project, which is now on the spot on the future for DH teaching. The Spanish CaseThis paper presents the Spanish DH programs developed at LINHD as an innovative case-study from the Spanish-speaking DH community with the aim to illustrate the global teaching panorama. Most of the syllabi of the DH courses at LINHD have been developed under a common collaboration between Spain and Argentina, focused on a broad Spanish-speaking audience and applying e-learning teaching methods. The use of these virtual technologies and methodologies is the success milestone of this initiative, as it constantly reaches students from different backgrounds and profiles and from different parts of the world (Spain but also Latin America and Europe) as long as they are Spanish speakers.We will first focus on the curricular design of all the programs offered by LINHD, followed by a detailed explanation of the teaching methodology. The Design Of A DH CurriculumLINHD was created in 2014 based on three axes: research, training and dissemination. It launched its first DH summer school: \"Introducción a las Humanidades Digitales\" in July 2014, getting more than 50 students involved. The summer school was followed by a longer DH program of 30 ECTS credits, lasting from December to September and amplifying all the contents that had been announced in the summer school. The strategy has been similar in the following years, in 2015, summer school was devoted to digital scholarly editing and a new longer winter training program was launched for 2015-2016. This last year, we decided to focus on DH methodologies applied to a specific field, poetry, which had as a result a high-quality summer school on this topic that is at the heart of LINHD projects and research interests. Taking into account the interests of the community, the needs of the field and the offers of new programs in different places, our new proposal for this year, starting in January 2017 is Semantic web technologies and language resources, introducing NLP possibilities for humanities, useful tools, and stylometry, Semantic web technologies will be the second focus of this summer school, supported and cofinanced by CLARIN as a CLARIN user involvement event.Our strategy for designing curricula for all these programs has been twofold: first, we have always used as a model the reference curricula for DH mentioned above, along with the needs of the scholars in our own environment. There have been three keys for success: having a DH compliant interdisciplinary teaching faculty, variety in content (our first DH course covered a huge range of topics going from databases, TEI markup, semantic web, visualization to digital libraries), and flexibility to change, thanks to the lessons learned every year. Distant Teaching MethodologyLINHD is based at UNED, the biggest Spanish university, founded more than 40 years ago, which uses semi-present and virtual teaching methods as its unique strategy. This has been a cornerstone for developing our DH teaching programs, as we have used UNED infrastructures as a key part of our teaching methodology. For summer schools, we use a double teaching model: there are keynote lessons with students in the room, but sessions are also recorded and broadcasted in streaming and can be viewed later. Students may choose to attend the school in either of these two ways and receive the same certification. Both options are combined with a virtual teaching platform (called aLF, designed by UNED) where professors upload slides and teaching materials and students interact with them.The longer winter courses are only offered online using the aLF virtual platform as the main communication channel among students and faculty members, but the platform includes videos, exercises and virtual research environments as teaching materials to train students all over the world.Anyway, the challenges we often face in our courses deal with:• Creation of the materials. Two sided: on one hand, the materials should be written in Spanish; on the other hand, most of the \"technical\" documentation lacks a DH perspective (Ej. PHP, MySQL databases). This is a very important point, as although there are a lot of open access materials on the web on DH training, it has not been enough for us to translate them (Cordell 2015). It has been necessary to re-create the problems, to adapt existing materials to our own needs and examples (our libraries, our projects), and also to regroup the different cues on a not-yet consolidated DH history in Spanish.• Teach an eminently hands-on field through a distant method and virtual learning (Forum, exercises). This has been one of our main pillars, in which we are proud to present our most innovative product in the DH world: online teaching has proven to be the present and the future of education, but has not been extended yet to the DH field (Berg- mann 2013, Koller 2012. This way of teaching implies, however a specialized production of teaching materials and also a special faculty training, as they have to produce slides, videos, written materials and quizzes and tests for which they have to offer interactive help and solutions. Although this might look an extra workload for our teachers, the experience shows that all these efforts remain as digital contents (it is nice to have a look at the libraries of our YouTube channel), in which we add as open access materials all the recorded sessions of our courses.• Make visible the DH training inside and outside the Spanish speaking world. This is an important point for us, not just as a way of getting more students, but also as a way of clearing the path to consolidating DH as an academic discipline (Trejos 2013). As our academic structure is pretty resistant to interdisciplinary work, we have to use social networks and other dissemination channels to go beyond barriers and reach our public outside our university. The result is a very mixed group of students coming from different parts of our country, and from other countries too. The challenge: identifying the ways to arrive at all these potentially interested groups. We work together in collaboration with scholars from Argentina, Mexico, Colombia and other universities and try to expand our visibility as much as possible.In this presentation we will explore the success and difficulties we have faced in the use of this combined methodological approach during this 3 years, involving more than 500 students of different backgrounds, languages and countries. Expanding this model to other teaching programs and continue experimenting with new DH curricula proposal are the next plans at LINHD.  ",
        "article_title": "Using e-learning systems for DH teaching: the Spanish Humanidades Digitales case study at LINHD",
        "authors": [
            {
                "given": "Elena",
                "family": "González-Blanco",
                "affiliation": [
                    {
                        "original_name": "Universidad Nacional de Educación a Distancia",
                        "normalized_name": "National University of Distance Education",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02msb5n36",
                            "GRID": "grid.10702.34"
                        }
                    }
                ]
            },
            {
                "given": "Universidad",
                "family": "Nacional De Educación",
                "affiliation": [
                    {
                        "original_name": "Universidad Nacional de Educación a Distancia",
                        "normalized_name": "National University of Distance Education",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02msb5n36",
                            "GRID": "grid.10702.34"
                        }
                    }
                ]
            },
            {
                "given": "Spain",
                "family": "Distancia",
                "affiliation": [
                    {
                        "original_name": "Universidad Nacional de Educación a Distancia",
                        "normalized_name": "National University of Distance Education",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02msb5n36",
                            "GRID": "grid.10702.34"
                        }
                    }
                ]
            },
            {
                "given": "Del",
                "family": "Gimena",
                "affiliation": [
                    {
                        "original_name": "Universidad Nacional de Educación a Distancia",
                        "normalized_name": "National University of Distance Education",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02msb5n36",
                            "GRID": "grid.10702.34"
                        }
                    }
                ]
            },
            {
                "given": "Clara",
                "family": "Martínez",
                "affiliation": [
                    {
                        "original_name": "Universidad Nacional de Educación a Distancia",
                        "normalized_name": "National University of Distance Education",
                        "country": "Spain",
                        "identifiers": {
                            "ror": "https://ror.org/02msb5n36",
                            "GRID": "grid.10702.34"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe advent of new funding streams and initiatives within broader humanities scholarship indicate that the collaborative research approaches have diffused beyond digital humanities. This paper presents the findings of the \"Humanities Collaboration and Research Practices: Exploring Scholarship in the Global Midwest\" project (HCRP), which examines the Humanities Without Walls initiative as a case study for how innovative, interdisciplinary humanities research draws upon models from digital humanities. BackgroundThe Humanities Without Walls (HWW) Global Midwest initiative supports collaborative research projects led by faculty from fifteen U.S. research universities in the Midwest. With its emphasis on multi-institutional, interdisciplinary collaboration and applied research, HWW Global Midwest presents rich research cases on the evolving nature of humanities research. LiteratureStudies of collaboration among digital humanities researchers and its impact on humanities scholarship have proliferated over the past decade (Siemens, 2009;Siemens, 2011;Deegan & McCarty, 2012;Given & Wilson, 2015). Focused studies of DH research practices also examine credit and authorship (Nowviskie, 2011;Nowviskie, 2012), infrastructure needs (ACLS, 2006, Edmond, 2015, and project management (Leon, 2011). Building upon this research, our study examines how the collaborative experimentations undertaken by HWW Global Midwest researchers influenced their research practices, data sharing, and final outputs. MethodThe project team conducted semi-structured interviews with 28 researchers funded by the first round of HWW Global Midwest awards. Participants were asked about project goals, collaboration development, tools used for project management, challenges, and research approaches.The project team recorded and transcribed the interviews, and coded them in ATLAS.ti 7. Each transcription was coded multiple times for inter-coder reliability. This study applies a qualitative content analysis method that expands upon prior studies by Brockman et al. (2001), Palmer & Neumann (2002), and Palmer (2005), as well as a grounded theory approach (Corbin & Strauss, 2008). FindingsInterviews with Global Midwest project awardees revealed a number of emerging practices and challenges common to the DH community and collaborative DH projects. Project Workflows and InfrastructureThe interviewed participants identified many project challenges within the HWW program model. These included finding personnel and eligible collaborators, aligning IRB approvals, and funding coordination.One participant summed up the sentiments of many on project management, saying \"that was definitely a learning curve for all of us.\" But most deemed this learning curve worth undertaking.Another key aspect of project workflows was the range of tools used by the HWW Global Midwest research groups (See Table 1). Tool selections ranged from cloud storage to unique platforms, including the software built for NINES and 18th Connect. But whether they used popular or specialized tools, one respondent's declaration captures their prevailing ethos: \"We're using an existing infrastructure and we're applying it in a quite different way.\"This process of translating tools to different uses is similar to the software adaptations seen in digital humanities research, and as scholars explore new ways to translate their research, they turn to multiple sources of expertise.  Methods of Collaborative AnalysisMany research groups carefully developed methods of analysis in ways that resonate with crossdisciplinary approaches in digital humanities research. One respondent characterized a group's work as having \"a lot of cross-fertilization of methodologies … not so much about content.\" Another project planned to employ several methods of analysis, including a short film, a series of interviews, and a performance of dancers and scholars rolling around on the floor \"because to resist was not going to happen.\" This type of collaborative process was described by one group as one that \"unfolds in an uncertain and, in that sense, an egalitarian manner because no one knows yet what the thing will be…. You go on a hunch and you see where it takes you. That is typical of ethnography, but also, I think, of collaboration, as well.\" These dynamic and educational elements of collaboration proved to be key to partnerships. Student EngagementSeveral interviews related a need for research assistance and dedicated project management, and respondents repeatedly attested to the value of graduate assistants who shouldered the management burden of the projects, or the (unfulfilled) need for such students. Projects navigated the tension between relying on student labor and acknowledging the intellectual contributions of the students with varying degrees of success, with the most positive assessment citing student participation as the true catalyst for collaborative practice: \"They're not just graduate students. They're fellow collaborators in the project at this point and they have tremendous resources of knowledge, you know. The multiplication is enormous. It's here that you really have the collaborating humanities.\" Digital Dissemination and CurationRespondents cited different formats for sharing their work, including performances, films, and websites as well as texts and presentations. Several respondents envisioned creating hybrid outputs, such as one respondent's plan \"to create some kind of interactive map [and] ideally a repository of sounds.\" Another discussed the possibility of sharing interview data as a form of dissemination, noting that \"we're still processing the data [and] deciding how to feature it… we're not tweeting the results or something like that.\" This response also highlights the complex characteristics of humanities data, and the multiplicity of factors that must be considered for data sharing and archiving.Respondents also saw avenues for making broader impacts via use of different platforms. As one respondent explained, \"I think we've contemplated scholarly output in the traditional platforms… whether they're online or in print, but we have contemplated getting research into the hands of stakeholders who are not scholars.\" Collaboration and CreditMany respondents were mindful of the importance of providing appropriate credit and recognition for project partners. One respondent noted that \"for us, the notion of collaboration was built around the idea that both parties would be equally acknowledged.\" Negotiating appropriate credit, however, also can reveal moments of tension within projects. Another respondent observed that \"there was a little bit of misunderstanding, and some disagreements […] had to do with who is being acknowledged for what.\"Respondents differed on their views of co-authored publications. One respondent noted, \"I didn't expect a lot of co-authoring, more of a co-design of the platform.\" Another viewed co-authorship as an important \"end product collaboration.\" While discussion of evaluation for tenure and promotion were present within the interviews, they were not as prevalent as might be expected. Yet a key theme that emerged in the responses was that culture shifts within humanities disciplines are essential to advancing the acceptance of research collaborations and co-authorship in peer evaluation criteria. Discussion and ConclusionTo bring emergent humanities research collaborations into dialogue with the digital humanities, we propose a set of recommendations as a foundation for fostering rigorous interdisciplinary collaboration:Build stronger connections between teaching and research through engaging students in research collaborations: Student participation in digital humanities projects has been essential to the growth of DH research, and humanities scholars can similarly bring collaborative research practice into the classroom in ways that acknowledge and recognize the students' labor. Experiment with new forms of dissemination that more accurately convey the full breadth of collaborative work: HWW Global Midwest researchers frequently sought new ways for disseminating interdisciplinary research findings:In the same way that digital humanities researchers employ new formats for publishing research data and findings, humanities scholars can experiment with new forms that reflect interdisciplinary approaches. Scholars should also consider protocols for establishing credit and coauthorship, such as a negotiated project charter that establishes workflows for the collaboration, standards for co-authorship and a grievance process.Encourage a culture of sharing data and interim findings: Administrators are in a key position to encourage shifts in humanities research practices by encouraging and explicitly ascribing value to related intellectual activities. Both leaders as well as researchers can encourage a culture of sharing data and interim phase research outputs that recognize the complexities of the communities and types of data in humanities research.Strategically expand institutional investments in humanities research collaborations in order to ensure research sustainability: To ensure sustainable collaborations, administrators may need to make financial and structural investments, and key to these decisions is understanding the motivations and requirements of multiple stakeholder groups represented within a project. For example, some team members may require explicit funding to dedicate allocations of their time, while other team members may need support staff assistance to manage budgets and project documentation. Another avenue is to leverage the embedded collaborative power of regional, national, and international consortia in order to ensure research sustainability. These recommendations drawn from our findings suggest that the expansion and sustainability of innovative research collaborations in the humanities has critical intersections with the evolving research practices of digital humanities research.Table 1 : Tools for Research1 ",
        "article_title": "Collaborations in the Global Midwest: The Diffusion of DH Values in Research Collaborations in the Humanities Without Walls Consortium",
        "authors": [
            {
                "given": "Harriett",
                "family": "Green",
                "affiliation": [
                    {
                        "original_name": "University of Illinois, Urbana-Champaign",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Megan",
                "family": "Senseney",
                "affiliation": [
                    {
                        "original_name": "University of Illinois, Urbana-Champaign",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Maria",
                "family": "Bonn",
                "affiliation": [
                    {
                        "original_name": "University of Illinois, Urbana-Champaign",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionWhat role does place, if any, figure in the digital scholarly edition? Gunn and Hart offer a prescriptive view of the role of place in Joyce's Ulysses: \"The topography of Dublin is \"on the page\" at least as much as are the meanings of the words \"priest,\" \"kidney,\" and ineluctable modality\": it is a part of the book's primary reference system, without which its full sense cannot be apprehended\" ( Gunn et al. 2004). Such a positivistic view grants a key role to place and suggests that the novel may be modeled, to some extent, along geospatial lines. This paper proposes a model for foregrounding geographical elements, within a digital scholarly edition, for Ulysses.However, rather than supporting the naturalistic view that Ulysses constitutes a sort of textual analogue to geographic representations of Dublin, previous work has suggested that Ulysses complicates this perspective (Bulson 2011). In this context Pierre Joris' formulation about the US poet Ronald Johnson, \"He also knew that to make history you have to disfigure geography\" might be applied equally to Joyce and the example of Ulysses (Joris 2009).While significant work has been done to date outlining the extracting of geographical elements from texts and using the resulting toponyms in GIS contexts (Gregory andHardie 2011, Cooper et al. 2015), this paper considers the question of toponyms in literary texts in both the context of modeling and the role of source texts in the representation of place. The key element of this research is making explicit the connections between source text and novel and how these connections are reflected in external ontologies.Eide (2014) asserts that maps, texts and landscapes all constitute different ways of modelling and different ways of experiencing place. Texts have the characteristic of being \"underspecified\": e.g. a text might designate a place as \"east\" as opposed to exposing coordinates on a map. Maps and texts work to create the condition of intermediality - a combined geocommunication system. Are there source texts, relative to Ulysses, that amplify this sense of intermediality? Do these texts, in conjunction with the novel, occupy a sort of liminal category between text and map? MethodologyThis approach extends earlier work using TEI in effectively modelling geographical elements in one episode of the novel and incorporates alternative models such as ontologies and RDF XML documents in augmenting TEI XML (Derven et al. 2012). Such an approach serves to instantiate Gabler's claim for the digital scholarly edition as a \"web of discourses\" (Gabler 2010). The paper also considers whether this approach can be generalised beyond the specific use case of geographic named entities in the text and considers whether the combination of TEI XML, external ontologies and paratextual data stored in RDF triples may be more widely extended to digital scholarly editions. The approach taken in the paper extracts named entities from the text, identify toponyms and incorporates them as geo-rectified elements in an encoding of the novel. A sample of two episodes encoded in TEI XML is used. However, the approach to modelling taken extends beyond that offered by a strictly TEI-based approach by using a semantic web based approach and links placenames in the text to ontologies (for example, geoNames). The paper weighs marking up place names directly in the encoding itself against linking to external ontologies. This paper also considers an interface for digital scholarly editions that accommodates multiple versions, displays geographic data extracted from the text and utilises an ontology to model and encode geographical elements within the text. The interface is used to interrogate a single episode from Joyce's Ulysses, a text that can be avowedly positioned within a geographic context. The tenth episode of the novel, Wandering Rocks, both foregrounds and privileges geographic elements in such a way that place itself takes on a narrative function. The use of place too alters slightly from manuscript to fair copy through to printed editions. The paper foregrounds the methodologies and tools used in assembling the interface. Toponyms are extracted, modelled in terms of both possible sources used in the construction of the text and existing ontologies, and presented as part of the interface for the digital scholarly edition through a GIS service. The interface includes a map that plots changes in the use of place name in the construction of the text. ConclusionsIn the context of a novel where place and place names function as narrative markers, investigating geographical elements in a computational context becomes another way to both read through the novel and assist in establishing a critical editing function. For example, considering the digital scholarly edition as interface, it becomes interesting to track such changes not only from the perspective of the critical edition but also in terms of GIS. How does the role and use of placenames change in the developing text? Is there evidence for paratextual or secondary sources in the development of place as this role changes? What is the most appropriate way to model and refer to place in the text? The paper, then, models episodes in Ulysses along these three dimensions: as a digital scholarly edition, as a mapping network, and through an ontology for modeling geographic elements. ",
        "article_title": "Modeling Place in Ulysses: Ontologies and Pre-texts",
        "authors": [
            {
                "given": "Caleb",
                "family": "Derven",
                "affiliation": [
                    {
                        "original_name": "Maynooth University",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            },
            {
                "given": "John",
                "family": "Keating",
                "affiliation": [
                    {
                        "original_name": "Maynooth University",
                        "normalized_name": "National University of Ireland, Maynooth",
                        "country": "Ireland",
                        "identifiers": {
                            "ror": "https://ror.org/048nfjm95",
                            "GRID": "grid.95004.38"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " ProposalMusic scholars have unprecedented access to an ever-growing digital corpus of music-related content, as libraries and institutions continue to digitize their holdings at an extraordinary pace. Of course, curators describe digital content in ways that best suit their collection's needs, using a variety of metadata standards such as MARC, Dublin Core, and LIDO (just to name a few). Despite the obvious benefits to such flexibility, interoperability between digital collections has proven difficult, forcing music scholars to navigate between several interfaces and platforms in order to locate research materials. More importantly, access to these resources is limited to scholars already aware of their existence.Platforms such as the Digital Public Library of America (DPLA), Europeana, and HathiTrust are working to solve these issues by aggregating digital collections into single catalogues, therefore making collection holdings accessible and interoperable. These platforms and others like them are adept at aggregating a variety and breadth of content (including, for instance, audio recordings, encoded content, musical scores, photographs, manuscripts, and printed texts) relevant to a variety of disciplines. The structure of these aggregators are heavily reflective of the library collections that make up the bulk of their data. But, unlike the discrete digital collections themselves, these platforms bring together information about holdings and digitized materials from diverse institutions, borders, and economies in order to make them discoverable in new ways.However, there is a host of content consistently left out of even the best and most extensive music catalogues and digital aggregators: born-digital scholarship. This new model for of scholarly knowledge production has motivated humanities scholars to consider how digital catalogs and collections should evaluate and make visible new forms of scholarship. When Jerome McGann introduces the DPLA in 2011's \"On Creating a Usable Future,\" he locates our humanities crisis not only in the evaluation of digital scholarship, but also \"the sustainability of born digital resources and the work they support\" and forging paths to make new forms of scholarly knowledge production \"a general institutional practice\" (182). To make digital scholarship an institutional practice, we must consider not only our social infrastructures (promotion and tenure), but the technical infrastructures that allow discoverability and interoperability -two concepts that are crucial to making our work visible and accessible across audiences and publics. For digital scholarship in music, no aggregators, platforms, or institutional ventures currently ensure that born-digital scholarship is widely discoverable and therefore accessible.Music Scholarship Online (MuSO) is a community of scholars dedicated to resolving these issues, thus providing greater access to born digital music scholarship. It began in 2015 as a Digital Humanities Start-up Grant funded by the National Endowment for the Humanities, in which music librarians, music encoders, and musicologists gathered to discuss issues surrounding aggregation and peer review for born-digital music scholarship. Participants in the planning meeting came together to ask and answer \"What do music scholars need in a digital curator and search mechanism?\" Ultimately the team chose to follow the model of the Networked Infrastructure for Nineteenth Century Electronic Scholarship (NINES) and the other members of Advanced Research Consortium (ARC). In 2016, MuSO officially joined ARC, a federation of virtual research environments that coordinates several thematic and period-specific aggregators, like MuSO, into a catalog \"containing resources spanning the bulk of Western civilization, from the medieval period to the early 20th-century\" (Grumbach and Mandell, 3). This short paper describes the present metadata prototyping efforts of the Music Scholarship Online (MuSO) project. Arguing that it is unnecessary for digital project teams to generate preservation-quality metadata, this paper describes how MuSO is working with ARC to generate a metadata schema for discovery that is lightweight and therefore a better option for digital projects facing constrained budgets and limited timescales.Beginning with a brief introduction to the unique challenges facing born-digital projects in music, the paper then presents the history of MuSO and its initial phase, in which the basic metadata guidelines and peer review processes were developed. Afterwards, the paper reports on current efforts to build a prototype schema for MuSO: a discovery-level metadata standard for digital projects in music. It shows how the MuSO and ARC leadership teams have worked together in examining present aggregators of music resources such as the the Digital Archive of the Beethoven-Haus, the Digital Image Archive for Medieval Music, and the Juilliard Manuscript Collection, as well as well-established catalogues such as Répertoire Internationale des Sources Musicales (RISM) and Répertoire Internationale de Littérature Musicale (RILM). It reveals the similarities and differences between the metadata standards of these resources, and then it identifies the most significant elements for discoverylevel metadata.The paper then concludes by comparing these elements with those present in the musical holdings of multidisciplinary collections such as the DPLA, Europeana, and the HathiTrust to ensure that MuSO's lightweight metadata schema accurately captures the level of description needed to work within the multidisciplinary ARC catalog.If born-digital scholarship is to become general institutional practice, the methods of discovery for that scholarship must be familiar to all humanists: all of the multidisciplinary, multilingual and international scholars that our institutions serve. The authors will therefore present findings concluded from an examination of the current MuSO Metadata recommendations as decided by the MuSO community alongside existing multidisciplinary metadata schemas. By investigating the push and pull between discovery-level metadata and preservation-level metadata, this short paper reports our efforts to ensure that Music Scholarship Online effectively describes the born-digital scholarship it intends to aggregate.  ",
        "article_title": "\"Come Together, Right Now\": Discovery and Interoperability for Born- Digital Music Scholarship",
        "authors": [
            {
                "given": "Timothy",
                "family": "Duguid",
                "affiliation": [
                    {
                        "original_name": "University of Glasgow",
                        "normalized_name": "University of Glasgow",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/00vtgdb53",
                            "GRID": "grid.8756.c"
                        }
                    }
                ]
            },
            {
                "given": "Elizabeth",
                "family": "Grumbach",
                "affiliation": [
                    {
                        "original_name": "Texas A&M University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " in the Film andVideo Makers Travel Sheet (data from 26 artists residing in New York City and New York state). In the above image, the artists' home address is represented in blue while the event location is represented in orange (the event locations are sized according to the number of events, 99 total). This visualization demonstrates the connections between organizations and individuals emerging from the data in the Travel Sheet. Here we can see the beginnings of a network that extends beyond the United States and into Europe and Canada. As data entry and analysis continues, the data promises to demonstrate a global network of artists and media arts organizations as a sample from the 1979 Film and Video Makers' Directory illustrates, below. While the term network was the preferred terminology of the Media Arts Center Movement, this is an overly simplistic understanding that masks the complexity of the MIMC data. Anthropologist Tim Ingold argues that the lines of a network are connectors, static points joining two nodes, that presume an absolute connection that does not fully illustrate the complexity of the relationship. In contrast, Ingold offers the concept of the meshwork, suggesting that these lines are not connectors, but the \"lines of becoming\" from Deluze and Guattari's rhizome (Ingold, 2013: 132). These lines do not meet - the nodes in the network map instead represent knots and entanglements in the meshwork, places where lines emerge and diverge rather than connecting in absolutes. The meshwork, like the rhizome, affords multiple narratives and entry points; it offers no hierarchy or structure. MIMC seeks to offer this alternative mapping of the meshwork -the entanglements between artists, distributors, museums, governmental bodies, local communities, and countless other actors in the fabric of the independent media arts.Unlike other digital humanities approaches to cinema and media history, like Jeffrey Klenotic's Mapping Movies, MIMC provides access not only to a representation of the history of the independent media arts, but access to the archives that hold these traces as well.The MIMC data model includes the provenance for each discrete data point in the database, linking individual records back to the primary source material from which it was derived. These connections will afford opportunities to link directly to the digital archives that hold the digital surrogates as these records, allowing MIMC to serve as an extension of these archival collections as a digital finding aid of sorts. In this way, MIMC is enfolded in the archives; the project does not derive-from but is entangled-with the archival organizations and other sites that hold the history that MIMC seeks to represent, continuing to build the meshwork that entangles the various organizations and individuals represented in the visualizations. In locating and documenting these archival traces, MIMC provides an understanding not only of the historical unfolding of the independent media arts and Media Arts Center Movement, but of the archivalization of this history as well - the project itself becoming further enmeshed and entangled in the very history that it seeks to uncover.This brief paper will introduce the MIMC project and discuss the development of the MIMC application as well as the potential impact of the project as a Public Digital Humanities resource for scholars and for the archives that collect and provide access to the primary source materials from which the MIMC data is derived. In addition to the database and visualizations, by preserving the source information for each record and linking to the digitized archival records (or archival finding aids), MIMC links the archive of the independent media arts that is distributed across archives, personal collections, the active and inactive organizations that are part of this vast meshwork. While there is still much work to be done, MIMC promises to provide widespread access to historical data that can be reused and re-imagined beyond the initial bounds of the project.  ",
        "article_title": "Mapping the Meshwork of the Independent Media Arts",
        "authors": [
            {
                "given": "Lindsay",
                "family": "Mattock",
                "affiliation": [
                    {
                        "original_name": "University of Iowa",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn Italian, il tavolo means 'the table', but la tavola, the feminine form, is 'the sense of family felt around the dinner table'. My grandparents taught me the meaning of la tavola first-hand; they emigrated from Italy in 1959. Through my research, I document the Italian dialect of immigrants in Sarnia, Ontario, Canada. Through community collaboration, we work to preserve their stories. What?Heritage languages are those brought by immigrants to a new place, and now spoken as a minority language by that community (Fishman, 2001). There are two parts of this research: to record the histories of this community in their native Italian dialect; and second, to create a permanent and public digital archive to preserve this Heritage Language. The research question is: What is the best approach to create a community-university partnership that will benefit both the heritage and academic communities? Why? From 1950-1969,000-30,000 Italians arrived each year in search of a better life (pier21.ca). Today, 1.4 million Canadians identify with an Italian heritage (StatsCan, 2013). Almost all Canadians have a 'hyphenated' identity: Italian-, Dutch-, French-, etc. By collaborating with the university and heritage community to build a digital archive, I will establish an interdisciplinary means to preserve Heritage Languages and the histories of Heritage Communities throughout Canada. Where?I have signed a two-year partnership with the Archives of Sarnia and Lambton County to create an online archive. Through this project, I digitized and uploaded hundreds of recordings, videos, images, and documents recently collected by myself, and by Caroline Di Cocco since the 1980s. Caroline is a local community historian who has contributed what she collected over the past 40 years. We will formally present the archive to Sarnia's Italian community in June, 2017. This serves as a starting point showing the project's feasibility.I have been given permission by Western University's Libraries to host this archive, permanently, at the University. The Western Archive of Dialects and Languages (WADL) is available for other language projects, and my Italian research is the initial project to be hosted there (www.ir.lib.uwo.ca/wadl). Western University is only an hour from Sarnia, which allows me to remain an active member of Sarnia's Italian community, and to keep the digital archive local. How?With the support of Western Libraries and Sarnia's Italian community, the archive is being created through my coordination of the community and academia. Through my work with Lambton County Archives: I code the website; use Dublin Core standards for metadata ( Kunze et al., 1998); and digitize materials. I use Omeka software for the framework.The archive is made up of many photographs, documents, and videos. I have collected and digitized just under 500 photographs so far. It is also made up of documents: digitized copies of boat tickets to cross to Canada, citizenship papers, letters, (delicious) recipes, and other ephemera of their Italian and Canadian experiences. I have just under 200 documents so far.I am currently recording interviews with Ciociaria dialect speakers, using sociolinguistic techniques to collect life histories. I also have 47 interviews I've digitized that were conducted in the 1980s by Caroline di Cocco. ConclusionWADL ensures that the community directly benefits from sharing their stories, materials, and time with the researcher. The information they share is returned in a permanent, secure, and digital way. This ensures that future generations will be able to access their heritage and history.The goal of this research is to show the importance of heritage-a significant part of the Canadian identity. My work will produce a collaborative partnership between Western University and Sarnia's Italian community. This framework can be applied to preserve and promote other communities across Canada.In summary, the stories, culture, and language of this community cannot wait any longer to be recorded and digitized. There is an urgency as Sarnia's original Italian community is reaching an advanced age. Now is the time to create an online presence: both for academics and for the public. There are research opportunities in this data, from a great number of disciplines. The community and broader public is interested in hearing these stories, seeing these photos, and cooking some of the recipes.I study how Sarnia's Ciociaria pronounce \"la tawola\", and ensure the survival of what they mean when they say it.   ",
        "article_title": "A Seat At \"La Tawola\"",
        "authors": [
            {
                "given": "Michael",
                "family": "Iannozzi",
                "affiliation": [
                    {
                        "original_name": "Western University",
                        "normalized_name": "Western University",
                        "country": "Cambodia",
                        "identifiers": {
                            "ror": "https://ror.org/02agqkc58",
                            "GRID": "grid.443228.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionTwitter data have been growingly used as a source for scholarly studies in various disciplines in recent years ( Williams et al., 2013). The value of such borndigital data as primary source materials for future researches in history is already being acknowledged (Webster, 2015, Steinhauer, 2015. But, at least for now, historians seem rather reluctant to make use of them, although some recent works deal with the perception and memory of the past on Twitter (Clavert, 2016, Turgeon, 2014) or propose both documentation and analysis of present time events (Ruest and Milli- gan, 2016).One possible reason of this reluctance could be the attachment of historians to traditional archival collections, e.g. those organized by professional archivists. But the creation of archives for social network sites data is not yet systematic and still in the beginning. As for the global Twitter archive of the Library of Congress, it is unknown when it will be functional (Zim- mer, 2015). As it is possible for one to retrieve Twitter datasets, a second reason of this reluctance could be the need for acquaintance with basic methods and tools for gathering, understanding and analyzing born-digital data. However, not all historians are trained to digital humanities and quantitative methods that provide for such skills. Last but not least, the main reason could be the relation historians have with time. It has always been difficult to define the moving frontier between the present time and the recent past in contemporary history (Bédarida, 2003). As instant ephemera data that belong in the very present time, Twitter data precisely underline the difficulty for historians to define their own territory in these temporalities.Nonetheless, for historians concerned with contemporary historical events - historical in the sense of a conjuncture that reveals a before and an after (Le Goff, 1999) - Twitter provides an original documentation. This documentation is generated in real time; organized around folksonomies - the hashtags - that reveal a direct perception from below; but also in close relation with media coverage. Since the creation of Twitter, a series of hashtagged global events (#IranE-lection, #15M, #Occupy, the 2011 Arab revolutions under various hashtags, the 2015 terrorist attacks in Paris...) update the concept of the monster-event (Nora, 1972) in that they are produced, lived, transmitted and shared in real-time around the globe - or at least in its connected parts. In spite of the known biases, mainly the fact that it is mainly used by relatively young and highly educated adults (Pew Research, 2016), Twitter offers an original kind of non-institutional primary sources (tweets) that can be complementary to the traditional ones the historians use.This paper is a tentative to document and to provide a first analysis, based on Twitter primary sources, of the Greek referendum of 2015. This event has already obtained a distinctive status in the \"before\" and the \"after\" the current crisis marks in Greece's posttransition to democracy history (after 1974) (Avgher- idis et al., 2015), although time and future historians will definitely tell. The paper considers the transnational phase of this event, which followed the Greek vote in favour of the \"no\" to further austerity measures, included negotiations in the instances of the EU and ended with the agreement of the Greek government to conclude a third harsh austerity programme. Our main research hypothesis is that the imbrication of different hashtags reveals different temporalities that allow researchers to construct regimes of historicity of an event. Event backgroundIn the aftermath of the 2008 financial crisis, Greece, an EU and Eurozone member, began going through a severe debt crisis that revealed the structural weaknesses of the European monetary union and soon expanded to other weak members (Portugal, Ireland, Cyprus and Spain). Since 2010, the crisis has been managed through the setup of a European financial assistance mechanism in exchange for national programmes of structural reforms and budgetary cuts. Two such programmes were applied to Greece in 2010 and in 2012, plus a debt restructuring, that were monitored by the European Commission, the European Central Bank and the International Monetary Fund (Papaconstantinou, 2016;Zettelmeyer, 2013). The ongoing crisis provoked profound social and political transformations in the country that brought to power a coalition government led by the radical Left party of Syriza in January 2015. Syriza won the election with the promise to put an end to austerity politics. The party emerged in the context of the post-2008 crisis that shook the countries of Southern Europe and the 2011 Indignant movements, just like Podemos in Spain. Thus, the referendum of July 2015 was far from being significant only in the context of the Greek crisis as an effort of the new government to ameliorate the terms of the Greek programmes, as it put at stake different visions for the EU and its crisis management politics. Data collection and analysis: method and toolsTweets using the hashtag #greferendum were collected with NodeXL, an add-in to MS Excel ( Smith et al., 2009). The collect was setup once daily from July, 6 to July, 16 2015. The size of the gathered sample was determined by the capacities of the tool, that can collect a maximum of around 20,000 tweets at once. A total of 204,714 tweets were collected of which 139,945 are retweets (68,36 %), 8, 686 responses (4,24 %), 56,086 mere tweets (27,39 %). Minor collects were also launched for other related hashtags (mainly #thisisacoup). Hashtag data were treated with OpenRefine and further explored with R software.Statistical analysis of textual data (tweets) was made with TXM-Textometry software (Heiden, 2010). The corresponding dataset had been previously encoded following the TEI P5/XML standard with use of the OxGarage service.Social network analysis and visualizations were made with Gephi software ( Bastian et al., 2009). Data analysis: first findings HashtagsThe first part of the research focused on reading the hashtags of the dataset. The hashtag #greferen-dum was used with a variety of hashtags, a total of some 12,000 words (all languages and variants included). A first study focused on the hashtags with a frequency over 99, which gave a total of 158 words. After an elementary typology was established, it was possible to distinguish: geographic names, names of persons, institutions, common names, neologisms that came out of contractions (such as \"greferendum\"), short phrases that had the function of commentary.The use of hashtags varied between tag and commentary, or included both functions at once ( Bruns and Burgess, 2011). The big majority of hashtags are in English (112 out of 158). However, in the thirty most frequent hashtags of the dataset, it is possible to find words in Spanish, Italian, French, and German. By consequence, the linguistic communities that participated in the global interactions were the ones that were the most concerned by the crisis. As for the Greek language, it is not entirely absent as such, but it is mainly present in its greeklish form: Greek words used as hashtags but written in Latin alphabet.A close reading of the thirty most frequent hashtags with parallel consideration of the associations of words (coocurrencies) shows the tractations that followed the Greek referendum were basically perceived as an intergovernmental affair with the EU actors occupying a secondary position.An interesting case is the emergence of the hashtag #thisisacoup as an act of solidarity of Spanish militants of Barcelona en Comú towards the Greek government during the Eurogroup and the Euro Summit negotiations (12-13 July). The corresponding dataset is more oriented to the expression of personal opinion than the dissemination of information with hashtags in the form of phrases that function more as commentary than tags (such as #yovoycongrecia). DomainsThe most tweeted domains were twitter.com (7,352 tweets) and theguardian.com (7,217 tweets). In general, it is possible to distinguish two main tendencies. First, the dissemination of information in social media (Twitter, YouTube, Instagram, Facebook ). Second, the dissemination of authoritative information (international media, specialized independent blogs, personal blogs). Communities detectionThe network of the #greferendum corpus is composed by 103,733 nodes and 204,713 relations. After nodes with a degree higher than 10 were isolated (around 4% of the total), 326 communities were detected with Gephi (Louvain method). These communities need to be further explored, however the first findings for the most important of them show that affinities developed around sources of information (media), political and/or intellectual personalities, professional communities, and also linguistic communities. ConclusionThe network and the detected communities seem to have been structured around the dissemination of information but also political affinities and/or militantism. However, further exploration is necessary in order to better understand the network structure.A quantitative analysis of the tweets, with emphasis on the associations between the hashtags, indicate coexistence of different temporalities within the temporality of the 2015 Greek referendum that are principally related to the Eurozone crisis, the associated national sub-crisis, and post-2008 anti-austerity movements. In this sense, Twitter primary sources offer insights from a transnational scale.   ",
        "article_title": "Twitter as a source for the history of the present : the 2015 Greek referendum as a case study",
        "authors": [
            {
                "given": "Sofia",
                "family": "Papastamkou",
                "affiliation": [
                    {
                        "original_name": "Maison Européenne des Sciences de l'Homme et de la Société",
                        "normalized_name": "Maison Européenne des Sciences de l'Homme et de la Société",
                        "country": "France",
                        "identifiers": {
                            "ror": "https://ror.org/00m090414",
                            "GRID": "grid.464005.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionDecades of field research have identified numerous characteristics and conditions, so-called \"determinants,\" such as education, wealth, social networks, government transparency, gender, and riskcommunication ( Brooks et al. 2005;Smit & Wandel 2006;Tol & Yohe 2007), that affect the ability of individuals and groups to prepare for and respond to the effects of climate change. But why these particular determinants and how do they affect our capacity to adapt?We apply methods from computational text analysis (Sinclair 1991) and network analysis (Brandes 2001;Blondel et al. 2008) to offer an innovative approach to understanding the concept of adaptive capacity. Computational analyses allow us to reveal the unconscious rhetoric of adaptive capacity that illuminates how determinants are interconnected and how they might work to build adaptive capacity. These patterns are not stated or visible in a single study but emerge from a field-wide analysis. Results depict a concept map of adaptive capacity that operationalize the academic discourse and highlights points of convergence and divergence to inform future research. Corpus and MethodsA Web of Science search for title = \"adaptive capacity\", years 1800-2015, returned 448 nonduplicate English language academic articles. Based on title, journal, and abstract, we categorized papers as focused on social (e.g., community, organization, government; n=295) or non-social systems (e.g., biological, engineering; n= 153). Of 295 social papers, 261 full length texts (88%) were accessible. Most (91%) were published post-2001, when the Intergovernmental Panel on Climate Change (IPCC) first recognized adaptive capacity as a major element of vulnerability to climate change (IPCC 2001), signaling the concept's rise to the forefront of climate and sustainability research.We used collocation analysis to develop a network of determinants that visualizes inter-connections and may be interrogated. Based on a close reading, we identified 164 determinants of adaptive capacity and 351 related terms (to account for regional spelling variations, synonyms, gerunds, etc.). Collocation analysis was used under the theory that two concepts whose terms frequently co-locate have a conceptual relationship. Collocates were identified in symmetric 15 word distance with significance of 0.01 using a Fisher's Exact Test.Measures of network structure, such as centrality and modularity, have been found in other fields to provide insights into functionality (Krackhardt 1990;Danon et al. 2005). Collocations between determinants were visualized as a network (149 nodes, 1877 edges, network density 0.09). Both degree and betweeness centrality (Brandes 2001) were calculated. The centrality of a determinant may provide insight as to its role and sphere of influence. Community detection ( Blondel et al., 2008), which has been shown in other cases to reveal functional groups ( Danon et al. 2005), was performed 10 times each at three resolutions (0.4, 0.7, 1.1) ( Lambiotte et al. 2008). ResultsResults provide substantial insight into potential roles for determinants. In many cases, results confirm expectations and establish consensus. In others, results raise new research questions and may provide an impetus to test assumptions currently held within the field. Results further suggest determinants group into hierarchical functional modules, which could provide a function-based framework to assess adaptive capacity. These patterns may also reconcile competing theories in adaptive capacity literature as to whether all determinants are critical or some may compensate for weaknesses in others (Tol & Yohe 2007).  ",
        "article_title": "Collocations and Network Structure as Insights to Functional Elements of Building Adaptive Capacity",
        "authors": [
            {
                "given": "Anne",
                "family": "Siders",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis proposed short paper examines the progress and some preliminary observations of a practical attempt to apply digital humanities methods to the letters of a second century Roman aristocrat Pliny the Younger. The preliminary results of this study are available at the Pliny Project site. Its aim is to present a case study of how an initial research idea can be expanded to connect with larger digital humanities work in a particular field.These letters, written in the early second century CE, are a treasure trove of social and literary information about the Roman elite during the period in which Roman territorial control reached its apex. As one of the most extensive collections of letters from the ancient world, as well as one of the most thoroughly explored, they are a rich data set on which to draw. They are paralleled by only a handful of similar letter sets from the Roman world, such as the letters of Marcus Tulilus Cicero or the four century orator Libanius.In a broader context, Classics has been a frequent and early adopter of digital humanities methods (see Online Coins of the Roman Empire, the literary comparative tool Tesserae, and the venerable Perseus Digital Library). The field has also produced some initial attempts to bridge various \"people indices\" into a standard prosopography (i.e. the Standards for Networking Ancient Prosopographies, hereafter SNAP). Pliny's correspondents have been integrated into such resources, but they are either often limited to major university research collections or so unwieldy as to make consultation difficult. Examples of this tension include the standard tool for the prosopography of the Roman Empire, the Prosopographia Imperii Romani, 2nd edition ( de Gruyter: 1933de Gruyter: -2015) (= PIR 2 ), which has reached a massive eight parts with numerous fascicles. Only the index is widely available online. Likewise the most recent work on Pliny's names, Anthony Birley's Onomasticon to the Younger Pliny (Birley, 2012), exists as a traditional monograph, albeit with a searchable PDF.Such resources, though tremendously important to scholars specializing in the field, often constrain access for the broader academic community. Moreover, the rich information they provide is not structured in a way for easy search and access.The research project on which this paper draws developed from a November 2015 - January 2016 affiliated fellowship at the American Academy in Rome, which gave me access to the prosopographic material in a single, well-organized location. Its primary objective was, and is, to create a comprehensive resource for Pliny's social network with an emphasis on the social class of his correspondents. My initial inquiries centered on compiling a list of Pliny's correspondents and attempting to identify them as best possible. The conventions of Roman naming, which resulted in many similar names within family groups, renders this difficult. The use of only a single name (compared to the somewhat standard use of two names) in one of the surviving manuscripts of the letters further complicates the task. Even if a family and identity of an individual is known, his or her social standing may not be clear. The Roman distinctions between a common citizen, the middling administrators of the equestrian class, and the upper rungs of the senatorial class were very sharp to them-so sharp they often saw no need to clarify who was of what class for posterity.This made data modeling and cleaning a significant challenge, for which I employed exploratory tools such as SocNetV and more recently Cytoscape, for exploratory visualization. (Note: this issue of authorial ambiguity is not new to DH and letters. It has been frequently confronted by projects such as Stanford's Mapping the Republic of Letters). Some preliminary results are available through Pliny Project (see above), but they have been revealing both in terms of confirming known associations and providing new clarity into the possible editorial methods Pliny used in selecting his letters, which were curated for publication within his lifetime.In order to construct a data set for such an analysis, I attempted to model a degree of closeness of connection by assigning a weight based on the number of times Pliny either mentioned someone in a letter or wrote to them to a reciprocal connection. This was saved in GraphML format to construct a diagram of centrality with shading of points to indicate the social class of Pliny's correspondents. My talk will, in addition to discussing the above methodology in greater detail, center on two examples of preliminary results from this research, and then turn to future plans for the project. First, the set of social acquaintances that have often been associated with as what is informally called \"Pliny Country\"-near his home near modern-day Como, Italy-and the set associated with the city of Tifernum, both appear clearly in the social network map as a set of closer intimates, largely from the same equestrian class of which Pliny's family originated (the original formulation comes from the work of the eminent historian Ronald Syme, see Syme, 1991, for his collected works and the exploration of some of Pliny's connections of Tifernum in Champlin, 2001).This gives preliminary confirmation that the methodology of simple weighting based on mentions as some approximation of closeness can be used in analyzing his social network.A secondary observation is a series of correspondents to whom Pliny writes roughly two to three letters in the second to outer circle of his acquaintances. Some of these individuals are men who had held the consulship, the highest office to which a Roman not of the imperial family could aspire and all were of the senatorial class. Pliny rose to that same class from middling origins during his career, thanks to the patronage of his uncle and adoptive father. That there is a cluster of these letters with a remarkably similar number speaks to an editorial hand at work in their selection. While at this point identifying a motivation is primarily speculative, at the least we can say that it reveals a trend not previously identified and demonstrates an editorial concern for cultivating Pliny's prestige by association.In addition to the specific application of this data to my own research, the longer term goals of this project are to provide this same dataset, edited and curated, to the broader scholarly community. I have currently published a simple database interface that allows users to search for Pliny's correspondents and note which letters are written to them. While this may seem on the surface a straightforward question, by integrating current scholarship and attempting to identify correspondents fully, it presents new and easier access for scholars, regardless of institutional affiliation.Nevertheless, the initial search functions, which allow a name search and a tentative search by social class, are not sufficient to realize the goals of the project. My current development work is focused on transitioning the database to using Django's web application functionality and object database modeling to allow for the relationships noted in my social network analysis to be available and searchable. This transition to a standard platform will also lead to a web application that can be cloned from a tool such as GitHub and used by DH scholars to build or innovate using my dataset. It also acknowledges the need to connect this new structuring of the prosopographic corpus for Pliny to the broader initiatives to create people indices by including links to PIR 2 search masks and SNAP.Such an approach takes the traditional field of Plinian prosopography and attempts to open it to a wider scholarly audience. It also emphasizes the importance of exploratory visualization techniques in examining datasets for novel connections. This discussion will offer the audience an opportunity to consider how a project focusing on a specific area of research can connect with larger scholarly endeavors in DH.  ",
        "article_title": "Mapping Pliny's Social Network: A Case Study in Digital Prosopography",
        "authors": [
            {
                "given": "Benjamin",
                "family": "Hicks",
                "affiliation": [
                    {
                        "original_name": "Princeton University",
                        "normalized_name": "Princeton University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00hx57361",
                            "GRID": "grid.16750.35"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe functionalities of an aggregated metadata collection are dependent on the quality of metadata records. Some examples from Europeana, the European digital library, to display the importance of metadata: (a) Several thousands records have the title \"Photo\" without further descriptions; how can a user find these objects?, (b) Several data providers listed in the \"Institution\" facet under multiple different names, should we expect that the user will select all name forms of an organization?, (c) Without formalized date value, we are not able to use the functionality of interactive date range selectors. The question is how can we determine which records should be improved, and which are good enough? The manual evaluation of each record is not affordable. This paper proposes a methodology and a software package, which can be used in Europeana and elsewhere in the domain of cultural heritage. Background and foundationsEuropeana collects and presents cultural heritage metadata records. The database contains more than 53 million records from more than 3200 institutions (figures extracted from the Europeana Search API) in the Europeana Data Model (EDM) schema. The organizations send their data in EDM or in another metadata standard. Due to the variety of original data formats, cataloging rules, languages and vocabularies, there are big differences in the quality of the individual records, which heavily affects the functionalities of Europeana's services.In 2015 a Europeana task force investigating the problem of metadata quality published a report (Dan- gerfield et al., 2015), however - as stated - \"there was not enough scope … to investigate … metrics for metadata quality ….\" In 2016 a wider Data Quality Committee was founded. The current research is conducted in collaboration with it, having the purpose of finding methods, metrics and building an open source tool (see also, the project's Github page) to measure metadata quality. State of the artThe computational methods of metadata quality assessment emerged in the last decade in the domain ( Bruce and Hillmann, 2004, Stvilia et al., 2007, Ochoa and Duval, 2009, Harper, 2016. Papers defined quality metrics and suggested computational implementations. They however mostly analyzed smaller volumes of records, metadata schemas which are less complex than EDM, and usually applied methods to more homogeneous data sets. The novelty of this research is that it increases the volume of records, introduces data visualizations, and provides open source implementation to use in other collections. MethodologyFor every record, features were extracted or deducted which somehow related to the quality of the records. The main feature groups are:• simple completeness - ratio of filled fields, • completeness of sub-dimensions -fields groups support particular functions, such as searching, or accessibility, • existence and cardinality of fields - which fields are filled and how intensively.The measurements happen on three levels: on individual records, on subsets (e.g. records of a data provider), and on the whole dataset. On second and third level we calculate aggregated metrics; the completeness of structural entities (such as the main descriptive part and the contextual entities - agent, concept, place, timespan - connecting the description to linked open data vocabularies).The final completeness score is the combination of two approaches. In the first one the weighting reflects sub-dimensions. In the second one, the main factor is the normalized version of cardinality to prevent biasing effect of extreme values.The tool -built on big data analytics software Apache Spark, the R statistical software and has a web front-end - is modular. There is a schema-independent core library and schema specific extensions. It is designed to be used in continuous integration for metadata quality assessment. ResultsComparison of the scores of the field importance and field cardinality approaches shows that they give different results (however they correlate by the Pearson's coefficient of 0.52.). Because of the nature of calculation the compound score is quite close to the first approach: the functionality based scores lie in the range of 0.186 and 0.76 and cardinality scores are in the range of 0.031 and 0.335, and it has smaller effect on the final score.There are data providers, where all (in some cases more than ten thousand) records get the same scores: they have uniform structure. The field-level analysis shows (what one simple score is not able to testify) that in these collections all the records has the very same (Dublin Core based) field set. On the other end there are collections where both scores diverge a lot. For example in the identifying sub-dimension a data provider has five distinct values (from 0.4 to 0.8) almost evenly distributed while one of the best collection (of the category) is almost homogeneous: 99,7% or the records have the same value: 0.9 (even the rest 0.3% has 0.8). It means that in the records of the first dataset the corresponding fields (dc:title, dcterms:alternative, dc:description, dc:type, dc:identifier, dc:date, dcterms:created and dcterms:issued in the ore:Proxy part and edm:provider and edm:dataProvider in the ore:Aggregation) are frequently not available, while they are almost always there in the second. The tool provides different graphs and tables to visualize the distribution of the scores. From the distribution of the fields the first conclusion is that lots of records miss contextual entities, and only a couple of data provider has 100% coverage (6% of the records has agent, 28% has place, 32% has timespan and 40% has concept entities). Only the mandatory technical elements appear in every records. There are fields, which are defined in the schema, but not filled in the records and there are overused fields - e. g. dc:description is frequently used instead of more specific fields (such as table of contents, subject related fields or alternative title).Users can check all the features on top, collection, and records level on the web interface. Data providers get a clear view of their data, and based on this analysis they can design a data cleaning or data improvement plan.Europeana is working on its new ingestion system which integrates the tool. When a new record-set will arrive, the measuring will run automatically, and the Ingestion Officer can check the quality report. Further workWe will examine other metrics (e.g. multilinguality, accuracy, information content, timeliness), and check known metadata anti-patterns. We plan to compare the scores with experts' evaluation and with usage data and to implement related W3C standards: Shapes Constraint Language ( Knublauch and Kontokostas, 2016), and Data Quality Vocabulary ( Albertoni and Isaac, 2016). ConclusionIn the research we re-thought the relationship between functionality and the metadata schema, implemented a framework which proved to be successful in measuring structural features which correlate with metadata issues, and we were able to select low and high quality records. We remarkably extended the volume of the analyzed records by introducing big data tools, which were not mentioned previously in the literature.I showed my research in case of a particular dataset and data schema but the method I follow based on generalized algorithms, so it is applicable to other data schema. Several DH researches based on schema defined cultural databases, and in those cases the research process could be improving by finding the weak points of the sources.",
        "article_title": "Measuring completeness as metadata quality metric in Europeana",
        "authors": [
            {
                "given": "Péter",
                "family": "Király",
                "affiliation": [
                    {
                        "original_name": "Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen",
                        "normalized_name": "Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00cd95c65",
                            "GRID": "grid.434972.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " In the original Google Books Settlement Agreement in 2008 (Courant 2009), funds were to be set aside to create a research center that would enable researchers worldwide to accomplish data-mining and analysis on texts in the public domain and under copyright in a manner that was secure and compliant with appropriate U.S. copyright law. This did not happen, because the court rejected the agreement in 2011. Despite this, in 2011, the HTDL announced that Indiana University Bloomington and the University of Illinois at UrbanaChampaign would run the HTRC under a cooperative funding agreement with the HathiTrust Board of Governors and the University of Michigan. Since 2014, HTRC has made available as an active production service tools to analyze a set of out-of-copyright content equaling around 4.4 million volumes. In 2016, the HTRC plans to enable analysis of the entirety of the 15 million-volume corpus currently held by the HTDL, the largest digital academic library in North America. HTRC and Non-Consumptive ResearchThe HTRC has developed a process to define and work within the concept of non-consumptive computational access to support the fair-use of the HTDL corpus as defined within the Google Books Settlement Agreement that was a part of the Authors Guild et al. v. Google Inc case.Currently the HTRC defines the process for non-consumptive use of the HTDL corpus as:Research in which computational analysis is performed on one or more books, but not research in which a researcher reads or displays.Operationally, from the perspective of the HTRC research cyberinfrastructure, the HTRC defines non-consumptive research as:That which requires that no action or set of actions on the part of users, either acting alone or in cooperation with other users over the duration of one or multiple sessions can result in sufficient information gathered from a collection of copyrighted works to reassemble pages from the collection.  HTRC as PublisherDuring the course of work with scholars using the HTRC tools and services to create derivative non-consumptive data sets, the Center has often taken on a set of the roles traditionally played by publishers. These data sets are reviewed by members of the HTRC staff for compliance with non-consumptive use standards prior to release to the authors.As part of this work, the HTRC has offered as a service the capability to publish these non-consumptive, compliant data sets using a DOI scheme (Downie;2015). This service enables the creation of new derivatives (Downie; 2015) of published non-consumptive compliant data sets.A second benefit of opening access to these data sets is the ability to replicate current experiments that have been developed using the HTDL corpus and the HTRC tool set. From this standpoint the HTRC functions as a distant publisher of non-consumptive compliant data sets in support of new models of research inquiry. Distant Publishing as ConceptPrior to defining the concept of distant publishing, it is first instructive to understand distant reading within the context of digital humanities. Distant reading was first codified in 2000 by noted humanist and scholar Franco Moretti:Distant reading: where distance . . . is a condition of knowledge: it allows you to focus on units that are much smaller or much larger than the text: devices, themes, tropes - or genres and systems. And if, between the very small and the very large, the text itself disappears, well, it is one of those cases when one can justifiably say, less is more. (Moretti 2000) Moretti later expanded the concept in his 2013 monograph of the same name (Moretti 2013). Much like Moretti's definition that focuses on enabling a broader view of the text, the distant publisher enables a broader view of data sets through bringing to bear the current corpus of computational tools for large-scale textual data mining and analysis. HTRC as a distant publisher is removed by at least one degree from the creator, and remains distinct from any standardized concept of publisher. Yet, data sets are published under the rubric of the HTRC, and these publications are freed from the constraints of copyright in this context due to their non-consumptive nature. Thus we define distant publishing as:Publication of a non-consumptive data set outside of any standardized publishing construct, removed by í µí±¥ degree from the original creator, openly available to the community of scholars for replication and available for re-use in support of the advancement of knowledge.This definition is one that the HTRC aims to further refine in the coming years. We welcome broader thoughts on this concept from those working to preserve open research data and the software that makes that data accessible for use in scientific experimental replication and re-use for the long-term benefit of the scholarly community. Distant Publishing Use CasesCurrently the HTRC is developing models that support our current definition of distant publishing. These models are illustrated in several use cases, outlined below.• Extracted Features Worksets -HTRC expects this concept to be further refined as we move toward the second round of HTRC Advanced Collaborative Support grants which will be funded in summer 2016. Our most progressive case for distant publishing at this point is leveraged through the publication and release of our main extracted features workset. The current workset is a prototype based on the 4.8 million volume public domain collection from the HTDL. Through 2016-17 this workset will be redefined to include more of the HTDL collection. From this initial workset publication we have seen further refinements of the workset by scholars such as Ted Underwood ( Underwood et al. 2013), Colin Allen (Murdock, Zeng, and Allen 2016), and Matthew Wilkens (Wilkens 2013).• HT+Bookworm -The HathiTrust+Book-worm (HT+BW) project (2016) presents textual content through interactive visualization. Whereas HT+BW has previously been used in standalone contexts with pre-determined metadata, currently HT+BW is enabling scholars to analyze custom personal collections from within the larger corpus and the use of HT+BW as a supplement to other uses of the HTRC. This concept could eventually become a new possibility for derived workset publication in its own right.• HTRC Workset Ontology - Currently in development, the HTRC Workset Ontology is part of a collections data model by the Workset Creation for Scholarly Analysis project (HTRC 2016), an HTRC research initiative funded by the Andrew W. Mellon Foundation. The resulting HTRC Workset data model is designed to aid humanities scholars by helping them to describe selected portions of the HTDL corpus that serve as the objects of their research. The resulting worksets are persistent, citable, and can be assessed by other scholars for reuse in additional research processes. ConclusionToday's digital scholars are embracing new opportunities to explore their disciplines through the type of enhanced computational analysis that the HTRC provides. As the Center works to define emerging possibilities within the context of non-consumptive research, distant publishing will enable us to engage with the community of open data and open software publishers to ensure that our collections are accessible, open and available for the next generation of distant readers and their plans for new forms of scholarship.This concept has been further refined in the course of the development of the HTRC Data Capsule (Zeng et al. 2014) for secure data analysis and the development of the HTRC Workset Ontology (Jett et al. 2016) and has been codified in the recently released HathiTrust Research Center Non-Consumptive Use Research Pol- icy (HTRC, 2016). ",
        "article_title": "Research Center as Distant Publisher: Developing Non-Consumptive Compliant Open Data Worksets to Support New Modes of Inquiry",
        "authors": [
            {
                "given": "Robert",
                "family": "Mcdonald",
                "affiliation": [
                    {
                        "original_name": "Indiana University",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionMoEML's gazetteer of 6500 London place name variants invites the mapping of datasets with a geographical component. As a textual editing project with an interest in print culture, we have long hoped to mobilize our GIS tools and gazetteer data in the service of mapping the English book trade. Our ultimate goal is to publish a layer showing the printing and/or retailing locations of the approximately 25,000 books printed in London between 1475 and 1640. Imprint lines in early modern books include highly granular location data, which has meant that book history has traditionally had an implicit geospatial dimension. A typical imprint line tells us that copies of a folio are \"Printed by Elizabeth Purslovv, and are to be sold by Nicholas Bourne, at his shop at the south entrance of the Royall Exchange, 1633.\" Using the information in such imprint lines, Kathleen Pantzer reorganized the items in the Short Title Catalogue under location headings (Pantzer numbers) in Vol. 3 of the catalogue. Her work facilitates questions about the proximity of one printer or bookseller to another, and thereby questions about affiliations, collaborations, and specialization among a key group of early modern cultural agents.However, considerable processing of Pantzer's printed lists is required to visualize or map STC items. Thus far, digital databases like Early English Books Online (EEBO) and the English Short Title Catalogue (ESTC) have captured the imprint line without parsing it into discrete data points, thereby leaving Pantzer's formidable interpretive work behind as we move into the era of digital historical bibliography. The Database of Early English Playbooks (DEEP) has included and corrected Pantzer numbers, but only for the printed plays, of course. MoEML has attempted to replicate Pantzer's work via datamining the ESTC. After several unsuccessful NER experiments on ESTC data, we are now mobilizing the curatorial work of DEEP and planning to extend their work beyond playbooks. In this paper, we take imprint lines and geospatial information about the book trade as a case study in mining carefully curated data. We explain the long history of this project as it extends back to Pantzer's own work creating the strict vocabulary for the print locations of early modern texts. We then discuss how MoEML has been able to put the STC data onto the Agas map, giving a better sense of the spatial relationship of printed early modern texts. In doing so, our argument centers on the necessity for authority names and strict vocabularies. Invoking Mike Poston's suggestion that we cannot predict the uses of our data, we use our own work on various print and digital databases to show how we can control and scaffold the mining processes to establish links between several pairs of projects in order to mine and ingest data from databases that do not share a common data field with the initial project in the sequence. We conclude with a list of considerations and principles for maximizing future interoperability between literary datasets. MethodologyAlthough not strictly based in MySQL technology, our methodology borrows from the work of digital humanists like Harvey Quamen and Jon Bath who use MySQL to design relational databases. Indeed, in order to establish valuable connections across diverse datasets, we must first identify what data points these datasets have in common (either directly or indirectly). For example, suppose that Dataset 1 contains raw data for categories A, B, and C and Database 2 contains raw data for categories X, Y, and C. By identifying common data points in category C between databases 1 and 2, it becomes possible to make further connections among categories A, B, X, and Y. From here, we could identify common data points in a third database that contains raw data for categories E, F, and X. We believe that relational databases provide the best platform to capture this \"web of relations\" in big data. Quamen and Bath describe relational databases as \"a series of interconnected spreadsheets. Each spreadsheet--called a table in database lingo--contains information on a real world entity such as People or Books or Songs or Birds or Rock Concerts or Places. Those tables are then tied together via relationships\" (Quamen and Bath, 2016;146-147). By providing a set of stepping stones or crosswalks between diverse datasets, relational databases enable us to build links between allied projects (i.e., ones that share a common data point) and more remote projects (i.e., ones that do not share a common data point) in order to combine expertise and mobilize already curated data in new environments. Past WorkIn 2014, MoEML research assistant Tye LandelsGruenewald undertook a directed study course with director Janelle Jenstad with the aim of geocoding the English Short Title Catalogue (ESTC) from 1475 through 1666. With the generous assistance of David Eichmann and Blaine Greteman of the Shakeosphere project (based at the University of Iowa), we were able to extract toponyms from transcribed imprints in the ESTC catalogue using natural language processing (NLP) technology. We had intended on using named entity recognition to find matches between the extracted ESTC toponyms and our own gazetteer of early modern London locations; however, the toponyms themselves included too many errors or extra text to make this feasible. As Grover, Givon, Tobin, and Ball note in their white paper on \"Named Entity Recognition for Digitised Historical Texts,\" there is still much work be done in order to teach named entity recognition software to recognize early modern English ( Grover et al., 2008).Concomitantly, Jenstad was manually compiling a spreadsheet of Pantzer numbers and crossreferencing them to MoEML location identifiers. Pantzer numbers are an alphanumeric string consisting of a letter and an integer. The letter indicates a general location. All the Pantzer numbers beginning with the letter O indicate locations in, near, or \"against\" the Royal Exchange. The numbers offer more granularity. For example, O.2 designates a location \"at the north side of the Royal Exchange.\" Key challenges in matching Pantzer numbers with MoEML IDs were (1) different controlled vocabularies, and (2) the different levels of granularity inherent in the projects. Pantzer's authority names came from the imprint line wording; MoEML authority names are standardized spellings of the official or most common toponym variant (determined by set of critical rules we codified in order to build our gazetteer). Granularity differences emerged from the different interests of the two projects. Book historians map the bookstalls in the Royal Exchange, a location for which MoEML considered as a single entity (ROYA1); MoEML finer granularity emerges elsewhere, in our mapping of conduits, landings, and the many other precise locations that John Stow mentions in his Survey of London. A full crosswalk between Pantzer and MoEML would require the addition of sublocations to MoEML's placeography, a goal we will likely realize via the development of MoEML microsites for the Royal Exchange and Paul's Churchyard. In the meantime, we lose some of the granularity of Pantzer's data by assigning the same MoEML id to two or more Pantzer numbers. Current WorkThese past-attempts at establishing interoperability between datasets illustrate the challenges in attempting to traverse projects that only weakly share common data points. Between MoEML and the ESTC are a number of assumptions, potential errors, and remediations that weaken the link between the two respective datasets. To get to our larger project of mapping the STC, we must take smaller steps.Our current work relates the playbook data collected by Zachary Lesser and Alan B. Farmer at The Database of Early English Playbooks (DEEP) to our own toponymic data, relying on Pantzer's vocabulary as a shared data-point. Jenstad's spreadsheet was transformed into a TEI-conformant XML table, which we ran across DEEP's openly available XML data. Doing so allows us to integrate DEEP numbers into the site, linking outwards to DEEP's newly static and predictable URLs.The DEEP data and Pantzer-MoEML table can be related, but we recognize that this relationship is not immutable. In other words, both datasets are \"living\" databases insofar as the data can-and should becurated and edited. Once Jenstad's spreadsheet was converted into TEI, Landels-Gruenewald was tasked with editing and refining Jenstad's initial findings to reflect the the growth of MoEML's gazetteer over the past two years (the MoEML team tagged nearly 2000 more toponyms between July 25, 2014-the last day Jenstad worked on the spreadsheet- and October 31, 2016, from 11,259 to 16,120). Lesser and Farmer have also recognized the need to amend Pantzer's findings in their data. Future WorkThe experiment with DEEP data has given us a stronger link to the ESTC. Now that we know Pantzer numbers are relatable to MoEML toponym IDs, we can now mobilize the data from Pantzer's appendix to connect MoEML with the ESTC. We plan to convert Pantzer's printed aggregations of STC numbers to digital files via OCR. With some curation, we will then have a list of all the STC numbers at each Pantzer number; using our crosswalk between Pantzer numbers and MoEML IDs, we will have a list of STC numbers (and therefore of unique print editions and issues) associated with MoEML locations. From the ESTC, we can obtain a crosswalk relating STC numbers to ESTC numbers. We add the caveat that Pantzer's locations will need to be corrected as book historians like Lesser and Farmer bring their knowledge to bear on her interpretation of STC data; every crosswalk dependent on her data will need to be refreshed and all the data maps remade. We can display these STC numbers as lists on MoEML location pages, much as Pantzer's print database does; in the digital environment, we can make dynamic links to DEEP or ESTC open-access pages for the book. We can also map these numbers on our open-layers Agas map platform as a layer of imprints associated with locations, eventually in combination with other tags (such as genre, now being added to EEBO by other scholars) or with other metadata fields harvested from the ESTC. All this data will pivot on the STC-MoEML data crosswalk that we are producing via Pantzer, following DEEP's initial work. Distant Future WorkA longer-term goal is to harvest from the ESTC's XML files the strings of characters transcribed in the imprint line metadata field. Since we will already know from the STC-MoEML crosswalk which location is described in the imprint line, we can sort the imprint lines by locations and do rapid human scans for outliers, which may be a quick way of correcting Pantzer's data. We can also wrap TEI tags around the toponyms in the imprint lines, thereby increasing the number of toponymic variants in the MoEML gazetteer. The more variants in the gazetteer, the more accurate any future NER or geoparsing of large corpora will be. Given that we already search the EEBO-TCP corpus manually for references to place, we aspire to run our gazetteer against the entire TCP corpus to find and then map toponyms. Principles and Practices of Curation for Future Mining and InteroperabilityAcknowledging that the most interesting future uses of a project's data have not yet been imagined (Poston, 2011), how can we maximize the opportunities for other people to do things with that data? We suggest the following principles and practices as a starting point for discussion:  ConclusionPantzer died in 2005, the year before MoEML was published at a public URL, but we like to think that she would have welcome the digital recreation, correction, curation, and connection of her data. She used the capacities of print to create a map and dense crossreferences. Having \"o'erleapt\" Pantzer's curatorial work in building our digital catalogues, we now need to capture her formidable scholarship of interpreting and relating disparate types of data. We began with the goal of relating MoEML toponyms to ESTC numbers, but discovered that Pantzer's hand-curated data was more reliable than the results of NER and NLP. Our new question then became: \"What sort of steps, processes, principles, and practices are necessary in doing this sort of work?\" Handcrafted data, in conjunction with computer processing, allows for greater interoperability between projects and begins to achieve the possibilities of the data not conceived by Pantzer.1 .1Make your data free to the world, preferably  in easily downloadable and manipulable  formats (in .json or .xml files, for example).  2. Be clear about how you compiled your data.  3. If you are aware of limitations in your data,  tell the world.  4. As you correct and refine your data,  communicate regularly about data updates.  5. If you are using other people's data in your  own applications, check back regularly and  rebuild the data crosswalks.  6. Know the weak link(s) in your data  crosswalks.  7. Plan for corrections as other projects  improve their data.  8. Be mindful of the potential for error to  compound. Errors in my data, combined  with errors in your data, have the potential  to lead scholars to false conclusions.  9. Test your data crosswalks in a variety of  ways. Take a small subset of the data and  compare NLP results to hand curated results,  for example.   ",
        "article_title": "Mapping the STC with MoEML and DEEP",
        "authors": [
            {
                "given": "Janelle",
                "family": "Jenstad",
                "affiliation": [
                    {
                        "original_name": "University of Victoria",
                        "normalized_name": "University of Victoria",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/04s5mat29",
                            "GRID": "grid.143640.4"
                        }
                    }
                ]
            },
            {
                "given": "Tye",
                "family": "Landels-Gruenewald",
                "affiliation": [
                    {
                        "original_name": "Queen's University",
                        "normalized_name": "Queen's University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02y72wh86",
                            "GRID": "grid.410356.5"
                        }
                    }
                ]
            },
            {
                "given": "Joseph",
                "family": "Takeda",
                "affiliation": [
                    {
                        "original_name": "University of British Columbia",
                        "normalized_name": "University of British Columbia",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03rmrcq20",
                            "GRID": "grid.17091.3e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionWe present in this paper work consisting in porting to an integrated ontology two central resources for the classification of folktales: The \"Motif-index of folkliterature\" (Thompson, 1977) and the \"Types of International Folktales\" (Uther, 2004). The first resource, often called Thompson Motif Index (abbreviated as TMI) is available as an online resource. The second resource is a classification system for folktale types, which was published by Hans-Jörg Uther (2004), extending former work by Antti Aarne (1961) and Stith Thompson (1977). In the following we are using the acronym ATU for referring to this classification system. Recently, a large amount of the ATU data has been made available online, offering also annotation facilities for tales in multilingual versions.Our work consisted in extracting from those resources, which are stored in different formats, classification relevant information and re-organizing them in two interrelated ontologies, using for this the W3C standards OWL (which stands for \"Web Ontology Language\" , RDF(s)-see the W3C recommendations for more details- and RDF. The aim is to make those classification resources machine readable, interoperable and to support by this formal representation of the metadata access to folktales annotated by those classification systems in the context of the Linked Open Data framework. Thompson Motif IndexA folktale motif can be defined as a \"repeated story element, e.g., a character, an object, an action, or an event that can be found in several stories\". In TMI motifs are organized in a tree structure, providing for a parent-child relation between the listed elements. One motif entry consists of a motif-id and a motif name. Optionally, a motif description and references are provided. Table 1 provides for an example of few motifs illustrating the tree structure and hierarchy of TMI. A folktale type can be described as a main story line that can be found in several cultures. The parts of this story line can refer to specific story elements also known as motifs. A folktale type is therefore a bigger unit than a motif. As can be seen in example 1, an entry in the ATU system consists of a type id (\"6*\"), a title (\"Animal Captor Talks with Booty in his Mouth (previously The Wolf Catches a Goose).\") and a text summarizing the typical \"storyline\" of this type of folktale. Within or at the end of this \"script\", links to corresponding Thompson Motif-Indices can be provided (\"[K561.1]\"). Finally (and optionally), similar or related types can be indicated. Generation of the OntologyThe OWL and RDF(s) representation for the ontology was generated semi-automatically from the html code of both TMI and ATU, responding to few design decisions we had to take. For TMI we went for a double representation: the hierarchy structure of the IDs is represented as an OWL subclass hierarchy, but all terminal nodes (leaves of the tree) are represented as both an instance of a class we call \"Motif\" and as an instance of the pre-terminal node in the taxonomy. This reflects our intuition that what Thompson called a motif is in most of the cases the content of the terminal nodes of the classification system, while the non-terminal nodes are more to be considered as abstraction helping in the taxonomic structures.We compared the automatically created ATU part of the ontology to the printed version of Hans-Jörg Uther's \"The Types of International Folktales\". Using the ontology editor \"Protégé\", we manually added missing subclasses and individuals, rearranged generated classes and corrected errors such as typos in the electronic version of ATU or splitting errors because of inconsistent punctuation. By this step we obtained 2802 ATU classes organized in seven main subclasses, which have also subclasses, in accordance with the hierarchical structure of types proposed in (Uther, 2004). Below we display examples of the encoding of ATU data in our ontology. We first show a main class (we use the Turtle syntax for serializing the RDF code) of our ATU model \":Type\": A subclass of \"Type\", for example the type \"6*\", has the following syntax:In this example, the reader can see how the type 6* is linked to a motif occurring typically in its storyline: we introduced for this a property called \"linkToTMI\". Additionally, the subclass relation is expressed, using the rdfs:subClassOf property. The \"rdfs:label\" property stores the original short title of the ATU type in English (\"@en\"). We encode the original description of the type as a value to the property \"rdfs:isDefinedBy\". A main aspect of the ontologisation of ATU (and TMI) is that each folktale type (or motif) is now represented by a Unique Resource Identifier (URI), and thus accessible in the Linked Data framework, once our data set has been published in its cloud.An example of a motif (\"K561.1\") is given just below. We focused for the time being only on motif-ids and names. This current limitation is due to the inconsistent format of the motif descriptions and references used in the html code of the online resource, which made it difficult to be automatically extracted. We will include this information in a next version of the ontology. As pointed out earlier, elements of the TMI are encoded in a dual fashion: as belonging to the class \"Motif\" but also to its immediate non-terminal node (here \"K561\"). The rdfs annotation property \"rdfs:label\" is used for encoding the name of the motif (here in English, marked by \"@en\"). Multilingual correspondences can also be included as values of this property.In this example, we also see the property \"linkFromTMIToATU\", which is the inverse property of the one pointing between ATU elements and motifs. Additionally, we have introduced a third \"linking\" property, called \"linkFromAaThToATU\", which ensures that types of former versions of ATU are linked to the new names in the final version of ATU. By this final step of expanding our TMI-ATU ontology we ended up with the number of 14,937 classes and the number of 49,752 individuals that are interconnected by 3 object properties: \"linkFromTMIToATU\", \"linkToTMI\" and \"linkFromAaThToATU\". We managed thus to convert two valuable, handcrafted resources of literary knowledge consisting of more than 4000 pages into a 15.4 MB in size ontology file that can be easily accessed and searched. ConclusionsWe presented our work on the ontology generation for two widely used folktale classification systems. This ontology can be visualised and processed by standard OWL tools such as Protégé. The integrated ontology will be made openly available soon, after last quality controls. Current work is on adding as instances of the ontologies URLs of folktales that areTable 1 :1A few motifs from Motif-index of folk-literature and  their hierarchical organisation  Aarne-Thompson-Uther Folktales Types  (ATU)   ",
        "article_title": "Towards a Linked Data Access to Folktales classified by Thompson's Motifs and Aarne-Thompson-Uther's Types",
        "authors": [
            {
                "given": "Thierry",
                "family": "Declerk",
                "affiliation": [
                    {
                        "original_name": "German Research Center for Artificial Intelligence",
                        "normalized_name": "German Research Centre for Artificial Intelligence",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01ayc5b57",
                            "GRID": "grid.17272.31"
                        }
                    },
                    {
                        "original_name": "Saarland University",
                        "normalized_name": "Saarland University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01jdpyv68",
                            "GRID": "grid.11749.3a"
                        }
                    }
                ]
            },
            {
                "given": "Lisa",
                "family": "Schäfer",
                "affiliation": [
                    {
                        "original_name": "Saarland University",
                        "normalized_name": "Saarland University",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/01jdpyv68",
                            "GRID": "grid.11749.3a"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Site-specific digital literature projectSite-specific digital literature, by allowing users to access place-bound, multimodal digital literature via mobile devices, opens doors for creative expression, place making as well as experiencing traditional texts in a new way. It can also contribute to a better understanding of the relationship between digital interfaces, spatiality and people and empower people in terms of what is called place-making practices (Kaye, 2000:203;Turner & Davenport, 217-220).Byderhand (Byderhand.net, 2016), is a site-specific digital literature project launched during the 2015 Aardklop National Arts Festival in Potchefstroom, South Africa. Users accessed multimodal texts on their mobile devices through QR-codes deployed around various sites at the North-West University. Byderhand was awarded the Aartvark prize for a ground breaking production, and the team was approached to host similar productions elsewhere. In 2016 the Byderhand-project and platform were used in various educational contexts. Most recently, the Byderhandteam, assisted 1st additional language (Afrikaans) learners at a Potchefstroom secondary school to publish their own site-specific digital literature on the school grounds.Although there is a demand for expansion of the Byderhand-project, cost and scalability are limiting factors. First, in its current form, implementing the project requires a team of dedicated members. is Second, functionality depends on the provision, maintenance and upgrading of infrastructure such as servers and physical QR-codes. On the user end, mobile data cost is a constraining factor. Therefore, we explored the design of the infrastructure for an automated platform that could empower a low-skilled user, to create, publish and access their own projects. This was important as usability of a platform can ether promote access or restrict it through the way the interface is developed (Shneiderman, B. 2003).The infrastructure chosen was a single-board computer (Raspberry Pi) and a platform was developed based on open standards. Virtual museum projectSouth Africa's heritage landscape remains largely skewed in ways that exclude intangible heritage such as stories and memories of such events as apartheidera forced removals. A virtual museum of forced removals aids in remedying this by offering an in situ space where people can give voice to their experience and preserve it for future generations.  ",
        "article_title": "Site-specific Cultural Infrastructure: Promoting Access and Conquering the Digital Divide",
        "authors": [
            {
                "given": "Andre",
                "family": "Goodrich",
                "affiliation": [
                    {
                        "original_name": "North-West University",
                        "normalized_name": "North-West University",
                        "country": "South Africa",
                        "identifiers": {
                            "ror": "https://ror.org/010f1sq29",
                            "GRID": "grid.25881.36"
                        }
                    }
                ]
            },
            {
                "given": "Gustaf",
                "family": "Templehoff",
                "affiliation": [
                    {
                        "original_name": "North-West University",
                        "normalized_name": "North-West University",
                        "country": "South Africa",
                        "identifiers": {
                            "ror": "https://ror.org/010f1sq29",
                            "GRID": "grid.25881.36"
                        }
                    }
                ]
            },
            {
                "given": "Juan",
                "family": "Steyn",
                "affiliation": [
                    {
                        "original_name": "North-West University",
                        "normalized_name": "North-West University",
                        "country": "South Africa",
                        "identifiers": {
                            "ror": "https://ror.org/010f1sq29",
                            "GRID": "grid.25881.36"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " , (Dow 1996), (Spangler 2003), (Morely 2003), and many more.Given that long-running television series broadcast hundreds of episodes and the major networks run dozens of series each season, these studies have largely had to rely on a close analysis of a small subset of series, episodes, and even scenes. Our project seeks to augment these approaches with computationallydriven analyses that can curate and aggregate the contents of tens of thousands of hours of television programming. Tracking these over the course of a scene and episode reveal characteristics of how characters interact and describe the narrative flow of the series. Figure 2. An example of scene classification from Bewitched. Each still image from the episode is tagged with the description of the sets on the sound stage. Following the progress of these over the course of the episode can serve as a way to compactly describe and aggregate the narrative arc of an episode. Comparing across episodes, seasons and shows reveals similarities and differences across the various series of interest.",
        "article_title": "Distant Seeing TV",
        "authors": [
            {
                "given": "Taylor",
                "family": "Arnold",
                "affiliation": [
                    {
                        "original_name": "University of Richmond",
                        "normalized_name": "University of Richmond",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03y71xh61",
                            "GRID": "grid.267065.0"
                        }
                    }
                ]
            },
            {
                "given": "Lauren",
                "family": "Tilton",
                "affiliation": [
                    {
                        "original_name": "University of Richmond",
                        "normalized_name": "University of Richmond",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03y71xh61",
                            "GRID": "grid.267065.0"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " RationaleLiterary canon formation and change has long been of concern to literary scholars. Writing in 1979, in terms that prefigure our current concerns with the scope of computational analysis, Alastair Fowler admits, \"The literature we criticize and theorize about is never the whole. At most we talk about sizable subsets of the writers and works of the past. This limited field is the current literary canon\" (97). Fowler defined six types of literary canons arising from various constraints and choices: official, personal, potential, accessible, selective, and critical. Anthologies are frequently taken to represent the literary canon because in themselves they constitute what Fowler calls a \"selective canon,\" a subset of works chosen by an editor (or editorial team) presumably for particular reasons. Anthologies make works accessible to a wide range of readers and also contribute to (or potentially constrain) the pedagogical canon, the subset of works that are taught (Harris 112-13). Anthologies often reflect the critical canon, as works that become interesting to scholars gradually start showing up in anthologies.In the so-called \"culture wars\" of the 1980s-1990s, debates about multiculturalism and changes to the humanities curriculum frequently focused on anthologies and syllabi as synecdoches for university education (Graff). As John Guillory suggests, both the conservative and progressive sides in this debate tended to rely on \"an ideology of tradition\" which invokes \"an autonomous history of literature, which is always a history of writers and not of writing\" (Guillory 63). For Guillory and Bourdieu, the history of writing can only be understood in relation to mechanisms of cultural value.When literary scholars write about anthologies, they frequently describe how the selective canon changes over time; examine the ideological assumptions that undergird the selection process; and point out historical or thematic gaps in anthology coverage. Yet the method that they use for doing this primarily rests upon counting authors (Golding, Harris, Latane). Others use the number of pages allotted to each author as a proxy for importance or weight within the anthology (Bode, Damrosch, Lecker).Such approaches reify the ideology of tradition in assuming that author names alone adequately describe the complexities of literary history. As Guillory suggests, \"histories of canon formation, when they consist primarily of a narrative of reputations, of the names which pass in and out of literary anthologies explain nothing. Such narrative histories fail to recognize generic or linguistic shifts which underlie the fortunes of individual authors by establishing what counts as literature at a given historical moment\" (Guillory 64). By offering a network analysis approach that allows us to explore \"what counts as literature\" rather than just \"who counted\" in different anthologies, we can better understand the structures of value instantiated and reproduced in these collections and thereby better understand the history of taste and value. For example, rather than simply noting the unsurprising fact that the Victorian poet laureate Alfred Tennyson is included in all anthologies of Victorian literature and accepting that he is thereby a canonical figure, this approach allows for discovering more precisely which poems by Alfred Tennyson were valued in 1880, 1930, or 1980, and how those choices corresponded with selections from other poets. MethodIn this paper I explore the utility of several different approaches to analyzing the structures of canon formation as represented in poetry anthologies. These include:• examining the structure of the bimodal network created between anthologies and poems; • measuring anthology similarity based on textual couplings; • examining the co-printing network created by connecting each poem in an anthology to every other poem printed in that same anthology • Each of these approaches will be explored using the full dataset and by using chronological slices that will further the understanding of historical changes in the anthology canon. In the first approach, I treat the relationships of poems to anthologies as a bimodal affiliate network. Although some researchers avoid bimodal representations of relationships because standard measures of centrality and other metrics do not apply, force-directed visualization of bimodal data \"is often extremely effective for transmitting a holistic understanding of the whole dataset\" (Borgatti 10). Faust and Borgatti have each recommended approaches to calculating centrality for affiliation networks that I will also explore. I also examine centralization (core-periphery structure) and structural equivalence measures for the affiliation network.The remaining two approaches derive from established practices in bibliographic network analysis, particularly bibliographic coupling and cocitation analysis. Although co-citation analysis has largely overtaken bibliographic coupling in recent decades, recent comparative studies suggest that the utility of each approach varies depending on the timeframe and construction of the citation dataset (Boyack et al).Bibliographic coupling draws an edge between two documents which each cite a third (Kessler). To understand the similarities among these anthologies, I create a textual coupling network, which consists only of anthology nodes, and create an edge between each anthology that prints the same poem. This network shows the degree of similarity between anthologies and filtering the poem nodes used to create the edge weights allows for exploration of which texts create distinctive differences among the anthologies.In co-citation analysis, an edge is created between two documents which are cited together in a third document (Small). In my co-printing network, which consists only of poem nodes, I create an edge between poems that are printed in the same anthology. Calculating modularity for this network reveals clusters of poems that frequently occur together in the same anthology. These clusters are made up of texts by multiple authors and can be used to explore components of canonicity, such as thematic, formal, or aesthetic qualities shared by poems in each cluster. This paper argues that network analysis is a useful approach to examine the structure of the cultural field of Victorian poetry as it was constituted in key literary and teaching anthologies published from 1880-2002.   ",
        "article_title": "Measuring Canonicity: a Network Analysis Approach to Poetry Anthologies",
        "authors": [
            {
                "given": "Natalie",
                "family": "Houston",
                "affiliation": [
                    {
                        "original_name": "University of Massachusetts - Lowell",
                        "normalized_name": "University of Massachusetts Lowell",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/03hamhx47",
                            "GRID": "grid.225262.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionEnjambment takes place when a syntactic unit is broken up across two lines of poetry ( Domínguez Ca- parrós, 2000: 103), giving rise to different stylistic effects (e.g. increased emphasis on elements of the broken-up phrase, or contrast between those elements), or creating double interpretations for the enjambed lines ( García-Paje, 1991).In Spanish poetry, the syntactic configurations under which enjambment takes place have been described extensively, and detailed studies on the use of enjambment by individual authors exist (see Martínez Cantón, 2011 for an overview) including, among others Quilis (1964), Domínguez Caparrós, (2000), Paraíso, (2000), Spang (1983) for a description of enjambment, and Alarcos (1966), Senabre (1982), Luján (2006), Martínez Fernández (2010) for case-studies on a single author. However, a larger-scale study to identify enjambment across hundreds of authors spanning several centuries, enabling distant reading (Moretti, 2013), was not previously available.Given that need, we have developed software, based on Natural Language Processing, that automatically identifies enjambment in Spanish, and applied it to a corpus of approx. 3750 sonnets by ca. 1000 authors, from the 15th to the 19th century. What is the interest of such large-scale automatic analyses of enjambment? First, the literature shows a debate about which specific syntactic units can be considered to trigger enjambment, if split across two lines, and whether lexical and syntactic criteria are sufficient to identify enjambment. Second, the stylistic effects that enjambment permits are also an object of current research (Martínez Fernández, 2010). Systematically collecting large amounts of enjambment examples provides helpful evidence to assess scholars' current claims, and may stimulate novel analyses. Finally, our study complements Navarro's (2016) automatic metrical analyses of Spanish Golden Age sonnets, by covering a wider period and focusing on enjambment.The abstract is structured thus: First we provide the definition of enjambment adopted. Then, our corpus and system are described, followed by an evaluation of the system. Finally, findings on enjambment in our diachronic sonnet corpus are discussed. The project's website provides details omitted here for space reasons, including samples for the corpus, results, and other details. Enjambment in SpanishSyntactic and metrical units often match in poetry. However, this trend has been broken since antiquity for various reasons (Parry (1929) on Homer, or Flores Gómez (1988) on early classical poetry).In Spanish tradition, enjambment (in Spanish, \"encabalgamiento\") is considered to take place when a pause suggested by poetic form (e.g. at the end of a line or across hemistichs) occurs between strongly connected lexical or syntactic units, triggering an unnatural cut between those units. Quilis (1964) performed poetry reading experiments, proposing that the following strongly connected elements give rise to enjambment, should a poetic-form pause break them up:• Lexical enjambment: Breaking up a word.We translated \"lexical enjambment\" from Quilis's terms \"encabalgamiento léxico\" or \"tmesis\".• Phrase-bounded enjambment: Within a phrase, breaking up sequences like \"noun + adjective\", \"verb + adverb\", \"auxiliary verb + main verb\", among others. We translated \"phrase-bounded enjambment\" from \"encabalgamiento sirremático\".• Cross-clause enjambment: Between a noun antecedent and the pronoun heading the relative clause that complements the antecedent. We translated \"cross-clause enjambment\" from Quilis's \"encabalgamiento oracional\".The project site includes Quilis's complete list of syntactic environments that can trigger enjambment, as well as the types identified by our system. Besides the enjambment types above, Spang (1983) noted that if a subject or direct object and their related verbs occur in two different lines of poetry, this can also feel unusual for a reader, even if the effect is less pronounced than in the environments identified by Quilis. To differentiate these cases from enjambment proper, Spang calls these cases \"enlace\", translated here as \"expansion\".Quilis (1964) was the only author so far to gather recitation-based experimental evidence on enjambment. His typology is still considered current, and was adopted by later authors, although complementary enjambment typologies have been proposed, as Martínez Cantón (2011) reviews. Our system identifies Quilis' types, besides Spang's expansion cases. CorpusThe corpus is based on two public online collections from Biblioteca Virtual Cervantes (García Gonzá-lez, R. (ed.), 2006a, 2006b). The first one covers 1088 sonnets by 477 authors from the 15th-17th centuries. The second one contains 2673 sonnets by 685 authors from the 19th century. We created scripts to download the poems, remove HTML and extract dates of birth and death for the authors (About 30% of the 15th to 17th century authors had exact dates of birth and death, for the rest only the centuries were available. Among the 19th century authors, ca. 45% had exact dates of birth and death). Table 1 shows the distribution of authors and poems by century. The corpus covers canonical as well as minor authors, inspired in distant reading approaches (Moretti, 2007(Moretti, , 2013. * Exact dates of birth and death are available for a minority of authors; often only the century was provided in the corpus sources. Periods ending in \".5\" cover authors who lived in two centuries. E.g. period \"15.5\" covers authors born in the 15th and deceased in the 16th century System descriptionThe system has three components: a preprocessing module to format input poems uniformly, an NLP pipeline, and the enjambment-detection module itself.The NLP pipeline is IXA Pipes ( Agerri et al., 2014). Its results for contemporary Spanish are competitive. Our system uses it to obtain part-of-speech tags, syntactic constituency (e.g. verb-phrase, noun-phrase) and syntactic dependencies (e.g. direct object).The enjambment detection module is rule and dictionary-based, and exploits the information provided by the NLP pipeline. Rules (30 in total) of different characteristics identify enjambed lines, assigning them a type among a list of 12 types, based on the typology in Section 2 (the full list of types identified by the system is available on the project site).• Some rules are very shallow and only take parts of speech into account.• Some rules additionally exploit constituency info.• Some rules use dependency information, e.g. to detect \"subject / object / verb\" relations.• For any type of rule, custom dictionaries can restrict rule application to a set of terms. E.g. certain verbs govern arguments introduced by one specific preposition; we itemized these verbs and their prepositions in a dictionary, to complement information provided by the NLP pipeline or correct parsing errors.Enjambment annotations are output in standoff format. Further details can be found on the project's site. System evaluation and discussion Test-corpusTo evaluate the system, we created two referencesets (SonnetEvol and Cantos20th), manually annotating enjambment in them. SonnetEvol: 100 sonnets (1400 lines) fromour diachronic sonnet corpus of ca. 3750 sonnets (Table Table). This test-set contains 260 pairs of enjambed lines (in other words, if there is an enjambment between lines 1 and 2, we consider that as \"pair of enjambed lines\" in the reference corpus). 2. Cantos20th: 1000 lines of 20 th century poetry (Colinas, 1983), showing natural contemporary syntax. We identified 277 pairs of enjambed lines.The distribution of enjambment types in the testcorpora is balanced ( Table 2). The SonnetEvol diachronic test-corpus is balanced across periods (Table 3). It should be noted that balancing across periods does not apply to the Cantos20th test-corpus: it covers the 20 th century only.We annotated the Cantos20th corpus in order to assess the system's performance on contemporary Spanish with natural diction, compared to its behaviour with the SonnetEvol corpus, which includes some archaic constructions and often shows an elevated register.For the evaluation reported here, each sonnet was annotated by a single annotator. Obtaining multiple annotators' input on the same sonnet to assess interannotator agreement (Artstein and Poesio, 2008) is part of our ongoing work. Table 2: Distribution of enjambment types in the manually annotated reference corpora, providing counts and each type's percentage of the total enjambments per corpus.Counts refer to pairs of enjambed lines. *The project site includes a description of each enjambment type. Table 3: Distribution of sonnets by period in the manually annotated SonnetEvol corpus. The 16th, 17th and 19th centuries cover ca. 30% of the corpus each, and the 15th century covers ca. 10% of the sonnets **Exact dates of birth and death are available for a minority of authors; often only the century was provided in the corpus sources. Periods ending in \".5\" cover sonnets for authors who lived in two centuries. E.g. period \"15.5\" covers sonnets for authors born in the 15th and deceased in the 16th century Enjambment-detection tasks evaluatedWe defined two enjambment-detection tasks:• Span-match: the positions of enjambed lines proposed by the system must match the positions in the reference corpus for a correct result to be counted.• Typed span-match: for a correct result, both the positions and the enjambment type assigned by the system to those positions must match the reference. System results and discussionPrecision, recall and F1 were obtained. The definitions for Precision (P), Recall (R) and F1 were the usual: Table 4 provides overall results for both corpora. Table 5 provides the per-type results on the diachronic test-corpus (SonnetEvol). The project's site contains more detailed results (e.g. per-type results for the Cantos20th corpus, or breakdowns for SonnetEvol per period). Table 4: Overall enjambment detection results. Number of test-items, Precision (P), Recall (R) and F1 in our two testcorpora, for the span-match and typed span-match enjambment detection tasks Table 5: Enjambment detection results per type. On the SonnetEvol corpus. Number of items per type, Precision (P), Recall (R) and F1 on the typed span-match enjambment detection task. * The types are described on our site: http://sites.google.com/site/spanishenjambment/enjambmen t-typesFor untyped detection (span-match), F1 reaches 80% in the SonnetEvol corpus, whereas F1 for typed detection is 66.31%. For the contemporary Spanish corpus (Cantos20th), F1 is higher: 80.63% typed detection, 86.51% span-match. This reflects additional difficulties posed by archaic language and historical varieties for the NLP system whose outputs our enjambment detection relies on. Expansions get lower F1 than phrase-bounded types overall. But we do not think that the F1 difference between SonnetEvol and Cantos20th is due to the higher proportion of expansions in SonnetEvol (Table 2): Results per-type (see the evaluation page of the project's site) show that phrase-bounded enjambment detection is 10 points of F1 lower in SonnetEvol than in Cantos20th. Also, phrase-bounded enjambment results for the 15 th -17 th period (with more archaic language) are 10 points of F1 lower than in the 19 th century.A common source of error was hyperbaton: the displacement of phrases triggers constituency and dependency parsing errors. Prepositional phrase (PP) attachment also posed challenges: Verbal adjuncts get mistaken for PPs complementing nouns or adjectives. This is a common problem in syntactic parsing, even for contemporary languages (see Agirre et al, 2008, for English). For historical varieties, Stein's (2016) results for verbal adjuncts and prepositional complements in Old French also suggest the difficulties posed by prepositional phrases.Creating a reparsing module to manage hyperbaton and improve PP attachment results may be fruitful future work. Scholarly results and discussionThe system's goal is detecting enjambment to help literary research on the phenomenon, via providing systematic evidence for its analysis.We consider our untyped enjambed-line detection results helpful, given an F1 of ca. 80% on the diachronic test-set. As an example application, we examined the distribution of enjambment according to position in the poem, particularly in positions across a verse-boundary (lines 4-5, 8-9 and 11-12). Comparing the results for the 15 th -to-17 th centuries vs. the 19 th century (Table 6 and Figure 1), we see that enjambment across the tercets increases clearly in the 19 th century, with a small increase of enjambment across the quatrains (lines 4-5) and across the octave-sestet divide (lines 8-9). Given the manageable data volume, we validated the counts for enjambment across a verse boundary (Table 6) manually (but not the more voluminous data for all other positions).The value of the tool is helping perform such analyses on a large corpus. This opens the door for scholars to assess the literary relevance of the findings, and search for the best interpretation.  with a small increase in the 19 th century. The division between the tercets blurs in the 19 th century, in the sense that enjambment across them is clearly higher than in the previous period OutlookThe characterization of enjambment in Spanish literary theory has unclear points. Systematically obtaining enjambment examples is helping us find additional evidence to analyze these unclear points. Moreover, we are not aware of a systematic large-sample study of enjambment across periods, literary movements, or versification types in Spanish, or other languages. Automatic detection can help answer interesting questions in verse theory, which would benefit from a quantitative approach, complementing small-sample analyses. e.g.: To what an extent is enjambment used differently in free verse vs. traditional versification? Students in our metrics classes are currently annotating enjambment for 450 sonnets. These annotations will permit inter-annotator agreement computation. We will also examine the possibility of using supervised machine learning to train a sequence labeling and classification model to complement our current detection rules.",
        "article_title": "Distant Rhythm: Automatic Enjambment Detection on Four Centuries of Spanish Sonnets",
        "authors": [
            {
                "given": "Pablo",
                "family": "Fabo",
                "affiliation": [
                    {
                        "original_name": "Lattice Lab - CNRS (Centre national de la recherche scientifique)",
                        "normalized_name": null,
                        "country": "France",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Clara",
                "family": "Cantón",
                "affiliation": [
                    {
                        "original_name": "Universidad Nacional de Educación a Distancia (UNED) (National Distance Education University)",
                        "normalized_name": null,
                        "country": "Spain",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Thierry ",
                "family": "Poibeau",
                "affiliation": [
                    {
                        "original_name": "Lattice Lab - CNRS (Centre national de la recherche scientifique)",
                        "normalized_name": null,
                        "country": "France",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Word embeddings, such as those created by the word2vec family of algorithms (Mikolov et al., 2013), are the current state of the art for modeling lexical semantics in Computational Linguistics. They are also getting more and more popular in the Digital Humanities , especially for diachronic language research (see below). Yet the most common methods for creating word embeddings are ill-suited for deriving qualitative conclusions since they typically involve random processes that severely limit the reliability of results-repeated experiments differ in which words are deemed most similar with each other (Hellrich and Hahn, 2016a,b). We provide a short overview of different embedding methods and demonstrate how this lack of reliability might affect the outcome of experiments. We also recommend a more recent embedding method, SVDPPMI (Levy et al., 2015), which seems immune to these reliability problems and, thus, much better suited (not only) for the Digital Humanities (Hamilton et al., 2016). Word embeddings are a form of computational dis-tributional semantics for determining a word's meaning \"from the company it keeps\" (Firth, 1957, p. 11), i.e., the words it co-occurs with. The word2vec algorithms have their origin in heavily trimmed artificial neural networks. Their skip-gram negative sampling (SGNS) variant is widely used because of its high performance and robustness (Mikolov et al., 2013; Levy et al., 2015). Two other word embedding methods were inspired by word2vec: GloVe (Pennington et al., 2014) tries to avoid the opaqueness stemming from word2vec's neural network heritage through an explicit word co-occurrence table, while the more recent SVDPPMI (Levy et al., 2015) is built upon the classical pointwise mutual information co-occurrence metric (Church and Hanks, 1990) enhanced with pre-processing steps and hyper-parameters from the two aforementioned algorithms. There are two sources of randomness affecting the training of SGNS and GloVe embeddings: First, the random initialization of all word embedding vectors before any examples are processed. Second, the order in which these examples are processed. Both can be replaced by deterministic alternatives, yet this would simply replace a random distortion with a fixed one, thus providing faux reliability only useful for testing purposes. In contrast, SVDPPMI is conceptually not affected by such reliability problems, as neither random initialization takes place nor is a relevant processing order established. Word embeddings can be compared with each other to measure the similarity of words (typically by cosine)-an ability by which they are often assessed (see e.g., Baroni et al. (2014) for more details on their evaluation). In the Digital Humanities, they have already been used to directly track diachronic changes in word meaning by comparing representations of the same word at different points in time (Kim et al., 2014; Kulkarni et al., 2015; Hellrich and Hahn, 2016c; Ham-ilton et al., 2016). They can also be used to track clusters of similar words over time and, thus, model the evolution of topics (Kenter et al., 2015) or compare neighborhoods in embedding spaces for preselected words (Jo, 2016). Besides temporal variations, word embeddings are also suited for analyzing geographic ones, e.g., the distinction between US American and British English variants (Kulkarni et al., 2016). In most of these approaches, the local neighborhood of selected words in the resulting embedding spaces, i.e., words deemed to be most similar with a word in question , are used to approximate their meaning at a given point in time or in a specific domain. Yet the aforemen-tioned randomness leads to a lack of replicability, since repeated experiments using the same data set and algorithms result in different neighborhoods and might thus mislead researchers. To investigate this problem, we trained three models each with three embedding methods, i.e., GloVe and SVDPPMI, on the same data set and measured how they differ in their outcomes on word neighborhoods. Our data set consists of 645 German texts from the 19 th century that are part of the Deutsches Textarchiv Kern-korpus (DTA) [German text archive core corpus] (Gey-ken, 2013; Jurish, 2013). The DTA contains manually transcribed texts selected for their representativeness and cultural importance; we use the orthographically normalized and lemmatized version, with casefolding. We evaluate the word embedding methods by calculating the percentage of neighbors for the most frequent nouns in the DTA on which all three models of each method agree. Overall, SVDPPMI provides perfect reliability, while the other two embedding methods lack reliability, SGNS dramatically so, which is consistent with our prior studies on word2vec (Hellrich and Hahn, 2016a,b). Figure 1 shows the reliability for each model evaluated against the 1000 most frequent nouns in the DTA when their first ten closest neighbors (from one up to ten) are compared. Larger neighborhood size had a small positive effect on the reliability of SGNS and GloVe, yet is clearly unable to mitigate the inherent un-reliability of these methods. A small inverse effect can be observed when the number of the most frequent nouns is modified while keeping a constant neighborhood size of five, as displayed in Figure 2. Finally, Table 1 provides differing neighborhoods for Herz [heart] as a qualitative example. In this case, though not necessarily in general, SGNS models featured a more anatomical view (e.g., bluten [to bleed]), whereas GloVe models uncovered metaphorical meaning (e.g., gemüt [mind]) and SVDPPMI came out with a mix thereof. Using SGNS or GloVe models to assess a word's meaning can be strongly misleading, as evi-denced by e.g., three SGNS models representing three different runs under the same experimental setup. They lead to completely different semantic characterizations of Herz [heart], since two provide negatively connotated words (e.g., schmerzen [pain]) as closest neighbors, whereas the third provides a more positive impression (e.g., herzen [to caress]).",
        "article_title": "Don't Get Fooled by Word Embeddings- Better Watch their Neighborhood",
        "authors": [
            {
                "given": "Johannes",
                "family": "Hellrich",
                "affiliation": [
                    {
                        "original_name": "Friedrich Schiller University Jena",
                        "normalized_name": "Friedrich Schiller University Jena",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05qpz1x62",
                            "GRID": "grid.9613.d"
                        }
                    }
                ]
            },
            {
                "given": "Friedrich",
                "family": "Schiller",
                "affiliation": [
                    {
                        "original_name": "Friedrich Schiller University Jena",
                        "normalized_name": "Friedrich Schiller University Jena",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05qpz1x62",
                            "GRID": "grid.9613.d"
                        }
                    }
                ]
            },
            {
                "given": "University",
                "family": "Jena",
                "affiliation": [
                    {
                        "original_name": "Friedrich Schiller University Jena",
                        "normalized_name": "Friedrich Schiller University Jena",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05qpz1x62",
                            "GRID": "grid.9613.d"
                        }
                    }
                ]
            },
            {
                "given": "Udo",
                "family": "Germany",
                "affiliation": [
                    {
                        "original_name": "Friedrich Schiller University Jena",
                        "normalized_name": "Friedrich Schiller University Jena",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05qpz1x62",
                            "GRID": "grid.9613.d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Willa Cather Archive (WCA) at the University of Nebraska-Lincoln (UNL) is currently working on transcription and annotation of 1500 letters to be released in 2018. As editors will write several thousand annotations, the workflow logistics are complicated. Annotonia ( a portmanteau of \"annotation\" and My Ántonia, a 1918 Willa Cather novel ) is a solution developed within the Center for Digital Research in Humanities (CDRH) that allows editors to write annotations directly on letters in a browser and insert those annotations into Text Encoding Initiative (TEI) XML files. Multiple editors review annotations, track letters' annotation statuses, and generate a new TEI file incorporating the annotations, avoiding having to manually edit each file. Annotonia utilizes both pre-existing, customized open source software and new software developed for this project. This paper describes the difficulties faced, the workflow of Annotonia, and its prospects for future annotation work. The challengeThe Complete Letters of Willa Cather is a National Endowment for the Humanities-funded project to publish a digital, fully annotated edition of the letters of Willa Cather, a 20th century American novelist. The project includes student editorial assistants, staff, and faculty both at UNL and working remotely. Because of differences in locale, technical abilities, and Cather-related expertise, a solution was sought for drafting, revising, and encoding letter-specific annotations that would fit the skillsets of all collaborators.The WCA has two types of annotations: authority files and letter-specific information. Editors skilled with TEI XML curate and encode people, places, and works shared across the corpus into authority files. Letter-specific annotations are more challenging to manage as they are written by a wider group of scholars, many unfamiliar with encoding XML. Editors previously used a cumbersome process of pasting documents into Word files, sharing them for annotation, and laboriously copying received annotations back into the XML. This tedious process introduced errors and was deemed unsustainable for the large number of anticipated letter-specific annotations.With these difficulties in mind, WCA editors collaborated with CDRH developers to envision tools that would allow viewing the content of letters with all associated materials; writing letter-specific annotations that might include images, links, or other materials; and exporting finalized versions of these annotations as TEI XML. Editors also identified the need to view annotations that had already been written so information contained in them would not be repeated. Project requirements• Display letters via HTML with existing controlled vocabulary annotations for people, places, and works.• Provide an interface where editors can browse, edit, approve, and flag annotations.• Export annotations to their preservation format, TEI P5 XML.• Insert <ref> tags into the TEI XML corresponding to ID's in the annotation file above. Existing software reviewBefore building Annotonia, we looked at existing technologies and methods for annotating HTML. Webbased annotation is not a new concept, by any means. The value of annotating HTML has long been recognized for its collaborative strengths, allowing users to identify errors, review, comment on, and bookmark documents (W3C Digital Publishing Interest Group, 2014). The W3C currently has a Web Annotation Working Group dedicated to creating specifications for \"interoperable, sharable, distributed Web annotation architecture\" (W3C Web Annotation Working Group, 2017;Web Annotation Data Model, 2017).Most annotation software reviewed was not suitable for the project. Many existing solutions (such as Annotation Studio, Recogito2, and Editor's Notes) did not allow in place annotation, instead requiring one to upload documents to their system. This would strip out the existing annotations, making work more difficult for editors. Some (such as PundIT) were not fully open source. Others (Such as Hypothes.is) were developed for public input and would require stripping out many features for our needs. Other problems encountered included installation roadblocks, lack of maintenance, and poor documentation. Importantly, our requirement of exporting annotations and embedding them into TEI documents was not supported by any existing systems.With these considerations, programmers chose to work with Annotator and its back end complement, Annotator Store, because the software interacts through an API and uses XPaths for pinpointing annotations' locations (Open Knowledge Foundation, 2016a;2016b). Annotator proved to be easy to install and extend with community plugins, such as rich text editing, keyword tagging, and filtering. Annotator Store's use of Elasticsearch reinforced the decision to use it, as Elasticsearch is widely used and receives generous community development and support. Annotator deals only with the annotation part of the requirement and not categorizing or workflow, which had to be developed in-house. Annotonia: the solutionThe first step was simply to display the letters' TEI XML in a browser in a way that minimally rearranged the structure of the documents. TEI Boilerplate uses browser XSLT capabilities to create an HTML/TEI hybrid representation of TEI documents with small alterations for links, images, and other elements ( Walsh et al. 2013). Boilerplate was therefore useful for constructing HTML from documents, while allowing for TEI files to be added and removed quickly from the site structure and workflow. In order to be able to annotate the documents displayed using TEI Boilerplate, programmers embedded Annotator in the web page and modified it so it had suitable options for the WCA editors. These included stripping out some of the text editing capabilities and adding workflow-specific options. Annotator does not include an interface for searching, browsing, and editing all of the annotations from the Annotator Store. PHP pages were written to provide these capabilities. The last requirement for Annotonia was a conversion script that inserts annotation references into TEI documents while exporting an authority file containing the annotations. The conversion script takes a subset of XML files, queries the Annotator Store API, adds tags for annotations based on XPaths, and outputs a list of possibly incorrect insertions that require review. Use and extensionThe collection of new scripts and open source software comprising Annotonia has been able to handle the workflow requirements of the WCA's letter annotations, though there is room for improvement. The areas that require the most attention are the rendering of TEI in HTML and the scripted editing of TEI files. TEI Boilerplate necessarily alters the TEI in order to mimic the behavior of HTML links and images, which means that occasionally the location of an annotation in the HTML view is difficult to match up programmatically due to differing XPaths.A new alternative to Boilerplate, CETEIcean, which promises to preserve \"the full structure and information from your TEI data model,\" may be one possibility to address this problem (Cayless and Viglianti, 2016). Programmers would need to evaluate how easily it can be incorporated into the Annotonia workflow, as well as the ability to load the Annotator JavaScript libraries in the page.Some aspects of the Annotonia code base are tailored for use only with TEI Boilerplate display and WCA file naming conventions. These functions would have to be generalized to make Annotonia easier to integrate with other tools and projects. Meanwhile, the Annotator community continues to improve the software. A new version is forthcoming which includes modifications to enhance the experience of creating, saving, and updating the highlighting of HTML content. When this version of Annotator is released, it may require some reworking of existing customizations, but updating Annotonia to incorporate its new features will support a broader variety of projects. ReceptionThe WCA began using Annotonia in September 2016. Guidelines and instructional videos demonstrating Annotonia's functions have alleviated difficulties from the varying technical skills of editors. Although still in the beginning stages of annotating individual letters, the tool works well for collaborating on, drafting, and revising annotations. Editors have estimated that each annotation automatically handled saves around 5 minutes of time, so the potential time savings is several months of work. Through an iterative process, improvements have been added as users uncover inefficiencies and provide feedback. By 2018, several thousand annotations will be created with Annotonia and published with the complete letters on the WCA. Further applicationsA frequent difficulty for digital archival projects is efficiently proofreading and annotating texts. In the CDRH, these workflows on other projects resemble the WCA's old process of marking up Word documents, or, worse yet, printing out entire websites and annotating by hand. Designing solutions by combining open source software with well documented, configurable scripts and workflows has proven to be effective in providing flexibility to cover a variety of needs. As we apply this method to extending and generalizing Annotonia for other projects like the Walt Whitman Archive and The William F. Cody Archive, we will further refine deployment and documentation.The PHP and conversion components of Annotonia have been published (\"Annotonia Status\" and \"Annotonia Converter\", and the customized pieces of existing software will be published soon. Full publication of Annotonia will involve further documentation and testing. It is the hope of the Annotonia team that this tool will not only prove to be useful internally, but will provide inspiration to other text based editions seeking to automate annotation processes. ",
        "article_title": "Annotonia: Annotations from Browser to TEI",
        "authors": [
            {
                "given": "Greg",
                "family": "Tunink",
                "affiliation": [
                    {
                        "original_name": "University of Nebraska-Lincoln Libraries United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    },
                    {
                        "original_name": "University of Nebraska-Lincoln Libraries United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    },
                    {
                        "original_name": "University of Nebraska-Lincoln Libraries United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    }
                ]
            },
            {
                "given": "Karin",
                "family": "Dalziel",
                "affiliation": [
                    {
                        "original_name": "University of Nebraska-Lincoln Libraries United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    },
                    {
                        "original_name": "University of Nebraska-Lincoln Libraries United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    },
                    {
                        "original_name": "University of Nebraska-Lincoln Libraries United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    }
                ]
            },
            {
                "given": "Jessica",
                "family": "Dussault",
                "affiliation": [
                    {
                        "original_name": "University of Nebraska-Lincoln Libraries United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    },
                    {
                        "original_name": "University of Nebraska-Lincoln Libraries United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    },
                    {
                        "original_name": "University of Nebraska-Lincoln Libraries United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    }
                ]
            },
            {
                "given": "Emily",
                "family": "Rau",
                "affiliation": [
                    {
                        "original_name": "University of Nebraska-Lincoln Libraries United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    },
                    {
                        "original_name": "University of Nebraska-Lincoln Libraries United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    },
                    {
                        "original_name": "University of Nebraska-Lincoln Libraries United States of America",
                        "normalized_name": "University of Nebraska–Lincoln",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/043mer456",
                            "GRID": "grid.24434.35"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe history of color is full of instances of how and why certain colors come to be associated with certain concepts, ideas, politics, status and power. Sometimes the connotations occur arbitrarily, like in the instance when pink was assigned to baby girls, and blue started to be associated with baby boys at the turn of 19 th Century [Paoletti, 1987]. Sometimes though, color associations have very tangible reasons, such as in the case of Marian blue, reserved only for painting Virgin Mary over the centuries. The reason is found in the scarcity of the rock lapis lazuli -even more valuable than goldfrom which the blue pigments were extracted. Individual colors have convoluted and contested histories, since they have been attached to many symbols at any given time. John Gage, an art historian who has devoted 30 years of research on the topic of color, explains the conundrum of what he terms \"politics of color\" in a simple fashion: \"The same colors, or combinations of colors can, for example, be shown to have held quite antithetical connotations in different periods and cultures, and even at the same time and in the same place.\" [Gage, 1990].The purpose of the present study is to introduce a method for automatically extracting color distributions and main colors of paintings, as well as color schemes of people in paintings. By visualizing these over time for cross-referencing with historical data, this study will reveal changes in how particular colors were used in a given time period and culture. In this study, we will look at artworks to find out whether certain colors or tones are associated with a specific sex, and if these connotations change over time. To that end, we apply algorithmic tools to process very large datasets automatically, and information visualization tools to depict the findings. Related WorkToday, major cultural heritage collections are available online. Digitization and preservation of artworks is an important occupation of museums and cultural heritage institutions, as well as many Digital Humanities projects. Portions of of such digitized collections are made available to further computer vision research in order to scrutinize art historical questions. Such collections are usually enriched with meticulously tagged metadata describing the origins of each artwork. However, these datasets do not provide comprehensive gender annotations. For example, Rijksmuseum's arts database has a wide selection of categories with rich metadata that is primarily about the art objects themselves (see Table 1 - the quantity of meta information and context vary between different art samples), but without any reference to what these artworks hold [Mensink and Van Gemert, 2014]. Automatically determining whether a sitter of a portrait is female or male in a painting is not an easy task. Several publications have appeared in recent years with the aim of automatic gender recognition. The survey by Ng et al. described a variety of approaches to gender recognition in natural images [Ng et al., 2012]. Xiong and De la Torre (2013) proposed a practical and effective method for automatically detecting faces in natural or man-made images. Once the face is detected, a supervised classifier is used to determine whether it belongs to a male or female. This requires the ground truth annotation of a large number of face images, from which the automatic classifier learns the visual boundary between these two classes.There has been focused studies to address face recognition tasks on artistic images [Srinivasan et al., 2015]. For the purposes of face detection, mainstream algorithms perform sufficiently well on paintings that are of interest for this study. Automatic male/female classification is not perfect, it will occasionally get confused and produce an incorrect label. However, over thousands of images, a small number of individual errors will not prevent us from seeing the general patterns of color usage with males and females. MethodologyIn this study, the aim is to analyze the trends of clothing color in different periods, separately for males and females. For this purpose, we work on a database of paintings, for which the era (or date) is provided, and we seek to annotate each image with the gender of the depicted person, as well as a rough segmentation of the area of the clothing. The general workflow of the proposed approach is depicted in Figure 1.  DatabaseThe Rijksmuseum is a Dutch national museum dedicated to arts and history in Amsterdam. The Rijksmuseum database contains 112.039 high-resolution images with extended meta data [Mensink and Van Ge- mert, 2014]. However, as mentioned previously in Section 2, the Rijksmuseum database has neither gender nor clothing color information embedded into its metadata. We describe briefly how we determine the missing information. Gender ClassificationWe have performed classification of the perceived sex from the face images. This process is commonly called Gender classification in computer vision -not to be mixed with characteristics of masculinity, femininity or sex organs, but what is perceived solely from the face crops on the paintings.For this purpose we have prepared a test dataset of face images from Rijksmuseum paintings and three training datasets of face images: 10k US Adult Faces [Bainbridge et al., 2013], Labeled faces in the wild [Huang et al., 2007] and in an approach similar to Jia's work [Jia and Cristianini, 2015], we have gen-erated our own IMDB dataset. IMDB dataset images are collected using the Google Image search, using actor and actress names as queries. In total, 5600 male and 5300 female faces were downloaded.None of the datasets have gender annotations, and hence we have performed face detection and facial landmark extraction methods in [Xiong and De la Torre, 2013], first, then hand-clean face detection and landmark extraction results against false positives and validate gender information (for all 10k US Adult Faces dataset and LFW dataset we had to manually annotate each image, but also Google Image search results for IMDB dataset are not perfectly robust, hence the IMDB dataset also had to be verified). Then we have aligned the faces to a mean shape [Gower, 1975], and extract features that are resistant to illumi-nation effects [Ojala et al., 2002]. We then train a classifier using the sequential minimal optimization (SMO) method [Platt et al., 1998].The biggest challenge for evaluating gender recognition performance on the paintings is to make sure the ground-truth gender data are actually correct [Mathias et al., 2014]. From our experience, this demanding task requires a full view of the painting, rather than just the detected face. Results of some combinations of the datasets are given in Table 2. We could reach above 75% accuracy on paintings, just by using photographs of actors and actresses in the training of the system. Some of misclassification examples are given in figure 2. Table 2. Gender recognition performance on Rijksmuseum.All results are com-parable and best (by small margin) is acquired when only the IMDB dataset is used. Figure 2. Misclassified paintings Clothing color informationPortrait paintings that are completely focused on the sitter's face have still a lot of background noise that disrupt the color representation of the paintings (see Figure 3). Our hypothesis is that color representation, when focused on the clothing of the model, will still reflect the color scheme that is associated with the gender of the sitter. In order to extract the color information of an outfit we need to do a coarse segmentation of the clothing. We used the GrabCut approach [Rother et al., 2004]. In this method, a user defines an area of interest, as well as foreground and background seeds for the segmentation. In our study, background and foreground seeds are initialized based on the detected face landmarks. Figure 4 provides an initial visualization of the dominant color distributions for each era, for males and females. Concentric circles have thickness associated with the frequency of the color. Bright colors are relatively rare, as the paintings in our tagged collection are generally dark, with the occasional shaft of light illuminating part of the painting. But a very distinct pattern can be observed in that females wear lighter colors compared to males, and show higher variance over the years. Some painting examples are given in Figure 5.   ConclusionsEvery period and location has certain dominant color associations and symbolism. To investigate hundreds of thousands paintings in a single sweep requires automatic analysis tools. Our main objective in this work in progress is to perform an analysis on the usage of color for different genders along the centuries, and to develop tools for establishing semantic associations of colors for each particular period of study. With the increased popularity of open-art, this study can be extended significantly by introducing more databases alongside Rijksmuseum, for example, drawing on the Europeana collection [Doerr et al., 2010]",
        "article_title": "Tracing the Colors of Clothing in Paintings with Image Analysis",
        "authors": [
            {
                "given": "Cihan",
                "family": "Sarı",
                "affiliation": [
                    {
                        "original_name": "Bogazici University (BU), Turkey",
                        "normalized_name": null,
                        "country": "Turkey",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Albert",
                "family": "Salah",
                "affiliation": [
                    {
                        "original_name": "Bogazici University (BU), Turkey",
                        "normalized_name": null,
                        "country": "Turkey",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Alkim Almila Akdag ",
                "family": "Salah",
                "affiliation": [
                    {
                        "original_name": "Istanbul Sehir University",
                        "normalized_name": null,
                        "country": "Turkey",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionTexts not only carry factual, but also affective information, such as expressions of joy or grief. In the humanities, especially literary studies, emotion expression and elicitation (often in texts from prior language stages) have been focused on in many contributions (see, e.g., Carroll and Gibson (2011), Poppe (2012), Hillebrand (2011)).Similarly, in natural language processing (NLP), emotion analytics have developed into an active area of research ( Liu, 2015). Nevertheless, there is little previous work explicitly addressing emotion in historical language and the specific methodological problems this raises. Hamilton et al. (2016) as well as Cook and Stevenson (2010) presented methods for identifying amelioration and pejoration of words. Acerbi et al. (2013) and Bentley et al. (2014) demonstrated the potential of emotion analysis for the Digital Humanities (DH) by linking temporal emotion patterns in texts to major sociopolitical events and trends in the 20 th century.Our work goes beyond these studies in two ways: we claim to be more adequate as we combine these two approaches to analyze non-contemporary text based on time-specific lexical resources. We also claim to be more informative as we employ the Valence-Arousal-Dominance (VAD) model of emotion (Bradley and Lang, 1994) instead of simple polarity (positiveness/negativeness) alone. We have already shown the latter to be beneficial in DH applications ( Buechel et al., 2016a). We hope that our work will be a step towards a new set of tools especially beneficial for areas such as literary studies (e.g., in distant reading (Moretti, 2013)) or history of mind. MethodsThe VAD model of emotion assumes that affective states can be described relative to three emotional dimensions, i.e., Valence (corresponding to the concept of polarity, see above), Arousal (the degree of excitement or calmness) and Dominance (feeling in or out of control of a social situation). The VAD dimensions allow for a more fine-grained modeling than polarity alone, e.g., words like orgasm and relaxed have similar Valence but opposing Arousal values (Bradley and Lang, 1999). Formally, the VAD model constitutes a three-dimensional vector space illustrated by Figure 1 ( Buechel and Hahn, 2016).The association of words with a VAD score is captured in emotion lexicons. These can either be empirically determined by aggregating subjective judgments of several human subjects; or they can be semiautomatically constructed allowing for greater size but reducing accuracy on individual words. For the semi-automatic construction, the typical approach is to expand an existing lexicon (the seed lexicon) based on word similarity (see below). There are several competing expansion algorithms. Cook and Stevenson (2010) were the first to describe expansion algorithms for the induction of the emotion value of words for non-contemporary language stages by using word similarity values determined from historical corpora. Extending this approach, we compared several emotion induction algorithms, viz., those by Turney and Littman (2003), Hamilton et al. (2016), andBest- gen (2008). The former two were slightly modified to make them deal with numerical input values (for a more detailed description of these methods, see Buechel et al. (2016b) and Hamilton et al. (2016)).We used point-wise mutual information with singular value decomposition ( Levy et al, 2015; SVDPPMI) to measure word similarity, since it turned out to be superior for DH applications in previous work (Hell- rich and Hahn, 2016). We used the German ANGST lexicon ( Schmidtke et al., 2014) as seed. The individual algorithms were evaluated by comparing our induced historical lexicons against judgments of knowledgeable PhD students from the humanities. For this task, we asked them to make their assessments as if they were contemporary readers from the respective time period. The Turney-Littman algorithm performed best in this set-up and was thus employed for all subsequent analyses. ExperimentsFor demonstration purposes, we here apply our methodology to the core corpus of the Deutsches Tex- tarchiv (DTA;Geyken, 2013, TCF version from May 11, 2016 [German Text Archive], a well-curated and balanced collection of historical German texts. We analyzed texts created between 1690 and 1899, splitting the resulting corpus into seven slices (each spanning 30 years) to achieve similarly sized and sufficiently large subcorpora for further processing. We computed word similarities within each of these slices and then applied the Turney-Littman expansion algorithm, thus creating seven distinct emotion lexicons, each describing the emotion of words for its specific period. Given these temporally stratified lexicons, we claim that shifts in emotion association of words can be traced over time by comparing the emotion values a word exhibits in different lexicons. To validate this claim, we selected the words for which we could compute similarity scores in each time step (as these methods are more accurate for high-frequency words, rare words were excluded from our study) and standardized their VAD values for each lexicon and dimension (VAD).  We illustrate this approach with an example from Figure 2. It displays an overall amelioration of Sünde [sin] whose onset roughly coincides with the age of enlightenment-often understood as the starting point of secularization (Sheehan, 2003), although care must be taken when interpreting these word course graphs since noise can be introduced from various sources (such as word similarity and emotion induction algorithms); both strongly depend on the amount of data available for each time step. This observation is in line with well-known findings from descriptive lexicography (Grimm and Grimm, 1942). The same semantic shift can also be discovered by a more established method, namely collocation analysis. Table 1 reveals that Sünde, at the end of our examination period, has acquired an additional moral-bourgeois meaning facet (implied, e.g., by Ausschweifung [excess], Ärgernis [nuisance] and Laster [vice]) which was not present in the beginning. There, only the religious sense is traceable.Going one step further, we then used these lexicons to examine how emotion is distributed over literary texts in the DTA in the course of time. We employed the Jena Emotion Analysis System (JEMAS; Buechel and Hahn, 2016), an open-source tool for emotion measurement using a configurable VAD lexicon. We processed each DTA text with the period-aligned lexicon, • • • • • • • • • • • • • • • • • • • • • Valence Arousal Dominanceleading to the main methodological contribution of our work: linking the research areas of automatically inducing historical word emotion (e.g., Hamilton et al., 2016) and emotion prediction in historical text (e.g., Acerbi et al., 2013). We scaled the resulting emotion values within each VAD dimension tracing the development of the three principal literary forms-Narrative, Lyric, and Drama-in German literature between 1690 and 1899. For each of the seven 30-year periods (organized in rows), we created three scatterplots (one for each pair of the VAD dimensions; organized in columns) resulting in 21 plots in total (Figure 3). Each data point represents one text-color and shape represent membership to the respective form.It is evident from the plots how the distinction of the individual forms increases and decreases in emotional terms in the course of time. This application differs from typical stylometric approaches since we employ emotional features instead of word counts. We find the most distinct emotional patterns between 1780 to 1809 (approximately covering the Weimar Classicism) and between 1870 to 1899 (covering the late German Realism). Drama displays consistently more Arousal than Lyric and Narrative since 1750, whereas Lyric seems to be the most positive class (Valence) expressing the least control (Dominance). Of course, the examination of the DTA offers many more intriguing findings, however, for brevity, we limit ourselves here to presenting examples. ConclusionIn this contribution, we described a novel methodological framework for quantifying emotion in noncontemporary text. Applying this approach to a 210-years section of the German DTA corpus, we find clear emotional signals for temporally evolving distinctions between the principal literary forms, viz. Narrative, Lyric, and Drama. All resources and software we developed for this work are publicly available.  AcknowledgmentsThis research was partially conducted within the Graduate School \"The Romantic Model\" supported by grant GRK 2041/1 from the Deutsche Forschungsgemeinschaft (DFG).   ",
        "article_title": "The Course of Emotion in Three Centuries of German Text-A Methodological Framework",
        "authors": [
            {
                "given": "Sven",
                "family": "Buechel",
                "affiliation": [
                    {
                        "original_name": "Jena University Language & Information Engineering (JULIE Lab) - Friedrich-Schiller-Universität Jena",
                        "normalized_name": "Friedrich Schiller University Jena",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05qpz1x62",
                            "GRID": "grid.9613.d"
                        }
                    }
                ]
            },
            {
                "given": "Johannes",
                "family": "Hellrich",
                "affiliation": [
                    {
                        "original_name": "Jena University Language & Information Engineering (JULIE Lab) - Friedrich-Schiller-Universität Jena",
                        "normalized_name": "Friedrich Schiller University Jena",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05qpz1x62",
                            "GRID": "grid.9613.d"
                        }
                    }
                ]
            },
            {
                "given": "Udo",
                "family": "Hahn",
                "affiliation": [
                    {
                        "original_name": "Jena University Language & Information Engineering (JULIE Lab) - Friedrich-Schiller-Universität Jena",
                        "normalized_name": "Friedrich Schiller University Jena",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/05qpz1x62",
                            "GRID": "grid.9613.d"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis paper will take my ongoing doctoral research designing resources for analysing individual graphic novels as a case study and starting point to discuss how we can produce data in the Humanities which can be called 'feminist'. It will engage with the significant debates on this topic which are emerging in the Digital Humanities (Clement, 2016)  (Drucker, 2012) (Losh et al., 2016) (Posner, 2016)  (Rhody, 2016), particularly the panel discussion on creating feminist infrastructure at DH 2016 ( Brown et al., 2016). Marking up images and designing ways in which to make them communicate with a comic's words are highly subjective enterprises. By explaining how I dealt with this issue in my research, this paper will outline how the subjectivity involved in creating our own data structures and ontologies, and these elements' inherent statuses as arguments about data, is a strength, an affordance, of such an approach, as well as something that embraces the situatedness and plurality of data that feminists in the sciences (Haraway, 1988)  (Irigaray, 1985) have advocated for and which, more recently, have been advocated for in the Digital Humanities. Although my case study focuses on contemporary comics, this paper will explain how some of the resulting principles can be employed by data creators today in other disciplines and the GLAM sector. Background and methodAs a Digital Humanist operating outside of a specific department or centre, the question that I hear most often about my work is how I make the resources which I create objective, how I avoid my datasets merely reflecting one individual's interpretation of a text. But when I mark up an image in a certain way, or use a database structure to reflect the rhizomatic structure of a graphic novel, I do so not because it is 'appropriate' or a 'good fit', but because that is an argument I want to make about comics and their meaning mechanism, and by applying that algorithm to the dataset that is the comic, I articulate that argument, mobilising it and making it available for evaluation. I am not trying to enact as little violence as possible to a text; I am making an argument about it. This idea that datasets, data structures and algorithms are arguments that are made about texts or other objects of study is relatively well-established in Digital Humanities ( Ramsay and Rockwell, 2012) but it is more often framed as a caution than an opportunity.Working on contemporary comics, where there is no pre-existing database, and no automated or straightforward tagging of images, it would be easy to see mark-up or tagging as a hurdle, and a problematic one at that, given the fraught nature of remediating pictorial information into values that can be entered into a database. But, although I must design my own data tags and ontology, I do not have a mandate to preserve, gatekeep, or distribute otherwise inaccessible data since my objects of study are widely available. Focussing on individual texts, too, affords me time to spend designing tailored data tags and ontologies. I do not need to preserve, that is my freedom; I cannot be 'objective', that is my strength. My objects of study are relatively small; that means I do not have to be singular, I can be multiple.Digital approaches, especially to comics (Walsh, 2012) ( Dunst et al., 2016), often rely on a single categorisation of each entity or attribute. This paper will argue, rather, that our databases ought to be multiple. Rather than text-mining, a metaphor which suggests the removal of gratuitous material, I would encourage thinking of this practice as data curation, or rather, curations. Consider the analogy of a virtual museum with access to a complete catalogue of material -for is this not like our complete texts? -where anybody can hang the material in whatever way, in whatever ways, they choose. Different paths through the information can be curated, different logics created, retaining the plurality of signification that each piece holds, resisting positivism. This may well tell us as much about our hypothetical hangers as about the hung objects, but therein lies an opportunity. If curation - like datasets, data structures and algorithms -is argument, then why not bring multiple perspectives into conversation? And if we can represent data multiplicitously, we can do it investigatively. By creating multiple ontologies and data tags it is possible to embrace Brian Massumi's judgement, \" [t]he question is not: is it true? But: does it work? What new thoughts does it make possible to think?\" (Deleuze and Guattari, 1987: xv) Since datasets are arguments, marking up the same data differently allows researchers to avoid asserting single values for complex, or simply ambiguous, pieces of information. It also liberates us to encode arguments we disagree with, or at least that we concede are problematic, in order to better understand or critique them. The reduction of gender to a binary, for instance, has been highlighted as an issue in quantitative approaches (Clement, 2016). This, of course, drags up familiar tensions between anti-essentialism and feminism, but if we can encode different modes of representing gender - or any other 'attribute' which is better represented on a spectrumincluding the reductive binary mode, we can maintain the plurality of our data, whilst retaining the possibility to see how the text subverts such a binary categorisation; we bring the text to bear on the theory and in so doing, better understand the theoretical position of the text. Image tagging can operate in a similar way, by tagging the same pictorial signifier in multiple ways we can tag with an intention to investigate, not merely minimise violence to the text. By contrasting and combining different ontologies it is possible to shed light on our texts and to allow our texts to shed light on our ontologies, all the while fracturing any notion of computational methods as objective black boxes by foregrounding their artificiality. ConclusionEmbracing a conceptualisation of Humanities data as complex and plural, this paper will use examples from my own research remediating graphic novels into databases to demonstrate how deploying multiple tags and multiple ontologies not only instantiates a more feminist approach to data but is actually a productive methodology for analysing texts. It champions not the analysis of datasets, but rather an analysis by datasets. As Laura Mandell said in Krakow, we need \"metadata built for thinking, not sustainability.\" (Brown et al., 2016)   ",
        "article_title": "Towards Feminist Data Production: A Case Study From Comics",
        "authors": [
            {
                "given": "Alexander",
                "family": "Turton",
                "affiliation": [
                    {
                        "original_name": "University of East Anglia",
                        "normalized_name": "University of East Anglia",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/026k5mg93",
                            "GRID": "grid.8273.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionSince is it widely accepted that authors tend to have their own unique writing style, and since it is also widely accepted that authors are not able to disguise their overall writing style, it is reasonable to ask if novelists can create different voices for the characters in their books. The appropriate null and alternative hypotheses for this research question are:• Null Hypothesis: A novelist's characters do not have distinctive voices.• Alternative Hypothesis: At least some of a novelist's characters have distinctive voices.Tim Hiatt andJohn Hilton (1990 and1993) considered the question of character voice for William Faulkner, James Joyce, Mark Twain and Robert Heinlein and concluded that although an author could create character voices, of the author they tested Faulkner alone was uniquely able to create characters with differing voices. However, we noted that their analyses were simplistic and did not use the multivariate statistical techniques commonly used current in stylometric analysis. Therefore, we chose to test the null hypothesis of non-distinctive character voices based on noncontextual word frequencies and using principle components analysis (PCA). MethodWe applied PCA to novels written by Jane Austen, Charles Dickens, James Fennimore Cooper and Mark Twain (Samuel Clements). We selected two novels from each author and separated the words quoted by each character. We only included characters whose quoted words exceeded 500 words. Austen created sixteen characters in Pride and Prejudice and fourteen characters in Sense and Sensibility who met the minimum number of quoted words, while Dickens created twenty-three characters in Oliver Twist and fourteen in Great Expectations. Similarly, Cooper created twelve characters in The Last of the Mohicans and ten in The Deerslayer, and Twain created nine characters The Adventures of Tom Sawyer and fourteen in The Adventures of Huckleberry Finn. We then split each character's quoted words into 200 word blocks for analysis. Since each book also had a narrator, we also split the narrator's words into 2000-word blocks. Figure 1 shows plots of the first two principle components for the four authors where each star dot represents a 2000-word block for the narrator and each circle dot represents a 200-word block for each character. Each novel's narrator clearly stands out from the characters in the book. It can also be seen in Figure  1 that the narrator's voices are similar for Austen's, Dickens', and Cooper's novels, while the narrator voices in Twain's novels are distinguishable. Results and DiscussionAlthough we do not show the plots here, a similar analysis as in Figure 1 comparing the author's own wordprint voice based on non-contextual word frequencies in the author's non-novel writing to the narrator's voice in each novel showed that the narrators' wordprints did not match the author's own wordprint for all four novelists Since the narrators for each author are obviously different in function word frequencies than the characters in each book, we only used the non-narrator words to compare the distinctiveness of the voices each author created for his or her characters. Again using PCA we applied multivariate analysis of variance (MANOVA) and constructed 95% confidence ellipsoids around the characters in each author's books. Figure 2 shows the ellipsoids using the first two principle components. Each dot in the plots represents one character. Each ellipsoid shows the diversity of character voices for one novel. It can be seen in Figure 2 that the character voices in one novel are distinguishable from the character voices in the other novel for each author, even though there is some overlap of the ellipsoids.The volumes of the ellipsoids for each author provide a measure of that author's character diversitythe larger the volume, the greater the diversity of character voices within a novel. Figure 3 shows a comparison across authors of the total volume of a 95% confidence ellipsoid encompassing both of each author's novels. author compared across all four authors. The log 10 is shown when the ellipsoid includes from two to fifteen principle components. For eight principle components and above, the diversity of character voices for Dickens is clearly greater than for the other three authors.It can be seen in Figure 3 that Dickens and Twain show greater character diversity than Austen and Cooper. For example, Dickens' character diversity (measured by ellipsoid volume with fifteen principle components) is about one hundred times bigger than Austen's. This shows that even among great authors, there is a spectrum of ability in creating character voices. ConclusionsWe examined the diversity of character voices created by four of the most respected nineteenth century novelists as shown in their characters' functional word frequencies. Using stylometric analyses we considered whether or not an author can create characters with different wordprint voices and found persuasive evidence that they can. We also found that the narrator in each novel speaks with a different voice than the characters and a different voice than the author's own nonnovel voice. Further, not only can a talented author create characters with differing voices among characters within a novel, the characters are different among novels. In addition, we found that authors exhibit varying ability to create character voice diversity. Although all four were great novelists and could created distinctive characters, Dickens' and Twain's characters showed even greater voice diversity than Austen and Cooper.",
        "article_title": "Characters in 19th Century Novels Display Distinctive Voices as Seen by Stylometric Analysis",
        "authors": [
            {
                "given": "Paul",
                "family": "Fields",
                "affiliation": [
                    {
                        "original_name": "Brigham Young University",
                        "normalized_name": "Brigham Young University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047rhhm47",
                            "GRID": "grid.253294.b"
                        }
                    }
                ]
            },
            {
                "given": "Larry",
                "family": "Bassist",
                "affiliation": [
                    {
                        "original_name": "Brigham Young University",
                        "normalized_name": "Brigham Young University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047rhhm47",
                            "GRID": "grid.253294.b"
                        }
                    }
                ]
            },
            {
                "given": "Matt",
                "family": "Roper",
                "affiliation": [
                    {
                        "original_name": "Brigham Young University",
                        "normalized_name": "Brigham Young University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047rhhm47",
                            "GRID": "grid.253294.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " SummaryThis paper describes a new method for dimensionality reduction, \"stable random projection,\" (hereafter \"SRP\") distinctly suited for large textual corpora like those used in the digital humanities. The method is computationally efficient and easily parallelizable; scales to the largest digital libraries; and creates a standard dimensionality reduction space for all texts so that corpora and models can be easily exchanged. The resulting space makes a wide variety of applications suitable to bag-of-words data, such as nearest neighbor searches, classification, and semantic querying possible with data sets an order of magnitude smaller in size than traditional feature counts.SRP is a minimal, universal dimensionality reduction with two distinctive features:1. It makes no distinction between in- and out-ofdomain vocabularies. In particular, unlike standard dimensionality reduction it creates a single space that can hold documents of any language.2. It is trivially parallelizable, both on a local machine and through web-based architectures because it relies only on code that can be easily transferred across servers, rather than requiring large matrices or model parameters.These two features allow dimensionality reduction to be conceived of as a piece of infrastructure for digital humanities work, rather than just an ad-hoc convention used in a particular project. This method is particularly useful for provisioners and users of text data on extremely large and/or multilingual corpora. This creates a number of new applications for dimensionality reduction, both in scale and in type. SRP features could usefully be distributed by libraries as a (much smaller and easier to work with) supplement to feature counts. After a description of the method, some novel uses for dimensionality reduction on such libraries are shown using a sharable dataset of approximately 4,500,000 books projected into SRP-space from the Hathi Trust. Description of the methodThe goal of SRP is to reduce of text of uncertain length to a much smaller fixed- length vector to which the many tools of textual analysis, machine learning, and linear algebra can be applied. The core technique used here for dimensionality reduction is random projection. Random matrix theory has emerged in the past few decades as an useful alternative to more computationally complex forms of dimensionality reduction.(Halko, Martinsson, and Tropp 2009) I make use here of the observation that it is possible to project into a space where points as determined purely by sampling randomly from the set [-1,1].(Achlioptas 2003) A true random number generator is not suitable for reproduction. The other core element of SRP, therefore, is a quasi-random projection for every individual word created using cryptographic hashes (specifically, SHA-1).This allows the method to be defined algorithmically, making it easy to apply to any text. I have written short code libraries to implement the transformation in the three most important language for DH tool development: Python, R, and Javascript. These include a few necessary additional conventions such as minimal tokenization rules, a method for expanding beyond the 160 dimensions provided by SHA, and the byte-encoding of the Unicode character sets. Comparison to existing methodsThe gold standard for dimensionality reduction are techniques that make use of co-occurrences in the term-document matrix such as latent semantic indexing and independent components analysis. More recent techniques such as semantic hashing can be even faster and more efficient at optimally organizing documents in various types of vector spaces designed especially for particular documents.(Salakhutdinov and Hinton 2009) Another strategy finding recent use in the digital humanities is using an LDA topic model as dimensionality reduction, which produces neatly interpretable dimensions for analysis (Schöch, 2016;Fitzgerald,2016). In both the digital humanities and computer science, scholars frequently use \"top-N\" words as a good enough approximation of the textual footprint, limiting the dimensions to a few hundred of the most common words in the corpus, producing what Maciej Eder has called \"endless discussions of how many frequent words or n-grams should be taken into account\" for stylometry.( Underwood 2014, Eder (2015) These methods suffer two problems that make them problematic as a general-use feature reduction. First, the better ones are computationally complex, and quite difficult to perform on a very large corpus. Second, it is difficult or impossible to project out-of-domain documents into the space from a standard projection if they contain vocabulary different than the training corpus. This out-of-domain problem presents a particularly great problem for multilingual corpora, because texts that are missing or in sparsely-represented languages will behave erratically in the new environment.Some other work in the digital humanities and computer science has used hashes, random projection, and other similar methods as an ad-hoc rather than infrastructural technique. SRP can be thought of as a particular species of locality-sensitive hashing, another version of which has been used by Douglas Duhaime to identify reuse in poetic texts based on three-letter phrases.(Duhaime 2016). Also related is the \"hashing trick\" in computer science( Weinberger et al. 2009), which is better than SRP in many ways for the short documents computer scientists frequently study, but takes significantly more memory to store for booklength documents (an edge case in the computer science literature, but among the most important for humanists). ApplicationsThis reduced space can be put to many of the same uses as a standard bag-of- words model in considerably less space and with the potential for building web facing tools. Among those to be described are:1. Duplicate detection. SRP is quite accurate at identifying duplicate books in a computationally tractable space using cosine similarity, both inside a corpus and across disparate corpora. 2. Similarity Search. A prototype web page allows any user to paste in any text; it will hashed on the client side into the standard space, and a server can return in a few seconds the most similar documents. The top entries can function for duplicate detection; the lower ones presenting interesting opportunities for exploratory analysis. A search for Huckleberry Finn, for example, finds a large number of other American adventure novels about boys in the American west. Classification• SRP features perform approximately as well as top-n words (~77%) on a pre-existing task described by Ted Underwood, separating high- from low-prestige poetry.(Underwood 2015)• A single hidden layer neural network trained with 640-dimensional SRP features can accurately classify a held-out sample of books into one of 225 Library of Congress Classification subclasses (for example, whether a work is PR: British Literature or PS: American Literature) with ~78% accuracy based on about 1 million training examples. A single classifier works in multiple languages simultaneously; its determinations on arbitrary pasted text are accessible for inspection through a web site.• A different single hidden layer neural network trained with SRP features and a novel encoding scheme for years using Google's TensorFlow framework can accurately predict the years for withheld books with a median error of four years from the true publication date. SRP as AccessSRP fits in the DH2017's theme of \"Access\" in two ways.First, it makes many forms of text analysis on huge digital libraries far more feasible for scholars without access to high performance computing resources. On large corpora, data storage and dimensionality reduction can be more resource-intensive than the actual analysis. The dimensionality-reduced dataset for the full Hathi Trust corpus can fit into 10 GB, easily storable on most computers; subsets are suitable for use in classroom or workshop settings.Second, the ease with which it works with distributed web architectures, and its language agnosticism, can create new routes into neglected portions of large archives, particularly those with insufficient metadata.  ",
        "article_title": "Stable Random Projection: Standardized universal dimensionality reduction for library-scale data",
        "authors": [
            {
                "given": "Benjamin",
                "family": "Schmidt",
                "affiliation": [
                    {
                        "original_name": "Northeastern University",
                        "normalized_name": "Universidad del Noreste",
                        "country": "Mexico",
                        "identifiers": {
                            "ror": "https://ror.org/02ahky613",
                            "GRID": "grid.441462.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionWhen access as a value of scholarship is foregrounded in publishing, libraries emerge as \"natural and efficient loci for scholarly publication\" (Courant and Jones, 2015). In a rapidly evolving digital publishing landscape, academic libraries are poised to address scholars' publishing concerns about gaining access to opportunities for support and re-skilling, providing open access to their intellectual content, and ensuring access to the audiences who will most benefit from their work. The growth of library-based publishing services is evidenced by the 115 college and university libraries currently listed in the Library Publishing Directory (Lippincott, 2016). This paper presents selected results from a US-based survey on the needs of humanities scholars in a contemporary publishing environment, emphasizing aspects of the survey responses that shed light on the question of access in publishing from three perspectives: access to support services, access to content, and access to audience. About PWWPublishing Without Walls (PWW) is a Mellonfunded initiative at the University of Illinois led by the University Library in partnership with the School of Information Sciences, the department of African American Studies, and the Illinois Program for Research in the Humanities. Our project is developing an innovative and experimental library-based digital scholarly publishing model that aims to be accessible, scalable, and sustainable. Our objective is to develop a model for library-based publishing services that can be adopted broadly by other academic libraries to address scholars' emerging needs in a contemporary publishing environment. The model itself places humanities scholar at the center of the ecosystem, with services informed by-and responsive toscholars' needs. Research and development within the project are strategically designed to address known gaps within the current landscape: the gap between what and how scholars want to publish and what existing systems of print publishing can accommodate; the gap between the everyday practices of humanities scholars and what high-level tools exist for producing and supporting digital scholarship; and the gap between digital scholarship and publishing opportunities at resource-rich institutions and Historically Black Colleges and Universities (HBCUs). Survey MethodThis paper presents the selected results of a largescale survey about scholars' publishing practices and perceived needs. The full survey aims to understand what and how scholars want to publish, when and why they choose to publish digitally, and how they perceive the success of their digital publications. Survey outcomes will directly inform the development of PWW's shareable service model, but we also anticipate that our survey results will be relevant to digital humanists and other scholars engaged in digital scholarly publishing, whether such efforts are located within or beyond an academic library.From June to October 2016, we conducted a largescale survey of scholars, especially targeted at humanities scholars and scholars at HBCUs in the United States. The survey was developed by the PWW Research Team in spring 2016, and comprised around 30 questions. The survey covers six broad themes: respondents' experiences with print and digital publishing; respondents' publishing objectives; publishing tools and platforms; publishing services and support; publishing from the scholars' perspective of reader as opposed to author; and general attitudes toward print and digital publishing. The survey was distributed through listservs and social media venues targeting scholars in the humanities generally as well as selected niche communities to encourage sufficient responses across disciplines and institutions. The survey received 250 responses.The team used the Qualtrics platform to present the survey and conduct initial analysis, with further quantitative data analysis in SPSS. Preliminary findings have been reported previously ( Fenlon et al., 2016), and analysis is ongoing. The survey instrument and a summary report will also be archived in the IDEALS repository ( Velez et al., in preparation). ResultsRespondents were asked to identify which aspects of publishing posed the most significant challenges with respect to their experiences with print and digital content. Figure 1 represents the percentage of respondents who indicated each potential issue as either \"quite challenging\" or \"extremely challenging\" in print or digital publishing. The top three challenges for digital publishing include getting adequate technical, editorial, and financial support for publication. Respondents were also asked to indicate and rank their top five publishing goals, which are illustrated in Figure 2 as a weighted bar graph where a first-place ranking is assigned 10 points, a second-place ranking is assigned 5 points, and a third-place ranking is assigned 1 point. The top three goals are consistent with the traditional expectations for scholarly publishing: contributing new information to one's field, encouraging and participating in dialogue about an area of study, and establishing a formal record of one's scholarship.  Figure 3 illustrates the top three audiences that scholars indicated they most wish to reach. The top two audiences relate to peers within the academic community. While interest in reaching the general educated reader and students is less frequently cited, it is sufficiently robust to consider how reaching these audiences may have an impact on scholars' decision making with regard to the medium they choose and the venues they seek for publication. Understanding how less traditional, but still prevalent, publishing goals affect these choices is also a potentially fruitful avenue for exploring how explicitly stated publishing objectives inform, and possibly shift, priorities regarding representation and dissemination within scholarly publishing. These themes are explored in a set of four charts in Figure 4.   . Comparing Scholars Selection Criteria for Publishing Medium and Venue in Relation to Target Audience and Publishing Goals DiscussionWhen comparing scholars' characterization of challenges in digital versus print publishing, speed to publication and reaching one's intended audience emerge as the two greatest challenges to print publication, but they are perceived as relatively less challenging in digital formats. This difference between print and digital suggests that these \"challenges\" might be considered the primary \"affordances\" that scholars perceive for digital publishing. For digital publishing, the top three challenges that scholars face all relate to receiving adequate support for the logistical aspects of the process, including technical, editorial, and financial support. Though not one of the top three challenges, another aspect of publication that the survey responses suggest is more challenging in digital than print publication is manuscript preparation. Despite the fact that the most prevalent challenges to digital publishing relate to issues of support, the support that scholars will receive from publishers never emerges as a major factor in a scholar's choice of publishing medium or venue, regardless of their specific publication goals and intended audiences.The top three considerations with respect to choosing both one's medium and the venue in which to publish are the ability to effectively represent the scholarship, the ability to reach one's target audience, and the reputation or prestige of the venue or medium. The weight of these and remaining factors, however, shifts when analyzed in conjunction with scholar's goals and target audiences, suggesting opportunities for developing more nuanced consultative support services when selecting tools and platforms in light of scholars' goals and intended audience. For digital publishing, the first two considerations (representation and audience) are likely to be the determining factor in a scholar's decision to shift away from traditional print publishing and to consider library-based digital publishing opportunities. The emphasis on reputation and prestige, however, may prove problematic for fledgling initiatives that seek to develop alternatives to the established publication models of university presses. Further research will investigate what constitutes acceptable markers of prestige to determine the importance of affiliation with an institution of higher education, which libraries already have, or affiliation with known university presses, which most libraries do not have. ConclusionCompared to other publishing models, situating support for scholarly communication in the research library creates possibilities for addressing challenges related to access and sustainability of digital scholarly publishing. This support can be performed efficiently as a part of library activities, leveraging pre-existing technical infrastructure that is designed to support discovery and preservation as well as digital scholarships programs within scholars' commons. These aspects of library-based publishing prove especially compelling in light of survey findings that the biggest challenges for digital publishing include securing adequate technical support services, in addition to financial and editorial support.The Publishing Without Walls initiative is seeking to offer attractive solutions for authors 1) whose scholarship is not sufficiently represented in the print medium and 2) who place a high value on the technological affordances provided by open access digital scholarship to reach their intended audiences. We further anticipate that developing value-added support services in the form of individual consultations and incubation workshops will help ease the support-related challenges cited by scholars, particularly when assessing which platforms and tools will best represent an author's scholarship.",
        "article_title": "Informing Library-Based Digital Publishing: A Survey of Scholars' Needs in a Contemporary Publishing Environment",
        "authors": [
            {
                "given": "Megan",
                "family": "Senseney",
                "affiliation": [
                    {
                        "original_name": "University of Illinois",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            },
            {
                "given": "Latesha",
                "family": "Velez",
                "affiliation": [
                    {
                        "original_name": "University of Illinois",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            },
            {
                "given": "Christopher",
                "family": "Maden",
                "affiliation": [
                    {
                        "original_name": "University of Illinois",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            },
            {
                "given": "Janet",
                "family": "Swatscheno",
                "affiliation": [
                    {
                        "original_name": "University of Illinois",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            },
            {
                "given": "Maria",
                "family": "Bonn",
                "affiliation": [
                    {
                        "original_name": "University of Illinois",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            },
            {
                "given": "Harriett",
                "family": "Green",
                "affiliation": [
                    {
                        "original_name": "University of Illinois",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            },
            {
                "given": "Katrina",
                "family": "Fenlon",
                "affiliation": [
                    {
                        "original_name": "University of Illinois",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis paper will argue that historical linked data should not be used solely as means of storing facts but also as a means of improving historiographical discourse by formalising rhetoric, clarifying premises and evidence, and allowing for a distant reading of historical interpretations.The need for such formalizations can be seen in the following interaction between two historians. An argument is made by one historian with a selection of evidence to support it. Another historian responds with a counter argument and counter evidence. The first becomes frustrated; the second did not understand their argument and their new evidence is irrelevant. The second is equally frustrated as the first has not properly addressed their concerns and instead glosses over them to return to his or her original argument. To the outside observer the conflict appears intractable; neither side is willing to concede the other's points and modify their interpretation of the event or process. This is not a case of academic stubbornness; it is historiographical switchtracking.Switchtracking (Stone and Heen, 2015) is the result of two similar but non-identical conversations taking place at the same time. In the above, both historians are discussing a single historical problem but have interpreted that historical problem in slightly different ways, responding to each other with own their interpretation of the question in mind. By leaving their specific goals implicit or relying upon ambiguous terminology they have allowed their arguments to be easily misconstrued or misidentified (Godden, 2013). For example, the historical problem \"What caused the Salem Witch Trials?\" might not only lead to different interpretations-community conflicts, economic disparity, rye-ergot poisoning, religious fanaticism-but also different incarnations of the question itself:• \"What were the causes of the Putnam accusation?\" or• \"Why did the Salem Witch Trials begin with the Putnam accusation?\" or• \"Why did the Putnam accusation escalate into a wider hysteria?\" The different interpretations that arise from these similar but non-identical questions can lead to historians speaking at cross-purposes and unnecessarily hinder our wider understanding of historical events. The simplest solution is to better define the premises and hypotheses of a given study. However, the challenge of providing a defined, testable hypothesis, combined with the semi-narrative writing style preferred by historians, often precludes this level of clarity. Limited by a perpetually incomplete evidence-base, fuzziness is assumed and allows if not promotes poorly aligned debates.Linked data may provide a two-fold solution to this problem. Traditionally, clarity and reproducibility in historical research has relied upon three methodological pillars: the quotation, the citation, and the acceptance of interpretive interoperability. The first two work in tandem, providing clear links to or examples of the precise evidence used. Limitations of publishing space have previously reduced the comprehensiveness of these pillars but now the ability to provide targeted hyperlinks and sustainable datasets online allows for a much greater degree of precision. However, the use of page and line numbers, hyperlinks, DOIs, and other edition indicators are inconsistent across the history publications. Likewise, citation standards and allusion conventions differ between publications, subfields, and other communities of practice. It would be difficult, and arguably undesirable, to suggest homogeneity. Without this, however, it is impossible to prevent switchtracking and the misinterpretation as to which precise evidence is being used for which purpose.The integration of a linked data layer, a meta-document attached to a piece of historical writing, could serve as a remedy to this problem. Bringing together existing ontologies for describing geographical, biographical, and chronological data as well as digital or digitised documents provides a straightforward means for creating strong, definite links between historiography and the data that underpins it. Because such a layer could vary in depth of detail, it could begin with the basic citation information expected of traditional footnoting, but flexibly add layers of detail that would be infeasible in traditional journal or monograph typesetting.Linked data is already in common usage in certain historiographical and heritage circles, usually in the publication of discrete datasets or in cataloguing digital, digitised and traditional archive collections (Meroño-Peñuela et al., 2013). However, their integration with specific piece of historiographical writing is more complex. At their most basic level, they differ little from traditional citation practises and the added value of precision may not fully compensate for the additional effort in producing this metadata layer. Instead, it is the interpretation of that evidence, and the analytical linkages made by historians, that provide the most significant opportunity for developing historical research. Within the historical and heritage community, linked data is often considered to be limited to 'fact-based' information and cannot convey or represent the analytical frameworks that the narrative provides. Indeed, as of writing, there appears to be only one complete ontology for expressing rhetorical logic, which is poorly maintained with no clear evidence of it being employed in academic debate (Dumontier, 2014). Moreover, it focused upon highly structured logical expression; historical research often relies upon highly fragmented data, requiring vary degrees of informed speculation, which, when undocumented, is the primary cause of switchtracking.Creating an ontology that can provide definite relationships between evidence, premises, correlations, causations, formal logical deductions and speculative interpretations would allow historians to maintain the semi-narrative writing style expected of historical research but add a layer of unequivocal-if less poeticstatements that provide unambiguous statements of their argument and its components. Beyond serving as a mechanism for researchers to refine their argumentation, presenting a complex historiographical interpretation as a collection of interconnected relationships-literal and rhetorical-would allow for a computational comparison of arguments; similar conclusions could have their evidences combined whereas contradictory interpretations would have a clear set of evidences and premises from which to begin an investigation of divergent views. This paper will therefore discuss the issues surrounding the creation of an ontology that combines well established ontologies regarding historical evidences and document provenance alongside rhetorical relationships between premises and conclusions. It will demonstrate how one might create a graphical representation of a narrative text and vice versa. The paper will be presented in both semi-narrative long form and RDF triples.   ",
        "article_title": "Binary Truths: Developing a Linked Data Model for Historiographical Arguments",
        "authors": [
            {
                "given": "M",
                "family": "Beals",
                "affiliation": [
                    {
                        "original_name": "Loughborough University",
                        "normalized_name": "Loughborough University",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/04vg4w365",
                            "GRID": "grid.6571.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn recent years, progress on information and communication technology (ICT) has affected the management of cultural heritage. While ICT plays a key role in the accessibility and informed experiences of the public, it is also becoming more apparent in creating participatory platforms for people who manage and enjoy cultural heritage ( Lekakis and Chrysanthi, 2011). Thus cultural memory institutions use this technology for creating digital content about their collections and making this content available from all over the world. Increasing the value of collections using sophisticated and innovative new media also affects the economic development of countries and provides more integrated awareness about cultural identity and crosscultural communication (Brizard, Derde, Silberman, 2007).ICT has provided a wide range of tools for cultural heritage management. Implementing these new tools also provides many advantages for both memory institutions and users. Memory institutions can manage their collections more easily with support for image processing, advanced publishing systems, and etc. Furthermore, while open access to public involvement in cultural heritage objects improves the public awareness and sense of belonging to the society, it also develops the content of the objects with crowdsourcing from all over the world. In order to benefit from these advantages of new media tools, cultural memory institutions have opened their collections to the global public regardless of geographic location. Thus, memory institutions also benefit from increasing their promotion and advertising among other countries and institutions (Myat, 2012).One of these new media platforms is Google Cultural Institute, which has been applied to the cultural heritage area in recent years, especially in museums.The purpose of this institution is to provide broad public awareness, use and augmented access online for cultural heritage objects. It offers a range of tools that make it easy for memory institutes to put the collections online. With the online exhibitions, memory institutions can create stories about their collections. People can reach these collections all around the world using a wide range of platforms, and can share the images they like with friends using their social media accounts (Google Cultural Institute, 2016). In his 2016 TED talk, the head of Google Cultural Institute, Amit Sood says \"The world is filled with incredible objects and rich cultural heritage. And when we get access to them, we are blown away, we fall in love. But most of the time, the world's population is living without real access to arts and culture.\" This new platform allows users to experience the world's cultural heritage objects, and to add comments about them as well. MethodThere are ten museums in Turkey putting their collections on Google Cultural Institute. All of these ten museums are private. They are as follows: In this study, the data about visibility and access of these ten museums is analyzed looking at the previous and next data of the collections that are added to this online platform. For detailed analysis of this issue, interviews were conducted with curators and managers",
        "article_title": "How does Google Cultural Institute Affect the Collections of Museums? The Case of Turkey",
        "authors": [
            {
                "given": "Sümeyye",
                "family": "Akça",
                "affiliation": [
                    {
                        "original_name": "Hacettepe University",
                        "normalized_name": "Hacettepe University",
                        "country": "Turkey",
                        "identifiers": {
                            "ror": "https://ror.org/04kwvgz42",
                            "GRID": "grid.14442.37"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionOptical character recognition (OCR) is the process of extracting text from images. The final results are machine readable versions of the original images. Nowadays every modern scanner comes with some kind of OCR, but the results may not be satisfying when the OCR is applied to historical texts, that 1. do not use standard fonts, 2. are not printed by a machine, 3. have varying paper and font quality.Furthermore, historical texts are not passed down through the centuries in their entirety but rather contain lacunae and fragmentary words. This makes automatic post-correction more difficult on historical texts than on modern ones.We used two tools to create language-and even document- specific recognition patterns (or so-called models) to recognize printed Coptic texts. Coptic is the last stage of the pre-Arabic, indigenous Egyptian language. It was used to create a rich and unique body of literature: monastic, \"Gnostic,\" Manichaean, magical and medical texts, hagiographies, and biblical and patristic translations. We found that Coptic texts have properties which make them excellent candidates for reading by computers. The characters can easily be distinguished due to their limited number and the fact that almost all the hand-written texts exhibit characters with highly consistent forms. Related WorkThe process of digitizing historical documents can be split up into at least three major steps: (1) pre-processing, (2) text prediction (OCR), and (3) post-processing or correction.Although many works already tackled subproblems ( He et al, 2005;Gupta et al, 2007;Kluzner et al, 2009), Springman et al.(2014) presented the first complete approach containing all major steps for historical Greek and Latin books.The first OCR results for printed Coptic texts were achieved by Mekhaiel (see Moheb's Coptic Pages) by using Tesseract to create a model for Coptic texts. Tesseract assumes that the image was printed with a standardized font. Although it can be trained to use many different fonts, creating a general model that would satisfy scholars is not feasible. In the end, this model is sufficient for pure printed Coptic texts, but creates a lot of noise for texts with mixed languages or annotations. Such drawbacks can be easily overcome by checking against a dictionary, but historical languages often do not have a dictionary that could be considered complete, and the texts might only be fragments that require further analysis.The recognition itself is performed by either Ocropy (Breuel, 2008) or Tesseract. Potentially, all character-based texts can be recognized. However, even though Mekhaiel provided a Coptic model for Tesseract, we were never able to achieve satisfying results on images which were not pre-processed. Data UsedFor training and testing, an expert on Coptic created a clean version and transcription of Kuhn's 1956 edition \"Letters and sermons of Besa.\" This will also be made available to the interested public.Besa is a fifth-century abbot of a monastery in Upper Egypt and Coptic writer, whose literary legacy consists mainly of letters to monks and nuns on questions of monastic life and discipline.Simplified pages were created to find the limits of the trained models with optimal input data. Since creating simplified pages consumes a lot of time, we consider this task as impractical for real use scenarios. Nevertheless, the results on these simplified pages show the best possible prediction.In Fig. 1 all characters and symbols that are going to be removed are marked red. The resulting simplified image can be seen in Fig. 2. By procedure, adjacent characters that are supposed to form one word are cut apart by gaps. Those gaps are going to be predicted differently by the two OCR engines.  MethodologyThere are two methods to train for Coptic texts: For this contribution, we created an Ocropy model with a training set containing approximately 5,000 characters. This set includes superlinear strokes, braces and foreign characters which are not part of the Coptic alphabet. Multilingual documents and documents containing foreign characters are considered complex. Stains on the document, bad image quality, and annotations like line numbers increase the complexity of documents as well. We, therefore, created special pages with reduced complexity. Our original pages were stripped offline numbering and footnote annotations. In the \"clean\" version, all foreign characters, punctuations and annotations inside the text were removed, leaving us with a pure Coptic text. We further stripped all clean versions of superlinear strokes, giving us the simplified version. For testing, the selected pages were transcribed with corresponding 'original', 'clean' and 'clean without stroke or simplified' ground truths. All results were compared with 'Ocreval' (Baumann 2014) [9] against the ground truth.(i) Results PredictionMekhaiel's original Tesseract model produced the best results on simplified pages with an accuracy of ~95%, while our Ocropy model performed better on the more complex pages. On the other hand, the Tesseract tends to produce predictable errors. Character ϣ will, for example, always be recognised as 񮽙 ; while, Ocropy produces unpredictable errors. Although our Ocropy model is less accurate on simplified pages, it surpasses Tesseract on noisier pages.  We measured that a skilled person needs roughly 10 minutes for manual transcription and 5 additional minutes for proofreading per page. Ocropy's models are built on top of transcribed images. Therefore, an initial ground truth is always required. Training with Ocropy does not require further human interaction but consumes up to two days of CPU power (Core i3/5 2.4GHz/3.2GHz, 8GB RAM, SSD), training cannot be run in parallel. Tesseract's training process, on the other hand, depends on the font extraction. We do not have enough data to estimate the time required to extract a font from an image. Both predictions still have to be checked manually, which can take up to 5 minutes. With clean pages and reduced proofreading time per page, Fig. 4 shows an optimal OCR workload reduction (red lines) in comparison to manual transcription (yellow line). A more realistic scenario is mentioned in the discussion. Fig. 4, workload comparison DiscussionOur result shows that Tesseract outperforms Ocropy on simplified pages in terms of accuracy and amount of human work. Unfortunately, in a realistic scenario, the pictures will always contain some of the previously described complexities. Pre-processing of the data is, therefore, essential to obtain good results. In Figure 4, we also computed a more realistic scenario (blue lines) with a higher workload on pre-processing for Tesseract. It shows that creating an Ocropy model pays off for larger and more complex document sets.Tesseract's overall acceptable performance is based on the fact that no model has to be trained. As creating and testing a model can consume more time than manual transcription and proofreading, the creation of clean images might still be less efficient than the manual approach even if a model can be reused.As long as cleaned images images are one of the desired results, our works shows that the workload can be reduced by half. This applies especially to Ocropy, since ground truth creation and training fit into the normal transcription workflow.Unicode ambiguities, which unfortunately result in encoding differences, require normalization and filtering. Otherwise, these encoding differences, which would not be seen as errors by humans, will be counted. Due to the same ambiguities, it is easy to mix characters from different code pages, especially on multilingual texts and text markings. It is, therefore, recommended that one use only corresponding code pages, especially with multilingual models. Tests with models containing multilingual fonts will be considered in further studies. ConclusionOCR of historical documents continues to be a hard problem, but we showed that utilizing OCR for the transcription of Coptic texts can reduce the overall workload. Since even the simplest images could not be recognized with 100% accuracy, further gains can only be achieved by better pre- and post-processing techniques.A bigger workload reduction can be achieved by model reuse. However, no Coptic OCR models have been published besides Mekhaiel's. Therefore, we highly recommend publishing models alongside the transcription and suggest that it is possible to predict almost all well-preserved texts.Also, although our model was able to partially predict multilingual texts, further studies are required. Multilingual texts require a specialized training process to compensate for the small numbers of foreign words.Fig. 1 ,1Fig.1, Original Image (excerpt), red elements are missing in the simplified version Fig. 3 ,3Fig. 3, OCR accuracy on different complexity levels Costs ",
        "article_title": "Optical Character Recognition with a Neural Network Model for Printed Coptic Texts",
        "authors": [
            {
                "given": "Kirill",
                "family": "Bulert",
                "affiliation": [
                    {
                        "original_name": "eTRAP Research Group - Georg-August-Universität Göttingen (University of Gottingen)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "So",
                "family": "Miyagawa",
                "affiliation": [
                    {
                        "original_name": "Georg-August-Universität Göttingen (University of Gottingen)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Marco",
                "family": "Büchler",
                "affiliation": [
                    {
                        "original_name": "eTRAP Research Group - Georg-August-Universität Göttingen (University of Gottingen)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionBy publishing his \"Graphische Litteratur-Tafel\" [Graphic Literature Table] in 1890, German writer Cäsar Flaischlen  aimed to portray the influences of foreign literatures on the development of German literature. Flaischlen produced a 58×86.5-cm poster, depicting German Literature as a stream with feeder rivers from mainly other European (national) literatures. His chart covers the development of German literature from its beginnings with various sources, forming two parallel rivers subsumed under the concepts of \"Volkspoesie\" [folk poetry] and \"Kunstpoesie\" [artistic poetry], intertwining and finally converging into one broad stream of German literature. The broadening river reaches from the beginnings of German literature at around 750 to Flaischlen's present, the 1890s.Cäsar Flaischlen was not much of a practising literary scholar, nor an academic. After completing a dissertation around the same time as he published his \"Graphische Litteratur-Tafel\", he left academia to continue writing (dialect) poetry, novels and plays, while working as an editor for arts and literary magazines.The flowchart, although being a flamboyantly beautiful one, is not as novel and unique, as one might think: It follows a long tradition of visualising developments along the axis of time (cf. Rosenberg and Graf- ton, 2010) and follows the patterns of the highly influential graphical visualisation of history, \"Strom der Zeiten\", published in 1804 by Austrian historiographer Friedrich Strass.Being a representative of positive thinking of his time, Flaischlen builds on the time-stream metaphor, but strives to connect this idea with the exact sciences. Although he does not mention the sources used for compiling his chart, this early visualisation of literary history relies on some kind of data, even if, in his 8-column preface, Flaischlen de-emphasises the connection between quantitative evidence and visualisation: For example, he points at the fact that the breadth of the stream was not calculated mathematically (\"nicht mathematisch berechnet\"), but nonetheless, there seems to be a connection between the selection and especially placement and typographical styling of influencing and influenced authors' names on the chart.As this example illustrates, the information density of the chart is enormously high:Flaischlen includes names of authors, texts, literary groups and schools and uses typography (font, font size, font decoration, colour), symbols (circles in various sizes, Roman and Latin numerals), shading of creeks and rivers and language-information to visualise the information.Our digital edition of Cäsar Flaischlen's \"Graphische Litteratur-Tafel\" aims to make digitally available the information deeply encoded in the table. Extracted data points are presented in two shapes: on an easily navigable website and in a machine-readable version.For this reason, the preface was OCRed and encoded in XML according to the TEI guidelines. The flowchart was scanned and transcribed following the recommendations on \"Advanced Uses of <surface> and <zone>\" (cf. TEI guidelines) to also record spatial information. Coordinates of authors and texts were calculated by help of the GIMP ImageMap editor, then linked to corresponding authority files (VIAF, GND, and Wikidata). The cartographic inventory of the map is marked-up and made searchable by using CSS as a descriptive language for capturing the rendering within TEI @style attributes.The TEI data is provided via a GitHub repository and processed via XSLT for displaying the flowchart as a web page.The interface combines the three separate sections of Flaischlen's map and allows for searches via an index. For presentation on the web, the sections were stitched and put together into one giant river. In an analogue format, the river would be almost three meters long, now one can scroll seamlessly down and up the river online. A prototype of the edition is available on the project website.Flaischlen's map is not only an inspiring prototype for contemporary attempts to visualise data of literary history with \"graphs, maps and trees\" (Moretti, 2007), but also challenges the development and encoding of older graphical representations of (literary) history on a methodological level.Flaischlen's 1890 visualisation \"Graphische Litteratur-Tafel\" deserves further recognition and our online edition could serve as a test case for other - so far unknown and/or not edited - flowcharts of literary history.    ",
        "article_title": "Cäsar Flaischlen's \"Graphische Litteratur- Tafel\" -digitising a giant historical flowchart of foreign influences on German literature",
        "authors": [
            {
                "given": "Angelika",
                "family": "Hechtl",
                "affiliation": [
                    {
                        "original_name": "Higher School of Economics Moscow",
                        "normalized_name": null,
                        "country": "Russia",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    },
                    {
                        "original_name": "Universität Wien",
                        "normalized_name": "University of Vienna",
                        "country": "Austria",
                        "identifiers": {
                            "ror": "https://ror.org/03prydq77",
                            "GRID": "grid.10420.37"
                        }
                    }
                ]
            },
            {
                "given": "Frank",
                "family": "Fischer",
                "affiliation": [
                    {
                        "original_name": "Universität Potsdam",
                        "normalized_name": "University of Potsdam",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/03bnmw459",
                            "GRID": "grid.11348.3f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionAdvances in technology and digital access have paved the way for the improved utilization and interpretation of scientific analyses of humanities materials for digital humanities studies. Integrating scientific analyses with humanities and curatorial knowledge (STEM: science, technology, engineering, and math, to STEAM: science, technology, engineering, art, and math) is a critical multidisciplinary approach for expanding the full potential of scientific techniques and technological advances, and realigning complementary disciplines that have been artificially segregated. Scientists and curators have exposed hidden and previously unknown contextual information within original source materials, such as changing \"subjects to citizens\" on the Rough Draft of the Declaration of Independence (Library of Congress, 2010). Hyperspectral imaging provides additional data layers by capturing images of documents in distinct narrow waveband regions of the visible and non-visible spectrum-from ultraviolet through visible to infrared. The cube of captured digital image files contains a wealth of information, but requires significant interpretation to process and analyze the collected data. Scientists, scholars, and students in both art and science disciplines have been collaborating to glean new information from historical manuscripts. The Library spectral imaging program includes a spectral reference database and integration of data from other non-invasive analytical techniques to create a full analytical mapping of heritage documents and objects for non-destructive analyses of collection materials (France, 2016) Digital imaging capabilities allow researchers to characterize pigments and inks on the document, retrieve hidden and lost text, and illuminate production and creation methods. The range of data captured allows greater access to the information available from fragile historic documents, including the 1507 Waldseemüller World Map and the 1513 Ptolemy Geographia, where investigations revealed links to the same original printing location (France, 2016). Scriptospatial (a term originally coined by Toth and France to refer to the viewing of associated imaging and materiality data linked on an image of a historic document) refers to applying a spatial information system approach to documents, creating an interactive interface for scholars and scientists to interact with the object and the data. Scriptospatial representations of digital data from documents utilize an accurate coordinate system that links scientific and scholarly analyses to the creation of a new digital cultural object (DCO), allowing inferences to be drawn to generate new knowledge. This approach to viewing digital cultural materials in multiple layers applies an archaeological approach toward uncovering and interconnecting information strata of historic and modern documents. Scriptospatial mapping of documents with an accurate coordinate system allows the layering of scientific and scholarly analyses to the DCO. This allows inferences to be drawn to generate new knowledge through analysis of the data linked to spatial points (or areas). This approach to viewing the DCO applies a GIS methodology toward uncovering and interconnecting information layers of cultural heritage artefacts, just as in the case of archaeological strata. Utilizing an object-oriented approach in conjunction with the spatial data layers allows the mapping of spatial and temporal data with increasing complexity. Examining and explaining the physical, spectral and chemical properties of these historic materials permit scientists and scholars to link these scientific analyses to other data about the creation of the object.Digital spectral imaging of cultural heritage objects at the Library of Congress has capitalized on over a decade of research and development into not only spectral imaging and processing, but also the development of standard spectral image products (France et al, 2010). Advances over the past decade by an experienced team have led to an advanced capability to study cultural heritage objects with a robust spectral imaging system that provides large-format, high quality images and standardized data output using advanced commercial off-the-shelf components (Chris- tens-Barry et al, 2009).Developing an object-oriented approach to data access and sharing requires integration of spectral imaging with data from other sources in a variety of formats. This requires effective spatial metadata to allow linkages to specific locations within the images. This is necessary not only to register locations on the same section of a manuscript leaf in various spectral bands, but also to link other images and transcriptions with the spectral images. Based on geospatial mapping and layering of data used to identify points on satellite images, the same technologies, work processes and skills can be applied to spectral images of manuscripts: A camera collecting images over a manuscript is similar to a satellite collecting geospatial data over the Earth. Using technologies developed for \"geospatial\" systems to link each point on the globe with images from earth resource satellites and data collected from other instruments, spectral imaging can link the \"scriptospatial data\" from each point on a manuscript with images from various imaging and scientific devices. This provides a standardized method to support links between images and data from the same object location.With multiple data entries for samples, precisely defining the specific point where the sample or scientific data collection takes place is critical in comparing data from different research types or objects. For samples taken from a larger, non-uniform, heterogeneous object such as a manuscript, textile or painting, the spatial location of the sample point on the object must be defined to be able to integrate the data from various research tools. Spatial metadata elements will allow linkages to specific locations on an object, potentially within images of the objects. Scriptospatial data can serve as an interface for scientific dialogue in \"one shared layer,\" linking data from various sources for indepth studies and analyses of a specific research topic or object.In many current research databases, the metadata elements for spatial location are not provided to capture detailed data on where an instrument collects data, or a sample is taken. This is not an issue for uniform, homogeneous samples of paints, pigments, media or other samples, but is critical for samples taken from a heterogeneous object like a painting, manuscript or textile. By defining a Cartesian coordinate system on an object or image of an object, as well as the degree of precision required, specific sample points on an object can be defined. This allows integration with other images of the same object and scientific samples from the same point.In its Scriptospatial Visualization Initiative, the Library of Congress PRTD has capitalized on developments with geospatial systems to apply Thermopylae \"i-spatial\" support and Google Map tiling and data formatting to the integration of large and complex visual scriptospatial datasets populated with scientific data from various instruments, research topics or objects. This provides data access in \"one shared layer\" of scientific data. This is an important first step in capitalizing on the three decades of technology development by the GIS community to advance preservation science and cultural heritage data sharing and research. The additional unique component will be the layering of scholarly research interpretations and publications, enabling ease of access to a rich resource of data directly linked to the original object. This will reduce the challenges faced with searching through data that is not well catalogued, or yet searchable without this expanded document interpretation and linking of scholarly knowledge.Integrated access to associated source material scientific data adds contextual value, provenance information, and can reveal non-visible information hidden within the original document. The Scriptospatial approach allows layering of scientific and scholarly or curatorial research data within one location, enhancing the analysis and depth of knowledge off the original document, and creating a more effective interface for interaction with historic materials (France et al, 2010). The innate layering of multiple sources of information of the Shared Canvas Model ( Sanderson and Albritton, eds, 2013) onto one view of an object, coupled with the strong annotation capabilities of the Web Annotation Data Model (Sanderson et al, eds, 2016) raises interest in the possible integration of the Scriptospatial model with IIIF efforts (International Image Operability Framework). One of the challenges we face with the current data deluge, is how to effectively access, select and link appropriate data and information. Scriptospatial is a value-added data approach, creating cohesive structured management of multidisciplinary data. The authors have been engaging with other US, European and United Kingdom colleagues to create a cohesive integrated and collaborative approach to this visualization. ",
        "article_title": "Integrating Humanities and Science: the Scriptospatial Visualization Interface",
        "authors": [
            {
                "given": "Fenella",
                "family": "France",
                "affiliation": [
                    {
                        "original_name": "Preservation Research and Testing Division - Library of Congress",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Alberto",
                "family": "Campagnolo",
                "affiliation": [
                    {
                        "original_name": "Preservation Research and Testing Division - Library of Congress",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "UMass Amherst, United States of America In the concluding days of the 2016 session of the NEH Advanced Topics in the Digital Humanities Summer Institute on Advanced Challenges in Theory and Practice in 3D Modeling of Cultural Heritage Sites held at UCLA, participants, faculty, and invited scholars fo-cused discussion on the critical issues facing academics working with 3D content. The goal of these conversations were threefold: 1) to clearly articulate the challenges facing researchers integrating 3D tools and methods into their scholarship, 2) to outline key questions and new lines of inquiry for future investigation, and 3) to develop actionable recommendations to position 3D work as a valid-and viable-mode of knowledge production. This paper describes that process , the topics chosen for discussion, and the resultant list of action items for the 3D community.",
        "article_title": "Can VR Survive Peer Review? Cultural Challenges for 3D Research",
        "authors": [
            {
                "given": "Lisa",
                "family": "Snyder",
                "affiliation": [
                    {
                        "original_name": "Urban Simulation Team - University of California, Los Angeles (UCLA)",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Alyson",
                "family": "Gill",
                "affiliation": [
                    {
                        "original_name": "University of Massachusetts (UMass) at Amherst",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn recent years, semantically enhanced Digital Humanities Research has become a widespread topic realized in different environments (e.g. CWRC, Pundit). While semantic graph technologies are mainly used to connect, annotate, query and aggregate strictly formalized entities, there is a lack of interfaces for enhancing acts of interpretations.Annotations are described as crucial in interpretations and are designated as a killer application (Juola, 2009), scholarly primitive (Unsworth, 2000) and considered as notetaking within main scholarly information activities (Palmer, et al. 2009). Concerning an interpretational act, limitations of annotations are identified (overlapping, flexibility), and there is a demand for customization to the research context and an iterative and agile of schema development (Piez, 2010). Drucker indicates the emergent qualities of interpretation, while suggesting an interface which \"supports acts of interpretation rather than simply returning selected results from a pre-existing data set\" (Drucker, 2013: 37). In this paper, this desideratum is addressed by designing an interface for collaborative and multi-layered spaces of interpretations based on a semantic graph (Suchman, 2007;Drucker, 2011;Rheinberger, 2010;Barad, 2003).A specific style of interpretation is chosen as a case study, i.e. collaborative analysis of face-to-face situations in small groups by creating a multiple layered space of interpretation -Objective Hermeneutics. In German-speaking countries, the approach of Objective Hermeneutics is one of the main methodologies used for qualitative analysis (Flick, 2005), which generates deep-structure analyses of cases by reconstructing actions and meanings. Creation of an interface that enables semantic annotations for these acts of interpretations makes it possible to elaborate and explicate a multiple layered space of interpretation.In the following, we describe settings in the interpretational act of Objective Hermeneutics. Furthermore, the background of the design is outlined in relation to methods, design and data. The main contribution is twofold: (i) realization of an interface focusing the semantic enhancement of the collaborative spaces of interpretation and accountability and (ii) examines the semantic explication of the research data (interactional protocols), the interlinked multiple layered annotations and the possibility to retrace the space of interpretation. The collaborative interpretational act of Objective HermeneuticsThe theoretical framework of Objective Hermeneutics is based on Oervermann's theory of professionalization (see Reichertz, 2004), whereby the act of interpretation follows strict principles for analyzing 'natural protocols' of social practices (transcripts). In a sequential multi-step procedure of interpretation, the structure of the case is reconstructed. The act of interpretation is realized in small groups where a common space of imagination is created collaboratively, wherein multiple layers of interpretations interfere and make use of falsification and abduction (Flick, 2005). Accordingly, the process of interpretation can be outlined as follows: 1) Specifying research question and analytical framework; 2) choosing appropriate transcript; 3) selecting sequence from interaction protocol (transcript); 4) creating and discussing step by step multiple corresponding stories, perspectives, and connections of the sequence; 5) recontextualisation to the concrete case, whereby in the long run hypotheses of the structure of the case are created iteratively and new sequences are selected (back to 3.). Additionally, 6) a proofing process is started (falsification). Based on this interpretative act, a detailed case structure is created which describes the conflicting motivations, interests and interactions of the actors. While in recent years special research data archives for archiving and re-using the transcripts (non-processable PDFs) have been established, the act of interpretation itself is still paper-based. This situation provides the opportunity for an appropriate case study for designing an interface for collaborative and multi-layered spaces of interpretations (for example, see the archive for pedagogical casuistry (ApaeK) archiving transcripts of classroom interactions) Methods, design, and dataThe research environment for Objective Hermeneutics is based on a participatory design and agile development approach, using Semantic MediaWiki framework. To fulfill the case-related special research requirements an extension for Semantic MediaWiki and a research ontology were collaboratively developed. Besides the analysis of needs and requirements (site visit, artefact analysis) rapid prototyping was used and three versions of the environment were thoroughly tested (the logfile analysis between the last two versions indicates a clear improvement at the interpretation process by reducing the break up rate from 33% to 0%.) A group of distributed researchers across Germany and interested in classroom interactions (topic othering) used the environment in practice over several months and supported the design process by attending meetings and giving feedback. Interfacing collaborative spaces of interpretations and accountability Explicating interaction protocols semanticallyThe act of interpretation in Objective Hermeneutics is based on 'natural protocols' of social practice, which pursue strict notation guidelines for the transcription process (e.g. anonymization, settings of actors, properties like loudness). The transcript is enriched with contextual metadata (e.g. collecting context, duration, and topic). Line by line, each speech act of an actor and relevant interactions are described in detail (based on audio recordings, maps, photographs). This initial base already allows for semantically enhancing the space of interpretation: Interlinking relevant documents, entities, properties, and relations for semantic browsing (Figure 1, 1+2). Additionally, a formula semantically (Figure 1, 3+4) outlines the interactions of the transcript in detail (actor, speech act/interaction, line number). Thus, each annotation of interpretation can be related to this empirical level and the process of interpretation can be retraced. Based on this semantically enhanced transcript, the researchers choose and define their sequence of interest to start their act of interpretation (segment selection). Interlinking spaces of interpretation and multiple layered annotationsThe act of interpretation in Objective Hermeneutics is semantically-enhanced and explicated by following guidelines for interpretation, whereby the flexibility of interpretation and the computer-mediated co-presence is taken into account. Each selected sequence of the transcript opens up the space of interpretation through multiple layered styles of annotations (stories, perspectives, connections, and contextualization) (Figure 2, 2+3). Subject to their discussions and notes, the researchers specify their arguments and elaborate a common ground for the case analysis. For an adequate interface of the phenomenon, the multiple layered styles of annotations need to be visualized in relation to the corresponding sequence of the transcript (compare Figure 2, 1+3). Closure respectively densification of the space of interpretation is semantically enhanced by creating specific case hypotheses. Researchers create connections between the layered annotations and specific case hypotheses, whereby a hypothesis and its related entities (e.g. layer of annotation, sequence, interaction, actor, or author) can be browsed semantically, described with texts and data representations. Each annotation is described with further relevant properties (e.g. timestamp, researcher name, related sequence) and interlinked within the sematic graph. Retracing the spaces of interpretation and accountabilityThe analysis and reflection of the research process as well as the spaces of interpretation are enhanced by using a semantic graph and explicating the interaction protocols (transcripts) offering new capacities for retracing the spaces of interpretations (Figure 3). The multiple layered semantic graph interlinks the acts of interpretation and facilitates multiple perspectives for accountability concerning the 1) interaction protocols and their interlinking (Figure 4, 2), 2) the chronological acts of the researchers, 3) the multiple layered annotations for interpretations (Figure 4, 1), and 4) the creation and interlinking of the case hypothesis. Besides these imminent possibilities of the research project, external aspects of accountability can be addressed. While in Humanities the practice of data citations is not widely spread, research communities in Object Hermeneutics have established a citation practice via the archives of the transcripts, referring to the interactions of the transcript in their publications (as bibliographic data). In retracing the spaces of interpretation, the relevant multiple layers of annotations as well as the hypothesis interlinking can be referenced, opened, and described with semantic vocabularies. The semantic graph has been mapped to relevant semantic vocabularies e.g. Wf4Ever Research Object Model (ro), Object Reuse and Exchange (ore), Named Graphs (rdfg), Web Annotation Data Model.  Discussion and outlookIn this paper, we discussed and demonstrated the design of an interface for collaborative and multi-layered spaces of interpretation, using the methodological approach of 'Objective Hermeneutics' as a case study. The interface is considered in relation to the phenomenon and the relevant performative materialdiscursive capacities for the interpretational act, focusing on the use of a semantic graph. The detailed semantic description of the research data (transcripts) and the associated spaces of interpretations (stories, perspectives, connections, and contextualisations along with hypothesis) enable a collaborative and distributed analysis and new ways of retracing the spaces of interpretation (interlinked data, chronological acts, multiple layered annotations, case hypothesis). But time and effort of the semantic enhancement need to be balanced against these added values in each new research project (e.g. interlinking, accountability, data manipulation, visualisation, citation, and openness).",
        "article_title": "Interfacing Collaborative and Multiple-Layered Spaces of Interpretation in Humanities Research. The Case of Semantically-Enhanced Objective Hermeneutics",
        "authors": [
            {
                "given": "Christoph",
                "family": "Schindler",
                "affiliation": [
                    {
                        "original_name": "German Institute for International Educational Research",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Cornelia",
                "family": "Veja",
                "affiliation": [
                    {
                        "original_name": "German Institute for International Educational Research",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Helge",
                "family": "Kminek",
                "affiliation": [
                    {
                        "original_name": "Johann-Wolfgang-Goethe-Universität Frankfurt am Main (Goethe University of Frankfurt)",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Greek epics, traditionally associated with Homer and Hesiod, play a special role among the early texts of Europe. Not only does European literature begin with these poems, but they also mark the transition from oral tradition to written texts. This intermediate position poses very special problems to every philologist, which are best known under the name 'Homeric Question' (going back to Friedrich August Wolf's Prolegomena ad Homerum, 1795): Have these texts been orally composed and later written down (as Oral Poetry Theory claims)? Or are they poems written by a single great poet experienced in oral tradition (as Neoanalysis maintains)? Or do they combine different passages stemming from different times and poets, compiled later (as Analysis assumes)? The core question is: How can we know?All three theories present evidence for their findings, but this evidence consists of preselected material - obviously selected according to the principle to present what fits best to the own theory. In order to improve our view, it is useful to look at the complete data instead. That is why as early as the 1970s, the University of Regensburg launched a computer aided project aiming at providing the linguistic data needed for an overview. The project was founded by Ernst Heitsch and Xaver Strasser, and its conception and aims are precisely described in Strasser's dissertation thesis (1984).Oral texts (as we know from Parry, etc.) are constructed not from single words (Lemmata), but from repeated word connections, which the oralists call 'formulae', but a neutral observer would better call 'iterata' (= lat. 'repetitions'). It has been known that repetitions are a key component of testing Homer theories since the 19th century, but at that time there was no reliable way to collect all of the required data. The Regensburg Project created the first complete directory of Epic repetitions (iterata), based on a lemmatized concordance of all Epic word forms. In the \"Regensburger Iteratenverzeichnis\" (RIV), which is still unpublished, 'iterata' are defined as semantic and syntactic meaningful phrases that occur at least twice in the corpus. It is e.g. possible a) to find passages that are interlinked by the usage of the same iterata throughout the corpus, and b) to have meaningful information about the usage (e.g. frequency, compactness) of words and phrases. Therefore, the RIV offers the possibility of collecting and presenting all iterates of a certain type, i.e. of specific frequencies or distributions over the epic texts.The new possibilities have been used for collecting and researching a complete group of iterata: the socalled 'Singulaere Iterata der Ilias', i.e. those repetitions which remain unparalleled in the Iliad. The results have been published in four dissertations (Ramersdorfer, 1981;Csajkas, 2002;Blößner, 1991;Roth, 1989). This group of repetitions is of special interest, because the Iliad is the largest and (according to common opinion) oldest of our epic texts. Therefore, an Oral Theory would expect that it is very small, because why should 'old formulae' be so rare in our oldest and largest text? However, this group contains 3,739 Iterata (out of 18,961 Iterata in sum), and in addition, linguistic and semantic research gives evidence in many cases that Oral Theory assumptions do not really explain the facts. It looks as if the claims of the Oral Theory are fundamentally based on (wrong) generalizations of some (correct) results. But also the Neoanalyst position is weakened by demonstrations that, in some hundred cases at least, passages of the Iliad presuppose the knowledge of 'younger' texts. These results do not only diminish the weight of widely spread theories, but offer concrete data on which new, and more reliable, theories can be built (cf. Blößner, 2006).Since these examinations, the methods of Computational Linguistics have improved a lot as has the processing power of today's computer hardware. This paper presents an approach to finding further subsets of iterata that continue and extend the idea of improving Epic theories. Extending the idea of the 'Singulaere Iterata of the Iliad'The search aims at finding passages in the text which react to each other. With these results, existing theories can be tested and better ones can be built.Searches of this kind are applications of the scholars' implicit knowledge. So in order to be able to create a computer assisted system this expert knowledge has to be transferred to describable rules and algorithms. The idea of the 'singulaere Iterata', e.g., defined a \"conspicuousness\" of a phrase that has an unexpected distribution (contrary to the expectation of some theories). This heuristic was proven valid by the results of the four dissertations mentioned above.Next we will present some ideas that have a wellknown foundation within Computational Linguistics but also can be seen as an extension of the 'singulaere Iterata' idea.The SIoI compares frequencies between two corpora, the Iliad and the complete corpus but the Iliad, where one frequency is very rare (one). So it can be seen as a subset of the frequency list comparison, which searches for terms that differ largely between two (or more) corpora. This method uses the frequency class which for a given corpus K and a term t is defined as where is the frequency of t in K and is the most frequent term in K. Now one could search for Iterata I where which obviously extends the SIoI.With the method of the frequency list comparison one could especially search for iterata that are rare in the Iliad but frequent outside of it, but it is hard to decide whether they are of significance regarding the search for parallel passages. So we could use the density of the occurrences outside of the Iliad as a filter for this class of Iterata. A well-known metric for this question is the Chi-squared test which for partitions R (e.g. overlapping passages of 300 verses ignoring book boundaries) of a corpus K and an iteratum I is according to (Rayson et al., 2004) defined as:A high Chi-squared value now suggests that this iteratum is compact in the sense that many of its occurrences are close to each other in respect to the overall corpus.Another approach to a density filter is the log-likelihood-ratio as proposed in (Dunning, 1993) that according to (Moore, 2004) is also valid for rare events (in this context iterata with low frequency). For partitions R of a given corpus K and an Iteratum I it is defined as:Again the expected occurrences of an iteratum in a given passage are compared to the actual frequency. But also the same metric is applied to the rest of the corpus aside from the considered passage. Again a high log-likelihood-ratio value indicates that the iteratum is compact in this passage. Also it is possible to find passages where an iteratum is more frequent than expected.An issue may arise as this metric is rather complicated from the perspective of a scholar in the humanities and it remains to be well explained; issues like this have been addressed in the \"ACID for the Humanities\" of the DARIAH project (Büchler, 2013) and also in the conclusions of Bestgen, 2013. First results and conclusionsThe ideas presented above have been fully implemented, and this implementation has been used in first applications. It has been suggested to use the Chisquared test and log-likelihood-ratio to test the validity of an assumption of the Oral poetry, which says that  ",
        "article_title": "Using Methods of Computational Linguistics for Resolving the \"Homeric Question\"",
        "authors": [
            {
                "given": "Christoph",
                "family": "Beierle",
                "affiliation": [
                    {
                        "original_name": "University of Hagen",
                        "normalized_name": "University of Hagen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04tkkr536",
                            "GRID": "grid.31730.36"
                        }
                    }
                ]
            },
            {
                "given": "Norbert",
                "family": "Blößner",
                "affiliation": [
                    {
                        "original_name": "Freie Universität Berlin",
                        "normalized_name": "Freie Universität Berlin",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/046ak2485",
                            "GRID": "grid.14095.39"
                        }
                    }
                ]
            },
            {
                "given": "Sebastian",
                "family": "Kruse",
                "affiliation": [
                    {
                        "original_name": "University of Hagen",
                        "normalized_name": "University of Hagen",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/04tkkr536",
                            "GRID": "grid.31730.36"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis study concerns Aby Warburg's last and most ambitious project: the Atlas Mnemosyne (or Bilderatlas), conceived in August 1926 and truncated three years later, unfinished, by Warburg's sudden death in October 1929. Mnemosyne consists of a series of large black panels, on which are attached a variable number of black-and-white photographs of paintings, sculptures, tarot cards, stamps, coins, and other types of images. The version we use is the one Warburg was working on at the time of his death, also known as the \"1-79 version\": it includes around a thousand images pinned on 63 panels (This version of the Atlas is published in various print editions, and available online). The Bilderatlas is a conceptual maze - the culmination of a life's scholarship in images and memory - through which perhaps the clearest thread is the concept of Pathosformel, or formula for (the expression of) passion. Much excellent work has been written on the concept but, to the best of our knowledge, nobody has yet tried to \"operationalise\" it -to turn it into a sequence of quantitative operations, or in other words, into an algorithm (Moretti, 2013). The Pathosformel and its OperationalisationOn the most basic level, the Pathosformel describes the portrayal passionate emotions through a formula, a repeatable visual paradigm. The Pathosformel owes much of its force, as Salvatore Settis has pointed out, to its combination of semantic opposites: an \"oxymoronic word, in that it merges in the same term the movement of pathos and the rigidity of the formulaschema\" (Settis 1997).Rather than attempting to operationalise the entire concept at once, we first break it down into its constituent components: first the morphology of formula, then the dynamism of pathos.We turn to two well-studied cases of Warburgian formula: the Death of Orpheus (where Warburg first named the concept of Pathosformel), and the Nymph, headhuntress and Fortuna (Bilderatlas panels 46-48). Looking at Warburg's examples of the Orpheus-formula, reproduced in Figure 1, we can hardly stop ourselves from spotting a formula which repeats across the ages; but how could this intuitive similarity be measured?Our algorithm comes as follows:1. We isolate each individual body from its context. It is clear the Pathosformel relates to individual characters - the object of study thus becomes not panels or photographs in the Bilderatlas, but individual bodies. 2. We take only the skeletons of such bodies.Here we are eliminating colour, clothes, hands, faces, gender, age. This is not to say that such factors aren't important: but they are not elementary to the formula (see for example, André Jolles' letter to Warburg 23rd December 1900, where the formula of Ghirlandaio's Nymph hops between Judith, Salome, Tobias, Gabriel etc.- see Ghelardi, 2004; or the formula in Bilderatlas Panel 47 shared between Giambolo-gna's Samson and Donatello's Judith.) 3. We compare these skeletons by measuring the angles of the main limbs of the body, as described in Section 5.Each of these steps is not merely a convenient quantification, but a conceptual wager. This is the strength of operationalisation as a critical tool: it forces one to be explicit about the conceptual choices one makes.  Anatomy, Emotion and PoseWarburg was certainly influenced by Darwin's The Expression of the Emotions in Man and Animals (Darwin 1872) -which, when talking about human emotions, largely concentrates on the face. Indeed, the first figure of the book is an anatomical diagram of the face - \"I shall often have to refer [...] to the muscles of the human face\" (Darwin 1872 p.23). Warburg was certainly struck by the book -writing in his diary \"ein Buch, das mir hilft!\" (Gombrich, p.72). He was also interested and capable of studying the face in art (see e.g. his discussion of faces in Ghirlandaio's Confirmation in Santa Trinita, in The Art of Portraiture in the Florentine Bourgeoisie (in Warburg 1999 p.185), yet never in relation to Pathos -his descriptions of the Pathosformel relate exclusively to the body.We can relate Warburg's decision to the large psychological literature on emotional recognition from bodies. Psychological studies are based on the Light Spots Model by Johansson (1973), often called 'biological motion', in which reducing body pose to 10-12 points -quite comparable to our own reduction -is judged to give a 'compelling impression of human walking, running, dancing etc.'. Using only Light Spots, observers can reliably tell gender and emotion from dynamic pose (Kozlowski 1977, Montepare 1987. Indeed, it has been suggested that our emotional understanding of faces is more influenced by our perception of the body than vice versa (Van den Stock 2007). Encoding Pathos through PoseThe Atlas is, even by today's dataset standards, quite sizeable: 1000 images across 63 panels, containing an order of 10 3 -10 4 depicted human figures. Scalable manual annotation is only therefore possible through crowdsourcing, which we did through the CrowdFlower platform.Accurately annotating every visible figure in an image is a difficult and ambiguous tasks. Additionally, if different workers annotate different figures in the same image, the annotations cannot be collated or averaged. We therefore developed a two-stage annotation process:Human figures are extracted from the painting by drawing bounding-boxes. This is done three times per image (by three separate workers).Having aggregated the information from the first stage, separate images are produced for each human figure. Detailed pose information (the position of major body-points) is then added by three separate workers, with the information aggregated.It should be clarified that the decision to annotate bodies in isolation (for annotation accuracy and just worker compensation) is quite separate to the earlier conceptual decision to analyse bodies individually, which relates to the object of study. It would be quite possible to do either one without the other.Using this two-stage annotation process, we have presently annotated ⅓ of the Bilderatlas (by panels), resulting in 1,665 aggregated human poses. The collection and aggregation of the data are described in greater technical detail elsewhere (Impett, 2016). Data Analysis: dimensionality reduction and dimensioned reductionismHaving encoded our static poses, how do we analyse and compare a collection of human figures of different sizes, proportions and orientations? We mirrored the poses horizontally and controlled for global rotation, ending up with a 11-dimensional vector P, describing the angles of the main limbs.From this angular pose vector, we can use circular statistics to find a morphological distance Da,b between two poses Pa and Pb:Where Pa, i is the i th angle of pose vector Pa , and ||α is the angular radian distance:These morphological pose-differences are perceptually meaningful over short distances. On a larger scale, they become less perceptually significant: is a sitting person 'closer' to a lying or standing person?In order to make our distance analysis perceptually relevant, therefore, we first clustered our 1,665 poses into 16 clusters by rotational K-means clustering (Dordet-Berdanet and Wicker 2008). Our two-stage clustering system is therefore as follows:I. K-means clustering (to produce meaningful clusters) II. Hierarchical clustering (for within-cluster morphological information)The number of clusters K is chosen by looking at the inter-cluster variance over K. The result of the first stage of clustering is shown in Figure 2; Figure 3 shows an example section from a hierarchical map of Cluster 1. Unity of the Pathosformeln: from distant to close readingSome of the clusters clearly represent physical activities -sitting, praying, embracing, dancing -whilst others seem more subtly communicative or expressive in nature. Having reorganised the 1,665 figures into 16 mean-centred clusters, we proceeded to trace the classical Pathosformeln - identified in the Atlas by primary and secondary literature - through our clusters.The canonical Pathosformeln are mainly mythological figures (Perseus, Pentheus, Orpheus) or recurring allegories (Graces, Nymphs, Fortuna). They were previously described as distinct, and we expected to find a taxonomy of such formulae through our clustering analysis.On the contrary, the statistical result was much stronger: a complete morphological unity. Almost every identified Pathosformel falls into Cluster 1, with few false positives - over 80% of the figures in Cluster 1 are an identified Pathosformel. The handful of exceptions are all borderline cases, placed in peripheral to Cluster 1 (Clusters 7 and 13).Looking more closely at the images themselves, as in Figure 4, this becomes visually clear: not only do the Pathosformeln share certain pose features (most importantly, a raised arm) present nowhere else in the dataset. To date, however, the authors know of no arthistorical literature that has identified such morphological unity.  Concluding remarksOur morphological model for Pathosformel is statistically strong: but what are the art-historical implications? The oppositional symmetry and raised arm of Cluster 1 (Fig. 2) reminds us of a Contrapposto, but the bodies themselves are far removed from such classical balance (e.g. Fig. 4, top). Rather than movement, tension (between upper and lower body) seems to be the fundamental element of Pathosformel - the nature of which will be the subject of a subsequent publication.Our morphological analysis has shown that static pose can identify Pathosformeln, and that a study of static pose through a large collection of artistic works can identify links across styles, periods and cultures.",
        "article_title": "From Mnemosyne to Terpsichore -the Bilderatlas after the Image",
        "authors": [
            {
                "given": "Leonardo",
                "family": "Impett",
                "affiliation": [
                    {
                        "original_name": "École Polytechnique Fédérale de Lausanne (EPFL)",
                        "normalized_name": null,
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Sabine",
                "family": "Süsstrunk",
                "affiliation": [
                    {
                        "original_name": "École Polytechnique Fédérale de Lausanne (EPFL)",
                        "normalized_name": null,
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Scheme of the systemComponents Word frequencies can be simply computed for English, but not for highly inflected languages such as Polish, which has more than 100 possible word forms of an adjective (however, almost-full sets of distinct forms exist only for some lemmas). In such languages, morphological forms have to be first mapped to lemmas by a morpho-syntactic tagger, e.g. WCRFT2 for Polish (Radziszewski, 2013). By applying different language tools, we can enrich texts with metadata revealing linguistic structures.LEM expands WebSty - an open stylometric system, adopting the following features for text description: segmentation-based (lengths of documents, paragraphs and sentences), morphological (words, punctuations, pseudo-suffixes and lemmas), grammatical classes and categories (e.g. from the Polish National Corpus -see Przepiórkowski et al, 2012- tagset, Broda andPiasecki, 2013) and their n-grams. This set has been additionally expanded in LEM with the following features, allowing for semantic analysis:• semantic Proper Name classes -recognised by a Named Entity Recogniser Liner2 (Marcińczuk et al, 2013), • temporal, spatial relation ( Kocoń and Marcińczuk, 2015), and selected semantic binary relations (e.g. owner of) , • lexical meanings - synsets in plWordNet (the Polish wordnet); assigned to words and selected multiword expressions by Word Sense Disambiguation tool WoSeDon ( Kędzia et al, 2015), • generalised lexical meanings -meanings mapped to more general synsets, e.g. an animal instead of a cheetah, • lexicographic domains from Wordnet.Rich text description is a good basis for several processing paradigms that LEM is going to support, namely:• linguistic text preprocessing -extraction of language data for further statistical analysis, i.e. computing frequencies as the initial feature values, e.g., of lemmas, tags, word senses, etc.,• topic modelling,• unsupervised semantic text clustering and analysis of characteristic features for clusters,• supervised semantic text classificationtrained on the manually annotated texts,• stylometric analysis -performed with the help of the WebSty system. Processing schemeThe processing paradigms share the following workflow:• Uploading a corpus of documents together with metadata in CMDI format ( Broeder et al, 2012) from the CLARIN infrastructure.• Text extraction and cleaning.• Choosing the features for the description of documents by users (see Fig. 1).• Setting up the parameters for processing (users).• Pre-processing texts with language tools.• Calculating feature values for the pre-processed texts.• Filtering and/or transforming the original feature values.• Data mining.• Presenting the results: visualisation or export of data.To facilitate the upload, users are encouraged to deposit large text collections in the CLARI-PL dSpace repository. Users are advised to use public licences, but private research corpora can be also uploaded.OCR-ed documents usually contain many language errors that should be corrected to some extent in the step 2. Moreover, metadata elements (e.g. page numbers, headers and footers) have to be separated during from the content and stored in a standalone annotation.Users are not expected to have advanced knowledge of Natural Language Engineering or Data Mining. Thus, in Step 4, default settings of parameters will be provided. More advanced users will be able to tune the tool to their needs (see Fig. 1) Figure 1. Web interface -a panel with a list of features In Step 5 language tools are run. Each text is analysed by a part-of-speech tagger (e.g. WCRFT2) and next piped to a name entity recognizer (e.g. Liner2, Marcińczuk et al, 2013), temporal expression recognition, word sense recognition (WoSeDon, see Kędzia et al, 2015), etc.Extraction of features encompasses counting frequencies, but also annotations matching patterns for every position in a document. In the case of wordnetbased features, meaning generalisation is done by iterating via wordnet structure.A dedicated feature extraction module was built that is similar to Fextor (Broda et al, 2013) but much more efficient by supporting parallel processing. As a result of Step 6 every document is represented as vector of feature values and/or a sequence of language elements.Filtering and transformation functions comes from the clustering packages or dedicated systems, e.g. SuperMatrix system (Broda and Piasecki, 2013).Step 8 differentiates between the processing paradigms. Topic modelling, e.g. by Mallet, takes documents represented as lemma sequences. They can be also processed by corpus tools, e.g. for concordances and frequencies. Documents as feature vectors can be processed by clustering systems e.g. Cluto, or used in machine learning, e.g. Weka system.Different processing paradigms provide varied perspectives on the data, e.g. topic modelling represents a document in terms of stochastic processes generating word occurrences from topic-related subsets in the text. Clustering reveals groups of documents based on content similarity. It is difficult to find a system that supports all paradigms.In LEM, clustering is expanded with the extraction of features characteristic for the individual clusters. Several functions (from Weka, scikit-learn and SciPy packages), based on mathematical statistics, information theory and machine learning, are offered. The rankings of features are presented on the screen for interactive browsing and can be downloaded.WebSty, based on elements of the same framework, can be applied to stylometric analysis.Step 9, visualisation of clustering results (see Fig.  4), is based on Spectral Embedding (also known as Laplacian Eigenmaps). The 3D representation of the data (represented by similarity matrix) is calculated using a spectral decomposition of the graph Laplacian. Texts similar to each other are mapped close to each other in the low dimensional space, preserving local distances. Use CaseThe LEM prototype was developed by the team working with a particular textual corpus of 2553 Polish texts, published in Teksty Drugie, an academic journal dedicated to literary studies. The corpus consisted two parts: OCRd scans (1990)(1991)(1992)(1993)(1994)(1995)(1996)(1997)(1998) and digital files (1999)(2000)(2001)(2002)(2003)(2004)(2005)(2006)(2007)(2008)(2009)(2010)(2011)(2012)(2013)(2014). Given the aim of this paper (software presentation) and the shortage of space, we will treat the results only as examples of the method, without getting into too much detail.The work on the prototype was divided into stages, conceived as a feedback loop for the developing team: on every stage a new service was added to application and the test run was performed. After the analysis of the result, the step was repeated or the team moved to the next phase. Phase 1. Cleaning. The OCR-ed corpus has been cleaned (e.g. wordbreaks and headers were removed) Phase 2. The corpus was lemmatized and parts of speech were tagged. Frequency lists were created what enabled the search for patterns in the textual output. For instance, Figure 2 shows the pattern of interest in particular Polish poets throughout 25 years, based on lemmatized mentions.  . Pattern of interest in particular Polish writers inTeksty Drugie ).Phase 3. The analysis of the word frequencies revealed some problems with the word list, especially with numbers, years and city names, which were preserved in bibliographic references. A functionality of adopting a custom stopword list was employed. The exclusion of corpus-specific problematic words and general meaningless words (e.g. a, this, that, if) allowed for visualisation of the most frequent words in Teksty Drugie (Fig. 3) Figure 3. 300 most frequent words from Teksty Drugie ) (meaningless words excluded) visualised with wordle.Phase 4. The texts were then grouped into clusters of 20, 50 and 100 in a series of experiments. Each grouping revealed a bit different level of generalization about the texts. LEM, thanks to visualisation features (Fig. 4), allows for real-time exploration of deeper relationships between the texts. by a spectral decomposition of the graph Laplacianspectral embedding method).By choosing the level of granularity (20, 50 or 100 clusters) we may analyse diverse patterns of discursive similarities between texts. Table 1 shows the differences in clustering of the same sample. The first option (20) shows the similarity between texts on a rather general level, that could be described as stylistic or genre similarity (e.g. formal vocabulary). Other options allow for more detailed exploration of general research approach (50) or particular topics analyzed in articles (100). Semantics of clusters is described by the identified characteristic features. Table 1. Differences between the clustering options (numbers reflect the quantity of texts assigned to particular cluster) Researchers may explore all options and analyse the vocabulary responsible for classifying particular texts into a certain group by a virtue of being over- or under-represented in comparison to the entire sample.The LEM is not a real time system. However, processing of the exemplar corpus (2553 documents from \"Teksty Drugie\") takes less than 20 minutes. This is due to the use of a private cloud and proprietary message-oriented engine for processing texts. We plan to speed up the process, by running larger number of instances of language tools and by compressing results at each stage. Moreover, the user is able to start processing from any stage, so the processing time is shorter when the user plays with different settings. Further DevelopmentCurrently LEM's GUI is developed in cooperation with potential users, literary scholars working on various types of texts (fiction, journal articles, blog posts). That is also why we call this software \"literary\", because further development will address the issues pertinent for literary theory, exceeding a purely linguistic perspective. Some literary-specific issues and functions will be expanded on the later stage of development, e.g. with adding language tools for Word Sense Disambiguation and partial analysis of the text structure, like anaphor resolution and discourse structure recognition. LEM's architecture is open for such extensions. With that said, in this paper we have focused on the current stage of development.LEM will be fully implemented and made available as a web application to the scholarly audience working on Polish. Next, it will be extended with with tools for other languages (e.g. English and German). As LEM has a modular architecture, it would require mostly linking new processing Web Services and adding converters. LEM has an open licences and we will be happy to share our tools, code and know-how with teams interested in doing so. Options for exporting to other formats will be added, so that researchers can easily create the output in a particular format (list, text, table) and upload it to other applications (e.g. Mallet) for further processing. ",
        "article_title": "Literary Exploration Machine: A New Tool for Distant Readers of Polish Literature",
        "authors": [
            {
                "given": "Maciej",
                "family": "Piasecki",
                "affiliation": [
                    {
                        "original_name": "Wrocław University of Science and Technology",
                        "normalized_name": "Wrocław University of Science and Technology",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/008fyn775",
                            "GRID": "grid.7005.2"
                        }
                    },
                    {
                        "original_name": "Polish Academy of Sciences",
                        "normalized_name": "Polish Academy of Sciences",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/01dr6c206",
                            "GRID": "grid.413454.3"
                        }
                    }
                ]
            },
            {
                "given": "Tomasz",
                "family": "Poland",
                "affiliation": [
                    {
                        "original_name": "Wrocław University of Science and Technology",
                        "normalized_name": "Wrocław University of Science and Technology",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/008fyn775",
                            "GRID": "grid.7005.2"
                        }
                    },
                    {
                        "original_name": "Polish Academy of Sciences",
                        "normalized_name": "Polish Academy of Sciences",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/01dr6c206",
                            "GRID": "grid.413454.3"
                        }
                    }
                ]
            },
            {
                "given": "Poland",
                "family": "Maciej",
                "affiliation": [
                    {
                        "original_name": "Wrocław University of Science and Technology",
                        "normalized_name": "Wrocław University of Science and Technology",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/008fyn775",
                            "GRID": "grid.7005.2"
                        }
                    },
                    {
                        "original_name": "Polish Academy of Sciences",
                        "normalized_name": "Polish Academy of Sciences",
                        "country": "Poland",
                        "identifiers": {
                            "ror": "https://ror.org/01dr6c206",
                            "GRID": "grid.413454.3"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " ILIAS Lab, University of Luxembourg, LuxembourgHistorical maps are progressively digitized and added to the inventory of digital libraries. Beside their value as historical objects, such maps are an important source of information for researchers in various scientific disciplines. This ranges from the actual history of cartography and general history to the geographic and social sciences. However, for most of these digital libraries, the available metadata include only limited information about the content of the maps, for example author, title, size, and/or creation date.Whereas given information extraction methods are designed for modern maps and mostly limited to certain types that share similar graphical features, there exist a limited number of tools that rely on a manual recording to visualize certain properties such as distortions as well as support a content-based querying. Examples concern the development of places over time, toponym changes over time, and the identification of the position of places (historical map vs. modern map). This also applies to place markers and text labels, which contain inherent information and so the annotation and geo-referencing of place markers is a crucial task, which can be supported with computer based tools ( Budig and Dijk, 2015, Höhn et al., 2013, Shaw and Bajcsy, 2011, Simon et al., 2011).As already presented in previous contributions (Höhn andSchommer, 2016, Höhn et al., 2013), the Referencing and Annotation Tool RAT supports an identification of place markers in digitized historical maps. RAT facilitates a geo-referencing by suggesting the most likely modern places based on an estimated mapping. The suggestions can be constrained by additional filters, for example by applying a phonetic search (with the Kölner Phonetik) to places, which sound similar to names given on the map. This allows an identification of modern places, whose historic name has changed over time but where its name still is close. RAT performs a template matching algorithm based on the normalized cross-correlation for the identification of place markers. If there are colored place markers in a map, a color segmentation methodology can be used to detect these markers. With respect to the geo-referencing, RAT uses the implemented phonetic search and an estimation of the positions of the place markers. In addition to the original template-based place marker recognition algorithm, we integrated a place marker recognition algorithm based on convolutional neural networks (CNNs; RAT 2.0).For these algorithms, the user is asked to manually annotate a small subset of the map. Regarding the template-based variant, the user has then to select a template for each type of place marker. From the templates, which are manually chosen by the user, the system creates automatically variations; based on the performance -measured with the annotated small subset of the map -, the best performing template variants are then chosen. In the template-matching algorithm, however, the normalized cross-correlation is used as a similarity measure because of its robustness against changes in brightness and contrast.Regarding the detection trough a convolutional neural network, the manually annotated (small) subset of the map is used and split in a training and validation part. This is used to train the network, which has - at this stage - a very basic structure. It consists only of convolutional and pooling layers as presented in Figure 1.Both the template matching approach and the convolutional neural network approach share similar performances. Our tests have shown -for the template matching approach -a detection precision of 98.2% and a recall rate (discovered place markers divided by all existing place markers on the map) of 87.7%. The convolutional neural network approach reaches only a precision of 94.4%, but gives a recall rate of 96.2%. So, there are more place markers found; but the result contains also some more wrong matches in between. Therefore, it depends on the use case, which result is \"better\", but for the manual post-correction it seems easier to check the CNN results for those additional wrong matches then finding the missed matches from the template-based approach.The reason behind the use of the convolutional neural network approach has been an algorithmic limitation of the template matching approach. So far, RAT 2.0 uses only a fundamental convolutional system (at present, there are no additional techniques used, like, for example, data augmentation or pre-training).As a future point, we work on training the convolutional neural network on multiple maps in order to find a classification model that learns the characteristics of place markers and that detects these on unseen maps.  ",
        "article_title": "RAT 2.0",
        "authors": [
            {
                "given": "Winfried",
                "family": "Höhn",
                "affiliation": [
                    {
                        "original_name": "ILIAS Lab",
                        "normalized_name": null,
                        "country": "Luxembourg",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    },
                    {
                        "original_name": "University of Luxembourg",
                        "normalized_name": "University of Luxembourg",
                        "country": "Luxembourg",
                        "identifiers": {
                            "ror": "https://ror.org/036x5ad56",
                            "GRID": "grid.16008.3f"
                        }
                    }
                ]
            },
            {
                "given": "Christoph",
                "family": "Schommer",
                "affiliation": [
                    {
                        "original_name": "ILIAS Lab",
                        "normalized_name": null,
                        "country": "Luxembourg",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    },
                    {
                        "original_name": "University of Luxembourg",
                        "normalized_name": "University of Luxembourg",
                        "country": "Luxembourg",
                        "identifiers": {
                            "ror": "https://ror.org/036x5ad56",
                            "GRID": "grid.16008.3f"
                        }
                    }
                ]
            },
            {
                "given": "Christoph",
                "family": "Lu",
                "affiliation": [
                    {
                        "original_name": "ILIAS Lab",
                        "normalized_name": null,
                        "country": "Luxembourg",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    },
                    {
                        "original_name": "University of Luxembourg",
                        "normalized_name": "University of Luxembourg",
                        "country": "Luxembourg",
                        "identifiers": {
                            "ror": "https://ror.org/036x5ad56",
                            "GRID": "grid.16008.3f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Guidelines of the Text Encoding Initiative are generally recognised in the digital humanities as important and foundational standards for many types of research in the field. The Guidelines of the TEI are generalistic, seeking to enable the largest possible user base encoding digital texts for a wide range of purposes. Working on many TEI-based projects, teaching TEI workshops, and advising researchers on data modelling needs, I have encountered many misunderstandings about the TEI. Indeed, one keynote lecture (not at DH) once told me that \"the problem with the TEI is it has too many tags and there is no way to change it\". Inspired by myths like this, this paper will detail and expose common misconceptions about the TEI -- all of which have been espoused to me at some point -- but will concentrate on the more technical myths in a hope to increase knowledge about the TEI while dispelling some misconceptions along the way. Some of those to be investigated include:\"The TEI is too big (or complicated)\"While there is some truth to this -- the TEI Guidelines are numerous, consisting of around 565 elements -- no single project needs them all. Indeed, the TEI has mechanisms for customisation and recommends doing so to any project. The Guidelines themselves are modular and not all chapters will be appropriate or necessary to read for all projects.\"There is no way to change the TEI\"Although I have heard even well-respected keynote lecturers (not at DH) espouse this belief, it is patently and demonstrably false. This myth arises from unfamiliarity with the fact that the TEI is a framework entirely based on the concepts of adaptability and modification. Not only does the TEI have a sophisticated literate programming methodology to create meta-schemas which subset, constrain, and extend the vocabulary for any individual encoding project, but it also provides a variety of tools to enable users to do so.\"The TEI is too small (or doesn't have <my:SpecialElement>)\"While seemingly the opposite of #1, a frequent complaint made by those unfamiliar with the customization mechanisms of the TEI is that it does not have the special element needed for a particuar encoding project. There is, naturally, a reluctance to add new elements to one's customization --and getting more generalized solutions into the TEI Guidelines themselves is indeed a better solution --however, many new elements are added to the Guidelines through community development across disciplines. Any user is free to add <my:SpecialElement> but generally it is a better idea to get a number of individuals or a special interest group to agree a more detailed proposal. \"The TEI is XML (and XML is broken or dead)\"This idea is usually espoused by those who want to support some other, newer, format. Leaving aside the need some feel to denigrate one format in order to support another, XML is a widely supported format which will be with us for many years to come. However, TEI is not XML -- it is currently serialized as such, but previously it has been serialized as SGML, and in the future it may be expressed in another format(s). While there is currently no other widely adopted format which meets the many and varied needs of the TEI's central format, this does not mean that the TEI cannot be used with many other formats (as input, output, integrated with it).\"XML (and thus TEI) can't handle overlapping hierarchies\"Many people have discussed their concerns of overlapping hierarchies in XML, and while it is true that there are limitations in expressing multiple hierarchies in XML, it also has solutions built into it, such as empty elements to represent one or more alternative hierarchies. Primarily, this misunderstanding is also based on the assumption that all markup is embedded markup. The TEI Guidelines include a chapter on representing non-hierarchical structures, and the TEI framework has many features for representing fragmented element structures, out-of-line and stand-off markup, and the association of additional annotation through URI-based pointing. In addition, many DH text encoding projects only require two hierarchies (e.g. intellectual vs physical representations) and the TEI provides transformation solutions to alternate between these.\"You can't do stand-off markup in XML (or TEI)\"This myth shows a misunderstanding of both XML and the TEI. The former is a language for markup vocabularies and puts no restriction on whether that markup is embedded, out-of-line, or entirely stand-off. The TEI Guidelines provide a number of solutions entirely geared to stand-off markup, and its community is working towards introducing more features in this area. The combination of fine-grained markup, URIbased pointing and/or XPointer schemes, and descriptive markup designed to function this way, means that stand-off markup is supported in the TEI. \"Youcan't get from TEI to $myPreferredFormat\"One of the benefits of XML is that it is easily processable to other formats. The TEI Consortium provides around 40 conversions to/from other formats, including, for example: bibtex, cocoa, csv, docbook, docx, dtd, epub, html(5), xsl-fo, json, InDesign, latex, markdown, mediawiki, nlm, odd, pdf, rdf, relaxng, slides, txt, wordpress, xlsx, xsd, and many more. There exist RESTful web services like OxGarage which can provide a pipeline for these and other conversions.\"There are no tools that understand the TEI\" This is false -- thousands of TEI projects have created many tools which process, mine, convert, and visualize TEI data. While the TEI Wiki lists some of these, one of the problems is that projects do not necessarily advertise and openly share their tools. Much of the software developed by projects is also bespoke and specific --they are not necessarily generalisable to other projects' needs. There are also many sophisticated encoding activities (such as stand-off markup) for which there are few general tools, since these are usually implemented in project-specific methods.\"If you create a TEI-based digital edition you must learn other $tech\"While historically it has been the case that to create TEI-based digital editions one must learn, or employ those who know, various technologies, this is increasingly less of an issue. Out of the box software like eXistdb's TEI Publisher and TEI Boilerplate mean researchers are able to publish digital editions for themselves. Moreover, the TEI has introduced implementation-agnostic methods for documentation of intended processing models in a TEI customization. This can then be used to generate project-specific code based on changes to the customization, as in the case of the eXist-db implementation of the TEI processing model. This new aspect of the TEI enables developers to write more generalized software which relies on the TEI ODD customization file for information on the processing model. \"TEI is only for Anglo/Western works\"There is much about the TEI Guidelines that is based in Anglo and Western European textual traditions, but the Guidelines also make an effort to enable use in other languages and cultures. The definitions and glosses of elements (etc.) can be viewed in a number of languages (English, German, Spanish, French, Italian, Japanese, Korean, Chinese). There is an entire internationalization framework built into the TEI Guidelines and the TEI Customization language, which means that the schemas can routinely display these internationalized definitions in editors and those creating customisations can have definitions, examples, and attribute value descriptions, in any Unicode-expressable language.\"Interoperability is impossible with the TEI\" Interoperability is a good and laudable goal, but the potential richness of TEI encoding for research and analysis purposes should not be sacrificed for this (depending on the point of the initial encoding project). While interoperability does suffer in a framework that is customizable and extendable (which are necessary for such a generalized system), it is certainly possible. Usually it is a process of crosswalks or some scripted transformation to a lowest common denominator that involves someone knowing both resources. The creation of sub-communities (such as the TEI subset EpiDoc), which agree encoding standards that are tighter than the necessarily general and flexible TEI, can improve this significantly.\"The TEI is only for digital edition(s)\"The TEI may be used for many forms of output, for example camera-ready copy. The primary mistake here is to assume a one-to-one relationship between TEI encoded files and a single particular output. If significant encoding has taken place, a wide variety of outputs are possible. If the format is used to its full potential, many aspects of an edition can be created, as well as supplementary files, indices, introductory material, interactive data visualizations, and more. The use of the TEI can also be used outside of editionbuilding, for the creation of linguistic corpora, digital facsimiles, and other resources. SummaryWhile these are only some of the myths surrounding the TEI, discussing these will be beneficial to the DH audience, and will hopefully lead potential TEI users to question other \"received wisdom\" about the Guidelines.  ",
        "article_title": "A World of Difference: Myths and misconceptions about the TEI",
        "authors": [
            {
                "given": "James",
                "family": "Cummings",
                "affiliation": [
                    {
                        "original_name": "University of Oxford",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionYasunari Kawabata was a Japanese novelist who received the Nobel Prize for Literature in 1968. He was famous for his masterpieces such as Snow Country, The Sound of the Mountain, The Old Capital, House of the Sleeping Beauties, and so on. Kawabata had a shattered childhood. He was orphaned at five years old, and his other relatives including his grandfather, grandmother, and elder sister also passed away before he was fourteen. The successive deaths of his loved ones induced mental problems, which became worse in the 1960s. He became addicted to sleeping pills during those years. However, two of Kawabata's masterpieces The Old Capital and House of the Sleeping Beauties were published during his sleeping pills addiction period. These two novels were suspected as having been written by ghostwriters because it was hard to imagine that Kawabata could continue writing novels in his mental condition. There are already some pieces of evidence for the ghostwriter issue of The Old Capital and House of the Sleeping Beauties. Kawabata sent a letter to Sawano before the publication of The Old Capital. In the letter he wrote: \"I have accepted to write a novel about Kyoto; the deadline is approaching but I even don't know how to start. Sawano was surprised when he received this letter. He went to Kyoto to meet Kawabata and gave advice on how to write The Old Capital. Itasaka mentions in his book that The Old Capital and House of the Sleeping Beauties were actually written by ghostwriters (Itasaka, 1997 show strong evidence suggesting the real author of The Old Capital and House of the Sleeping Beauties from a data analysis approach. MethodThe method of this study includes three main steps. Firstly, we digitized more than ten novels of both Kawabata and the three possible ghostwriters. Then, we extracted stylometric features from the novels, and all chapters of The Old Capital and House of the Sleeping Beauties. Finally, we applied the unsupervised and supervised methods to infer the possible author of The Old Capital and House of the Sleeping Beauties.We used bigrams of characters and punctuation marks, part-of-speech bigrams, and phrase patterns as stylometric features, which have been proven useful in Japanese authorship attribution (Matsuura and Kanada, 2000;Jin, 2003Jin, , 2013.Bigrams of characters and punctuation marks are pairs of two adjacent characters or punctuation marks extracted from plain text. Japanese texts should be tokenized previously for the extraction of part-of-speech features. We applied the Japanese morphological analyzer called MeCab to separate a Japanese sentence into morphemes. MeCab outputs the parts-of-speech of words in several layers. Deeper layers process more detailed information. In this study, we use information from the first layer. As an example, part-of-speech bigrams in the Japanese sentence \"Ronbun wo kaku.\" are \"Noun_Particle,\" \"Particle_Verb,\" and \"Verb_Period.\"Phrase pattern is a powerful feature that can be extracted in terms of syntax. A Japanese parser (CaboCha) was introduced to separate sentences into phrases. Phrase pattern is defined as the smallest unit that divides the sentence into unnatural parts (Jin, 2013). It is a combination of two parts. One is the original form of the particles and punctuation marks, while the other is the parts-of-speech of the other materials, except for the particles and punctuation marks in the same phrase. The two phrase patterns in the sentence \"Ronbun wo kaku.\" are \"Noun_wo\" and \"Verb_Period.\"We applied unsupervised methods and the integrated classification algorithm in this study. The idea of the integrated classification algorithm was to combine the results of several stylometric features and classifiers. It achieved highest classification accuracy in authorship attribution of literature (Jin, 2014). The integrated classification algorithm combines the results of stylometric features and classifiers to avoid the bias under a majority vote rule. AdaBoost (ADA), High-dimensional Discriminant Analysis (HDDA), Logistic Model Tree (LMT), Random Forest (RF), and Support Vector Machine (SVM) were used as the base classifiers. ResultsThe result of House of the Sleeping Beauties reveals that compared to Mishima, all chapters of House of the Sleeping Beauties are more likely to be written by Kawabata. The result in the classification between Kawabata and Hokujyo shows that the writing style in all chapters of The Old Capital is more like Kawabata's.). The Old Capi- tal may have been written by Kawabata's three disci- ples whose names are Hisao Sawano, Makoto Hokujyo, and Yukio Mishima. House of the Sleeping Beauties may have been written by Yukio Mishima. In this study, we ",
        "article_title": "Ghostwriter identification in Yasunari Kawabata's works in the 1960s",
        "authors": [
            {
                "given": "Hao",
                "family": "Sun",
                "affiliation": [
                    {
                        "original_name": "Doshisha University",
                        "normalized_name": "Doshisha University",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/01fxdkm29",
                            "GRID": "grid.255178.c"
                        }
                    }
                ]
            },
            {
                "given": "Mingzhe",
                "family": "Jin",
                "affiliation": [
                    {
                        "original_name": "Doshisha University",
                        "normalized_name": "Doshisha University",
                        "country": "Japan",
                        "identifiers": {
                            "ror": "https://ror.org/01fxdkm29",
                            "GRID": "grid.255178.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " OverviewWellcome Collection is one of the world's major resources for the study of health and histories. Over the past few years Wellcome have been developing a world-class digital library by digitising a substantial proportion of their holdings. As part of this effort, approximately 5,500 Medical Officer of Health (MOH) reports for London spanning from 1848-1972 were digitised in 2012. Currently Wellcome holds the most comprehensive digital collection of the London MOH reports. Since September 2016 Wellcome have been digitising 70,000 more reports covering the rest of the United Kingdom (UK).The MOH reports were published annually by the Medical Officers of Health employed by local authorities across the UK. These reports provided vital statistics and a general statement on the health of the population. MOH reports concentrated on reporting infectious diseases and resolving the problems as well as covering other areas of social responsibilities. (Chave, 1987) They have been long regarded as an important source for the 19 th and 20 th century history of Public Health and stem from reaction to infectious disease in the mid-19 th century. Although there were attempts at standardisation, the reports display each MOH's interest, idiosyncrasies and particular strengths. Therefore, they also provide a particular perspective on the everyday lives of Londoners over several generations. No digital techniques have yet been applied successfully to add value to this very rich resource.As part of the Smelly London project, the OCR-ed text of the MOH London reports has been text-mined using the Python programming language. Through text mining we produced a geo-referenced dataset containing smell categories for visualisation to explore the data. At the end of the Smelly London project the MOH smell data will also be available through other platforms such as Good City Life and Layers of London. This will allow the public and other researchers to compare smells in London from the 19th century to present day. This has the further potential benefit of engaging with the public. This is a collaborative, interdisciplinary project which will allow us to enhance and demonstrate the capabilities of innovative text mining tools we design to allow the automatic extraction of information from OCR-ed text. This paper presents the intended aims of the project; how this was achieved; an analysis of the findings; an interactive map of the results and a browser game of smells and disease. Data and visualisationAs Roy Porter famously remarked that \"todays history comes deodorised\", sensory history is a relatively new historical approach. Historians rarely provide us an opportunity to hear, taste or smell the past. Medical historians have incorporated some aspects of sensory history into their research and explored the past belief that bad smells were causes of disease. However, there is very little research carried out covering this period.Furthermore, smell has a great influence over how we perceive places and contributes to the construction of a place's identity ( Quercia et al., 2015). During the 19 th century the paranoia surrounding smells associated with poor hygiene heightened in many European cities (Reinarz, 2014). The Great Stink of 1858 resulted in the discussion of moving Parliament outside London for example. Despite the rise of germ theory (Pasteur and Koch) in the 1880s, concerns with disease-causing miasma (smells) did not disappear entirely. The MOH reports are one of the richest available sources on local public health administration and patterns of disease.We enriched the text-mining pipeline with Natural Language Processing (NLP), including lemmatisation and part-of-speech tagging. The first iteration of the project has a feature to identify the category of the smells found by using a mapping table to work out the most common smell types. This step complements the close reading analysis and enables us to scale up the amount of information extracted from the texts. Our next research plan is to work on automatic identification of smell terms based on their contextual features to discover new categories that escaped previous classifications. This will allow us to identify smell categories in a data-driven fashion.As the data becomes more structured, they can be more readily overlaid with other maps and images such as Charles Booth's London Poverty Map and 19th century disease maps. Having multiple layers will enable us to run various comparisons and assess if there are any correlations between smells and diseases as well as links to the socio-economic identity of areas in London.During the first phase of the project we created a smelly map based on the number of smell hits to visualise the first set of results. Figure 1. Smelly Map of London showing all smellsFrom the list of the existing London local authorities for the MOH reports we compiled, the geographic coordinates of present-day equivalents were extracted using an API. For the places that did not exist in the API, we manually added the geographic coordinates from Wikipedia. On the map each of the points marks the number of smells occurring at the centroid of each of the locations. We grouped the number of smells into sets of ten (e.g. 1- 10, 11 - 20) to avoid having giant points on the map for the places where there are almost 100 smells recorded. Finally, the map scrolls through the years. The data displayed in the mapping visualisation was obtained using text-mining via Python scripting. Python was the language of choice due to its high productivity rate and the fact that there are a large amount of third party libraries that offer highly useful functionality with just a few lines of code. For example, NLTK is a popular Python set of libraries that can achieve advanced NLP.The next generation of map we produced during the second phase displays different smell categories that are colour-coded. The smell categories used for this map are Sewer; Waste-rubbish; Waste-excrement; Thames; Water; Food; Trade; Animal; Factory-fuel; Disinfectant; School; Air; Decomposition; Habitation;and Absence of smell. These categories were obtained through manual inspection of the data produced from searching for sentences containing smell-related words. In our codebase, we first analysed 5500 MOH London reports to find sentences that contained smell related words. Once the sentences were further analysed and categorised manually, the results were stored down into a local database by year, borough and a unique ID programmatically. Figure 2. Smelly map of London showing smell categoriesComputer programming can be used to perform tasks thousands of times faster than humans. In the Python code written to extract the data from the MOH reports, parallel processing was employed to speed up the running time of the program. Inside a computer there is a CPU which runs the tasks given by the program. Modern CPUs have multiple cores which allows the calculations to be run concurrently. In our project the CPU had four cores which allowed the running time of the program to be shortened by as much as three times. The next objective for the project is to scale up the size of the text-mining from 5,500 reports to over 70,000 reports covering the entire UK. In order to process such large datasets we are investigating the use of distributed computing resources such as Amazon Web Service (AWS). The code written for this project has been made open source under the MIT license along with documentation so that other programmers or researchers can use the codebase in their own text mining projects. The code has already been used in another project at Wellcome to investigate the idea of women's right to work during the 19 th and 20 th century London VisionThe Smelly London project aims to bring together historical data with modern digitisation and visualisation to give us a unique, revealing and visceral glimpse into a London of the past and what it tells us about London today. Analysing the MOH reports tells the intimate narratives of the everyday experiences of 19th and 20th century Londoners through the 'smellscape'. The Smelly London project provides a great opportunity to demonstrate how new knowledge and insights have risen from the use of powerful digital applications. This project will produce models that facilitate new kinds of humanities research. All outputs generated from the project will be open access and open source. Our data is available in a public repository on GitHub and other platforms.   ",
        "article_title": "Smelly London: visualising historical smells through text-mining, geo- referencing and mapping",
        "authors": [
            {
                "given": "Deborah",
                "family": "Leem",
                "affiliation": [
                    {
                        "original_name": "Wellcome Trust",
                        "normalized_name": null,
                        "country": null,
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " SimulatorBabbage's Analytical Engine remains unconstructed, but today we can simulate the execution of programs on it digitally, based on the detailed accounts of the Engine's design provided by Babbage and Lovelace. As an experiment in hearing what Lovelace might have imagined, we coded an algorithm to run on a simulator in order to generate number sequences which are then mapped to instruments. The numbers are strictly faithful to nineteenth-century mathematics. Human intervention decides the algorithmic parameters and the mapping of the numbers to notes and instruments, to be explored based on the musical context of the time. This experiment led to the creation of the Numbers into Notes web site, making the tools available to others for investigation and composition. PerformanceEmily Howard has also responded creatively to Note A, and the life of Lovelace, in composing her Lovelace Trilogy (Petri-Preis, 2013). 'Ada sketches' is a short operatic work for mezzo-soprano, flute, clarinet and percussion. Howard's time as Composer in Residence at the University of Liverpool's Department of Mathematics, and her work there with Lasse Rempe-Gillen, led to this piece which has been performed in formats that encourage audience response and participation. A performance at the University of Oxford as part of celebrations to mark Lovelace's 200th birthday investigated the audience's reception of the work. Its most recent performance, at the Royal Northern College of Music, used our Numbers into Notes web application. Digital-physicalThe Numbers into Notes software invites a thought experiment: had Lovelace lived longer, and had Babbage successfully built the Analytical Engine, what might have happened in pursuit of Lovelace's musical observation? We extended this thought experiment to ask \"what might Lovelace do today?\" To explore this, we constructed multiple physical devices (based on the Arduino open-source electronic prototyping platform) to re-enact the algorithms designed for the Analytical Engine (De Roure, 2016b). Today Lovelace could combine multiple machines, and the computational power would enable real-time synthesis, putting into practice the mathematical notions of consonance that were established in the eighteenth century. Experimental humanitiesThis approach, which we suggest might be framed as experimental humanities, has attracted engagement during events and online, and has also been a successful vehicle in teaching (in the Social Humanities strand of the Digital Humanities at Oxford Summer School). Our work uses digital tools, co-designing digital and digital-physical artefacts, to explore and re-imagine prospective and theoretical technologies of Lovelace's day. Through this we provoke new responses and discoveries relating to music practice and performance, and to the philosophy and history of technology. The practice of this experimental humanities approach enables critical reflection and re-interpretation: we suggest that the digital artefacts we have produced are each interpretations drawing on the life and writing of Lovelace, and the value of this practice lies in the new insights and works they inspire.This paper recounts these experiments that play at once into generative design and into alternative histories of algorithms and mechanisms. Through making, through prototyping and co-design, we close-read the thought processes Lovelace and Babbage recorded. We point to paths in the development of computing and programming that were not taken, and extend beyond what was practicable in the nineteenth century. Our work also touches on creativity, as anticipated by Lovelace and recast in the \"Lovelace questions\" (Bo- den, 1990), and manifest today in the fields of computational creativity and creative computing.  ",
        "article_title": "Numbers into Notes: digital prototyping as close reading of Ada Lovelace's 'Note A'",
        "authors": [
            {
                "given": "David",
                "family": "De Roure",
                "affiliation": [
                    {
                        "original_name": "Oxford University",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Pip",
                "family": "Willcox",
                "affiliation": [
                    {
                        "original_name": "Oxford University",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionStudies on user experience in the digital medium are often related to Human-Computer Interaction (HCI) and the construction of user models or the performance of usability tests in order to support design and evaluation of digital artefacts. User modelling research has mainly focused on the construction of \"usable\" and \"useful\" tools providing the users with \"experiences fitting their specific background knowledge and objectives\" (Fischer, 2001: 65). A variety of characteristics have been used to inform such models, from demographic information (age, gender, native language) or relevant experience (novice, advanced, expert), to interests, goals and plans (general interest categories, task-related objectives/sequences of actions) or contextual information (location, time, physical environment) (Sosnovsky and Dicheva, 2010: 33- 34). Many of the approaches merge cognitive science and artificial intelligence ( Webb et al., 2001;Biswas and Robinson, 2010;Mohamad and Kouroupetroglou, 2013), whilst usability testing, as a technique from user-centred design, often involves the iterative refinement of a prototype based on user's feedback (Massanari, 2010). Usability studies also evaluate how a tool is actually used (Brown and Hocutt, 2015) exploring constructs such as ease of use and learnability. Other researches, from the fields of philosophy of technology or digital hermeneutics, go beyond the usefulness and usability aspects of the technology, trying to address questions related to the \"human, social, cultural, ethical, and political implications of those technologies\" (Fallman, 2007: 296) or to the \"self-interpretation of human beings\" (Capurro, 2010: 10) in the light of the code. Further directions of study propose a re-orientation of the HCI as \"an aesthetic field\" (Ber- telsen and Pold, 2004: 23) or a cultural perspective on the \"reflexive relationship between user and medium\" as a \"remediation\" of the self (Bolter and Grusin, 2000: 230), considered as \"humanistic HCI\" ( Bardzell and Bardzell, 2016).This article tries to bridge the fields of HCI and Digital Humanities (DH), where HCI techniques are used to evaluate tools developed in DH projects and the results of this evaluation are analysed via DH methods, with the intention of potential development inspired by the literary theory of aesthetic response (Iser, 1980). The paper elaborates on previous work (Armaselu and Jones, 2016) and presents two case studies of usability tests conducted within the framework of interface and game design for digital historical editions and digital cultural heritage (Section 2). Section 3 describes the type of analysis applied to users' responses, whereby we propose a typology of users and interpretation of users' experience, followed by conclusions and future work (Section 4). Two case studiesThe first case refers to the design and implementation of an XML-TEI-based platform (Transviewer) allowing the exploration of digital editions of historical documents through features for page-by-page navigation, side-by-side view (facsimile/transcription), freetext and named entities search. The usability tests inspired by previous studies (Nielsen, 2000;Jones and Weber, 2012) involved a user-group of 8 researchers in history, political science and linguistics, 4 males and 4 females, aged 25-64. They had to complete 17 tasks using the prototype and to fill-in a USE-based questionnaire (Lund, 2001). During the experiments, the users were asked to think-aloud and the audio and screen interactions were recorded. The common language was English, although none of the participants was a native speaker.The second case uses data collected during three sessions of gameplay conducted as part of a requirements gathering and co-design process for Pilot 4 of the H2020 Crosscult project. Players were asked to play a board game and contribute reflections as they encountered historical objects pinned to various locations in the city (the Board was derived from a map of Luxembourg City). The first session contained 6 players (5 females, 1 male), the second 5 players (all female) and the third 5 players (4 males, 1 female). All players, aged 25 to 50, worked in a research environment, and none of the participants used English as his/her mother tongue. In the first session, players had 10 roles of the dice to score as many points as possible in successive rounds, the game was shortened so players had to score the most points and reach the end of a score board first. Analysing user responseFor both cases, the users' responses from the questionnaires were transcribed, when not already in electronic form. Partial transcription of the think-aloud audio recordings for the first case was performed (reflections on the experience, improvement suggestions, expressions of disorientation or frustration); the transcription of the second case video-audio recordings is not yet completed, therefore not included in the study. The transcribed snapshots were pre-processed (TXT, XML, R) according to the formats required by the analysis phase. Three types of software were used: Textexture -a tool for representing the text as a network (Paranyushkin, 2011); TXM - a statistical tool for corpus analysis; TheySay - a sentiment analysis package.The first experiment with Textexture drew attention to noteworthy connections between different clusters of meaning related to users' experience as expressed in their responses. Figure 1 presents two examples: the first highlights how the notion of trust is related to the side-by-side view feature of the interface, as allowing the users to compare the transcription with the scanned original and make sure it can be trusted (left); the second illustrates the linking of the sub-networks for player (reflection, discussion, exchange, opinion), place (location, malta, luxembourg) and story (card, map, point), which reveals the relations, at a conceptual level, between the significant features and interactions of the game. TXM allowed contrasting the specificities scores (Lafon, 1980) corresponding to each user, in terms of overuse/deficit of words usage, as compared to the rest of the corpus. Table 1 shows the positive/negative specificities diagrams based on these measures for three groups of linguistic features. The scores above/under a banality threshold (+/-2.0) indicate highest specificity for responses from particular types of respondents, which allowed us to make hypotheses about a potential \"typology\" of users that can be described within both case studies. For instance, some users are characterised by an overuse of I, my or you, your, others by an alternation of them, which can create the impression of an \"immersive\", \"distant\" or \"versatile\" point of view: \"Which I found strange. Yes, I have not yet used the big arrow buttons\", \"if you scroll, you have to scroll both\" (Transviewer); \"prefer to elaborate my own answer, without influence\", \"I think it triggers your own thinking process\" (Crosscult). Similarly, the use of conditionals, negations and uncertainty adverbs are suggestive of a \"sceptical\" user, in contrast to experiences described with appreciative adjectives and superlatives indicative of an \"enthusiastic\" standpoint.After exploring the results in TXM and identifying possible types of users, we analysed the responses via TheySay (Table 2). The results enabled us to explore differences in sentiment between the types of users. For example, the \"enthusiastic\" user from both experiments scores highly with respect to the measure of positive polarity, whilst the sceptical user scores are a bit lower but, interestingly enough, higher than the immersed user's.It was also observed that sometimes, irrespective the type of user, sentences with high score for humour may actually point to interaction-related aspects like disorientation, confusion, contrariety: \"I was ... where was I?\", \"I clicked on people but I don't know what happened\" (scores 0.996 and 1, Transviewer); \"I've never been in the flow because I can't focus on other gamers\", \"didn't use any, but I don't think I would\" (scores 0.996 and 1, Crosscult). Conclusion and future workThe paper describes two case studies in interface and game design dealing with the application of textual analysis to user-response via three systems, for visualisation of the text as a network (Textexture), corpus analysis (TXM), and sentiment analysis (TheySay). The research is still in progress and more experiments with new cases are expected to further support, test and validate the proposed user typologies and interpretation modalities, which might in the future inform humanistic interface design and approaching of user models. In addition, we expect to explore the theoretical matters, assuming that this kind of analysis, beyond its usability-oriented value, may inspire new paths of reflection on user's self-projection in the digital space, at the intersection of digital hermeneutics, digital aesthetics, and the theory of literary response. ",
        "article_title": "From Usability Testing and Text Analysis to User- Response Criticism",
        "authors": [
            {
                "given": "Florentina",
                "family": "Armaselu",
                "affiliation": [
                    {
                        "original_name": "University of Luxembourg",
                        "normalized_name": "University of Luxembourg",
                        "country": "Luxembourg",
                        "identifiers": {
                            "ror": "https://ror.org/036x5ad56",
                            "GRID": "grid.16008.3f"
                        }
                    }
                ]
            },
            {
                "given": "Catherine",
                "family": "Jones",
                "affiliation": [
                    {
                        "original_name": "University of Luxembourg",
                        "normalized_name": "University of Luxembourg",
                        "country": "Luxembourg",
                        "identifiers": {
                            "ror": "https://ror.org/036x5ad56",
                            "GRID": "grid.16008.3f"
                        }
                    }
                ]
            },
            {
                "given": "Catherine",
                "family": "Lu",
                "affiliation": [
                    {
                        "original_name": "University of Luxembourg",
                        "normalized_name": "University of Luxembourg",
                        "country": "Luxembourg",
                        "identifiers": {
                            "ror": "https://ror.org/036x5ad56",
                            "GRID": "grid.16008.3f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionSince Kuhn's distinction between \"textbook\" science and \"article\" science, studies of scientific texts have established the social nature of academic writing. Analysis of scientific texts can yield epistemological, disciplinary, and historical insights. These texts are the arena in which knowledge claims are raised (My- ers 1985), trials of strength held (Latour 1987), intradisciplinary boundaries drawn (Wolfe 1990), disciplinary histories traced (Bazerman 1988). Genres of scientific texts have thus been shown as socially enacted structures (Berkenkotter, Huckin, Ackerman 1994) rather than as transparent styles. Despite their many merits, the hitherto available empirical studies on scientific writing have been constrained by either a focus on early history (such as the original Philosophical Transactions of the Royal Society) or reductive sampling for traditional content analysis that allow researchers to grasp the otherwise immense textual data. Approaches inspired by digital humanities approaches offer new possibilities of studying disciplinary formations, as was demonstrated by Goldstone and Underwood (2014) in their distant reading of literary studies texts. This paper follows suit in reporting the results of applying digital humanities methods of text analysis to the corpus of research articles in sociology, specifically, in Czech Sociological Review. Writing in wide-scope disciplines, such as sociology, is of particular interest because it embodies the conflict between literature and the notion of social science (Lepenies 1988). Sociology has been revealed as a discipline of two writing cultures, monographic and journal (Wolfe 1990). The writing in sociology is also oscillating between the aspiration to the positivist ideal of science (Leenhardt 1992) and the acceptance of diverse styles (Agger 2002). Abbot and Barman (1997) have concluded, on the basis of sequence comparison, that research articles in sociology lack \"rhetorical rigidity\". The discipline thus offers a particularly opportune resource for the analysis of genre and topical variations. Czech Sociological Review was chosen as an example of a \"core\" journal in the country. As Oromaner (2008) demonstrated, \"core journals\" in sociology have tendency to become central to the discipline's \"intellectual integration\". Thus the results of the analysis can be taken as indicative of the mainstream tendencies in Czech sociology. The focus on the Czech sociology has the additional advantage of representing a interesting example of a discipline undergoing substantial transformation in the wake of academic and wider societal changes that came about with the fall of the Communist Party regimes in 1989. Also, the journal offers open access to its content. The data for the analysis were scrapped from the journal's website in September 2016 and the resulting data set contains 3483 articles. A preliminary exploration of metadata revealed noteworthy patterns around the year in which a new policy for science evaluation had been introduced (cf. the figure).The data collection and analysis is carried out using R language. Besides crude measures of the corpus, the paper will also report the analysis of the textual data, using text mining techniques to comment on the issues that have been raised in the available literature. Topic modeling through LDA model will be used to assess the topical changes across time. Annual frequencies of particular words will be used as indices of changes of the transforming disciplines (this includes, especially, the words relating contemporary sociology to its \"communist\" variety, such as references to Marx or \"communism\"). Multidimensional scaling will then be employed to reveal term clustering around further keywords that arguably important in sociology. Quantitative bias, or a lack of thereof, will be measured by the presence of numbers. The overall purpose of the analysis is to address the questions raised in pre-existing literature using the specific example of a Czech social science discipline and to demonstrate the usefulness of text mining techniques in the analysis of scientific writing.   ",
        "article_title": "Topics and genre changes in Czech sociological articles",
        "authors": [
            {
                "given": "Radim",
                "family": "Hládik",
                "affiliation": [
                    {
                        "original_name": "Czech Academy of Sciences",
                        "normalized_name": "Czech Academy of Sciences",
                        "country": "Czechia",
                        "identifiers": {
                            "ror": "https://ror.org/053avzc18",
                            "GRID": "grid.418095.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Archival finding aids (also called collection guides) are meant to enhance access to archival collections for researchers, and have been presented online for almost two decades. However, researchers still struggle to understand and use them, and the poor functional-ity of finding aids can often impede the research process. Online finding aids frequently violate many tenets of basic web usability by presenting large blocks of text, complex collection hierarchies, and connections between relevant materials in arcane and unintuitive formats. Many scholars in the humanities have struggled with archival discovery and navigation systems in the course of their research or teaching; cumulatively, these individual annoyances add up to significant interference in the production of humanities scholarship in the humanities. Ciaran Trace and Andrew Dillon have argued that user problems with archival finding aids may be rooted in the system of power inherent in these tools. According to Trace and Dillon, the traditional archival finding aid has always \"reflected, privileged, enabled, and given control to the writer (archivist) more so than to the receiver (researcher)\" (Dillon 2012). Users of online finding aids are merely receivers of the information , and in many systems have no control over how the information is presented, or even what information is presented. As a static document, the finding aid limits interpretive possibilities and aggregate analysis , because it cannot be (re)configured to meet us-ers' research needs. Passive, search-based finding aid systems hinder researchers' potential for creativity, and obscure opportunities for serendipitous discovery. Digital humanities modalities suggest compelling ways to disrupt this power paradigm. Johanna Drucker has called for \"knowledge-generating visuali-zations,\" which empower the user to produce new interpretations and arguments through manipulation and augmentation of the data presented (Drucker 2014). Tim Sherratt has advocated for the development of digital collection interfaces that enable users to visualize, augment, problematize, and critique collections and collections data (Sherratt 2011). These appeals invite questions about how users might respond to finding aids that present archival information in newly visual ways. What happens for the user when archival finding aids are stretched beyond traditional modalities to invite new interpretations of collections? This presentation will introduce multiple models for including visualizations in both individual finding aids and discovery systems. Using case studies derived from pilot projects at Oregon State University's Special Collections and Archives Research Center (SCARC), the presenters will discuss models that allow the researcher (and the archivist) to compare, match, recognize , distinguish, arrange, combine, construct, and organize data across a constellation of data points in ways that traditional, textual finding aids cannot. Models discussed will include proof-of-concept designs for interactive geographic timelines, force-directed network graphs, circle packing units, cluster force designs , alluvial diagrams, and treemaps that enable \"distant reading\" of a finding aid or corpus of finding aids. These visualizations allow users to identify patterns over time or space, relationships between collections , proportions between categories of data, and more. The presenters will explore how these visuali-zations can impact every stage of the research process, including topic exploration, identification of relevant archival collections, and establishing context and background.",
        "article_title": "Data Visualization in Archival Finding Aids: A New Paradigm for Access",
        "authors": [
            {
                "given": "Anne",
                "family": "Bahde",
                "affiliation": [
                    {
                        "original_name": "Oregon State University",
                        "normalized_name": "Oregon State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00ysfqy60",
                            "GRID": "grid.4391.f"
                        }
                    }
                ]
            },
            {
                "given": "Cole",
                "family": "Crawford",
                "affiliation": [
                    {
                        "original_name": "Oregon State University",
                        "normalized_name": "Oregon State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00ysfqy60",
                            "GRID": "grid.4391.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Stephan Risi risi@stanford.edu Stanford University, United States of AmericaWhat happens when researchers have access to more documents than they could read in a lifetime? As a result of litigation, historians of tobacco have access to over 14 million formerly secret tobacco industry documents, containing incriminating internal memos and research reports but also newspaper clippings and consumer letters (UCSF Library and Center for Knowledge Management). Historians have used this treasure trove to document widespread fraud and systematic deception of smokers by the tobacco industry (Brandt, 2007;Proctor, 2011). However, industryfriendly historians and tobacco lawyers have started to rewrite some this history by claiming that smokers always knew that smoking is addictive and causes cancer (Brandt, 2007;Proctor, 2011). Usually, these claims rest on a few, well selected documents that support a particular industry claim. Robert Proctor has called processes like this \"agnotology,\" the cultural production of ignorance (Proctor, 2008). Indeed, the arguments of both pro- and anti-tobacco industry historians rely on the same corpus of data: an immense amount of publicly available and full-text searchable documents. Given 14 million documents, there will be some supporting almost any claim.In this paper, I present one way to counter such agnotological assertions by studying broad trends across millions of documents with frequency analyses. In particular, I counter the claim that smokers always knew that smoking was addictive, an argument often made by tobacco lawyers in court to assign full responsibility to the smoker (Henningfield, Rose, & Zeller, 2006) To refute this assertion, I use frequency analyses with a validation measure to show that smoking only became widely understood as an addiction in the late 1980s and early 1990s, when scientists recognized that the same neural pathways were involved in dependence to both nicotine and harder drugs like heroin and cocaine. This inscription of addiction into the brain replaced older explanations of why people smoke, like personality traits or an oral fixation. Ultimately, I trace how the neurological understanding of nicotine addiction moved from research laboratories to the public: it led to the Surgeon General's warning labels; it enabled smokers to seek out new nicotine replacement therapies; and it made it possible for smokers to successfully sue the tobacco industry for the first time.Frequency analysis, popularized by the Google Ngram Viewer, is one of the simplest mathematical tools in the arsenal of the digital humanities ( Michel et al, 2011). By calculating the usage frequency of terms and expressions over time, it enables users to get a sense of when a term became more or less important. The mathematical simplicity confers it an important advantage: it scales very well not just to thousands but to millions of documents. For this study, I used all 14 million documents (about 10 billion tokens) dated between 1940 and 1998 to create a publicly available website (www.tobacco-analytics.org), where users can create their run their own frequency analyses, akin to Google ngrams. The main drawback of this method is that the patterns found in the graphs of the Google Ngram Viewer are hard to validate: Does a spike in a particular year represent a statistically significant event or is it just a fluke? Is it caused by 10 or 1000 documents? I address this problem in two ways: First, I allow users to display the absolute number of appearances of a term by year to give them a sense of the number of documents that cause a spike. Second, I am developing a comparison statistic to calculate z-scores using the Corpus of Historical American English (COHA) (Davies, 2010). By comparing frequencies between the tobacco documents and the reference corpus (COHA), it allows me to calculate when frequencies in the tobacco corpus deviate in a statistically significant way (Darwin, 2008, p. 208-222). Given, for example, the above graph of the relative frequencies of the term \"nicotine addiction,\" z-scores can be used to show that the relative frequencies only started to deviate significantly from the comparison corpus in the 1980s.The tobacco documents provide us with an opportunity to think through the problems that come with access to millions of secret documents. What if millions of dollars in settlements hinge on historical arguments? What if there are immense financial incentives to make false historical claims: to present narratives that are borne out in a few well selected documents, but which misrepresent the corpus as a whole? In the realm of tobacco, historical arguments and knowledge circulate far outside of academia in courtrooms to sway juries or in policy documents to change legislation. The immense size of the tobacco documents archive makes it possible to find a few documents supporting almost any claim. Findings from one group of documents can cancel out the findings from other documents; statements by one expert discredit those of another one. In these cases, quantitative analyses using the whole corpus can be an arbiter of these claims. They are not sufficient to advance historical arguments in themselves, but they can be used to test and disprove hypotheses made on the basis of a smaller set of documents. The Tobacco Analytics project makes powerful digital humanities tools available to tobacco researchers who may not have a technical background, and it allows historians to trace developments within the tobacco industry by examining the whole corpus with the click of a mouse.",
        "article_title": "Ngrams Against Agnotology: Combatting Tobacco Industry Narratives About Addiction Through A Quantitative Analysis Of 14 Million Documents",
        "authors": [
            {
                "given": "Stephan",
                "family": "Risi",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionIn previous articles (di Lenardo et al, 2016;Seguin et al, 2016), we explored how efficient visual search engines operating not on the basis of textual metadata but directly through visual queries, could fundamentally change the navigation in large databases of work of arts. In the present work, we extended our search engine in order to be able to search not only for global similarity between paintings, but also for matching details. This feature is of crucial importance for retrieving the visual genealogy of a painting, as it is often the case that one composition simply reuses a few elements of other works. For instance, some workshops of the 16th century had repertoires of specific characters (a peasant smoking a pipe, a couple of dancing, etc.) and anatomical parts (head poses, hands, etc.) ,that they reused in many compositions (van den Brink, 2001;Tagliaferro et al, 2009). In some cases it is possible to track the circulation of these visual patterns over long spatial and temporal migrations, as they are progressively copied by several generations of painters. Identifying these links permits to reconstruct the production context of a painting, and the connections between workshops and artists. In addition, it permits a fine-grained study of taste evolution in the history of collections, following specific motives successfully reused in a large number of paintings.Tracking these graphical replicators is challenging as they can vary in texture and medium. For instance, a particular character or a head pose of a painting may have been copied from a drawing, an engraving or a tapestry. It is therefore important that the search for matching details still detects visual reuse even across such different media and styles. In the rest of the paper, we describe the matching method and discuss some results obtained using this approach. MethodMatching patterns in a bank of images is a problem that has been extensively studied in the Computer Vision community as « Visual instance retrieval » ( Sivic and Zisserman, 2003). The definition of the task is : given a region of a query image, can we identify matching regions in other images of a large iconographic collection ?Historically, the most successful methods were based on feature point descriptors (like SIFT, see Lowe, 2004) used in a Bag-of-Words (Jegou et al, 2008) fashion. The global architecture can be summarized as follows:For each image in the collection:• Extract the feature points for the image.• Quantize the point descriptors to a VisualBag-of-Words representation.Given a query region:• Use the Bag-of-Words signatures to rank the first N most likely candidates of the collection.• For each candidate, re-rank them according to a spatial verification of the matching points with the query region.In practice, such an approach works extremely well and numerous improvements (Shen et al, 2009;Crow- ley and Zisserman, 2014) have been brought over the years to the different steps of the procedure. Hence it is still considered state of the art for the traditional datasets which focus on building and object retrieval in photographs. However, it was shown recently ( Seguin et al, 2016;Crowley and Zisserman, 2014) that such approaches break completely when not dealing with the same physical objects and important style variations like we do in paintings. An example can be seen on Figure 2. A CNN is a multi-layer architecture where each layer transforms its input according to some parameters (also called weights). What makes them so powerful is that all these parameters can be learned « endto-end » (for example in the case of object classification, just with images and their corresponding labels). It has been shown in Donahue et al (2014) that CNN pretrained on very large datasets, like Deng et al (2009), for object classification tasks offer a very good abstract representation of the image information, and are thus applicable for other vision problems. They generalize much better when transferring from photographs to paintings, contrary to the traditional Bag-ofWords techniques ( Seguin et al, 2016;Crowley and Zisserman, 2014). However, its application to visual instance retrieval was always hindered by the fact that they traditionally output a single global descriptor for the image, hence not directly allowing for region (subimage) retrieval ( Babenko et al, 2014).In Razavian et al (2014), the authors proposed to just precompute the CNN descriptors for some subdivision of the image. However, such approach limits the possible granularity of the windows, and multiply the memory requirement by a huge factor.Another more promising approach introduced in Tolias et al (2015) is to work directly on the CNN feature maps. More precisely, a common way of extracting a CNN descriptor for image retrieval is to take an image of size (H, W, 3) (height of H pixels, width of W pixels and 3 color channels RGB) go through all the convolutional layers of CNN, which outputs the feature maps : a structure of size (H', W', F) (F channels of size H' and W'). From these feature maps, taking the sum (or the max) of each channel gives a signature of size F which is (after normalization) the descriptor of the image.Traditionally, the network used is the VGG16 (Simonyan and Zisserman, 2014) architecture which given an image of size (H, W, 3) creates feature maps of size (H/32, W/32, 512). Now, starting from the feature maps, computing the signature of a sub-part of an image is already easier, we just need to compute the sum of the corresponding region in the feature maps to obtain the descriptor of size F. However, evaluating many different regions would still be prohibitive from a computation point of view.In order to alleviate the performance problem, Tolias et al (2015) proposed to use integral images. Given an image I, the integral image í µí°¼ ∫ is í µí°¼ ∫ (í µí±¦, í µí±¥) = í µí°¼(í µí±—, í µí±–)+,-,.,/. This allow for extremely quick computation of the sum of an image for a given area (í µí±¦ 0 , í µí±¦ 1 , í µí±¥ 0 , í µí±¥ 1 ) (Fig.5) : This trick allows for extensive evaluation for the best matching window for the query image in the target collection. The global procedure for searching is the following, quite close to the Bag-of-Word approach: For each image in the collection:• Extract the feature maps of the image, and compute the corresponding integral images.• Compute the global signature (with the whole image as window). Given a query region:• Use the global signatures to rank the first N most likely candidates of the collection.• For each candidate, re-rank them according to an extensive look of the sub-windows in the images using their pre-computed integral images.In order to greatly improve the results, we add the following improvements:• The parameters of the network we use were fine-tuned using the Replica dataset (Seguin et al, 2016) (image retrieval in paintings). This dramatically improves the system resilience to color and style.• We use Spatial Pooling according to Ra- zavian et al (2014) which consists of extracting 4 blocks per evaluated region instead of 1. It makes the search roughly 4 times slower but allow for much better retrieval of complex patterns by directly encoding spatiality. ResultsThe following experiment was run on the whole Web Gallery of Art collection (38'000 elements). Each image was resized so that its smaller dimension is 512 pixels, and the integral images of the feature maps computed on it. Given a query region for an image, the 300 most likely candidates are extracted from the WGA collection, and re-ranked according to the best matching window on each of them. Using 35 cores on a server machine, the complete request takes less than 4 seconds.Examples of queries and their results are shown on Figure 6. In query 1 and 2, the starting image is Leonardo da Vinci's Virgine delle Rocce (Paris, Louvres), first version of this subject (1483)(1484)(1485)(1486). Leornado is an interesting case study and his influence in Europe is known to be extremely important for the general compositions of paintings, character typologies and landscape patterns. In query 1, the group of Mary, the angel and the two children is selected. The first result is the version of the same subject (London, National Gallery) finished a decade after the Paris version, with the contribute of Ambrogio de Predis. The painting is different in color but has the same composition. The second and third results are other versions of the same subject by unidentified painters of the XVIth century. This constitutes typical examples of the propagation of a complex theme.In query 2, only a detail from the landscape is chosen. Interestingly, the second result is a painting from Bernardino de Conti, which is a variation on the same theme, reusing the landscape but without the angel and with the two children kissing.For query 3, we use a painting by Marco d'Oggiono, a follower of Leonardo, very similar to the one by de Conti and we select only the two children kissing. Results 1 and 3 feature paintings where only the children are present, showing that this replicator has an autonomy of its own. The third result by Joos van Cleve confirms the historical migration of this subject, as autonomous, from Italy to Flanders.These 3 simple queries illustrate how the detail matching method can easily unveil the transmission network between different series of paintings. PerspectivesThe search of matching details in large-scale databases of paintings may enable to find undocumented links and therefore new historical connections between paintings. By tracking the propagation and transformations of a replicator, it becomes possible to follow the evolution through time of repertoires of forms and view each painting as a temporary vehicle playing the role of an intermediary node in a long history of images transmissions. Although in continuation with traditional methods in Art History, such a tool opens the avenue for research at a much larger scale, searching for patterns and finding new links simultaneously in millions of digitized paintings.",
        "article_title": "Tracking transmission of details in paintings",
        "authors": [
            {
                "given": "Benoit",
                "family": "Seguin",
                "affiliation": [
                    {
                        "original_name": "DHLab - École Polytechnique Fédérale de Lausanne (EPFL)",
                        "normalized_name": null,
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Isabella",
                "family": " di Lenardo",
                "affiliation": [
                    {
                        "original_name": "DHLab - École Polytechnique Fédérale de Lausanne (EPFL)",
                        "normalized_name": null,
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Frédéric",
                "family": "Kaplan",
                "affiliation": [
                    {
                        "original_name": "DHLab - École Polytechnique Fédérale de Lausanne (EPFL)",
                        "normalized_name": null,
                        "country": "Switzerland",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " MotivationThis research supports a larger project on the discourse of 'creativity' in post-WWII America. The anecdotal observation that creativity has become a buzzword in recent years is supported by graphs of word frequency available through platforms such as the Google Ngram viewer and JSTOR Data for Research, which show creativity only entered the American lexicon in the twentieth century, diffusing rapidly after about 1950. 'Creative' appears to have enjoyed a similar growth spurt over the same period, but it preceded creativity by about three hundred years.Unfortunately, these graphs do not reveal the longterm changes in meaning nor the distinct contexts in which the language of creativity accrued its contemporary salience. It is obvious from contemporary usage that the word 'creative' has a tangle of interrelated but distinct meanings, ranging from generative or constructive to artistic to nonconformist. These meanings are distributed unevenly over time and across communities of discourse. To understand why and through what routes creativity arose when it did, it will be essential to tease apart these various meanings of creative, and the contexts in which they have been strongest over the long term.We believe topic modeling can help. First, it can help us identify and distinguish between the several discourses in which creative has been a keyword-for example in theology versus education versus psychology-whilst still reflecting the historically shifting connections and overlaps between those. Second, we can then apply those topics to only those texts containing the token 'creativity,' to reveal which of the discourses and meanings of 'creative' seem to be at work. By this process we can achieve a more granular picture of the creativity boom, helping us answer the basic question 'what do we talk about when we talk about creativity?' ApproachTopic modeling enables us to observe more higherlevel concepts than keyword searching and collocations would allow. Topic modeling depends on a certain class of mixed model clustering, but we believe that the two should not be conflated. The connotation of 'topic modeling' implies a qualitative interpretability. Surfacing what would be recognized as concepts is not solely a case of running a modeling algorithm on words from a text. Instead, it needs to be paired with a series of preparatory and parameterization steps tailored to the particular problem.We developed a workflow for training better topic models to track a specific concept in a temporally-biased corpus. This involves standard pre-processing such as stoplisting words, but also contributes three novel steps: selective page-level sampling, weighted training, and explicitly imbalanced prior assumptions on how likely a document is to be reflected by each topic. The sampling helps focus the models on creativity, the weighted training counteracts temporal biases to retain older topics to surface, and the asymmetric priors help find more granular topics.For a dataset cross-cutting published work broadly, we used a recent release of the HTRC Extracted Features Dataset (Capitanu 2016). The Extracted Features Dataset includes term counts for every page of 13.7m volumes in the HathiTrust Digital Library and benefits from a mostly indiscriminate digitization policy, allowing us to observe a term's usage in a wide spectrum of texts. Topic Modeling PreparationIn topic modeling, the goal is surfacing patterns that represent qualitatively intuitive concepts. However, to the methods used for topic modeling, the mark of success is being able to represent documents in the desired number of topics with as little error as possible. This divergence between our needs and the machine's makes the text preparation important. One such preparation is to remove words that are not interesting to a human reader. An algorithm may find a meaning in a word like 'however' or 'whereas', but as a proxy for topicality, such words are usually not desired.For tracking trends in creativity discourse, we used Latent Dirichlet Allocation (LDA) combined with standard preprocessing: removing the most common words in the English language, less interesting partsof-speech (e.g. adverbs, determiners, numbers), and cutting off the sparser end of the vocabulary. In addition, we developed three less common preparations in the service of issues arising from tracking concept diffusion.Sampling. One possible approach to finding the most common topics for a keyword is to look at the underlying term-topic probabilities for the keyword, post-training, and identifying the topics where the word is most common. This approach scales well to multiple keywords but provides low specificity for tracking them. Instead, we sampled only pages that use the word 'creativity' or variants of 'creative'. The size of the HTRC EF Dataset affords the small contextual window and selective sampling, as there were slightly more than 2 million volumes found that have at least a single mention of the keyword list.Weighted training. When training topic models, earlier texts have an outsize influence on the topics that emerge. This is a problem for our use case, where we expected a topical shift alongside a steep increase in usage. A randomized training order would reflex later texts very strongly, at the risk of missing topics which are prominent in older texts. To counteract this, we applied weighting to the randomized training order, to soften the temporal bias without entirely removing is. When deciding on the next text to send to the training algorithm, texts are weighted for sampling with weight(decade) = 1/ n(decade). The following figure shows this weighting in action: at the important start of training, newer texts are only slightly more common. Since a disproportionate number of older texts are used early on, there are few left by the end of training.Honeypot topics. As part of the estimation process for LDA topics, we have to formalize our best guess for how likely any given topic is to be assigned to a document. Past work has found value in allowing for these prior assumptions to be uneven - e.g. one topic can be considered more likely than another (Wallach, Mimno, and McCallum 2009). We found initial success with a heuristic intended to find many smaller trends in the collection by provided the first three topics the majority of the probability mass and dividing the remainder between the remaining topics. In qualitative comparisons with evenly distributed probabilities, we found that setting asymmetric priors in this way set traps to catch broadly common documents in predictable topics, while allowing other topics to surface more highlyspecific topical hotspots. Two general topics and two niche topics ResultsThe training yielded several topics which confirm where we would expect to find the language of creativity. Some of these reflect specialized uses, such as in advertising and evolutionary biology, while others reflect the broad humanistic discussions of the nature of thought, art, and religious creation. By graphing these topics over time we can see that our temporally weighted sampling appears to have been successful in revealing archaic topics that are nonetheless essential to understanding the connotative textures of the language of creativity in our own time.The following figures show a small selection of topics where the usage has grown in the past 150 years, and topics where it has fallen. Generally, we see that the language of creativity has transitioned from religious and natural notions of creation toward the language of economic and human capital. Future workThis work has a number of future directions. We have thus far focused on a number of words (creative, creativity, creativeness); moving forward, we intend to map how the verb and noun uses compare. Also, while much of the development has been qualitatively development against our particular problem, we hope to compare variants of our workflow in more contexts. ConclusionIn the proposed paper, we will present our method for tracking longitudinal trends in a diffuse and shifting context. Motivated by work on the language of creativity and particularly the noun 'creativity', our contributions are in text processing and parameterization for topic modeling, allowing clear and specific concepts to be revealed. ",
        "article_title": "Modeling Creativity: Tracking Long-term Lexical Change",
        "authors": [
            {
                "given": "Peter",
                "family": "Organisciak",
                "affiliation": [
                    {
                        "original_name": "University of Illinois",
                        "normalized_name": "University of Illinois at Urbana-Champaign",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047426m28",
                            "GRID": "grid.35403.31"
                        }
                    }
                ]
            },
            {
                "given": "Samuel",
                "family": "Franklin",
                "affiliation": [
                    {
                        "original_name": "Brown University",
                        "normalized_name": "Brown University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/05gq02987",
                            "GRID": "grid.40263.33"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe goal of the present study is to look at body parts within the class of published Norwegian books. Our main question is to look at the difference between referencing a body part directly from making a reference via a possessive pronoun. The body is important in our culture and is studied both within and outside the digital realm, e.g. the papers in Christopher E Forth; Ivan Crozier (2005) studies the body in culture, while Mahlberg (2013) uses corpus methods in a literary study.We report on a pilot study that considers body parts across the whole book collection without breaking the collection up into different genres or time periods. Although the pilot study is limited to an acrossthe-books analysis, it is part of effort to study the effect of different genres, as well as newspapers and journals. Method and dataWords for body parts are taken from Norwegian books in the period 1810 up to 2000, using the digitized books made available through the Norwegian National Library, approximately 460 000 books.The contexts we consider for nouns describing body parts fall into three types as described in Lødrup (2011) and Delsing (1998). Norwegian may express possessives parallel to the English pattern, like \"hans arm (his arm)\", alongside the definite version like \"armene hans (arms.PL.DEF his)\". Modern Norwegian seems to prefer the definite plus possessive, in particular for body parts, which therefore will be the construction we focus mostly on here.A slight complicating factor in Norwegian possessive construction is that sometimes the possessor may not be expressed e.g. Lødrup (op.cit.), in contrast to English, where the possessor cannot easily be replaced with \"the\" in \"John had a pain in his arm\" while in Norwegian this is the norm if the possessor is the subject \"John hadde vondt i armen/John had pain in the arm\".Each possessor phrase, pronoun plus body part gets a collocation graph as described in Brezina et.al (2015) where each edge in the graph is weighted with PMI (Pointwise Mutual Information, e.g. Lewandow- ska-Tomaszczyk (2007), Romesburg (2004)). Collocation graphs can be seen as a cluster of words for the phrase generating it, ordered by PMI.As an example, the first three words in the cluster (or collocation graph) for \"håret hennes (her hair)\", computed from approximately 12 000 concordance samples, go like this:There are 29 Norwegian body words in all going into this study, some duplicated in singular and plural, resulting in 21 unique body parts from head to toe.Each cluster is cut down to its top 200 words which are then compared using two standard similarity measures, the cosine and the Jaccard-similarity, where the former accentuates similarity of the clusters as weighted distributions, the latter highlights the set equality of the clusters. ResultsOur research question is how references to body parts differ when referenced using a pronoun, or with no pronoun specified. Note that even though no pronoun is specified, it is not required that the reference is done without a possessor, so there will be a certain overlap in the samples.Our main result is that female and male possessive constructions generate clusters that are closer than standalone body words. Looking at the clusters themselves we see that some is due to the expressiveness of the body, for example \"øynene/the eyes\", which ranks on top between female and male parts, has words like \"glimt/sparkle\" and \"lyste/lightened\" which is used to express emotion emanating from the beholder. These words are absent for the cluster for the word \"øyne\"eyes\". Also, words like \"hendene/the hands\" in the possessive construction yield words of action like \"grep/gripped\", \"slapp/released\", while outside the possessor construction generic actions like \"klappe/clap\" is found.The next step is to study these constructions with respect to a distinction between classes of works, using the metadata of national bibliography, and also the  ",
        "article_title": "Body Parts in Norwegian Books",
        "authors": [
            {
                "given": "Lars",
                "family": "Gunnarsønn Johnsen",
                "affiliation": [
                    {
                        "original_name": "National Library, Norway",
                        "normalized_name": null,
                        "country": "Norway",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Lean workingIn summarising the findings of multiple digital historical studies, Alves has argued that they 'implicitly confirm the efficiency of digital means… but also… that their application is, often, generally associated with expensive projects requiring extensive human resources with diverse skills ' (2014). We questioned whether digital research is necessarily expensive or requiring of extensive resource. Drawing inspiration from commercial software development, we asked what is the Minimum Viable Product (Leanstack, 2016); can teams achieve meaningful research relatively inexpensively through an agile approach of iterative investigation and identification of emergent areas of interest rather than pre-defining fixed research questions. Skills and knowledgeIt could be argued that there is a skills or knowledge gap within traditional historical research communities which inhibits conducting digital research. However, we questioned if this was truly a barrier when evidence from citation patterns shows increases in collaborative approaches to digital humanities research (Nyhan and Williams, 2013). We questioned if extending the scope of traditional research teams to include commercial development partners could bring new insights and capabilities. Crossing collectionsWellcome's digital collections include intensely heterogeneous material along with data sourced from multiple institutions. We questioned whether there are practical or technical barriers to break down divisions between collections and drawing from a range of content. Hitchcock (2013) has argued that 'the lack of flexibility of the available digital tools [has] enabled only the effective utilization and analysis of quantitative sources or sources easily transformed into a quantitative format'. As many of Wellcome's collections are archival and contain large quantities of handwritten and pictorial material, we specifically wanted to explore the possibilities of digital for non-quantitative research, and research which still requires 'close reading' of sources (Van Dijk, 1985). Quality and consistencyAreas of enquiry in this umbrella included whether our digital collection is suitable in its scale and scope, and the quality and consistency of OCR and collection metadata. We also questioned if we we had the right kind of web services available and their usability for individuals with different levels of experience.To explore and better understand these questions we designed an experimental approach, adapting a concept becoming familiar to cultural institutionsthe 'hack day' - and extending it out into a week-long intensive R&D project, where small teams led by a mix of independent and academic researchers would work collaboratively to explore the Wellcome Library digital collections. Our participants included research staff from several UK universities, librarians and archivists, commercial designers and software developers. Through careful screening of participants, we selected researchers with shared enthusiasm for, but variable experience of digital historical research. This choice was deliberate in order to focus in on barriers relating to experience, skills and technical feasibility without confounding these variables with any reluctance to use digital methods. However, the enthusiasm for digital methods in the historical community is an important question and undoubtedly merits further investigation.Research areas were open-ended, with a focus on experimentation rather than production of finished work. However, we did agree broad areas, including handwritten records from a private asylum, 5000 Medical Officer of Health reports covering London from 6,600 issues of the trade journal Chemist and Druggist and the 79,000 books digitised by the UK-MHL project (Wellcome Library, 2016).Drawing from approaches of participative and cooperative inquiry (Reason and Bradbury, 2008) we embedded Wellcome staff in mixed teams of academic staff, developers and designers, positioning the teams as researchers of the collections, participants in a broader experiment of research production and observers and documenters of the experiment itself. We encouraged self-documentation by teams through use of project boards, blogs and wikis, conducted individual interviews with participants throughout the process and held daily reviews and a plenary session to discuss progress and reflect on the experiment.During the week we produced multiple tools and visualisations, and also historical findings. These are documented on the project blog, along with the processes each team went through. This clearly demonstrated the potential of the collections, but also the barriers to working with them. Interestingly, the project using digitised archival material without OCR was one of the most successful - partly due to the synergy of the team members, but also the clarity of the challenge for the material. Two of the five projects seeded in the week continue to be developed, and we are continuing to provide support to the researchers leading them.Feedback from participants in the project identified particularly the benefits of working in teams with mixed skillsets. Developers and library staff gained from acquiring greater understanding of research processes and interests, while researchers gained access to technical skills, and exposure to a different approach to digital working. One participant remarked: 'I found working with other people, from a variety of different backgrounds, really generative -both for thinking about why people who do different kinds of work approach digital resources in different ways, and for thinking about my own research along new lines.'The web services we provide for programmatic access to digital collections were identified as a particular barrier during the week. While the images and bibliographic metadata for our collections are exposed through an international standard, the Image Interoperability Framework (IIIF), this proved conceptually complex for developers to quickly prototype with, requiring combining and processing multiple JSON responses to access digital items. Adding further complexity to this, our collection OCR is available primarily through ALTO XML (Library of Congress, 2016) which our developers found challenging to process at scale.As cultural heritage institution and a research funder, we are continuing to unpack the implications of the findings. As a library, we found great value in coproduction with research users so will be repeating a similar annual event, investigating different aspects of our collections. We have also identified particular issues related to the usability of our collections and added these to our development roadmap. As a funder we are considering options for increasing innovation by seeding early stage research through similar collaborative processes. To take this forward we will be running a series of pilot events through our interdisciplinary research residency, The Hub.   ",
        "article_title": "New potentials in the digital archives: a participative inquiry into interdisciplinary collaboration in digital historical research at the Wellcome Trust",
        "authors": [
            {
                "given": "Alex",
                "family": "Green",
                "affiliation": [
                    null
                ]
            },
            {
                "given": "Lalita",
                "family": "Kaplish",
                "affiliation": [
                    null
                ]
            },
            {
                "given": "Hannah",
                "family": "Walker",
                "affiliation": [
                    null
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Anthology of American Literature, World Literature, Western Literature, Poetry, Drama, Theory and Criticism, Short Fiction, Literature by Women, African American Literature, Latino Literature, Jewish American Literature, etc.Much can be learned about the ways in which the Nortons were designed from these titles alone. First, the largest anthologies are defined by both geography and a linear temporality influenced by conventions of periodization. The Norton Anthology of World Literature lays claim to it all, from Afghanistan to Zimbabwe, and from Gilgamesh to Orhan Pamuk. Western Literature claims a smaller (if vaguer) part of the world, and English and American literature focus on national literatures, including postcolonial and expatriate writers within the bounds of the nation-concept. While these geographic anthologies are ostensibly genre-agnostic, others are genre-specific (Poetry, Drama, Short Fiction). And the last type focuses on writing by and about writers of a specific gender (By Women), ethnicity (Latino), or religion (Jewish).One of the premises we read as implicit in the Norton's design, then, is that some authors and works become significant enough to include only in specific contexts. Making it into the World Literature anthology seems to denote significance at a greater level than inclusion in the Western Literature anthology alone would. Likewise, seeing a writer anthologized in Short Fiction but not World Literature seems to imply a significance limited to that literary form. Canon formation has always relied on a logic of a ranking or tiering, and the pool of authors and texts against which a given work \"competes\" is greatest at the largest scale of population. We argue that the geographically bound Nortons can be read in such a way that they imply a hierarchy even among the canons the anthologies already connote.Of course, all of the decisions we measure across these many tables of contents are underwritten by a human element. Many practical and historical factors that exist at a slant to the question of a work's \"canonicity\" attend the production of an anthology that stretches to more than 6,000 pages, serves tens of thousands of instructors and students, and our analysis attempts to account for these factors. A quantitative approach is necessary but not sufficient to read the ways in which the Nortons have represented and continue to represent works that, taken together, lay claim to the status of a national, generic, or global literature. Two key examples of the incommensurability of the form to its implicit claims: Because of its length, the novel is poorly served by the anthology form. Some shorter novels and novellas do get anthologized. But, more often than not, writers who are primarily known as novelists are represented by a single short story, or an excerpt from a novel. The second overarching practicality is the influence of authorial estates and the cost of printing rights, especially for 20th and 21st century authors.As a way of approaching these institutional and qualitative questions, we have begun a set of interviews with Martin Puchner, the current general editor of the World and Western anthologies, and will discuss some of his insights into the decision-making process in the presentation. Among these include the influence of instructor and student surveys on texts that get selected, the impact of rights costs on the texts that get chosen for a given author, and the place of editorial intervention in relation to these powerful practicalities.Our database and the attendant institutional research on the Nortons as the product of both scholarly editing and the demands of the market allow us to see the trajectories not only of individual authors and works, but broader trends of inclusion and exclusion in the Norton's canon. By gathering data about both the works and the authors who wrote them, we reveal the ways in which the Norton has responded to the expansion of the literary canon, growing in size while simultaneously giving a greater share of its pages to authors and ideas that would not have been considered canonical 1962. In the process, we find authors whose literary reputation has waxed or waned (or both); those whose names have been a constant presence, but whose representative works have dramatically changed; those who were slated for canonization but never \"made it;\" and those who have arrived late but seem to be here to stay.Like many of the Stanford Literary Lab's projects, \"Reading Norton Anthologies\" operates at several scales at once. We are interested both in individual texts and authors, as well as broader patterns of representation and contextualization within the confines of this object that occupies liminal spaces between statement and syllabus, and between the market and the canon. ",
        "article_title": "Reading the Norton Anthologies: Databases, Canons, and \"Careers\"",
        "authors": [
            {
                "given": "Erik",
                "family": "Fredner",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "Mcclure",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "JD",
                "family": "Porter",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Digital Humanities Summer Institute, or DHSI, provides \"an ideal environment for discussing and learning about new computing technologies and how they are influencing teaching, research, dissemination, creation, and preservation in different disciplines, via a community-based approach\" (DHSI). What began in 2001 as a small event at Malaspina University-College is now an annual 2-week affair at the University of Vic-toria which offers dozens of sessions and attracts a global audience. The growth of DHSI over the past 16 years mirrors the larger development of digital humanities as an academic (inter)discipline that has shifted from a niche endeavor for computational linguists and technology early-adopters to both a mainstream methodological approach and a distributed community of practitioners. DHConnections examines DHSI attendance data. This website functions both as a tool for research about DHSI (and thus the history of disciplinary training in the digital humanities), and as a platform for DHSI alumni to connect with other researchers with similar interests, or to reconnect with contacts from previous DHSI sessions. In this way, DHConnections intersects with the \"Collaborators\" component of center-Net's excellent DH Commons website, but is more explicitly focused on the distributed community of DHSI alumni. The DHSI organizational team maintains an archive of past DHSI sessions and participants. With the permission of DHSI and the Electronic Textual Cultures Laboratory at University of Victoria, I scraped this collection and cleaned it with OpenRefine to remove typographical errors, standardize attendee and organization names, and validate the data. There is no participant data (only instructors) for 2001-2003, but since 2004 there is an accurate list matching every attendee to the session he or she attended. This information is further broken out by role-student, instructor, speaker, or staff, with numerous specific subcatego-ries of each. From 2006 onward, attendee institutional and organizational affiliations are also included. I also built a controlled vocabulary of DH topics and manually added topics to each session based on the session title and abstract when available. Together, these components form a temporal, topical, spatial, and biographical dataset which captures the attendance data for 2,678 individuals who collectively attended 254 sessions across 4,932 instances. DHConnections allows users to access and interpret this dataset. Researchers can access the raw data via a JSON endpoint, but DHConnections also features numerous interactive interfaces which provide intuitive ways to understand the growth of DHSI through what Joanna Drucker calls \"visual epistemologies … ways of knowing that are presented and processed visually\" (2014, 8). These include a searchable table of all participants ; charts of the growth of DHSI over time by country, and by total institutional attendance; a face-table map of participants' institutional affiliations; a graph which examines the popularity of DHSI session topics and when they were introduced (figure 1); and a searchable network graph of participants, linked by session attendance, scaled by frequency, and color-coded by participant role. These and other visualiza-tions provide information about the growth of an international DH and DHSI constituency, identify major contributors and organizations within the field, and assist DH researchers interested in finding contacts.",
        "authors": [
            {
                "given": "Cole",
                "family": "Crawford",
                "affiliation": [
                    {
                        "original_name": "Oregon State University",
                        "normalized_name": "Oregon State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00ysfqy60",
                            "GRID": "grid.4391.f"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionCultural diversity has been an increasing source of debate within the digital humanities community. The concentration within the Debates in Digital Humanities series (Gold, 2012;Gold and Klein, 2016) of pieces reflecting the increasing prominence of matters related to race, gender, cultural diversity and difference is but one marker of the extent to which diversity matters. The Orlando Project in feminist literary history incorporated an intersectional understanding of identity categories from the outset (Brown, Clements andGrundy, 2006-2017). Translating Orlando's Extensible Markup Language (XML) data into linked open data (LOD) to make it accessible, interoperable, and amenable to a range of analytical approaches (Simpson and Brown) requires an ontology that will serve both Orlando and the broader research community hosted by the Canadian Writing Research Collaboratory (CWRC). This paper outlines the CWRC ontology design and the challenges of shifting from semi-structured to structured data (Smith, 2016: 273).Much work on digital diversity expresses skepticism of the ability of systematized knowledge structures to capture the performative, processual, and contingent nature of lived subjectivities. Tara McPherson stresses that \"computers are themselves encoders of culture\" and calls for more attention to be paid to the interconnectedness of the structures of code and the management of race socially: \"Just as the relational database works by normalizing data-that is, by stripping it of meaningful, idiosyncratic context, creating a system of interchangeable equivalencies-our own scholarly practices tend to exist in relatively hermetically sealed boxes or nodes.\" Scholars including Lisa Nakamura (2002: 120) and Moya Bailey (2011) see value in \"messiness\" as a way to push against and redefine the contours of a digital humanities scholarship that remains rooted in predominantly white epistemology.At the same time, relegating representations of difference to narrative rather than structured data will produce gaps within big data that are both impoverishing for humanities inquiry and dangerous in their political implications (Lerman, 2013;Trevinarus, 2014;\"Use\";Brown and Simpson, 2013). Adriel DeanHall and Robert Warren (2013) have advocated approaches that respect the privacy and preferences of lived human subjects while improving the responsiveness of online systems to diversity and complexity. Within a LOD context, what are finally findable, processable, and reusable on the global graph are things, not strings, so the challenge is the extent to which nuance, context, and indeed messiness can be incorporated into a LOD ontology.The Orlando Project (Brown, et al., 2006(Brown, et al., -2017) charted a middle ground between narrative and structure for its bespoke XML tagset. The team struggled with the hierarchical nature of XML particularly in relation to identity categories, torn between knowledge that readers would turn to Orlando to find writers associated with particular cultural identities and recognition that such categories are discursive rather than essential (Fuss, 2013). It devised a \"Cultural Formation\" tagset to depict identity as neither unitary nor immutable, and as much related to representational acts as to the lived experiences into which those representations blur. Precisely because constituted through discursive and social practices, vocabularies associated with subjectivities and identities can shift over time and place, and throughout an individual's lifetime. Cultural formation tagsetThe Cultural Formation (CF) tagset recognizes categorization as endemic to social experience, while incorporating variation in terminology and contextualization of identity categories by employing tags at different discursive levels. CF tags describe the subject positions of individuals through 1) contextual tags that encode substantial discussions: class; language; nationality; race and ethnicity; religion; and sexuality; and 2) granular tags that describe, in a word or short phrase, class; ethnicity; gender; geographical heritage; language; nationality; national heritage; political affiliation; race or colour; religious denomination, and sexual identity. With the exception of gender and social class, the Orlando schema eschewed fixed attribute values for the granular tags, allowing the prose to employ the most appropriate language for the context. The structure was not entirely logical or parallel, and we are making the ontology more consistent. The granular tags possess attributes regarding forebears and whether a subject self-identified with a particular term. The tagset aimed to highlight the extent to which social classification is culturally produced and discursively embedded. Rather than disambiguating leaky cultural categories, it considered them as mutually constitutive with historically specific discursive frameworks, including our tagging structures.CF encoding pointed users towards a framework for raising and debating complex matters for cultural investigation rather than standardized classifications, refusing to neatly group writers into distinct and fixed categories, since those categories were neither stable nor mutually exclusive (Algee-Hewitt, Porter, Walser, forthcoming). It can represent quite complex identities, as in the case of Anna Leonowens, the writer whose story of life as governess to the royal Siamese harem was popularized in The King and I. Partial markup for the first paragraph of her CF description is shown in Figure 1. The CF component of Orlando's knowledge representation is thus crucial to its intersectional approach to identity (Brown et al., 2006). Creating a LOD ontology that was not self-referential, however, requires translating the strings or literal values from CF tags, to link Orlando's semantic structures to other semantic web communities. LOD ontology creationAn ontology \"is a formal naming and definition of the types, properties, and interrelationships of the entities that really or fundamentally exist for a particular domain of discourse\" (Wikipedia, Ontology -Information Science). Using a standard ontology language such as OWL allows others to interact and exchange with a particular view of the world through a computational process of mediation. As a representation of that understanding, an ontology can be referenced, (dis)agreed with, extended, and used operationally. The coexistence of different representations provides the foundation for translations between LOD concepts.Ontology creation in our case, as in many others, was driven by the idiosyncrasies and limitations of an existing data set. The information architectures of application databases or XML stores are not always reconcilable to a consistent information system. The CF tagset represents a major challenge in that its structure was designed to eschew disambiguation. Even the major tags were difficult to relate within a concise ontology (Figure 2). For example, nationality and national heritage are not employed as commensurate with citizenship, a well-defined legal concept related to an organized state. They can also be related to a geographical area, which may or may not coincide with a state. Finally, nationhood can reference socio-political constructs such as Lesbian Nation (Johnston, 1973;Ross, 1995;Munt, 1998) or disavowals of nationality such as Virginia Woolf's (1938: 197), which Orlando quotes alongside assigning Woolf an English nationality, a contradiction that requires contextual evidence to make sense. Linked into contextWe decided to make all human-readable annotations within the dataset instances of contextual notes to which the ontological classes are directly tied (Fig- ure 3). classificatory labels relate to predicates and external ontologies. Skos:narrower/broader relationships are also used, but omitted here to improve legibility Thus we model the discursive context within a Race[or]EthnicityContext class. The note instance links to instances of granular category labels, here RaceColour; it provides the provenance and the basis for links to source information. Linking to the provenance of the LOD is particularly important for disputed or contradictory information, as in our example. We are modeling the original Orlando narrative as a source document for our LOD provenance using the the Web Annotation Data Model's subproperty instances. We aim to link every triple to the prose from which it is derived, providing provenance information and contains citations to the sources on which identity assertions are based. Relating cultural formationsCultural formation for Orlando is understood primarily as representational, which is not to say that cultural formation is not real or that it has no material effects. The complex signifiers of cultural identities float across Orlando tags as cdata or free text in a semistructured representation of cultural identities and categories. For the CWRC ontology, we strategized to relate this ontological perspective to that of external vocabularies without conflating our truth with theirs. Our architecture does not import other ontologies wholesale, but adopts components of major vocabularies such as BIBO, FOAF, and FRBR, and relates to large vocabularies in defined ways. As indicated in Fig- ure 3, the instances of cwrc:whiteRaceColour and cwrc:whiteEthnicity within the CWRC ontology are subclasses of the cwrc:whiteLabel. This retains the ambiguity of terms such as \"white\" or \"Jewish\" precisely as labels that draw together particular types of identity categories, as well as subClasses of those labels. As indicated, those subClasses can be linked to terms in external vocabularies, but both internal and external terms are understood within the CWRC ontology as labels. Indeed, constructing this ontology has brought home to us the need for the LOD community to think through with greater care the relationship between representation and \"reality\" in LOD ontologies. A further complication is that identity categories are not only historically contingent but often also change over a particular individual's lifetime. The Orlando dataset supports such nuance in only a few cases, so we have not started with this gnarly problem, but we aim to build into the ontology the capacity to represent such cultural formation dynamics in order to accommodate more temporally precise data. ConclusionThe CWRC ontology design avoids representing RDF extractions from Orlando data as positivist assertions, and yet produces machine-readable OWL/RDFcompliant graph structures. It allows references to, without endorsing, external ontological vocabularies that are nevertheless part of documenting intersectional cultural processes and identities.We will present CWRC ontology as built around the CF design described here, and we will demonstrate its implications through several practical examples. Fig- ure 4 shows schematically the intersectionality of multiple identity categories associated with Leonowens, including the ways that instances are related by subclass relationships in accordance with OWL principles. This importantly allows us to reference components of other ontologies (here the Muninn Appearances ontology, Library of Congress Subject Headings, Getty Art and Architecture Thesaurus, and DBpedia) without adopting them wholesale.   the edge between Elizabeth Sarah Gooch and cwrc:jewishReligion is hasReligion) Our live presentation will demonstrate the ontology in action using the interactive HuViz (Humanities Visualizer) interface with a larger dataset.",
        "article_title": "Cultural (Re-)formations: Structuring a Linked Data Ontology for Intersectional Identities",
        "authors": [
            {
                "given": "Susan",
                "family": "Brown",
                "affiliation": [
                                        {
                        "original_name": "University of Guelph",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Abigel",
                "family": "Lemak",
                "affiliation": [
                    {
                        "original_name": "University of Guelph",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Colin",
                "family": "Faulkner",
                "affiliation": [
                    {
                        "original_name": "University of Guelph",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Kim",
                "family": "Martin",
                "affiliation": [
                    {
                        "original_name": "University of Guelph",
                        "normalized_name": "University of Guelph",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/01r7awg59",
                            "GRID": "grid.34429.38"
                        }
                    }
                ]
            },
            {
                "given": "Rob",
                "family": "Warren",
                "affiliation": [
                    {
                        "original_name": "Carleton University",
                        "normalized_name": "Carleton University",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/02qtvee93",
                            "GRID": "grid.34428.39"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis talk explores how new vector-based approaches to computational semantics both afford new methods to digital humanities research, and raise interesting questions for eighteenth-century literary studies in particular. New semantic models known as \"word embedding models\" have generated excitement recently in the natural language processing and machine learning communities, due to their ability to represent and predict semantic relationships as complex as analogy. \"Man\" is to \"woman\" as \"king\" is to what?, one can ask of the model; \"queen,\" it will most likely reply. These models formulate analogical and other semantic relationships by computing mathematical vectors for words, such that, if V(x) denotes the vector for the word x, then the above analogy can be expressed as V(woman) - V(man) + V(king) ≈ V(queen). Although these models have a longer history- vector space semantics dates from the '70s, having been first developed for the SMART information retrieval system (Sal- ton, 1971) by Gerard Salton and his colleagues (Salton et al, 1975)\" (Turney and Pantel, 2010)- new innovations in their speed and accuracy (see Note [1]) have renewed researchers' interests-a development begun, in part, by Google, when researchers there unveiled newly efficient algorithms in 2013, packaged in software they released called word2vec. (The word2vec algorithm was originally described by Mikolov et al, 2013. It introduced the neural network to vector space semantics, providing an efficient means by which to compute word vectors. The GloVe algorithm from the Stanford NLP Group eschews the neural network approach, instead performing a novel method of dimensionality reduction on word collocation counts).\"Word vectors,\" as these new methods are sometimes informally called, have already enabled published research into questions relevant to humanistic research, such as a recent landmark paper from researchers in the Stanford NLP Group into patterns of semantic change across centuries of discourse (Hamilton et al). However, unfortunately, word vectors have so far rarely appeared in research from the digital humanities community itself. Moreover, what work that does exist has so far been primarily circulated through blogs, rather than through published proceedings or articles. Ben Schmidt, for instance, has written an influential introduction to word vectors in his blog post \"Vector Space Models for the Digital Humanities\" (2015a), which also includes a documented R package for computing them. Also notable is his post, \"Rejecting the gender binary\" (Schmidt, 2015b), which uses word vectors to dissect the polysemy of words; as well as Michael Gavin's post, \"The Arithmetic of Concepts\" (2015), which explores the conceptual implications of adding and subtracting word vectors.On the whole, the current research landscape of word vectors in the digital humanities resembles the landscape of topic modeling years ago, when the original LDA algorithm (published in 2003 [Blei et al]), before appearing in landmark published DH studies such as Matt Jockers' Macroanalysis (2013), was employed for humanistic research as early as 2006 by researchers working outside or tangentially to the digital humanities (Newman and Block).Given this scarcity of digital-humanities research on word vectors, work that seeks equally to explain, interpret, and demonstrate their potential seems particularly useful. With these goals in mind, this paper attempts first to unpack for a digital-humanities audience how word vectors work, with reference to the canonical analogy cited above: \"man is to woman as king is to queen.\" Second, in order to interpret word vectors' conceptual implications for eighteenth-century literature, I move away from this canonical analogy to one central to a particularly influential argument in the period: \"Learning is to Genius as Riches are to Virtue.\" Lastly, I turn from this close reading of word vectors to methods of distant-reading analogies that lie implicit in eighteenth-century literature. Explaining Word VectorsHow do word vectors work? In the interests of space, I have omitted this section of my talk from the abstract. Readers curious about the mechanics of word vectors can read more on my blog, which also links to a number of other explanatory resources (Heuser, \"Methods\"). Close-reading Word VectorsWord vectors provide a persuasive computational means for the semantic representation and analysis of analogies. They combine a mathematical elegance with an intuitive interpretability to yield what is, potentially, a method useful not only for large-scale semantic analysis, but also for smaller-scale explorations of particular analogies in literature, and their specific forms of analogical argumentation. For instance, analogy lies at the heart of Edward Young's essay Conjectures on Original Composition (1759), which argued for the superior aesthetic interest of modern, \"original\" composition over the neoclassical imitation of the ancients. Crucially, Young makes his argument through analogy, identifying several other conceptual contrasts as analogues to his central one between original and imitative composition: \"I would compare Genius to Virtue, and Learning to Riches,\" Young writes; \"[a]s Riches are most wanted where there is least Virtue; so Learning where there is least Genius.\" In this way, Young's valuation of \"Genius\" over \"Learning,\" and of original over imitative composition, become ethically justified through their analogy with another, more obviously moral contrast between \"Virtue\" and \"Riches.\"But what is the logic behind this analogy? Here, word vectors provide the close reader with a framework, language, and method of exploring the semantic implications at work in an analogy. In terms of vectors, we can ask, what does V(Virtue)-V(Riches) (Also sometimes expressed here, in a shorthand, as V(VirtueRiches) mean, and is it in fact correlated with V(Genius)-V(Learning) in the broader discourse of the period? Asking this question of a word2vec model trained on the 80 million words of eighteenth-century literature in the ECCO-TCP corpus, we find that \"Riches\" are to \"Virtue\" as \"Learning\" is to... \"Genius\" is the sixth closest word vector, or the sixth most likely solution, to this analogy. How to test the significance of this result is not immediately clear, but, out of tens of thousands of possibilities, it's certainly provocative: it raises the possibility that word vectors might provide computational assistance to close readings. Indeed, the other words in this list amplify the semantic profile of this analogy in a way that might help to clarify its underlying implications. For instance, the contrast between the intrinsic form of value in \"Virtue\" and the extrinsic form of value in \"Riches\" seems underscored for me by the contrast here between the extrinsic writerly attribute of \"Learning,\" associated with an Oxbridge education, and the intrinsic attributes of morality, genius, and wisdom.Ultimately, however, what does it mean to closeread word vectors? This is a question raised by Gabriel Recchia in a blogpost responding to my interpretation above as it first appeared on my blog (Recchia; Heuser, \"Concepts\"). Recchia's post explores other vector operations that even more reliably yield \"genius,\" namely V(learning)+V(virtue) and V(talents)+V(abilities)+V(erudition). To me, however, these alternative \"paths\" to genius do not exclude one another; instead, each contributes to our understanding of the semantics of genius in the period. My goal with this interpretation is not to \"prove\" Young's analogy, but rather to suggest that, by \"amplifying\" a particular analogy through its semantic associations across a corpus, word vectors help contextualize our interpretations of particular analogies in literature. As Recchia writes, \"the computational exercise has helped us focus our search.\" Distant-reading Word VectorsIf, then, vectors help us explore this micro-analytic scale of interpretation, they also help us scale those same interpretive models up to the level of macroanalysis. For instance, inspired by the foregoing closereading of Young's complex web of analogies (Table 1), we might continue Young's project of obsessive analogization by way of a distant reading. By defining vectors for a range of common eighteenth-century contrasts (Table 2), and then measuring the correlation between them, we can in fact construct another complex web of analogies-this time gleaned computationally, from a large-scale archive of the period's discourse.  (1748), as well as a number of essays from the period; they are not meant to be exhaustive. This is an admittedly unsatisfactory method; I am currently exploring ways to discover conceptual contrasts computationally.Looking at a particularly strong correlation among the contrasts in Table 2, between V(Simplicity-Refinement) and V(Virtue-Vice), we can see how their correlation emerges from the way in which both contrasts carry similar semantic associations across the same set of words (Figure 3). . 1,000 most frequent nouns in the ECCO-TCP corpus. On the x-axis is their cosine similarity with the V(Simplicity-Refinement) vector: if above 0, then associated more with refinement; if below, more with simplicity. Conversely, on the y-axis, above 0 means associated more with Vice; below 0, more with Virtue.In other words, this graph shows that there are more words for simple virtues (e.g. \"graces\") than refined virtues (e.g. \"science\"), and more words for refined vices (e.g. \"corruption\") than simple vices (e.g. \"murder\"). This correlation between their semantic associations (R^2 = 0.41) reveals, then, an analogy emerging from the period's broader discursive practices-Simplicity is to Refinement as Virtue is to Vice-even as that analogy might appear only implicitly in particular essays, such as in Hume's \"Of Simplicity and Refinement in Writing\" (1742), when Hume loosely associates refinement with the moral decline of post-Augustan Rome.This macro-analytic approach to discovering implicit discursive analogies allows us to visualize the ways in which the frequent conceptual contrasts in eighteenth-century literature are implicitly analogized in the discourse, and how those implicit analogical relationships may have helped to structure what Peter De Bolla has called the \"conceptual architecture\" of the period (Figure 4). the R^2 value of their correlation, across the 1,000 most frequent nouns (as in Figure 3), is greater than 0.1. Blue lines read in the natural order (e.g. Simplicity is to Refinement as Woman is to Man); red lines read in reverse order (e.g. Simplicity is to Refinement as the King is to Parliament). Nodes are sized by betweenness centrality, and colored by network community. Edges are sized by the R^2 value.From this network of correlated contrasts, we can see which of them, for instance, are implicitly gendered in the period's discourse. \"Woman\" is to \"man,\" for instance, as \"queen\" is to \"king\"-but also as the beautiful is to the sublime, as simplicity is to refinement, and as passion is to reason. Similarly, we can see which contrasts are moralized in the period: \"virtue\" is to \"vice\" as wisdom is to folly, as pity is to fear, as the mind is to the body. Moreover, the contrasts of virtue and vice, and simplicity and refinement, might actually play a central role in such a conceptual architecture of analogies, as seen from their centrality within the network. ConclusionI hope to have demonstrated some of the ways in which word vectors might be useful for the digital humanities, and particularly for eighteenth-century literary studies, both by demonstrating how they might help us to close-read specific analogical maneuvers, as well as distant-read analogies as they emerge from patterns in their usage across a literary discourse. Notes[1] According to statistics provided in the original paper for the Stanford NLP group's \"GloVe,\" a competing algorithm to word2vec, a word2vec model trained on a large English-language corpus can accurately solve 65% of analogies in a test dataset, and GloVe 75% (Pennington et al, Table 2). As a rough comparison to the accuracy we would expect from human subjects, we might look to the Miller Analogy Test from Pearson-an admittedly unrelated analogy test, which is given to some graduate student applicants. In the MAT of 2002-3, to accurately solve 65% or more of its 100 analogies places a student above the 80th percentile (Pearson). Although not directly comparable, these statistics make more probable the assessment that word vectors are capable of capturing semantic relationships at a level competitive with human subjects. ",
        "article_title": "Word Vectors in the Eighteenth Century",
        "authors": [
            {
                "given": "Ryan",
                "family": "James Heuser",
                "affiliation": [
                    {
                        "original_name": "Stanford University",
                        "normalized_name": "Stanford University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00f54p054",
                            "GRID": "grid.168010.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Linked notation in support of musicologyNotation examples are a vital part of analytical essays in musicology, helping to illustrate analytical observations and justify hypotheses, arguments and conclusions. They can be excerpts from a score, or custommade notations which add annotations or comments to the original notation.Furthermore, the presentation of multiple analogous musical extracts for comparison is often required to support a musicological narrative. Paradigmatic analysis, for example, involves passages of music placed one above another such that analogous elements are directly juxtaposed, with gaps left as necessary to ensure that vertical alignment is preserved. Stacked presentation of different scores or different parts of the same score have been used for well over a century, but they quickly become unwieldy and hard to interpret, especially as the number of extracts increases. What is not available in such paper-based approaches is the interactivity that can make complex comparisons between many extracts practical by turning a static presentation into an iterative exploration of digital materials.In this paper we consider the example of a digital companion presenting the contents of a Research Object studying the interpretation of leitmotif examples from Wagner's compositions, specifically the Ring cycle, as they are presented in numerous historical introductions, opera guides, leitmotivic threads and leitmotif lists included in libretto editions and piano scores. The study of the incidences of these particular leitmotif identifications consists of both the gathering of source material and its digitisation and cataloguing, and a musicological study of the potential relationships, influence and evolution between leitmotif interpretations. To enable the extension and repurposing of the identified leitmotif relationships they are structured using an ontology.Notation examples in leitmotif guides are usually abstractions drawn from a piano score. When reporting findings from this research it is desirable to present and relate the scholarly arguments back to the musicological context within which they are made: from the score excerpts in the source material; and to MEI encodings that illustrate and encode both the examples from which they are drawn and to complete (piano) scores of the overall operas.This enables matching and linking of the examples as they are described in the scholarly text, via semantic hyperlinks, to and from the score, including exact matches and variants, illustrating interpretations, and situating the examples back in context. Encoding interpretations in the form of notation examples as variant readings of a certain passage could thereby chart the 'understanding' of the work as a history of its variants.( For example comparing: Richard Wagner, Die Walküre, piano score by Felix Mottl, Leipzig, Peters, 1914, p.165; Hans von Wolzogen, Thematischer Leitfaden durch die Musik zu Richard Wagners Festspiel Der Ring des Nibelungen, 2 nd ed., Leipzig, Schloemp, 1876, p.58, 'Schicksalsmotiv'; Gustav Kobbé, Wagner's Music Dramas Analyzed With the Leading Motives, New York: Schirmer, 1923, p. 57, 'Motive of Fate'; George Dunning Gribble, The Master Works of Richard Wagner, London, Everett, 1913, p. 289, 'Fate Motif'.) Introducing MELD: Music Encoding and Linked DataTo realise the digital notation companion we have developed the MELD framework (Music Encoding and Linked Data). MELD enables the interactive presentation of multimedia contents of the Research Object, such as the images, text, audio, and MEI encoded music notation described in the previous section. These can be explored contextually alongside each other through the use of semantic links, encoded using RDF, which describe the musicological relationships between the resources (and elements within them). In contrast to earlier technologies which have typically aligned resources against a timeline (e.g. in milliseconds, or using MIDI), MELD expresses relationships anchored to musically meaningful items scoped using MEI. Figure  1 shows a screenshot of MELD displaying text and notation, highlighting leitmotifs as identified in different historical guides. To render our music notation (encoded using MEI) we use Verovio ( Pugin et al. 2013), an open-source MEI renderer that produces beautiful SVG renditions of the score. In addition, Verovio provides an architecture in which identifiers (in other words, anchors for our relational Linked Data expressions in the MEI XML) are persisted through to the rendering (in SVG) which can be connected to identifiers in our contextual information (in RDF). When rendered (and re-rendered) for the user in our web based application interface, the browser uses these identifiers to generate interface elements and undertake actions that combine information from the MEI and the Linked Data. Within the Research Object, we treat the XML IDs of elements within the MEI resource as fragment identifiers, so URIs can be straightforwardly generated for each notation element of interest. We employ the Web Annotation Data Model ( Sanderson et al. 2017), using these URIs as targets of annotations representing each musicological marking. Corresponding annotation bodies are associated with semantic tags defined to encode the different musicological interpretations, which are in turn the annotation bodies of a top-level annotation targeting the URI of the files currently being viewed, including the music encoding (MEI) and scholarly interpretation (HTML). A simplified example of such relationships is shown in Figure 2. The MELD client then uses HTML/CSS and JavaScript, served by a simple web service implemented with Python Flask, and illustrated in Figure 3. The procedure driving the rendering and user interaction is illustrated in Figure 1. The client processes a framed (see the explanation of framing) JSON-LD representation of the RDF graph instantiating the data model. It then performs an HTTP GET call to acquire the MEI resource targeted by the top-level annotation, and renders the corresponding musical score to SVG using Verovio. User interactions are captured using HTML divs drawn as bounding boxes over portions of the SVG corresponding to MEI elements of interest; this is simplified by Verovio's retention of MEI identifiers in the produced SVG output.",
        "article_title": "Contextual interpretation of digital music notation",
        "authors": [
            {
                "given": "Kevin",
                "family": "Page",
                "affiliation": [
                    {
                        "original_name": "University of Oxford",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "Lewis",
                "affiliation": [
                    {
                        "original_name": "University of Oxford",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            },
            {
                "given": "David",
                "family": "Weigl",
                "affiliation": [
                    {
                        "original_name": "University of Oxford",
                        "normalized_name": "University of Oxford",
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": "https://ror.org/052gg0110",
                            "GRID": "grid.4991.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionCinema as a new cultural industry at the dawn of the twentieth century has had a significant impact on the social, cultural and economic infrastructures of modernizing societies. Although cinema from its first emergence was widely adopted, the penetration of cinemas on the exhibition market (number of cinemas, attendance figures) has shown remarkable differences between European countries. Considering the density of cinema theatres and attendance figures as a marker for market penetration, in particular The Netherlands and Belgium stand out: while Belgium abounded in cinemas and film attendance, cinema density and attendance in the Netherlands were traditionally low (Biltereyst and Meers, 2007).Several scholars have attempted to explain these differences in market penetration, yet no comprehensive, satisfactory explanation has hitherto been found. The low cinema density and attendance in The Netherlands has been explained from the organization of society in various religious and ideological 'pillars' (pillarization) that, until at least the 1970s, compartmentalized the social, cultural, economic and political spheres of society but that excluded the cinema market, that operated 'neutrally' (Dibbets, 2006). However, in similarly compartementalized Flanders a pillarized cinema landscape was successfully created (Biltereyst and Meers, 2007). Others have pointed out that class might have been a possible factor (Thissen and Van der Velden, 2009). Finally, the organization and economics of the industry have been identified as influencing the distribution of cinema theatres (Dib- bets, 2006;Boter and Pafort-Overduin, 2009). Central questionIn the context of the research project CINEMAPS the authors aim to map cinema markets in the Netherlands and Flanders in the 1950s, 1960s and 1970s in a comparative study, combining a geospatial analysis of cinema density in both areas with data on pillarization, class and the organization and economics of industry. By projecting these data on historical maps in QGIS, the geographical distribution of different types of cinemas can be compared with patterns in cinema-going and local governmental policies in both countries. This multi-layered, international map functions as a heuristic tool to identify those areas that are interesting for further, in-depth analysis of the factors that explain the differences in market penetration. As such, the project will provide an answer to the core question of how the development of the cinema, as a specific cultural industry, interrelates with the social and cultural dimensions of modern public life in The Netherlands and Flanders, in particular pillarization, class and organization and economics of the industry. In this short paper we discuss our approach and method and present the first results. MethodIn the past decade, the use of GIS mapping technologies has proven a productive tool for analyzing the geo-spatial dimensions of cinema culture (Horak, 2016). The use of mapping is coherent with recent spatial orientations in film-historical scholarship, which \"focuses on cinema as social experience, conditioned by factors such as transportation networks, ethnicity, and social group as well as cinema architecture, ticket prices, and the changing patterns of work and leisure.\" (Hallam and Roberts, 2014: 20) Such a comprehensive, spatial approach can help to understand how cinema was experienced in the past and how cinema-going influenced the construction of social identity.The use of GIS technology allows us to study the interrelation between cinema location and the social and economic dimensions of cinema culture at an unprecedented scale. For the comparative research on The Netherland and Flanders, we adopt a three-tiered approach. First, we map all the cinemas according to their typologies, distinguishing between permanent theatres, theatres with occasional screenings and travelling cinemas. Second, while acknowledging the fact that cartels such as the Netherlands Cinema Alliance (hereafter: NBB) extended a strong control over the cinema market (Van Oort, 2016), we also include local government data on the map, to acknowledge the influence of pillarized municipal policy on local cinema cultures. Finally, we map (expected) audiences in relation to their political ideology, religious denomination and income or class. In order to account for the limitations of geospatial technologies in explaining complex human cultural interaction (Verhoeven et al., 2009), in the next phase of our research we will use these maps to identify case studies for further, in-depth analysis. Mapping cinema density in its socioeconomic context -first resultsThe datasets used (census data, data on religious denomination, local election results, the Cinema Context and Verlichte Stad cinema databases) partly had to be digitized, and all of them had to be harmonized. The harmonization processes consists of equaling the granularity of comparison both nationally and internationally (e.g., comparing municipalities and cantons), the classification of categories (typologies of cinemas, political parties, and religious denomination), and solving discrepancies in periodization between the different datasets. For some datasets harmonization models are available (e.g., for religious orientation), for others they need to be created. The data on cinema locations in Flanders are currently being georeferenced and combined with the harmonized census data and other social datasets.Since the Dutch data were most complete, in the first phase of the project we focused on mapping cinema density in the Netherlands. The map in Figure 1 shows the geographical distribution of Dutch commercial cinemas in relation to the expected cinema attendance. In general, we can conclude that the distribution of cinemas correlates with the level of expected cinema attendance: many permanent theatres in areas with a high expected attendance, mostly travelling cinemas in areas with a low expected attendance. This is not the case for the area in the middle and East of the country (cities of Apeldoorn and Enschede), which couples high attendance to low cinema density. This invites further research into the particularities of those areas.Besides the influence of the organization and economics of the industry on the distribution of cinema theatres by the NBB, the local municipal policies were another major influence on cinemas and film screenings. Municipalities issued or refused permits for opening cinemas. Besides, they could prohibit certain film titles by arguing that they presented a threat to local order. Lastly, municipalities could impose (high) local taxes on cinema screenings. In short, this hitherto ignored data provides insights on the influence of pillarization on the distribution of cinemas. The map in Figure 2 shows a correlation between protestant municipalities (known for discouraging cinema attendance) and low cinema density, whilst Catholic municipalities show a higher number of cinemas. This suggests that religious orientation did influence cinema market penetration, in spite of the supposed neutralizing role of the NBB. Conclusions and future workThe first geospatial analysis of Dutch cinema culture yielded a number of preliminary conclusions. First, when identifying the data on Dutch cinema locations, it turned out that previous studies had ignored the large number of commercial cinema screenings in places frequented by travelling cinemas (310 in 1949). Although these cinema screenings constitute only a small percentage of the total cinema attendance (1%), the presence of these travelling cinemas does give rise to revisit the assumption that cinema density in the Netherlands was extremely low. Second, in some areas we see a high level of expected cinema-going, but very few cinema theatres -this can be researched further by combining the analysis with data on income and class of the expected audience, as well as further indepth case study analysis. Third, the mapping of cinema density in relation to religious orientation questioned the assumption that pillarization was not relevant because of the neutralizing role of the NBB. These findings invite a more fine-grained study of local policies as well as film programming. In the next phase we will extend this study by comparing the Dutch cinema market to the Belgian one and use this comparative research to identify case studies for more in-depth research of local specificities of (not) going to the movies.  ",
        "article_title": "Not) Going to the movies: a geospatial analysis of cinema markets in The Netherlands and Flanders (1950-1975",
        "authors": [
            {
                "given": "Julia",
                "family": "Noordegraaf",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            },
            {
                "given": "Jolanda",
                "family": "Visser",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            },
            {
                "given": "Jaap",
                "family": "Boter",
                "affiliation": [
                    {
                        "original_name": "VU University",
                        "normalized_name": null,
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Daniel",
                "family": "Biltereyst",
                "affiliation": [
                    {
                        "original_name": "Ghent University",
                        "normalized_name": "Ghent University",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/00cv9y106",
                            "GRID": "grid.5342.0"
                        }
                    }
                ]
            },
            {
                "given": "Philippe",
                "family": "Meers",
                "affiliation": [
                    {
                        "original_name": "University of Antwerp",
                        "normalized_name": "University of Antwerp",
                        "country": "Belgium",
                        "identifiers": {
                            "ror": "https://ror.org/008x57b05",
                            "GRID": "grid.5284.b"
                        }
                    }
                ]
            },
            {
                "given": "Ivan",
                "family": "Kisjes",
                "affiliation": [
                    {
                        "original_name": "University of Amsterdam",
                        "normalized_name": "University of Amsterdam",
                        "country": "Netherlands",
                        "identifiers": {
                            "ror": "https://ror.org/04dkp9463",
                            "GRID": "grid.7177.6"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The Neoclassica research frameworkThe Neoclassica research framework ( Donig et al., 2016) was conceived to provide scholars with new instruments and methods for analyzing and classifying artifacts and aesthetic forms from the era of Classicism (ca. 1760-1860). The neoclassic movement was of almost global scale-affecting architecture and design from Sidney to New York, and from Athens to the outreach of the Russian Urals-while relating to a common reference in classical antiquity, therefore making it an ideal topic for studying processes of stylistic transformation.It accommodates both traditional knowledge representation as a formal ontology and data-driven knowledge discovery, where cultural patterns will be identified by means of algorithms in statistical analysis and machine learning, having in particular the potential to uncover hitherto unknown patterns in the source data. The outcomes of the top-down and the bottom-up approach will be united in a consistent, unified formal knowledge representation.Motivated by the need to combine object classification with domain knowledge representation, the ontology focuses at the moment on artifacts (in particular furniture and architecture) and their components. Following the preliminary hypotheses that aesthetic forms in furniture and architecture are in closest communication with each other due to constructional commonalities and their shared reference of the Classic, we decided to start developing the knowledge discovery module of Neoclassica by classifying artifacts in digital images. Knowledge discoveryIn this paper, we report on our efforts for using deep learning for classifying artifacts in digital visuals. We chose a deep learning approach for our classification method because of its current superiority over other methods and still rising accuracy over the last years in nearly all image classification and object detection challenges.Initially, we compiled a body of images both from commercial sources such as auction houses, antique dealers and other public sources. Due to the complex copyright situation, this corpus can not be redistributed. In order to make our experiment reproducible and since the Metropolitan Museum of Art (MET) has released 375,000 images in the public domain (The Metropolitan Museum of Art, 2017). we assembled a corpus of 379 artifacts relevant to our research. We processed this corpus with the same algorithm as the original proprietary corpus and released the data together with the source code (The Neoclassica Project, 2017). Classifier descriptionThe main classifier for our experiments is a Convolutional Neural Network (CNN). It classifies an input image as a whole.In a first step, we applied a standard implementation of a CNN (namely VGG19 ( Russakovsky et al., 2015)). The results were not satisfactory for our needs. It classified the type of the object depicted in the image with an accuracy of 0.37.In a second step, we opted to employ pre-training, a common technique for improving accuracy in neural networks. We experienced that available pre-trained classifiers for generic image classification proved ineffective in our case. Most of them are trained on a specific subset of ImageNet ( Deng et al., 2009), containing 1000 classes. These classes are broadly spread around everyday objects like dogs and planes. This led us to assume that the amount of very different classes that don't occur in our corpora interfere with the classification. Following that hypothesis, we decided to train the algorithm on a specific subset compiled from ImageNet mainly containing different furniture objects like tables, chairs, and cabinets. They sum up to 35,000 images. The first training step with these images resulted in an accuracy of 0.54 of classifying the object correctly. First layoutThe first corpus contained 2,129 images representing 300 European period artifacts mostly in a colored format of highly diverging quality and resolution. They depict the objects fully, partially or are close-up shots of specific forms. We coarsely annotated these images by manually labeling them on the level of folders. The concepts applied during this labeling process are directly taken from the Neoclassica ontology and describe concepts for types of artifacts. These concepts were derived from period sources.The depth of the class hierarchy was partly reflected by the folder structure. The folder labeled \"Chest of drawers\" contains all instances of this class. Their labels in turn reflect the names of all the subclasses in the most extensive specification (e.g. semainier, Wellington chest, commode scriban).After pre-training, the next step was fine-tuning with this corpus. The accuracy was 0.44, the F1 measure 0.44. Second layoutThe second corpus was assembled from open data released by the MET. It contains 1,246 images representing 379 European and American period artifacts ranging roughly from 1780-1840 including some transition pieces, drawings, and prints. They also depict the objects fully, partially, or are close-up shots of specific forms. We used the titles provided by the MET and manually aligned them with the Neoclassica ontology.The overall mean accuracy over all classes was 0.36, the F1 measure 0.21. For the computation of these numbers, all results that are non-computable (due to only having one image in either the train or test set) were removed. These low numbers result from the existence of two many artifacts represented by only one image, thus making a split in training and testing data meaningless. However, applying pretraining using same ImageNet corpus as in the first layout yielded a mean accuracy over all classes of 0.59 and a F1 measure of 0.58.In order to achieve better results and since the classifier classifies the image as whole, we excluded all images that did not depict the whole artifact. We kept multiple copies of the same image if they were used to describe a different but similar object. We split the images depicting multiple objects so that the resulting images represent only one artifact. We also processed these images so that neighboring objects were covered with solid colors. The images that could not be split (e.g. room interiors) were excluded from the corpus.Using the same settings with the curated corpus and with pre-training we achieved an accuracy of 0.77 and an F1 measure of 0.76. ChallengesWhile pre-training and improving the curation process helped us to raise the accuracy, we assume that there is room for improvement.Parameters to be taken into consideration include the small size of corpora and how to overcome this limitation since this limits the effectiveness of a neural net. Additionally, since pre-training has been proven to enhance the results, it is rational to assume that a pre-training corpus better suited to period artifacts would improve the results further. Third, our experiment was affected by the limitation of the standard implementation of the CNN which classifies the image as a whole and not parts of it.Outlining parts inside an image and classifying them is a difficult task for machine learning methods. Recently, a new type of neural net emerged that tackles this challenge: Regional CNN (RCNN). It is implemented most prominently in an algorithm called Mul- tiPath Network (Zagoruyko et al., 2016).4Current improvements: Using a Regional CNN We are manually annotating regions within the images with classes from the ontology for training a RCNN that locates objects.The implementation of the RCNN is divided into two steps. First, it detects objects in the image and draws their outline as a polygon. The second step is classifying the outlined objects using the included standard CNN.Preliminary results of MultiPath Network with default pre-trained settings show that the first step of the RCNN already outlines objects in our corpora within reasonable limits. The corpus for pre-training is COCO (Lin et al., 2014). Naturally, specific domain objects are not located and the class names are too generic. For our purpose, fine-tuning on a custom annotated corpus is essential. An RCNN requires a more detailed corpus. The exhaustive task of manually draw the objects' outlines within an image promises higher quality in locating objects (first step) and is necessary to classify the objects according to the ontology (second step). Future workWe decided to take two steps in the near future for improving our results.First, we are compiling a new corpus to train the RCNN with, avoiding pitfalls like inconsistent quality, heterogeneous image rights and an inadequate distribution of image per class. Here we would like to go a dual approach. Together with domain experts, we intend to collate a corpus from the large repository of a major auction house, providing us not only with a selection of artifacts' images but also with texts to be used in multimodal analysis.On the other hand, this kind of artifacts may exhibit provenance issues (e.g. heterogeneity or lack of provenance). We will thus compensate for such issues by digitizing a major corpus of neoclassical artifacts forming an ensemble and comprising artifacts in multiple modes having evolved in close reference to each other. Therefore, we have entered a partnership with the Dessau-Wörlitz UNESCO world-heritage site, an almost untouched complex of manor houses and their furnishings in early neoclassical style.Regarding the annotations, we are developing our own semantic annotation and ontology population tool since January 2017. The tool will create an annotated corpus. The actual annotation process will be conducted in cooperation with emerging domain experts from the chair of Visual Culture and Art History at the University Passau. ",
        "article_title": "Object Classification in Images of Neoclassical Artifacts Using Deep Learning Classifying aesthetic forms -a methodology at the heart of art history",
        "authors": [
            {
                "given": "Bernhard",
                "family": "Bermeitinger",
                "affiliation": [
                    {
                        "original_name": "Universität Passau",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Simon",
                "family": "Germany",
                "affiliation": [
                    {
                        "original_name": "Universität Passau",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Maria",
                "family": "Christoforaki",
                "affiliation": [
                    {
                        "original_name": "Universität Passau",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "André ",
                "family": "Freitas",
                "affiliation": [
                    {
                        "original_name": "Universität Passau",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Siegried ",
                "family": "Handschuh",
                "affiliation": [
                    {
                        "original_name": "Universität Passau",
                        "normalized_name": null,
                        "country": "Germany",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionVisualization of text can be a useful exploration tool for looking at the corpus of a poet, especially when dealing with a prolific author with a large body of output over the years. In this work, we describe a flexible and extensible tool for analyzing the corpus of a poet, and make a case study of Nâzım Hikmet Ran. Since poetry has its own challenges over plain text, we have developed novel ways of visualizing the structure, the rhythm and affective tone of each poem, as well as ways of looking at the continuities (or discontinuities) of features in the entire corpus over the years. The designed system integrates a database for holding metainformation, and a website for creating and linking interactive, parameterized visualizations.Nâzım Hikmet Ran is one of the most famous poets of Turkey. Although he was a great patriot, he has spent many years in prison and in exile due to his communistic political views. His poetry is translated into more than fifty languages. We believe this tool can be particularly useful to compare different translations of the poet's work, to see how certain stylistic or semantic features are retained (or lost) during translations. Related WorkMost poetry visualizations focus on the aesthetics of information rather than the functional aspects. An example is Diana Lange's visualizations that transform individual poems into beautiful visual displays, resembling flowers. A similar project is Boris Müller's Poetry on the Road, which turns a text into an image through an arbitrary transformation function, for instance by treating every word as a location and creating a heat map of the entire text. The outcomes of these projects do not tell us much about the poets or the oeuvres in question.In contrast to such artful renderings of poems, there are studies that focus solely on the grammatical and structural problems of poetry writing. Such studies rather try to find quantitative ways to analyze poems, enabling a computational approach for the evaluation of technical quality and subtlety of the rhymes (Opara, 2014;Dalvean2015). In the same line of research, there are visualization tools such as Graphwave, SentimentGraph, SentimentWheel and Ambiances ( Meneses and Furuta, 2015). Such visualization examples constitute the starting point of our explorations for devising a new visualization system that is both scalable and modular in nature, i.e. a tool that would accommodate different natural language processing (NLP) tools, as well as new visualization techniques. MethodologyIn this section, we briefly describe the database structure, as well as the software tools used to create the system. DatabaseAfter his death, Nâzım Hikmet's collected works appeared in a single volume (Nâzım, 2007). The digital version of this volume is not publicly available, but we received a special permission from the publishers to use this volume.Nâzım is a poet who paid special attention to the visual structure of his poems and it is imperative to retain this structure as accurately as possible. Consequently, line indentations were kept intact for each line, as well as the fonts of the individual words. We also paid attention to the fact that the collected works included some text written in prose. Thus, the database structure, depicted in Fig. 1, is entirely hierarchical and ordered according to books, works in a book, lines in a work, words in a line, and characters in the words. This may seem to be an extensively elaborate representation, but it allows detailed structural analysis, as well as the analysis of visual and rhythmic features of each work.  SoftwareSince the project involves a dynamic, parametric and interactive system, many software technologies were used. To keep structured data, user data and web page related content, a MySQL database was used. A Turkish-based affect analysis tool was integrated with the system, and Perl was employed to read and parse data for the affect analysis tool. The main programing language of the project is Java and all back end code is also developed in this language. The Spring Framework was used to create the model-view-controller (MVC) structure of the application. In the front end of the application, AngularJS was used to create the MVC structure and to create a single page application. Moreover, to make the application responsive, CSS3 and Bootstrap technologies were used for mobile phone support. NLP ChallengesIn order to parse Nâzım's corpus, a Turkish Morphological Parser and Disambiguator was used (Sak et al., 2008). With the help of this tool, we get part of speech tags of the words, as well as some grammatical information about verbs (i.e. conjugation and tense) and about words' grammatical number. For certain instances, the morphological parser suggests more than one form or number. To solve such problematic instances, an off-the-shelf disambiguator was used. The results of this disambiguation tool suggests the most appropriate form for a given context, which helps in making a decision on the preferred form of a given verb, noun, or pronoun.The system was enhanced with a text-based affect analysis tool, which returns valence, arousal and dominance values for a given sentence and each word in that sentence (Aydın Oktay et al., 2015). One of the challenges in parsing poems versus prose texts is the lack of a specific notation for indicating the end of a sentence. For the sake of simplicity, each line of a given poem was treated as a sentence, and valence, arousal and dominance values were computed for every word and line individually. These values are stored in the database for fast retrieval. The System InterfaceThe generated system works as an interactive visualization tool with a web interface. For the user experience of the web system, a responsive interface is prepared that can even be reached via smart phones. Also, to keep data alive and to allow flexible operations, a single page application is created, with which users can surf between different tabs without losing information. The system design is modular and expandable, as each work unit is separated such that new visualizations can be easily added to the system. The system can also be tailored to visualize a new database easily. The only requirement is that the work of the artist be parsed in the same hierarchical way, and placed on a SQL-capable database. VisualizationsOne of the motivations behind the visualizations is to give information about the analysis on the corpus of Nâzım, and other poets when the database is expanded by the addition of new authors. The tool incorporates a search function, and allows different visualizations to be prepared from the results of the search. Most of the prepared visualizations are interactive charts. They can be used for showing a term's usage over the years, over geographic locations and over publications. The search can be conducted on a collection of works, or in a single work. A separate page was created for searching for a sequence of words, and to prepare comparative visualizations.We briefly describe two visualizations here to serve as examples. The first visualization is called the \"poetry barcode\" (see Fig. 2). In this visualization, one poem is visualized, and each line of the poem is represented by a horizontal line. The length and the color of the lines are set according to NLP and affect analysis results, and the lines form six different columns, which show the change of line lengths, usage of active/passive phrases in a line, inflections of words in a line according to person information, as well as valence, arousal, and dominance values of each line.Nâzım has a lot of stylistic features in his poems. To be able to analyze and extract these features, we have prepared visualizations about the usage of alliteration and his unique verse structure. Alliteration is a stylistic device, in which a number of words, having the same consonant sounds, occur close together in a series. To quantify alliteration, a measure was developed that uses the background frequency of each letter in the poet's corpus. By using a sliding window based evaluation, letter frequencies are calculated, and compared with the base frequency of that letter in the corpus. Fig. 3 illustrates the automatic alliteration detection. Fig. 4 shows a number of additional visualizations in a bird's eye view, including a visualization about passive/active voice usage. ConclusionsA complete web page opened to the wider public is in construction, as it requires some security features due to copyrights of the works in the database. But the system is operational in the offline mode, and already provides many visualization options.Since the database contains composition years and places for poems (where available), it is possible to search for words that are historically relevant for Nâzım. To give an example, a search for the words \"hürriyet,\" (freedom) and \"hapis\" (jail) restricted to 1919-1938 and 1938-1963, we can easily see that these words' usages are significantly increasing after 1938, when he was arrested for the first time. Other possible uses include the visualization of words associated with different colors, prominent in his poetry, over the years.The proposed system keeps data and visualizations separate, but well-connected. This enables the addition of new artists to the proposed system. The tool also can be used as a platform to evaluate poetry translation. We show grammatical and affective features of the words in some visualizations like the \"poetry barcode\". It is possible to use these visualizations to get an idea about translation quality in literal translations, where the emphasis is on word-for-word translation.   Fig. 1 :1Fig. 1: Database structure. Fig. 2 :2Fig. 2: A poem's barcode, visualizing the structure of the poem together with verb conjugations, passive/active verb usage, and emotional tone. Fig. 3 :3Fig. 3: Visualizing alliteration in the poem. Best viewed in color. Fig. 4 :4Fig. 4: A bird's eye view of several visualization options in the system. Best viewed in color. ",
        "article_title": "A Generic Tool for Visualizing Patterns in Poetry",
        "authors": [
            {
                "given": "Onur",
                "family": "Musaoğlu",
                "affiliation": [
                    {
                        "original_name": "Bogazici University (BU), Turkey",
                        "normalized_name": null,
                        "country": "Turkey",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Alkim Almila Akdag ",
                "family": "Salah",
                "affiliation": [
                    {
                        "original_name": "Bogazici University (BU), Turkey",
                        "normalized_name": null,
                        "country": "Turkey",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Albert",
                "family": "Salah",
                "affiliation": [
                    {
                        "original_name": "Computer Science Department - Bogazici University (BU), Turkey",
                        "normalized_name": null,
                        "country": "Turkey",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Emerging from the crucible of the religious upheaval that characterizes the English Renaissance is arguably the most influential English book ever printed in terms of its impact on Anglophone religious, literary, popular, and legal culture: The Book of Common Prayer (BCP). Encoded within its pages is a kind of algorithm, an annually recurring process, a ritual-ization of both private devotion and public worship for generations of post-Reformation English readers. Taking our cue from Brad Pasanek (Metaphors of Mind: An Eighteenth-Century Dictionary, 2015) and Peter Stally-brass (\"Against Thinking.\" PMLA, Vol. 122, No. 5), both of whom have drawn useful analogies between the database and the commonplace book, we employ the creative anachronism of the \"Bible app\" to describe the function of the BCP in early modern England. As the first such \"app\" of its kind, the BCP choreographed religious meaning and ritualized worship for a whole generation of English Bible readers, shaping them into religio-political subjects who were then able to situate their lived experiences within a communally shared time and space. From the perspective of the Early Modern layperson, the BCP provides mediated access to the newly translated biblical text. Of course, from the abstracted perspective of the nascent nation-state of England, the BCP functions as a way to mitigate new anxieties surrounding the democratization of sacred scripture. As the legally established, official means by which sacred text is encountered, the BCP is nothing less than a masterpiece of social engineering. To extend the metaphor of text as program, the BCP can also be thought of as a class, one which can be inherited and sub-classed, instantiated and \"hacked\" according to the agenda of particular readers who would produce, via their nuanced reading of BCP ritual, slightly different kinds of subjects according to the specific context in which they find themselves. Of particular interest to the project at hand is a 1586 BCP which has been highly \"sub-classed\" by one of its owners. Bound together with the prayer book is an entire psalter, whose collection of 150 psalms is cross-refer-enced in a single hand, which also makes occasional thematic/tonal annotations. In our examination of this prayer book, we wish to develop a methodology for ac-cessing the kind of subject such a \"re-engineered\" BCP might have produced. Implied in the very notion of access, of course, is mediation. Within the limited scope of our project, we do not have recourse to the intense amount of labor required to perform a rigorous exegesis of the entire psalter according to how its 16 th-century readers might have read it. What we do have, via the psalter's marginalia, is what one (or perhaps two) reader(s) selected as noteworthy in their BCP-regulated practice of reading the Psalms. We also have our own attempts to thematize and register the tone of those same texts. Given these assets, we attempt to provide via the Psal-ter Project a representation of how a subject produced by this prayer book might look from our perspective. Our hope is that, despite the inherently mediated nature of such a representation, we might provide students and scholars alike a better understanding of the \"programmatic\" nature of religious para-texts like the BCP.",
        "article_title": "The Psalter Project: Providing Mediated Access to Religio-Political Subjects in Early Modern England",
        "authors": [
            {
                "given": "Nandra",
                "family": "Perry",
                "affiliation": [
                    {
                        "original_name": "Texas A&M University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Bryan",
                "family": "Tarpley",
                "affiliation": [
                    {
                        "original_name": "Texas A&M University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Rationale & AnalysisInk After Print is a full-body, playable literary interface. Exhibited at rock festivals, public libraries and train stations (in a French copy of the Danish original), Ink brings full-body haptics to the unbound book in ways that resonate with the embodied online social marketing of YA [young adult] titles. Both Ink and Selp-Helf ask the reader to do real things in the world, and leave traces of those activities in the literary interface, whether it's navigating through the sea of words in \"Ink\" and printing the results, or posting photos of oneself when meeting the YA author, or dressing up as a character (in this case, teen girls dressing as Miranda Sings.)In both cases, writer/readers or \"w/readers\" (Landau, 1999) are having authentic experiences with literary interfaces. \"Spreadable media\" (Jenkins, Ford and Green, 2013) is a byproduct that empowers Ballinger's fans to use her book as a springboard to articulate their own perspectives on identity and gender. Use of digital skills is the precondition for fan interaction. Jenkins reminds us that audiences are individuals, \"produced through measurement and surveillance, usually unaware of how the traces they leave can be calibrated by the media industries.\" Publics are collectives that \"actively direct attention onto the messages they value\" (166). An entire subculture of book fans-often of young adult literature-is using books as totems around which to build worlds made by and through participatory media; Selp-Helf is one strong example. Selp-Helf and other YA books like it are centerpieces of book-specific media microecologies with particularized rules of conduct, aesthetics, and dynamic interaction. \"Playability\" focuses through the book, but exceeds the bounded dimensions of the book itself.As more book marketing focuses on live events captured for and refracted through social media, this paper proposes that book interactivity should do more to engage the actual practice of reading to draw audiences into memorable relationships with the works. Analysis will focus on how the physical aspects of unbound book reading disclose new quantitative and qualitative shifts in mass market book reading practices.Book publishers, loathe to develop content that they can't expressly monetize, run cheap social media campaigns in platforms they don't own like YouTube, Twitter, Instagram and Snapchat. That's where book publishers should look to literary experimental pieces like Ink for how to create \"eventness\" (pace Bahktin) around distribution and play beyond social media.This would involve investment in digital-first book design and possibly a reading apparatus that could be physically moved location to location. Such techniques could scale, having a few select interactive reading \"shows\" that are captured in and for the social media audiences. Book publishers have built the expectation among YA readers that social media is their gathering space. Following the example of Ink After Print, publishers could offer actual, embodied, interactive reading experiences.Ink After Print provides a rich context for readers to experiment with their affective experience of boundedness. The mechanics of Ink gameplay are sufficiently challenging that readers might feel a sense of reward in assembling a poem using the hand-held \"books\"; in this sense, the printed receipt is token of achievement. But it is also a highly portable object and a potential gift: to the ephemeral community of others playing Ink After Print, where you can share your poems with others who have played; and to the virtual community where Ink \"receipts\" are stored in the blog. When I curated a media arts show and exhibited Ink, I observed readers also folding their receipts into small objects that they then shared with others. The untrackability of what people do with their Ink \"receipts\" stands in stark contrast to the databased traces of participation left by fans of Selp-Helf. While Ink does output to a blog, its outputs focus on the words themselves, not the user identity. In this sense, Ink resists the types of identity quantification that feeds and funds corporate sponsorship of social media platforms.  Note  ",
        "article_title": "Rewards: Books, Boundedness and Reading in Participatory Culture",
        "authors": [
            {
                "given": "Kathi",
                "family": "Berens",
                "affiliation": [
                    {
                        "original_name": "Portland State University",
                        "normalized_name": "Portland State University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00yn2fy02",
                            "GRID": "grid.262075.4"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " HermeneuticaIn Hermeneutica, Geoffrey Rockwell and Stéfan Sin- clair (2016) argue for an approach to the digital humanities that deemphasizes the tool and positivist notions of proof. Their proposed approach, also called Hermeneutica, champions tool accessibility over tool sophistication. Similarly, scholarly play is legitimated as a useful step in developing research questions and as a means to reconsider established notions within literary disciplines. The aim of Hermeneutica as a methodology seems to be the generation of interesting humanistic questions as much as the resolution of open questions.Rockwell and Sinclair demonstrate the difference between Hermeneutica and typical DH approaches by quoting from Gary Wong's 2009 blog post:[Typical DH] takes the worst part of the scientific papers (really really long sets of tabular data in the body of the text) and the worst part of papers from the humanities (really really complicated language where simple language would have done) and puts it in one. If this is what the cooperation of computational text analysis and traditional literary analysis yield, I am scared.Because Hermeneutica attempts to join the best parts of these fields, it has the potential to turn DH into a discipline that is more useful for the vast majority of non-DH humanists. It could be the means of accelerating the mainstreaming of DH methods and bringing us to the eventual point where all humanities are digital-a destination Claire Clivaz described succinctly (DARIAH, 2016). VoyantOne feature that distinguishes Hermeneutica from many other DH approaches is its companion set of tools meant to demonstrate its application. Voyant Tools, now referred to simply as Voyant, is a webbased, modular suite of tools meant to be \"worth thinking with\" (Rockwell and Sinclair, 2016: 10, original emphasis). The goal is to accommodate playful exploration of text and sharing of corpora across the web. It is not designed as an industrial-grade text analysis tool, but as a \"toy\" that allows scholars to uncover new questions and gain new appreciation of texts. Current limitations of HermeneuticaA fundamental component of Hermeneutica is that the scholar views text through the lens of Voyant (or other computational text analysis tools), and then synthesizes that experience with their prior knowledge of the text and its milieu. A problem that Voyant addresses, but does not solve, is that many scholars who know the most about specific texts lack the technological skills that would be considered pre-novice in DH circles. Voyant allows everyone with a text and a browser to explore word frequencies, collocations, etc., but it presupposes that the text is available and clean enough for use. In order for Hermeneutica to appeal to non-DH humanities scholars, these issues of text availability and the lack of user skill must first be addressed.On the issue of text availability, it is not often that scholars wish to analyze text that is rare or missing. More often they are interested in text that is protected by various copyright laws, which prohibit posting the text to public websites such as Voyant. Thankfully, in the Unites States at least, Google Books' recent court victory (Stohr, 2016) now permits scholars to publish online the analysis results derived from copyrighted texts, so long as the original text is not recoverable by the user. To this end Rockwell and Sinclair developed Voyant 2's \"non-consumptive\" mode which restricts access to tools that allow full-text views.While such developments represent Rockwell and Sinclair's amenability to meet the ever-evolving needs of Hermeneuticans, accommodating users' lack of technology skill is beyond the scope of their involvement. For example, it is not reasonable to expect the Voyant developers to be concerned over issues of text acquisition or text preparation. Rather, those concerns-while critical to expanding the pool of potential Hermeneuticans-are issues of local implementation. Similarly, it makes sense that Voyant would offer the ability to link to a corpus after uploading the text, but uploading the text and keeping track of various versions of corpora is beyond the scope of Voyant. A local practice of adding some structure around the Voyant suite ought to make Hermeneutica useful to a far greater audience than it is now. ScaffoldingIn the field of instructional design, such structure is called scaffolding. Specifically, scaffolding refers to the process of providing learners adequate introduction and examples before allowing them to attempt a task on their own (Bruner, 1978). For scaffolded Hermeneutica, DH-savvy professionals can work to acquire, clean, and upload text to Voyant (and other tools), and then provide public listings of the resulting corpora. Examples of scaffolded HermeneuticaWe have implemented this scaffolded Hermeneutica approach in our Office of Digital Humanities beginning with the Cormac McCarthy Corpus Project (CMCP). The CMCP includes 13 Voyant corpora of McCarthy's 10 novels: one for the complete works, one for each novel, and two for novels (The Orchard Keeper and The Road) where the narration has been segregated from the dialogue. But the linchpin of scaffolded Hermeneutica is the CMCP's publicly-accessible website that organizes these Voyant corpora. The website is built on WordPress with the Pods content management plugin, and contains information on McCarthy's work, descriptions of Voyant (and other tools), and listings of links to the Voyant corpora. An essential feature of the website's structure is the ability to accommodate revisions to the current corpora as well as the addition of other tools in the future. Already, there is a non-Voyant sentence structure search tool attached as a beta-testing option.A rough version of the Cormac McCarthy Corpus Project was presented at the 2015 conference of the Cormac McCarthy Society. The reaction to these tools being available for public use was strongly positive. One attendee referred to the website as \"a gamechanger.\"The same scaffolded Hermeneutica is being implemented on two other projects: Machado à longa distanciâ and The Modernist Short Fiction Project. Preliminary demonstrations of the approach have yielded similar reactions to what we observed with the CMCP. Non-DH scholars become excited rather than anxious when the digital analysis tools are scaffolded to provide them ready access. In fact, these demonstrations turn into play sessions where non-DH scholars repeatedly request for certain words to be added to the frequency charts and other Voyant panels. ConclusionHermeneutica and Voyant represent the greatest potential for growth in DH not because they are the most technologically or theoretically advanced developments, but because they are the most accessible to non-DH scholars. Still, they don't quite reach the ground level of technology skills possessed by most researchers in the humanities. The scaffolded Hermeneutica approach proposed in this paper seems to span that gap to make Hermeneutica more accessible.  ",
        "article_title": "Scaffolded Hermeneutica for Literary Scholars with Novice Technical Skills",
        "authors": [
            {
                "given": "Jeremy",
                "family": "Browne",
                "affiliation": [
                    {
                        "original_name": "Brigham Young University",
                        "normalized_name": "Brigham Young University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/047rhhm47",
                            "GRID": "grid.253294.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " The HathiTrust Digital LibraryThe HathiTrust Digital Library (HTDL) comprises digitized representations of 15.1 million volumes: approximately 7.47 million book titles, 418,216 serial titles, and 5.3 billion pages, across 460 languages. HTDL is best described as \"a partnership of major research institutions and libraries working to ensure that the cultural record is preserved and accessible long into the future\".The HathiTrust Research Center (HTRC) develops software models, tools, and infrastructure to help digital humanities (DH) scholars conduct new computational analyses of works in the HTDL. For many scholars the size of the HTDL corpus is both attractive and daunting: many existing DH tools are designed for smaller collections, and many research inquiries are facilitated by more focused, homogeneous collections of texts ( Gibbs and Owens, 2012). WorksetsIn many, if not most, DH research endeavours, performing an analytical task across the whole HTDL is neither practical nor productive ( Kambatla et al., 2014). For example, a tool trained to identify genre attributes of 18th century English language prose fiction may not be applicable to 20th century French poetry. The first step is to identify the subset -- of works, editions, volumes, chapters, pages -- to set an initial investigative scope and, indeed, subsequent iterative refinements of a subset as research proceeds. In a corpus as large and complex as the HTDL, finding materials and then defining the sought after subset can be extraordinarily difficult.HTRC has come to call collections of digital items brought together by a scholar for her analyses a \"workset\", created to help the researcher build, manipulate, iteratively define and compare their collections. Reflecting upon input and advice from the DH community, Jett (2015) defines a workset as a machine-actionable research collection realised as:1. Broadly, item 1 identifies the actual data used in an analysis; whereas the remaining metadata items describe the workset itself, aiding workset management throughout the research cycle. Cross-corpus worksetsAs alluded above, numerous criteria can be used to select the constituents of a workset; and several technological implementations could, in theory, realise worksets. In researching the design and realisation of worksets and associated tooling, we are also mindful to remain grounded in their practical application and the needs of scholarly users. We have therefore undertaken our work through discipline-based scenarios in which we can explore the strengths and weaknesses of the HTDL viewed through the prism of worksets.We report one such exploration here, questioning whether (relatively) small, well explored, and well understood corpora can be superimposed over the HTDL to aid navigation and investigation of the much larger and superficially understood HTDL collection?From a system perspective, a cross-corpus workset requires exposing compatible metadata (items 2-6 above) from multiple collections, first used to align common elements, and then to assemble worksets. We take a Linked Data approach and achieve compatibility through ontologies, which might initially be bibliographic (and derived from library records) but should be iteratively extensible into the domain of the subject of study. Examples in early English printEarly English Books Online Text Creation Partnership (EEBO-TCP) is a partnership with ProQuest and over 150 libraries and universities, led by Michigan and Oxford, to generate highly accurate, fully-searchable texts tracing the history of English thought and learning from the first book printed in English in 1473 through to 1700. Between 2000-2009 EEBO-TCP Phase I converted 25,000 selected texts from the EEBO corpus into TEI-compliant, XML-encoded hand-transcribed texts, subsequently freely released in January 2015.In the work reported here, we have conjoined EEBO-TCP with a HathiTrust subset consisting of all materials described in their metadata as being in English and published between 1470 and 1700.To ensure a prototype which simultaneously explored the fit of scholars' needs to the technology and exercised the technical challenges outlined in the previous section, we undertook a 'complete circuit' through the datasets (Figure 1). We: (i) ran a consultative workshop to choose investigations which might form the basis of worksets; (ii) used these abstract worksets to identify concrete requirements for the conjoined metadata; (iii) generated metadata from both corpora according to these specifications; (iv) aligned elements from both datasets in an overlapping superset; (v) realised the worksets identified in (i) using this metadata. Figure 1. Overview of the metadata circuit leading to our cross-corpora workset Motivating worksetsFollowing the workshop we identified the following workset selections; we describe their implementation in subsequent sections:• Find all the works, appearing in both datasets, written by Richard Baxter.• Find works in both datasets published in Oxford.• Find works published outside of London (where the bulk were published).• Find works from both datasets published outside of London in the mid-to late 1600s.• Find all works in the two datasets for authors who have at least once published on the subject of \"Political science\".• Find all works in these two datasets for authors who have at least once published works which are categorised as \"biography\".Regarding the penultimate workset, it is of particular note that this returns results across both datasets, since our EEBO-TCP import did not contain genre or topic information; this association must be entirely inferred from the semantic links via the technology described below. Implementation Metadata requirements and ontology selectionBuilding on Nurmikko-Fuller et al. (2015) and Jett et al. (2016) we surveyed the addressable resources and the schema expressivity of ontologies that could parameterise these classes of workset. We identified parsable information structures in the EEBO-TCP TEI data, appropriate to the test worksets, and selected ontology terms to encode this EEBO-TCP metadata, ensuring compatibility (or at least, for our purposes, comparison) with RDF from the HathiTrust. The resultant ontology collection - the EEBO Ontology, or EE-BOO -includes selections from MODS, Bibframe, and PROV, along with custom elements encoding additional structures (e.g. dates). Creating EEBOO RDF and alignment with HTDLPython scripts manipulated TEI P5 XML, then the Karma Data Integration Tool mapped EEBO-TCP data structures into the EEBOO ontology. Particular attention was paid to dates encoded within strings, an example of rich semi-structured data that can be extracted into structured RDF. Links to author records in VIAF and the Library of Congress (LoC), and multimedia pages in the HTDL and 'JISC Digital Books' website, were generated and added. Finally, author names were aligned between the EEBOO and HTDL triples using a reconfiguration of the SALT tool ( Weigl et al. 2016).24,926 EEBO-TCP Phase 1 records were processed, with 22 distinct types of information in the headers, including 6 different ID types and 3 types of date (publication date of historical work, author associated historical date(s), XML publication/editing dates). EE-BOO incorporates 7 of these datatypes, and extends into subcategories for author names and date types. EEBOO contains 713 unique places, 6,489 unique expressions of Person of which 3,588 have VIAF and LoC IDs.  Workset construction and viewingA Virtuoso triplestore (see also, the Virtuoso Github repository) stores the RDF data (totalling 1,137,502 triples) and provides a SPARQL query interface. Figure 2 shows the overall system architecture. The workset constructor user interface (Figure 3) allows the user to select parameters in a web interface which are, in the background, assembled into SPARQL queries used to create a workset. The interface automatically populates valid attributes that are themselves retrieved from the triplestore, using ontological terms having equivalent meaning across datasets. In combination, the generated triples and SPARQL queries are fully sufficient for expressing the motivating workset definitions described earlier.The workset viewer (also Figure 3) then retrieves RDF workset contents, record metadata, data links, and multimedia links (to the Historic Books collection or the HTDL). Both web applications are written in Python, using the Flask framework, and both rely on the semantic information encoded in RDF and queried using SPARQL.  Conclusion and future workWe have demonstrated the general feasibility of cross-corpus worksets in bringing together HathiTrust content with specialised collections through a specific implementation for early English printed books linking the HathiTrust to EEBO-TCP. Using Linked Data, we see that metadata can be extended in a piecemeal or iterative fashion, potentially moving beyond traditional bibliographic metadata to include semantic structures emerging from scholarly investigation of the worksets themselves; and in doing so support academic motivations and requirements for workset creation.",
        "article_title": "Building worksets for scholarship by linking complementary corpora Background and General Motivation",
        "authors": [
            {
                "given": "Kevin",
                "family": "Page",
                "affiliation": [
                    {
                        "original_name": "Oxford University",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Terhi ",
                "family": "Nurmikko-Fuller",
                "affiliation": [
                    {
                        "original_name": "Oxford University",
                        "normalized_name": null,
                        "country": "United Kingdom",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Timothy W. ",
                "family": "Cole",
                "affiliation": [
                    {
                        "original_name": "University of Illinois, Urbana-Champaign",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "J. Stephen",
                "family": "Downie",
                "affiliation": [
                    {
                        "original_name": "Graduate School of Library and Information Science (GSLIS) - University of Illinois, Urbana-Champaign",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " DiscoveryA major challenge facing scholarship online is that a vast number of digital resources, particularly those produced independently by scholars or smaller institutions, do not have standardized metadata records and are not accessible via any centralized scientific index. GH-D involves development of a peer-reviewed index of scholarly digital objects using Dublin Core (DC) and CLARIN's Component MetaData Infrastructure (CMDI) standards via a customized Blacklight technology stack. PreservationFor scholars developing historical digital projects in North America, there exists no inter-institutional infrastructure for preserving their data and making it openly available. With consultative and knowledge support from CLARIN-D, which is part of the European research infrastructure CLARIN, the GH-D project will establish the first portal to CLARIN in North America at GHI Washington. Central to this process is the implementation of a repository that allows a sustainable storage of the content and the inclusion in a digital environment to ease access, search and an interoperable data formats. The content of the repository and the repository itself adheres to international, widely accepted and supported standards. The high quality of the technical solution and the conformance to standards is secured by an independent organization that gives out the Data Seal of Approval. Like the majority of CLARIN centres in Germany, the GH-D will use a Fedora Commons repository with Apache Solr for indexing and search, components included in the technology stack of Project Hydra. Our partnership with CLARIN promotes open access, open science and knowledge co-creation in the North American context, and is an important component in the overall digital humanities research strategy of the GHI. As an institute of the Max Weber Stiftung, we are also in partnership with DARIAH-DE and arrangements have been made for DARIAH to provide long term preservation of GHI digital projects in their entirety, beginning with the first edition of German History in Documents and Images. Beginning with the GHDI project, GH-D is part of the DARIAH-DE Service Lifecycle program. Production and PublicationAs a knowledge co-creation platform, GH-D will bring together editors, researchers and citizen scientists in the development of innovative online projects. Three such pilot projects are currently in development based on customization, including support for TEI, and internationalization of the Scalar 2.0 platform. GH-D is using Scalar 2.0 for the baseline content management system, particularly on account of its interface features, support for RDF, connectivity to external repositories, Dublin Core support. Hypothes.is integration, and its multiple path navigation system. AnalysisHistorians are increasingly using digital humanities tools to analyze data and express their research findings. A further advantage of storing digital objects within the CLARIN repository the GHI wants to built up is that the full range of corpus linguistic analytic tools of CLARIN can be applied by scientists to GHI textual content. During the first phase of the project we also look to prototyping connectivity to PARTHENOS, another major European infrastructure project. PAR-THENOS integrates within a virtual research environment (VRE) access to data from numerous national archives and a broad set of digital tools which can be chained together into analytic processing workflows. CommunityThe GH-D platform integrates blog aggregation, an advanced discussion system, community-oriented tools, and social media, to facilitate collaborative knowledge communities and open research. This is a pioneering aspect of our project that will investigate the adoption by historians of social and community digital tools in their research activities. We also intend to make use of the unique role the GHI plays as a hub of transatlantic scholarly dialogue and a major knot within an international network of historians in order to facilitate connections between different scholarly communities.  ",
        "article_title": "German History-Digital: A Platform for Transnational Historical Knowledge Co- creation",
        "authors": [
            {
                "given": "Matthew",
                "family": "Hiebert",
                "affiliation": [
                    {
                        "original_name": "University of Cologne",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Simone",
                "family": "Lässig",
                "affiliation": [
                    {
                        "original_name": "University of Cologne",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            },
            {
                "given": "Andreas",
                "family": "Witt",
                "affiliation": [
                    {
                        "original_name": "University of Cologne",
                        "normalized_name": "University of Cologne",
                        "country": "Germany",
                        "identifiers": {
                            "ror": "https://ror.org/00rcxh774",
                            "GRID": "grid.6190.e"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionAs for many civilizations, poetry is an essential part of Chinese literature. Poetry has influenced the development of the literature and language of both classical and vernacular Chinese. Certain of the words that we use today can be tracked all the way back to the Shijing (詩經/shi1 jing1/ -- We show the pronunciations of Chinese characters in Hanyu Pinyin followed by their tones. Here, /shi1/ is for \"詩\" and /jin1/ is for \"經\".), c. 1046BC. Research on Chinese poetry is thus instrumental for understanding Chinese culture, and a lot of invaluable results have been accumulated over the past thousands of years from the study and analysis of Chinese poetry.The availability of digital tools and resources enable researchers to compare and analyze the poetry from certain perspectives that were hard to achieve in the past. In many cases, we can verify the claims of previous researches with solid data, and, in others, we may enrich our understanding of the poetry.The accessibility of increasingly larger datasets strengthens our research potential. In earlier stages of digital humanities, pioneers focused their work on Tang and Song poetry (it is beyond our capacity to list all previous research in this proposal, and we provide just two samples: Hu and Yu, 2001;and Lo et al, 1997) . Now, we can access digitized texts of poems that were published in the periods from 1046BC to modern days.Software tools allow us to study the data from a wide variety of perspectives in an efficient way. Search engines and information retrieval techniques (Man- ning et al, 2008) help us extract relevant texts from a large dataset. Then, researchers can employ domain knowledge for advanced studies with the use of additional tools.In this paper, we showcase research results that we achieved by handling the available data with existing tools in flexible ways. We collected nine representative corpora of Chinese poetry, one each for a major dynasty in Chinese history between 1046BC and 1644AD. We list the corpora in Table 1, where we assign an acronym to each corpus for ease of reference (See Notes, 1). We also show their Chinese names (Collection) and the periods of publication (Time). A collection for the Qing dynasty is unavailable yet because an editorial committee is still working toward the completion of this very challenging goal (Zhu, 1994). Excluding the punctuation marks that were added by the data providers, we have more than 16.5 million characters (see Notes, 2) in the corpora. By flexibly integrating and migrating tools to offer new functions, we can provide researchers with opportunities to investigate Chinese poetry from new perspectives. In the first example, we show a new way to compare the ways that poets use words in their poems. In the second, with our own tools, we can find shared collocations and patterns of poems in different corpora, and this capability allows us to study and compare the styles of individual poets and their dynasties.  A Multi-Faceted ComparisonJiang (2003) compared the usage of \"wind\" (風 /feng1/) and \"moon\" (月/yue4/) in the poems of two of the most famous poets, Li Bai (李白/li3 bai2/) and Du Fu (杜甫/du4 fu4/), of the Tang Dynasty (which existed between 618 and 907AD) by comparing the contents of selected poems. Liu et al. (2015) listed the frequencies of frequent words that used \"wind\" and \"moon\" in Li's and Du's poems. The numerical comparison shows the differences of the poets in a vivid way.The software tools can be designed so that we can inspect not just the original poems or the raw statistics about the original poems, but also more complex comparisons. Table 2 lists the frequencies of frequent bigrams (again, see Notes, 2) that appeared in the poems of four poets, i.e., LSY (for 李商隱/li3 shang1 yin3/), LB (for Li Bai), DM (for 杜牧/du4 mu4/), and DF (for Du Fu) (Note: 李商隱/li3 shang1 yin3/ and杜牧/du4 mu4/ are two very famous poets of the Tang Dynasty)These bigrams are special in that they are formed by concatenating either \"春\"/chun1/ or \"秋\"/qiu1/ with another character, and they represent something related to \"spring\" and \"autumn\", respectively (when used individually, \"春\"/chun1/ or \"秋\"/qiu1/ typically represent \"spring\" and \"autumn\", respectively-see Notes, 1). The numbers \"14;2;16\" in the row of \"春風; 秋風\" and in the column for \"LSY\" indicate that we have 14 and 2 of LSY's poems in which \"春風\" and \"秋 風\" were used, respectively. \"16\" is the sum of 14 and 2.The statistics in Table 2 shed light on the differences in word preferences among the poets. Note that the samples in Table 2 are limited, and that a close reading is necessary to reach any further interpretations. Despite these limitations, we still can explore comparisons from various perspectives. \"春風\" and \" 秋風\" are the most common choices among all of the rows(They appeared 192 times, i.e., 16+98+29+49). In contrast, \"春月\" and \"秋月\" were not as popular (They appeared only 42 times, i.e., 40+2), and none of the poets used \"春月\". In terms of personal preference, \"春風 \" appeared in LB's poems three times often than \"秋風 \". The results of LSY are similar to those fore LB, but DF seems to prefer \"秋風\" instead (The ratio for \"春風\": \"秋風\" is 72:26 for Li Bai, 14:2 for Li Shang Yin, and 19:30 for Du Fu).The entries that have \"0\"s can be linked to strong personal preferences. For instance, LB did not use \"春 雨\" and \"春來\", while he did use \"秋雨\" and \"秋來\". DM is special in that he did not use \"春天\" or \"秋天\".We can provide different ways to compare the styles of poets, e.g., converting the frequencies in Table  2 to probabilities of seeing the same word in the poems. By building a vector space representation (Man- ning et al, 2008) for each poet, we can calculate a score of similarity for style as in many other researches. Networking Names and WordsIn addition to comparing the words of the famous poets, we may also attempt to compare the words and themes of the poems that were produced by friends. We can look up whether two poets were friends in professional databases like the China Biographical Database (CBDB) (Fuller, 2015). A database like the CBDB can also provide alternative names of poets so that we may algorithmically find friendships among poets by checking their writings (Liu et al, 2015). After identifying a group of poets who were friends, we can investigate whether the words, styles, and themes of their poems are related. A procedure such as which we used to produce statistics like those in Table 2 may be useful.A poet may be influenced by another poet even though they were not personally acquainted. It is believed that poets of the same school of poetry (we use \"school of poetry\" to translate \"詩派\" /shi1 pai4/) share similar styles or words. Hence, information about the membership of a school of poetry provides a starting point for an investigation.We may also search for poets who shared the same words and collocations in their poems as a clue for an indirect friendship. Given the millions of characters in our corpora, we need to have an efficient mechanism to identify poems that shared collocations and patterns ( Liu and Luo, 2016), and using our own tools, we can precisely identify words that were shared by poems of different poets and of different dynasties (see Notes, 2).The ability to identify the shared words between individual poets also automatically allows us to compare the patterns that are frequently shared between any two corpora. In Figure 1, two words are connected if they frequently co-occurred in poems. Part (a) shows the shared collocations in poems in the YSX and CTP, and (b) is for the shared collocations in poems of the LCSJ and CTP. The differences between (a) and (b) indicate that the highly shared collocations changed from dynasty to dynasty, i.e., from Tang to Yuan and from Tang to Ming. A collocation with a different word may suggest that the word contributes a different sense in the poems, e.g., \"春風-桃花\" and \"春風-何處\" in (b), and this can be verified by reading the poems that used these collocations. Sometimes, the links suggest replaceable words, for instance, both \"千里\" and \" 萬里\" can go with \"十年\" in both (a) and (b). It should be noted that the collocations often carry information about the imagery of the poems. Figure 1. Frequently shared collocations between poems of two corpora (a) YSX and CTP (b) LCSJ and CTP We briefly discussed how we studied two new research problems by flexible applications of our tools. The new tools provide new forms of data as in Table 2 and Figure 1 for interesting and useful research. We are working toward an in-depth understanding of Chinese words by studying when, who ( Liu and Luo, 2016), and how the words (see Figure 2) and their collocations and patterns were used in Chinese poetry, and our tools will help domain experts study challenging and interesting problems about it (Liu, 2016). We also hope that the information and visualization that we have found and established for words can contribute to an interactive version of the complete Chinese lexicon ( Cheng et al, 2016).  Concluding Remarks AcknowledgmentsThis research was supported in part by the grant 104-2221-E-004-005-MY3 from the Ministry of Science and Technology of Taiwan, the grant USA-HAR-105-V02 from the Top University Strategic Alliance of Taiwan, and the Senior Fulbright Research Grant 2016-2017. Notes1. 詩經 /shi1 jing1/, 楚辭 /chu3 ci2/, 漢賦(文選 ) /han4 fu4 (wen2 xuan3)/, 先秦漢魏晉南北 朝詩/xian1 qin2 han4 wei4 jin4 nan2 bei3 chao2 shi1/, 全唐詩/quan2 tang2 shi1/, 全宋 詩/quan2 song4 shi1/, 全宋詞/quan2 song4 ci2/, 元詩選/yuan2 shi1 xuan3/, 列朝詩集 /lie4 chao2 shi1 ji2/ 2. It is important to briefly mention the differences between Chinese characters and words for readers who are not familiar with the Chinese written language. Characters are the basic units for Chinese words. A Chinese word can be formed by one or more characters. For instance, \"水\"/shui3/ and \"果\"/guo3/ are two characters. They can be used individually to represent \"water\" and \"results\", respectively. A word consisting of n Chinese characters can be called an n-gram in linguistics, e.g., \"水果\" is a bigram that represents \"fruit\". While the majority of the words in vernacular Chinese are bigrams and trigrams, the proportion of unigrams in classical Chinese is very large. ",
        "article_title": "Flexible Computing Services for Comparisons and Analyses of Classical Chinese Poetry",
        "authors": [
            {
                "given": "Chao-Lin",
                "family": "Liu",
                "affiliation": [
                    {
                        "original_name": "National Chengchi University",
                        "normalized_name": "National Chengchi University",
                        "country": "Taiwan",
                        "identifiers": {
                            "ror": "https://ror.org/03rqk8h36",
                            "GRID": "grid.412042.1"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThis project uses network graphs to depict musicians' careers in late seventeenth-century Venice. The current network graph, viewable on the Musicians in Venice web page, demonstrates relationships between musicians and the institutions that employed them. The graph is bimodal, with nodes representing musicians (\"people\" nodes) and institutions (\"place\" nodes). Archival texts are incorporated into the visualization, with transcriptions of records that indicate musicians' activity included in the node attributes. The entire project is text-based, with data derived from XML transcriptions of archival records in addition to assigned metadata. The current website is a proof of concept for a larger project that would demonstrate ways of displaying text as part of the network graph and using texts in creating network visualizations.The current graph focuses on the career of the composer Giovanni Legrenzi. Legrenzi worked for several prominent Venetian institutions from the early 1670s to his death in 1690 and ultimately was appointed to the most prestigious musical posts in the city. The musicians he worked with also served multiple institutions, either simultaneously or in succession, and their relationships with other musicians, patrons, and administrators often facilitated their movement between institutions. Legrenzi's Venetian career presents an excellent case study of these musical connections in late seventeenth-century Venice, and studying these connections demonstrates how networks of musicians functioned in this time period. This study also provides a representative sample of how network visualization effectively demonstrates patterns in musicians' careers in Venice.The texts in the network graph include transcribed administrative documents, mostly unpublished, from the Venetian institutions where Legrenzi was active between 1670 and 1690. These are primarily payment, hiring, and termination records, which document the activity of the musicians employed by or affiliated with these institutions. Several generations of musicologists have used these documents to identify where different musicians were working, when they were employed, and in what capacity (Bonta, 1964;Moore, 1981;Termini, 1981;and Selfridge-Field, 1994). In this sense, this treatment of the documents is standard to the field, but applies DH methodology and processes to create visual representations the data. This provides a new perspective on the sources. For instance, grouping the \"person\" nodes by centrality results in multiple sub-groups, and proximity among these nodes demonstrates shared institutions and implied communities. (Hanneman and Riddle, 2005: Chapter 10) (Mary Russell Mitford's text-based network analysis of Robert Southey's Thalaba the Destroyer provides an excellent example of this. In addition to demonstrating the behavior of agile, well-connected nodes, the graph displays modules of nodes as units of analysis ( Wasserman and Faust, 1994: 4). MethodologyThe transcriptions of the original documents were encoded in XML with tags and attributes determined by the Text Encoding Initiative (TEI). I used the XML markup to tag the information I wanted to include in my data while retaining the complete original text. In general, I tagged the name of the musician, the name of the institution with which they were connected, and the date and location of the connection.I then extracted the information for the network from the XML document using an XSLT. For this project, the stylesheet extracted the data assembled in the person index of the XML document and combined it with the entire text that documented each event. The resultant document was a CSV file of raw data that had to be separated into sets of nodes and edges to be read by the visualization platform. The data visualization for this project was generated using Gephi, which is an open-source platform for exploratory data analysis. For this project, I generated the network graph in Gephi by importing two CSV files: one with the nodes and their attributes and ID numbers, and one with the edges and the source and target data. The online version of the network graph includes all the text associated with a \"person\" node in the network as a node attribute. When the user selects a person node in the online version, it displays the transcriptions for every document mentioning that person in the Information Pane. (I exported the data using the SigmaExporter plugin for Gephi. Version 0.9.0 from Gephi Thirdparties Plugins. The code is available on Github.) Displaying all the text associated with a person node presented a challenge as the XSLT exported each connection between a musician and an institution as a new line in the CSV. For instance, if a musician was hired by a church in 1674, received a raise in 1678, and retired in 1686, there would be three lines in the CSV. When imported into Gephi, this would create three connecting edges between the person node and the place node. This created a misleading network graph, as the number of documented connections between people and places depended on the nature and availability of the records. To combine all the texts associated with the events for each person in the index, I concatenated the raw data using a PHP script and saved the results as a TSV file that could be imported into Gephi. As a result, the entire context of the decrees and decisions surrounding that musician's activity appears in the Information Pane, which also displays and links to the institutions associated with that musician. The PHP script also included HTML formatting to better distinguish the entries from one another in the visualization. Once my nodes and edges were imported into Gephi, I used a layout that grouped the nodes by degree of centrality (Algorithm in Gephi based on Blondel, Guillaume, Lambiotte, andLefebvre, 2008. Resolution in Gephi based Lambiotte, Delvenne, andBarahona, 2009). Network graph in Gephi Next stepsThe current graph is the foundation of a more extensive project that uses musicians and musical activity in Early Modern Venice to benefit scholars working with TEI in similar for-hire environments. In these environments, hubs of activity define the relationship between individual practitioners in the historical equivalent of a sharing economy. In the long-term, I propose an online data repository and network graph of musicians working in seventeenth-century Venice that would eventually provide a roadmap for scholars embedded in the TEI transcription model, but with an interest in an automated process for applying TEI text analysis to network analysis.In addition to the bimodal graph that highlights relationships between musicians and their employing institutions, I also hope to create a unimodal graph documenting relationships among the musicians. Creating the unimodal network and a supporting web application will be a challenge as relationships between individuals can be complicated and dynamic. This will take considerable work in effectively diagramming entity relationships and building web applications that go beyond the \"out of the box\" functionality of the Gephi export plugin. The website for the Six Degrees of Francis Bacon project provides an excellent example of a unimodal graph; here the relationships are defined by circumstances (such as \"father to\" or \"colleague of\") or actions (such as \"met\" or \"wrote to\"). Linked Jazz, which uses documents as node attributes to demonstrate relationships and meaningful connections between jazz musicians in an interactive network graph, is also an excellent example. Both sites feature custom-designed entity relationships and interactive features in their web design that I want to emulate in my own network graph.For the unimodal version, I would expand the data beyond Legrenzi's Venetian career, increasing the time frame and the number of institutions represented in the graph, and include different kinds of sources, such as periodicals, correspondence, and notarial records. Developing relationship typologies is such a crucial component to the project. This will require creating custom node metadata based on the information from the primary sources and agile development as the number and variety documents expand. The result will be a project that can better serve other scholars of Venetian music and culture.  ",
        "article_title": "Using Archival Texts to Create Network Graphs of Musicians in Early Modern Venice",
        "authors": [
            {
                "given": "Mollie",
                "family": "Ables",
                "affiliation": [
                    {
                        "original_name": "Indiana University",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            },
            {
                "given": "Tassie",
                "family": "Gniady",
                "affiliation": [
                    {
                        "original_name": "Indiana University",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            },
            {
                "given": "Kalani",
                "family": "Craig",
                "affiliation": [
                    {
                        "original_name": "Indiana University",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            },
            {
                "given": "Grace",
                "family": "Thomas",
                "affiliation": [
                    {
                        "original_name": "Indiana University",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            },
            {
                "given": "Adam",
                "family": "Hochstetter",
                "affiliation": [
                    {
                        "original_name": "Indiana University",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " BackgroundAs one of the defining features of Latin literature, the influence exerted by the Greek linguistic and textual traditions has remained a key focus of the Classics. One of the manifestations of this intersection of cultures is in the topic of wordplay, or puns. Authors such as Vergil and Lucretius have been shown to utilize the metrical rules of pronunciation to embed Greek word forms in their poetry, and in the process, reference others works of literature and convey ideas beyond the superficial level of the text. While the influence of Greek writings upon Latin literature has received considerable scrutiny, the use of Greek puns has received only limited inquiry. Previous studies on puns, notably those of Snyder (1980) andAhl (1985) are limited in scope and focus mainly on the relationship of Latin puns to Latin literature. Furthermore, studies which focus on Greek puns within Latin are limited primarily to writing about specific examples or a specific Latin author, such as O' Hara (1996) and his discussion of Greek word use in Vergil. Similarly, the recent (2013) compilation of studies edited by Kwapisz, Szymanski, and Petrain examining Greek and Latin wordplay does not look at the intersection of the two. Building upon previous strategies for data mining including the Tesserae Project, this paper seeks to offer a digital means to cross-reference Latin literature with Greek texts in order to find puns. The programSince the formation of our collaboration, we have produced software capable of detecting potential candidates for Greek puns. To achieve this end, we first set out to account for the rules of pronunciation within Latin poetry by referencing works such as Halporn et al. (1980), with issues such as elision and the lack of pronunciation of certain syllables addressed within the software. We then looked to set equivalence of pronunciation between Greek and Latin letters and groups of syllables, such as diphthongs. This was accomplished through examining transliterated Greek names and words which appear in Latin literature and using grammars including Smyth (1920). Additionally, certain Greek vowels and consonants which could correspond to multiple vowels, consonants, or syllables in Latin were also considered.Accounting for these rules, we developed a processing system whereby Greek pronunciation equivalence is run through strands of Latin text. The software begins by reading one chosen Latin text in .txt format and then reading one or more Greek texts in .txt format, with all texts requiring UNICODE encoding for proper registration within the identification system. Once a Latin and Greek text(s) have been selected, the file paths are passed asynchronously into the model for processing. Both texts are then processed at the same time on different threads and the calling method waits for the completion of both methods.The Greek text string is first processed by stripping off any accent or breathing mark and returning the base letter to lower case. While iterating through all characters in the text, when a space or new line character is reached, the previous word captured is added to a list. A word is not added to the list if it has previously been added to the list or the length of the word is less than a number for which the program allows specification. The result of this method is a list of those unique Greek words which appear in a chosen Greek text.The Latin text string is first processed by converting all letters to lower case and taking account of the rules of elision and pronunciation. Similar to the Greek processing, the processed Latin text is converted into words. Each Latin word in the list is processed to determine if part of the word should be elided, based on the starting letter of the next word. The final step is to condense all the Latin words together into one text string without spaces, while keeping a list of the starting position of each word in the text string.Once the two processing methods reach completion, the model has the list of unique Greek words and a string of Latin words, including the starting position in the original text of each word. The final processing method compares the Greek words to the Latin text string to find matches and returns a list of matches, which include the starting position of the match, ending position of the match, and the Greek word that matched. First, each letter of every Greek word is converted to a list of possible Latin characters or pairs of characters. A dictionary exists that for each base Greek lower case letter there corresponds a list of Latin letters or pairs of letters. Similarly, the existence of a diphthong is determined by referencing the values of a Greek-Latin diphthong dictionary. Both dictionaries were developed by examining those grammars and references noted earlier. By iterating though every Greek letter in a Greek word the possible Latin letters can be looked up in the dictionary, and, if the current letter and next letter in the Greek word are in the diphthong dictionary, this will override the regular dictionary lookup function. Once a list of every possible Latin correspondence for every letter in a Greek word exists, all permutations of letters are returned in a list of strings. This list is the list of every possible Latin series of characters that could match a Greek word. Using a regular expression every match in the Latin text string is found based on the previous list. These results are returned as a list of Greek words, including the starting and ending position in the original Latin text of the match.The unique list of Latin-to-Greek results is returned to the view and is displayed in a grid, which can be sorted alphabetically, wherein the left column contains the uncompressed Latin character matches and the right column the Greek word equivalent in Greek characters. Also, a flow document is created which contains the original text with the matches highlighted in yellow and the matching Greek words to the right for each line. The Greek words in brackets are words that correspond to the same Latin text location, thus assisting the program user in contextualizing and applying significance to the results. Results and applicationsThrough operation of the software, possible candidates for Greek puns within various Latin authors have been identified. By cross-referencing opensource texts of Vergil's Aeneid and the Carmina of Catullus with the Homeric epics and Argonautica, we have discovered hereto unnoticed Greek words, the meanings of which often correspond to the thematic concerns of the passage. Considering the limited extent of the search, both in duration and in those texts analyzed, these results seem to suggest that there are a considerable number of Greek puns remaining to be found within the corpus of Latin literature. While we are currently still in the process of refining the software and in consideration of more comprehensive ways to cross-reference Greek texts, we are still looking to expand the search both towards those texts already analyzed and to other Latin authors. With a view to the future, it is our hope that this software will eventually become available for public use and that it can lead to a more nuanced view of the role which Greek culture and language plays in Latin literature.",
        "article_title": "Puns and Intertextuality: A Digital Approach to Greek Wordplay in Latin Literature",
        "authors": [
            {
                "given": "Evan",
                "family": "Brubaker",
                "affiliation": [
                    {
                        "original_name": "Tulane University",
                        "normalized_name": "Tulane University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04vmvtb21",
                            "GRID": "grid.265219.b"
                        }
                    }
                ]
            },
            {
                "given": "Brandon",
                "family": "Lafreniere",
                "affiliation": [
                    {
                        "original_name": "Tulane University",
                        "normalized_name": "Tulane University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04vmvtb21",
                            "GRID": "grid.265219.b"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Placing Segregation is the primary result of this research effort - a new, open access digital project at the University of Iowa Libraries' Digital Scholarship andPublishing Studio that explores research questions about housing segregation and socioeconomic disparities across nineteenth century American cities through a series of fully interactive maps and scholarly interpretations derived from the geolocated census data. This presentation introduces core functionality of the digital exhibit and also explains in detail the process of developing the data and the website.Built primarily upon Leaflet.js, an open-source JavaScript library for mobile friendly interactive maps, as well as other openly available JavaScript libraries such as Fuse.js, this digital map exhibit gives public audiences direct access to detailed census data for the years 1860 and 1870 in these select U.S. cities. A search feature allows visitors to quickly locate the historical residences of persons of interest and read complete census information about them, while a number of simple layer filters permit users to explore potentially infinite topics by giving them direct control over the geographic data. The primary digital exhibit gives audiences without GIS experience the power to ask research questions in a spatial environment.Initial research findings from the project indicate that urban historians have been substantially underestimating the degree of housing segregation experienced by free black residents, even though some past research built on ward-level analyses closely approximated the extent of housing segregation between the native white population and various immigrant groups. The data generated for this project illustrate American cities were in fact significantly divided according to social class and wealth, long before the rise of the automobile. However, residents were less divided by wealth differences than they were separated spatially by skin color in Washington D.C. or by German and Irish origin in Nashville and Omaha.  ",
        "article_title": "Placing Segregation",
        "authors": [
            {
                "given": "Robert",
                "family": "Shepard",
                "affiliation": [
                    {
                        "original_name": "University of Iowa Libraries",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "The Graph Poem Project at University of Ottawa develops tools for poetry computational analysis and applies graph theory and network graph computational apps in structuring, analyzing, and visualizing poetic corpora. The concept involves generating network graphs (multigraphs) in which the vertices are poems and the edges represent various commonalities between poems in terms of subject, diction, form, style, and other criteria. By computationally analyzing the network graph certain extremely interesting and potentially useful information can be extracted regarding a specific corpus. For instance, by identifying cut ver-tices, we find out which poem(s) play a crucial role in the connectivity of the network and therefore also in the cohesion of the corpus, since by removing that particular poem-node the whole network becomes disconnected , and thus the corpus per se without that particular poem would become disarticulate and divided ; and similarly, cut edges signal connections between certain poems that are of paramount importance in the connectivity of the entire network. Identifying cliques of nodes, on the other hand, for instance , shows how poems within a corpus are clustered and which parts of a specific oeuvre, collection, or anthology are more self-contained than others or which poems belong together across or within divides such as authorship, school, period or region, etc. More generally, representing corpora as network multi-graphs makes possible analyzing and visualizing both small and significantly large datasets (so far up to hundreds of thousands) of poems, and reach certain information and conclusions about both particular poems in that corpus and the corpus as a whole or as compared to other corpora. This is one of the first respects in which the issue of access becomes of dramatic importance for our research and for developing and testing our tools; and virtually for any contemporary digital project in poetry. Namely the access to databases and the ability to convert existing files into formats that are compatible to computational processing and analysis. The paper will focus on the existing databases, will review previous work in the field and analyze the kind of data they have employed, will examine the premises and prospects for big data and/or data intensive research in poetry (one of the main tenets of our project), and the arguable absence of such approaches in the published research in the area so far. Terms like \"crawl\" and \"rip\" for the poetry available online, and issues related to digitization and/or computationally analyzing poetry in print will also be placed under close scrutiny while we will also report on our own solutions and results , and compare them to those in other projects in digital humanities (DH), digital literary studies, or computational linguistics/analysis. Our research, tools, publications, and future work will therefore be then presented in a comparative manner in the wider context of current trends, theories , and debates in DH in general and text analysis in particular-by considering for instance the potential relevance of the Graph Poem Project to at least some of the issues raised in the \"Forum: Text Analysis at Scale\" section of Debates in the Digital Humanities 2016 (eds. Lauren F. Klein and Matthew K. Gold), or literature-related tools/projects such as Syuzhet or heureCLÉA-, in natural language processing (NLP)-by reviewing other projects that have dealt with literary computational analysis and particular poetry processing, but also other works that focused on various issues in computational linguistics, such as syntax or trope processing and analysis-, and in digital pedagogy, by relating it to other poetry related digital pedagogical projects (such as those reviewed by Chuck Rybak, for",
        "article_title": "Access(ed) Poetry. The Graph Poem Project and the Place of Poetry in Digital Humanities",
        "authors": [
            {
                "given": "Chris",
                "family": "Tanasescu",
                "affiliation": [
                    {
                        "original_name": "Université d'Ottawa (University of Ottawa)",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Diana ",
                "family": "Inkpen",
                "affiliation": [
                    {
                        "original_name": "Université d'Ottawa (University of Ottawa)",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Vaibhav ",
                "family": "Kesarwani",
                "affiliation": [
                    {
                        "original_name": "Université d'Ottawa (University of Ottawa)",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Bryan",
                "family": "Paget",
                "affiliation": [
                    {
                        "original_name": "Université d'Ottawa (University of Ottawa)",
                        "normalized_name": null,
                        "country": "Canada",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Woolf Online project, which recently completed its second phase of development at Loyola University Chicago, sought to address the following fundamental questions about the nature and development of literature:• how does a literary text come into being?• what kinds of influence are at work upon the writer during the process of initial composition, and thereafter?The Woolf Online project sought to investigate various ways in which different recoverable histories of a particular text could be used to illuminate the process of its composition. By recording the history of a particular text, its cultural, political, and autobiographical contexts and their interaction, visually we began to answer some of the following important questions:• how is textual history related to other histories of a text?• what use does literary criticism make of textual and contextual histories? Publication of Digital Scholarly EditionsAs part of the development of the Woolf Online project, we developed an extensible development and publication framework, called 'Mojulem', for editing, publication, and visualisation of digital scholarly editions. We are now continuing this development with the Verne Digital Corpus, which focuses upon the work of Jules Verne, including original French language editions and their myriad, often questionable, English language translations.Mojulem allows us to build on the concept of 'knowledge sites', as suggested by Peter Shillingsburg (2006), supplementing a core publication framework with modules/plugins such as OCR, editors, and image viewers.Mojulem also enables us to host multiple projects within one installed framework, thereby enabling cross-project research, where applicable, and the option to aggregate specified data. Development of Mojulem, with the Woolf Online project and Verne Digital Corpus as examples of the current ongoing working environments, initially followed the need for four underlying core structures. These structures include CorPix, CorTex, CorCode, and CorForm, which are detailed as follows. CorPixManuscripts and printed texts materially unite the iconic and lexical, the autographic and allographic, whereas all digital representations separate these constituent elements into images and transcriptions. With Mojulem projects, the common default display reunites the image and transcription by mapping the one to the other at the pixel level. CorPix software currently includes eHinman Transparent, TransparentOCR, Magnify, and Zoom. For example, pixel-level positioning and coordinate fixing is an inherent feature of both Transparent and TransparentOCR, within both editor and visualisation tools. eHinman is a digital adaptation of the original Hinman collator, and enables fade from the image of a page from one copy to another, thereby enabling a visual collation of multiple copies. Transparent is used within both the visualisation and editing stages of a project's development, enabling an editor and user alike to view the image as the primary entry consideration for the project. CorTexThe CorTex is the stable resource containing the merged or compacted plain text transcriptions of the variant expressions of a work. It stores all information about text and variations, ready to be extracted for display of variation amongst versions; it is not necessary to recompute them.The CorTex is the entity to which all standoff properties (markup, annotations, links, etc.) points, and on whose stability the system depends. It is as the source of each version's text and variation from other texts. The stability and endurance of the CorTex is protected by multiplying duplicate copies locked with a digital signature, which verifies for each user that a CorTex copy is viable. Analysis of the CorTex variable forms provides statistical feedback to guide the production of a conflated text, for example with the English language translations of a given Verne edition. Whilst these statistical results are no guarantee of an ultimately correct translation, they offer a conflated text with the highest viable agreement amongst the provided collated texts. Textual disagreements are currently resolved by assigning probability values, a higher value defining a greater probability of accuracy and agreement amongst the collated texts. Using such probability results, we are currently able to filter problematic passages in each translation to conflate a text with the highest probability of agreement amongst the translations per edition.These results can then be provided for further research and assessment, and act as a suitable starting guide for further analysis of the conflated text, and translation in the example of Verne's text. CorCodeCorCode is the add-on value of analysis, argument, and explanation. Mojulem stores markup separately, as standoff properties, applying it as the user invokes it for the rendering of a specific item's image or text within a given visualisation, such as a transparent view of a page of the Initial Holograph Draft of 'To the Lighthouse'. To do this, Mojulem includes an editor which saves text and encoding separately, and filters for converting legacy, code-embedded transcriptions, including TEI encoded documents, into separate forms with markup analysed into properties, and filters for reversing this process. CorFormA CorForm 8 is a CSS stylesheet, containing special formatting rules, used to transform the overlapping properties of the CorCode into HTML. Each CorCode has a default CorForm, but other CorForms can be used in combination or as alternatives. Since a CorTex may have many CorCodes, and each CorCode many CorForms, structuring or formatting of the text can be attained by specifying some combination of already available resources, or by supplying new ones.The CorPix, CorTex, CorCode, and CorForm are aggregated for a project within the Mojulem framework. Each such item is identified by a unique key, which is used as an index into the repository or database.In addition to the initial four cores, identified above, we have begun development of CorAssess, allowing effective assessment and analysis of Cor data for the Verne translations and conflated English language texts. CorAssessCorAssess works in tandem with the CorTex to provide analysis of statistical and end results relative to the conflated text output. CorAssess allows us to visualise where text has been conflated based upon resolved disagreements, the points of disagreement and resultant probabilities between collated texts per edition, variance between collated texts, and visualise alternative resolution patterns relative to variation distance for given points of disagreements in the conflated text. With Verne texts, for example, we will be able to visualise conflation decisions, and offer alternatives for given decisions and disagreements, where applicable, in our conflated English language translations. Why Verne?After Woolf Online, we chose to focus upon the corpus of Jules Verne, including original French language editions and English language translations. We have begun collecting, collating, and preparing digitised copies of as many digitised editions as extant online. We have also been digitising early editions to provide an ever-growing dataset of Verne material.The nature of early English language translations of Verne's editions is a continuing source of frustration for those interested in the works of Jules Verne. His early categorisation as a predominantly children's author in English language countries, unlike the publishing by Hetzel, coupled with early restricted access to original French language editions, simply compounded the issue.The corpus of Jules Verne offers an interesting opportunity for literary and contextual analysis coupled with data processing and automated analysis. The myriad existing digitised English language translations, including US and British variant editions, often more prevalent than their counterpart, original French editions in our current digitised corpus, allows us to examine the development of those texts by comparing agreements, disagreements, omissions, and continuing revisions in said translations since a novel's first edition. We are hoping to use this analysis to filter the noise of years of collective translations to collate a unified English translation for each French language edition.We will then be offering a comparison of French language edition against a filtered, collated English language edition. This will allow further consideration of the requisite merits of the English language translation directly juxtaposed to the original French language edition. ConclusionThe development and combination of the initial four cores, CorPix, CorTex, CorCode, and CorForm, within the modular and adaptable framework Mojulem, allowed the second phase of the Woolf Online project to begin to approach the fundamental questions about the nature and development of literature, as briefly outlined in the introduction. With the addition of CorAssess, we are now beginning to address additional issues with the publication, transmission, and development of texts. We are also testing, and proving, the viability of Mojulem beyond the Woolf Online project.The corpus of Jules Verne provides a particularly fascinating opportunity to test these cores, and provide a resultant conflated, English language translation per extant French language edition. This paper will briefly introduce the Mojulem framework and its initial four cores, grounded in the example of the Woolf Online project, and detail the ongoing developments to augment this work with the above new work on the corpus of Jules Verne, and the ongoing Verne Digital Corpus  ",
        "article_title": "A Cor infrastructure for textual analysis -From Woolf to Verne",
        "authors": [
            {
                "given": "Nicholas",
                "family": "Hayward",
                "affiliation": [
                    {
                        "original_name": "Loyola University Chicago",
                        "normalized_name": "Loyola University Chicago",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/04b6x2g63",
                            "GRID": "grid.164971.c"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe study of character speech is a topic of fairly consistent interest among digital literary scholars. It is usually acknowledged that voices of characters are essentially different from narrator's own voice and should be treated separately. Some researchers have fictional dialogue removed from the texts they studied before any tools of computational investigation are applied (Hoover, 2004). Quite a lot of effort has been made recently to address the problem of identifying character speech in prose and attributing it to the correct speaker (ссылки!). One of the outcomes of such research is the possibility to study voices of different characters on relatively large scale and apply computational tools that measure their recurring stylistic parameters. MethodThe study of character speech has traditionally had strong ties to the fields of stylometry and authorship attribution, as their methods proved quite useful for studying idiolect of a fictional speaker. Suffice it to say that one of the seminal works in stylometry, Computation into criticism# by Burrows (Burrows, 1987), was focused on the study of character speech in Jane Austen's novels. The method developed by Burrows grew into what is currently known as Delta, a widelyadopted standard for authorship attribution. Delta has been consistently and successfully applied to identifying the author of an unattributed text of different languages and genres, but at the same time it saw considerable usage as a purely stylometric tool for the study of text where authorship is undisputed. Among other things, this included research into the specific idiolects of fictional characters (see, for example, Rybicki, 2006).In our research Delta was used as one of the two possible approaches to studying character voices in Leo Tolstoy's War and peace. Much like in case of Senkewic (Rybicki, 2006), there's certain critical opinion (Eikhenbaum, 2009) that Tolstoy's characters are quite distinct from each other in their speech. Our own experience of carefully reading speech instances extracted from War and peace (for details on extraction procedure see (Skorinkin, Bonch-Osmolovskaya, 2015) supports the opinion. So it seemed natural to try and test computational methods that already showed their applicability to precisely such task. We used R package stylo by (Eder et al, 2013) Testing the method on Russian materialSurprisingly enough, we were unable to find any work that applied Delta to any Russian material. Therefore we felt obliged to conduct a couple of experiments that would test its general applicability to Russian before we proceed with character speech. At the first stage we tried Delta's ability to distinguish between Tolstoy and Dostoevsky. The training set contained one of the six parts of Dostoyevsky's Crime and Punishment and three of the fifteen books of Tolstoy's War and Peace. The remaining 18 pieces of text (5 by Dostoevsky and 13 by Tolstoy) constituted the test set. The results with different settings can be seen in Table  1   The second experiment involved four Russian authors Tolstoy, Dostoevsky, Goncharov and Turgenev. All four represent (roughly) the same epoch of Russian literature and all four are recognized as masters of realistic prose. We used three novels by each author for our experiment. At the first stage two out of each three were placed in the training corpus, and Delta was supposed to attribute the remaining one. All four novels from the test corpus were attributed correctly. At the second stage we reverted the experiment and left only one novel by each author in the training set. In this case Delta consistently showed 7 out of 8 correct attributions (the only mistake being Tolstoy's Family Happiness incorrectly attributed to Dostoevsky.A possible explanation could be that Family Hap-piness is written in first person from the point of view of a young woman, something uncommon for Tolstoy; and the only Dostoevsky's work the training corpus contained was The Insulted and Humiliated , also a firstperson narrative). Fig. 3 shows Delta scores for all the novels visualized with help of principal component analysis.  Applying Delta to Tolstoy's charactersHaving thus shown that Delta is applicable to Russian, we proceeded with our experiment. In the first place we applied the method to top 5 characters by the total number of speech instances. We split the total sets of speeches by each character and then tried authorship attribution The results are shown in Table 2. The most common mistakes are between princess Marya Bolkonskaya and Natasha Rostova and between prince Andrew Bolkonsky and Pierre Bezukhov. Their closeness can be seen in Figure 4: The quality of speech authorship attribution inevitably got worse once we expanded our selection from 5 to 15 characters. The results were still quite tolerable reaching 10 out of 15 with certain settings. The analysis of mistakes showed that a) they're less likely to occur between characters of different gender and b) the mistaken characters have quite a lot of mutual conversations.Further on, we decided to pay more attention to overall Delta scores of character voices and see if they give us any meaningful clustering of characters. Figure  6 shows PCA of characters based on Delta. One can easily see the clustering of Rostov family, to a lesser extent this applies to Bolkonsky family as well. Dolokhov, Denisov and Kutuzov could constitute the 'war' cluster.We then made another expansion and moved from 15 to 30 characters. Figure 7 demonstrates PCA of Delta scores for this selection. The most striking thing here is the obvious separation of Vera Rostova from the rest of Rostov family. The difference between cold, tempered and rational Vera and her emotional and very sentimental relatives is outspoken and obvious to the reader, but it seems valuable to have this potential quantiative support in the form of different Delta scores. What is even more striking is that Vera is quite close to Berg, a rationalizing careerist who becomes her husband. Note also the closeness of Boris and Julie Karagine, another pragmatic couple happily united in a marriage of convenience. Applying alternative featuresHaving tried Delta, we proceeded with our own set of alternative features for character voice analysis (a typical step, as dectibed in Eder, 2015). These features are not related to the lexical makeup of character speech and attempt to reduce the influence of genderrelated morphological features of Russian language and the factor of mutual interactions between characters. At this stage we limited ourselves to four features only: the average number of words, the ratio of exclamatory sentences, the ratio oа question sentences and the ratio of punctuation marks (latter being a crude approximation of the 'disruptedness' of speech, which seems rather typical of certain more emotional and lively characters).When the character set is limited to 5 characters these features even manage to distinguish character speech with some tolerable accuracy (though worse than Delta). However, the analysis of mistakes shows that they capture fundamentally different types of similarities than Delta does. For instance, joyful Natasha in this case is never mistaken for sentimental and melancholic Marya, but rather for her boisterous brother Nikolai. Pierre, on the other hand, is mistaken for Marya rather than for Andrey, who is distinct from them all. Figures 8 and 9 show the results of PCA and hierarchical clustering for these characters based on our own alternative features. If we compare figures 8 and 9 to their counterparts from the Delta experiment (figures 4 and 5) we can see that the alternative features ignore gender or mutual interactions/ The hypothesis is that they enable us with a more indepth view of a character personality, his/her emotional type and so on. Figures 10-12 show data on wider selections of characters using alternative features. Fig. 1 .1Fig. 1. Delta PCA on 150 most frequent character 4-grams, Tolstoy vs Dostoevsky Fig. 3 .3Fig. 3. Delta PCA for 12 Russian novels of 1850-1870-ies, 250 most frequent words Fig. 4 .4Fig. 4. Delta PCA for top 5 most talkative characters in War and Peace, 100 most frequent words Fig. 6 .6Fig. 6. Delta-based PCA for top 15 most talkative characters in War and Peace, 100 most frequent words Fig. 7 .7Fig. 7. Delta-based PCA for top 30 most talkative characters in War and Peace, 100 most frequent words Fig. 8 .8Fig. 8. PCA for top 5 most talkative characters in War and Peace, 4 alternative features Fig. 10 .10Fig. 10. PCA for top 15 most talkative characters in War and Peace, alternative features  Table 2 .2 ",
        "article_title": "Character-distinguishing features in fictional dialogue: the case of War and Peace",
        "authors": [
            {
                "given": "Daniil",
                "family": "Skorinkin",
                "affiliation": [
                    {
                        "original_name": "National Research University Higher School of Economics",
                        "normalized_name": "National Research University Higher School of Economics",
                        "country": "Russia",
                        "identifiers": {
                            "ror": "https://ror.org/055f7t516",
                            "GRID": "grid.410682.9"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "UC Los Angeles, United States of America As humanities scholars embrace digital research and more content is digitized, tools and methods such as text mining and spatial analysis become increasingly more vital, especially to graduate students who are at a formative stage in their careers. Librarians and library staff are keen to support these researchers and meet the growing demand. To address this demand, UCLA librarians developed DResSUP (Digital Research Start Up Partnerships), a six-week summer program designed to create partnerships between library staff and researchers. At our institution, a core group of library staff have the expertise to engage with digital research projects beyond the initial phase of data discovery. Therefore DResSUP was initially built around this core group, allowing us to build a local community of practice and test our capacity for supporting digital projects. We have purposely kept the program small; working with a cohort of four to six graduate student \"partners\" each summer. Our strategy was, no doubt, also influenced by discussions of sustainability in the Digital Humanities community (see, for example, Maron and Pickle, 2014). What if we built a new program as though we were already in the sunset period? How could we minimize costs, maximize impact, build deep connections with researchers, respect ethical issues of graduate student labor, and facilitate engagement between library staff and researchers? We knew about minimal computing, but could we create a \"min-imal program\" and still have an impact? The goal of the program is to provide graduate students with skills and methods to continue the projects on their own. Rather than doing projects for them, we focus instead on teaching students to start with a small sample of their own dataset, working it through the research data lifecycle: collecting, cleaning, and analyz-ing/visualizing data, supplemented by a workshop on project management and the specific tools that are applicable to the students' research projects. In the second half of the program, we focus on the execution of their individual projects, building prototypes and adjusting workflows that allow them to complete their projects independently. After successfully offering this program for the past two years, we are expanding the program in ways that we hope will increase the library's capacity to support digital scholarship, which will make the program more sustainable over time and allow us to reach more researchers. Perhaps because we took a low-profile, grassroots approach, and because of general resistance to \"reskilling\" or \"retooling\" efforts, the most surprising response to DResSUP for us has been the enthusiasm and curiosity of our library colleagues. As they have learned more about the program, they have expressed interest in participating. However, at the same time, they raised legitimate concerns about their ability to support advanced digital research methods. Recognizing that they face a steep learning curve, one that will require additional resources to surmount, we have designed an expansion to the program, developing a second track for professional development for librarians. Similar to Columbia University Library's Developing Librarian and Indiana's Research Now librarian training programs, we will take a project-based approach to professional development for librarians. But, rather than developing a separate program, librarians will participate in the extended DResSUP program , starting earlier in the year, gradually merging with the summer DResSUP graduate students. We will begin working with a small cohort of librarians in Jan-uary 2017, who will follow the same curriculum as the graduate students. By Summer, the librarian cohort will have a plan for a collaborative digital research project. Our short paper will begin by presenting DResSUP as it has functioned the past two years and our plans for the next three years, based on preliminary findings from this second librarian track of DResSUP which, by DH2017, will be well underway. We reason that by building capacity in our library community among permanent staff we can extend and grow DResSUP in",
        "article_title": "Libraries and Digital Research: Sharing the Incubator",
        "authors": [
            {
                "given": "Zoe",
                "family": "Borovsky",
                "affiliation": [
                    {
                        "original_name": "University of California, Los Angeles (UCLA)",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Claudia",
                "family": "Horning",
                "affiliation": [
                    {
                        "original_name": "University of California, Los Angeles (UCLA)",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Dawn",
                "family": "Childress",
                "affiliation": [
                    {
                        "original_name": "University of California, Los Angeles (UCLA)",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " IntroductionThe Marianne Moore Digital Archive (MMDA) has begun to publish digital editions of the 122 manuscript notebooks of Modernist poet Marianne Moore. The notebooks contain a plethora of materials, include reading notes, recorded conversations Moore participated in or overheard, drafts of poems, travel descriptions, financial records, notes on concerts, lectures, classes, and sermons Moore attended, and other miscellanea documenting an active and increasingly prominent life as part of New York's literary and cultural scene from the 1910s to the 1960s. These unique resources pose significant resistance to accessing the trove of literary and cultural data they contain: they have never been published in any form, can be visited by appointment at a library in Philadelphia for only limited hours during the week, most of the notebooks are written in a cramped, uneven hand with frequent abbreviation and shorthand, Moore repurposed calendars and other cheap or free items that include preprinted material interspersed with her writing, many notebooks are now too fragile for normal handling, and previous attempts at conservation have led to the disordering of many pages. The unfamiliar and sometimes obscure historical and cultural references that are needed to trace the broad scope of Moore's readings and activities present an additional layer of difficulty for most of the potential audience that could take advantage of the notebooks for literary, historical, or cultural research. In response to these challenges, the MMDA has created a team of Moore scholars and scholarly digital editors at three universities and partnered with the Center for Unified Biometrics and Sensors (CUBS), an automated handwriting recognition and machine learning lab at the University at Buffalo. This paper reflects on our efforts to increase access with respect to the notebooks in several senses: design choices with respect to the digital editorial apparatus needed to make the notebooks usable to non-specialists; the implementation of a HubZero 2.0 collaborative hub platform, designed for use in the sciences and engineering, for a humanities project; and the development of a customized, integrated editor/viewer based on existing digital manuscript editing tools that can enable non-technical editors to participate more directly in the digital workflow. The hub platform and workflow solution are primarily designed to support the specific research needs of the MMDA and its users, however the project code and our methodologies can be readily applied to other digital editing projects and digital humanities collaborations. AbstractMarianne Moore ) was among the foremost modernist poets of the early twentieth-century. Her work contributed to the revolution in poetic form and conceptions of poetry occurring during the 1910s and 1920s and she remained a poet of profound reflection and innovative design throughout her lifetime. In particular, Moore was among the first to conceive the poem as constructed through both language and visual design on the page and she was the first Anglo-American poet to divorce the poetic line from syntax. While her peers such as T. S. Eliot, Ezra Pound, William Carlos Williams, Wallace Stevens, and H.D. often excoriated each others' work, all were profound admirers of Moore's thought and poetry (see, for example, Leavell 2014, and Miller, 1995and 2005. Moore was also significant to modernism in her decades-long reviewing of work by her contemporaries, her publication of essays on aesthetic and cultural topics, and in her editing of one of the premier periodicals of modernist literature and art, The Dial, from 1925-1929. Moore received the Pulitzer Prize, the Bollingen Prize, the National Book Award, and the Gold Medal of the National Institute of Arts and Letters-among other awards. Additionally, she was made Chevalier de l'ordre des Lettres in France and received a Gold Medal Award for Lifetime Achievement from the Poetry Society. Her notebooks constitute a unique and extraordinary resource for understanding the composition, experience, and intellectual alertness of a brilliant thinker and poet to a very broad range of popular, mundane, intellectual, artistic, and historically significant events of her time.The MMDA is making digital reproductions and transcriptions of Moore's notebooks readily accessible to scholarly, classroom, and non-academic readers for the first time. The transcriptions are supported by annotations contextualizing Moore's writing and life, including citations to the original source texts she invokes, and an image-text linking feature that makes it easy to move back and forth between the facsimile and the transcription. The digital editions of the notebooks are supported by a growing collection of related materials, such as indexes, a glossary, an interactive timeline of Moore's life and publications, searchable reproductions of the now hard-to-find Marianne Moore Newsletter, and faceted text and image search tools currently under development. This site will, we hope, revolutionize criticism on this significant poet; contribute to popular understanding of the modernist period's history and culture; and develop new tools for the digital editing and publication of handwritten materials.Moore's notebooks offer extraordinary challenges for editors and digital designers because they include multiple genres, images (Moore frequently sketched objects that interested her), genetic layers of text (evidence of Moore's later editing of and additions to earlier notes, sometimes with different writing implements), and references to decades' worth of popular and academic source materials. Fully edited and annotated, Moore's notebooks suggest the deep cultural genesis of her poems: Moore's notebooks constitute not nearly finished drafts of poems and essays (that is, pre-publication texts) but, instead, a rich and varied collection of notes and compilations from which Moore drew the materials that went into her drafts and published work. Part commonplace book, part scrapbook, part sketchbook, part diary, each of Moore's notebooks offers an extraordinary window into the eclectic print and visual culture of the twentieth century and the ways in which one of America's most innovative poets responded to her world.The MMDA addresses the special challenges and opportunities presented by these documents through a customized version of the Edition Visualization Technology (EVT), an open-source software project directed by Roberto Rosselli Del Turco (Del Turco et al, 2014-15). We have extended the tool significantly to allow for the transcription and facsimile coordinates to be edited directly in the browser, then saved automatically to underlying TEI XML files according to our project's customized TEI schema (Our viewer/editor tool was developed on the basis of the EVT version 1.0, which utilizes XSLT, CSS, and Javascript to generate its viewer. The EVT version 2 recently released utilizes a different method that removes the need for XSLT and relies on converting the TEI XML documents into JSON, which we are considering as an alternative for future development. See the EVT blog for more information about the proposed changes for version 2). This has significantly extended the utility of the EVT, and we will continue to add more advanced editing features for modern manuscript collections. This combination of editing and publishing capacity makes the editing of these manuscripts accessible to any of our registered team members directly in the browser, allowing us to address the needs of the site's editorial team and its users through the development of an integrated web platform, which we are continuing to develop along with manuscript editing tools being developed at the University at Buffalo in collaboration with the Computer Science and Engineering and the Center for Unified Biometrics and Sensors (CUBS) at the University at Buffalo. Our partners in CUBS are also developing automated handwriting recognition software that we are integrating into our platform. We aim to create a complete editorial workflow solution for transcription and annotation of manuscript collections. This is possible through the use of the HubZero 2.0 platform, based on Joomla, which to our knowledge has not previously been used for a digital humanities project.The MMDA is making a vast quantity of unpublished writing by a major American poet and cultural figure freely available and easily accessible for the first time. Such access will have extraordinary impact. It is already transforming Moore studies and, as the number of publications on the site increases, it will contribute significantly to historical, cultural, and literary studies of modernism and of twentieth-century women's lives. The specialized tools and platforms for collaboration on digital manuscript editions we are developing will, we hope, serve as useful models for making manuscript collections more accessible to scholars and editors in collaboration.  ",
        "article_title": "Multiplying Access: the Marianne Moore Digital Archive's Tools and Methods for Collaboration",
        "authors": [
            {
                "given": "Nikolaus",
                "family": "Wasmoen",
                "affiliation": [
                    {
                        "original_name": "University at Buffalo, State University of New York (SUNY)",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " Structure of the WorkshopPart 1: Visual programming, workflows, data input and preprocessing First, we will show the basics of Orange: how to load the data, inspect and visualize it. Participants will be introduced to several options for data import, from standard Corpus to Twitter, Guardian and Text Import. Once the corpus is loaded, we will preprocess it and display the result in a word cloud. A particular emphasis will be on the use of custom preprocessing techniques and how to successfully apply them to the corpus. The results of each technique will be observed in an interactive word cloud and concordances. Next, we will use Twitter data to construct an author prediction pipeline and test some classifiers. We will fetch author Timelines from Twitter and observe the retrieved corpus. This time we will introduce a pre-trained tweet tokenizer and pass the preprocessed corpus through a bag of words. We will discuss bag of words parameters and how to best prepare the data for further analysis. The results of using different parameters will be observed in a data table to understand the underlying data structures. For comparison, we will use deep-learning-based embedding to derive vector representation of tweets and in this way enable machine learning.We will explain how we can use machine learning in text mining and introduce a number of techniques for predictive analysis. We will use cross-validation to test the constructed bag of words models and compare classification scores for each algorithm. We will discuss the quality of constructed models and what scores are usually the best for observing model quality. Additionally, we will inspect misclassified tweets in a confusion matrix and even further in Corpus Viewer, to leverage the possibilities of a close(r) reading. Part 3: Data clustering, sentiment analysis, image and geo analyticsIn the third part, we will work on geomapping and image analytics. We will transform textual and visual data into feature vectors and plot these data onto a world map to discover interesting relations.We will discuss how to acquire geolocated data from Twitter and why this is useful. Next, we will use geotagged Twitter data and apply a pre-trained sentiment analysis model to acquire sentiment orientation. We will map the sentiment-tagged tweets and explore how to use sentiment together with geomapping.Finally, the participants will be introduced to image analytics for humanities research. We will explain why and how to transform raw images into multidimensional vectors and how to work with the new data. We will cluster Instagram images into groups and explore how to map image-containing tweets on a world map. Do images correspond to geolocation? We will see. ",
        "article_title": "Hands on Text Analytics with Orange",
        "authors": [
            {
                "given": "Ajda",
                "family": "Pretnar",
                "affiliation": [
                    {
                        "original_name": "University of Ljubljana",
                        "normalized_name": null,
                        "country": "Slovenia",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Niko ",
                "family": "Colnerič",
                "affiliation": [
                    {
                        "original_name": "University of Ljubljana",
                        "normalized_name": null,
                        "country": "Slovenia",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Lan",
                "family": "Žagar",
                "affiliation": [
                    {
                        "original_name": "University of Ljubljana",
                        "normalized_name": null,
                        "country": "Slovenia",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": " PhotogrammetryPhotogrammetry (generating 3D models from a series of partially-overlapping 2D images) is quickly gaining favor as an efficient way to develop models of everything from small artifacts that fit in a light box to large archaeological sites, using drone photography. Stitching photographs together, generating point clouds, and generating the dense mesh that underlies a final model are all computationally-intensive processes that can take up to tens of hours for a small object to weeks for a landscape to be stitched on a highpowered desktop. Using a high-performance compute cluster can reduce the computation time to about ten hours for human-sized statues and twenty-four hours for small landscapes. Generating a dense cloud, in particular, sees a significant performance when run on GPU nodes, which are increasingly common in institutional HPC clusters and available through Compute Canada and XSEDE.One disadvantage of doing photogrammetry on an HPC cluster is that it requires use of the command line and Photoscan's Python API. Since it is not reasonable to expect that all, or even most, scholars who would benefit from photogrammetry are proficient with Python, UC Berkeley has developed a Jupyter notebook that walks through the steps of the photogrammetry process, with opportunities for users to configure the settings along the way. Jupyter notebooks embed documentation along with code, and can serve both as a resource tool for researchers who are learning Python, and as a stand-alone utility for those who want to simply run the code, rather than write it. Indiana University, on the other hand, has developed a workflow using a remote desktop interface so that all the GUI capabilities and workflows of PhotoScan are still available. A python script is still needed so that the user may avail herself of the compute nodes, but the rest of the workflow is very similar traditional PhotoScan usage. Finally, both methods offload the processing the HPC cluster, allowing users to continue to work on a computer that might normally be tied up by the processing demands of photogrammetry.The workshop will give participants hands-on experience creating a 3D model using two different approaches: first, by accessing the Photoscan graphical user interface on a virtual desktop running on XSEDE's Jetstream cloud resource; and second, by using a Jupyter notebook running on an HPC cluster. OCROptical Character Recognition (OCR) is a tool used for extracting text from images and is perhaps most well known as a core technology behind the creation of the Google Books and HathiTrust corpora. OCR continues to open historical texts for analysis at large scale, fuelling a significant portion of research work within the digital humanities to the point that it would be difficult to think of the \"million books problem\" existing without this technology. While there are many OCR tools available the most popular tool that is also free and open source is Tesseract.This portion of the workshop will also make use of Jupyter Notebooks to provide templates for learning the development process and that can then be taken away to speed development of future code. We will feature two projects for participants to practice with. A \"traditional\" OCR task that will have workshop participants processing images from the London Times in a demonstration of the improvements in OCR over the past few years and a task focusing on processing historical photographs to find text that can be added to the associated metadata to improve the searchability of an index. Target AudienceWe anticipate that this workshop will appeal particularly to scholars who work with cultural heritage materials (a field where photogrammetry is an increasingly common method for generating digital surrogates), as well as those who work with archival photographs, and scholars with large corpora of photographs. It will also be relevant for scholars who already engage in computational analysis of primary sources, who wish to increase the efficiency of their analysis by leveraging high-performance compute environments. No previous experience with HPC environments is necessary. This workshop can accommodate 25 participants. Instructors Quinn DombrowskiQuinn is the Humanities Domain Expert at Berkeley Research Computing. At UC Berkeley, Quinn works with humanities researchers and research computing staff at Research IT to bridge the gap between humanities research questions and campus-provided resources for computation and research data management. She was previously a member of the program team for the Mellon-funded cyberinfrastructure initiative Project Bamboo, has led the DiRT tool directory and served as the technical editor of DHCommons. Quinn has an MLIS from the University of Illinois, and a BA and MA in Slavic linguistics from the University of Chicago. Tassie GniadyTassie manages the Cyberinfrastructure for Digital Humanities group at Indiana University. She has a PhD in Early Modern English Literature from the University of California-Santa Barbara where she began her digital humanities journey in 2002 under the wing of Patricia Fumerton. She coded the first version of the NEH-funded English Broadside Ballad Archive, making many mistakes and learning much along the way. She now has an MIS from Indiana University, teaches a digital humanities course in the Department of Information and Library Science at IU, and holds regular workshops on text analysis with R and photogrammetry. Megan Meredith-LobayMegan Meredith-Lobay is the digital humanities and social sciences analyst, as well as the Vice President, for Advanced Research Computing at the University of Briitsh Columbia. She holds a PhD from the University of Cambridge in medieval archaeology where she used a variety of computing resources to investigate ritual landscapes in early medieval Scotland Scotland. Megan has worked at the University of Alberta where she supported research computing for the Faculty of Arts, and at the University of Oxford where she was the programme coordinator for Digital Social Research, an Economic and Social Research Council project to promote advanced ICT in Social Science research. John SimpsonJohn Simpson joined Compute Canada in January 2015 as a Digital Humanities Specialist and bringing a diverse background in Philosophy and Computing. Prior to Compute Canada, he was involved in a research-intensive postdoctoral fellowship focusing on developing semantic web expertise and prototyping tools capable of assisting academics in consuming and curating the new data made available by digital environments. He has a PhD in Philosophy from the University of Alberta, and an MA in Philosophy and BA in Philosophy & Economics from the University of Waterloo. In addition to his role at WestGrid, John is also a Member-at-Large of the Canadian Society for Digital Humanities (CSDH-SCHN), a Programming Instructor with the Digital Humanities Summer Institute (DHSI), and the national coordinator for Software  ",
        "article_title": "High Performance Computing for Photogrammetry and OCR Made Easy",
        "authors": [
            {
                "given": "Quinn",
                "family": "Dombrowski",
                "affiliation": [
                    {
                        "original_name": "UC Berkeley",
                        "normalized_name": "University of California, Berkeley",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01an7q238",
                            "GRID": "grid.47840.3f"
                        }
                    }
                ]
            },
            {
                "given": "Tassie",
                "family": "Gniady",
                "affiliation": [
                    {
                        "original_name": "Indiana University",
                        "normalized_name": "Indiana University",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/01kg8sb98",
                            "GRID": "grid.257410.5"
                        }
                    }
                ]
            },
            {
                "given": "Megan",
                "family": "Meredith-Lobay",
                "affiliation": [
                    {
                        "original_name": "University of British Columbia",
                        "normalized_name": "University of British Columbia",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/03rmrcq20",
                            "GRID": "grid.17091.3e"
                        }
                    }
                ]
            },
            {
                "given": "John",
                "family": "Simpson",
                "affiliation": [
                    {
                        "original_name": "University of Alberta",
                        "normalized_name": "University of Alberta",
                        "country": "Canada",
                        "identifiers": {
                            "ror": "https://ror.org/0160cpw27",
                            "GRID": "grid.17089.37"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    },
    {
        "url": null,
        "identifier": {
            "string_id": null,
            "id_scheme": null
        },
        "abstract": "Galleries, libraries, archives, and museums (GLAMs) increasingly seek to make digitized and born-digital collections accessible as data optimized for computational methods and tools common to the Digital Humanities. Preparation and publication of collections as data extends possible collection use beyond the analog object interactions that collection interfaces tend to try and emulate. In line with open data efforts, libraries, archives, and museums typically work to assign open licenses to these data. Current access methods are widely divergent, spanning simple provision of compressed collection objects in ZIP files, exposing static collection websites that can be crawled using a tool like rsync, leveraging Github for text collection access, provisioning an API, enabling FTP access to collections, mediating computational processes performed on collection data through a platform, to facilitating data access through use of torrent technology. Concurrently, in response to researcher requests for data-mining, commercial publishers have developed a range of processes for delivering proprietary corpuses with terms and conditions that significantly limit or expressly forbid data sharing, including providing libraries with physical hard drives loaded with the data. There are no consensus-driven best practices that guide the generation, description, and provisioning of computationally amenable GLAM collections for the range of communities that fall within the Digital Humanities. Without best practices in this space, institutions run the risk of misplaced investment of resources that foster the creation of irregular, ultimately disorienting data access environments. Indeed , the panoply of institutional approaches poses a challenge to GLAM institutions seeking best practices and clear guidelines for publishing collections as data. One major barrier to the development of consensus driven best practice is an incomplete understanding of how digital humanists, among others, are using and reusing cultural heritage data. This workshop aims to make progress towards bridging that gap. Research indicates that types of use exhibited by digital humanists include but are not limited to text analysis, image analysis, mapping, sound analysis, and network analysis. Orientation to the full scope of academic use types can be gained through in-depth analysis of data use practices across disciplines as represented in core Digital Humanities journals (Padilla and Higgins 2016), by reviewing works at the annual global Digital Humanities conference (Weingart 2016), and by studying edited volumes that have to this point effectively compiled a broad range of research in this space (Gold 2012; Gold and Klein 2016; Burdick, Drucker, Lunen-feld et al 2012; Schreibman, Siemens, Unsworth 2016). This workshop will build upon this orientation by engaging directly with digital humanists' existing and projected research and pedagogical practices that draw upon ever growing GLAM collections. Blending short talks by practitioners, guided discussion, and workshopping of the organizers' draft framework (further described below), the workshop will focus on how researchers and educators use GLAM collections that have been made accessible as data, and will extend to consider how these uses should inform collection creation and access.",
        "article_title": "Shaping Humanities Data: Use, Reuse, and Paths Toward Computationally Amenable Cultural Heritage Collections",
        "authors": [
            {
                "given": "Thomas",
                "family": "Padilla",
                "affiliation": [
                    {
                        "original_name": "UC Santa Barbara",
                        "normalized_name": "University of California, Santa Barbara",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/02t274463",
                            "GRID": "grid.133342.4"
                        }
                    }
                ]
            },
            {
                "given": "Sarah",
                "family": "Potvin",
                "affiliation": [
                    {
                        "original_name": "Texas A&M University",
                        "normalized_name": null,
                        "country": "United States",
                        "identifiers": {
                            "ror": null,
                            "GRID": null
                        }
                    }
                ]
            },
            {
                "given": "Laurie",
                "family": "Allen",
                "affiliation": [
                    {
                        "original_name": "University of Pennsylvania",
                        "normalized_name": "University of Pennsylvania",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00b30xv10",
                            "GRID": "grid.25879.31"
                        }
                    }
                ]
            },
            {
                "given": "Stewart",
                "family": "Varner",
                "affiliation": [
                    {
                        "original_name": "University of Pennsylvania",
                        "normalized_name": "University of Pennsylvania",
                        "country": "United States",
                        "identifiers": {
                            "ror": "https://ror.org/00b30xv10",
                            "GRID": "grid.25879.31"
                        }
                    }
                ]
            }
        ],
        "publisher": null,
        "date": "2017",
        "keywords": null,
        "journal_title": "ADHO Conference Abstracts",
        "volume": null,
        "issue": null,
        "ISSN": [
            {
                "value": null,
                "type": null
            }
        ]
    }
]